[{"id": "1909.00061", "submitter": "John Sun", "authors": "John Sun, Christopher S. Wang, Ellie S. Krossa", "title": "Investigating Sprawl using AIC and Recursive Partitioning Trees: A\n  Machine Learning Approach to Assessing the Association between Poverty and\n  Commute Time", "comments": "35 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sprawl, according to Glaeser and Kahn, is the 21st century phenomenon that\nsome people are not dependent on city-living due to automobiles and therefore\ncan live outside public transportation spheres and cities. This is usually seen\nas pleasant and accompanied by improved qualities of life, but as they\naddressed, the problem remains that sprawl causes loss of jobs for those who\ncannot afford luxurious alternatives but only inferior substitutes (Glaeser and\nKahn 2004). Therefore, through our question, we hope to suggest that sprawl has\noccurred in the U.S. and poverty is one of the consequences.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:35:44 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 20:09:24 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2019 16:29:33 GMT"}, {"version": "v4", "created": "Thu, 13 Feb 2020 13:31:02 GMT"}, {"version": "v5", "created": "Sat, 27 Feb 2021 17:51:42 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sun", "John", ""], ["Wang", "Christopher S.", ""], ["Krossa", "Ellie S.", ""]]}, {"id": "1909.00566", "submitter": "Sascha Timme", "authors": "Bernd Sturmfels, Sascha Timme, Piotr Zwiernik", "title": "Estimating linear covariance models with numerical nonlinear algebra", "comments": "23 pages, 2 figures, 5 tables", "journal-ref": "Alg. Stat. 11 (2020) 31-52", "doi": "10.2140/astat.2020.11.31", "report-no": null, "categories": "stat.CO math.AG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical nonlinear algebra is applied to maximum likelihood estimation for\nGaussian models defined by linear constraints on the covariance matrix. We\nexamine the generic case as well as special models (e.g. Toeplitz, sparse,\ntrees) that are of interest in statistics. We study the maximum likelihood\ndegree and its dual analogue, and we introduce a new software package\nLinearCovarianceModels.jl for solving the score equations. All local maxima can\nthus be computed reliably. In addition we identify several scenarios for which\nthe estimator is a rational function.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 06:56:41 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Sturmfels", "Bernd", ""], ["Timme", "Sascha", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1909.00698", "submitter": "Denis Belomestny", "authors": "Denis Belomestny and Leonid Iosipoi", "title": "Fourier transform MCMC, heavy tailed distributions and geometric\n  ergodicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods become increasingly popular in applied\nmathematics as a tool for numerical integration with respect to complex and\nhigh-dimensional distributions. However, application of MCMC methods to heavy\ntailed distributions and distributions with analytically intractable densities\nturns out to be rather problematic. In this paper, we propose a novel approach\ntowards the use of MCMC algorithms for distributions with analytically known\nFourier transforms and, in particular, heavy tailed distributions. The main\nidea of the proposed approach is to use MCMC methods in Fourier domain to\nsample from a density proportional to the absolute value of the underlying\ncharacteristic function. A subsequent application of the Parseval's formula\nleads to an efficient algorithm for the computation of integrals with respect\nto the underlying density. We show that the resulting Markov chain in Fourier\ndomain may be geometrically ergodic even in the case of heavy tailed original\ndistributions. We illustrate our approach by several numerical examples\nincluding multivariate elliptically contoured stable distributions.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 12:54:26 GMT"}, {"version": "v2", "created": "Tue, 31 Dec 2019 14:03:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Belomestny", "Denis", ""], ["Iosipoi", "Leonid", ""]]}, {"id": "1909.00836", "submitter": "Ivan Fernandez-Val", "authors": "Shuowen Chen, Victor Chernozhukov, Iv\\'an Fern\\'andez-Val and Ye Luo", "title": "SortedEffects: Sorted Causal Effects in R", "comments": "15 pages, 6 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chernozhukov et al. (2018) proposed the sorted effect method for nonlinear\nregression models. This method consists of reporting percentiles of the partial\neffects in addition to the average commonly used to summarize the heterogeneity\nin the partial effects. They also proposed to use the sorted effects to carry\nout classification analysis where the observational units are classified as\nmost and least affected if their causal effects are above or below some tail\nsorted effects. The R package SortedEffects implements the estimation and\ninference methods therein and provides tools to visualize the results. This\nvignette serves as an introduction to the package and displays basic\nfunctionality of the functions within.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 19:13:51 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 15:46:51 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 22:12:32 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Chen", "Shuowen", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Luo", "Ye", ""]]}, {"id": "1909.01691", "submitter": "Alexander Fisch", "authors": "Alexander T M Fisch, Idris A Eckley, Paul Fearnhead", "title": "Subset Multivariate Collective And Point Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in identifying anomalous\nstructure within multivariate data streams. We consider the problem of\ndetecting collective anomalies, corresponding to intervals where one or more of\nthe data streams behaves anomalously. We first develop a test for a single\ncollective anomaly that has power to simultaneously detect anomalies that are\neither rare, that is affecting few data streams, or common. We then show how to\ndetect multiple anomalies in a way that is computationally efficient but avoids\nthe approximations inherent in binary segmentation-like approaches. This\napproach, which we call MVCAPA, is shown to consistently estimate the number\nand location of the collective anomalies, a property that has not previously\nbeen shown for competing methods. MVCAPA can be made robust to point anomalies\nand can allow for the anomalies to be imperfectly aligned. We show the\npractical usefulness of allowing for imperfect alignments through a resulting\nincrease in power to detect regions of copy number variation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:58:46 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fisch", "Alexander T M", ""], ["Eckley", "Idris A", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1909.01836", "submitter": "Pulong Ma", "authors": "Pulong Ma, Georgios Karagiannis, Bledar A. Konomi, Taylor G. Asher,\n  Gabriel R. Toro, Andrew T. Cox", "title": "Multifidelity Computer Model Emulation with High-Dimensional Output: An\n  Application to Storm Surge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricane-driven storm surge is one of the most deadly and costly natural\ndisasters, making precise quantification of the surge hazard of great\nimportance. Inference of such systems is done through physics-based computer\nmodels of the process. Such surge simulators can be implemented with a wide\nrange of fidelity levels, with computational burdens varying by several orders\nof magnitude due to the nature of the system. The danger posed by surge makes\ngreater fidelity highly desirable, however such models and their high-volume\noutput tend to come at great computational cost, which can make detailed study\nof coastal flood hazards prohibitive. These needs make the development of an\nemulator combining high-dimensional output from multiple complex computer\nmodels with different fidelity levels important. We propose a parallel partial\nautoregressive cokriging model to predict highly-accurate storm surges in a\ncomputationally efficient way over a large spatial domain. This emulator has\nthe capability of predicting storm surges as accurately as a high-fidelity\ncomputer model given any storm characteristics and allows accurate assessment\nof the hazards from storm surges over a large spatial domain.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:33:23 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:12:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 18:45:34 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 19:27:51 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2020 15:56:24 GMT"}, {"version": "v6", "created": "Tue, 5 May 2020 18:27:22 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ma", "Pulong", ""], ["Karagiannis", "Georgios", ""], ["Konomi", "Bledar A.", ""], ["Asher", "Taylor G.", ""], ["Toro", "Gabriel R.", ""], ["Cox", "Andrew T.", ""]]}, {"id": "1909.02102", "submitter": "Yifei Wang", "authors": "Yifei Wang and Wuchen Li", "title": "Accelerated Information Gradient flow", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for Nesterov's accelerated gradient flows in\nprobability space. Here four examples of information metrics are considered,\nincluding Fisher-Rao metric, Wasserstein-2 metric, Kalman-Wasserstein metric\nand Stein metric. For both Fisher-Rao and Wasserstein-2 metrics, we prove\nconvergence properties of accelerated gradient flows. In implementations, we\npropose a sampling-efficient discrete-time algorithm for Wasserstein-2,\nKalman-Wasserstein and Stein accelerated gradient flows with a restart\ntechnique. We also formulate a kernel bandwidth selection method, which learns\nthe gradient of logarithm of density from Brownian-motion samples. Numerical\nexperiments, including Bayesian logistic regression and Bayesian neural\nnetwork, show the strength of the proposed methods compared with\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 20:43:21 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 03:30:39 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Wang", "Yifei", ""], ["Li", "Wuchen", ""]]}, {"id": "1909.02426", "submitter": "James Tucker", "authors": "Kyungmin Ahn, J. Derek Tucker, Wei Wu, and Anuj Srivastava", "title": "Regression Models Using Shapes of Functions as Predictors", "comments": "30 pages", "journal-ref": null, "doi": "10.1016/j.csda.2020.107017", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional variables are often used as predictors in regression problems. A\ncommonly-used parametric approach, called {\\it scalar-on-function regression},\nuses the $\\ltwo$ inner product to map functional predictors into scalar\nresponses. This method can perform poorly when predictor functions contain\nundesired phase variability, causing phases to have disproportionately large\ninfluence on the response variable. One past solution has been to perform\nphase-amplitude separation (as a pre-processing step) and then use only the\namplitudes in the regression model. Here we propose a more integrated approach,\ntermed elastic functional regression model (EFRM), where phase-separation is\nperformed inside the regression model, rather than as a pre-processing step.\nThis approach generalizes the notion of phase in functional data, and is based\non the norm-preserving time warping of predictors. Due to its invariance\nproperties, this representation provides robustness to predictor phase\nvariability and results in improved predictions of the response variable over\ntraditional models. We demonstrate this framework using a number of datasets\ninvolving gait signals, NMR data, and stock market prices.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:02:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 14:53:40 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 15:48:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ahn", "Kyungmin", ""], ["Tucker", "J. Derek", ""], ["Wu", "Wei", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1909.02736", "submitter": "Clara Grazian", "authors": "Clara Grazian and Yanan Fan", "title": "A review of Approximate Bayesian Computation methods via density\n  estimation: inference for simulator-models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review of Approximate Bayesian Computation (ABC)\nmethods for carrying out Bayesian posterior inference, through the lens of\ndensity estimation. We describe several recent algorithms and make connection\nwith traditional approaches. We show advantages and limitations of models based\non parametric approaches and we then draw attention to developments in machine\nlearning, which we believe have the potential to make ABC scalable to higher\ndimensions and may be the future direction for research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 06:52:23 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Grazian", "Clara", ""], ["Fan", "Yanan", ""]]}, {"id": "1909.02838", "submitter": "Dimas Dutra", "authors": "Dimas Abreu Archanjo Dutra", "title": "Collocation-Based Output-Error Method for Aircraft System Identification", "comments": "2019 AIAA Aviation Forum 17-21 June 2019 Dallas, Texas", "journal-ref": null, "doi": "10.2514/6.2019-3087", "report-no": null, "categories": "stat.CO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The output-error method is a mainstay of aircraft system identification from\nflight-test data. It is the method of choice for a wide range of applications,\nfrom the estimation of stability and control derivatives for aerodynamic\ndatabase generation to sensor bias estimation in flight-path reconstruction.\nHowever, notable limitations of the output-error method are that it requires ad\nhoc modifications for applications to unstable systems and it is an iterative\nmethod which is particularly sensitive to the initial guess. In this paper, we\nshow how to reformulate the estimation as a collocation problem, an approach\ncommon in other disciplines but seldomly used in flight vehicle system\nidentification. Both formulations are equivalent in terms of having the same\nsolution, but the collocation-based can be applied without modifications or\nloss of efficiency to unstable systems. Examples with simulated and real-world\nflight-test data also show that convergence to the optimum is obtained even\nwith poor initial guesses.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 18:26:50 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Dutra", "Dimas Abreu Archanjo", ""]]}, {"id": "1909.02989", "submitter": "Luca Rossini", "authors": "Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini, Weixuan Zhu", "title": "A P\\'olya-Gamma Sampler for a Generalized Logistic Regression", "comments": "Revised Version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel Bayesian data augmentation approach for\nestimating the parameters of the generalised logistic regression model. We\npropose a P\\'olya-Gamma sampler algorithm that allows us to sample from the\nexact posterior distribution, rather than relying on approximations. A\nsimulation study illustrates the flexibility and accuracy of the proposed\napproach to capture heavy and light tails in binary response data of different\ndimensions. The methodology is applied to two different real datasets, where we\ndemonstrate that the P\\'olya-Gamma sampler provides more precise estimates than\nthe empirical likelihood method, outperforming approximate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:07:07 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 08:23:43 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 07:43:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Valle", "Luciana Dalla", ""], ["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Zhu", "Weixuan", ""]]}, {"id": "1909.03433", "submitter": "Xialiang Dou", "authors": "Xialiang Dou, Mihai Anitescu", "title": "Distributionally Robust Optimization with Correlated Data from Vector\n  Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a distributionally robust formulation of a stochastic optimization\nproblem for non-i.i.d vector autoregressive data. We use the Wasserstein\ndistance to define robustness in the space of distributions and we show, using\nduality theory, that the problem is equivalent to a finite convex-concave\nsaddle point problem. The performance of the method is demonstrated on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 11:30:04 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Dou", "Xialiang", ""], ["Anitescu", "Mihai", ""]]}, {"id": "1909.03566", "submitter": "Zdravko Botev", "authors": "Zdravko I. Botev and Pierre L'Ecuyer", "title": "Sampling Conditionally on a Rare Event via Generalized Splitting", "comments": "29 pages; 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a generalized splitting method to sample approximately\nfrom a distribution conditional on the occurrence of a rare event. This has\nimportant applications in a variety of contexts in operations research,\nengineering, and computational statistics. The method uses independent trials\nstarting from a single particle. We exploit this independence to obtain\nasymptotic and non-asymptotic bounds on the total variation error of the\nsampler. Our main finding is that the approximation error depends crucially on\nthe relative variability of the number of points produced by the splitting\nalgorithm in one run, and that this relative variability can be readily\nestimated via simulation. We illustrate the relevance of the proposed method on\nan application in which one needs to sample (approximately) from an intractable\nposterior density in Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 23:43:29 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Botev", "Zdravko I.", ""], ["L'Ecuyer", "Pierre", ""]]}, {"id": "1909.03575", "submitter": "Hamed Nikbakht", "authors": "Hamed Nikbakht, Konstantinos G. Papakonstantinou", "title": "A direct Hamiltonian MCMC approach for reliability estimation", "comments": "UNCECOMP 2019; 3rd ECCOMAS Thematic Conference on Uncertainty\n  Quantification in Computational Sciences and Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient estimation of rare events probabilities is of\nsignificant importance, since often the occurrences of such events have\nwidespread impacts. The focus in this work is on precisely quantifying these\nprobabilities, often encountered in reliability analysis of complex engineering\nsystems, by introducing a gradient-based Hamiltonian Markov Chain Monte Carlo\n(HMCMC) framework, termed Approximate Sampling Target with Post-processing\nAdjustment (ASTPA). The basic idea is to construct a relevant target\ndistribution by weighting the high-dimensional random variable space through a\none-dimensional likelihood model, using the limit-state function. To sample\nfrom this target distribution we utilize HMCMC algorithms that produce Markov\nchain samples based on Hamiltonian dynamics rather than random walks. We\ncompare the performance of typical HMCMC scheme with our newly developed\nQuasi-Newton based mass preconditioned HMCMC algorithm that can sample very\nadeptly, particularly in difficult cases with high-dimensionality and very\nsmall failure probabilities. To eventually compute the probability of interest,\nan original post-sampling step is devised at this stage, using an inverse\nimportance sampling procedure based on the samples. The involved user-defined\nparameters of ASTPA are then discussed and general default values are\nsuggested. Finally, the performance of the proposed methodology is examined in\ndetail and compared against Subset Simulation in a series of static and dynamic\nlow- and high-dimensional benchmark problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 00:41:54 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2019 16:24:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Nikbakht", "Hamed", ""], ["Papakonstantinou", "Konstantinos G.", ""]]}, {"id": "1909.03813", "submitter": "Alessandro Gasparini", "authors": "Alessandro Gasparini and Tim P. Morris and Michael J. Crowther", "title": "INTEREST: INteractive Tool for Exploring REsults from Simulation sTudies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation studies allow us to explore the properties of statistical methods.\nThey provide a powerful tool with a multiplicity of aims; among others:\nevaluating and comparing new or existing statistical methods, assessing\nviolations of modelling assumptions, helping with the understanding of\nstatistical concepts, and supporting the design of clinical trials. The\nincreased availability of powerful computational tools and usable software has\ncontributed to the rise of simulation studies in the current literature.\nHowever, simulation studies involve increasingly complex designs, making it\ndifficult to provide all relevant results clearly. Dissemination of results\nplays a focal role in simulation studies: it can drive applied analysts to use\nmethods that have been shown to perform well in their settings, guide\nresearchers to develop new methods in a promising direction, and provide\ninsights into less established methods. It is crucial that we can digest\nrelevant results of simulation studies. Therefore, we developed INTEREST: an\nINteractive Tool for Exploring REsults from Simulation sTudies. The tool has\nbeen developed using the Shiny framework in R and is available as a web app or\nas a standalone package. It requires uploading a tidy format dataset with the\nresults of a simulation study in R, Stata, SAS, SPSS, or comma-separated\nformat. A variety of performance measures are estimated automatically along\nwith Monte Carlo standard errors; results and performance summaries are\ndisplayed both in tabular and graphical fashion, with a wide variety of\navailable plots. Consequently, the reader can focus on simulation parameters\nand estimands of most interest. In conclusion, INTEREST can facilitate the\ninvestigation of results from simulation studies and supplement the reporting\nof results, allowing researchers to share detailed results from their\nsimulations and readers to explore them freely.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:47:43 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 13:24:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Gasparini", "Alessandro", ""], ["Morris", "Tim P.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "1909.03948", "submitter": "Umberto Villa", "authors": "Umberto Villa and Noemi Petra and Omar Ghattas", "title": "hIPPYlib: An Extensible Software Framework for Large-Scale Inverse\n  Problems Governed by PDEs; Part I: Deterministic Inversion and Linearized\n  Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.MS cs.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extensible software framework, hIPPYlib, for solution of\nlarge-scale deterministic and Bayesian inverse problems governed by partial\ndifferential equations (PDEs) with infinite-dimensional parameter fields (which\nare high-dimensional after discretization). hIPPYlib overcomes the prohibitive\nnature of Bayesian inversion for this class of problems by implementing\nstate-of-the-art scalable algorithms for PDE-based inverse problems that\nexploit the structure of the underlying operators, notably the Hessian of the\nlog-posterior. The key property of the algorithms implemented in hIPPYlib is\nthat the solution of the deterministic and linearized Bayesian inverse problem\nis computed at a cost, measured in linearized forward PDE solves, that is\nindependent of the parameter dimension. The mean of the posterior is\napproximated by the MAP point, which is found by minimizing the negative\nlog-posterior. This deterministic nonlinear least-squares optimization problem\nis solved with an inexact matrix-free Newton-CG method. The posterior\ncovariance is approximated by the inverse of the Hessian of the negative log\nposterior evaluated at the MAP point. This Gaussian approximation is exact when\nthe parameter-to-observable map is linear; otherwise, its logarithm agrees to\ntwo derivatives with the log-posterior at the MAP point, and thus it can serve\nas a proposal for Hessian-based MCMC methods. The construction of the posterior\ncovariance is made tractable by invoking a low-rank approximation of the\nHessian of the log-likelihood. Scalable tools for sample generation are also\nimplemented. hIPPYlib makes all of these advanced algorithms easily accessible\nto domain scientists and provides an environment that expedites the development\nof new algorithms. hIPPYlib is also a teaching tool to educate researchers and\npractitioners who are new to inverse problems and the Bayesian inference\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 15:57:47 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 03:36:20 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Villa", "Umberto", ""], ["Petra", "Noemi", ""], ["Ghattas", "Omar", ""]]}, {"id": "1909.04010", "submitter": "Damian Campo", "authors": "Damian Campo, Alejandro Betancourt, Lucio Marcenaro, Carlo Regazzoni", "title": "Static force field representation of environments based on agents\n  nonlinear motions", "comments": null, "journal-ref": "EURASIP Journal on Advances in Signal Processing, December 2017,\n  2017:13", "doi": "10.1186/s13634-017-0444-5", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a methodology that aims at the incremental representation\nof areas inside environments in terms of attractive forces. It is proposed a\nparametric representation of velocity fields ruling the dynamics of moving\nagents. It is assumed that attractive spots in the environment are responsible\nfor modifying the motion of agents. A switching model is used to describe near\nand far velocity fields, which in turn are used to learn attractive\ncharacteristics of environments. The effect of such areas is considered radial\nover all the scene. Based on the estimation of attractive areas, a map that\ndescribes their effects in terms of their localizations, ranges of action, and\nintensities is derived in an online way. Information of static attractive areas\nis added dynamically into a set of filters that describes possible interactions\nbetween moving agents and an environment. The proposed approach is first\nevaluated on synthetic data; posteriorly, the method is applied on real\ntrajectories coming from moving pedestrians in an indoor environment.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 17:46:16 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Campo", "Damian", ""], ["Betancourt", "Alejandro", ""], ["Marcenaro", "Lucio", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1909.04024", "submitter": "Priyam Das", "authors": "Priyam Das, Debsurya De, Raju Maiti, Mona Kamal, Katherine A.\n  Hutcheson, Clifton D. Fuller, Bibhas Chakraborty and Christine B. Peterson", "title": "Estimating the Optimal Linear Combination of Biomarkers using\n  Spherically Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a binary classification problem, the optimal linear\ncombination of continuous predictors can be estimated by maximizing an\nempirical estimate of the area under the receiver operating characteristic\n(ROC) curve (AUC). For multi-category responses, the optimal predictor\ncombination can similarly be obtained by maximization of the empirical\nhypervolume under the manifold (HUM). This problem is particularly relevant to\nmedical research, where it may be of interest to diagnose a disease with\nvarious subtypes or predict a multi-category outcome. Since the empirical HUM\nis discontinuous, non-differentiable, and possibly multi-modal, solving this\nmaximization problem requires a global optimization technique. Estimation of\nthe optimal coefficient vector using existing global optimization techniques is\ncomputationally expensive, becoming prohibitive as the number of predictors and\nthe number of outcome categories increases. We propose an efficient\nderivative-free black-box optimization technique based on pattern search to\nsolve this problem. Through extensive simulation studies, we demonstrate that\nthe proposed method achieves better performance compared to existing methods\nincluding the step-down algorithm. Finally, we illustrate the proposed method\nto predict swallowing difficulty after radiation therapy for oropharyngeal\ncancer based on radiation dose to various structures in the head and neck.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 03:51:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 05:33:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Das", "Priyam", ""], ["De", "Debsurya", ""], ["Maiti", "Raju", ""], ["Kamal", "Mona", ""], ["Hutcheson", "Katherine A.", ""], ["Fuller", "Clifton D.", ""], ["Chakraborty", "Bibhas", ""], ["Peterson", "Christine B.", ""]]}, {"id": "1909.04060", "submitter": "Alireza Vafaei Sadr", "authors": "Alireza Vafaei Sadr, Bruce A. Bassett and Martin Kunz", "title": "A Flexible Framework for Anomaly Detection via Dimensionality Reduction", "comments": "6 pages", "journal-ref": "Proceeding, 6th International Conference on Soft Computing &\n  Machine Intelligence (ISCMI), Johannesburg, South Africa, 2019, pp. 106-110", "doi": "10.1109/ISCMI47871.2019.9004400", "report-no": null, "categories": "cs.LG astro-ph.IM cs.AI stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection is challenging, especially for large datasets in high\ndimensions. Here we explore a general anomaly detection framework based on\ndimensionality reduction and unsupervised clustering. We release DRAMA, a\ngeneral python package that implements the general framework with a wide range\nof built-in options. We test DRAMA on a wide variety of simulated and real\ndatasets, in up to 3000 dimensions, and find it robust and highly competitive\nwith commonly-used anomaly detection algorithms, especially in high dimensions.\nThe flexibility of the DRAMA framework allows for significant optimization once\nsome examples of anomalies are available, making it ideal for online anomaly\ndetection, active learning and highly unbalanced datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 18:00:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Sadr", "Alireza Vafaei", ""], ["Bassett", "Bruce A.", ""], ["Kunz", "Martin", ""]]}, {"id": "1909.04600", "submitter": "Theodoros Mathikolonis", "authors": "Theodoros Mathikolonis and Serge Guillas", "title": "Surrogate-based Optimization using Mutual Information for Computer\n  Experiments (optim-MICE)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational burden of running a complex computer model can make\noptimization impractical. Gaussian Processes (GPs) are statistical surrogates\n(also known as emulators) that alleviate this issue since they cheaply replace\nthe computer model. As a result, the exploration vs. exploitation trade-off\nstrategy can be accelerated by building a GP surrogate. In this paper, we\npropose a new surrogate-based optimization scheme that minimizes the number of\nevaluations of the computationally expensive function. Taking advantage of\nparallelism of the evaluation of the unknown function, the uncertain regions\nare explored simultaneously, and a batch of input points is chosen using Mutual\nInformation for Computer Experiments (MICE), a sequential design algorithm\nwhich maximizes the information theoretic Mutual Information over the input\nspace. The computational efficiency of interweaving the optimization scheme\nwith MICE (optim-MICE) is examined and demonstrated on test functions.\nOptim-MICE is compared with state-of-the-art heuristics such as Efficient\nGlobal Optimization (EGO) and GP-Upper Confidence Bound (GP-UCB). We\ndemonstrate that optim-MICE outperforms these schemes on a large range of\ncomputational experiments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 16:13:37 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Mathikolonis", "Theodoros", ""], ["Guillas", "Serge", ""]]}, {"id": "1909.04852", "submitter": "Guangyao Zhou", "authors": "Guangyao Zhou", "title": "Mixed Hamiltonian Monte Carlo for Mixed Discrete and Continuous\n  Variables", "comments": "Results with different discrete proposals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) has emerged as a powerful Markov Chain Monte\nCarlo (MCMC) method to sample from complex continuous distributions. However, a\nfundamental limitation of HMC is that it can not be applied to distributions\nwith mixed discrete and continuous variables. In this paper, we propose mixed\nHMC (M-HMC) as a general framework to address this limitation. M-HMC is a novel\nfamily of MCMC algorithms that evolves the discrete and continuous variables in\ntandem, allowing more frequent updates of discrete variables while maintaining\nHMC's ability to suppress random-walk behavior. We establish M-HMC's\ntheoretical properties, and present an efficient implementation with Laplace\nmomentum that introduces minimal overhead compared to existing HMC methods. The\nsuperior performances of M-HMC over existing methods are demonstrated with\nnumerical experiments on Gaussian mixture models (GMMs), variable selection in\nBayesian logistic regression (BLR), and correlated topic models (CTMs).\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:03:08 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 05:06:05 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 23:48:24 GMT"}, {"version": "v4", "created": "Wed, 21 Oct 2020 06:33:59 GMT"}, {"version": "v5", "created": "Thu, 18 Feb 2021 05:45:02 GMT"}, {"version": "v6", "created": "Mon, 15 Mar 2021 04:17:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhou", "Guangyao", ""]]}, {"id": "1909.04857", "submitter": "Jacob Priddle", "authors": "Jacob W. Priddle, Scott A. Sisson, David T. Frazier, Christopher\n  Drovandi", "title": "Efficient Bayesian synthetic likelihood with whitening transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods are an established approach for performing\napproximate Bayesian inference for models with intractable likelihood\nfunctions. However, they can be computationally demanding. Bayesian synthetic\nlikelihood (BSL) is a popular such method that approximates the likelihood\nfunction of the summary statistic with a known, tractable distribution --\ntypically Gaussian -- and then performs statistical inference using standard\nlikelihood-based techniques. However, as the number of summary statistics\ngrows, the number of model simulations required to accurately estimate the\ncovariance matrix for this likelihood rapidly increases. This poses significant\nchallenge for the application of BSL, especially in cases where model\nsimulation is expensive. In this article we propose whitening BSL (wBSL) -- an\nefficient BSL method that uses approximate whitening transformations to\ndecorrelate the summary statistics at each algorithm iteration. We show\nempirically that this can reduce the number of model simulations required to\nimplement BSL by more than an order of magnitude, without much loss of\naccuracy. We explore a range of whitening procedures and demonstrate the\nperformance of wBSL on a range of simulated and real modelling scenarios from\necology and biology.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:25:40 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 03:37:37 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Priddle", "Jacob W.", ""], ["Sisson", "Scott A.", ""], ["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1909.05097", "submitter": "Chieh Wu T", "authors": "Chieh Wu, Jared Miller, Yale Chang, Mario Sznaier, Jennifer Dy", "title": "Spectral Non-Convex Optimization for Dimension Reduction with\n  Hilbert-Schmidt Independence Criterion", "comments": "arXiv admin note: substantial text overlap with arXiv:1909.03093", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hilbert Schmidt Independence Criterion (HSIC) is a kernel dependence\nmeasure that has applications in various aspects of machine learning.\nConveniently, the objectives of different dimensionality reduction applications\nusing HSIC often reduce to the same optimization problem. However, the\nnonconvexity of the objective function arising from non-linear kernels poses a\nserious challenge to optimization efficiency and limits the potential of\nHSIC-based formulations. As a result, only linear kernels have been\ncomputationally tractable in practice. This paper proposes a spectral-based\noptimization algorithm that extends beyond the linear kernel. The algorithm\nidentifies a family of suitable kernels and provides the first and second-order\nlocal guarantees when a fixed point is reached. Furthermore, we propose a\nprincipled initialization strategy, thereby removing the need to repeat the\nalgorithm at random initialization points. Compared to state-of-the-art\noptimization algorithms, our empirical results on real data show a run-time\nimprovement by as much as a factor of $10^5$ while consistently achieving lower\ncost and classification/clustering errors. The implementation source code is\npublicly available on https://github.com/endsley.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 20:41:04 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Wu", "Chieh", ""], ["Miller", "Jared", ""], ["Chang", "Yale", ""], ["Sznaier", "Mario", ""], ["Dy", "Jennifer", ""]]}, {"id": "1909.05142", "submitter": "Majnu John", "authors": "Sujit Vettam, Majnu John", "title": "Regularized deep learning with nonconvex penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization methods are often employed in deep learning neural networks\n(DNNs) to prevent overfitting. For penalty based DNN regularization methods,\nconvex penalties are typically considered because of their optimization\nguarantees. Recent theoretical work have shown that nonconvex penalties that\nsatisfy certain regularity conditions are also guaranteed to perform well with\nstandard optimization algorithms. In this paper, we examine new and currently\nexisting nonconvex penalties for DNN regularization. We provide theoretical\njustifications for the new penalties and also assess the performance of all\npenalties with DNN analyses of seven datasets.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 15:32:44 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 15:48:42 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2019 18:24:15 GMT"}, {"version": "v4", "created": "Thu, 19 Nov 2020 19:56:37 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Vettam", "Sujit", ""], ["John", "Majnu", ""]]}, {"id": "1909.05201", "submitter": "F Din-Houn Lau Dr", "authors": "F. Din-Houn Lau and Sebastian Krumscheid", "title": "Plateau Proposal Distributions for Adaptive Component-wise Multiple-Try\n  Metropolis", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are sampling methods that have become\na commonly used tool in statistics, for example to perform Monte Carlo\nintegration. As a consequence of the increase in computational power, many\nvariations of MCMC methods exist for generating samples from arbitrary,\npossibly complex, target distributions. The performance of an MCMC method is\npredominately governed by the choice of the so-called proposal distribution\nused. In this paper, we introduce a new type of proposal distribution for the\nuse in MCMC methods that operates component-wise and with multiple trials per\niteration. Specifically, the novel class of proposal distributions, called\nPlateau distributions, do not overlap, thus ensuring that the multiple trials\nare drawn from different regions of the state space. Furthermore, the Plateau\nproposal distributions allow for a bespoke adaptation procedure that lends\nitself to a Markov chain with efficient problem dependent state space\nexploration and improved burn-in properties. Simulation studies show that our\nnovel MCMC algorithm outperforms competitors when sampling from distributions\nwith a complex shape, highly correlated components or multiple modes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:29:10 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 21:01:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lau", "F. Din-Houn", ""], ["Krumscheid", "Sebastian", ""]]}, {"id": "1909.05494", "submitter": "Faicel Chamroukhi", "authors": "Fa\u007f\\\"icel Chamroukhi, Florian Lecocq, and Hien D. Nguyen", "title": "Regularized Estimation and Feature Selection in Mixtures of\n  Gaussian-Gated Experts Models", "comments": "Research School on Statistics and Data Science - RSSDS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures-of-Experts models and their maximum likelihood estimation (MLE) via\nthe EM algorithm have been thoroughly studied in the statistics and machine\nlearning literature. They are subject of a growing investigation in the context\nof modeling with high-dimensional predictors with regularized MLE. We examine\nMoE with Gaussian gating network, for clustering and regression, and propose an\n$\\ell_1$-regularized MLE to encourage sparse models and deal with the\nhigh-dimensional setting. We develop an EM-Lasso algorithm to perform parameter\nestimation and utilize a BIC-like criterion to select the model parameters,\nincluding the sparsity tuning hyperparameters. Experiments conducted on\nsimulated data show the good performance of the proposed regularized MLE\ncompared to the standard MLE with the EM algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 07:56:27 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chamroukhi", "Fa\u007f\u00efcel", ""], ["Lecocq", "Florian", ""], ["Nguyen", "Hien D.", ""]]}, {"id": "1909.05782", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, and Blaise Melly", "title": "Fast Algorithms for the Quantile Regression Process", "comments": "29 pages, 3 figures, 4 tables; for associated Stata package, see\n  https://sites.google.com/site/blaisemelly/home/computer-programs/fast", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of quantile regression methods depends crucially on the\nexistence of fast algorithms. Despite numerous algorithmic improvements, the\ncomputation time is still non-negligible because researchers often estimate\nmany quantile regressions and use the bootstrap for inference. We suggest two\nnew fast algorithms for the estimation of a sequence of quantile regressions at\nmany quantile indexes. The first algorithm applies the preprocessing idea of\nPortnoy and Koenker (1997) but exploits a previously estimated quantile\nregression to guess the sign of the residuals. This step allows for a reduction\nof the effective sample size. The second algorithm starts from a previously\nestimated quantile regression at a similar quantile index and updates it using\na single Newton-Raphson iteration. The first algorithm is exact, while the\nsecond is only asymptotically equivalent to the traditional quantile regression\nestimator. We also apply the preprocessing idea to the bootstrap by using the\nsample estimates to guess the sign of the residuals in the bootstrap sample.\nSimulations show that our new algorithms provide very large improvements in\ncomputation time without significant (if any) cost in the quality of the\nestimates. For instance, we divide by 100 the time required to estimate 99\nquantile regressions with 20 regressors and 50,000 observations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:34:37 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 19:25:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Melly", "Blaise", ""]]}, {"id": "1909.05922", "submitter": "Chenguang Dai", "authors": "Chenguang Dai and Jun S. Liu", "title": "Monte Carlo Approximation of Bayes Factors via Mixing with Surrogate\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By mixing the target posterior distribution with a surrogate distribution, of\nwhich the normalizing constant is tractable, we propose a method for estimating\nthe marginal likelihood using the Wang-Landau algorithm. We show that a faster\nconvergence of the proposed method can be achieved via the momentum\nacceleration. Two implementation strategies are detailed: (i) facilitating\nglobal jumps between the posterior and surrogate distributions via the\nMultiple-try Metropolis; (ii) constructing the surrogate via the variational\napproximation. When a surrogate is difficult to come by, we describe a new\njumping mechanism for general reversible jump Markov chain Monte Carlo\nalgorithms, which combines the Multiple-try Metropolis and a directional\nsampling algorithm. We illustrate the proposed methods on several statistical\nmodels, including the Log-Gaussian Cox process, the Bayesian Lasso, the\nlogistic regression, and the g-prior Bayesian variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 19:52:56 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 02:52:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dai", "Chenguang", ""], ["Liu", "Jun S.", ""]]}, {"id": "1909.06039", "submitter": "Neil G. Marchant", "authors": "Neil G. Marchant, Andee Kaplan, Daniel N. Elazar, Benjamin I. P.\n  Rubinstein, Rebecca C. Steorts", "title": "d-blink: Distributed End-to-End Bayesian Entity Resolution", "comments": "32 pages, 6 figures, 5 tables. Includes 22 pages of supplementary\n  material. This revision incorporates a case study on the 2010 U.S. Decennial\n  Census", "journal-ref": null, "doi": "10.1080/10618600.2020.1825451", "report-no": null, "categories": "stat.CO cs.DB cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER; also known as record linkage or de-duplication) is the\nprocess of merging noisy databases, often in the absence of unique identifiers.\nA major advancement in ER methodology has been the application of Bayesian\ngenerative models, which provide a natural framework for inferring latent\nentities with rigorous quantification of uncertainty. Despite these advantages,\nexisting models are severely limited in practice, as standard inference\nalgorithms scale quadratically in the number of records. While scaling can be\nmanaged by fitting the model on separate blocks of the data, such a na\\\"ive\napproach may induce significant error in the posterior. In this paper, we\npropose a principled model for scalable Bayesian ER, called \"distributed\nBayesian linkage\" or d-blink, which jointly performs blocking and ER without\ncompromising posterior correctness. Our approach relies on several key ideas,\nincluding: (i) an auxiliary variable representation that induces a partition of\nthe entities and records into blocks; (ii) a method for constructing\nwell-balanced blocks based on k-d trees; (iii) a distributed\npartially-collapsed Gibbs sampler with improved mixing; and (iv) fast\nalgorithms for performing Gibbs updates. Empirical studies on six data\nsets---including a case study on the 2010 Decennial Census---demonstrate the\nscalability and effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 05:28:37 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 00:58:17 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 13:42:27 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Marchant", "Neil G.", ""], ["Kaplan", "Andee", ""], ["Elazar", "Daniel N.", ""], ["Rubinstein", "Benjamin I. P.", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1909.06116", "submitter": "Simone Riggi", "authors": "S. Riggi, F. Vitello, U. Becciani, C. Buemi, F. Bufano, A. Calanducci,\n  F. Cavallaro, A. Costa, A. Ingallinera, P. Leto, S. Loru, R.P. Norris, F.\n  Schillir\\`o, E. Sciacca, C. Trigilio, G. Umana", "title": "CAESAR source finder: recent developments and testing", "comments": "15 pages, 10 figures", "journal-ref": "Publications of the Astronomical Society of Australia, 2019, 36,\n  E037", "doi": "10.1017/pasa.2019.29", "report-no": null, "categories": "astro-ph.IM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new era in radioastronomy will begin with the upcoming large-scale surveys\nplanned at the Australian Square Kilometre Array Pathfinder (ASKAP). ASKAP\nstarted its Early Science program in October 2017 and several target fields\nwere observed during the array commissioning phase. The SCORPIO field was the\nfirst observed in the Galactic Plane in Band 1 (792-1032 MHz) using 15\ncommissioned antennas. The achieved sensitivity and large field of view already\nallow to discover new sources and survey thousands of existing ones with\nimproved precision with respect to previous surveys. Data analysis is currently\nongoing to deliver the first source catalogue. Given the increased scale of the\ndata, source extraction and characterization, even in this Early Science phase,\nhave to be carried out in a mostly automated way. This process presents\nsignificant challenges due to the presence of extended objects and diffuse\nemission close to the Galactic Plane. In this context we have extended and\noptimized a novel source finding tool, named CAESAR , to allow extraction of\nboth compact and extended sources from radio maps. A number of developments\nhave been done driven by the analysis of the SCORPIO map and in view of the\nfuture ASKAP Galactic Plane survey. The main goals are the improvement of\nalgorithm performances and scalability as well as of software maintainability\nand usability within the radio community. In this paper we present the current\nstatus of CAESAR and report a first systematic characterization of its\nperformance for both compact and extended sources using simulated maps. Future\nprospects are discussed in light of the obtained results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:51:43 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Riggi", "S.", ""], ["Vitello", "F.", ""], ["Becciani", "U.", ""], ["Buemi", "C.", ""], ["Bufano", "F.", ""], ["Calanducci", "A.", ""], ["Cavallaro", "F.", ""], ["Costa", "A.", ""], ["Ingallinera", "A.", ""], ["Leto", "P.", ""], ["Loru", "S.", ""], ["Norris", "R. P.", ""], ["Schillir\u00f2", "F.", ""], ["Sciacca", "E.", ""], ["Trigilio", "C.", ""], ["Umana", "G.", ""]]}, {"id": "1909.06540", "submitter": "David Warne", "authors": "David J. Warne (1), Ruth E. Baker (2), Matthew J. Simpson (1) ((1)\n  Queensland University of Technology, (2) University of Oxford)", "title": "Rapid Bayesian inference for expensive stochastic models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.CB q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all fields of science rely upon statistical inference to estimate\nunknown parameters in theoretical and computational models. While the\nperformance of modern computer hardware continues to grow, the computational\nrequirements for the simulation of models are growing even faster. This is\nlargely due to the increase in model complexity, often including stochastic\ndynamics, that is necessary to describe and characterize phenomena observed\nusing modern, high resolution, experimental techniques. Such models are rarely\nanalytically tractable, meaning that extremely large numbers of stochastic\nsimulations are required for parameter inference. In such cases, parameter\ninference can be practically impossible. In this work, we present new\ncomputational Bayesian techniques that accelerate inference for expensive\nstochastic models by using computationally inexpensive approximations to inform\nfeasible regions in parameter space, and through learning transforms that\nadjust the biased approximate inferences to closer represent the correct\ninferences under the expensive stochastic model. Using topical examples from\necology and cell biology, we demonstrate a speed improvement of an order of\nmagnitude without any loss in accuracy. This represents a substantial\nimprovement over current state-of-the-art methods for Bayesian computations\nwhen appropriate model approximations are available.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 06:22:15 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 02:53:48 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2021 00:48:45 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Warne", "David J.", ""], ["Baker", "Ruth E.", ""], ["Simpson", "Matthew J.", ""]]}, {"id": "1909.06631", "submitter": "Wei Jiang", "authors": "Wei Jiang, Malgorzata Bogdan, Julie Josse, Blazej Miasojedow, Veronika\n  Rockova, TraumaBase Group", "title": "Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing\n  Values", "comments": "R package https://github.com/wjiang94/ABSLOPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in high-dimensional settings\nwith missing observations among the covariates. To address this relatively\nunderstudied problem, we propose a new synergistic procedure -- adaptive\nBayesian SLOPE -- which effectively combines the SLOPE method (sorted $l_1$\nregularization) together with the Spike-and-Slab LASSO method. We position our\napproach within a Bayesian framework which allows for simultaneous variable\nselection and parameter estimation, despite the missing values. As with the\nSpike-and-Slab LASSO, the coefficients are regarded as arising from a\nhierarchical model consisting of two groups: (1) the spike for the inactive and\n(2) the slab for the active. However, instead of assigning independent spike\npriors for each covariate, here we deploy a joint \"SLOPE\" spike prior which\ntakes into account the ordering of coefficient magnitudes in order to control\nfor false discoveries. Through extensive simulations, we demonstrate\nsatisfactory performance in terms of power, FDR and estimation bias under a\nwide range of scenarios. Finally, we analyze a real dataset consisting of\npatients from Paris hospitals who underwent a severe trauma, where we show\nexcellent performance in predicting platelet levels. Our methodology has been\nimplemented in C++ and wrapped into an R package ABSLOPE for public use.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 17:09:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:12:37 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jiang", "Wei", ""], ["Bogdan", "Malgorzata", ""], ["Josse", "Julie", ""], ["Miasojedow", "Blazej", ""], ["Rockova", "Veronika", ""], ["Group", "TraumaBase", ""]]}, {"id": "1909.06753", "submitter": "Willem Van Den Boom", "authors": "Willem van den Boom, Galen Reeves, David B. Dunson", "title": "Approximating posteriors with high-dimensional nuisance parameters via\n  integrated rotated Gaussian approximation", "comments": "32 pages, 8 figures", "journal-ref": "Biometrika 108 (2021) 269-282", "doi": "10.1093/biomet/asaa068", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior computation for high-dimensional data with many parameters can be\nchallenging. This article focuses on a new method for approximating posterior\ndistributions of a low- to moderate-dimensional parameter in the presence of a\nhigh-dimensional or otherwise computationally challenging nuisance parameter.\nThe focus is on regression models and the key idea is to separate the\nlikelihood into two components through a rotation. One component involves only\nthe nuisance parameters, which can then be integrated out using a novel type of\nGaussian approximation. We provide theory on approximation accuracy that holds\nfor a broad class of forms of the nuisance component and priors. Applying our\nmethod to simulated and real data sets shows that it can outperform\nstate-of-the-art posterior approximation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 07:34:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Boom", "Willem van den", ""], ["Reeves", "Galen", ""], ["Dunson", "David B.", ""]]}, {"id": "1909.06945", "submitter": "Sim\\'on Rodr\\'iguez", "authors": "Sim\\'on Rodr\\'iguez Santana, Daniel Hern\\'andez-Lobato", "title": "Adversarial $\\alpha$-divergence Minimization for Bayesian Approximate\n  Inference", "comments": "47 pages, 10 figures (41 pages for the main article, 6 for the\n  supplementary material)", "journal-ref": null, "doi": "10.1016/j.neucom.2020.09.076", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are popular state-of-the-art models for many different\ntasks.They are often trained via back-propagation to find a value of the\nweights that correctly predicts the observed data. Although back-propagation\nhas shown good performance in many applications, it cannot easily output an\nestimate of the uncertainty in the predictions made. Estimating the uncertainty\nin the predictions is a critical aspect with important applications, and one\nmethod to obtain this information is following a Bayesian approach to estimate\na posterior distribution on the model parameters. This posterior distribution\nsummarizes which parameter values are compatible with the data, but is usually\nintractable and has to be approximated. Several mechanisms have been considered\nfor solving this problem. We propose here a general method for approximate\nBayesian inference that is based on minimizing{\\alpha}-divergences and that\nallows for flexible approximate distributions. The method is evaluated in the\ncontext of Bayesian neural networks on extensive experiments. The results show\nthat, in regression problems, it often gives better performance in terms of the\ntest log-likelihoodand sometimes in terms of the squared error. In\nclassification problems, however, it gives competitive results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:27:58 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 08:39:51 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 16:45:25 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Santana", "Sim\u00f3n Rodr\u00edguez", ""], ["Hern\u00e1ndez-Lobato", "Daniel", ""]]}, {"id": "1909.07060", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, R. Fahrni, M. Klippel, A. Frangi, B. Sudret", "title": "Bayesian calibration and sensitivity analysis of heat transfer models\n  for fire insulation panels", "comments": null, "journal-ref": null, "doi": "10.1016/j.engstruct.2019.110063", "report-no": "RSUQ-2019-002B", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to assess the performance of fire insulation panels is the\ncomponent additive method (CAM). The parameters of the CAM are based on the\ntemperature-dependent thermal material properties of the panels. These material\nproperties can be derived by calibrating finite element heat transfer models\nusing experimentally measured temperature records. In the past, the calibration\nof the material properties was done manually by trial and error approaches,\nwhich was inefficient and prone to error. In this contribution, the calibration\nproblem is reformulated in a probabilistic setting and solved using the\nBayesian model calibration framework. This not only gives a set of best-fit\nparameters but also confidence bounds on the latter. To make this framework\nfeasible, the procedure is accelerated through the use of advanced surrogate\nmodelling techniques: polynomial chaos expansions combined with principal\ncomponent analysis. This surrogate modelling technique additionally allows one\nto conduct a variance-based sensitivity analysis at no additional cost by\ngiving access to the Sobol' indices. The calibration is finally validated by\nusing the calibrated material properties to predict the temperature development\nin different experimental setups.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:41:59 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 08:57:59 GMT"}, {"version": "v3", "created": "Tue, 7 Jan 2020 10:07:43 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Wagner", "P. -R.", ""], ["Fahrni", "R.", ""], ["Klippel", "M.", ""], ["Frangi", "A.", ""], ["Sudret", "B.", ""]]}, {"id": "1909.07266", "submitter": "Dominic Schuhmacher", "authors": "Raoul M\\\"uller, Dominic Schuhmacher and Jorge Mateu", "title": "Metrics and barycenters for point pattern data", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the transport-transform (TT) and the relative\ntransport-transform (RTT) metrics between finite point patterns on a general\nspace, which provide a unified framework for earlier point pattern metrics, in\nparticular the generalized spike time and the normalized and unnormalized OSPA\nmetrics. Our main focus is on barycenters, i.e. minimizers of a $q$-th order\nFr\\'echet functional with respect to these metrics.\n  We present a heuristic algorithm that terminates in a local minimum and is\nshown to be fast and reliable in a simulation study. The algorithm serves as an\numbrella method that can be applied on any state space where an appropriate\nalgorithm for solving the location problem for individual points is available.\nWe present applications to geocoded data of crimes in Euclidean space and on a\nstreet network, illustrating that barycenters serve as informative summary\nstatistics. Our work is a first step towards statistical inference in\ncovariate-based models of repeated point pattern observations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 15:12:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["M\u00fcller", "Raoul", ""], ["Schuhmacher", "Dominic", ""], ["Mateu", "Jorge", ""]]}, {"id": "1909.07320", "submitter": "Leonard Heinz", "authors": "Leonard P. Heinz, Helmut Grubm\\\"uller", "title": "Computing spatially resolved rotational hydration entropies from\n  atomistic simulations", "comments": null, "journal-ref": null, "doi": "10.1021/acs.jctc.9b00926", "report-no": null, "categories": "physics.bio-ph physics.chem-ph physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a first principles understanding of macromolecular processes, a\nquantitative understanding of the underlying free energy landscape and in\nparticular its entropy contribution is crucial. The stability of biomolecules,\nsuch as proteins, is governed by the hydrophobic effect, which arises from\ncompeting enthalpic and entropic contributions to the free energy of the\nsolvent shell. While the statistical mechanics of liquids, as well as molecular\ndynamics simulations have provided much insight, solvation shell entropies\nremain notoriously difficult to calculate, especially when spatial resolution\nis required. Here, we present a method that allows for the computation of\nspatially resolved rotational solvent entropies via a non-parametric\nk-nearest-neighbor density estimator. We validated our method using analytic\ntest distributions and applied it to atomistic simulations of a water box. With\nan accuracy of better than 9.6%, the obtained spatial resolution should shed\nnew light on the hydrophobic effect and the thermodynamics of solvation in\ngeneral.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:29:45 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2019 13:57:01 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Heinz", "Leonard P.", ""], ["Grubm\u00fcller", "Helmut", ""]]}, {"id": "1909.07680", "submitter": "Fabian Wagner", "authors": "Fabian Wagner, Jonas Latz, Iason Papaioannou, Elisabeth Ullmann", "title": "Multilevel Sequential Importance Sampling for Rare Event Estimation", "comments": null, "journal-ref": "SIAM J. Sci. Comput. 42(4), pp. A2062-A2087, 2020", "doi": "10.1137/19M1289601", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the probability of rare events is an important task in\nreliability and risk assessment. We consider failure events that are expressed\nin terms of a limit state function, which depends on the solution of a partial\ndifferential equation (PDE). Since numerical evaluations of PDEs are\ncomputationally expensive, estimating such probabilities of failure by Monte\nCarlo sampling is intractable. More efficient sampling methods from reliability\nanalysis, such as Subset Simulation, are popular, but can still be\nimpracticable if the PDE evaluations are very costly. In this article, we\ndevelop a novel, highly efficient estimator for probabilities of rare events.\nOur method is based on a Sequential Importance sampler using discretizations of\nPDE-based limit state functions with different accuracies. A twofold adaptive\nalgorithm ensures that we obtain an estimate based on the desired\ndiscretization accuracy. In contrast to the Multilevel Subset Simulation\nestimator of [Ullmann, Papaioannou 2015; SIAM/ASA J. Uncertain. Quantif.\n3(1):922-953], our estimator overcomes the nestedness problem. Of particular\nimportance in Sequential Importance sampling algorithms is the correct choice\nof the MCMC kernel. Instead of the popular adaptive conditional sampling\nmethod, we propose a new algorithm that uses independent proposals from an\nadaptively constructed von Mises-Fischer-Nakagami distribution. The proposed\nalgorithm is applied to test problems in 1D and 2D space, respectively, and is\ncompared to the Multilevel Subset Simulation estimator and to single-level\nversions of Sequential Importance Sampling and Subset Simulation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:46:13 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2020 14:54:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Wagner", "Fabian", ""], ["Latz", "Jonas", ""], ["Papaioannou", "Iason", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "1909.08356", "submitter": "Johan Wahlstr\\\"om", "authors": "Johan Wahlstr\\\"om, Manon Kok, Pedro Porto Buarque de Gusmao, Traian E.\n  Abrudan, Niki Trigoni, and Andrew Markham", "title": "Sensor Fusion for Magneto-Inductive Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magneto-inductive navigation is an inexpensive and easily deployable solution\nto many of today's navigation problems. By utilizing very low frequency\nmagnetic fields, magneto-inductive technology circumvents the problems with\nattenuation and multipath that often plague competing modalities. Using\ntriaxial transmitter and receiver coils, it is possible to compute position and\norientation estimates in three dimensions. However, in many situations,\nadditional information is available that constrains the set of possible\nsolutions. For example, the receiver may be known to be coplanar with the\ntransmitter, or orientation information may be available from inertial sensors.\nWe employ a maximum a posteriori estimator to fuse magneto-inductive signals\nwith such complementary information. Further, we derive the Cramer-Rao bound\nfor the position estimates and investigate the problem of detecting distortions\ncaused by ferrous material. The performance of the estimator is compared to the\nCramer-Rao bound and a state-of-the-art estimator using both simulations and\nreal-world data. By fusing magneto-inductive signals with accelerometer\nmeasurements, the median position error is reduced almost by a factor of two.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 11:13:27 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wahlstr\u00f6m", "Johan", ""], ["Kok", "Manon", ""], ["de Gusmao", "Pedro Porto Buarque", ""], ["Abrudan", "Traian E.", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "1909.08418", "submitter": "Benjamin Cramer", "authors": "Benjamin Cramer, David St\\\"ockel, Markus Kreft, Michael Wibral,\n  Johannes Schemmel, Karlheinz Meier, Viola Priesemann", "title": "Control of criticality and computation in spiking neuromorphic networks\n  with plasticity", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-16548-3", "report-no": null, "categories": "cs.ET stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The critical state is assumed to be optimal for any computation in recurrent\nneural networks, because criticality maximizes a number of abstract\ncomputational properties. We challenge this assumption by evaluating the\nperformance of a spiking recurrent neural network on a set of tasks of varying\ncomplexity at - and away from critical network dynamics. To that end, we\ndeveloped a spiking network with synaptic plasticity on a neuromorphic chip. We\nshow that the distance to criticality can be easily adapted by changing the\ninput strength, and then demonstrate a clear relation between criticality,\ntask-performance and information-theoretic fingerprint. Whereas the\ninformation-theoretic measures all show that network capacity is maximal at\ncriticality, this is not the case for performance on specific tasks: Only the\ncomplex, memory-intensive tasks profit from criticality, whereas the simple\ntasks suffer from it. Thereby, we challenge the general assumption that\ncriticality would be beneficial for any task, and provide instead an\nunderstanding of how the collective network state should be tuned to task\nrequirement to achieve optimal performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 12:15:47 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 09:19:11 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Cramer", "Benjamin", ""], ["St\u00f6ckel", "David", ""], ["Kreft", "Markus", ""], ["Wibral", "Michael", ""], ["Schemmel", "Johannes", ""], ["Meier", "Karlheinz", ""], ["Priesemann", "Viola", ""]]}, {"id": "1909.08498", "submitter": "Mohammad Arashi", "authors": "M. Taavoni and M. Arashi", "title": "High-dimensional generalized semiparametric model for longitudinal data", "comments": "25 pages, 3 Figures, 4 Tables-The supplementary file includes the\n  proofs, but not added here", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimation in the generalized\nsemiparametric model for longitudinal data when the number of parameters\ndiverges with the sample size. A penalization type of generalized estimating\nequation method is proposed, while we use the regression spline to approximate\nthe nonparametric component. The proposed procedure involves the specification\nof the posterior distribution of the random effects, which cannot be evaluated\nin a closed-form. However, it is possible to approximate this posterior\ndistribution by producing random draws from the distribution using a Metropolis\nalgorithm. Under some regularity conditions, the resulting estimators enjoy the\noracle properties, under the high-dimensional regime. Simulation studies are\ncarried out to assess the performance of our proposed method, and two real data\nsets are analyzed to illustrate the procedure.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:23:26 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:32:22 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Taavoni", "M.", ""], ["Arashi", "M.", ""]]}, {"id": "1909.09266", "submitter": "Mert Korkali", "authors": "Zhixiong Hu, Yijun Xu, Mert Korkali, Xiao Chen, Lamine Mili, and\n  Charles H. Tong", "title": "Uncertainty Quantification in Stochastic Economic Dispatch using\n  Gaussian Process Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing penetration of renewable energy resources in power systems,\nrepresented as random processes, converts the traditional deterministic\neconomic dispatch problem into a stochastic one. To solve this stochastic\neconomic dispatch, the conventional Monte Carlo method is prohibitively time\nconsuming for medium- and large-scale power systems. To overcome this problem,\nwe propose in this paper a novel Gaussian-process-emulator-based approach to\nquantify the uncertainty in the stochastic economic dispatch considering wind\npower penetration. Based on the dimension-reduction results obtained by the\nKarhunen-Lo\\`eve expansion, a Gaussian-process emulator is constructed. This\nsurrogate allows us to evaluate the economic dispatch solver at sampled values\nwith a negligible computational cost while maintaining a desirable accuracy.\nSimulation results conducted on the IEEE 118-bus system reveal that the\nproposed method has an excellent performance as compared to the traditional\nMonte Carlo method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 00:16:05 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Hu", "Zhixiong", ""], ["Xu", "Yijun", ""], ["Korkali", "Mert", ""], ["Chen", "Xiao", ""], ["Mili", "Lamine", ""], ["Tong", "Charles H.", ""]]}, {"id": "1909.09370", "submitter": "Sothea Has", "authors": "Aur\\'elie Fisher (LPSM UMR 8001), Mathilde Mougeot (CMLA, ENSIIE, LPSM\n  UMR 8001), Sothea Has (LPSM UMR 8001)", "title": "A clusterwise supervised learning procedure based on aggregation of\n  distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many machine learning procedures are available on the shelve and\nmay be used easily to calibrate predictive models on supervised data. However,\nwhen the input data consists of more than one unknown cluster, and when\ndifferent underlying predictive models exist, fitting a model is a more\nchallenging task. We propose, in this paper, a procedure in three steps to\nautomatically solve this problem. The KFC procedure aggregates different models\nadaptively on data. The first step of the procedure aims at catching the\nclustering structure of the input data, which may be characterized by several\nstatistical distributions. It provides several partitions, given the\nassumptions on the distributions. For each partition, the second step fits a\nspecific predictive model based on the data in each cluster. The overall model\nis computed by a consensual aggregation of the models corresponding to the\ndifferent partitions. A comparison of the performances on different simulated\nand real data assesses the excellent performance of our method in a large\nvariety of prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:37:04 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:20:31 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 15:51:30 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Fisher", "Aur\u00e9lie", "", "LPSM UMR 8001"], ["Mougeot", "Mathilde", "", "CMLA, ENSIIE, LPSM\n  UMR 8001"], ["Has", "Sothea", "", "LPSM UMR 8001"]]}, {"id": "1909.09591", "submitter": "Alexandre Thiery", "authors": "Aaron Myers, Alexandre H. Thiery, Kainan Wang, Tan Bui-Thanh", "title": "Sequential Ensemble Transform for Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Sequential Ensemble Transform (SET) method, an approach for\ngenerating approximate samples from a Bayesian posterior distribution. The\nmethod explores the posterior distribution by solving a sequence of discrete\noptimal transport problems to produce a series of transport plans which map\nprior samples to posterior samples. We prove that the sequence of Dirac mixture\ndistributions produced by the SET method converges weakly to the true posterior\nas the sample size approaches infinity. Furthermore, our numerical results\nindicate that, when compared to standard Sequential Monte Carlo (SMC) methods,\nthe SET approach is more robust to the choice of Markov mutation kernels and\nrequires less computational efforts to reach a similar accuracy when used to\nexplore complex posterior distributions. Finally, we describe adaptive schemes\nthat allow to completely automate the use of the SET method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 16:13:47 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 07:29:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Myers", "Aaron", ""], ["Thiery", "Alexandre H.", ""], ["Wang", "Kainan", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1909.10279", "submitter": "Alec Koppel", "authors": "Alec Koppel, Amrit Singh Bedi, Brian M. Sadler, and Victor Elvira", "title": "Nearly Consistent Finite Particle Estimates in Streaming Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian inference, we seek to compute information about random variables\nsuch as moments or quantiles on the basis of {available data} and prior\ninformation. When the distribution of random variables is {intractable}, Monte\nCarlo (MC) sampling is usually required. {Importance sampling is a standard MC\ntool that approximates this unavailable distribution with a set of weighted\nsamples.} This procedure is asymptotically consistent as the number of MC\nsamples (particles) go to infinity. However, retaining infinitely many\nparticles is intractable. Thus, we propose a way to only keep a \\emph{finite\nrepresentative subset} of particles and their augmented importance weights that\nis \\emph{nearly consistent}. To do so in {an online manner}, we (1) embed the\nposterior density estimate in a reproducing kernel Hilbert space (RKHS) through\nits kernel mean embedding; and (2) sequentially project this RKHS element onto\na lower-dimensional subspace in RKHS using the maximum mean discrepancy, an\nintegral probability metric. Theoretically, we establish that this scheme\nresults in a bias determined by a compression parameter, which yields a tunable\ntradeoff between consistency and memory. In experiments, we observe the\ncompressed estimates achieve comparable performance to the dense ones with\nsubstantial reductions in representational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:06:15 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 16:51:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Koppel", "Alec", ""], ["Bedi", "Amrit Singh", ""], ["Sadler", "Brian M.", ""], ["Elvira", "Victor", ""]]}, {"id": "1909.10635", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Yannick D\\\"uren, Kristoffer H. Hellton and Johannes\n  Lederer", "title": "Tuning parameter calibration for prediction in personalized medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine has become an important part of medicine, for instance\npredicting individual drug responses based on genomic information. However,\nmany current statistical methods are not tailored to this task, because they\noverlook the individual heterogeneity of patients. In this paper, we look at\npersonalized medicine from a linear regression standpoint. We introduce an\nalternative version of the ridge estimator and target individuals by\nestablishing a tuning parameter calibration scheme that minimizes prediction\nerrors of individual patients. In stark contrast, classical schemes such as\ncross-validation minimize prediction errors only on average. We show that our\npipeline is optimal in terms of oracle inequalities, fast, and highly effective\nboth in simulations and on real data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 22:00:14 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 09:39:10 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 10:57:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Huang", "Shih-Ting", ""], ["D\u00fcren", "Yannick", ""], ["Hellton", "Kristoffer H.", ""], ["Lederer", "Johannes", ""]]}, {"id": "1909.10678", "submitter": "Evan Martin", "authors": "Evan A Martin and Audrey Qiuyan Fu", "title": "A Bayesian Approach to Directed Acyclic Graphs with a Candidate Graph", "comments": "Included analyses for data from GEUVADIS and GTEX", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs represent the dependence structure among variables.\nWhen learning these graphs from data, different amounts of information may be\navailable for different edges. Although many methods have been developed to\nlearn the topology of these graphs, most of them do not provide a measure of\nuncertainty in the inference. We propose a Bayesian method, baycn (BAYesian\nCausal Network), to estimate the posterior probability of three states for each\nedge: present with one direction ($X \\rightarrow Y$), present with the opposite\ndirection ($X \\leftarrow Y$), and absent. Unlike existing Bayesian methods, our\nmethod requires that the prior probabilities of these states be specified, and\ntherefore provides a benchmark for interpreting the posterior probabilities. We\ndevelop a fast Metropolis-Hastings Markov chain Monte Carlo algorithm for the\ninference. Our algorithm takes as input the edges of a candidate graph, which\nmay be the output of another graph inference method and may contain false\nedges. In simulation studies our method achieves high accuracy with small\nvariation across different scenarios and is comparable or better than existing\nBayesian methods. We apply baycn to genomic data to distinguish the direct and\nindirect targets of genetic variants.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:51:55 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 00:25:47 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Martin", "Evan A", ""], ["Fu", "Audrey Qiuyan", ""]]}, {"id": "1909.10804", "submitter": "Virgilio Gomez-Rubio", "authors": "Francisco Palmi-Perales and Virgilio Gomez-Rubio and Miguel A.\n  Martinez-Beneito", "title": "Bayesian Multivariate Spatial Models for Lattice Data with INLA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The INLAMSM package for the R programming language provides a collection of\nmultivariate spatial models for lattice data that can be used with package INLA\nfor Bayesian inference. The multivariate spatial models include different\nstructures to model the spatial variation of the variables and the\nbetween-variables variability. In this way, fitting multivariate spatial models\nbecomes faster and easier. The use of the different models included in the\npackage is illustrated using two different datasets: the well-known North\nCarolina SIDS data and mortality by three causes of death in Comunidad\nValenciana (Spain).\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 10:47:54 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 08:14:29 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Palmi-Perales", "Francisco", ""], ["Gomez-Rubio", "Virgilio", ""], ["Martinez-Beneito", "Miguel A.", ""]]}, {"id": "1909.10881", "submitter": "Amir Karami", "authors": "Amir Karami", "title": "Application of Fuzzy Clustering for Text Data Dimensionality Reduction", "comments": "arXiv admin note: text overlap with arXiv:1712.05997", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large textual corpora are often represented by the document-term frequency\nmatrix whose elements are the frequency of terms; however, this matrix has two\nproblems: sparsity and high dimensionality. Four dimension reduction strategies\nare used to address these problems. Of the four strategies, unsupervised\nfeature transformation (UFT) is a popular and efficient strategy to map the\nterms to a new basis in the document-term frequency matrix. Although several\nUFT-based methods have been developed, fuzzy clustering has not been considered\nfor dimensionality reduction. This research explores fuzzy clustering as a new\nUFT-based approach to create a lower-dimensional representation of documents.\nPerformance of fuzzy clustering with and without using global term weighting\nmethods is shown to exceed principal component analysis and singular value\ndecomposition. This study also explores the effect of applying different\nfuzzifier values on fuzzy clustering for dimensionality reduction purpose.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 03:15:04 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Karami", "Amir", ""]]}, {"id": "1909.11560", "submitter": "Peter Neal Dr", "authors": "Jessica Welding and Peter Neal", "title": "Real time analysis of epidemic data", "comments": "33 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases have severe health and economic consequences for society.\nIt is important in controlling the spread of an emerging infectious disease to\nbe able to both estimate the parameters of the underlying model and identify\nthose individuals most at risk of infection in a timely manner. This requires\nhaving a mechanism to update inference on the model parameters and the\nprogression of the disease as new data becomes available. However, Markov chain\nMonte Carlo (MCMC), the gold standard for statistical inference for infectious\ndisease models, is not equipped to deal with this important problem. Motivated\nby the need to develop effective statistical tools for emerging diseases and\nusing the 2001 UK Foot-and-Mouth disease outbreak as an exemplar, we introduce\na Sequential Monte Carlo (SMC) algorithm to enable real-time analysis of\nepidemic outbreaks. Naive application of SMC methods leads to significant\nparticle degeneracy which are successfully overcome by particle perturbation\nand incorporating MCMC-within-SMC updates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:43:46 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Welding", "Jessica", ""], ["Neal", "Peter", ""]]}, {"id": "1909.11564", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Analytical confidence intervals for the number of different objects in\n  data streams", "comments": "accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops a new mathematical-statistical approach to analyze a\nclass of Flajolet-Martin algorithms (FMa), and provides analytical confidence\nintervals for the number F0 of distinct elements in a stream, based on Chernoff\nbounds. The class of FMa has reached a significant popularity in bigdata stream\nlearning, and the attention of the literature has mainly been based on\nalgorithmic aspects, basically complexity optimality, while the statistical\nanalysis of these class of algorithms has been often faced heuristically. The\nanalysis provided here shows deep connections with mathematical special\nfunctions and with extreme value theory. The latter connection may help in\nexplaining heuristic considerations, while the first opens many numerical\nissues, faced at the end of the present paper. Finally, the algorithms are\ntested on an anonymized real data stream and MonteCarlo simulations are\nprovided to support our analytical choice in this context.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 15:46:11 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 10:37:18 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 16:50:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1909.11773", "submitter": "Dana Yang", "authors": "David Pollard and Dana Yang", "title": "Rapid mixing of a Markov chain for an exponentially weighted aggregation\n  estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-Hastings method is often used to construct a Markov chain with\na given $\\pi$ as its stationary distribution. The method works even if $\\pi$ is\nknown only up to an intractable constant of proportionality. Polynomial time\nconvergence results for such chains (rapid mixing) are hard to obtain for high\ndimensional probability models where the size of the state space potentially\ngrows exponentially with the model dimension. In a Bayesian context, Yang,\nWainwright, and Jordan (2016) (=YWJ) used the path method to prove rapid mixing\nfor high dimensional linear models. This paper proposes a modification of the\nYWJ approach that simplifies the theoretical argument and improves the rate of\nconvergence. The new approach is illustrated by an application to an\nexponentially weighted aggregation estimator.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 21:07:05 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Pollard", "David", ""], ["Yang", "Dana", ""]]}, {"id": "1909.11784", "submitter": "Achim Zeileis", "authors": "Nikolaus Umlauf, Nadja Klein, Thorsten Simon, Achim Zeileis", "title": "bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond)", "comments": "48 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last decades, the challenges in applied regression and in predictive\nmodeling have been changing considerably: (1) More flexible model\nspecifications are needed as big(ger) data become available, facilitated by\nmore powerful computing infrastructure. (2) Full probabilistic modeling rather\nthan predicting just means or expectations is crucial in many applications. (3)\nInterest in Bayesian inference has been increasing both as an appealing\nframework for regularizing or penalizing model estimation as well as a natural\nalternative to classical frequentist inference. However, while there has been a\nlot of research in all three areas, also leading to associated software\npackages, a modular software implementation that allows to easily combine all\nthree aspects has not yet been available. For filling this gap, the R package\nbamlss is introduced for Bayesian additive models for location, scale, and\nshape (and beyond). At the core of the package are algorithms for\nhighly-efficient Bayesian estimation and inference that can be applied to\ngeneralized additive models (GAMs) or generalized additive models for location,\nscale, and shape (GAMLSS), also known as distributional regression. However,\nits building blocks are designed as \"Lego bricks\" encompassing various\ndistributions (exponential family, Cox, joint models, ...), regression terms\n(linear, splines, random effects, tensor products, spatial fields, ...), and\nestimators (MCMC, backfitting, gradient boosting, lasso, ...). It is\ndemonstrated how these can be easily recombined to make classical models more\nflexible or create new custom models for specific modeling challenges.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 21:31:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Umlauf", "Nikolaus", ""], ["Klein", "Nadja", ""], ["Simon", "Thorsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1909.11827", "submitter": "Vivekananda Roy", "authors": "Vivekananda Roy", "title": "Convergence diagnostics for Markov chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is one of the most useful approaches to\nscientific computing because of its flexible construction, ease of use and\ngenerality. Indeed, MCMC is indispensable for performing Bayesian analysis. Two\ncritical questions that MCMC practitioners need to address are where to start\nand when to stop the simulation. Although a great amount of research has gone\ninto establishing convergence criteria and stopping rules with sound\ntheoretical foundation, in practice, MCMC users often decide convergence by\napplying empirical diagnostic tools. This review article discusses the most\nwidely used MCMC convergence diagnostic tools. Some recently proposed stopping\nrules with firm theoretical footing are also presented. The convergence\ndiagnostics and stopping rules are illustrated using three detailed examples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 00:35:17 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 21:18:13 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Roy", "Vivekananda", ""]]}, {"id": "1909.12237", "submitter": "Ruobin Gong", "authors": "Ruobin Gong", "title": "Exact Inference with Approximate Computation for Differentially Private\n  Data via Perturbations", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy protects individuals' confidential information by\nsubjecting data summaries to probabilistic perturbation mechanisms, carefully\ndesigned to minimize undue sacrifice of statistical efficiency. When properly\naccounted for, differentially private data are conducive to exact inference\nwhen approximate computation techniques are employed. This paper shows that\napproximate Bayesian computation, a practical suite of methods to simulate from\napproximate posterior distributions of complex Bayesian models, produces exact\nposterior samples when applied to differentially private perturbation data. An\nimportance sampling implementation of Monte Carlo expectation-maximization for\nlikelihood inference is also discussed. The results illustrate a duality\nbetween approximate computation on exact data, and exact computation on\napproximate data. A cleverly designed inferential procedure exploits the\nalignment between the statistical tradeoff of privacy versus efficiency, and\nthe computational tradeoff of approximation versus exactness, so that paying\nthe cost of one gains the benefit of both.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 16:34:18 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 18:18:36 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["Gong", "Ruobin", ""]]}, {"id": "1909.12530", "submitter": "Rui Zhou", "authors": "Rui Zhou, Junyan Liu, Sandeep Kumar, Daniel P. Palomar", "title": "Robust Factor Analysis Parameter Estimation", "comments": "Presented at Eurocast 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robustly estimating the parameters of a\nheavy-tailed multivariate distribution when the covariance matrix is known to\nhave the structure of a low-rank matrix plus a diagonal matrix as considered in\nfactor analysis (FA). By assuming the observed data to follow the multivariate\nStudent's t distribution, we can robustly estimate the parameters via maximum\nlikelihood estimation (MLE). However, the MLE of parameters becomes an\nintractable problem when the multivariate Student's t distribution and the FA\nstructure are both introduced. In this paper, we propose an algorithm based on\nthe generalized expectation maximization (GEM) method to obtain estimators. The\nrobustness of our proposed method is further enhanced to cope with missing\nvalues. Finally, we show the performance of our proposed algorithm using both\nsynthetic data and real financial data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 07:32:19 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zhou", "Rui", ""], ["Liu", "Junyan", ""], ["Kumar", "Sandeep", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1909.13017", "submitter": "Kabir Olorede Opeyemi", "authors": "Kabir Opeyemi Olorede and Waheed Babatunde Yahya", "title": "A New Covariance Estimator for Sufficient Dimension Reduction in\n  High-Dimensional and Undersized Sample Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": "JCGS-19-251", "categories": "stat.ME cs.CC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of standard sufficient dimension reduction methods for\nreducing the dimension space of predictors without losing regression\ninformation requires inverting the covariance matrix of the predictors. This\nhas posed a number of challenges especially when analyzing high-dimensional\ndata sets in which the number of predictors $\\mathit{p}$ is much larger than\nnumber of samples $n,~(n\\ll p)$. A new covariance estimator, called the\n\\textit{Maximum Entropy Covariance} (MEC) that addresses loss of covariance\ninformation when similar covariance matrices are linearly combined using\n\\textit{Maximum Entropy} (ME) principle is proposed in this work. By\nbenefitting naturally from slicing or discretizing range of the response\nvariable, y into \\textit{H} non-overlapping categories, $\\mathit{h_{1},\\ldots\n,h_{H}}$, MEC first combines covariance matrices arising from samples in each y\nslice $\\mathit{h\\in H}$ and then select the one that maximizes entropy under\nthe principle of maximum uncertainty. The MEC estimator is then formed from\nconvex mixture of such entropy-maximizing sample covariance $S_{\\mbox{mec}}$\nestimate and pooled sample covariance $\\mathbf{S}_{\\mathit{p}}$ estimate across\nthe $\\mathit{H}$ slices without requiring time-consuming covariance\noptimization procedures. MEC deals directly with singularity and instability of\nsample group covariance estimate in both regression and classification\nproblems. The efficiency of the MEC estimator is studied with the existing\nsufficient dimension reduction methods such as \\textit{Sliced Inverse\nRegression} (SIR) and \\textit{Sliced Average Variance Estimator} (SAVE) as\ndemonstrated on both classification and regression problems using real life\nLeukemia cancer data and customers' electricity load profiles from smart meter\ndata sets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 03:34:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Olorede", "Kabir Opeyemi", ""], ["Yahya", "Waheed Babatunde", ""]]}, {"id": "1909.13118", "submitter": "Ritabrata Dutta", "authors": "Lorenzo Pacchiardi, Pierre Kunzli, Marcel Schoengens, Bastien Chopard,\n  Ritabrata Dutta", "title": "Distance-learning For Approximate Bayesian Computation To Model a\n  Volcanic Eruption", "comments": null, "journal-ref": "Sankhya B (2020)", "doi": "10.1007/s13571-019-00208-8", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) provides us with a way to infer\nparameters of models, for which the likelihood function is not available, from\nan observation. Using ABC, which depends on many simulations from the\nconsidered model, we develop an inferential framework to learn parameters of a\nstochastic numerical simulator of volcanic eruption. Moreover, the model itself\nis parallelized using Message Passing Interface (MPI). Thus, we develop a\nnested-parallelized MPI communicator to handle the expensive numerical model\nwith ABC algorithms. ABC usually relies on summary statistics of the data in\norder to measure the discrepancy model output and observation. However,\ninformative summary statistics cannot be found for the considered model. We\ntherefore develop a technique to learn a distance between model outputs based\non deep metric-learning. We use this framework to learn the plume\ncharacteristics (eg. initial plume velocity) of the volcanic eruption from the\ntephra deposits collected by field-work associated with the 2450 BP Pululagua\n(Ecuador) volcanic eruption.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 16:14:08 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Kunzli", "Pierre", ""], ["Schoengens", "Marcel", ""], ["Chopard", "Bastien", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "1909.13602", "submitter": "Arnaud Guyader", "authors": "Qiming Du and Arnaud Guyader", "title": "Variance Estimation in Adaptive Sequential Monte Carlo", "comments": "42 pages v2: some minor changes and two appendices added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods represent a classical set of techniques\nto simulate a sequence of probability measures through a simple\nselection/mutation mechanism. However, the associated selection functions and\nmutation kernels usually depend on tuning parameters that are of first\nimportance for the efficiency of the algorithm. A standard way to address this\nproblem is to apply Adaptive Sequential Monte Carlo (ASMC) methods, which\nconsist in exploiting the information given by the history of the sample to\ntune the parameters. This article is concerned with variance estimation in such\nASMC methods. Specifically, we focus on the case where the asymptotic variance\ncoincides with the one of the \"limiting\" Sequential Monte Carlo algorithm as\ndefined by Beskos et al. (2016). We prove that, under natural assumptions, the\nestimator introduced by Lee and Whiteley (2018) in the nonadaptive case (i.e.,\nSMC) is also a consistent estimator of the asymptotic variance for ASMC\nmethods. To do this, we introduce a new estimator that is expressed in terms of\ncoalescent tree-based measures, and explain its connection with the previous\none. Our estimator is constructed by tracing the genealogy of the associated\nInteracting Particle System. The tools we use connect the study of Particle\nMarkov Chain Monte Carlo methods and the variance estimation problem in SMC\nmethods. As such, they may give some new insights when dealing with complex\ngenealogy-involved problems of Interacting Particle Systems in more general\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 11:40:21 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2020 12:34:10 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Du", "Qiming", ""], ["Guyader", "Arnaud", ""]]}]