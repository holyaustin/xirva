[{"id": "0912.1586", "submitter": "Robert B. Gramacy", "authors": "Matthew A. Taddy, Robert B. Gramacy, and Nicholas G. Polson", "title": "Dynamic Trees for Learning and Design", "comments": "37 pages, 8 figures, 3 tables; accepted at JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic regression trees are an attractive option for automatic regression\nand classification with complicated response surfaces in on-line application\nsettings. We create a sequential tree model whose state changes in time with\nthe accumulation of new data, and provide particle learning algorithms that\nallow for the efficient on-line posterior filtering of tree-states. A major\nadvantage of tree regression is that it allows for the use of very simple\nmodels within each partition. The model also facilitates a natural division of\nlabor in our sequential particle-based inference: tree dynamics are defined\nthrough a few potential changes that are local to each newly arrived\nobservation, while global uncertainty is captured by the ensemble of particles.\nWe consider both constant and linear mean functions at the tree leaves, along\nwith multinomial leaves for classification problems, and propose default prior\nspecifications that allow for prediction to be integrated over all model\nparameters conditional on a given tree. Inference is illustrated in some\nstandard nonparametric regression examples, as well as in the setting of\nsequential experiment design, including both active learning and optimization\napplications, and in on-line classification. We detail implementation\nguidelines and problem specific methodology for each of these motivating\napplications. Throughout, it is demonstrated that our practical approach is\nable to provide better results compared to commonly used methods at a fraction\nof the cost.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2009 23:49:08 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2009 00:04:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2010 20:11:53 GMT"}, {"version": "v4", "created": "Sun, 21 Nov 2010 05:31:05 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Taddy", "Matthew A.", ""], ["Gramacy", "Robert B.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "0912.2380", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer, Livia B. P\\'artay, G\\'abor Cs\\'anyi", "title": "Diffusive Nested Sampling", "comments": "Accepted for publication in Statistics and Computing. C++ code\n  available at http://lindor.physics.ucsb.edu/DNest", "journal-ref": "Statistics and Computing, 2011, 21, 4, 649-656", "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We introduce a general Monte Carlo method based on Nested Sampling (NS), for\nsampling complex probability distributions and estimating the normalising\nconstant. The method uses one or more particles, which explore a mixture of\nnested probability distributions, each successive distribution occupying ~e^-1\ntimes the enclosed prior mass of the previous distribution. While NS\ntechnically requires independent generation of particles, Markov Chain Monte\nCarlo (MCMC) exploration fits naturally into this technique. We illustrate the\nnew method on a test problem and find that it can achieve four times the\naccuracy of classic MCMC-based Nested Sampling, for the same computational\neffort; equivalent to a factor of 16 speedup. An additional benefit is that\nmore samples and a more accurate evidence value can be obtained simply by\ncontinuing the run for longer, as in standard MCMC.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2009 00:17:37 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2010 18:36:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2010 23:17:34 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Brewer", "Brendon J.", ""], ["P\u00e1rtay", "Livia B.", ""], ["Cs\u00e1nyi", "G\u00e1bor", ""]]}, {"id": "0912.2695", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng and Rui Song", "title": "Nonparametric Independence Screening in Sparse Ultra-High Dimensional\n  Additive Models", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variable screening procedure via correlation learning was proposed Fan and\nLv (2008) to reduce dimensionality in sparse ultra-high dimensional models.\nEven when the true model is linear, the marginal regression can be highly\nnonlinear. To address this issue, we further extend the correlation learning to\nmarginal nonparametric learning. Our nonparametric independence screening is\ncalled NIS, a specific member of the sure independence screening. Several\nclosely related variable screening procedures are proposed. Under the\nnonparametric additive models, it is shown that under some mild technical\nconditions, the proposed independence screening methods enjoy a sure screening\nproperty. The extent to which the dimensionality can be reduced by independence\nscreening is also explicitly quantified. As a methodological extension, an\niterative nonparametric independence screening (INIS) is also proposed to\nenhance the finite sample performance for fitting sparse additive models. The\nsimulation results and a real data analysis demonstrate that the proposed\nprocedure works well with moderate sample size and large dimension and performs\nbetter than competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2009 17:35:50 GMT"}, {"version": "v2", "created": "Tue, 18 Jan 2011 18:17:27 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Song", "Rui", ""]]}, {"id": "0912.3880", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Bootstrapping Confidence Levels for Hypotheses about Quadratic\n  (U-Shaped) Regression Models", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping can produce confidence levels for hypotheses about quadratic\nregression models - such as whether the U-shape is inverted, and the location\nof optima. The method has several advantages over conventional methods: it\nprovides more, and clearer, information, and is flexible - it could easily be\napplied to a wide variety of different types of models. The utility of the\nmethod can be enhanced by formulating models with interpretable coefficients,\nsuch as the location and value of the optimum. Keywords: Bootstrap resampling;\nConfidence level; Quadratic model; Regression, U-shape.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2009 09:48:17 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 20:36:37 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2012 08:38:44 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2012 12:35:12 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "0912.4729", "submitter": "Yanan Fan Dr", "authors": "G. W. Peters, S. A. Sisson and Y. Fan", "title": "Likelihood-free Bayesian inference for alpha-stable models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\alpha$-stable distributions are utilised as models for heavy-tailed noise\nin many areas of statistics, finance and signal processing engineering.\n  However, in general, neither univariate nor multivariate $\\alpha$-stable\nmodels admit closed form densities which can be evaluated pointwise. This\ncomplicates the inferential procedure.\n  As a result, $\\alpha$-stable models are practically limited to the univariate\nsetting under the Bayesian paradigm, and to bivariate models under the\nclassical framework.\n  In this article we develop a novel Bayesian approach to modelling univariate\nand multivariate $\\alpha$-stable distributions based on recent advances in\n\"likelihood-free\" inference.\n  We present an evaluation of the performance of this procedure in 1, 2 and 3\ndimensions, and provide an analysis of real daily currency exchange rate data.\nThe proposed approach provides a feasible inferential methodology at a moderate\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2009 20:53:12 GMT"}], "update_date": "2009-12-24", "authors_parsed": [["Peters", "G. W.", ""], ["Sisson", "S. A.", ""], ["Fan", "Y.", ""]]}, {"id": "0912.4896", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, Iain Murray, David J.C. MacKay", "title": "Nonparametric Bayesian Density Modeling with Gaussian Processes", "comments": "26 pages, 4 figures, submitted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Gaussian process density sampler (GPDS), an exchangeable\ngenerative model for use in nonparametric Bayesian density estimation. Samples\ndrawn from the GPDS are consistent with exact, independent samples from a\ndistribution defined by a density that is a transformation of a function drawn\nfrom a Gaussian process prior. Our formulation allows us to infer an unknown\ndensity from data using Markov chain Monte Carlo, which gives samples from the\nposterior distribution over density functions and from the predictive\ndistribution on data space. We describe two such MCMC methods. Both methods\nalso allow inference of the hyperparameters of the Gaussian process.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2009 18:46:38 GMT"}], "update_date": "2009-12-25", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Murray", "Iain", ""], ["MacKay", "David J. C.", ""]]}]