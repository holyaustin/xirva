[{"id": "1303.0238", "submitter": "James M. Flegal", "authors": "James M. Flegal and Lei Gong", "title": "Relative fixed-width stopping rules for Markov chain Monte Carlo\n  simulations", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) simulations are commonly employed for\nestimating features of a target distribution, particularly for Bayesian\ninference. A fundamental challenge is determining when these simulations should\nstop. We consider a sequential stopping rule that terminates the simulation\nwhen the width of a confidence interval is sufficiently small relative to the\nsize of the target parameter. Specifically, we propose relative magnitude and\nrelative standard deviation stopping rules in the context of MCMC. In each\nsetting, we develop sufficient conditions for asymptotic validity, that is\nconditions to ensure the simulation will terminate with probability one and the\nresulting confidence intervals will have the proper coverage probability. Our\nresults are applicable in a wide variety of MCMC estimation settings, such as\nexpectation, quantile, or simultaneous multivariate estimation. Finally, we\ninvestigate the finite sample properties through a variety of examples and\nprovide some recommendations to practitioners.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2013 18:04:30 GMT"}], "update_date": "2013-03-04", "authors_parsed": [["Flegal", "James M.", ""], ["Gong", "Lei", ""]]}, {"id": "1303.0383", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Daniel W. Apley", "title": "Local Gaussian process approximation for large computer experiments", "comments": "29 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new approach to approximate emulation of large computer\nexperiments. By focusing expressly on desirable properties of the predictive\nequations, we derive a family of local sequential design schemes that\ndynamically define the support of a Gaussian process predictor based on a local\nsubset of the data. We further derive expressions for fast sequential updating\nof all needed quantities as the local designs are built-up iteratively. Then we\nshow how independent application of our local design strategy across the\nelements of a vast predictive grid facilitates a trivially parallel\nimplementation. The end result is a global predictor able to take advantage of\nmodern multicore architectures, while at the same time allowing for a\nnonstationary modeling feature as a bonus. We demonstrate our method on two\nexamples utilizing designs sized in the thousands, and tens of thousands of\ndata points. Comparisons are made to the method of compactly supported\ncovariances.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2013 12:56:45 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2014 22:04:18 GMT"}, {"version": "v3", "created": "Tue, 8 Apr 2014 13:28:46 GMT"}, {"version": "v4", "created": "Fri, 10 Oct 2014 13:16:56 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Apley", "Daniel W.", ""]]}, {"id": "1303.1125", "submitter": "Paul Slater", "authors": "Paul B. Slater", "title": "Two-Qubit Rational-Valued Entanglement-Boundary Probability Densities\n  and a Fisher Information Equality Conjecture", "comments": "15 pages, 14 figures, 2 tables. Further analyses included.\n  Substantial expansion", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph math-ph math.MP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a pair of one-parameter (alpha) families of generalized two-qubit\ndeterminantal Hilbert-Schmidt probability distributions, p_{alpha}(|rho^{PT}|)\nand q_{alpha}(|rho|), where rho is a 4 x 4 density matrix, rho^{PT}, its\npartial transpose, with |rho^{PT}| \\in [-1/16,1/256] and |rho| \\in [0, 1/256].\nThe Dyson-index-like (random matrix) parameter alpha is 1/2 for the\n9-dimensional generic two-rebit systems, 1 for the 15-dimensional generic\ntwo-qubit systems,... Numerical (moment-based\nprobability-distribution-reconstruction) analyses suggest the conjecture that\nthe Fisher information--a measure over alpha--is identical for the two distinct\nfamilies. Further, we study the values of p_{alpha}(0), the probability\ndensities at the separability-entanglement boundary, with evidence strongly\nindicating that p_2(0) =7425/34 and p_3(0)= 7696/69. Despite extensive results\nof such a nature, we have not yet succeeded--in contrast with the corresponding\nrational-valued separability probabilities (arXiv.org:1301.6617)--in generating\nan underlying, explanatory (\"concise\") formula for p_{\\alpha}(0), even though\nthe corresponding denominators of both sets of rational-valued results are\nclosely related, having almost identical (small) prime factors. The first\nderivatives p_{alpha}^{'}(0) are positive for alpha = 1/2 and 1, but negative\nfor alpha > 1.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2013 18:20:14 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 16:25:00 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Slater", "Paul B.", ""]]}, {"id": "1303.1327", "submitter": "Giulio Cottone", "authors": "Giulio Cottone, Mario Di Paola, Roberta Santoro", "title": "A Novel Exact Representation of Stationary Colored Gaussian Processes\n  (Fractional Differential Approach)", "comments": null, "journal-ref": "Journal of Physics A: Mathematical and Theoretical, 2010, Volume\n  43, Number 8, page 085002", "doi": "10.1088/1751-8113/43/8/085002", "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel representation of functions, called generalized Taylor form, is\napplied to the filtering of white noise processes. It is shown that every\nGaussian colored noise can be expressed as the output of a set of linear\nfractional stochastic differential equation whose solution is a weighted sum of\nfractional Brownian motions. The exact form of the weighting coefficients is\ngiven and it is shown that it is related to the fractional moments of the\ntarget spectral density of the colored noise.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2013 13:27:45 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Cottone", "Giulio", ""], ["Di Paola", "Mario", ""], ["Santoro", "Roberta", ""]]}, {"id": "1303.1993", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin and James V. Burke and Gianluigi Pillonetto", "title": "Optimization viewpoint on Kalman smoothing, with applications to robust\n  and sparse estimation", "comments": "46 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the optimization formulation of the Kalman\nfiltering and smoothing problems, and use this perspective to develop a variety\nof extensions and applications. We first formulate classic Kalman smoothing as\na least squares problem, highlight special structure, and show that the classic\nfiltering and smoothing algorithms are equivalent to a particular algorithm for\nsolving this problem. Once this equivalence is established, we present\nextensions of Kalman smoothing to systems with nonlinear process and\nmeasurement models, systems with linear and nonlinear inequality constraints,\nsystems with outliers in the measurements or sudden changes in the state, and\nsystems where the sparsity of the state sequence must be accounted for. All\nextensions preserve the computational efficiency of the classic algorithms, and\nmost of the extensions are illustrated with numerical examples, which are part\nof an open source Kalman smoothing Matlab/Octave package.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 13:53:40 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 10:35:41 GMT"}], "update_date": "2013-03-12", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1303.2140", "submitter": "Jeroen Ooms", "authors": "Jeroen Ooms", "title": "Possible Directions for Improving Dependency Versioning in R", "comments": null, "journal-ref": "The R Journal Vol. 5/1, June 2013", "doi": null, "report-no": null, "categories": "cs.SE cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most powerful features of R is its infrastructure for contributed\ncode. The built-in package manager and complementary repositories provide a\ngreat system for development and exchange of code, and have played an important\nrole in the growth of the platform towards the de-facto standard in statistical\ncomputing that it is today. However, the number of packages on CRAN and other\nrepositories has increased beyond what might have been foreseen, and is\nrevealing some limitations of the current design. One such problem is the\ngeneral lack of dependency versioning in the infrastructure. This paper\nexplores this problem in greater detail, and suggests approaches taken by other\nopen source communities that might work for R as well. Three use cases are\ndefined that exemplify the issue, and illustrate how improving this aspect of\npackage management could increase reliability while supporting further growth\nof the R community.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2013 22:32:22 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 18:47:18 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Ooms", "Jeroen", ""]]}, {"id": "1303.2423", "submitter": "Houying Zhu", "authors": "Josef Dick, Daniel Rudolf, Houying Zhu", "title": "Discrepancy bounds for uniformly ergodic Markov chain quasi-Monte Carlo", "comments": "Accepted for publication in the Annals of Applied Probability, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chains can be used to generate samples whose distribution approximates\na given target distribution. The quality of the samples of such Markov chains\ncan be measured by the discrepancy between the empirical distribution of the\nsamples and the target distribution. We prove upper bounds on this discrepancy\nunder the assumption that the Markov chain is uniformly ergodic and the driver\nsequence is deterministic rather than independent $U(0,1)$ random variables. In\nparticular, we show the existence of driver sequences for which the discrepancy\nof the Markov chain from the target distribution with respect to certain test\nsets converges with (almost) the usual Monte Carlo rate of $n^{-1/2}$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 04:34:38 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 03:47:19 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 07:00:43 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Dick", "Josef", ""], ["Rudolf", "Daniel", ""], ["Zhu", "Houying", ""]]}, {"id": "1303.2530", "submitter": "Arno Solin", "authors": "Arno Solin and Simo S\\\"arkk\\\"a", "title": "Infinite-dimensional Bayesian filtering for detection of quasi-periodic\n  phenomena in spatio-temporal data", "comments": "9 pages, 5 figures (added references, extended discussion, other\n  minor improvements)", "journal-ref": "Phys. Rev. E 88, 052909 (2013)", "doi": "10.1103/PhysRevE.88.052909", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a spatio-temporal resonator model and an inference\nmethod for detection and estimation of nearly periodic temporal phenomena in\nspatio-temporal data. The model is derived as a spatial extension of a\nstochastic harmonic resonator model, which can be formulated in terms of a\nstochastic differential equation (SDE). The spatial structure is included by\nintroducing linear operators, which affect both the oscillations and damping,\nand by choosing the appropriate spatial covariance structure of the driving\ntime-white noise process. With the choice of the linear operators as partial\ndifferential operators, the resonator model becomes a stochastic partial\ndifferential equation (SPDE), which is compatible with infinite-dimensional\nKalman filtering. The resulting infinite-dimensional Kalman filtering problem\nallows for a computationally efficient solution as the computational cost\nscales linearly with measurements in the temporal dimension. This framework is\napplied to weather prediction and to physiological noise elimination in fMRI\nbrain data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 14:16:54 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2013 17:38:43 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1303.2714", "submitter": "Matthias Morzfeld", "authors": "Alexandre J. Chorin, Matthias Morzfeld", "title": "Conditions for successful data assimilation", "comments": null, "journal-ref": null, "doi": "10.1002/2013JD019838", "report-no": null, "categories": "math-ph math.MP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, using idealized models, that numerical data assimilation can be\nsuccessful only if an effective dimension of the problem is not excessive. This\neffective dimension depends on the noise in the model and the data, and in\nphysically reasonable problems it can be moderate even when the number of\nvariables is huge. We then analyze several data assimilation algorithms,\nincluding particle filters and variational methods. We show that well-designed\nparticle filters can solve most of those data assimilation problems that can be\nsolved in principle, and compare the conditions under which variational methods\ncan succeed to the conditions required of particle filters. We also discuss the\nlimitations of our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2013 23:13:41 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 14:04:11 GMT"}, {"version": "v3", "created": "Wed, 22 Oct 2014 19:44:06 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Chorin", "Alexandre J.", ""], ["Morzfeld", "Matthias", ""]]}, {"id": "1303.2797", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos, Laura A. Hatfield, Bradley P. Carlin and Johanna\n  J.M. Takkenberg", "title": "Combining Dynamic Predictions from Joint Models for Longitudinal and\n  Time-to-Event Data using Bayesian Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint modeling of longitudinal and time-to-event data is an active area\nof statistics research that has received a lot of attention in the recent\nyears. More recently, a new and attractive application of this type of models\nhas been to obtain individualized predictions of survival probabilities and/or\nof future longitudinal responses. The advantageous feature of these predictions\nis that they are dynamically updated as extra longitudinal responses are\ncollected for the subjects of interest, providing real time risk assessment\nusing all recorded information. The aim of this paper is two-fold. First, to\nhighlight the importance of modeling the association structure between the\nlongitudinal and event time responses that can greatly influence the derived\npredictions, and second, to illustrate how we can improve the accuracy of the\nderived predictions by suitably combining joint models with different\nassociation structures. The second goal is achieved using Bayesian model\naveraging, which, in this setting, has the very intriguing feature that the\nmodel weights are not fixed but they are rather subject- and time-dependent,\nimplying that at different follow-up times predictions for the same subject may\nbe based on different models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 07:39:31 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Rizopoulos", "Dimitris", ""], ["Hatfield", "Laura A.", ""], ["Carlin", "Bradley P.", ""], ["Takkenberg", "Johanna J. M.", ""]]}, {"id": "1303.2827", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, James V. Burke and Gianluigi Pillonetto", "title": "Linear system identification using stable spline kernels and PLQ\n  penalties", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach to linear system identification is given by parametric\nPrediction Error Methods (PEM). In this context, model complexity is often\nunknown so that a model order selection step is needed to suitably trade-off\nbias and variance. Recently, a different approach to linear system\nidentification has been introduced, where model order determination is avoided\nby using a regularized least squares framework. In particular, the penalty term\non the impulse response is defined by so called stable spline kernels. They\nembed information on regularity and BIBO stability, and depend on a small\nnumber of parameters which can be estimated from data. In this paper, we\nprovide new nonsmooth formulations of the stable spline estimator. In\nparticular, we consider linear system identification problems in a very broad\ncontext, where regularization functionals and data misfits can come from a rich\nset of piecewise linear quadratic functions. Moreover, our anal- ysis includes\npolyhedral inequality constraints on the unknown impulse response. For any\nformulation in this class, we show that interior point methods can be used to\nsolve the system identification problem, with complexity O(n3)+O(mn2) in each\niteration, where n and m are the number of impulse response coefficients and\nmeasurements, respectively. The usefulness of the framework is illustrated via\na numerical experiment where output measurements are contaminated by outliers.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:31:29 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Burke", "James V.", ""], ["Pillonetto", "Gianluigi", ""]]}, {"id": "1303.2836", "submitter": "Silvia Liverani", "authors": "Silvia Liverani, David I. Hastie, Lamiae Azizi, Michail Papathomas,\n  Sylvia Richardson", "title": "PReMiuM: An R Package for Profile Regression Mixture Models using\n  Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PReMiuM is a recently developed R package for Bayesian clustering using a\nDirichlet process mixture model. This model is an alternative to regression\nmodels, non-parametrically linking a response vector to covariate data through\ncluster membership. The package allows Bernoulli, Binomial, Poisson, Normal and\ncategorical response, as well as Normal and discrete covariates. Additionally,\npredictions may be made for the response, and missing values for the covariates\nare handled. Several samplers and label switching moves are implemented along\nwith diagnostic tools to assess convergence. A number of R functions for\npost-processing of the output are also provided. In addition to fitting\nmixtures, it may additionally be of interest to determine which covariates\nactively drive the mixture components. This is implemented in the package as\nvariable selection.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2013 10:54:28 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2013 18:18:12 GMT"}, {"version": "v3", "created": "Fri, 25 Apr 2014 10:29:50 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Liverani", "Silvia", ""], ["Hastie", "David I.", ""], ["Azizi", "Lamiae", ""], ["Papathomas", "Michail", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1303.3079", "submitter": "Jeffrey Regier", "authors": "Jeffrey C. Regier and Philip B. Stark", "title": "Mini-Minimax Uncertainty Quantification for Emulators", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification. 3-1 (2015), pp.\n  686-708", "doi": "10.1137/130917909", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider approximating a \"black box\" function $f$ by an emulator $\\hat{f}$\nbased on $n$ noiseless observations of $f$. Let $w$ be a point in the domain of\n$f$. How big might the error $|\\hat{f}(w) - f(w)|$ be? If $f$ could be\narbitrarily rough, this error could be arbitrarily large: we need some\nconstraint on $f$ besides the data. Suppose $f$ is Lipschitz with known\nconstant. We find a lower bound on the number of observations required to\nensure that for the best emulator $\\hat{f}$ based on the $n$ data, $|\\hat{f}(w)\n- f(w)| \\le \\epsilon$. But in general, we will not know whether $f$ is\nLipschitz, much less know its Lipschitz constant. Assume optimistically that\n$f$ is Lipschitz-continuous with the smallest constant consistent with the $n$\ndata. We find the maximum (over such regular $f$) of $|\\hat{f}(w) - f(w)|$ for\nthe best possible emulator $\\hat{f}$; we call this the \"mini-minimax\nuncertainty\" at $w$. In reality, $f$ might not be Lipschitz or---if it is---it\nmight not attain its Lipschitz constant on the data. Hence, the mini-minimax\nuncertainty at $w$ could be much smaller than $|\\hat{f}(w) - f(w)|$. But if the\nmini-minimax uncertainty is large, then---even if $f$ satisfies the optimistic\nregularity assumption---$|\\hat{f}(w) - f(w)|$ could be large, no matter how\ncleverly we choose $\\hat{f}$. For the Community Atmosphere Model, the maximum\n(over $w$) of the mini-minimax uncertainty based on a set of 1154~observations\nof $f$ is no smaller than it would be for a single observation of $f$ at the\ncentroid of the 21-dimensional parameter space. We also find lower confidence\nbounds for quantiles of the mini-minimax uncertainty and its mean over the\ndomain of $f$. For the Community Atmosphere Model, these lower confidence\nbounds are an appreciable fraction of the maximum.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2013 02:29:37 GMT"}, {"version": "v2", "created": "Wed, 8 May 2013 00:08:50 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2013 17:25:32 GMT"}, {"version": "v4", "created": "Tue, 8 Apr 2014 20:16:06 GMT"}, {"version": "v5", "created": "Mon, 14 Sep 2015 21:29:54 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Regier", "Jeffrey C.", ""], ["Stark", "Philip B.", ""]]}, {"id": "1303.3480", "submitter": "Raydonal Ospina", "authors": "L\\'aercio Dias, Francisco Cribari-Neto, Raydonal Ospina", "title": "Interval edge estimation in SAR images", "comments": "The manuscript is accepted for publication in IEEE Transactions on\n  Geoscience and Remote Sensing", "journal-ref": null, "doi": "10.1109/TGRS.2014.2329140", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers edge interval estimation between two regions of a\nSynthetic Aperture Radar (SAR) image which differ in texture. This is a\ndifficult task because SAR images are contaminated with speckle noise.\nDifferent point estimation strategies under multiplicative noise are discussed\nin the literature. It is important to assess the quality of such point\nestimates and to also perform inference under a given confidence level. This\ncan be achieved through interval parameter estimation. To that end, we propose\nbootstrap-based edge confidence interval. The relative merits of the different\ninference strategies are compared using Monte Carlo simulation. The results\nshow that interval edge estimation can be used to assess the accuracy of an\nedge point estimate. They also show that interval estimates can be quite\naccurate and that they can indicate the absence of an edge. In order to\nillustrate interval edge estimation, we also analyze a real dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2013 15:33:39 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 02:16:19 GMT"}, {"version": "v3", "created": "Wed, 28 May 2014 21:59:43 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dias", "L\u00e1ercio", ""], ["Cribari-Neto", "Francisco", ""], ["Ospina", "Raydonal", ""]]}, {"id": "1303.3775", "submitter": "Alexandru  Am\\u{a}rioarei", "authors": "Alexandru Amarioarei, Cristian Preda", "title": "Approximation for the Distribution of Three-dimensional Discrete Scan\n  Statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the discrete three dimensional scan statistics. Viewed as the\nmaximum of an 1-dependent stationary r.v.'s sequence, we provide approximations\nand error bounds for the probability distribution of the three dimensional scan\nstatistics. Importance sampling algorithm is used to obtains sharp bounds for\nthe simulation error. Simulation results and comparisons with other\napproximations are presented for the binomial and Poisson models.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2013 14:01:16 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Amarioarei", "Alexandru", ""], ["Preda", "Cristian", ""]]}, {"id": "1303.4434", "submitter": "Pinghua Gong", "authors": "Pinghua Gong, Changshui Zhang, Zhaosong Lu, Jianhua Huang, Jieping Ye", "title": "A General Iterative Shrinkage and Thresholding Algorithm for Non-convex\n  Regularized Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-convex sparsity-inducing penalties have recently received considerable\nattentions in sparse learning. Recent theoretical investigations have\ndemonstrated their superiority over the convex counterparts in several sparse\nlearning settings. However, solving the non-convex optimization problems\nassociated with non-convex penalties remains a big challenge. A commonly used\napproach is the Multi-Stage (MS) convex relaxation (or DC programming), which\nrelaxes the original non-convex problem to a sequence of convex problems. This\napproach is usually not very practical for large-scale problems because its\ncomputational cost is a multiple of solving a single convex problem. In this\npaper, we propose a General Iterative Shrinkage and Thresholding (GIST)\nalgorithm to solve the nonconvex optimization problem for a large class of\nnon-convex penalties. The GIST algorithm iteratively solves a proximal operator\nproblem, which in turn has a closed-form solution for many commonly used\npenalties. At each outer iteration of the algorithm, we use a line search\ninitialized by the Barzilai-Borwein (BB) rule that allows finding an\nappropriate step size quickly. The paper also presents a detailed convergence\nanalysis of the GIST algorithm. The efficiency of the proposed algorithm is\ndemonstrated by extensive experiments on large-scale data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2013 21:41:53 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Gong", "Pinghua", ""], ["Zhang", "Changshui", ""], ["Lu", "Zhaosong", ""], ["Huang", "Jianhua", ""], ["Ye", "Jieping", ""]]}, {"id": "1303.4805", "submitter": "Jabed Hossain Tomal", "authors": "Jabed H. Tomal, William J. Welch, Ruben H. Zamar", "title": "Ensembling classification models based on phalanxes of variables with\n  applications in drug discovery", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS778 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 69-93", "doi": "10.1214/14-AOAS778", "report-no": "IMS-AOAS-AOAS778", "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical detection of a rare class of objects in a two-class\nclassification problem can pose several challenges. Because the class of\ninterest is rare in the training data, there is relatively little information\nin the known class response labels for model building. At the same time the\navailable explanatory variables are often moderately high dimensional. In the\nfour assays of our drug-discovery application, compounds are active or not\nagainst a specific biological target, such as lung cancer tumor cells, and\nactive compounds are rare. Several sets of chemical descriptor variables from\ncomputational chemistry are available to classify the active versus inactive\nclass; each can have up to thousands of variables characterizing molecular\nstructure of the compounds. The statistical challenge is to make use of the\nrichness of the explanatory variables in the presence of scant response\ninformation. Our algorithm divides the explanatory variables into subsets\nadaptively and passes each subset to a base classifier. The various base\nclassifiers are then ensembled to produce one model to rank new objects by\ntheir estimated probabilities of belonging to the rare class of interest. The\nessence of the algorithm is to choose the subsets such that variables in the\nsame group work well together; we call such groups phalanxes.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 01:23:50 GMT"}, {"version": "v2", "created": "Sat, 27 Apr 2013 18:13:27 GMT"}, {"version": "v3", "created": "Tue, 18 Mar 2014 17:12:23 GMT"}, {"version": "v4", "created": "Fri, 15 May 2015 04:36:39 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Tomal", "Jabed H.", ""], ["Welch", "William J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1303.4808", "submitter": "Jeroen Ooms", "authors": "Jeroen Ooms", "title": "The RAppArmor Package: Enforcing Security Policies in R Using Dynamic\n  Sandboxing on Linux", "comments": null, "journal-ref": "Journal of Statistical Software, Vol. 55, Issue 7, Nov 2013", "doi": null, "report-no": null, "categories": "cs.CR cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of cloud computing and scientific super computers\nbrings great potential for making R accessible through public or shared\nresources. This allows us to efficiently run code requiring lots of cycles and\nmemory, or embed R functionality into, e.g., systems and web services. However\nsome important security concerns need to be addressed before this can be put in\nproduction. The prime use case in the design of R has always been a single\nstatistician running R on the local machine through the interactive console.\nTherefore the execution environment of R is entirely unrestricted, which could\nresult in malicious behavior or excessive use of hardware resources in a shared\nenvironment. Properly securing an R process turns out to be a complex problem.\nWe describe various approaches and illustrate potential issues using some of\nour personal experiences in hosting public web services. Finally we introduce\nthe RAppArmor package: a Linux based reference implementation for dynamic\nsandboxing in R on the level of the operating system.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2013 01:57:36 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2013 17:53:41 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2013 17:53:22 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Ooms", "Jeroen", ""]]}, {"id": "1303.5294", "submitter": "Jeffrey Andrews", "authors": "Jeffrey L. Andrews and Paul D. McNicholas", "title": "Variable Selection for Clustering and Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data sets continue to grow in size and complexity, effective and efficient\ntechniques are needed to target important features in the variable space. Many\nof the variable selection techniques that are commonly used alongside\nclustering algorithms are based upon determining the best variable subspace\naccording to model fitting in a stepwise manner. These techniques are often\ncomputationally intensive and can require extended periods of time to run; in\nfact, some are prohibitively computationally expensive for high-dimensional\ndata. In this paper, a novel variable selection technique is introduced for use\nin clustering and classification analyses that is both intuitive and\ncomputationally efficient. We focus largely on applications in mixture\nmodel-based learning, but the technique could be adapted for use with various\nother clustering/classification methods. Our approach is illustrated on both\nsimulated and real data, highlighted by contrasting its performance with that\nof other comparable variable selection techniques on the real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2013 15:26:30 GMT"}], "update_date": "2013-03-22", "authors_parsed": [["Andrews", "Jeffrey L.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1303.6042", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques, SAF)", "title": "Multifidelity variance reduction for pick-freeze Sobol index estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mathematical models involve input parameters, which are not precisely\nknown. Global sensitivity analysis aims to identify the parameters whose\nuncertainty has the largest impact on the variability of a quantity of interest\n(output of the model). One of the statistical tools used to quantify the\ninfluence of each input variable on the output is the Sobol sensitivity index,\nwhich can be estimated using a large sample of evaluations of the output. We\npropose a variance reduction technique, based on the availability of a fast\napproximation of the output, which can enable significant computational savings\nwhen the output is costly to evaluate.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 07:50:13 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Janon", "Alexandre", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques, SAF"]]}, {"id": "1303.6223", "submitter": "Rajen Shah", "authors": "Rajen Dinesh Shah, Nicolai Meinshausen", "title": "Random Intersection Trees", "comments": "23 pages, 4 figures", "journal-ref": "Journal of Machine Learning Research 15 (2014) 629-654", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interactions between variables in large and high-dimensional datasets\nis often a serious computational challenge. Most approaches build up\ninteraction sets incrementally, adding variables in a greedy fashion. The\ndrawback is that potentially informative high-order interactions may be\noverlooked. Here, we propose at an alternative approach for classification\nproblems with binary predictor variables, called Random Intersection Trees. It\nworks by starting with a maximal interaction that includes all variables, and\nthen gradually removing variables if they fail to appear in randomly chosen\nobservations of a class of interest. We show that informative interactions are\nretained with high probability, and the computational complexity of our\nprocedure is of order $p^\\kappa$ for a value of $\\kappa$ that can reach values\nas low as 1 for very sparse data; in many more general settings, it will still\nbeat the exponent $s$ obtained when using a brute force search constrained to\norder $s$ interactions. In addition, by using some new ideas based on min-wise\nhash schemes, we are able to further reduce the computational cost.\nInteractions found by our algorithm can be used for predictive modelling in\nvarious forms, but they are also often of interest in their own right as useful\ncharacterisations of what distinguishes a certain class from others.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2013 17:29:24 GMT"}], "update_date": "2016-04-27", "authors_parsed": [["Shah", "Rajen Dinesh", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1303.6451", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques), Thierry Klein (IMT), Agnes Lagnoux-Renaudie (IMT), Ma\\\"elle\n  Nodet (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann),\n  Cl\\'ementine Prieur (INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques)", "title": "Asymptotic normality and efficiency of two Sobol index estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many mathematical models involve input parameters, which are not precisely\nknown. Global sensitivity analysis aims to identify the parameters whose\nuncertainty has the largest impact on the variability of a quantity of interest\n(output of the model). One of the statistical tools used to quantify the\ninfluence of each input variable on the output is the Sobol sensitivity index.\nWe consider the statistical estimation of this index from a finite sample of\nmodel outputs: we present two estimators and state a central limit theorem for\neach. We show that one of these estimators has an optimal asymptotic variance.\nWe also generalize our results to the case where the true output is not\nobservable, and is replaced by a noisy version.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 12:20:03 GMT"}], "update_date": "2013-03-27", "authors_parsed": [["Janon", "Alexandre", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux-Renaudie", "Agnes", "", "IMT"], ["Nodet", "Ma\u00eblle", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Prieur", "Cl\u00e9mentine", "", "INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean\n  Kuntzmann, - M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques"]]}, {"id": "1303.6529", "submitter": "Roberto Fontana", "authors": "Roberto Fontana", "title": "Random generation of optimal saturated designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient algorithms for searching for optimal saturated designs are widely\navailable. They maximize a given efficiency measure (such as D-optimality) and\nprovide an optimum design. Nevertheless, they do not guarantee a \\emph{global}\noptimal design. Indeed, they start from an initial random design and find a\nlocal optimal design. If the initial design is changed the optimum found will,\nin general, be different. A natural question arises. Should we stop at the\ndesign found or should we run the algorithm again in search of a better design?\nThis paper uses very recent methods and software for discovery probability to\nsupport the decision to continue or stop the sampling. A software tool written\nin SAS has been developed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 15:29:24 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2013 11:49:05 GMT"}], "update_date": "2013-03-29", "authors_parsed": [["Fontana", "Roberto", ""]]}, {"id": "1303.6618", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (- M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques, LM-Orsay), Ma\\\"elle Nodet (- M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, INRIA Grenoble\n  Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann), Cl\\'ementine Prieur (-\n  M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques,\n  INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann)", "title": "Goal-oriented error estimation for the reduced basis method, with\n  application to sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reduced basis method is a powerful model reduction technique designed to\nspeed up the computation of multiple numerical solutions of parametrized\npartial differential equations. We consider a quantity of interest, which is a\nlinear functional of the PDE solution. A new probabilistic error bound for the\nreduced model is proposed. It is efficiently and explicitly computable, and we\nshow on different examples that this error bound is sharper than existing ones.\nWe include application of our work to sensitivity analysis studies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2013 19:37:12 GMT"}, {"version": "v2", "created": "Thu, 10 Jul 2014 18:46:45 GMT"}], "update_date": "2014-07-11", "authors_parsed": [["Janon", "Alexandre", "", "- M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques, LM-Orsay"], ["Nodet", "Ma\u00eblle", "", "- M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, INRIA Grenoble\n  Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"], ["Prieur", "Cl\u00e9mentine", "", "-\n  M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques,\n  INRIA Grenoble Rh\u00f4ne-Alpes / LJK Laboratoire Jean Kuntzmann"]]}, {"id": "1303.7318", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Nikolas Kantas, Elena Ehrlich", "title": "Approximate Inference for Observation Driven Time Series Models with\n  Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we consider approximate Bayesian parameter inference\nfor observation driven time series models. Such statistical models appear in a\nwide variety of applications, including econometrics and applied mathematics.\nThis article considers the scenario where the likelihood function cannot be\nevaluated point-wise; in such cases, one cannot perform exact statistical\ninference, including parameter estimation, which often requires advanced\ncomputational algorithms, such as Markov chain Monte Carlo (MCMC). We introduce\na new approximation based upon approximate Bayesian computation (ABC). Under\nsome conditions, we show that as $n\\rightarrow\\infty$, with $n$ the length of\nthe time series, the ABC posterior has, almost surely, a maximum \\emph{a\nposteriori} (MAP) estimator of the parameters which is different from the true\nparameter. However, a noisy ABC MAP, which perturbs the original data,\nasymptotically converges to the true parameter, almost surely. In order to draw\nstatistical inference, for the ABC approximation adopted, standard MCMC\nalgorithms can have acceptance probabilities that fall at an exponential rate\nin $n$ and slightly more advanced algorithms can mix poorly. We develop a new\nand improved MCMC kernel, which is based upon an exact approximation of a\nmarginal algorithm, whose cost per-iteration is random but the expected cost,\nfor good performance, is shown to be $\\mathcal{O}(n^2)$ per-iteration.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2013 07:44:19 GMT"}], "update_date": "2013-04-01", "authors_parsed": [["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""], ["Ehrlich", "Elena", ""]]}]