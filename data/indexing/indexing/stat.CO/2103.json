[{"id": "2103.00173", "submitter": "Luyao Zhang", "authors": "Yulin Liu, Luyao Zhang and Yinhong Zhao", "title": "Deciphering Bitcoin Blockchain Data by Cohort Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.NA math.NA q-fin.CP q-fin.EC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bitcoin is a peer-to-peer electronic payment system that popularized rapidly\nin recent years. Usually, we need to query the complete history of Bitcoin\nblockchain data to acquire variables with economic meaning. This becomes\nincreasingly difficult now with over 1.6 billion historical transactions on the\nBitcoin blockchain. It is thus important to query Bitcoin transaction data in a\nway that is more efficient and provides economic insights. We apply cohort\nanalysis that interprets Bitcoin blockchain data using methods developed for\npopulation data in social science. Specifically, we query and process the\nBitcoin transaction input and output data within each daily cohort, which\nenables us to create datasets and visualizations for some key indicators of\nBitcoin transactions, including the daily lifespan distributions of spent\ntransaction output (STXO) and the daily age distributions of the accumulated\nunspent transaction output (UTXO). We provide a computationally feasible\napproach to characterize Bitcoin transactions, which paves the way for the\nfuture economic studies of Bitcoin.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:24:18 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 06:39:52 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yulin", ""], ["Zhang", "Luyao", ""], ["Zhao", "Yinhong", ""]]}, {"id": "2103.00318", "submitter": "Lucia Yang", "authors": "Lucia Minah Yang and Ian Grooms", "title": "Machine Learning Techniques to Construct Patched Analog Ensembles for\n  Data Assimilation", "comments": "21 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using generative models from the machine learning literature to create\nartificial ensemble members for use within data assimilation schemes has been\nintroduced in [Grooms QJRMS, 2020] as constructed analog ensemble optimal\ninterpolation (cAnEnOI). Specifically, we study general and variational\nautoencoders for the machine learning component of this method, and combine the\nideas of constructed analogs and ensemble optimal interpolation in the data\nassimilation piece. To extend the scalability of cAnEnOI for use in data\nassimilation on complex dynamical models, we propose using patching schemes to\ndivide the global spatial domain into digestible chunks. Using patches makes\ntraining the generative models possible and has the added benefit of being able\nto exploit parallelism during the generative step. Testing this new algorithm\non a 1D toy model, we find that larger patch sizes make it harder to train an\naccurate generative model (i.e. a model whose reconstruction error is small),\nwhile conversely the data assimilation performance improves at larger patch\nsizes. There is thus a sweet spot where the patch size is large enough to\nenable good data assimilation performance, but not so large that it becomes\ndifficult to train an accurate generative model. In our tests the new patched\ncAnEnOI method outperforms the original (unpatched) cAnEnOI, as well as the\nensemble square root filter results from [Grooms QJRMS, 2020].\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 20:47:27 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yang", "Lucia Minah", ""], ["Grooms", "Ian", ""]]}, {"id": "2103.00533", "submitter": "Raphael Huser", "authors": "Peng Zhong, Rapha\\\"el Huser and Thomas Opitz", "title": "Exact Simulation of Max-Infinitely Divisible Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-infinitely divisible (max-id) processes play a central role in\nextreme-value theory and include the subclass of all max-stable processes. They\nallow for a constructive representation based on the componentwise maximum of\nrandom functions drawn from a Poisson point process defined on a suitable\nfunctions space. Simulating from a max-id process is often difficult due to its\ncomplex stochastic structure, while calculating its joint density in high\ndimensions is often numerically infeasible. Therefore, exact and efficient\nsimulation techniques for max-id processes are useful tools for studying the\ncharacteristics of the process and for drawing statistical inferences. Inspired\nby the simulation algorithms for max-stable processes, we here develop theory\nand algorithms to generalize simulation approaches tailored for certain\nflexible (existing or new) classes of max-id processes. Efficient simulation\nfor a large class of models can be achieved by implementing an adaptive\nrejection sampling scheme to sidestep a numerical integration step in the\nalgorithm. We present the results of a simulation study highlighting that our\nsimulation algorithm works as expected and is highly accurate and efficient,\nsuch that it clearly outperforms customary approximate sampling schemes. As a\nbyproduct, we also develop here new max-id models, which can be represented as\npointwise maxima of general location scale mixtures, and which possess flexible\ntail dependence structures capturing a wide range of asymptotic dependence\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 15:17:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhong", "Peng", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""]]}, {"id": "2103.00831", "submitter": "Jonghyeon Ko", "authors": "Jonghyeon Ko and Marco Comuzzi", "title": "Online anomaly detection using statistical leverage for streaming\n  business process events", "comments": "12 pages, 4 figures, conference (Proceedings of the 1st International\n  Workshop on Streaming Analytics for Process Mining (SA4PM 2020) in\n  conjunction with International Conference on Process Mining, Accepted for\n  publication (Sep 2020))", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several techniques for detecting trace-level anomalies in event logs in\noffline settings have appeared recently in the literature, such techniques are\ncurrently lacking for online settings. Event log anomaly detection in online\nsettings can be crucial for discovering anomalies in process execution as soon\nas they occur and, consequently, allowing to promptly take early corrective\nactions. This paper describes a novel approach to event log anomaly detection\non event streams that uses statistical leverage. Leverage has been used\nextensively in statistics to develop measures to identify outliers and it has\nbeen adapted in this paper to the specific scenario of event stream data. The\nproposed approach has been evaluated on both artificial and real event streams.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:01:49 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ko", "Jonghyeon", ""], ["Comuzzi", "Marco", ""]]}, {"id": "2103.01220", "submitter": "Prathamesh Muzumdar", "authors": "Prathamesh Muzumdar, George Kurian", "title": "Empirical study to explore the influence of salesperson's customer\n  orientation on customer loyalty", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.00054", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study tries to examine the influence of salesperson's customer\norientation on customer loyalty. Customer orientation is the approach taken by\na salesperson to improve customer relationship and increase sales. Many\norganizations prefer sales orientation as a strategic approach towards\nincreasing sales. Though successful in its objective, sales orientation fails\nto attract repetitive purchase. It has become a necessity to train frontline\nemployees to better understand the customer needs, keeping in mind the firm's\nultimate objective. This study examines the improvements customer orientation\ncan bring to increase repurchases thus leading to customer loyalty. The\nfindings suggest that product assortment, long lines of customers, customers'\nannual income, and the listening skills of salesperson were the significant\nantecedents of customer loyalty.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 05:15:59 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Muzumdar", "Prathamesh", ""], ["Kurian", "George", ""]]}, {"id": "2103.01254", "submitter": "Vincenzo Nardelli", "authors": "Giorgio Alleva, Giuseppe Arbia, Piero Demetrio Falorsi, Vincenzo\n  Nardelli, Alberto Zuliani", "title": "Spatial sampling design to improve the efficiency of the estimation of\n  the critical parameters of the SARS-CoV-2 epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic linked to COVID-19 infection represents an unprecedented\nclinical and healthcare challenge for many medical researchers attempting to\nprevent its worldwide spread. This pandemic also represents a major challenge\nfor statisticians involved in quantifying the phenomenon and in offering timely\ntools for the monitoring and surveillance of critical pandemic parameters. In a\nrecent paper, Alleva et al. (2020) proposed a two-stage sample design to build\na continuous-time surveillance system designed to correctly quantify the number\nof infected people through an indirect sampling mechanism that could be\nrepeated in several waves over time to capture different target variables in\nthe different stages of epidemic development. The proposed method exploits the\nindirect sampling (Lavalle, 2007; Kiesl, 2016) method employed in the\nestimation of rare and elusive populations (Borchers, 2009; Lavall\\'ee and\nRivest, 2012) and a capture/recapture mechanism (Sudman, 1988; Thompson and\nSeber, 1996). In this paper, we extend the proposal of Alleva et al. (2020) to\ninclude a spatial sampling mechanism (M\\\"uller, 1998; Grafstr\\\"om et al., 2012,\nJauslin and Till\\`e, 2020) in the process of data collection to achieve the\nsame level of precision with fewer sample units, thereby facilitating the\nprocess of data collection in a situation where timeliness and costs are\ncrucial elements. We present the basic idea of the new sample design,\nanalytically prove the theoretical properties of the associated estimators and\nshow the relative advantages through a systematic simulation study where all\nthe typical elements of an epidemic are accounted for.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:08:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Alleva", "Giorgio", ""], ["Arbia", "Giuseppe", ""], ["Falorsi", "Piero Demetrio", ""], ["Nardelli", "Vincenzo", ""], ["Zuliani", "Alberto", ""]]}, {"id": "2103.01327", "submitter": "Trong-Nghia Nguyen", "authors": "Minh-Ngoc Tran, Trong-Nghia Nguyen, and Viet-Hung Dao", "title": "A practical tutorial on Variational Bayes", "comments": "43 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial gives a quick introduction to Variational Bayes (VB), also\ncalled Variational Inference or Variational Approximation, from a practical\npoint of view. The paper covers a range of commonly used VB methods and an\nattempt is made to keep the materials accessible to the wide community of data\nanalysis practitioners. The aim is that the reader can quickly derive and\nimplement their first VB algorithm for Bayesian inference with their data\nanalysis problem. An end-user software package in Matlab together with the\ndocumentation can be found at https://vbayeslab.github.io/VBLabDocs/\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 22:11:42 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Nguyen", "Trong-Nghia", ""], ["Dao", "Viet-Hung", ""]]}, {"id": "2103.01526", "submitter": "Oswaldo Gressani", "authors": "Oswaldo Gressani, Christel Faes, Niel Hens", "title": "Laplacian-P-splines for Bayesian inference in the mixture cure model", "comments": "36 pages, 7 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture cure model for analyzing survival data is characterized by the\nassumption that the population under study is divided into a group of subjects\nwho will experience the event of interest over some finite time horizon and\nanother group of cured subjects who will never experience the event\nirrespective of the duration of follow-up. When using the Bayesian paradigm for\ninference in survival models with a cure fraction, it is common practice to\nrely on Markov chain Monte Carlo (MCMC) methods to sample from posterior\ndistributions. Although computationally feasible, the iterative nature of MCMC\noften implies long sampling times to explore the target space with chains that\nmay suffer from slow convergence and poor mixing. An alternative strategy for\nfast and flexible sampling-free Bayesian inference in the mixture cure model is\nsuggested in this paper by combining Laplace approximations and penalized\nB-splines. A logistic regression model is assumed for the cure proportion and a\nCox proportional hazards model with a P-spline approximated baseline hazard is\nused to specify the conditional survival function of susceptible subjects.\nLaplace approximations to the conditional latent vector are based on analytical\nformulas for the gradient and Hessian of the log-likelihood, resulting in a\nsubstantial speed-up in approximating posterior distributions. The statistical\nperformance and computational efficiency of the proposed Laplacian-P-splines\nmixture cure (LPSMC) model is assessed in a simulation study. Results show that\nLPSMC is an appealing alternative to classic MCMC for approximate Bayesian\ninference in standard mixture cure models. Finally, the novel LPSMC approach is\nillustrated on three applications involving real survival data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:15:27 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 10:01:00 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Gressani", "Oswaldo", ""], ["Faes", "Christel", ""], ["Hens", "Niel", ""]]}, {"id": "2103.01532", "submitter": "Ranjan Maitra", "authors": "Subrata Pal and Somak Dutta and Ranjan Maitra", "title": "Model-based Personalized Synthetic MR Imaging", "comments": "13 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Magnetic Resonance (MR) imaging predicts images at new design\nparameter settings from a few observed MR scans. Model-based methods, that use\nboth the physical and statistical properties underlying the MR signal and its\nacquisition, can predict images at any setting from as few as three scans,\nallowing it to be used in individualized patient- and anatomy-specific\ncontexts. However, the estimation problem in model-based synthetic MR imaging\nis ill-posed and so regularization, in the form of correlated Gaussian Markov\nRandom Fields, is imposed on the voxel-wise spin-lattice relaxation time,\nspin-spin relaxation time and the proton density underlying the MR image. We\ndevelop theoretically sound but computationally practical matrix-free\nestimation methods for synthetic MR imaging. Our evaluations demonstrate\nexcellent ability of our methods to synthetize MR images in a clinical\nframework and also estimation and prediction accuracy and consistency. An added\nstrength of our model-based approach, also developed and illustrated here, is\nthe accurate estimation of standard errors of regional means in the synthesized\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:24:35 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:16:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pal", "Subrata", ""], ["Dutta", "Somak", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2103.01621", "submitter": "Edouard Ollier", "authors": "Edouard Ollier", "title": "Fast selection of nonlinear mixed effect models using penalized\n  likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear Mixed effects models are hidden variables models that are widely\nused in many field such as pharmacometrics. In such models, the distribution\ncharacteristics of hidden variables can be specified by including several\nparameters such as covariates or correlations which must be selected. Recent\ndevelopment of pharmacogenomics has brought averaged/high dimensional problems\nto the field of nonlinear mixed effects modeling for which standard covariates\nselection techniques like stepwise methods are not well suited. This work\nproposes to select covariates and correlation parameters using a penalized\nlikelihood approach. The penalized likelihood problem is solved using a\nstochastic proximal gradient algorithm to avoid inner-outer iterations. Speed\nof convergence of the proximal gradient algorithm is improved by the use of\ncomponent-wise adaptive gradient step sizes. The practical implementation and\ntuning of the proximal gradient algorithm is explored using simulations.\nCalibration of regularization parameters is performed by minimizing the\nBayesian Information Criterion using particle swarm optimization, a zero order\noptimization procedure. The use of warm restart and parallelization allows to\nreduce significantly computing time. The performance of the proposed method\ncompared to the traditional grid search strategy is explored using simulated\ndata. Finally, an application to real data from two pharmacokinetics studies is\nprovided, one studying an antifibrinolitic and the other studying an\nantibiotic.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:24:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ollier", "Edouard", ""]]}, {"id": "2103.02366", "submitter": "Mads Lindskou", "authors": "Mads Lindskou, Torben Tvedebrink, Poul Svante Eriksen and Niels\n  Morling", "title": "Detecting Outliers in High-dimensional Data with Mixed Variable Types\n  using Conditional Gaussian Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Outlier detection has gained increasing interest in recent years, due to\nnewly emerging technologies and the huge amount of high-dimensional data that\nare now available. Outlier detection can help practitioners to identify\nunwanted noise and/or locate interesting abnormal observations. To address\nthis, we developed a novel method for outlier detection for use in, possibly\nhigh-dimensional, datasets with both discrete and continuous variables. We\nexploit the family of decomposable graphical models in order to model the\nrelationship between the variables and use this to form an exact likelihood\nratio test for an observation that is considered an outlier. We show that our\nmethod outperforms the state-of-the-art Isolation Forest algorithm on a real\ndata example.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:40:12 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 13:21:40 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 08:14:57 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lindskou", "Mads", ""], ["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""], ["Morling", "Niels", ""]]}, {"id": "2103.02407", "submitter": "Christopher Drovandi Dr", "authors": "Christopher Drovandi and David T Frazier", "title": "A Comparison of Likelihood-Free Methods With and Without Summary\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods are useful for parameter estimation of complex models\nwith intractable likelihood functions for which it is easy to simulate data.\nSuch models are prevalent in many disciplines including genetics, biology,\necology and cosmology. Likelihood-free methods avoid explicit likelihood\nevaluation by finding parameter values of the model that generate data close to\nthe observed data. The general consensus has been that it is most efficient to\ncompare datasets on the basis of a low dimensional informative summary\nstatistic, incurring information loss in favour of reduced dimensionality. More\nrecently, researchers have explored various approaches for efficiently\ncomparing empirical distributions in the likelihood-free context in an effort\nto avoid data summarisation. This article provides a review of these full data\ndistance based approaches, and conducts the first comprehensive comparison of\nsuch methods, both qualitatively and empirically. We also conduct a substantive\nempirical comparison with summary statistic based likelihood-free methods. The\ndiscussion and results offer guidance to practitioners considering a\nlikelihood-free approach. Whilst we find the best approach to be problem\ndependent, we also find that the full data distance based approaches are\npromising and warrant further development. We discuss some opportunities for\nfuture research in this space.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:56:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Drovandi", "Christopher", ""], ["Frazier", "David T", ""]]}, {"id": "2103.02438", "submitter": "Adam Foster", "authors": "Adam Foster, Desi R. Ivanova, Ilyas Malik, Tom Rainforth", "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of\nadaptive Bayesian experimental design that allows experiments to be run in\nreal-time. Traditional sequential Bayesian optimal experimental design\napproaches require substantial computation at each stage of the experiment.\nThis makes them unsuitable for most real-world applications, where decisions\nmust typically be made quickly. DAD addresses this restriction by learning an\namortized design network upfront and then using this to rapidly run (multiple)\nadaptive experiments at deployment time. This network represents a design\npolicy which takes as input the data from previous steps, and outputs the next\ndesign using a single forward pass; these design decisions can be made in\nmilliseconds during the live experiment. To train the network, we introduce\ncontrastive information bounds that are suitable objectives for the sequential\nsetting, and propose a customized network architecture that exploits key\nsymmetries. We demonstrate that DAD successfully amortizes the process of\nexperimental design, outperforming alternative strategies on a number of\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:43:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:18:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Foster", "Adam", ""], ["Ivanova", "Desi R.", ""], ["Malik", "Ilyas", ""], ["Rainforth", "Tom", ""]]}, {"id": "2103.02506", "submitter": "Michael Lingzhi Li", "authors": "Dimitris Bertsimas, Michael Lingzhi Li", "title": "Stochastic Cutting Planes for Data-Driven Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a stochastic version of the cutting-plane method for a large\nclass of data-driven Mixed-Integer Nonlinear Optimization (MINLO) problems. We\nshow that under very weak assumptions the stochastic algorithm is able to\nconverge to an $\\epsilon$-optimal solution with high probability. Numerical\nexperiments on several problems show that stochastic cutting planes is able to\ndeliver a multiple order-of-magnitude speedup compared to the standard\ncutting-plane method. We further experimentally explore the lower limits of\nsampling for stochastic cutting planes and show that for many problems, a\nsampling size of $O(\\sqrt[3]{n})$ appears to be sufficient for high quality\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:21:32 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Li", "Michael Lingzhi", ""]]}, {"id": "2103.02659", "submitter": "Nicholas Syring", "authors": "Nicholas Syring, Ryan Martin", "title": "Stochastic optimization for numerical evaluation of imprecise\n  probabilities", "comments": "18 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In applications of imprecise probability, analysts must compute lower (or\nupper) expectations, defined as the infimum of an expectation over a set of\nparameter values. Monte Carlo methods consistently approximate expectations at\nfixed parameter values, but can be costly to implement in grid search to locate\nminima over large subsets of the parameter space. We investigate the use of\nstochastic iterative root-finding methods for efficiently computing lower\nexpectations. In two examples we illustrate the use of various stochastic\napproximation methods, and demonstrate their superior performance in comparison\nto grid search.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 20:15:18 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Syring", "Nicholas", ""], ["Martin", "Ryan", ""]]}, {"id": "2103.02721", "submitter": "Virgilio Gomez-Rubio", "authors": "Martin Outzen Berild and Sara Martino and Virgilio G\\'omez-Rubio and\n  H{\\aa}vard Rue", "title": "Importance Sampling with the Integrated Nested Laplace Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Integrated Nested Laplace Approximation (INLA) is a deterministic\napproach to Bayesian inference on latent Gaussian models (LGMs) and focuses on\nfast and accurate approximation of posterior marginals for the parameters in\nthe models. Recently, methods have been developed to extend this class of\nmodels to those that can be expressed as conditional LGMs by fixing some of the\nparameters in the models to descriptive values. These methods differ in the\nmanner descriptive values are chosen. This paper proposes to combine importance\nsampling with INLA (IS-INLA), and extends this approach with the more robust\nadaptive multiple importance sampling algorithm combined with INLA (AMIS-INLA).\n  This paper gives a comparison between these approaches and existing methods\non a series of applications with simulated and observed datasets and evaluates\ntheir performance based on accuracy, efficiency, and robustness. The approaches\nare validated by exact posteriors in a simple bivariate linear model; then,\nthey are applied to a Bayesian lasso model, a Bayesian imputation of missing\ncovariate values, and lastly, in parametric Bayesian quantile regression. The\napplications show that the AMIS-INLA approach, in general, outperforms the\nother methods, but the IS-INLA algorithm could be considered for faster\ninference when good proposals are available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:14:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Berild", "Martin Outzen", ""], ["Martino", "Sara", ""], ["G\u00f3mez-Rubio", "Virgilio", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2103.03122", "submitter": "Giovanni Cerulli", "authors": "Giovanni Cerulli", "title": "Machine Learning using Stata/Python", "comments": "Keywords: Machine Learning, Stata, Python, Optimal tuning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two related Stata modules, r_ml_stata and c_ml_stata, for fitting\npopular Machine Learning (ML) methods both in regression and classification\nsettings. Using the recent Stata/Python integration platform (sfi) of Stata 16,\nthese commands provide hyper-parameters' optimal tuning via K-fold\ncross-validation using greed search. More specifically, they make use of the\nPython Scikit-learn API to carry out both cross-validation and outcome/label\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:31:44 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Cerulli", "Giovanni", ""]]}, {"id": "2103.03321", "submitter": "Karla Monterrubio G\\'omez", "authors": "Karla Monterrubio-G\\'omez and Sara Wade", "title": "On MCMC for variationally sparse Gaussian processes: A pseudo-marginal\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are frequently used in machine learning and\nstatistics to construct powerful models. However, when employing GPs in\npractice, important considerations must be made, regarding the high\ncomputational burden, approximation of the posterior, choice of the covariance\nfunction and inference of its hyperparmeters. To address these issues, Hensman\net al. (2015) combine variationally sparse GPs with Markov chain Monte Carlo\n(MCMC) to derive a scalable, flexible and general framework for GP models.\nNevertheless, the resulting approach requires intractable likelihood\nevaluations for many observation models. To bypass this problem, we propose a\npseudo-marginal (PM) scheme that offers asymptotically exact inference as well\nas computational gains through doubly stochastic estimators for the intractable\nlikelihood and large datasets. In complex models, the advantages of the PM\nscheme are particularly evident, and we demonstrate this on a two-level GP\nregression model with a nonparametric covariance function to capture\nnon-stationarity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:48:29 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Monterrubio-G\u00f3mez", "Karla", ""], ["Wade", "Sara", ""]]}, {"id": "2103.03462", "submitter": "Nicholas Kissel", "authors": "Nicholas Kissel, Lucas Mentch", "title": "Forward Stability and Model Path Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scientific publications follow the familiar recipe of (i) obtain data,\n(ii) fit a model, and (iii) comment on the scientific relevance of the effects\nof particular covariates in that model. This approach, however, ignores the\nfact that there may exist a multitude of similarly-accurate models in which the\nimplied effects of individual covariates may be vastly different. This problem\nof finding an entire collection of plausible models has also received\nrelatively little attention in the statistics community, with nearly all of the\nproposed methodologies being narrowly tailored to a particular model class\nand/or requiring an exhaustive search over all possible models, making them\nlargely infeasible in the current big data era. This work develops the idea of\nforward stability and proposes a novel, computationally-efficient approach to\nfinding collections of accurate models we refer to as model path selection\n(MPS). MPS builds up a plausible model collection via a forward selection\napproach and is entirely agnostic to the model class and loss function\nemployed. The resulting model collection can be displayed in a simple and\nintuitive graphical fashion, easily allowing practitioners to visualize whether\nsome covariates can be swapped for others with minimal loss.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:01:45 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kissel", "Nicholas", ""], ["Mentch", "Lucas", ""]]}, {"id": "2103.03475", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Balasubramanian Narasimhan, Trevor Hastie", "title": "Elastic Net Regularization Paths for All Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lasso and elastic net are popular regularized regression models for\nsupervised learning. Friedman, Hastie, and Tibshirani (2010) introduced a\ncomputationally efficient algorithm for computing the elastic net\nregularization path for ordinary least squares regression, logistic regression\nand multinomial logistic regression, while Simon, Friedman, Hastie, and\nTibshirani (2011) extended this work to Cox models for right-censored data. We\nfurther extend the reach of the elastic net-regularized regression to all\ngeneralized linear model families, Cox models with (start, stop] data and\nstrata, and a simplified version of the relaxed lasso. We also discuss\nconvenient utility functions for measuring the performance of these fitted\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:18:01 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Narasimhan", "Balasubramanian", ""], ["Hastie", "Trevor", ""]]}, {"id": "2103.03647", "submitter": "Mads Lindskou", "authors": "Mads Lindskou, S{\\o}ren H{\\o}jsgaard, Poul Svante Eriksen, Torben\n  Tvedebrink", "title": "sparta: Sparse Tables and their Algebra with a View Towards High\n  Dimensional Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graphical model is a multivariate (potentially very high dimensional)\nprobabilistic model, which is formed by combining lower dimensional components.\nInference (computation of conditional probabilities) is based on message\npassing algorithms that utilize conditional independence structures. In\ngraphical models for discrete variables with finite state spaces, there is a\nfundamental problem in high dimensions: A discrete distribution is represented\nby a table of values, and in high dimensions such tables can become\nprohibitively large. In inference, such tables must be multiplied which can\nlead to even larger tables. The sparta package meets this challenge by\nimplementing methods that efficiently handles multiplication and\nmarginalization of sparse tables. The package was written in the R programming\nlanguage and is freely available from the Comprehensive R Archive Network\n(CRAN). The companion package jti, also on CRAN, was developed to showcase the\npotential of sparta in connection to the Junction Tree Algorithm. We show, that\njti is able to handle highly complex graphical models which are otherwise\ninfeasible due to lack of computer memory, using sparta as a backend for table\noperations.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:20:57 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 17:48:59 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 05:40:42 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 08:06:59 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lindskou", "Mads", ""], ["H\u00f8jsgaard", "S\u00f8ren", ""], ["Eriksen", "Poul Svante", ""], ["Tvedebrink", "Torben", ""]]}, {"id": "2103.03706", "submitter": "Yair Daon", "authors": "Yair Daon, Amit Huppert, Uri Obolski", "title": "DOPE: D-Optimal Pooling Experimental design with application for\n  SARS-CoV-2 screening", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Testing individuals for the presence of severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2), the pathogen causing the coronavirus disease 2019\n(COVID-19), is crucial for curtailing transmission chains. Moreover, rapidly\ntesting many potentially infected individuals is often a limiting factor in\ncontrolling COVID-19 outbreaks. Hence, pooling strategies, wherein individuals\nare grouped and tested simultaneously, are employed. We present a novel pooling\nstrategy that implements D-Optimal Pooling Experimental design (DOPE). DOPE\ndefines optimal pooled tests as those maximizing the mutual information between\ndata and infection states. We estimate said mutual information via Monte-Carlo\nsampling and employ a discrete optimization heuristic for maximizing it. DOPE\noutperforms common pooling strategies both in terms of lower error rates and\nfewer tests utilized. DOPE holds several additional advantages: it provides\nposterior distributions of the probability of infection, rather than only\nbinary classification outcomes; it naturally incorporates prior information of\ninfection probabilities and test error rates; and finally, it can be easily\nextended to include other, newly discovered information regarding COVID-19.\nHence, we believe that implementation of Bayesian D-optimal experimental design\nholds a great promise for the efforts of combating COVID-19 and other future\npandemics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:31:05 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Daon", "Yair", ""], ["Huppert", "Amit", ""], ["Obolski", "Uri", ""]]}, {"id": "2103.04031", "submitter": "Yifan Chen", "authors": "Yifan Chen, Yun Yang", "title": "Accumulations of Projections--A Unified Framework for Random Sketches in\n  Kernel Ridge Regression", "comments": "To appear in the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a sketch of an n-by-n empirical kernel matrix is a common approach\nto accelerate the computation of many kernel methods. In this paper, we propose\na unified framework of constructing sketching methods in kernel ridge\nregression (KRR), which views the sketching matrix S as an accumulation of m\nrescaled sub-sampling matrices with independent columns. Our framework\nincorporates two commonly used sketching methods, sub-sampling sketches (known\nas the Nystr\\\"om method) and sub-Gaussian sketches, as special cases with m=1\nand m=infinity respectively. Under the new framework, we provide a unified\nerror analysis of sketching approximation and show that our accumulation scheme\nimproves the low accuracy of sub-sampling sketches when certain incoherence\ncharacteristic is high, and accelerates the more accurate but computationally\nheavier sub-Gaussian sketches. By optimally choosing the number m of\naccumulations, we show that a best trade-off between computational efficiency\nand statistical accuracy can be achieved. In practice, the sketching method can\nbe as efficiently implemented as the sub-sampling sketches, as only minor extra\nmatrix additions are needed. Our empirical evaluations also demonstrate that\nthe proposed method may attain the accuracy close to sub-Gaussian sketches,\nwhile is as efficient as sub-sampling-based sketches.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:02:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Yifan", ""], ["Yang", "Yun", ""]]}, {"id": "2103.05104", "submitter": "Ali Al-Sharadqah", "authors": "Ali A. Al-Sharadqah and Lorenzo Rull", "title": "New Methods for Detecting Concentric Objects With High Accuracy", "comments": "31 pages, 18 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting concentric geometric objects to digitized data is an important\nproblem in many areas such as iris detection, autonomous navigation, and\nindustrial robotics operations. There are two common approaches to fitting\ngeometric shapes to data: the geometric (iterative) approach and algebraic\n(non-iterative) approach. The geometric approach is a nonlinear iterative\nmethod that minimizes the sum of the squares of Euclidean distances of the\nobserved points to the ellipses and regarded as the most accurate method, but\nit needs a good initial guess to improve the convergence rate. The algebraic\napproach is based on minimizing the algebraic distances with some constraints\nimposed on parametric space. Each algebraic method depends on the imposed\nconstraint, and it can be solved with the aid of the generalized eigenvalue\nproblem. Only a few methods in literature were developed to solve the problem\nof concentric ellipses. Here we study the statistical properties of existing\nmethods by firstly establishing a general mathematical and statistical\nframework for this problem. Using rigorous perturbation analysis, we derive the\nvariances and biasedness of each method under the small-sigma model. We also\ndevelop new estimators, which can be used as reliable initial guesses for other\niterative methods. Then we compare the performance of each method according to\ntheir theoretical accuracy. Not only do our methods described here outperform\nother existing non-iterative methods, they are also quite robust against large\nnoise. These methods and their practical performances are assessed by a series\nof numerical experiments on both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 08:19:18 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Al-Sharadqah", "Ali A.", ""], ["Rull", "Lorenzo", ""]]}, {"id": "2103.05122", "submitter": "Sanket Jantre", "authors": "Sanket R. Jantre and Zichao Wendy Di", "title": "Low-Rank Tensor Regression for X-Ray Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tomographic imaging is useful for revealing the internal structure of a 3D\nsample. Classical reconstruction methods treat the object of interest as a\nvector to estimate its value. Such an approach, however, can be inefficient in\nanalyzing high-dimensional data because of the underexploration of the\nunderlying structure. In this work, we propose to apply a tensor-based\nregression model to perform tomographic reconstruction. Furthermore, we explore\nthe low-rank structure embedded in the corresponding tensor form. As a result,\nour proposed method efficiently reduces the dimensionality of the unknown\nparameters, which is particularly beneficial for ill-posed inverse problem\nsuffering from insufficient data. We demonstrate the robustness of our proposed\napproach on synthetic noise-free data as well as on Gaussian noise-added data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:20:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jantre", "Sanket R.", ""], ["Di", "Zichao Wendy", ""]]}, {"id": "2103.05161", "submitter": "Robert L Obenchain PhD", "authors": "Robert L. Obenchain", "title": "The Efficient Shrinkage Path: Maximum Likelihood of Minimum MSE Risk", "comments": "21 pages, 9 figures. arXiv admin note: substantial text overlap with\n  withdrawn arXiv:2005.14291", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generalized ridge regression shrinkage path is proposed that is as\nshort as possible under the restriction that it must pass through the vector of\nregression coefficient estimators that make the overall Optimal Variance-Bias\nTrade-Off under Normal distribution-theory. Five distinct types of ridge TRACE\ndisplays plus other graphics for this efficient path are motivated and\nillustrated here. These visualizations provide invaluable data-analytic\ninsights and improved self-confidence to researchers and data scientists\nfitting linear models to ill-conditioned (confounded) data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 01:04:55 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 15:56:19 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 19:16:58 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 21:19:09 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Obenchain", "Robert L.", ""]]}, {"id": "2103.05176", "submitter": "Willem van den Boom", "authors": "Willem van den Boom, Ajay Jasra, Maria De Iorio, Alexandros Beskos,\n  Johan G. Eriksson", "title": "Unbiased approximation of posteriors via coupled particle Markov chain\n  Monte Carlo", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a powerful methodology for the\napproximation of posterior distributions. However, the iterative nature of MCMC\ndoes not naturally facilitate its use with modern highly parallelisable\ncomputation on HPC and cloud environments. Another concern is the\nidentification of the bias and Monte Carlo error of produced averages. The\nabove have prompted the recent development of fully (`embarrassingly')\nparallelisable unbiased Monte Carlo methodology based on couplings of MCMC\nalgorithms. A caveat is that formulation of effective couplings is typically\nnot trivial and requires model-specific technical effort. We propose couplings\nof sequential Monte Carlo (SMC) by considering adaptive SMC to approximate\ncomplex, high-dimensional posteriors combined with recent advances in unbiased\nestimation for state-space models. Coupling is then achieved at the SMC level\nand is, in general, not problem-specific. The resulting methodology enjoys\ndesirable theoretical properties. We illustrate the effectiveness of the\nalgorithm via application to two statistical models in high dimensions: (i)\nhorseshoe regression; (ii) Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:02:06 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Boom", "Willem van den", ""], ["Jasra", "Ajay", ""], ["De Iorio", "Maria", ""], ["Beskos", "Alexandros", ""], ["Eriksson", "Johan G.", ""]]}, {"id": "2103.05217", "submitter": "Valentina Di Marco", "authors": "Valentina Di Marco (School of Mathematical Science, Monash University)\n  and Jonathan Keith (School of Mathematical Science, Monash University)", "title": "Sequential Importance Sampling With Corrections For Partially Observed\n  States", "comments": "19 pages, 6 figures, uses arxiv.sty", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider an evolving system for which a sequence of observations is being\nmade, with each observation revealing additional information about current and\npast states of the system. We suppose each observation is made without error,\nbut does not fully determine the state of the system at the time it is made.\n  Our motivating example is drawn from invasive species biology, where it is\ncommon to know the precise location of invasive organisms that have been\ndetected by a surveillance program, but at any time during the program there\nare invaders that have not been detected.\n  We propose a sequential importance sampling strategy to infer the state of\nthe invasion under a Bayesian model of such a system. The strategy involves\nsimulating multiple alternative states consistent with current knowledge of the\nsystem, as revealed by the observations. However, a difficult problem that\narises is that observations made at a later time are invariably incompatible\nwith previously simulated states. To solve this problem, we propose a two-step\niterative process in which states of the system are alternately simulated in\naccordance with past observations, then corrected in light of new observations.\nWe identify criteria under which such corrections can be made while maintaining\nappropriate importance weights.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:34:59 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Di Marco", "Valentina", "", "School of Mathematical Science, Monash University"], ["Keith", "Jonathan", "", "School of Mathematical Science, Monash University"]]}, {"id": "2103.05238", "submitter": "Yifan Chen", "authors": "Yifan Chen, Yun Yang", "title": "Fast Statistical Leverage Score Approximation in Kernel Ridge Regression", "comments": "To appear in the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nystr\\\"om approximation is a fast randomized method that rapidly solves\nkernel ridge regression (KRR) problems through sub-sampling the n-by-n\nempirical kernel matrix appearing in the objective function. However, the\nperformance of such a sub-sampling method heavily relies on correctly\nestimating the statistical leverage scores for forming the sampling\ndistribution, which can be as costly as solving the original KRR. In this work,\nwe propose a linear time (modulo poly-log terms) algorithm to accurately\napproximate the statistical leverage scores in the stationary-kernel-based KRR\nwith theoretical guarantees. Particularly, by analyzing the first-order\ncondition of the KRR objective, we derive an analytic formula, which depends on\nboth the input distribution and the spectral density of stationary kernels, for\ncapturing the non-uniformity of the statistical leverage scores. Numerical\nexperiments demonstrate that with the same prediction accuracy our method is\norders of magnitude more efficient than existing methods in selecting the\nrepresentative sub-samples in the Nystr\\\"om approximation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:57:08 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chen", "Yifan", ""], ["Yang", "Yun", ""]]}, {"id": "2103.05684", "submitter": "Kam\\'elia Daudel", "authors": "Kam\\'elia Daudel, Randal Douc and Fran\\c{c}ois Roueff", "title": "Monotonic Alpha-divergence Minimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a novel iterative algorithm which carries out\n$\\alpha$-divergence minimisation by ensuring a systematic decrease in the\n$\\alpha$-divergence at each step. In its most general form, our framework\nallows us to simultaneously optimise the weights and components parameters of a\ngiven mixture model. Notably, our approach permits to build on various methods\npreviously proposed for $\\alpha$-divergence minimisation such as gradient or\npower descent schemes. Furthermore, we shed a new light on an integrated\nExpectation Maximization algorithm. We provide empirical evidence that our\nmethodology yields improved results, all the while illustrating the numerical\nbenefits of having introduced some flexibility through the parameter $\\alpha$\nof the $\\alpha$-divergence.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:41:03 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Daudel", "Kam\u00e9lia", ""], ["Douc", "Randal", ""], ["Roueff", "Fran\u00e7ois", ""]]}, {"id": "2103.05766", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj", "title": "Interpretable Machines: Constructing Valid Prediction Intervals with\n  Random Forests", "comments": "20 pages including four figures in the main article. Supplementary\n  material available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important issue when using Machine Learning algorithms in recent research\nis the lack of interpretability. Although these algorithms provide accurate\npoint predictions for various learning problems, uncertainty estimates\nconnected with point predictions are rather sparse. A contribution to this gap\nfor the Random Forest Regression Learner is presented here. Based on its\nOut-of-Bag procedure, several parametric and non-parametric prediction\nintervals are provided for Random Forest point predictions and theoretical\nguarantees for its correct coverage probability is delivered. In a second part,\na thorough investigation through Monte-Carlo simulation is conducted evaluating\nthe performance of the proposed methods from three aspects: (i) Analyzing the\ncorrect coverage rate of the proposed prediction intervals, (ii) Inspecting\ninterval width and (iii) Verifying the competitiveness of the proposed\nintervals with existing methods. The simulation yields that the proposed\nprediction intervals are robust towards non-normal residual distributions and\nare competitive by providing correct coverage rates and comparably narrow\ninterval lengths, even for comparably small samples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 23:05:55 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ramosaj", "Burim", ""]]}, {"id": "2103.06315", "submitter": "Jinglai Li", "authors": "Linjie Wen, Jinglai Li", "title": "Linear-Mapping based Variational Ensemble Kalman Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a linear-mapping based variational Ensemble Kalman filter for\nsequential Bayesian filtering problems with generic observation models.\nSpecifically, the proposed method is formulated as to construct a linear\nmapping from the prior ensemble to the posterior one, and the linear mapping is\ncomputed via a variational Bayesian formulation, i.e., by minimizing the\nKullback-Leibler divergence between the transformed distribution by the linear\nmapping and the actual posterior. A gradient descent scheme is proposed to\nsolve the resulting optimization problem. With numerical examples we\ndemonstrate that the method has competitive performance against existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 19:46:02 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:15:06 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 22:18:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Wen", "Linjie", ""], ["Li", "Jinglai", ""]]}, {"id": "2103.06324", "submitter": "Zhumengmeng Jin", "authors": "Zhumengmeng Jin and James P. Hobert", "title": "Dimension free convergence rates for Gibbs samplers for Bayesian linear\n  mixed models", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of big data has led to a growing interest in so-called\nconvergence complexity analysis, which is the study of how the convergence rate\nof a Monte Carlo Markov chain (for an intractable Bayesian posterior\ndistribution) scales as the underlying data set grows in size. Convergence\ncomplexity analysis of practical Monte Carlo Markov chains on continuous state\nspaces is quite challenging, and there have been very few successful analyses\nof such chains. One fruitful analysis was recently presented by Qin and Hobert\n(2021b), who studied a Gibbs sampler for a simple Bayesian random effects\nmodel. These authors showed that, under regularity conditions, the geometric\nconvergence rate of this Gibbs sampler converges to zero (indicating immediate\nconvergence) as the data set grows in size. It is shown herein that similar\nbehavior is exhibited by Gibbs samplers for more general Bayesian models that\npossess both random effects and traditional continuous covariates, the\nso-called mixed models. The analysis employs the Wasserstein-based techniques\nintroduced by Qin and Hobert (2021b).\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:06:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Jin", "Zhumengmeng", ""], ["Hobert", "James P.", ""]]}, {"id": "2103.06347", "submitter": "Ivor Cribben", "authors": "Martin Ondrus, Emily Olds, Ivor Cribben", "title": "Factorized Binary Search: change point detection in the network\n  structure of multivariate high-dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) time series data presents a\nunique opportunity to understand temporal brain connectivity, and models that\nuncover the complex dynamic workings of this organ are of keen interest in\nneuroscience. Change point models can capture and reflect the dynamic nature of\nbrain connectivity, however methods that translate well into a high-dimensional\ncontext (where $p>>n$) are scarce. To this end, we introduce\n$\\textit{factorized binary search}$ (FaBiSearch), a novel change point\ndetection method in the network structure of multivariate high-dimensional time\nseries. FaBiSearch uses non-negative matrix factorization, an unsupervised\ndimension reduction technique, and a new binary search algorithm to identify\nmultiple change points. In addition, we propose a new method for network\nestimation for data between change points. We show that FaBiSearch outperforms\nanother state-of-the-art method on simulated data sets and we apply FaBiSearch\nto a resting-state and to a task-based fMRI data set. In particular, for the\ntask-based data set, we explore network dynamics during the reading of Chapter\n9 in $\\textit{Harry Potter and the Sorcerer's Stone}$ and find that change\npoints across subjects coincide with key plot twists. Further, we find that the\ndensity of networks was positively related to the frequency of speech between\ncharacters in the story. Finally, we make all the methods discussed available\nin the R package $\\textbf{fabisearch}$ on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:25:20 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ondrus", "Martin", ""], ["Olds", "Emily", ""], ["Cribben", "Ivor", ""]]}, {"id": "2103.07453", "submitter": "Hiba Nassar", "authors": "Rani Basna, Hiba Nassar and Krzysztof Podg\\'orski", "title": "Machine Learning Assisted Orthonormal Basis Selection for Functional\n  Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In implementations of the functional data methods, the effect of the initial\nchoice of an orthonormal basis has not gained much attention in the past.\nTypically, several standard bases such as Fourier, wavelets, splines, etc. are\nconsidered to transform observed functional data and a choice is made without\nany formal criteria indicating which of the bases is preferable for the initial\ntransformation of the data into functions. In an attempt to address this issue,\nwe propose a strictly data-driven method of orthogonal basis selection. The\nmethod uses recently introduced orthogonal spline bases called the splinets\nobtained by efficient orthogonalization of the B-splines. The algorithm learns\nfrom the data in the machine learning style to efficiently place knots. The\noptimality criterion is based on the average (per functional data point) mean\nsquare error and is utilized both in the learning algorithms and in comparison\nstudies. The latter indicates efficiency that is particularly evident for the\nsparse functional data and to a lesser degree in analyses of responses to\ncomplex physical systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:27:29 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Basna", "Rani", ""], ["Nassar", "Hiba", ""], ["Podg\u00f3rski", "Krzysztof", ""]]}, {"id": "2103.07515", "submitter": "Ian Langmore", "authors": "Ian Langmore, Michael Dikovsky, Scott Geraedts, Peter Norgaard, Rob\n  von Behren", "title": "Hamiltonian Monte Carlo in Inverse Problems; Ill-Conditioning and\n  Multi-Modality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamiltonian Monte Carlo (HMC) method allows sampling from continuous\ndensities. Favorable scaling with dimension has led to wide adoption of HMC by\nthe statistics community. Modern auto-differentiating software should allow\nmore widespread usage in Bayesian inverse problems. This paper analyzes the two\nmajor difficulties encountered using HMC for inverse problems: poor\nconditioning and multi-modality. Novel results on preconditioning and replica\nexchange Monte Carlo parameter selection are presented in the context of\nspectroscopy. Recommendations are analyzed rigorously in the Gaussian case, and\nshown to generalize in a fusion plasma reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 20:35:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Langmore", "Ian", ""], ["Dikovsky", "Michael", ""], ["Geraedts", "Scott", ""], ["Norgaard", "Peter", ""], ["von Behren", "Rob", ""]]}, {"id": "2103.08026", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Sirui Bi, Guannan Zhang", "title": "A Scalable Gradient-Free Method for Bayesian Experimental Design with\n  Implicit Models", "comments": "This paper has been accepted by the 24th International Conference on\n  Artificial Intelligence and Statistics (AISTATS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian experimental design (BED) is to answer the question that how to\nchoose designs that maximize the information gathering. For implicit models,\nwhere the likelihood is intractable but sampling is possible, conventional BED\nmethods have difficulties in efficiently estimating the posterior distribution\nand maximizing the mutual information (MI) between data and parameters. Recent\nwork proposed the use of gradient ascent to maximize a lower bound on MI to\ndeal with these issues. However, the approach requires a sampling path to\ncompute the pathwise gradient of the MI lower bound with respect to the design\nvariables, and such a pathwise gradient is usually inaccessible for implicit\nmodels. In this paper, we propose a novel approach that leverages recent\nadvances in stochastic approximate gradient ascent incorporated with a smoothed\nvariational MI estimator for efficient and robust BED. Without the necessity of\npathwise gradients, our approach allows the design process to be achieved\nthrough a unified procedure with an approximate gradient for implicit models.\nSeveral experiments show that our approach outperforms baseline methods, and\nsignificantly improves the scalability of BED in high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 20:28:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Bi", "Sirui", ""], ["Zhang", "Guannan", ""]]}, {"id": "2103.08069", "submitter": "I\\~naki Ucar", "authors": "I\\~naki Ucar, Dirk Eddelbuettel", "title": "Binary R Packages for Linux: Past, Present and Future", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-compiled binary packages provide a very convenient way of efficiently\ndistributing software that has been adopted by most Linux package management\nsystems. However, the heterogeneity of the Linux ecosystem, combined with the\ngrowing number of R extensions available, poses a scalability problem. As a\nresult, efforts to bring binary R packages to Linux have been scattered, and\nlack a proper mechanism to fully integrate them with R's package manager. This\nwork reviews past and present of binary distribution for Linux, and presents a\npath forward by showcasing the `cran2copr' project, an RPM-based\nproof-of-concept implementation of an automated scalable binary distribution\nsystem with the capability of building, maintaining and distributing thousands\nof packages, while providing a portable and extensible bridge to the system\npackage manager.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:53:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ucar", "I\u00f1aki", ""], ["Eddelbuettel", "Dirk", ""]]}, {"id": "2103.08478", "submitter": "Sebastiano Grazzi", "authors": "Joris Bierkens, Sebastiano Grazzi, Frank van der Meulen and Moritz\n  Schauer", "title": "Sticky PDMP samplers for sparse and local inference problems", "comments": "31 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a new class of efficient Monte Carlo methods based on\ncontinuous-time piecewise deterministic Markov processes (PDMP) suitable for\ninference in high dimensional sparse models, i.e. models for which there is\nprior knowledge that many coordinates are likely to be exactly $0$. This is\nachieved with the fairly simple idea of endowing existing PDMP samplers with\nsticky coordinate axes, coordinate planes etc. Upon hitting those subspaces, an\nevent is triggered, during which the process sticks to the subspace, this way\nspending some time in a sub-model. That introduces non-reversible jumps between\ndifferent (sub-)models. The approach can also be combined with local\nimplementations of PDMP samplers to target measures that additionally exhibit a\nsparse dependency structure. We illustrate the new method for a number of\nstatistical models where both the sample size $N$ and the dimensionality $d$ of\nthe parameter space are large.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:53:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bierkens", "Joris", ""], ["Grazzi", "Sebastiano", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "2103.09017", "submitter": "Torben Sell", "authors": "Jacob Vorstrup Goldman, Torben Sell, and Sumeetpal Sidhu Singh", "title": "Gradient-Based Markov Chain Monte Carlo for Bayesian Inference With\n  Non-Differentiable Priors", "comments": "Accepted for publication by the Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of non-differentiable priors in Bayesian statistics has become\nincreasingly popular, in particular in Bayesian imaging analysis. Current state\nof the art methods are approximate in the sense that they replace the posterior\nwith a smooth approximation via Moreau-Yosida envelopes, and apply\ngradient-based discretized diffusions to sample from the resulting\ndistribution. We characterize the error of the Moreau-Yosida approximation and\npropose a novel implementation using underdamped Langevin dynamics. In\nmisson-critical cases, however, replacing the posterior with an approximation\nmay not be a viable option. Instead, we show that Piecewise-Deterministic\nMarkov Processes (PDMP) can be utilized for exact posterior inference from\ndistributions satisfying almost everywhere differentiability. Furthermore, in\ncontrast with diffusion-based methods, the suggested PDMP-based samplers place\nno assumptions on the prior shape, nor require access to a computationally\ncheap proximal operator, and consequently have a much broader scope of\napplication. Through detailed numerical examples, including a\nnon-differentiable circular distribution and a non-convex genomics model, we\nelucidate the relative strengths of these sampling methods on problems of\nmoderate to high dimensions, underlining the benefits of PDMP-based methods\nwhen accurate sampling is decisive.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 12:28:26 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Goldman", "Jacob Vorstrup", ""], ["Sell", "Torben", ""], ["Singh", "Sumeetpal Sidhu", ""]]}, {"id": "2103.09090", "submitter": "Asif Shakeel", "authors": "Ji\\v{r}\\'i Lebl, Asif Shakeel", "title": "Variational Quantum Algorithms for Euclidean Discrepancy and\n  Covariate-Balancing", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic discrepancy theory seeks efficient algorithms to find those\ntwo-colorings of a set that minimize a given measure of coloring imbalance in\nthe set, its {\\it discrepancy}. The {\\it Euclidean discrepancy} problem and the\nproblem of balancing covariates in randomized trials have efficient randomized\nalgorithms based on the Gram-Schmidt walk (GSW). We frame these problems as\nquantum Ising models, for which variational quantum algorithms (VQA) are\nparticularly useful. Simulating an example of covariate-balancing on an IBM\nquantum simulator, we find that the variational quantum eigensolver (VQE) and\nthe quantum approximate optimization algorithm (QAOA) yield results comparable\nto the GSW algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 14:13:29 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lebl", "Ji\u0159\u00ed", ""], ["Shakeel", "Asif", ""]]}, {"id": "2103.09175", "submitter": "Ali Eshragh", "authors": "Ali Eshragh, Glen Livingston, Thomas McCarthy McCann, Luke Yerbury", "title": "Rollage: Efficient Rolling Average Algorithm to Estimate ARMA Models for\n  Big Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new method to estimate an ARMA model in the presence of big time\nseries data. Using the concept of a rolling average, we develop a new efficient\nalgorithm, called Rollage, to estimate the order of an AR model and\nsubsequently fit the model. When used in conjunction with an existing\nmethodology, specifically Durbin's algorithm, we show that our proposed method\ncan be used as a criterion to optimally fit ARMA models. Empirical results on\nlarge-scale synthetic time series data support the theoretical results and\nreveal the efficacy of this new approach, especially when compared to existing\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:23:41 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Eshragh", "Ali", ""], ["Livingston", "Glen", ""], ["McCann", "Thomas McCarthy", ""], ["Yerbury", "Luke", ""]]}, {"id": "2103.09572", "submitter": "Guillaume Damblin", "authors": "Guillaume Damblin and Alberto Ghione", "title": "Adaptive use of replicated Latin Hypercube Designs for computing Sobol'\n  sensitivity indices", "comments": "29 pages, 11 figures", "journal-ref": "Reliability Engineering & System Safety, Volume 212, August 2021,\n  107507", "doi": "10.1016/j.ress.2021.107507", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As recently pointed out in the field of Global Sensitivity Analysis (GSA) of\ncomputer simulations, the use of replicated Latin Hypercube Designs (rLHDs) is\na cost-saving alternative to regular Monte Carlo sampling to estimate\nfirst-order Sobol' indices. Indeed, two rLHDs are sufficient to compute the\nwhole set of those indices regardless of the number of input variables. This\nrelies on a permutation trick which, however, only works within the class of\nestimators called Oracle 2. In the present paper, we show that rLHDs are still\nbeneficial to another class of estimators, called Oracle 1, which often\noutperforms Oracle 2 for estimating small and moderate indices. Even though\nunlike Oracle 2 the computation cost of Oracle 1 depends on the input\ndimension, the permutation trick can be applied to construct an averaged\n(triple) Oracle 1 estimator whose great accuracy is presented on a numerical\nexample.\n  Thus, we promote an adaptive rLHDs-based Sobol' sensitivity analysis where\nthe first stage is to compute the whole set of first-order indices by Oracle 2.\nIf needed, the accuracy of small and moderate indices can then be reevaluated\nby the averaged Oracle 1 estimators. This strategy, cost-saving and\nguaranteeing the accuracy of estimates, is applied to a computer model from the\nnuclear field.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:19:40 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Damblin", "Guillaume", ""], ["Ghione", "Alberto", ""]]}, {"id": "2103.09805", "submitter": "Ryan Hornby", "authors": "Ryan Hornby and Jingchen Hu", "title": "Bayesian Estimation of Attribute Disclosure Risks in Synthetic Data with\n  the $\\texttt{AttributeRiskCalculation}$ R Package", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data is a promising approach to privacy protection in many\ncontexts. A Bayesian synthesis model, also known as a synthesizer, simulates\nsynthetic values of sensitive variables from their posterior predictive\ndistributions. The resulting synthetic data can then be released in place of\nthe confidential data. An important evaluation prior to synthetic data release\nis its level of privacy protection, which is often in the form of disclosure\nrisks evaluation. Attribute disclosure, referring to an intruder correctly\ninferring the confidential values of synthetic records, is one type of\ndisclosure that is challenging to be computationally evaluated. In this paper,\nwe review and discuss in detail some Bayesian estimation approaches to\nattribute disclosure risks evaluation, with examples of commonly-used Bayesian\nsynthesizers. We create the $\\texttt{AttributeRiskCalculation}$ R package to\nfacilitate its implementation, and demonstrate its functionality with examples\nof evaluating attribute disclosure risks in synthetic samples of the Consumer\nExpenditure Surveys.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:43:38 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hornby", "Ryan", ""], ["Hu", "Jingchen", ""]]}, {"id": "2103.09929", "submitter": "Aaron Osgood-Zimmerman", "authors": "Aaron Osgood-Zimmerman, Jon Wakefield", "title": "A Statistical Introduction to Template Model Builder: A Flexible Tool\n  for Spatial Modeling", "comments": "84 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The integrated nested Laplace approximation (INLA) is a well-known and\npopular technique for spatial modeling with a user-friendly interface in the\nR-INLA package. Unfortunately, only a certain class of latent Gaussian models\nare amenable to fitting with INLA. In this paper we describe Template Model\nBuilder (TMB), an existing technique which is well-suited to fitting complex\nspatio-temporal models. TMB is relatively unknown to the spatial statistics\ncommunity, but is a highly flexible random effects modeling tool which allows\nusers to define complex random effects models through simple C++ templates.\nAfter contrasting the methodology behind TMB with INLA, we provide a\nlarge-scale simulation study assessing and comparing R-INLA and TMB for\ncontinuous spatial models, fitted via the Stochastic Partial Differential\nEquations (SPDE) approximation. The results show that the predictive fields\nfrom both methods are comparable in most situations even though TMB estimates\nfor fixed or random effects may have slightly larger bias than R-INLA. We also\npresent a smaller discrete spatial simulation study, in which both approaches\nperform well. We conclude with an analysis of breast cancer incidence and\nmortality data using a joint model which cannot be fit with INLA.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:55:37 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Osgood-Zimmerman", "Aaron", ""], ["Wakefield", "Jon", ""]]}, {"id": "2103.10875", "submitter": "Timoth\\'ee Stumpf-F\\'etizon", "authors": "Omiros Papaspiliopoulos, Timoth\\'ee Stumpf-F\\'etizon, Giacomo Zanella", "title": "Scalable computation for Bayesian hierarchical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is about algorithms for learning Bayesian hierarchical models,\nthe computational complexity of which scales linearly with the number of\nobservations and the number of parameters in the model. It focuses on crossed\nrandom effect and nested multilevel models, which are used ubiquitously in\napplied sciences, and illustrates the methodology on two challenging real data\nanalyses on predicting electoral results and real estate prices respectively.\nThe posterior dependence in both classes is sparse: in crossed random effects\nmodels it resembles a random graph, whereas in nested multilevel models it is\ntree-structured. For each class we develop a framework for scalable computation\nbased on collapsed Gibbs sampling and belief propagation respectively. We\nprovide a number of negative (for crossed) and positive (for nested) results\nfor the scalability (or lack thereof) of methods based on sparse linear\nalgebra, which are relevant also to Laplace approximation methods for such\nmodels. Our numerical experiments compare with off-the-shelf variational\napproximations and Hamiltonian Monte Carlo. Our theoretical results, although\npartial, are useful in suggesting interesting methodologies and lead to\nconclusions that our numerics suggest to hold well beyond the scope of the\nunderlying assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:01:36 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Stumpf-F\u00e9tizon", "Timoth\u00e9e", ""], ["Zanella", "Giacomo", ""]]}, {"id": "2103.10943", "submitter": "Sylvain Le Corff", "authors": "Achille Thin (CMAP), Yazid Janati (IP Paris, TIPIC-SAMOVAR, CITI),\n  Sylvain Le Corff (IP Paris, TIPIC-SAMOVAR, CITI), Charles Ollion (CMAP),\n  Arnaud Doucet, Alain Durmus (CMLA), Eric Moulines (CMAP), Christian Robert\n  (CEREMADE)", "title": "Invertible Flow Non Equilibrium sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously sampling from a complex distribution with intractable\nnormalizing constant and approximating expectations under this distribution is\na notoriously challenging problem. We introduce a novel scheme, Invertible Flow\nNon Equilibrium Sampling (InFine), which departs from classical Sequential\nMonte Carlo (SMC) and Markov chain Monte Carlo (MCMC) approaches. InFine\nconstructs unbiased estimators of expectations and in particular of normalizing\nconstants by combining the orbits of a deterministic transform started from\nrandom initializations.When this transform is chosen as an appropriate\nintegrator of a conformal Hamiltonian system, these orbits are optimization\npaths. InFine is also naturally suited to design new MCMC sampling schemes by\nselecting samples on the optimization paths.Additionally, InFine can be used to\nconstruct an Evidence Lower Bound (ELBO) leading to a new class of Variational\nAutoEncoders (VAE).\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 09:09:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Thin", "Achille", "", "CMAP"], ["Janati", "Yazid", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Corff", "Sylvain Le", "", "IP Paris, TIPIC-SAMOVAR, CITI"], ["Ollion", "Charles", "", "CMAP"], ["Doucet", "Arnaud", "", "CMLA"], ["Durmus", "Alain", "", "CMLA"], ["Moulines", "Eric", "", "CMAP"], ["Robert", "Christian", "", "CEREMADE"]]}, {"id": "2103.11712", "submitter": "Shigekazu Nakagawa", "authors": "Shigekazu Nakagawa, Hiroki Hashiguchi and Yoko Ono", "title": "Approximation to probability density functions in sampling distributions\n  based on Fourier cosine series", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a simple and precise approximation to probability density functions\nin sampling distributions based on the Fourier cosine series. After clarifying\nthe required conditions, we illustrate the approximation on two examples: the\ndistribution of the sum of uniformly distributed random variables, and the\ndistribution of sample skewness drawn from a normal population. The probability\ndensity function of the first example can be explicitly expressed, but that of\nthe second example has no explicit expression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:32:31 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 05:00:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nakagawa", "Shigekazu", ""], ["Hashiguchi", "Hiroki", ""], ["Ono", "Yoko", ""]]}, {"id": "2103.11773", "submitter": "Fan Cheng", "authors": "Fan Cheng, Rob J Hyndman, Anastasios Panagiotelis", "title": "Manifold learning with approximate nearest neighbors", "comments": "46 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Manifold learning algorithms are valuable tools for the analysis of\nhigh-dimensional data, many of which include a step where nearest neighbors of\nall observations are found. This can present a computational bottleneck when\nthe number of observations is large or when the observations lie in more\ngeneral metric spaces, such as statistical manifolds, which require all\npairwise distances between observations to be computed. We resolve this problem\nby using a broad range of approximate nearest neighbor algorithms within\nmanifold learning algorithms and evaluating their impact on embedding accuracy.\nWe use approximate nearest neighbors for statistical manifolds by exploiting\nthe connection between Hellinger/Total variation distance for discrete\ndistributions and the L2/L1 norm. Via a thorough empirical investigation based\non the benchmark MNIST dataset, it is shown that approximate nearest neighbors\nlead to substantial improvements in computational time with little to no loss\nin the accuracy of the embedding produced by a manifold learning algorithm.\nThis result is robust to the use of different manifold learning algorithms, to\nthe use of different approximate nearest neighbor algorithms, and to the use of\ndifferent measures of embedding accuracy. The proposed method is applied to\nlearning statistical manifolds data on distributions of electricity usage. This\napplication demonstrates how the proposed methods can be used to visualize and\nidentify anomalies and uncover underlying structure within high-dimensional\ndata in a way that is scalable to large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:04:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cheng", "Fan", ""], ["Hyndman", "Rob J", ""], ["Panagiotelis", "Anastasios", ""]]}, {"id": "2103.12019", "submitter": "Yiqun Xie", "authors": "Yiqun Xie, Shashi Shekhar, Yan Li", "title": "Statistically-Robust Clustering Techniques for Mapping Spatial Hotspots:\n  A Survey", "comments": "36 pages, 5 figures, submitted to ACM Computing Surveys", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping of spatial hotspots, i.e., regions with significantly higher rates or\nprobability density of generating certain events (e.g., disease or crime\ncases), is a important task in diverse societal domains, including public\nhealth, public safety, transportation, agriculture, environmental science, etc.\nClustering techniques required by these domains differ from traditional\nclustering methods due to the high economic and social costs of spurious\nresults (e.g., false alarms of crime clusters). As a result, statistical rigor\nis needed explicitly to control the rate of spurious detections. To address\nthis challenge, techniques for statistically-robust clustering have been\nextensively studied by the data mining and statistics communities. In this\nsurvey we present an up-to-date and detailed review of the models and\nalgorithms developed by this field. We first present a general taxonomy of the\nclustering process with statistical rigor, covering key steps of data and\nstatistical modeling, region enumeration and maximization, significance\ntesting, and data update. We further discuss different paradigms and methods\nwithin each of key steps. Finally, we highlight research gaps and potential\nfuture directions, which may serve as a stepping stone in generating new ideas\nand thoughts in this growing field and beyond.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:22:30 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xie", "Yiqun", ""], ["Shekhar", "Shashi", ""], ["Li", "Yan", ""]]}, {"id": "2103.12094", "submitter": "Harry Spearing", "authors": "Harry Spearing, Jonathan Tawn, David Irons, Tim Paulden", "title": "Modelling intransitivity in pairwise comparisons with application to\n  baseball data", "comments": "26 pages, 7 figures, 2 tables in the main text. 17 pages in the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most commonly used ranking systems, some level of underlying transitivity\nis assumed. If transitivity exists in a system then information about pairwise\ncomparisons can be translated to other linked pairs. For example, if typically\nA beats B and B beats C, this could inform us about the expected outcome\nbetween A and C. We show that in the seminal Bradley-Terry model knowing the\nprobabilities of A beating B and B beating C completely defines the probability\nof A beating C, with these probabilities determined by individual skill levels\nof A, B and C. Users of this model tend not to investigate the validity of this\ntransitive assumption, nor that some skill levels may not be statistically\nsignificantly different from each other; the latter leading to false\nconclusions about rankings. We provide a novel extension to the Bradley-Terry\nmodel, which accounts for both of these features: the intransitive\nrelationships between pairs of objects are dealt with through interaction terms\nthat are specific to each pair; and by partitioning the $n$ skills into\n$A+1\\leq n$ distinct clusters, any differences in the objects' skills become\nsignificant, given appropriate $A$. With $n$ competitors there are $n(n-1)/2$\ninteractions, so even in multiple round robin competitions this gives too many\nparameters to efficiently estimate. Therefore we separately cluster the\n$n(n-1)/2$ values of intransitivity into $K$ clusters, giving $(A,K)$\nestimatable values respectively, typically with $A+K<n$. Using a Bayesian\nhierarchical model, $(A,K)$ are treated as unknown, and inference is conducted\nvia a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm. The model is\nshown to have an improved fit out of sample in both simulated data and when\napplied to American League baseball data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:00:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Spearing", "Harry", ""], ["Tawn", "Jonathan", ""], ["Irons", "David", ""], ["Paulden", "Tim", ""]]}, {"id": "2103.12423", "submitter": "Matthias Troffaes", "authors": "Nawapon Nakharutai and Matthias C. M. Troffaes and Camila C. S. Caiado", "title": "Improving and benchmarking of algorithms for $\\Gamma$-maximin,\n  $\\Gamma$-maximax and interval dominance", "comments": "29 pages, 12 figures", "journal-ref": "International Journal of Approximate Reasoning 133 (2021) 95-115", "doi": "10.1016/j.ijar.2021.03.005", "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\Gamma$-maximin, $\\Gamma$-maximax and inteval dominance are familiar\ndecision criteria for making decisions under severe uncertainty, when\nprobability distributions can only be partially identified. One can apply these\nthree criteria by solving sequences of linear programs. In this study, we\npresent new algorithms for these criteria and compare their performance to\nexisting standard algorithms. Specifically, we use efficient ways, based on\nprevious work, to find common initial feasible points for these algorithms.\nExploiting these initial feasible points, we develop early stopping criteria to\ndetermine whether gambles are either $\\Gamma$-maximin, $\\Gamma$-maximax or\ninterval dominant. We observe that the primal-dual interior point method\nbenefits considerably from these improvements. In our simulation, we find that\nour proposed algorithms outperform the standard algorithms when the size of the\ndomain of lower previsions is less or equal to the sizes of decisions and\noutcomes. However, our proposed algorithms do not outperform the standard\nalgorithms in the case that the size of the domain of lower previsions is much\nlarger than the sizes of decisions and outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:04:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Nakharutai", "Nawapon", ""], ["Troffaes", "Matthias C. M.", ""], ["Caiado", "Camila C. S.", ""]]}, {"id": "2103.12666", "submitter": "Sara P\\'erez-Vieites", "authors": "Sara P\\'erez-Vieites and Joaqu\\'in M\\'iguez", "title": "Nested Gaussian filters for recursive Bayesian inference and nonlinear\n  tracking in state space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new sequential methodology to calibrate the fixed parameters\nand track the stochastic dynamical variables of a state-space system. The\nproposed method is based on the nested hybrid filtering (NHF) framework of [1],\nthat combines two layers of filters, one inside the other, to compute the joint\nposterior probability distribution of the static parameters and the state\nvariables. In particular, we explore the use of deterministic sampling\ntechniques for Gaussian approximation in the first layer of the algorithm,\ninstead of the Monte Carlo methods employed in the original procedure. The\nresulting scheme reduces the computational cost and so makes the algorithms\npotentially better-suited for high-dimensional state and parameter spaces. We\ndescribe a specific instance of the new method and then study its performance\nand efficiency of the resulting algorithms for a stochastic Lorenz 63 model\nwith uncertain parameters.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:38:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["P\u00e9rez-Vieites", "Sara", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "2103.12946", "submitter": "Linquan Ma", "authors": "Linquan Ma, Lan Liu, Wei Yang", "title": "Envelope Methods with Ignorable Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Envelope method was recently proposed as a method to reduce the dimension of\nresponses in multivariate regressions. However, when there exists missing data,\nthe envelope method using the complete case observations may lead to biased and\ninefficient results. In this paper, we generalize the envelope estimation when\nthe predictors and/or the responses are missing at random. Specifically, we\nincorporate the envelope structure in the expectation-maximization (EM)\nalgorithm. As the parameters under the envelope method are not pointwise\nidentifiable, the EM algorithm for the envelope method was not straightforward\nand requires a special decomposition. Our method is guaranteed to be more\nefficient, or at least as efficient as, the standard EM algorithm. Moreover,\nour method has the potential to outperform the full data MLE. We give\nasymptotic properties of our method under both normal and non-normal cases. The\nefficiency gain over the standard EM is confirmed in simulation studies and in\nan application to the Chronic Renal Insufficiency Cohort (CRIC) study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:48:01 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ma", "Linquan", ""], ["Liu", "Lan", ""], ["Yang", "Wei", ""]]}, {"id": "2103.13357", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li", "title": "A Two-Stage Variable Selection Approach for Correlated High Dimensional\n  Predictors", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models, some predictors are often found to be\ncorrelated with each other, and functioning together. Many group variable\nselection methods are developed to select the groups of predictors that are\nclosely related to the continuous or categorical response. These existing\nmethods usually assume the group structures are well known. For example,\nvariables with similar practical meaning, or dummy variables created by\ncategorical data. However, in practice, it is impractical to know the exact\ngroup structure, especially when the variable dimensional is large. As a\nresult, the group variable selection results may be selected. To solve the\nchallenge, we propose a two-stage approach that combines a variable clustering\nstage and a group variable stage for the group variable selection problem. The\nvariable clustering stage uses information from the data to find a group\nstructure, which improves the performance of the existing group variable\nselection methods. For ultrahigh dimensional data, where the predictors are\nmuch larger than observations, we incorporated a variable screening method in\nthe first stage and shows the advantages of such an approach. In this article,\nwe compared and discussed the performance of four existing group variable\nselection methods under different simulation models, with and without the\nvariable clustering stage. The two-stage method shows a better performance, in\nterms of the prediction accuracy, as well as in the accuracy to select active\npredictors. An athlete's data is also used to show the advantages of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:28:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Zhiyuan", ""]]}, {"id": "2103.13385", "submitter": "Sebastian Reuschen", "authors": "Sebastian Reuschen and Fabian Jobst and Wolfgang Nowak", "title": "Sequential pCN-MCMC, an efficient MCMC method for Bayesian inversion of\n  high-dimensional multi-Gaussian priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In geostatistics, Gaussian random fields are often used to model\nheterogeneities of soil or subsurface parameters. To give spatial\napproximations of these random fields, they are discretized. Then, different\ntechniques of geostatistical inversion are used to condition them on\nmeasurement data. Among these techniques, Markov chain Monte Carlo (MCMC)\ntechniques stand out, because they yield asymptotically unbiased conditional\nrealizations. However, standard Markov Chain Monte Carlo (MCMC) methods suffer\nthe curse of dimensionality when refining the discretization. This means that\ntheir efficiency decreases rapidly with an increasing number of discretization\ncells. Several MCMC approaches have been developed such that the MCMC\nefficiency does not depend on the discretization of the random field. The\npre-conditioned Crank Nicolson Markov Chain Monte Carlo (pCN-MCMC) and the\nsequential Gibbs (or block-Gibbs) sampling are two examples. In this paper, we\nwill present a combination of the pCN-MCMC and the sequential Gibbs sampling.\nOur algorithm, the sequential pCN-MCMC, will depend on two tuning-parameters:\nthe correlation parameter $\\beta$ of the pCN approach and the block size\n$\\kappa$ of the sequential Gibbs approach. The original pCN-MCMC and the Gibbs\nsampling algorithm are special cases of our method. We present an algorithm\nthat automatically finds the best tuning-parameter combination ($\\kappa$ and\n$\\beta$) during the burn-in-phase of the algorithm, thus choosing the best\npossible hybrid between the two methods. In our test cases, we achieve a\nspeedup factors of $1-5.5$ over pCN and of $1-6.5$ over Gibbs. Furthermore, we\nprovide the MATLAB implementation of our method as open-source code.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:56:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Reuschen", "Sebastian", ""], ["Jobst", "Fabian", ""], ["Nowak", "Wolfgang", ""]]}, {"id": "2103.14315", "submitter": "Konstantin Posch", "authors": "Konstantin Posch, Maximilian Arbeiter, Martin Pleschberger, Juergen\n  Pilz", "title": "Variable Selection Using Nearest Neighbor Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel Bayesian approach to the problem of variable selection using Gaussian\nprocess regression is proposed. The selection of the most relevant variables\nfor a problem at hand often results in an increased interpretability and in\nmany cases is an essential step in terms of model regularization. In detail,\nthe proposed method relies on so-called nearest neighbor Gaussian processes,\nthat can be considered as highly scalable approximations of classical Gaussian\nprocesses. To perform a variable selection the mean and the covariance function\nof the process are conditioned on a random set $\\mathcal{A}$. This set holds\nthe indices of variables that contribute to the model. While the specification\nof a priori beliefs regarding $\\mathcal{A}$ allows to control the number of\nselected variables, so-called reference priors are assigned to the remaining\nmodel parameters. The application of the reference priors ensures that the\nprocess covariance matrix is (numerically) robust. For the model inference a\nMetropolis within Gibbs algorithm is proposed. Based on simulated data, an\napproximation problem from computer experiments and two real-world datasets,\nthe performance of the new approach is evaluated.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:11:05 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Posch", "Konstantin", ""], ["Arbeiter", "Maximilian", ""], ["Pleschberger", "Martin", ""], ["Pilz", "Juergen", ""]]}, {"id": "2103.15394", "submitter": "Claudia Di Caterina", "authors": "Claudia Di Caterina, Nancy Reid, Nicola Sartori", "title": "Accurate directional inference in Gaussian graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional tests to compare nested parametric models are developed in the\ngeneral context of covariance selection for Gaussian graphical models. The\nexactness of the underlying saddlepoint approximation leads to exceptional\naccuracy of the proposed approach. This is verified by simulation experiments\nwith high-dimensional parameters of interest, where the accuracy of standard\nasymptotic approximations to the likelihood ratio test and some of its\nhigher-order modifications fails. The directional p-value isused to illustrate\nthe assessment of Markovian dependencies in a dataset from a veterinary trial\non cattle. A second example with microarray data shows how to select the graph\nstructure related to genetic anomalies due to acute lymphocytic leukemia.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 07:42:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Di Caterina", "Claudia", ""], ["Reid", "Nancy", ""], ["Sartori", "Nicola", ""]]}, {"id": "2103.15976", "submitter": "Pierre L'Ecuyer", "authors": "Pierre L'Ecuyer and Florian Puchhammer", "title": "Density Estimation by Monte Carlo and Quasi-Monte Carlo", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the density of a continuous random variable X has been studied\nextensively in statistics, in the setting where n independent observations of X\nare given a priori and one wishes to estimate the density from that. Popular\nmethods include histograms and kernel density estimators. In this review paper,\nwe are interested instead in the situation where the observations are generated\nby Monte Carlo simulation from a model. Then, one can take advantage of\nvariance reduction methods such as stratification, conditional Monte Carlo, and\nrandomized quasi-Monte Carlo (RQMC), and obtain a more accurate density\nestimator than with standard Monte Carlo for a given computing budget. We\ndiscuss several ways of doing this, proposed in recent papers, with a focus on\nmethods that exploit RQMC. A first idea is to directly combine RQMC with a\nstandard kernel density estimator. Another one is to adapt a simulation-based\nderivative estimation method such as smoothed perturbation analysis or the\nlikelihood ratio method to obtain a continuous estimator of the cdf, whose\nderivative is an unbiased estimator of the density. This can then be combined\nwith RQMC. We summarize recent theoretical results with these approaches and\ngive numerical illustrations of how they improve the convergence of the mean\nsquare integrated error.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:35:37 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["L'Ecuyer", "Pierre", ""], ["Puchhammer", "Florian", ""]]}, {"id": "2103.16035", "submitter": "Hanwen Huang", "authors": "Hanwen Huang", "title": "LASSO risk and phase transition under dependence", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a $k$-sparse signal\n${\\mbox{$\\beta$}}_0\\in\\mathbb{R}^p$ from noisy observations $\\bf y={\\bf\nX}\\mbox{$\\beta$}_0+{\\bf w}\\in\\mathbb{R}^n$. One of the most popular approaches\nis the $l_1$-regularized least squares, also known as LASSO. We analyze the\nmean square error of LASSO in the case of random designs in which each row of\n${\\bf X}$ is drawn from distribution $N(0,{\\mbox{$\\Sigma$}})$ with general\n${\\mbox{$\\Sigma$}}$. We first derive the asymptotic risk of LASSO in the limit\nof $n,p\\rightarrow\\infty$ with $n/p\\rightarrow\\delta$. We then examine\nconditions on $n$, $p$, and $k$ for LASSO to exactly reconstruct\n${\\mbox{$\\beta$}}_0$ in the noiseless case ${\\bf w}=0$. A phase boundary\n$\\delta_c=\\delta(\\epsilon)$ is precisely established in the phase space defined\nby $0\\le\\delta,\\epsilon\\le 1$, where $\\epsilon=k/p$. Above this boundary, LASSO\nperfectly recovers ${\\mbox{$\\beta$}}_0$ with high probability. Below this\nboundary, LASSO fails to recover $\\mbox{$\\beta$}_0$ with high probability.\nWhile the values of the non-zero elements of ${\\mbox{$\\beta$}}_0$ do not have\nany effect on the phase transition curve, our analysis shows that $\\delta_c$\ndoes depend on the signed pattern of the nonzero values of $\\mbox{$\\beta$}_0$\nfor general ${\\mbox{$\\Sigma$}}\\ne{\\bf I}_p$. This is in sharp contrast to the\nprevious phase transition results derived in i.i.d. case with\n$\\mbox{$\\Sigma$}={\\bf I}_p$ where $\\delta_c$ is completely determined by\n$\\epsilon$ regardless of the distribution of $\\mbox{$\\beta$}_0$. Underlying our\nformalism is a recently developed efficient algorithm called approximate\nmessage passing (AMP) algorithm. We generalize the state evolution of AMP from\ni.i.d. case to general case with ${\\mbox{$\\Sigma$}}\\ne{\\bf I}_p$. Extensive\ncomputational experiments confirm that our theoretical predictions are\nconsistent with simulation results on moderate size system.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 02:43:32 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Huang", "Hanwen", ""]]}, {"id": "2103.16048", "submitter": "Leah F. South", "authors": "Leah F. South, Marina Riabiz, Onur Teymur, Chris. J. Oates", "title": "Post-Processing of MCMC", "comments": "Version 2 has an updated description of burn-in removal and some\n  additional content on control variates. When citing this paper, please use\n  the following: South, LF, Riabiz, M, Teymur, O & Oates, CJ. 2022.\n  Post-Processing of MCMC. Annual Review of Statistics and Its Application. 9:\n  Submitted. DOI: 10.1146/annurev-statistics-040220-091727", "journal-ref": null, "doi": "10.1146/annurev-statistics-040220-091727", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is the engine of modern Bayesian statistics,\nbeing used to approximate the posterior and derived quantities of interest.\nDespite this, the issue of how the output from a Markov chain is post-processed\nand reported is often overlooked. Convergence diagnostics can be used to\ncontrol bias via burn-in removal, but these do not account for (common)\nsituations where a limited computational budget engenders a bias-variance\ntrade-off. The aim of this article is to review state-of-the-art techniques for\npost-processing Markov chain output. Our review covers methods based on\ndiscrepancy minimisation, which directly address the bias-variance trade-off,\nas well as general-purpose control variate methods for approximating expected\nquantities of interest.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 03:29:58 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 00:41:45 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["South", "Leah F.", ""], ["Riabiz", "Marina", ""], ["Teymur", "Onur", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2103.16245", "submitter": "Bartosz Meglicki", "authors": "Bartosz Meglicki", "title": "Linear time DBSCAN for sorted 1D data and laser range scan segmentation", "comments": "Technical report from personal research (2016/2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.DS eess.SP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper introduces new algorithm for line extraction from laser range data\nincluding methodology for efficient computation. The task is cast to series of\none dimensional problems in various spaces. A fast and simple specialization of\nDBSCAN algorithm is proposed to solve one dimensional subproblems. Experiments\nsuggest that the method is suitable for real-time applications, handles noise\nwell and may be useful in practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:50:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Meglicki", "Bartosz", ""]]}, {"id": "2103.16336", "submitter": "Ranjan Maitra", "authors": "Emily M. Goren and Ranjan Maitra", "title": "Model-based clustering of partial records", "comments": "18 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.HE cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially recorded data are frequently encountered in many applications and\nusually clustered by first removing incomplete cases or features with missing\nvalues, or by imputing missing values, followed by application of a clustering\nalgorithm to the resulting altered dataset. Here, we develop clustering\nmethodology through a model-based approach using the marginal density for the\nobserved values, assuming a finite mixture model of multivariate $t$\ndistributions. We compare our approximate algorithm to the corresponding full\nexpectation-maximization (EM) approach that considers the missing values in the\nincomplete data set and makes a missing at random (MAR) assumption, as well as\ncase deletion and imputation methods. Since only the observed values are\nutilized, our approach is computationally more efficient than imputation or\nfull EM. Simulation studies demonstrate that our approach has favorable\nrecovery of the true cluster partition compared to case deletion and imputation\nunder various missingness mechanisms, and is at least competitive with the full\nEM approach, even when MAR assumptions are violated. Our methodology is\ndemonstrated on a problem of clustering gamma-ray bursts and is implemented at\nhttps://github.com/emilygoren/MixtClust.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:30:59 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:18:14 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 12:54:29 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Goren", "Emily M.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2103.16620", "submitter": "Giorgos Vasdekis", "authors": "Giorgos Vasdekis and Gareth O. Roberts", "title": "Speed Up Zig-Zag", "comments": "32 pages, 4 figures , 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zig-Zag is Piecewise Deterministic Markov Process, efficiently used for\nsimulation in an MCMC setting. As we show in this article, it fails to be\nexponentially ergodic on heavy tailed target distributions. We introduce an\nextension of the Zig-Zag process by allowing the process to move with a\nnon-constant speed function $s$, depending on the current state of the process.\nWe call this process Speed Up Zig-Zag (SUZZ). We provide conditions that\nguarantee stability properties for the SUZZ process, including non-explosivity,\nexponential ergodicity in heavy tailed targets and central limit theorem.\nInterestingly, we find that using speed functions that induce explosive\ndeterministic dynamics may lead to stable algorithms that can even mix faster.\nWe further discuss the choice of an efficient speed function by providing an\nefficiency criterion for the one-dimensional process and we support our\nfindings with simulation results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:46:25 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Vasdekis", "Giorgos", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2103.16649", "submitter": "Rodolphe Le Riche", "authors": "Rodolphe Le Riche, Victor Picheny", "title": "Revisiting Bayesian Optimization in the light of the COCO benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is commonly believed that Bayesian optimization (BO) algorithms are highly\nefficient for optimizing numerically costly functions. However, BO is not often\ncompared to widely different alternatives, and is mostly tested on narrow sets\nof problems (multimodal, low-dimensional functions), which makes it difficult\nto assess where (or if) they actually achieve state-of-the-art performance.\nMoreover, several aspects in the design of these algorithms vary across\nimplementations without a clear recommendation emerging from current practices,\nand many of these design choices are not substantiated by authoritative test\ncampaigns. This article reports a large investigation about the effects on the\nperformance of (Gaussian process based) BO of common and less common design\nchoices. The experiments are carried out with the established COCO (COmparing\nContinuous Optimizers) software. It is found that a small initial budget, a\nquadratic trend, high-quality optimization of the acquisition criterion bring\nconsistent progress. Using the GP mean as an occasional acquisition contributes\nto a negligible additional improvement. Warping degrades performance. The\nMat\\'ern 5/2 kernel is a good default but it may be surpassed by the\nexponential kernel on irregular functions. Overall, the best EGO variants are\ncompetitive or improve over state-of-the-art algorithms in dimensions less or\nequal to 5 for multimodal functions. The code developed for this study makes\nthe new version (v2.1.1) of the R package DiceOptim available on CRAN. The\nstructure of the experiments by function groups allows to define priorities for\nfuture research on Bayesian optimization.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:45:18 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 14:17:15 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 08:00:09 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Riche", "Rodolphe Le", ""], ["Picheny", "Victor", ""]]}, {"id": "2103.16948", "submitter": "Leonardo Egidi PhD", "authors": "Leonardo Egidi, Roberta Pappad\\`a, Francesco Pauli, Nicola Torelli", "title": "pivmet: Pivotal Methods for Bayesian Relabelling and k-Means Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The identification of groups' prototypes, i.e. elements of a dataset that\nrepresent different groups of data points, may be relevant to the tasks of\nclustering, classification and mixture modeling. The R package pivmet presented\nin this paper includes different methods for extracting pivotal units from a\ndataset. One of the main applications of pivotal methods is a Markov Chain\nMonte Carlo (MCMC) relabelling procedure to solve the label switching in\nBayesian estimation of mixture models. Each method returns posterior estimates,\nand a set of graphical tools for visualizing the output. The package offers\nJAGS and Stan sampling procedures for Gaussian mixtures, and allows for\nuser-defined priors' parameters. The package also provides functions to perform\nconsensus clustering based on pivotal units, which may allow to improve\nclassical techniques (e.g. k-means) by means of a careful seeding. The paper\nprovides examples of applications to both real and simulated datasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 10:01:55 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pappad\u00e0", "Roberta", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "2103.17060", "submitter": "Masanari Kimura", "authors": "Masanari Kimura and Hideitsu Hino", "title": "$\\alpha$-Geodesical Skew Divergence", "comments": null, "journal-ref": "Entropy. 2021; 23(5):528", "doi": "10.3390/e23050528", "report-no": null, "categories": "cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The asymmetric skew divergence smooths one of the distributions by mixing it,\nto a degree determined by the parameter $\\lambda$, with the other distribution.\nSuch divergence is an approximation of the KL divergence that does not require\nthe target distribution to be absolutely continuous with respect to the source\ndistribution. In this paper, an information geometric generalization of the\nskew divergence called the $\\alpha$-geodesical skew divergence is proposed, and\nits properties are studied.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:27:58 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 05:40:47 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 11:16:15 GMT"}, {"version": "v4", "created": "Mon, 26 Apr 2021 03:08:24 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kimura", "Masanari", ""], ["Hino", "Hideitsu", ""]]}]