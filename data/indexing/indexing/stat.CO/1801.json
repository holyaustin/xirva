[{"id": "1801.00319", "submitter": "Pulong Ma", "authors": "Pulong Ma, Bledar A. Konomi, Emily L. Kang", "title": "An Additive Approximate Gaussian Process Model for Large Spatio-Temporal\n  Data", "comments": "Accepted in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a large ground-level ozone dataset, we propose a new\ncomputationally efficient additive approximate Gaussian process. The proposed\nmethod incorporates a computational-complexity-reduction method and a separable\ncovariance function, which can flexibly capture various spatio-temporal\ndependence structure. The first component is able to capture nonseparable\nspatio-temporal variability while the second component captures the separable\nvariation. Based on a hierarchical formulation of the model, we are able to\nutilize the computational advantages of both components and perform efficient\nBayesian inference. To demonstrate the inferential and computational benefits\nof the proposed method, we carry out extensive simulation studies assuming\nvarious scenarios of underlying spatio-temporal covariance structure. The\nproposed method is also applied to analyze large spatio-temporal measurements\nof ground-level ozone in the Eastern United States.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 17:17:16 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 01:38:06 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 17:19:58 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ma", "Pulong", ""], ["Konomi", "Bledar A.", ""], ["Kang", "Emily L.", ""]]}, {"id": "1801.00718", "submitter": "Charles Truong", "authors": "Charles Truong, Laurent Oudre, Nicolas Vayatis", "title": "Selective review of offline change point detection methods", "comments": null, "journal-ref": "Signal Processing, 167:107299, 2020", "doi": "10.1016/j.sigpro.2019.107299", "report-no": null, "categories": "cs.CE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a selective survey of algorithms for the offline\ndetection of multiple change points in multivariate time series. A general yet\nstructuring methodological strategy is adopted to organize this vast body of\nwork. More precisely, detection algorithms considered in this review are\ncharacterized by three elements: a cost function, a search method and a\nconstraint on the number of changes. Each of those elements is described,\nreviewed and discussed separately. Implementations of the main algorithms\ndescribed in this article are provided within a Python package called ruptures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:44:08 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 10:19:28 GMT"}, {"version": "v3", "created": "Thu, 26 Mar 2020 08:54:35 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Truong", "Charles", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1801.00736", "submitter": "Manuel Oviedo de la Fuente", "authors": "Manuel Febrero-Bande, Wenceslao Gonz\\'alez-Manteiga and Manuel Oviedo\n  de la Fuente", "title": "Variable selection in Functional Additive Regression Models", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper considers the problem of variable selection in regression models\nin the case of functional variables that may be mixed with other type of\nvariables (scalar, multivariate, directional, etc.). Our proposal begins with a\nsimple null model and sequentially selects a new variable to be incorporated\ninto the model based on the use of distance correlation proposed by\n\\cite{Szekely2007}. For the sake of simplicity, this paper only uses additive\nmodels. However, the proposed algorithm may assess the type of contribution\n(linear, non linear, ...) of each variable. The algorithm has shown quite\npromising results when applied to simulations and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:28:34 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 16:55:42 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Febrero-Bande", "Manuel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["de la Fuente", "Manuel Oviedo", ""]]}, {"id": "1801.00826", "submitter": "Charles Truong", "authors": "Charles Truong, Laurent Oudre, Nicolas Vayatis", "title": "ruptures: change point detection in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ruptures is a Python library for offline change point detection. This package\nprovides methods for the analysis and segmentation of non-stationary signals.\nImplemented algorithms include exact and approximate detection for various\nparametric and non-parametric models. ruptures focuses on ease of use by\nproviding a well-documented and consistent interface. In addition, thanks to\nits modular structure, different algorithms and models can be connected and\nextended within this package.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 20:35:23 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Truong", "Charles", ""], ["Oudre", "Laurent", ""], ["Vayatis", "Nicolas", ""]]}, {"id": "1801.00885", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky, John D. Jakeman", "title": "Gradient-based Optimization for Regression in the Functional\n  Tensor-Train Format", "comments": "24 pages", "journal-ref": null, "doi": "10.1016/j.jcp.2018.08.010", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of low-multilinear-rank functional regression, i.e.,\nlearning a low-rank parametric representation of functions from scattered\nreal-valued data. Our first contribution is the development and analysis of an\nefficient gradient computation that enables gradient-based optimization\nprocedures, including stochastic gradient descent and quasi-Newton methods, for\nlearning the parameters of a functional tensor-train (FT). The functional\ntensor-train uses the tensor-train (TT) representation of low-rank arrays as an\nansatz for a class of low-multilinear-rank functions. The FT is represented by\na set of matrix-valued functions that contain a set of univariate functions,\nand the regression task is to learn the parameters of these univariate\nfunctions. Our second contribution demonstrates that using nonlinearly\nparameterized univariate functions, e.g., symmetric kernels with moving\ncenters, within each core can outperform the standard approach of using a\nlinear expansion of basis functions. Our final contributions are new rank\nadaptation and group-sparsity regularization procedures to minimize\noverfitting. We use several benchmark problems to demonstrate at least an order\nof magnitude lower accuracy with gradient-based optimization methods than\nstandard alternating least squares procedures in the low-sample number regime.\nWe also demonstrate an order of magnitude reduction in accuracy on a test\nproblem resulting from using nonlinear parameterizations over linear\nparameterizations. Finally we compare regression performance with 22 other\nnonparametric and parametric regression methods on 10 real-world data sets. We\nachieve top-five accuracy for seven of the data sets and best accuracy for two\nof the data sets. These rankings are the best amongst parametric models and\ncompetetive with the best non-parametric methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 02:34:14 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 04:53:22 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Jakeman", "John D.", ""]]}, {"id": "1801.01243", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin, Adrian Wills, Brett Ninness", "title": "Constructing Metropolis-Hastings proposals using damped BFGS updates", "comments": "16 pages, 2 figures. Accepted for publication in the Proceedings of\n  the 18th IFAC Symposium on System Identification (SYSID)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of Bayesian estimates of system parameters and functions of\nthem on the basis of observed system performance data is a common problem\nwithin system identification. This is a previously studied issue where\nstochastic simulation approaches have been examined using the popular\nMetropolis--Hastings (MH) algorithm. This prior study has identified a\nrecognised difficulty of tuning the {proposal distribution so that the MH\nmethod provides realisations with sufficient mixing to deliver efficient\nconvergence. This paper proposes and empirically examines a method of tuning\nthe proposal using ideas borrowed from the numerical optimisation literature\naround efficient computation of Hessians so that gradient and curvature\ninformation of the target posterior can be incorporated in the proposal.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 04:33:57 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 00:06:21 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Dahlin", "Johan", ""], ["Wills", "Adrian", ""], ["Ninness", "Brett", ""]]}, {"id": "1801.01357", "submitter": "Xiao-Long Ren", "authors": "Xiao-Long Ren and Niels Gleinig and Dirk Helbing and Nino\n  Antulov-Fantulin", "title": "Generalized Network Dismantling", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": "10.1073/pnas.1806108116", "report-no": null, "categories": "cs.SI cond-mat.stat-mech physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the set of nodes, which removed or (de)activated can stop the spread\nof (dis)information, contain an epidemic or disrupt the functioning of a\ncorrupt/criminal organization is still one of the key challenges in network\nscience. In this paper, we introduce the generalized network dismantling\nproblem, which aims to find the set of nodes that, when removed from a network,\nresults in a network fragmentation into subcritical network components at\nminimum cost. For unit costs, our formulation becomes equivalent to the\nstandard network dismantling problem. Our non-unit cost generalization allows\nfor the inclusion of topological cost functions related to node centrality and\nnon-topological features such as the price, protection level or even social\nvalue of a node. In order to solve this optimization problem, we propose a\nmethod, which is based on the spectral properties of a novel node-weighted\nLaplacian operator. The proposed method is applicable to large-scale networks\nwith millions of nodes. It outperforms current state-of-the-art methods and\nopens new directions in understanding the vulnerability and robustness of\ncomplex systems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 14:13:32 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 14:30:19 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Ren", "Xiao-Long", ""], ["Gleinig", "Niels", ""], ["Helbing", "Dirk", ""], ["Antulov-Fantulin", "Nino", ""]]}, {"id": "1801.01782", "submitter": "Xu Wu", "authors": "Xu Wu, Tomasz Kozlowski, Hadi Meidani, Koroush Shirvan", "title": "Inverse Uncertainty Quantification using the Modular Bayesian Approach\n  based on Gaussian Process, Part 1: Theory", "comments": "27 pages, 10 figures, article", "journal-ref": null, "doi": "10.1016/j.nucengdes.2018.06.004", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nuclear reactor system design and safety analysis, the Best Estimate plus\nUncertainty (BEPU) methodology requires that computer model output\nuncertainties must be quantified in order to prove that the investigated design\nstays within acceptance criteria. \"Expert opinion\" and \"user self-evaluation\"\nhave been widely used to specify computer model input uncertainties in previous\nuncertainty, sensitivity and validation studies. Inverse Uncertainty\nQuantification (UQ) is the process to inversely quantify input uncertainties\nbased on experimental data in order to more precisely quantify such ad-hoc\nspecifications of the input uncertainty information. In this paper, we used\nBayesian analysis to establish the inverse UQ formulation, with systematic and\nrigorously derived metamodels constructed by Gaussian Process (GP). Due to\nincomplete or inaccurate underlying physics, as well as numerical approximation\nerrors, computer models always have discrepancy/bias in representing the\nrealities, which can cause over-fitting if neglected in the inverse UQ process.\nThe model discrepancy term is accounted for in our formulation through the\n\"model updating equation\". We provided a detailed introduction and comparison\nof the full and modular Bayesian approaches for inverse UQ, as well as pointed\nout their limitations when extrapolated to the validation/prediction domain.\nFinally, we proposed an improved modular Bayesian approach that can avoid\nextrapolating the model discrepancy that is learnt from the inverse UQ domain\nto the validation/prediction domain.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 15:11:15 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wu", "Xu", ""], ["Kozlowski", "Tomasz", ""], ["Meidani", "Hadi", ""], ["Shirvan", "Koroush", ""]]}, {"id": "1801.01810", "submitter": "Mathieu Carmassi", "authors": "Mathieu Carmassi, Pierre Barbillon, Merlin Keller, Eric Parent,\n  Matthieu Chiodetti", "title": "Bayesian calibration of a numerical code for prediction", "comments": null, "journal-ref": "Journal SFDS, Vol. 160, No. 1, 2019", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Field experiments are often difficult and expensive to make. To bypass these\nissues, industrial companies have developed computational codes. These codes\nintend to be representative of the physical system, but come with a certain\namount of problems. The code intends to be as close as possible to the physical\nsystem. It turns out that, despite continuous code development, the difference\nbetween the code outputs and experiments can remain significant. Two kinds of\nuncertainties are observed. The first one comes from the difference between the\nphysical phenomenon and the values recorded experimentally. The second concerns\nthe gap between the code and the physical system. To reduce this difference,\noften named model bias, discrepancy, or model error, computer codes are\ngenerally complexified in order to make them more realistic. These improvements\nlead to time consuming codes. Moreover, a code often depends on parameters to\nbe set by the user to make the code as close as possible to field data. This\nestimation task is called calibration. This paper proposes a review of Bayesian\ncalibration methods and is based on an application case which makes it possible\nto discuss the various methodological choices and to illustrate their\ndivergences. This example is based on a code used to predict the power of a\nphotovoltaic plant.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 16:06:48 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 09:14:42 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 10:44:23 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Carmassi", "Mathieu", ""], ["Barbillon", "Pierre", ""], ["Keller", "Merlin", ""], ["Parent", "Eric", ""], ["Chiodetti", "Matthieu", ""]]}, {"id": "1801.01874", "submitter": "Mengyang Gu", "authors": "Mengyang Gu, Jes\\'us Palomo and James O. Berger", "title": "RobustGaSP: Robust Gaussian Stochastic Process Emulation in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian stochastic process emulation is a powerful tool for approximating\ncomputationally intensive computer models. However, estimation of parameters in\nthe GaSP emulator is a challenging task. No closed-form estimator is available\nand many numerical problems arise with standard estimates, e.g., the maximum\nlikelihood estimator. In this package, we implement a marginal posterior mode\nestimator, for special priors and parameterizations, an estimation method that\nmeets the robust parameter estimation criteria discussed in\n\\cite{Gu2018robustness}; mathematical reasons are provided therein to explain\nwhy robust parameter estimation can greatly improve predictive performance of\nthe emulator. In addition, inert inputs (inputs that almost have no effect on\nthe variability of a function) can be identified from the marginal posterior\nmode estimation, at no extra computational cost. The package also implements\nthe parallel partial Gaussian stochastic process (PP GaSP) emulator\n(\\cite{gu2016parallel}) for the scenario where the computer model has multiple\noutputs on e.g. spatial-temporal coordinates. The package can be operated in a\ndefault mode, but also allows numerous user specifications, such as the\ncapability of specifying trend functions and noise terms. Examples are studied\nherein to highlight the performance of the package in terms of out-of-sample\nprediction.}\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 18:58:14 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 02:30:40 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 15:29:09 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Gu", "Mengyang", ""], ["Palomo", "Jes\u00fas", ""], ["Berger", "James O.", ""]]}, {"id": "1801.02106", "submitter": "Marcela Mendoza", "authors": "Marcela Mendoza, Alexis Allegra and Todd P. Coleman", "title": "Bayesian Lasso Posterior Sampling via Parallelized Measure Transport", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that the Lasso can be interpreted as a Bayesian posterior\nmode estimate with a Laplacian prior. Obtaining samples from the full posterior\ndistribution, the Bayesian Lasso, confers major advantages in performance as\ncompared to having only the Lasso point estimate. Traditionally, the Bayesian\nLasso is implemented via Gibbs sampling methods which suffer from lack of\nscalability, unknown convergence rates, and generation of samples that are\nnecessarily correlated. We provide a measure transport approach to generate\ni.i.d samples from the posterior by constructing a transport map that\ntransforms a sample from the Laplacian prior into a sample from the posterior.\nWe show how the construction of this transport map can be parallelized into\nmodules that iteratively solve Lasso problems and perform closed-form linear\nalgebra updates. With this posterior sampling method, we perform maximum\nlikelihood estimation of the Lasso regularization parameter via the EM\nalgorithm. We provide comparisons to traditional Gibbs samplers using the\ndiabetes dataset of Efron et al. Lastly, we give an example implementation on a\ncomputing system that leverages parallelization, a graphics processing unit,\nwhose execution time has much less dependence on dimension as compared to a\nstandard implementation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 00:38:51 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mendoza", "Marcela", ""], ["Allegra", "Alexis", ""], ["Coleman", "Todd P.", ""]]}, {"id": "1801.02248", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsk\\'y", "title": "Exact distribution of selected multivariate test criteria by numerical\n  inversion of their characteristic functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of the exact statistical inference frequently leads to a\nnon-standard probability distributions of the considered estimators or test\nstatistics. The exact distributions of many estimators and test statistics can\nbe specified by their characteristic functions. Typically, distribution of many\nestimators and test statistics can be structurally expressed as a linear\ncombination or product of independent random variables with known distributions\nand characteristic functions, as is the case for many standard multivariate\ntest criteria. The characteristic function represents complete characterization\nof the distribution of the random variable. However, analytical inversion of\nthe characteristic function, if possible, frequently leads to a complicated and\ncomputationally rather strange expressions for the corresponding distribution\nfunction (CDF/PDF) and the required quantiles. As an efficient alternative,\nhere we advocate to use the well-known method based on numerical inversion of\nthe characteristic functions --- a method which is, however, ignored in popular\nstatistical software packages. The applicability of the approach is illustrated\nby computing the exact distribution of the Bartlett's test statistic for\ntesting homogeneity of variances in several normal populations and the Wilks's\n$\\Lambda$-distribution used in multivariate hypothesis testing.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 20:47:42 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Witkovsk\u00fd", "Viktor", ""]]}, {"id": "1801.02309", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Yuansi Chen, Martin J. Wainwright, Bin Yu", "title": "Log-concave sampling: Metropolis-Hastings algorithms are fast", "comments": "42 pages, 11 Figures; The first two authors contributed equally; A\n  subset of results were presented in an extended abstract at COLT 2018", "journal-ref": "Journal of Machine Learning Research, 2019", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove a non-asymptotic upper bound on the mixing time of\nthe Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by\nsimulating a Markov chain obtained from the discretization of an appropriate\nLangevin diffusion, combined with an accept-reject step. Relative to known\nguarantees for the unadjusted Langevin algorithm (ULA), our bounds show that\nthe use of an accept-reject step in MALA leads to an exponentially improved\ndependence on the error-tolerance. Concretely, in order to obtain samples with\nTV error at most $\\delta$ for a density with condition number $\\kappa$, we show\nthat MALA requires $\\mathcal{O} \\big(\\kappa d \\log(1/\\delta) \\big)$ steps, as\ncompared to the $\\mathcal{O} \\big(\\kappa^2 d/\\delta^2 \\big)$ steps established\nin past work on ULA. We also demonstrate the gains of MALA over ULA for weakly\nlog-concave densities. Furthermore, we derive mixing time bounds for the\nMetropolized random walk (MRW) and obtain $\\mathcal{O}(\\kappa)$ mixing time\nslower than MALA. We provide numerical examples that support our theoretical\nfindings, and demonstrate the benefits of Metropolis-Hastings adjustment for\nLangevin-type sampling algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 05:39:14 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 04:51:16 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 21:20:49 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2019 01:26:44 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Chen", "Yuansi", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1801.02597", "submitter": "Claudia Di Caterina", "authors": "Claudia Di Caterina, Giuliana Cortese, Nicola Sartori", "title": "Monte Carlo modified profile likelihood in models for clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main focus of the analysts who deal with clustered data is usually not on\nthe clustering variables, and hence the group-specific parameters are treated\nas nuisance. If a fixed effects formulation is preferred and the total number\nof clusters is large relative to the single-group sizes, classical frequentist\ntechniques relying on the profile likelihood are often misleading. The use of\nalternative tools, such as modifications to the profile likelihood or\nintegrated likelihoods, for making accurate inference on a parameter of\ninterest can be complicated by the presence of nonstandard modelling and/or\nsampling assumptions. We show here how to employ Monte Carlo simulation in\norder to approximate the modified profile likelihood in some of these\nunconventional frameworks. The proposed solution is widely applicable and is\nshown to retain the usual properties of the modified profile likelihood. The\napproach is examined in two instances particularly relevant in applications,\ni.e. missing-data models and survival models with unspecified censoring\ndistribution. The effectiveness of the proposed solution is validated via\nsimulation studies and two clinical trial applications.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 18:20:53 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 08:29:24 GMT"}, {"version": "v3", "created": "Sat, 29 Dec 2018 15:44:18 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Di Caterina", "Claudia", ""], ["Cortese", "Giuliana", ""], ["Sartori", "Nicola", ""]]}, {"id": "1801.03080", "submitter": "Ian Langmore", "authors": "Josh Dillon and Ian Langmore", "title": "Quadrature Compound: An approximating family of distributions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Compound distributions allow construction of a rich set of distributions.\nTypically they involve an intractable integral. Here we use a quadrature\napproximation to that integral to define the quadrature compound family.\nSpecial care is taken that this approximation is suitable for computation of\ngradients with respect to distribution parameters. This technique is applied to\ndiscrete (Poisson LogNormal) and continuous distributions. In the continuous\ncase, quadrature compound family naturally makes use of parameterized\ntransformations of unparameterized distributions (a.k.a \"reparameterization\"),\nallowing for gradients of expectations to be estimated as the gradient of a\nsample mean. This is demonstrated in a novel distribution, the diffeomixture,\nwhich is is a reparameterizable approximation to a mixture distribution.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 18:50:46 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Dillon", "Josh", ""], ["Langmore", "Ian", ""]]}, {"id": "1801.03184", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon, Samuel E. Jackson, Jonathan A. Cumming", "title": "Known Boundary Emulation of Complex Computer Models", "comments": "31 pages, 12 figures. Version accepted for publication by SIAM/ASA\n  Journal on Uncertainty Quantification. Updated references and slightly\n  shortened section 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are now widely used across a range of scientific disciplines\nto describe various complex physical systems, however to perform full\nuncertainty quantification we often need to employ emulators. An emulator is a\nfast statistical construct that mimics the complex computer model, and greatly\naids the vastly more computationally intensive uncertainty quantification\ncalculations that a serious scientific analysis often requires. In some cases,\nthe complex model can be solved far more efficiently for certain parameter\nsettings, leading to boundaries or hyperplanes in the input parameter space\nwhere the model is essentially known. We show that for a large class of\nGaussian process style emulators, multiple boundaries can be formally\nincorporated into the emulation process, by Bayesian updating of the emulators\nwith respect to the boundaries, for trivial computational cost. The resulting\nupdated emulator equations are given analytically. This leads to emulators that\npossess increased accuracy across large portions of the input parameter space.\nWe also describe how a user can incorporate such boundaries within standard\nblack box GP emulation packages that are currently available, without altering\nthe core code. Appropriate designs of model runs in the presence of known\nboundaries are then analysed, with two kinds of general purpose designs\nproposed. We then apply the improved emulation and design methodology to an\nimportant systems biology model of hormonal crosstalk in Arabidopsis Thaliana.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:27:53 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 11:51:07 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Vernon", "Ian", ""], ["Jackson", "Samuel E.", ""], ["Cumming", "Jonathan A.", ""]]}, {"id": "1801.03567", "submitter": "Danilo Alvares", "authors": "Danilo Alvares, Sebastien Haneuse, Catherine Lee, Kyu Ha Lee", "title": "SemiCompRisks: An R Package for Independent and Cluster-Correlated\n  Analyses of Semi-Competing Risks Data", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-competing risks refer to the setting where primary scientific interest\nlies in estimation and inference with respect to a non-terminal event, the\noccurrence of which is subject to a terminal event. In this paper, we present\nthe R package SemiCompRisks that provides functions to perform the analysis of\nindependent/clustered semi-competing risks data under the illness-death\nmulti-state model. The package allows the user to choose the specification for\nmodel components from a range of options giving users substantial flexibility,\nincluding: accelerated failure time or proportional hazards regression models;\nparametric or non-parametric specifications for baseline survival functions;\nparametric or non-parametric specifications for random effects distributions\nwhen the data are cluster-correlated; and, a Markov or semi-Markov\nspecification for terminal event following non-terminal event. While estimation\nis mainly performed within the Bayesian paradigm, the package also provides the\nmaximum likelihood estimation for select parametric models. The package also\nincludes functions for univariate survival analysis as complementary analysis\ntools.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 21:54:42 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 04:26:48 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Alvares", "Danilo", ""], ["Haneuse", "Sebastien", ""], ["Lee", "Catherine", ""], ["Lee", "Kyu Ha", ""]]}, {"id": "1801.03612", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Using probabilistic programs as proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo inference has asymptotic guarantees, but can be slow when using\ngeneric proposals. Handcrafted proposals that rely on user knowledge about the\nposterior distribution can be efficient, but are difficult to derive and\nimplement. This paper proposes to let users express their posterior knowledge\nin the form of proposal programs, which are samplers written in probabilistic\nprogramming languages. One strategy for writing good proposal programs is to\ncombine domain-specific heuristic algorithms with neural network models. The\nheuristics identify high probability regions, and the neural networks model the\nposterior uncertainty around the outputs of the algorithm. Proposal programs\ncan be used as proposal distributions in importance sampling and\nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and\ncan be optimized offline using inference compilation. Support for optimizing\nand using proposal programs is easily implemented in a sampling-based\nprobabilistic programming runtime. The paper illustrates the proposed technique\nwith a proposal program that combines RANSAC and neural networks to accelerate\ninference in a Bayesian linear regression with outliers model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 02:07:39 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 19:47:08 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1801.04153", "submitter": "Francois-Xavier Briol", "authors": "Xiaoyue Xi, Fran\\c{c}ois-Xavier Briol, Mark Girolami", "title": "Bayesian Quadrature for Multiple Related Integrals", "comments": "Proceedings of the 35th International Conference on Machine Learning\n  (ICML), PMLR 80:5369-5378, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian probabilistic numerical methods are a set of tools providing\nposterior distributions on the output of numerical methods. The use of these\nmethods is usually motivated by the fact that they can represent our\nuncertainty due to incomplete/finite information about the continuous\nmathematical problem being approximated. In this paper, we demonstrate that\nthis paradigm can provide additional advantages, such as the possibility of\ntransferring information between several numerical methods. This allows users\nto represent uncertainty in a more faithful manner and, as a by-product,\nprovide increased numerical efficiency. We propose the first such numerical\nmethod by extending the well-known Bayesian quadrature algorithm to the case\nwhere we are interested in computing the integral of several related functions.\nWe then prove convergence rates for the method in the well-specified and\nmisspecified cases, and demonstrate its efficiency in the context of\nmulti-fidelity models for complex engineering systems and a problem of global\nillumination in computer graphics.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 12:49:32 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 11:16:00 GMT"}, {"version": "v3", "created": "Mon, 5 Mar 2018 10:53:01 GMT"}, {"version": "v4", "created": "Wed, 7 Mar 2018 15:03:05 GMT"}, {"version": "v5", "created": "Wed, 6 Jun 2018 12:48:30 GMT"}, {"version": "v6", "created": "Thu, 19 Jul 2018 10:29:51 GMT"}, {"version": "v7", "created": "Mon, 30 Jul 2018 18:44:13 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Xi", "Xiaoyue", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Girolami", "Mark", ""]]}, {"id": "1801.05186", "submitter": "Emanuele Borgonovo Prof.", "authors": "Emanuele Borgonovo, Max D. Morris and Elmar Plischke", "title": "Functional ANOVA with Multiple Distributions: Implications for the\n  Sensitivity Analysis of Computer Experiments", "comments": "To Appear on SIAM/ASA Journal on Uncertainty Quantification 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional ANOVA expansion of a multivariate mapping plays a fundamental\nrole in statistics. The expansion is unique once a unique distribution is\nassigned to the covariates. Recent investigations in the environmental and\nclimate sciences show that analysts may not be in a position to assign a unique\ndistribution in realistic applications. We offer a systematic investigation of\nexistence, uniqueness, orthogonality, monotonicity and ultramodularity of the\nfunctional ANOVA expansion of a multivariate mapping when a multiplicity of\ndistributions is assigned to the covariates. In particular, we show that a\nmultivariate mapping can be associated with a core of probability measures that\nguarantee uniqueness. We obtain new results for variance decomposition and\ndimension distribution under mixtures. Implications for the global sensitivity\nanalysis of computer experiments are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 09:56:47 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Borgonovo", "Emanuele", ""], ["Morris", "Max D.", ""], ["Plischke", "Elmar", ""]]}, {"id": "1801.05305", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, Sukjin Han and Amanda\n  Kowalski", "title": "Censored Quantile Instrumental Variable Estimation with Stata", "comments": "12 pages, 1 table, associated software can be found at\n  https://ideas.repec.org/c/boc/bocode/s457478.html. arXiv admin note: text\n  overlap with arXiv:1104.4580. We have updated the command to report standard\n  errors and bootstrap percentile-t confidence intervals", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications involve a censored dependent variable and an endogenous\nindependent variable. Chernozhukov et al. (2015) introduced a censored quantile\ninstrumental variable estimator (CQIV) for use in those applications, which has\nbeen applied by Kowalski (2016), among others. In this article, we introduce a\nStata command, cqiv, that simplifes application of the CQIV estimator in Stata.\nWe summarize the CQIV estimator and algorithm, we describe the use of the cqiv\ncommand, and we provide empirical examples.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 14:47:37 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 22:37:23 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 22:21:27 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Han", "Sukjin", ""], ["Kowalski", "Amanda", ""]]}, {"id": "1801.05661", "submitter": "Radoslav Harman", "authors": "Radoslav Harman, Lenka Filov\\'a, Peter Richt\\'arik", "title": "A Randomized Exchange Algorithm for Computing Optimal Approximate\n  Designs of Experiments", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of subspace ascent methods for computing optimal\napproximate designs that covers both existing as well as new and more efficient\nalgorithms. Within this class of methods, we construct a simple, randomized\nexchange algorithm (REX). Numerical comparisons suggest that the performance of\nREX is comparable or superior to the performance of state-of-the-art methods\nacross a broad range of problem structures and sizes. We focus on the most\ncommonly used criterion of D-optimality that also has applications beyond\nexperimental design, such as the construction of the minimum volume ellipsoid\ncontaining a given set of data-points. For D-optimality, we prove that the\nproposed algorithm converges to the optimum. We also provide formulas for the\noptimal exchange of weights in the case of the criterion of A-optimality. These\nformulas enable one to use REX for computing A-optimal and I-optimal designs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 13:49:47 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Harman", "Radoslav", ""], ["Filov\u00e1", "Lenka", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1801.05832", "submitter": "Renato J Cintra", "authors": "D. F. G. Coelho, R. J. Cintra, V. S. Dimitrov", "title": "Efficient Computation of the 8-point DCT via Summation by Parts", "comments": "Fixed Fig. 1 with the block diagram of the proposed architecture.\n  Manuscript contains 13 pages, 4 figures, 2 tables", "journal-ref": "J Sign Process Syst (2017)", "doi": "10.1007/s11265-017-1270-6", "report-no": null, "categories": "cs.DS cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new fast algorithm for the 8-point discrete cosine\ntransform (DCT) based on the summation-by-parts formula. The proposed method\nconverts the DCT matrix into an alternative transformation matrix that can be\ndecomposed into sparse matrices of low multiplicative complexity. The method is\ncapable of scaled and exact DCT computation and its associated fast algorithm\nachieves the theoretical minimal multiplicative complexity for the 8-point DCT.\nDepending on the nature of the input signal simplifications can be introduced\nand the overall complexity of the proposed algorithm can be further reduced.\nSeveral types of input signal are analyzed: arbitrary, null mean, accumulated,\nand null mean/accumulated signal. The proposed tool has potential application\nin harmonic detection, image enhancement, and feature extraction, where input\nsignal DC level is discarded and/or the signal is required to be integrated.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 19:21:15 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:43:07 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Coelho", "D. F. G.", ""], ["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1801.05935", "submitter": "Koulik Khamaru", "authors": "Koulik Khamaru, Rahul Mazumder", "title": "Computation of the Maximum Likelihood estimator in low-rank Factor\n  Analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor analysis, a classical multivariate statistical technique is popularly\nused as a fundamental tool for dimensionality reduction in statistics,\neconometrics and data science. Estimation is often carried out via the Maximum\nLikelihood (ML) principle, which seeks to maximize the likelihood under the\nassumption that the positive definite covariance matrix can be decomposed as\nthe sum of a low rank positive semidefinite matrix and a diagonal matrix with\nnonnegative entries. This leads to a challenging rank constrained nonconvex\noptimization problem. We reformulate the low rank ML Factor Analysis problem as\na nonlinear nonsmooth semidefinite optimization problem, study various\nstructural properties of this reformulation and propose fast and scalable\nalgorithms based on difference of convex (DC) optimization. Our approach has\ncomputational guarantees, gracefully scales to large problems, is applicable to\nsituations where the sample covariance matrix is rank deficient and adapts to\nvariants of the ML problem with additional constraints on the problem\nparameters. Our numerical experiments demonstrate the significant usefulness of\nour approach over existing state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 04:50:42 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Khamaru", "Koulik", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1801.06296", "submitter": "Rico Krueger", "authors": "Rico Krueger, Akshay Vij, Taha H. Rashidi", "title": "A Dirichlet Process Mixture Model of Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mixed multinomial logit (MNL) model, which leverages the\ntruncated stick-breaking process representation of the Dirichlet process as a\nflexible nonparametric mixing distribution. The proposed model is a Dirichlet\nprocess mixture model and accommodates discrete representations of\nheterogeneity, like a latent class MNL model. Yet, unlike a latent class MNL\nmodel, the proposed discrete choice model does not require the analyst to fix\nthe number of mixture components prior to estimation, as the complexity of the\ndiscrete mixing distribution is inferred from the evidence. For posterior\ninference in the proposed Dirichlet process mixture model of discrete choice,\nwe derive an expectation maximisation algorithm. In a simulation study, we\ndemonstrate that the proposed model framework can flexibly capture\ndifferently-shaped taste parameter distributions. Furthermore, we empirically\nvalidate the model framework in a case study on motorists' route choice\npreferences and find that the proposed Dirichlet process mixture model of\ndiscrete choice outperforms a latent class MNL model and mixed MNL models with\ncommon parametric mixing distributions in terms of both in-sample fit and\nout-of-sample predictive ability. Compared to extant modelling approaches, the\nproposed discrete choice model substantially abbreviates specification\nsearches, as it relies on less restrictive parametric assumptions and does not\nrequire the analyst to specify the complexity of the discrete mixing\ndistribution prior to estimation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:12:16 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Krueger", "Rico", ""], ["Vij", "Akshay", ""], ["Rashidi", "Taha H.", ""]]}, {"id": "1801.06768", "submitter": "Xun Huan", "authors": "Khachik Sargsyan, Xun Huan, Habib N. Najm", "title": "Embedded Model Error Representation for Bayesian Model Calibration", "comments": "Preprint 34 pages, 13 figures; v1 submitted on January 19, 2018; v2\n  submitted on February 5, 2019. v2 changes: addition of various clarifications\n  and references, and minor language edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.comp-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model error estimation remains one of the key challenges in uncertainty\nquantification and predictive science. For computational models of complex\nphysical systems, model error, also known as structural error or model\ninadequacy, is often the largest contributor to the overall predictive\nuncertainty. This work builds on a recently developed framework of embedded,\ninternal model correction, in order to represent and quantify structural\nerrors, together with model parameters, within a Bayesian inference context. We\nfocus specifically on a Polynomial Chaos representation with additive\nmodification of existing model parameters, enabling a non-intrusive procedure\nfor efficient approximate likelihood construction, model error estimation, and\ndisambiguation of model and data errors' contributions to predictive\nuncertainty. The framework is demonstrated on several synthetic examples, as\nwell as on a chemical ignition problem.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 04:57:24 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 17:18:26 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Sargsyan", "Khachik", ""], ["Huan", "Xun", ""], ["Najm", "Habib N.", ""]]}, {"id": "1801.07000", "submitter": "Anna Wigren", "authors": "Anna Wigren, Lawrence Murray and Fredrik Lindsten", "title": "Improving the particle filter in high dimensions using conjugate\n  artificial process noise", "comments": null, "journal-ref": null, "doi": "10.1016/j.ifacol.2018.09.207", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The particle filter is one of the most successful methods for state inference\nand identification of general non-linear and non-Gaussian models. However,\nstandard particle filters suffer from degeneracy of the particle weights, in\nparticular for high-dimensional problems. We propose a method for improving the\nperformance of the particle filter for certain challenging state space models,\nwith implications for high-dimensional inference. First we approximate the\nmodel by adding artificial process noise in an additional state update, then we\ndesign a proposal that combines the standard and the locally optimal proposal.\nThis results in a bias-variance trade-off, where adding more noise reduces the\nvariance of the estimate but increases the model bias. The performance of the\nproposed method is empirically evaluated on a linear-Gaussian state space model\nand on the non-linear Lorenz'96 model. For both models we observe a significant\nimprovement in performance over the standard particle filter.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 09:03:44 GMT"}, {"version": "v2", "created": "Thu, 22 Nov 2018 16:41:49 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Wigren", "Anna", ""], ["Murray", "Lawrence", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1801.07165", "submitter": "Bochao Jia", "authors": "Bochao Jia", "title": "The application of Monte Carlo methods for learning generalized linear\n  model", "comments": null, "journal-ref": "Biom Biostat Int J. 2018; 7(5): 422-427", "doi": "10.15406/bbij.2018.07.00241", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo method is a broad class of computational algorithms that rely on\nrepeated random sampling to obtain numerical results. They are often used in\nphysical and mathematical problems and are most useful when it is difficult or\nimpossible to use other mathematical methods. Basically, many statisticians\nhave been increasingly drawn to Monte Carlo method in three distinct problem\nclasses: optimization, numerical integration, and generating draws from a\nprobability distribution. In this paper, we will introduce the Monte Carlo\nmethod for calculating coefficients in Generalized Linear Model(GLM),\nespecially for Logistic Regression. Our main methods are Metropolis\nHastings(MH) Algorithms and Stochastic Approximation in Monte Carlo\nComputation(SAMC). For comparison, we also get results automatically using MLE\nmethod in R software.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 22:51:12 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 19:55:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Jia", "Bochao", ""]]}, {"id": "1801.07873", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, David J. Nott, Robert Kohn", "title": "Gaussian variational approximation for high-dimensional state space\n  models", "comments": "Significantly revised, especially the multivariate stochastic\n  volatility model example", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article considers a Gaussian variational approximation of the posterior\ndensity in a high-dimensional state space model. The variational parameters to\nbe optimized are the mean vector and the covariance matrix of the\napproximation. The number of parameters in the covariance matrix grows as the\nsquare of the number of model parameters, so it is necessary to find simple yet\neffective parameterizations of the covariance structure when the number of\nmodel parameters is large. We approximate the joint posterior distribution over\nthe high-dimensional state vectors by a dynamic factor model, having Markovian\ntime dependence and a factor covariance structure for the states. This gives a\nreduced description of the dependence structure for the states, as well as a\ntemporal conditional independence structure similar to that in the true\nposterior. The usefulness of the approach is illustrated for prediction in two\nhigh-dimensional applications that are challenging for Markov chain Monte Carlo\nsampling. The first is a spatio-temporal model for the spread of the Eurasian\nCollared-Dove across North America; the second is a Wishart-based multivariate\nstochastic volatility model for financial returns.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 06:30:46 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 05:05:24 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 01:28:06 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Quiroz", "Matias", ""], ["Nott", "David J.", ""], ["Kohn", "Robert", ""]]}, {"id": "1801.08002", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich, Frank Konietschke, Markus Pauly", "title": "Analysis of Multivariate Data and Repeated Measures Designs with the R\n  Package MANOVA.RM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical availability of statistical inference methods for a modern and\nrobust analysis of longitudinal- and multivariate data in factorial experiments\nis an essential element in research and education. While existing approaches\nthat rely on specific distributional assumptions of the data (multivariate\nnormality and/or characteristic covariance matrices) are implemented in\nstatistical software packages, there is a need for user-friendly software that\ncan be used for the analysis of data that do not fulfill the aforementioned\nassumptions and provide accurate p-value and confidence interval estimates.\nTherefore, newly developed statistical methods for the analysis of repeated\nmeasures designs and multivariate data that neither assume multivariate\nnormality nor specific covariance matrices have been implemented in the freely\navailable R-package MANOVA.RM. The package is equipped with a graphical user\ninterface for plausible applications in academia and other educational purpose.\nSeveral motivating examples illustrate the application of the methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 14:37:53 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Friedrich", "Sarah", ""], ["Konietschke", "Frank", ""], ["Pauly", "Markus", ""]]}, {"id": "1801.08227", "submitter": "Haolei Weng", "authors": "Rahul Mazumder, Diego F. Saldana, Haolei Weng", "title": "Matrix Completion with Nonconvex Regularization: Spectral Operators and\n  Scalable Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the popularly dubbed matrix completion problem, where\nthe task is to \"fill in\" the unobserved entries of a matrix from a small subset\nof observed entries, under the assumption that the underlying matrix is of\nlow-rank. Our contributions herein, enhance our prior work on nuclear norm\nregularized problems for matrix completion (Mazumder et al., 2010) by\nincorporating a continuum of nonconvex penalty functions between the convex\nnuclear norm and nonconvex rank functions. Inspired by SOFT-IMPUTE (Mazumder et\nal., 2010; Hastie et al., 2016), we propose NC-IMPUTE- an EM-flavored\nalgorithmic framework for computing a family of nonconvex penalized matrix\ncompletion problems with warm-starts. We present a systematic study of the\nassociated spectral thresholding operators, which play an important role in the\noverall algorithm. We study convergence properties of the algorithm. Using\nstructured low-rank SVD computations, we demonstrate the computational\nscalability of our proposal for problems up to the Netflix size (approximately,\na $500,000 \\times 20, 000$ matrix with $10^8$ observed entries). We demonstrate\nthat on a wide range of synthetic and real data instances, our proposed\nnonconvex regularization framework leads to low-rank solutions with better\npredictive performance when compared to those obtained from nuclear norm\nproblems. Implementations of algorithms proposed herein, written in the R\nprogramming language, are made available on github.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 22:42:36 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 18:07:47 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Mazumder", "Rahul", ""], ["Saldana", "Diego F.", ""], ["Weng", "Haolei", ""]]}, {"id": "1801.09064", "submitter": "Rodrigo Martinez", "authors": "Miguel Gonz\\'alez, Rodrigo Mart\\'inez and Cristina Guti\\'errez", "title": "Bayesian inference in Y-linked two-sex branching processes with\n  mutations: ABC approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Y-linked two-sex branching process with mutations and blind choice of males\nis a suitable model for analyzing the evolution of the number of carriers of an\nallele and its mutations of a Y-linked gene. Considering a two-sex monogamous\npopulation, in this model each female chooses her partner from among the male\npopulation without caring about his type (i.e., the allele he carries). In this\nwork, we deal with the problem of estimating the main parameters of such model\ndeveloping the Bayesian inference in a parametric framework. Firstly, we\nconsider, as sample scheme, the observation of the total number of females and\nmales up to some generation as well as the number of males of each genotype at\nlast generation. Later, we introduce the information of the mutated males only\nin the last generation obtaining in this way a second sample scheme. For both\nsamples, we apply the Approximate Bayesian Computation (ABC) methodology to\napproximate the posterior distributions of the main parameters of this model.\nThe accuracy of the procedure based on these samples is illustrated and\ndiscussed by way of simulated examples.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 10:05:50 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Gonz\u00e1lez", "Miguel", ""], ["Mart\u00ednez", "Rodrigo", ""], ["Guti\u00e9rrez", "Cristina", ""]]}, {"id": "1801.09065", "submitter": "Luca Martino", "authors": "Luca Martino", "title": "A Review of Multiple Try MCMC algorithms for Signal Processing", "comments": "Digital Signal Processing, 2018", "journal-ref": null, "doi": "10.1016/j.dsp.2018.01.004", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in signal processing require the estimation of some\nparameters of interest given a set of observed data. More specifically,\nBayesian inference needs the computation of {\\it a-posteriori} estimators which\nare often expressed as complicated multi-dimensional integrals. Unfortunately,\nanalytical expressions for these estimators cannot be found in most real-world\napplications, and Monte Carlo methods are the only feasible approach. A very\npowerful class of Monte Carlo techniques is formed by the Markov Chain Monte\nCarlo (MCMC) algorithms. They generate a Markov chain such that its stationary\ndistribution coincides with the target posterior density. In this work, we\nperform a thorough review of MCMC methods using multiple candidates in order to\nselect the next state of the chain, at each iteration. With respect to the\nclassical Metropolis-Hastings method, the use of multiple try techniques foster\nthe exploration of the sample space. We present different Multiple Try\nMetropolis schemes, Ensemble MCMC methods, Particle Metropolis-Hastings\nalgorithms and the Delayed Rejection Metropolis technique. We highlight\nlimitations, benefits, connections and differences among the different methods,\nand compare them by numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 10:27:19 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Martino", "Luca", ""]]}, {"id": "1801.09124", "submitter": "Lenka Filova", "authors": "Lenka Filov\\'a, Radoslav Harman", "title": "Ascent with Quadratic Assistance for the Construction of Exact\n  Experimental Designs", "comments": "Substantially reworked version to emphasize the advantages of the\n  proposed approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of statistical planning, there is a large body of theoretical\nknowledge and computational experience concerning so-called optimal approximate\ndesigns of experiments. However, for an approximate design to be executed in\npractice, it must be converted into an exact, i.e., integer, design, which is\nusually done via rounding procedures. Although rapid, rounding procedures have\nmany drawbacks; in particular, they often yield worse exact designs than\nheuristics that do not require approximate designs at all.\n  In this paper, we build on an alternative principle of utilizing optimal\napproximate designs for the computation of optimal, or nearly-optimal, exact\ndesigns. The principle, which we call ascent with quadratic assistance (AQuA),\nis an integer programming method based on the quadratic approximation of the\ndesign criterion in the neighborhood of the optimal approximate information\nmatrix.\n  To this end, we present quadratic approximations of all Kiefer's criteria\nwith an integer parameter, including D- and A-optimality and, by a model\ntransformation, I-optimality. Importantly, we prove a low-rank property of the\nassociated quadratic forms, which enables us to apply AQuA to large design\nspaces, for example via mixed integer conic quadratic solvers. We numerically\ndemonstrate the robustness and superior performance of the proposed method for\nmodels under various types of constraints. More precisely, we compute optimal\nsize-constrained exact designs for the model of spring-balance weighing, and\noptimal symmetric marginally restricted exact designs for the Scheffe mixture\nmodel. We also show how can iterative application of AQuA be used for a\nstratified information-based subsampling of large datasets under a lower bound\non the quality and an upper bound on the cost of the subsample.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 18:57:19 GMT"}, {"version": "v2", "created": "Sun, 15 Jul 2018 19:41:22 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2019 11:56:39 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Filov\u00e1", "Lenka", ""], ["Harman", "Radoslav", ""]]}, {"id": "1801.09141", "submitter": "Alberto Sorrentino", "authors": "Federica Sciacchitano and Alberto Sorrentino and A Gordon Emslie and\n  Anna Maria Massone and Michele Piana", "title": "Identification of multiple hard X-ray sources in solar flares: A\n  Bayesian analysis of the February 20 2002 event", "comments": "accepted in ApJ", "journal-ref": null, "doi": "10.3847/1538-4357/aacc27", "report-no": null, "categories": "astro-ph.SR astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hard X-ray emission in a solar flare is typically characterized by a\nnumber of discrete sources, each with its own spectral, temporal, and spatial\nvariability. Establishing the relationship amongst these sources is critical to\ndetermine the role of each in the energy release and transport processes that\noccur within the flare. In this paper we present a novel method to identify and\ncharacterize each source of hard X-ray emission. The method permits a\nquantitative determination of the most likely number of subsources present, and\nof the relative probabilities that the hard X-ray emission in a given subregion\nof the flare is represented by a complicated multiple source structure or by a\nsimpler single source. We apply the method to a well-studied flare on\n2002~February~20 in order to assess competing claims as to the number of\nchromospheric footpoint sources present, and hence to the complexity of the\nunderlying magnetic geometry/toplogy. Contrary to previous claims of the need\nfor multiple sources to account for the chromospheric hard X-ray emission at\ndifferent locations and times, we find that a simple\ntwo-footpoint-plus-coronal-source model is the most probable explanation for\nthe data. We also find that one of the footpoint sources moves quite rapidly\nthroughout the event, a factor that presumably complicated previous analyses.\nThe inferred velocity of the footpoint corresponds to a very high induced\nelectric field, compatible with those in thin reconnecting current sheets.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 21:23:16 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 15:02:36 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Sciacchitano", "Federica", ""], ["Sorrentino", "Alberto", ""], ["Emslie", "A Gordon", ""], ["Massone", "Anna Maria", ""], ["Piana", "Michele", ""]]}, {"id": "1801.09299", "submitter": "Cyril Chimisov", "authors": "Cyril Chimisov, Krzysztof Latuszynski, Gareth Roberts", "title": "Adapting The Gibbs Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The popularity of Adaptive MCMC has been fueled on the one hand by its\nsuccess in applications, and on the other hand, by mathematically appealing and\ncomputationally straightforward optimisation criteria for the Metropolis\nalgorithm acceptance rate (and, equivalently, proposal scale). Similarly\nprincipled and operational criteria for optimising the selection probabilities\nof the Random Scan Gibbs Sampler have not been devised to date.\n  In the present work, we close this gap and develop a general purpose Adaptive\nRandom Scan Gibbs Sampler that adapts the selection probabilities. The\nadaptation is guided by optimising the $L_2-$spectral gap for the target's\nGaussian analogue, gradually, as target's global covariance is learned by the\nsampler. The additional computational cost of the adaptation represents a small\nfraction of the total simulation effort. ` We present a number of moderately-\nand high-dimensional examples, including truncated Gaussians, Bayesian\nHierarchical Models and Hidden Markov Models, where significant computational\ngains are empirically observed for both, Adaptive Gibbs, and Adaptive\nMetropolis within Adaptive Gibbs version of the algorithm. We argue that\nAdaptive Random Scan Gibbs Samplers can be routinely implemented and\nsubstantial computational gains will be observed across many typical Gibbs\nsampling problems.\n  We shall give conditions under which ergodicity of the adaptive algorithms\ncan be established.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 21:54:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chimisov", "Cyril", ""], ["Latuszynski", "Krzysztof", ""], ["Roberts", "Gareth", ""]]}, {"id": "1801.09309", "submitter": "Cyril Chimisov", "authors": "Cyril Chimisov, Krzysztof Latuszynski, Gareth Roberts", "title": "Air Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of Adapted Increasingly Rarely Markov Chain Monte Carlo\n(AirMCMC) algorithms where the underlying Markov kernel is allowed to be\nchanged based on the whole available chain output but only at specific time\npoints separated by an increasing number of iterations. The main motivation is\nthe ease of analysis of such algorithms. Under the assumption of either\nsimultaneous or (weaker) local simultaneous geometric drift condition, or\nsimultaneous polynomial drift we prove the $L_2-$convergence, Weak and Strong\nLaws of Large Numbers (WLLN, SLLN), Central Limit Theorem (CLT), and discuss\nhow our approach extends the existing results. We argue that many of the known\nAdaptive MCMC algorithms may be transformed into the corresponding Air\nversions, and provide an empirical evidence that performance of the Air version\nstays virtually the same.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 22:20:31 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Chimisov", "Cyril", ""], ["Latuszynski", "Krzysztof", ""], ["Roberts", "Gareth", ""]]}, {"id": "1801.09661", "submitter": "Jared Huling", "authors": "Jared D. Huling and Peter Z. G. Qian", "title": "Fast Penalized Regression and Cross Validation for Tall Data with the\n  oem Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of research has focused on theory and computation for variable\nselection techniques for high dimensional data. There has been substantially\nless work in the big tall data paradigm, where the number of variables may be\nlarge, but the number of observations is much larger. The orthogonalizing\nexpectation maximization (OEM) algorithm is one approach for computation of\npenalized models which excels in the big tall data regime. The oem package is\nan efficient implementation of the OEM algorithm which provides a multitude of\ncomputation routines with a focus on big tall data, such as a function for\nout-of-memory computation, for large-scale parallel computation of penalized\nregression models. Furthermore, in this paper we propose a specialized\nimplementation of the OEM algorithm for cross validation, dramatically reducing\nthe computing time for cross validation over a naive implementation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 18:36:44 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Huling", "Jared D.", ""], ["Qian", "Peter Z. G.", ""]]}, {"id": "1801.09664", "submitter": "I\\~naki Ucar", "authors": "I\\~naki Ucar, Jos\\'e Alberto Hern\\'andez, Pablo Serrano, Arturo\n  Azcorra", "title": "Design and Analysis of 5G Scenarios with 'simmer': An R Package for Fast\n  DES Prototyping", "comments": "7 pages, 4 figures", "journal-ref": "IEEE Communications Magazine, vol. 56, no. 11, pp. 145-151,\n  November 2018", "doi": "10.1109/MCOM.2018.1700960", "report-no": null, "categories": "cs.NI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation frameworks are important tools for the analysis and design of\ncommunication networks and protocols, but they can result extremely costly\nand/or complex (for the case of very specialized tools), or too naive and\nlacking proper features and support (for the case of ad-hoc tools). In this\npaper, we present an analysis of three 5G scenarios using 'simmer', a recent R\npackage for discrete-event simulation that sits between the above two\nparadigms. As our results show, it provides a simple yet very powerful syntax,\nsupporting the efficient simulation of relatively complex scenarios at a low\nimplementation cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 18:42:04 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Ucar", "I\u00f1aki", ""], ["Hern\u00e1ndez", "Jos\u00e9 Alberto", ""], ["Serrano", "Pablo", ""], ["Azcorra", "Arturo", ""]]}, {"id": "1801.09739", "submitter": "Thomas Nagler", "authors": "Thomas Nagler, Christian Bumann, Claudia Czado", "title": "Model selection in sparse high-dimensional vine copula models with\n  application to portfolio risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas allow to build flexible dependence models for an arbitrary\nnumber of variables using only bivariate building blocks. The number of\nparameters in a vine copula model increases quadratically with the dimension,\nwhich poses new challenges in high-dimensional applications. To alleviate the\ncomputational burden and risk of overfitting, we propose a modified Bayesian\ninformation criterion (BIC) tailored to sparse vine copula models. We show that\nthe criterion can consistently distinguish between the true and alternative\nmodels under less stringent conditions than the classical BIC. The new\ncriterion can be used to select the hyper-parameters of sparse model classes,\nsuch as truncated and thresholded vine copulas. We propose a computationally\nefficient implementation and illustrate the benefits of the new concepts with a\ncase study where we model the dependence in a large stock stock portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 20:10:38 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 17:17:38 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 11:37:46 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Nagler", "Thomas", ""], ["Bumann", "Christian", ""], ["Czado", "Claudia", ""]]}]