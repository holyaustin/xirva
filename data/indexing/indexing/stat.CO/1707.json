[{"id": "1707.00487", "submitter": "Perttu Luukko", "authors": "P. J. J. Luukko, J. Helske and E. R\\\"as\\\"anen", "title": "Introducing libeemd: A program package for performing the ensemble\n  empirical mode decomposition", "comments": "The final publication is available at Springer via\n  https://dx.doi.org/10.1007/s00180-015-0603-9", "journal-ref": "Comput. Stat. 31 545 (2016)", "doi": "10.1007/s00180-015-0603-9", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble empirical mode decomposition (EEMD) and its complete variant\n(CEEMDAN) are adaptive, noise-assisted data analysis methods that improve on\nthe ordinary empirical mode decomposition (EMD). All these methods decompose\npossibly nonlinear and/or nonstationary time series data into a finite amount\nof components separated by instantaneous frequencies. This decomposition\nprovides a powerful method to look into the different processes behind a given\ntime series data, and provides a way to separate short time-scale events from a\ngeneral trend. We present a free software implementation of EMD, EEMD and\nCEEMDAN and give an overview of the EMD methodology and the algorithms used in\nthe decomposition. We release our implementation, libeemd, with the aim of\nproviding a user-friendly, fast, stable, well-documented and easily extensible\nEEMD library for anyone interested in using (E)EMD in the analysis of time\nseries data. While written in C for numerical efficiency, our implementation\nincludes interfaces to the Python and R languages, and interfaces to other\nlanguages are straightforward.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 11:39:48 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Luukko", "P. J. J.", ""], ["Helske", "J.", ""], ["R\u00e4s\u00e4nen", "E.", ""]]}, {"id": "1707.00558", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Bhargav Srinivasa Desikan", "title": "Pycobra: A Python Toolbox for Ensemble Learning and Visualisation", "comments": null, "journal-ref": "Journal of Machine Learning Research (JMLR), 18(190):1--5, 2018", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce \\texttt{pycobra}, a Python library devoted to ensemble learning\n(regression and classification) and visualisation. Its main assets are the\nimplementation of several ensemble learning algorithms, a flexible and generic\ninterface to compare and blend any existing machine learning algorithm\navailable in Python libraries (as long as a \\texttt{predict} method is given),\nand visualisation tools such as Voronoi tessellations. \\texttt{pycobra} is\nfully \\texttt{scikit-learn} compatible and is released under the MIT\nopen-source license. \\texttt{pycobra} can be downloaded from the Python Package\nIndex (PyPi) and Machine Learning Open Source Software (MLOSS). The current\nversion (along with Jupyter notebooks, extensive documentation, and continuous\nintegration tests) is available at\n\\href{https://github.com/bhargavvader/pycobra}{https://github.com/bhargavvader/pycobra}\nand official documentation website is\n\\href{https://modal.lille.inria.fr/pycobra}{https://modal.lille.inria.fr/pycobra}.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 14:05:34 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 10:24:54 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 06:04:41 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Guedj", "Benjamin", ""], ["Desikan", "Bhargav Srinivasa", ""]]}, {"id": "1707.00807", "submitter": "Sotirios Sabanis", "authors": "Raj Kumari Bahl and Sotirios Sabanis", "title": "General Price Bounds for Guaranteed Annuity Options", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the valuation of Guaranteed Annuity\nOptions (GAOs) under the most generalised modelling framework where both\ninterest and mortality rates are stochastic and correlated. Pricing these type\nof options in the correlated environment is a challenging task and no closed\nform solution exists in the literature. We employ the use of doubly stochastic\nstopping times to incorporate the randomness about the time of death and employ\na suitable change of measure to facilitate the valuation of survival benefit,\nthere by adapting the payoff of the GAO in terms of the payoff of a basket call\noption. We derive general price bounds for GAOs by utilizing a conditioning\napproach for the lower bound and arithmetic-geometric mean inequality for the\nupper bound. The theory is then applied to affine models to present some very\ninteresting formulae for the bounds under the affine set up. Numerical examples\nare furnished and benchmarked against Monte Carlo simulations to estimate the\nprice of a GAO for a variety of affine processes governing the evolution of\nmortality and the interest rate.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 03:05:07 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Bahl", "Raj Kumari", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "1707.00892", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Jonathan Rougier", "title": "A sparse linear algebra algorithm for fast computation of prediction\n  variances with Gaussian Markov random fields", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields are used in a large number of disciplines in\nmachine vision and spatial statistics. The models take advantage of sparsity in\nmatrices introduced through the Markov assumptions, and all operations in\ninference and prediction use sparse linear algebra operations that scale well\nwith dimensionality. Yet, for very high-dimensional models, exact computation\nof predictive variances of linear combinations of variables is generally\ncomputationally prohibitive, and approximate methods (generally interpolation\nor conditional simulation) are typically used instead. A set of conditions are\nestablished under which the variances of linear combinations of random\nvariables can be computed exactly using the Takahashi recursions. The ensuing\ncomputational simplification has wide applicability and may be used to enhance\nseveral software packages where model fitting is seated in a maximum-likelihood\nframework. The resulting algorithm is ideal for use in a variety of spatial\nstatistical applications, including \\emph{LatticeKrig} modelling, statistical\ndownscaling, and fixed rank kriging. It can compute hundreds of thousands exact\npredictive variances of linear combinations on a standard desktop with ease,\neven when large spatial GMRF models are used.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:15:01 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 13:09:54 GMT"}, {"version": "v3", "created": "Tue, 6 Feb 2018 22:19:36 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Rougier", "Jonathan", ""]]}, {"id": "1707.01660", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "Tomasz C\\k{a}ka{\\l}a, B{\\l}a\\.zej Miasojedow, Wojciech Niemiro", "title": "Particle MCMC with Poisson Resampling: Parallelization and Continuous\n  Time Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new version of particle filter in which the number of\n\"children\" of a particle at a given time has a Poisson distribution. As a\nresult, the number of particles is random and varies with time. An advantage of\nthis scheme is that descendants of different particles can evolve\nindependently. It makes easy to parallelize computations. Moreover, particle\nfilter with Poisson resampling is readily adapted to the case when a hidden\nprocess is a continuous time, piecewise deterministic semi-Markov process. We\nshow that the basic techniques of particle MCMC, namely particle independent\nMetropolis-Hastings, particle Gibbs Sampler and its version with ancestor\nsampling, work under our Poisson resampling scheme. Our version of particle\nGibbs Sampler is uniformly ergodic under the same assumptions as its standard\ncounterpart. We present simulation results which indicate that our algorithms\ncan compete with the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 07:17:55 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 15:44:31 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["C\u0105ka\u0142a", "Tomasz", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "1707.01815", "submitter": "Amrei Stammann", "authors": "Amrei Stammann (Heinrich-Heine University Duesseldorf)", "title": "Fast and Feasible Estimation of Generalized Linear Models with\n  High-Dimensional k-way Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and memory efficient algorithm for the estimation of\ngeneralized linear models with an additive separable k-way error component. The\nbrute force approach uses dummy variables to account for the unobserved\nheterogeneity, but quickly faces computational limits. Thus, we show how a\nweighted version of the Frisch-Waugh-Lovell theorem combined with the method of\nalternating projections can be incorporated into a Newton-Raphson algorithm to\ndramatically reduce the computational costs. The algorithm is especially useful\nin situations, where generalized linear models with k-way fixed effects based\non dummy variables are computationally demanding or even infeasible due to time\nor memory limitations. In a simulation study and an empirical application we\ndemonstrate the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:36:18 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 19:45:08 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 13:04:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Stammann", "Amrei", "", "Heinrich-Heine University Duesseldorf"]]}, {"id": "1707.01845", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber, Nicolas Chopin and Nick Whiteley", "title": "Negative association, ordering and convergence of resampling methods", "comments": "54 pages, including 30 pages of supplementary materials (a typo in\n  Algorithm 1 has been corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convergence and convergence rates for resampling schemes. Our first\nmain result is a general consistency theorem based on the notion of negative\nassociation, which is applied to establish the almost-sure weak convergence of\nmeasures output from Kitagawa's (1996) stratified resampling method. Carpenter\net al's (1999) systematic resampling method is similar in structure but can\nfail to converge depending on the order of the input samples. We introduce a\nnew resampling algorithm based on a stochastic rounding technique of Srinivasan\n(2001), which shares some attractive properties of systematic resampling, but\nwhich exhibits negative association and therefore converges irrespective of the\norder of the input samples. We confirm a conjecture made by Kitagawa (1996)\nthat ordering input samples by their states in $\\mathbb{R}$ yields a faster\nrate of convergence; we establish that when particles are ordered using the\nHilbert curve in $\\mathbb{R}^d$, the variance of the resampling error is\n${\\scriptscriptstyle\\mathcal{O}}(N^{-(1+1/d)})$ under mild conditions, where\n$N$ is the number of particles. We use these results to establish asymptotic\nproperties of particle algorithms based on resampling schemes that differ from\nmultinomial resampling.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 15:58:39 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 09:24:05 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2020 15:52:51 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Gerber", "Mathieu", ""], ["Chopin", "Nicolas", ""], ["Whiteley", "Nick", ""]]}, {"id": "1707.02129", "submitter": "Clara Happ", "authors": "Clara Happ", "title": "Object-Oriented Software for Functional Data", "comments": null, "journal-ref": null, "doi": "10.18637/jss.v093.i05", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the funData R package as an object-oriented\nimplementation of functional data. It implements a unified framework for dense\nunivariate and multivariate functional data on one- and higher dimensional\ndomains as well as for irregular functional data. The aim of this package is to\nprovide a user-friendly, self-contained core toolbox for functional data,\nincluding important functionalities for creating, accessing and modifying\nfunctional data objects, that can serve as a basis for other packages. The\npackage further contains a full simulation toolbox, which is a useful feature\nwhen implementing and testing new methodological developments.\n  Based on the theory of object-oriented data analysis, it is shown why it is\nnatural to implement functional data in an object-oriented manner. The classes\nand methods provided by funData are illustrated in many examples using two\nfreely available datasets. The MFPCA package, which implements multivariate\nfunctional principal component analysis, is presented as an example for an\nadvanced methodological package that uses the funData package as a basis,\nincluding a case study with real data. Both packages are publicly available on\nGitHub and CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:46:31 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 14:00:06 GMT"}, {"version": "v3", "created": "Wed, 6 Jun 2018 08:23:27 GMT"}, {"version": "v4", "created": "Fri, 17 Aug 2018 07:55:16 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Happ", "Clara", ""]]}, {"id": "1707.02294", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Raghav Somani and Sreangsu Acharyya", "title": "A case study of Empirical Bayes in User-Movie Recommendation system", "comments": "14 pages, 3 figures, 4 subfigures", "journal-ref": null, "doi": "10.1080/23737484.2017.1392266", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we provide a formulation of empirical bayes described by\nAtchade (2011) to tune the hyperparameters of priors used in bayesian set up of\ncollaborative filter. We implement the same in MovieLens small dataset. We see\nthat it can be used to get a good initial choice for the parameters. It can\nalso be used to guess an initial choice for hyper-parameters in grid search\nprocedure even for the datasets where MCMC oscillates around the true value or\ntakes long time to converge.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:54:55 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Somani", "Raghav", ""], ["Acharyya", "Sreangsu", ""]]}, {"id": "1707.02502", "submitter": "Daniel Gerhard", "authors": "Daniel Gerhard, Christian Ritz", "title": "Marginalization in nonlinear mixed-effects models with an application to\n  dose-response analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in hierarchical nonlinear models needs careful consideration about\ntargeting parameters that have either a conditional or population-average\ninterpretation. For the special case of mixed-effects nonlinear sigmoidal\nmodels we propose a method for the estimation of derived parameters with a\nmarginal interpretation, but also maintaining the random effect structure of\nthe nonlinear model, by using a combination of numerical quadrature and the\ndelta method, integrating over the random effect distribution conditional on\nthe estimated variance components. The difference between these marginalized\nestimates, generalized nonlinear least squares estimates, and conditional\nestimation is characterised by means of two representative case studies. The\ncase studies consist of the estimation of effective dose levels in a human\ntoxicology study, and the relative potency estimation for two herbicides in an\nagricultural field trial. Both case studies exhibit an experimental design that\nresults in data with at least one hierarchical level of between- and\nwithin-cluster variation. A user-friendly software implementation is made\navailable with the R package medrc, providing an automated framework for\nmixed-effects dose-response modelling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 23:13:34 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Gerhard", "Daniel", ""], ["Ritz", "Christian", ""]]}, {"id": "1707.02548", "submitter": "Duncan Ermini Leaf", "authors": "Duncan Ermini Leaf", "title": "Unified Method for Markov Chain Transition Model Estimation Using\n  Incomplete Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Future Elderly Model and related microsimulations are modeled as Markov\nchains. These simulations rely on longitudinal survey data to estimate their\ntransition models. The use of survey data presents several incomplete data\nproblems, including coarse and irregular spacing of interviews, data collection\nfrom subsamples, and structural changes to surveys over time. The\nExpectation-Maximization algorithm is adapted to create a method for maximum\nlikelihood estimation of Markov chain transition models using incomplete data.\nThe method is demonstrated on a simplified version of the Future Elderly Model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 09:32:52 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Leaf", "Duncan Ermini", ""]]}, {"id": "1707.02688", "submitter": "Xiu Yang", "authors": "Xiu Yang, Xiaoliang Wan and Lin Lin and Huan Lei", "title": "A General Framework for Enhancing Sparsity of Generalized Polynomial\n  Chaos Expansions", "comments": "Corrected the lemmas in the previous version using perturbation\n  theory of singular value decomposition. arXiv admin note: text overlap with\n  arXiv:1506.04344", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing has become a powerful addition to uncertainty\nquantification when only limited data is available. In this paper we provide a\ngeneral framework to enhance the sparsity of the representation of uncertainty\nin the form of generalized polynomial chaos expansion. We use alternating\ndirection method to identify new sets of random variables through iterative\nrotations such that the new representation of the uncertainty is sparser.\nConsequently, we increases both the efficiency and accuracy of the compressive\nsensing-based uncertainty quantification method. We demonstrate that the\npreviously developed iterative method to enhance the sparsity of Hermite\npolynomial expansion is a special case of this general framework. Moreover, we\nuse Legendre and Chebyshev polynomials expansions to demonstrate the\neffectiveness of this method with applications in solving stochastic partial\ndifferential equations and high-dimensional (O(100)) problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 03:48:21 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 20:12:39 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 19:44:45 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Yang", "Xiu", ""], ["Wan", "Xiaoliang", ""], ["Lin", "Lin", ""], ["Lei", "Huan", ""]]}, {"id": "1707.02695", "submitter": "Kevin Lin", "authors": "Andrew Leach and Kevin K. Lin and Matthias Morzfeld", "title": "Symmetrized importance samplers for stochastic differential equations", "comments": "Added brief discussion of Hamilton-Jacobi equation. Also made various\n  minor corrections. To appear in Communciations in Applied Mathematics and\n  Computational Science", "journal-ref": "Commun. Appl. Math. Comput. Sci. 13 (2018) 215-241", "doi": "10.2140/camcos.2018.13.215", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of importance sampling methods for stochastic differential\nequations (SDEs). A small-noise analysis is performed, and the results suggest\nthat a simple symmetrization procedure can significantly improve the\nperformance of our importance sampling schemes when the noise is not too large.\nWe demonstrate that this is indeed the case for a number of linear and\nnonlinear examples. Potential applications, e.g., data assimilation, are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 05:07:14 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 01:28:03 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Leach", "Andrew", ""], ["Lin", "Kevin K.", ""], ["Morzfeld", "Matthias", ""]]}, {"id": "1707.03307", "submitter": "Matteo Fasiolo", "authors": "M. Fasiolo, S. N. Wood, M. Zaffran, R. Nedellec and Y.Goude", "title": "Fast calibrated additive quantile regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1725521", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 14:47:43 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 10:44:08 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 17:48:43 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 10:18:31 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Fasiolo", "M.", ""], ["Wood", "S. N.", ""], ["Zaffran", "M.", ""], ["Nedellec", "R.", ""], ["Goude", "Y.", ""]]}, {"id": "1707.03494", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy, Akhilesh Gotmare, Martin Jaggi", "title": "Unsupervised robust nonparametric learning of hidden community\n  properties", "comments": "Experiments with new types of adversaries added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning of fundamental properties of communities in large noisy\nnetworks, in the prototypical situation where the nodes or users are split into\ntwo classes according to a binary property, e.g., according to their opinions\nor preferences on a topic. For learning these properties, we propose a\nnonparametric, unsupervised, and scalable graph scan procedure that is, in\naddition, robust against a class of powerful adversaries. In our setup, one of\nthe communities can fall under the influence of a knowledgeable adversarial\nleader, who knows the full network structure, has unlimited computational\nresources and can completely foresee our planned actions on the network. We\nprove strong consistency of our results in this setup with minimal assumptions.\nIn particular, the learning procedure estimates the baseline activity of normal\nusers asymptotically correctly with probability 1; the only assumption being\nthe existence of a single implicit community of asymptotically negligible\nlogarithmic size. We provide experiments on real and synthetic data to\nillustrate the performance of our method, including examples with adversaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:28:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 16:22:55 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Gotmare", "Akhilesh", ""], ["Jaggi", "Martin", ""]]}, {"id": "1707.03543", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer", "title": "Computing Entropies With Nested Sampling", "comments": "Accepted for publication in Entropy. 21 pages, 3 figures. Software\n  available at https://github.com/eggplantbren/InfoNest", "journal-ref": null, "doi": "10.3390/e19080422", "report-no": null, "categories": "stat.CO astro-ph.IM cs.IT math.IT physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Shannon entropy, and related quantities such as mutual information, can\nbe used to quantify uncertainty and relevance. However, in practice, it can be\ndifficult to compute these quantities for arbitrary probability distributions,\nparticularly if the probability mass functions or densities cannot be\nevaluated. This paper introduces a computational approach, based on Nested\nSampling, to evaluate entropies of probability distributions that can only be\nsampled. I demonstrate the method on three examples: a simple gaussian example\nwhere the key quantities are available analytically; (ii) an experimental\ndesign example about scheduling observations in order to measure the period of\nan oscillating signal; and (iii) predicting the future from the past in a\nheavy-tailed scenario.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 05:14:43 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 03:37:47 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Brewer", "Brendon J.", ""]]}, {"id": "1707.03663", "submitter": "Niladri Chatterji", "authors": "Xiang Cheng, Niladri S. Chatterji, Peter L. Bartlett and Michael I.\n  Jordan", "title": "Underdamped Langevin MCMC: A non-asymptotic analysis", "comments": "23 pages; Correction to Corollary 7", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the underdamped Langevin diffusion when the log of the target\ndistribution is smooth and strongly concave. We present a MCMC algorithm based\non its discretization and show that it achieves $\\varepsilon$ error (in\n2-Wasserstein distance) in $\\mathcal{O}(\\sqrt{d}/\\varepsilon)$ steps. This is a\nsignificant improvement over the best known rate for overdamped Langevin MCMC,\nwhich is $\\mathcal{O}(d/\\varepsilon^2)$ steps under the same\nsmoothness/concavity assumptions.\n  The underdamped Langevin MCMC scheme can be viewed as a version of\nHamiltonian Monte Carlo (HMC) which has been observed to outperform overdamped\nLangevin MCMC methods in a number of application areas. We provide quantitative\nrates that support this empirical wisdom.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 12:08:55 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 04:23:36 GMT"}, {"version": "v3", "created": "Thu, 2 Nov 2017 20:55:26 GMT"}, {"version": "v4", "created": "Sat, 11 Nov 2017 20:06:25 GMT"}, {"version": "v5", "created": "Mon, 1 Jan 2018 23:24:29 GMT"}, {"version": "v6", "created": "Tue, 16 Jan 2018 19:28:32 GMT"}, {"version": "v7", "created": "Fri, 26 Jan 2018 21:56:51 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cheng", "Xiang", ""], ["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.03897", "submitter": "Chavent Marie", "authors": "Marie Chavent and Vanessa Kuentz-Simonet and Amaury Labenne and\n  J\\'er\\^ome Saracco", "title": "ClustGeo: an R package for hierarchical clustering with spatial\n  constraints", "comments": null, "journal-ref": null, "doi": "10.1007/s00180-018-0791-1", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Ward-like hierarchical clustering algorithm\nincluding spatial/geographical constraints. Two dissimilarity matrices $D_0$\nand $D_1$ are inputted, along with a mixing parameter $\\alpha \\in [0,1]$. The\ndissimilarities can be non-Euclidean and the weights of the observations can be\nnon-uniform. The first matrix gives the dissimilarities in the \"feature space\"\nand the second matrix gives the dissimilarities in the \"constraint space\". The\ncriterion minimized at each stage is a convex combination of the homogeneity\ncriterion calculated with $D_0$ and the homogeneity criterion calculated with\n$D_1$. The idea is then to determine a value of $\\alpha$ which increases the\nspatial contiguity without deteriorating too much the quality of the solution\nbased on the variables of interest i.e. those of the feature space. This\nprocedure is illustrated on a real dataset using the R package ClustGeo.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:29:59 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 10:50:40 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Chavent", "Marie", ""], ["Kuentz-Simonet", "Vanessa", ""], ["Labenne", "Amaury", ""], ["Saracco", "J\u00e9r\u00f4me", ""]]}, {"id": "1707.04112", "submitter": "Ali Akbar Jafari", "authors": "Mohmammad Reza Kazemi and Ali Akbar Jafari", "title": "Small Sample Inference for the Common Coefficient of Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper utilizes the modified signed log-likelihood ratio method for the\nproblem of inference about the common coefficient of variation in several\nindependent normal populations. This method is applicable for both the problem\nof hypothesis testing and constructing a confidence interval for this\nparameter. Simulation studies show that the coverage probability of this\nproposed approach is close to the confidence coefficient. Also, its expected\nlength is smaller than expected lengths of other competing approaches. In fact,\nthe proposed approach is very satisfactory regardless of the number of\npopulations and the different values of the common coefficient of variation\neven for very small sample size. Finally, we illustrate the proposed method\nusing two real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 13:25:15 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Kazemi", "Mohmammad Reza", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1707.04314", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Tuan Anh Le, Jan-Willem van de Meent, Michael A.\n  Osborne, Frank Wood", "title": "Bayesian Optimization for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first general purpose framework for marginal maximum a\nposteriori estimation of probabilistic program variables. By using a series of\ncode transformations, the evidence of any probabilistic program, and therefore\nof any graphical model, can be optimized with respect to an arbitrary subset of\nits sampled variables. To carry out this optimization, we develop the first\nBayesian optimization package to directly exploit the source code of its\ntarget, leading to innovations in problem-independent hyperpriors, unbounded\noptimization, and implicit constraint satisfaction; delivering significant\nperformance improvements over prominent existing packages. We present\napplications of our method to a number of tasks including engineering design\nand parameter optimization.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:49:29 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Rainforth", "Tom", ""], ["Le", "Tuan Anh", ""], ["van de Meent", "Jan-Willem", ""], ["Osborne", "Michael A.", ""], ["Wood", "Frank", ""]]}, {"id": "1707.04400", "submitter": "Antonio Punzo", "authors": "Antonio Punzo", "title": "A new look at the inverse Gaussian distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse Gaussian (IG) is one of the most famous and considered\ndistributions with positive support. We propose a convenient mode-based\nparameterization yielding the reparametrized IG (rIG) distribution; it\nallows/simplifies the use of the IG distribution in various statistical fields,\nand we give some examples in nonparametric statistics, robust statistics, and\nmodel-based clustering. In nonparametric statistics, we define a smoother based\non rIG kernels. By construction, the estimator is well-defined and free of\nboundary bias. We adopt likelihood cross-validation to select the smoothing\nparameter. In robust statistics, we propose the contaminated IG distribution, a\nheavy-tailed generalization of the rIG distribution to accommodate mild\noutliers; they can be automatically detected by the model via maximum a\nposteriori probabilities. To obtain maximum likelihood estimates of the\nparameters, we illustrate an expectation-maximization (EM) algorithm. Finally,\nfor model-based clustering and semiparametric density estimation, we present\nfinite mixtures of rIG distributions. We use the EM algorithm to obtain ML\nestimates of the parameters of the mixture model. Applications to economic and\ninsurance data are finally illustrated to exemplify and enhance the use of the\nproposed models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:26:09 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Punzo", "Antonio", ""]]}, {"id": "1707.04464", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Debasis Kundu and Tumati Kiran Kumar", "title": "Hierarchical EM algorithm for estimating the parameters of Mixture of\n  Bivariate Generalized Exponential distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a mixture modeling framework using the bivariate\ngeneralized exponential distribution. We study different properties of this\nmixture distribution. Hierarchical EM algorithm is developed for finding the\nestimates of the parameters. The algorithm takes very large sample size to work\nas it contains many stages of approximation. Numerical Results are provided for\nmore illustration.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 11:28:41 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 05:27:04 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Kundu", "Debasis", ""], ["Kumar", "Tumati Kiran", ""]]}, {"id": "1707.04476", "submitter": "Johannes Buchner", "authors": "Johannes Buchner", "title": "Collaborative Nested Sampling: Big Data vs. complex physical models", "comments": "Resubmitted to PASP Focus on Machine Intelligence in Astronomy and\n  Astrophysics after first referee report. Figure 6 demonstrates the scaling\n  for Collaborative MultiNest, PolyChord and RadFriends implementations. Figure\n  10 application to MUSE IFU data. Implementation at\n  https://github.com/JohannesBuchner/massivedatans", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The data torrent unleashed by current and upcoming astronomical surveys\ndemands scalable analysis methods. Many machine learning approaches scale well,\nbut separating the instrument measurement from the physical effects of\ninterest, dealing with variable errors, and deriving parameter uncertainties is\noften an after-thought. Classic forward-folding analyses with Markov Chain\nMonte Carlo or Nested Sampling enable parameter estimation and model\ncomparison, even for complex and slow-to-evaluate physical models. However,\nthese approaches require independent runs for each data set, implying an\nunfeasible number of model evaluations in the Big Data regime. Here I present a\nnew algorithm, collaborative nested sampling, for deriving parameter\nprobability distributions for each observation. Importantly, the number of\nphysical model evaluations scales sub-linearly with the number of data sets,\nand no assumptions about homogeneous errors, Gaussianity, the form of the model\nor heterogeneity/completeness of the observations need to be made.\nCollaborative nested sampling has immediate application in speeding up analyses\nof large surveys, integral-field-unit observations, and Monte Carlo\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:07:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 23:08:39 GMT"}, {"version": "v3", "created": "Wed, 30 Aug 2017 16:10:24 GMT"}, {"version": "v4", "created": "Mon, 27 Aug 2018 12:59:00 GMT"}, {"version": "v5", "created": "Tue, 2 Oct 2018 20:27:38 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Buchner", "Johannes", ""]]}, {"id": "1707.04878", "submitter": "Matthew Charles Edwards", "authors": "Matthew C. Edwards, Renate Meyer, and Nelson Christensen", "title": "Bayesian nonparametric spectral density estimation using B-spline priors", "comments": null, "journal-ref": "Edwards, M.C., Meyer, R. & Christensen, N. Stat Comput (2018).\n  https://doi.org/10.1007/s11222-017-9796-9", "doi": "10.1007/s11222-017-9796-9", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Bayesian nonparametric approach to estimating the spectral\ndensity of a stationary time series. A nonparametric prior based on a mixture\nof B-spline distributions is specified and can be regarded as a generalization\nof the Bernstein polynomial prior of Petrone (1999a,b) and Choudhuri et al.\n(2004). Whittle's likelihood approximation is used to obtain the\npseudo-posterior distribution. This method allows for a data-driven choice of\nthe number of mixture components and the location of knots. Posterior samples\nare obtained using a Metropolis-within-Gibbs Markov chain Monte Carlo\nalgorithm, and mixing is improved using parallel tempering. We conduct a\nsimulation study to demonstrate that for complicated spectral densities, the\nB-spline prior provides more accurate Monte Carlo estimates in terms of\n$L_1$-error and uniform coverage probabilities than the Bernstein polynomial\nprior. We apply the algorithm to annual mean sunspot data to estimate the solar\ncycle. Finally, we demonstrate the algorithm's ability to estimate a spectral\ndensity with sharp features, using real gravitational wave detector data from\nLIGO's sixth science run, recoloured to match the Advanced LIGO target\nsensitivity.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 13:20:34 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 15:25:42 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Edwards", "Matthew C.", ""], ["Meyer", "Renate", ""], ["Christensen", "Nelson", ""]]}, {"id": "1707.04929", "submitter": "Efe Onaran", "authors": "Efe Onaran and Soledad Villar", "title": "Projected Power Iteration for Network Alignment", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The network alignment problem asks for the best correspondence between two\ngiven graphs, so that the largest possible number of edges are matched. This\nproblem appears in many scientific problems (like the study of protein-protein\ninteractions) and it is very closely related to the quadratic assignment\nproblem which has graph isomorphism, traveling salesman and minimum bisection\nproblems as particular cases. The graph matching problem is NP-hard in general.\nHowever, under some restrictive models for the graphs, algorithms can\napproximate the alignment efficiently. In that spirit the recent work by Feizi\nand collaborators introduce EigenAlign, a fast spectral method with convergence\nguarantees for Erd\\H{o}s-Reny\\'i graphs. In this work we propose the algorithm\nProjected Power Alignment, which is a projected power iteration version of\nEigenAlign. We numerically show it improves the recovery rates of EigenAlign\nand we describe the theory that may be used to provide performance guarantees\nfor Projected Power Alignment.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 19:02:31 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Onaran", "Efe", ""], ["Villar", "Soledad", ""]]}, {"id": "1707.05200", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock and Alexandre H. Thiery", "title": "A Discrete Bouncy Particle Sampler", "comments": "New theory for anisotropic targets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Markov chain Monte Carlo methods operate in discrete time and are\nreversible with respect to the target probability. Nevertheless, it is now\nunderstood that the use of non-reversible Markov chains can be beneficial in\nmany contexts. In particular, the recently-proposed Bouncy Particle Sampler\nleverages a continuous-time and non-reversible Markov process and empirically\nshows state-of-the-art performances when used to explore certain probability\ndensities; however, its implementation typically requires the computation of\nlocal upper bounds on the gradient of the log target density.\n  We present the Discrete Bouncy Particle Sampler, a general algorithm based\nupon a guided random walk, a partial refreshment of direction, and a\ndelayed-rejection step. We show that the Bouncy Particle Sampler can be\nunderstood as a scaling limit of a special case of our algorithm. In contrast\nto the Bouncy Particle Sampler, implementing the Discrete Bouncy Particle\nSampler only requires point-wise evaluation of the target density and its\ngradient. We propose extensions of the basic algorithm for situations when the\nexact gradient of the target density is not available. In a Gaussian setting,\nwe establish a scaling limit for the radial process as dimension increases to\ninfinity. We leverage this result to obtain the theoretical efficiency of the\nDiscrete Bouncy Particle Sampler as a function of the partial-refreshment\nparameter, which leads to a simple and robust tuning criterion. A further\nanalysis in a more general setting suggests that this tuning criterion applies\nmore generally. Theoretical and empirical efficiency curves are then compared\nfor different targets and algorithm variations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 14:57:50 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 16:12:33 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 12:04:02 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 14:33:01 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sherlock", "Chris", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1707.05861", "submitter": "Cheng Ju", "authors": "Cheng Ju, Joshua Schwab, Mark J. van der Laan", "title": "On Adaptive Propensity Score Truncation in Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The positivity assumption, or the experimental treatment assignment (ETA)\nassumption, is important for identifiability in causal inference. Even if the\npositivity assumption holds, practical violations of this assumption may\njeopardize the finite sample performance of the causal estimator. One of the\nconsequences of practical violations of the positivity assumption is extreme\nvalues in the estimated propensity score (PS). A common practice to address\nthis issue is truncating the PS estimate when constructing PS-based estimators.\nIn this study, we propose a novel adaptive truncation method,\nPositivity-C-TMLE, based on the collaborative targeted maximum likelihood\nestimation (C-TMLE) methodology. We demonstrate the outstanding performance of\nour novel approach in a variety of simulations by comparing it with other\ncommonly studied estimators. Results show that by adaptively truncating the\nestimated PS with a more targeted objective function, the Positivity-C-TMLE\nestimator achieves the best performance for both point estimation and\nconfidence interval coverage among all estimators considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Ju", "Cheng", ""], ["Schwab", "Joshua", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1707.05987", "submitter": "James Ridgway", "authors": "James Ridgway", "title": "Probably approximate Bayesian computation: nonasymptotic convergence of\n  ABC under misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a widely used inference method in\nBayesian statistics to bypass the point-wise computation of the likelihood. In\nthis paper we develop theoretical bounds for the distance between the\nstatistics used in ABC. We show that some versions of ABC are inherently robust\nto misspecification. The bounds are given in the form of oracle inequalities\nfor a finite sample size. The dependence on the dimension of the parameter\nspace and the number of statistics is made explicit. The results are shown to\nbe amenable to oracle inequalities in parameter space. We apply our theoretical\nresults to given prior distributions and data generating processes, including a\nnon-parametric regression model. In a second part of the paper, we propose a\nsequential Monte Carlo (SMC) to sample from the pseudo-posterior, improving\nupon the state of the art samplers.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 09:04:34 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 14:06:20 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Ridgway", "James", ""]]}, {"id": "1707.06156", "submitter": "Tomer Lancewicki Ph.D.", "authors": "Tomer Lancewicki", "title": "Regularization of the Kernel Matrix via Covariance Matrix Shrinkage\n  Estimation", "comments": "7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The kernel trick concept, formulated as an inner product in a feature space,\nfacilitates powerful extensions to many well-known algorithms. While the kernel\nmatrix involves inner products in the feature space, the sample covariance\nmatrix of the data requires outer products. Therefore, their spectral\nproperties are tightly connected. This allows us to examine the kernel matrix\nthrough the sample covariance matrix in the feature space and vice versa. The\nuse of kernels often involves a large number of features, compared to the\nnumber of observations. In this scenario, the sample covariance matrix is not\nwell-conditioned nor is it necessarily invertible, mandating a solution to the\nproblem of estimating high-dimensional covariance matrices under small sample\nsize conditions. We tackle this problem through the use of a shrinkage\nestimator that offers a compromise between the sample covariance matrix and a\nwell-conditioned matrix (also known as the \"target\") with the aim of minimizing\nthe mean-squared error (MSE). We propose a distribution-free kernel matrix\nregularization approach that is tuned directly from the kernel matrix, avoiding\nthe need to address the feature space explicitly. Numerical simulations\ndemonstrate that the proposed regularization is effective in classification\ntasks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 15:31:54 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Lancewicki", "Tomer", ""]]}, {"id": "1707.06360", "submitter": "Lu Wang", "authors": "Lu Wang, Zhengwu Zhang, David Dunson", "title": "Common and Individual Structure of Brain Networks", "comments": "29 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the problem of studying shared- and\nindividual-specific structure in replicated networks or graph-valued data. In\nparticular, the observed data consist of $n$ graphs, $G_i, i=1,\\ldots,n$, with\neach graph consisting of a collection of edges between $V$ nodes. In brain\nconnectomics, the graph for an individual corresponds to a set of\ninterconnections among brain regions. Such data can be organized as a $V \\times\nV$ binary adjacency matrix $A_i$ for each $i$, with ones indicating an edge\nbetween a pair of nodes and zeros indicating no edge. When nodes have a shared\nmeaning across replicates $i=1,\\ldots,n$, it becomes of substantial interest to\nstudy similarities and differences in the adjacency matrices. To address this\nproblem, we propose a method to estimate a common structure and low-dimensional\nindividual-specific deviations from replicated networks. The proposed Multiple\nGRAph Factorization (M-GRAF) model relies on a logistic regression mapping\ncombined with a hierarchical eigenvalue decomposition. We develop an efficient\nalgorithm for estimation and study basic properties of our approach. Simulation\nstudies show excellent operating characteristics and we apply the method to\nhuman brain connectomics data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 03:34:44 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 17:34:34 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Wang", "Lu", ""], ["Zhang", "Zhengwu", ""], ["Dunson", "David", ""]]}, {"id": "1707.07149", "submitter": "Marjolein Fokkema", "authors": "Marjolein Fokkema", "title": "Fitting Prediction Rule Ensembles with R Package pre", "comments": null, "journal-ref": "Journal of Statistical Software 92 (2020) 12 1-30", "doi": "10.18637/jss.v092.i12", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction rule ensembles (PREs) are sparse collections of rules, offering\nhighly interpretable regression and classification models. This paper presents\nthe R package pre, which derives PREs through the methodology of Friedman and\nPopescu (2008). The implementation and functionality of package pre is\ndescribed and illustrated through application on a dataset on the prediction of\ndepression. Furthermore, accuracy and sparsity of PREs is compared with that of\nsingle trees, random forest and lasso regression in four benchmark datasets.\nResults indicate that pre derives ensembles with predictive accuracy comparable\nto that of random forests, while using a smaller number of variables for\nprediction.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 12:03:59 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 10:34:16 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 15:49:23 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 12:24:09 GMT"}, {"version": "v5", "created": "Sat, 28 Mar 2020 01:01:55 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Fokkema", "Marjolein", ""]]}, {"id": "1707.07657", "submitter": "Ehsan Sadrfaridpour", "authors": "E. Sadrfaridpour, T. Razzaghi, I. Safro", "title": "Engineering fast multilevel support vector machines", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational complexity of solving nonlinear support vector machine\n(SVM) is prohibitive on large-scale data. In particular, this issue becomes\nvery sensitive when the data represents additional difficulties such as highly\nimbalanced class sizes. Typically, nonlinear kernels produce significantly\nhigher classification quality to linear kernels but introduce extra kernel and\nmodel parameters which requires computationally expensive fitting. This\nincreases the quality but also reduces the performance dramatically. We\nintroduce a generalized fast multilevel framework for regular and weighted SVM\nand discuss several versions of its algorithmic components that lead to a good\ntrade-off between quality and time. Our framework is implemented using PETSc\nwhich allows an easy integration with scientific computing tasks. The\nexperimental results demonstrate significant speed up compared to the\nstate-of-the-art nonlinear SVM libraries.\n  Reproducibility: our source code, documentation and parameters are available\nat https:// github.com/esadr/mlsvm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 17:32:37 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 00:41:30 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 02:37:58 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sadrfaridpour", "E.", ""], ["Razzaghi", "T.", ""], ["Safro", "I.", ""]]}, {"id": "1707.08090", "submitter": "Pedro Ramos", "authors": "Jorge Alberto Achcar, Pedro Luiz Ramos, Edson Zangiacomi Martinez", "title": "Some Computational Aspects to Find Accurate Estimates for the Parameters\n  of the Generalized Gamma distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss computational aspects to obtain accurate inferences\nfor the parameters of the generalized gamma (GG) distribution. Usually, the\nsolution of the maximum likelihood estimators (MLE) for the GG distribution\nhave no stable behavior depending on large sample sizes and good initial values\nto be used in the iterative numerical algorithms. From a Bayesian approach,\nthis problem remains, but now related to the choice of prior distributions for\nthe parameters of this model. We presented some exploratory techniques to\nobtain good initial values to be used in the iterative procedures and also to\nelicited appropriate informative priors. Finally, our proposed methodology is\nalso considered for data sets in the presence of censorship.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 17:02:10 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Achcar", "Jorge Alberto", ""], ["Ramos", "Pedro Luiz", ""], ["Martinez", "Edson Zangiacomi", ""]]}, {"id": "1707.08136", "submitter": "Thomas Galtier", "authors": "H. Chraibi, A. Dutfoy, T. Galtier, J. Garnier", "title": "Monte-Carlo acceleration: importance sampling and hybrid dynamic systems", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of a complex industrial system can rarely be assessed\nanalytically. As system failure is often a rare event, crude Monte-Carlo\nmethods are prohibitively expensive from a computational point of view. In\norder to reduce computation times, variance reduction methods such as\nimportance sampling can be used. We propose an adaptation of this method for a\nclass of multi-component dynamical systems. We address a system whose failure\ncorresponds to a physical variable of the system (temperature, pressure, water\nlevel) entering a critical region. Such systems are common in hydraulic and\nnuclear industry. In these systems, the statuses of the components (on, off, or\nout-of-order) determine the dynamics of the physical variables, and is altered\nboth by deterministic feedback mechanisms and random failures or repairs. In\norder to deal with this interplay between components status and physical\nvariables we model trajectory using piecewise deterministic Markovian processes\n(PDMP). We show how to adapt the importance sampling method to PDMP, by\nintroducing a reference measure on the trajectory space, and we present a\nbiasing strategy for importance sampling. A simulation study compares our\nimportance sampling method to the crude Monte-Carlo method for a\nthree-component-system.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:04:08 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 15:05:11 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Chraibi", "H.", ""], ["Dutfoy", "A.", ""], ["Galtier", "T.", ""], ["Garnier", "J.", ""]]}, {"id": "1707.08208", "submitter": "Thakshila Wimalajeewa", "authors": "Thakshila Wimalajeewa and Pramod K. Varshney", "title": "Robust Detection of Random Events with Spatially Correlated Data in\n  Wireless Sensor Networks via Distributed Compressive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we exploit the theory of compressive sensing to perform\ndetection of a random source in a dense sensor network. When the sensors are\ndensely deployed, observations at adjacent sensors are highly correlated while\nthose corresponding to distant sensors are less correlated. Thus, the\ncovariance matrix of the concatenated observation vector of all the sensors at\nany given time can be sparse where the sparse structure depends on the network\ntopology and the correlation model. Exploiting the sparsity structure of the\ncovariance matrix, we develop a robust nonparametric detector to detect the\npresence of the random event using a compressed version of the data collected\nat the distributed nodes. We employ the multiple access channel (MAC) model\nwith distributed random projections for sensors to transmit observations so\nthat a compressed version of the observations is available at the fusion\ncenter. Detection is performed by constructing a decision statistic based on\nthe covariance information of uncompressed data which is estimated using\ncompressed data. The proposed approach does not require any knowledge of the\nnoise parameter to set the threshold, and is also robust when the distributed\nrandom projection matrices become sparse.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 02:26:10 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Wimalajeewa", "Thakshila", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1707.08213", "submitter": "Lee Richardson", "authors": "Lee F. Richardson, William F. Eddy", "title": "The 2D Tree Sliding Window Discrete Fourier Transform", "comments": "15 pages, 4 figures, submitted to ACM TOMS", "journal-ref": "ACM Transactions on Mathematical Software, Vol. 45, No. 1, Article\n  12. Publication date: February 2019", "doi": "10.1145/3264426", "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for the 2D Sliding Window Discrete Fourier\nTransform (SWDFT). Our algorithm avoids repeating calculations in overlapping\nwindows by storing them in a tree data-structure based on the ideas of the\nCooley- Tukey Fast Fourier Transform (FFT). For an $N_0 \\times N_1$ array and\n$n_0 \\times n_1$ windows, our algorithm takes $O(N_0 N_1 n_0 n_1)$ operations.\nWe provide a C implementation of our algorithm for the Radix-2 case, compare\nours with existing algorithms, and show how our algorithm easily extends to\nhigher dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:48:46 GMT"}, {"version": "v2", "created": "Tue, 17 Jul 2018 18:27:27 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Richardson", "Lee F.", ""], ["Eddy", "William F.", ""]]}, {"id": "1707.08220", "submitter": "Yajuan Si", "authors": "Yajuan Si, Rob Trangucci, Jonah Sol Gabry and Andrew Gelman", "title": "Bayesian hierarchical weighting adjustment and survey inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Bayesian prediction and weighted inference as a unified approach\nto survey inference. The general principles of Bayesian analysis imply that\nmodels for survey outcomes should be conditional on all variables that affect\nthe probability of inclusion. We incorporate the weighting variables under the\nframework of multilevel regression and poststratification, as a byproduct\ngenerating model-based weights after smoothing. We investigate deep\ninteractions and introduce structured prior distributions for smoothing and\nstability of estimates. The computation is done via Stan and implemented in the\nopen source R package \"rstanarm\" ready for public use. Simulation studies\nillustrate that model-based prediction and weighting inference outperform\nclassical weighting. We apply the proposal to the New York Longitudinal Study\nof Wellbeing. The new approach generates robust weights and increases\nefficiency for finite population inference, especially for subsets of the\npopulation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 21:02:26 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 15:51:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Trangucci", "Rob", ""], ["Gabry", "Jonah Sol", ""], ["Gelman", "Andrew", ""]]}, {"id": "1707.08339", "submitter": "H{\\aa}kon Tjelmeland", "authors": "H{\\aa}kon Tjelmeland and Xin Luo", "title": "Prior specification for binary Markov mesh models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose prior distributions for all parts of the specification of a Markov\nmesh model. In the formulation we define priors for the sequential\nneighborhood, for the parametric form of the conditional distributions and for\nthe parameter values. By simulating from the resulting posterior distribution\nwhen conditioning on an observed scene, we thereby obtain an automatic model\nselection procedure for Markov mesh models. To sample from such a posterior\ndistribution, we construct a reversible jump Markov chain Monte Carlo algorithm\n(RJMCMC). We demonstrate the usefulness of our prior formulation and the\nlimitations of our RJMCMC algorithm in two examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 09:45:18 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Tjelmeland", "H\u00e5kon", ""], ["Luo", "Xin", ""]]}, {"id": "1707.08384", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (1), S\\'everine Demeyer (1), Nicolas Fischer (1), Julien\n  Bect (2), Emmanuel Vazquez (3) ((1) LNE, (2) L2S, (3) GdR MASCOT-NUM)", "title": "Sequential design of experiments to estimate a probability of exceeding\n  a threshold in a multi-fidelity stochastic simulator", "comments": "61th World Statistics Congress of the International Statistical\n  Institute (ISI 2017), Jul 2017, Marrakech, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a stochastic numerical simulator to assess the\nimpact of some factors on a phenomenon. The simulator is seen as a black box\nwith inputs and outputs. The quality of a simulation, hereafter referred to as\nfidelity, is assumed to be tunable by means of an additional input of the\nsimulator (e.g., a mesh size parameter): high-fidelity simulations provide more\naccurate results, but are time-consuming. Using a limited computation-time\nbudget, we want to estimate, for any value of the physical inputs, the\nprobability that a certain scalar output of the simulator will exceed a given\ncritical threshold at the highest fidelity level. The problem is addressed in a\nBayesian framework, using a Gaussian process model of the multi-fidelity\nsimulator. We consider a Bayesian estimator of the probability, together with\nan associated measure of uncertainty, and propose a new multi-fidelity\nsequential design strategy, called Maximum Speed of Uncertainty Reduction\n(MSUR), to select the value of physical inputs and the fidelity level of new\nsimulations. The MSUR strategy is tested on an example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:35:58 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Bect", "Julien", "", "L2S"], ["Vazquez", "Emmanuel", "", "GdR MASCOT-NUM"]]}, {"id": "1707.08407", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, Min Zhu, Keith E. Muller", "title": "A Note on Implementing a Special Case of the LEAR Covariance Model in\n  Standard Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated measures analyses require proper choice of the correlation model to\nensure accurate inference and optimal efficiency. The linear exponent\nautoregressive (LEAR) correlation model provides a flexible two-parameter\ncorrelation structure that accommodates a variety of data types in which the\ncorrelation within-sampling unit decreases exponentially in time or space. The\nLEAR model subsumes three classic temporal correlation structures, namely\ncompound symmetry, continuous-time AR(1), and MA(1), while maintaining\nparsimony and providing appealing statistical and computational properties. It\nalso supplies a plausible correlation structure for power analyses across many\nexperimental designs. However, no commonly used statistical packages provide a\nstraightforward way to implement the model, limiting its use to those with the\nappropriate programming skills. Here we present a reparameterization of the\nLEAR model that allows easily implementing it in standard software for the\nspecial case of data with equally spaced temporal or spatial intervals.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:28:51 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Simpson", "Sean L.", ""], ["Zhu", "Min", ""], ["Muller", "Keith E.", ""]]}, {"id": "1707.08513", "submitter": "Roberto Fontana", "authors": "Roberto Fontana, Francesca Romana Crucinio", "title": "Markov Chain Monte Carlo sampling for conditional tests: A link between\n  permutation tests and algebraic statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider conditional tests for non-negative discrete exponential families.\nWe develop two Markov Chain Monte Carlo (MCMC) algorithms which allow us to\nsample from the conditional space and to perform approximated tests. The first\nalgorithm is based on the MCMC sampling described by Sturmfels. The second MCMC\nsampling consists in a more efficient algorithm which exploits the optimal\npartition of the conditional space into orbits of permutations. We thus\nestablish a link between standard permutation and algebraic-statistics-based\nsampling. Through a simulation study we compare the exact cumulative\ndistribution function (cdf) with the approximated cdfs which are obtained with\nthe two MCMC samplings and the standard permutation sampling. We conclude that\nthe MCMC sampling which exploits the partition of the conditional space into\norbits of permutations gives an estimated cdf, under $H_0$, which is more\nreliable and converges to the exact cdf with the least steps. This sampling\ntechnique can also be used to build an approximation of the exact cdf when its\nexact computation is computationally infeasible.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 15:59:57 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Fontana", "Roberto", ""], ["Crucinio", "Francesca Romana", ""]]}, {"id": "1707.08538", "submitter": "Jarod Yan Liang Lee", "authors": "Jarod Y.L. Lee, Peter J. Green and Louise M. Ryan", "title": "On the \"Poisson Trick\" and its Extensions for Fitting Multinomial\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with the fitting of multinomial regression models\nusing the so-called \"Poisson Trick\". The work is motivated by Chen & Kuo (2001)\nand Malchow-M{\\o}ller & Svarer (2003) which have been criticized for being\ncomputationally inefficient and sometimes producing nonsense results. We first\ndiscuss the case of independent data and offer a parsimonious fitting strategy\nwhen all covariates are categorical. We then propose a new approach for\nmodelling correlated responses based on an extension of the Gamma-Poisson\nmodel, where the likelihood can be expressed in closed-form. The parameters are\nestimated via an Expectation/Conditional Maximization (ECM) algorithm, which\ncan be implemented using functions for fitting generalized linear models\nreadily available in standard statistical software packages. Compared to\nexisting methods, our approach avoids the need to approximate the intractable\nintegrals and thus the inference is exact with respect to the approximating\nGamma-Poisson model. The proposed method is illustrated via a reanalysis of the\nyogurt data discussed by Chen & Kuo (2001).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 16:55:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Lee", "Jarod Y. L.", ""], ["Green", "Peter J.", ""], ["Ryan", "Louise M.", ""]]}, {"id": "1707.08692", "submitter": "Ryan Tibshirani", "authors": "Trevor Hastie, Robert Tibshirani, Ryan J. Tibshirani", "title": "Extended Comparisons of Best Subset Selection, Forward Stepwise\n  Selection, and the Lasso", "comments": "18 pages main paper, 34 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exciting new work, Bertsimas et al. (2016) showed that the classical best\nsubset selection problem in regression modeling can be formulated as a mixed\ninteger optimization (MIO) problem. Using recent advances in MIO algorithms,\nthey demonstrated that best subset selection can now be solved at much larger\nproblem sizes that what was thought possible in the statistics community. They\npresented empirical comparisons of best subset selection with other popular\nvariable selection procedures, in particular, the lasso and forward stepwise\nselection. Surprisingly (to us), their simulations suggested that best subset\nselection consistently outperformed both methods in terms of prediction\naccuracy. Here we present an expanded set of simulations to shed more light on\nthese comparisons.\n  The summary is roughly as follows: (a) neither best subset selection nor the\nlasso uniformly dominate the other, with best subset selection generally\nperforming better in high signal-to-noise (SNR) ratio regimes, and the lasso\nbetter in low SNR regimes; (b) best subset selection and forward stepwise\nperform quite similarly throughout; (c) the relaxed lasso (actually, a\nsimplified version of the original relaxed estimator defined in Meinshausen,\n2007) is the overall winner, performing just about as well as the lasso in low\nSNR scenarios, and as well as best subset selection in high SNR scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 03:04:28 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 23:46:57 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1707.08885", "submitter": "Tomer Lancewicki Ph.D.", "authors": "Tomer Lancewicki", "title": "Sequential Inverse Approximation of a Regularized Sample Covariance\n  Matrix", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the goals in scaling sequential machine learning methods pertains to\ndealing with high-dimensional data spaces. A key related challenge is that many\nmethods heavily depend on obtaining the inverse covariance matrix of the data.\nIt is well known that covariance matrix estimation is problematic when the\nnumber of observations is relatively small compared to the number of variables.\nA common way to tackle this problem is through the use of a shrinkage estimator\nthat offers a compromise between the sample covariance matrix and a\nwell-conditioned matrix, with the aim of minimizing the mean-squared error. We\nderived sequential update rules to approximate the inverse shrinkage estimator\nof the covariance matrix. The approach paves the way for improved large-scale\nmachine learning methods that involve sequential updates.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 14:37:45 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Lancewicki", "Tomer", ""]]}, {"id": "1707.09334", "submitter": "Xun Huan", "authors": "Xun Huan, Cosmin Safta, Khachik Sargsyan, Zachary P. Vane, Guilhem\n  Lacaze, Joseph C. Oefelein, Habib N. Najm", "title": "Compressive Sensing with Cross-Validation and Stop-Sampling for Sparse\n  Polynomial Chaos Expansions", "comments": "Preprint 29 pages, 16 figures (56 small figures); v1 submitted to the\n  SIAM/ASA Journal on Uncertainty Quantification on July 28, 2017; v2 submitted\n  on March 12, 2018. v2 changes: minor edits involving some content\n  reorganization and clarification; v3 submitted on May 5, 2018. v3 changes:\n  minor edits", "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification 6 (2018) 907-936", "doi": "10.1137/17M1141096", "report-no": null, "categories": "stat.CO cs.IT math.IT physics.comp-ph physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing is a powerful technique for recovering sparse solutions\nof underdetermined linear systems, which is often encountered in uncertainty\nquantification analysis of expensive and high-dimensional physical models. We\nperform numerical investigations employing several compressive sensing solvers\nthat target the unconstrained LASSO formulation, with a focus on linear systems\nthat arise in the construction of polynomial chaos expansions. With core\nsolvers of l1_ls, SpaRSA, CGIST, FPC_AS, and ADMM, we develop techniques to\nmitigate overfitting through an automated selection of regularization constant\nbased on cross-validation, and a heuristic strategy to guide the stop-sampling\ndecision. Practical recommendations on parameter settings for these techniques\nare provided and discussed. The overall method is applied to a series of\nnumerical examples of increasing complexity, including large eddy simulations\nof supersonic turbulent jet-in-crossflow involving a 24-dimensional input.\nThrough empirical phase-transition diagrams and convergence plots, we\nillustrate sparse recovery performance under structures induced by polynomial\nchaos, accuracy and computational tradeoffs between polynomial bases of\ndifferent degrees, and practicability of conducting compressive sensing for a\nrealistic, high-dimensional physical application. Across test cases studied in\nthis paper, we find ADMM to have demonstrated empirical advantages through\nconsistent lower errors and faster computational times.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 17:20:39 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 22:31:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 10:24:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Huan", "Xun", ""], ["Safta", "Cosmin", ""], ["Sargsyan", "Khachik", ""], ["Vane", "Zachary P.", ""], ["Lacaze", "Guilhem", ""], ["Oefelein", "Joseph C.", ""], ["Najm", "Habib N.", ""]]}, {"id": "1707.09705", "submitter": "Dangna Li", "authors": "Dangna Li and Wing H Wong", "title": "Mini-batch Tempered MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a general framework of performing MCMC with only a\nmini-batch of data. We show by estimating the Metropolis-Hasting ratio with\nonly a mini-batch of data, one is essentially sampling from the true posterior\nraised to a known temperature. We show by experiments that our method,\nMini-batch Tempered MCMC (MINT-MCMC), can efficiently explore multiple modes of\na posterior distribution. Based on the Equi-Energy sampler (Kou et al. 2006),\nwe developed a new parallel MCMC algorithm based on the Equi-Energy sampler,\nwhich enables efficient sampling from high-dimensional multi-modal posteriors\nwith well separated modes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 03:07:00 GMT"}, {"version": "v2", "created": "Sun, 6 Aug 2017 16:11:05 GMT"}, {"version": "v3", "created": "Sat, 7 Oct 2017 18:11:09 GMT"}, {"version": "v4", "created": "Sun, 5 Nov 2017 21:24:30 GMT"}, {"version": "v5", "created": "Thu, 8 Feb 2018 04:28:23 GMT"}, {"version": "v6", "created": "Mon, 26 Feb 2018 23:29:04 GMT"}, {"version": "v7", "created": "Wed, 25 Apr 2018 01:03:24 GMT"}, {"version": "v8", "created": "Mon, 21 May 2018 18:09:49 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Li", "Dangna", ""], ["Wong", "Wing H", ""]]}, {"id": "1707.09974", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Biplab Paul", "title": "Some variations of EM algorithms for Marshall-Olkin bivariate Pareto\n  distribution with location and scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Asimit et. al used an EM algorithm to estimate Marshall-Olkin\nbivariate Pareto distribution. The distribution has seven parameters. We\ndescribe few alternative approaches of EM algorithm. A numerical simulation is\nperformed to verify the performance of different proposed algorithms. A\nreal-life data analysis is also shown for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:40:40 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Paul", "Biplab", ""]]}]