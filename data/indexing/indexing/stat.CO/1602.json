[{"id": "1602.00207", "submitter": "Jonathan Friedman", "authors": "Jonathan Malcolm Friedman", "title": "Binomial and Multinomial Proportions: Accurate Estimation and Reliable\n  Assessment of Accuracy", "comments": "61 pages, 24 figures; Small changes occurred (Figs 13-18, A1 & A2,\n  Tables 1, S1) after fixing a slight bug in the the source code. For\n  comparison, version (N-1) prior to fixing the bug is at:\n  http://www.researchgate.net/profile/Jonathan_Friedman", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misestimates of $\\sigma_{P_o}$, the \\emph{uncertainty} in $P_o$ from a\n2-state Bayes equation used for binary classification, apparently arose from\n$\\hat{\\sigma}_{p_i}$, the uncertainty in underlying pdfs estimated from\nexperimental $b$-bin histograms. To address this, several Bayesian estimator\npairs $(\\hat{p}_i, \\hat{\\sigma}_{p_i})$ were compared for agreement between\nnominal confidence level ($\\xi$) and calculated coverage values ($C$). Large\n$\\xi$-to-$C$ inconsistency for large $b$ and $ p_i \\gg \\frac{1}{b}$ arises for\nall multinomial estimators since priors downweight low likelihood, high $p_i$\nvalues. To improve $\\xi$-to-$C$ matching, $(\\xi-C)^2$ was minimized against\n$\\alpha_0$ in a more general prior pdf\n($\\mathcal{B}[\\alpha_0,(b-1)\\alpha_0;x]$) to obtain\n$(\\hat{p_i})_{\\xi\\leftrightarrow C}$. This improved matching for $b=2$, but for\n$b>2$, $\\xi$-to-$C$ matching by $(\\hat{p_i})_{\\xi\\leftrightarrow C}$ required\nan effective value \"$b=2$\" and renormalization, and this reduced\n$\\hat{p}_i$-to-$p_i$ matching. Better $\\hat{p}_i$-to-$p_i$ matching came from\nthe original multinomial estimators, a new discrete-domain estimator\n$\\hat{p}(n_i,N)$, or an earlier \\emph{joint} estimator, $(\\hat{p_i})_{\\bowtie}$\nthat co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean\nvector. Best simultaneous $\\xi$-to-$C$ and $\\hat{p}_i$-to-$p_i$ matching came\nby \\emph{de-noising} initial estimates of underlying pdfs. For $b=100$,\n$N<12800$, de-noised $\\hat{p}$ needed $\\approx 10\\times$ fewer observations to\nachieve $\\hat{p}_i$-to-$p_i$ matching equivalent to that found for\n$\\hat{p}(n_i,N)$, $(\\hat{p_i})_{\\bowtie}$ or the original multinomial\n$\\hat{p}_i$. De-noising each different type of initial estimate yielded\nsimilarly high accuracy in Monte-Carlo tests.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 06:39:07 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Friedman", "Jonathan Malcolm", ""]]}, {"id": "1602.00346", "submitter": "Art Owen", "authors": "Katelyn Gao and Art B. Owen", "title": "Efficient moment calculations for variance components in large\n  unbalanced crossed random effects models", "comments": null, "journal-ref": null, "doi": "10.1214/17-EJS1236", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large crossed data sets, described by generalized linear mixed models, have\nbecome increasingly common and provide challenges for statistical analysis. At\nvery large sizes it becomes desirable to have the computational costs of\nestimation, inference and prediction (both space and time) grow at most\nlinearly with sample size. Both traditional maximum likelihood estimation and\nnumerous Markov chain Monte Carlo Bayesian algorithms take superlinear time in\norder to obtain good parameter estimates. We propose moment based algorithms\nthat, with at most linear cost, estimate variance components, measure the\nuncertainties of those estimates, and generate shrinkage based predictions for\nmissing observations. When run on simulated normally distributed data, our\nalgorithm performs competitively with maximum likelihood methods.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2016 23:46:36 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Gao", "Katelyn", ""], ["Owen", "Art B.", ""]]}, {"id": "1602.01120", "submitter": "Daniel McDonald", "authors": "Darren Homrighausen and Daniel J. McDonald", "title": "On the Nystr\\\"om and Column-Sampling Methods for the Approximate\n  Principal Components Analysis of Large Data Sets", "comments": "20 pages", "journal-ref": "Journal of Computational and Graphical Statistics, 25(2), 2016", "doi": "10.1080/10618600.2014.995799", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze approximate methods for undertaking a principal\ncomponents analysis (PCA) on large data sets. PCA is a classical dimension\nreduction method that involves the projection of the data onto the subspace\nspanned by the leading eigenvectors of the covariance matrix. This projection\ncan be used either for exploratory purposes or as an input for further\nanalysis, e.g. regression. If the data have billions of entries or more, the\ncomputational and storage requirements for saving and manipulating the design\nmatrix in fast memory is prohibitive. Recently, the Nystr\\\"om and\ncolumn-sampling methods have appeared in the numerical linear algebra community\nfor the randomized approximation of the singular value decomposition of large\nmatrices. However, their utility for statistical applications remains unclear.\nWe compare these approximations theoretically by bounding the distance between\nthe induced subspaces and the desired, but computationally infeasible, PCA\nsubspace. Additionally we show empirically, through simulations and a real data\nexample involving a corpus of emails, the trade-off of approximation accuracy\nand computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 21:26:48 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Homrighausen", "Darren", ""], ["McDonald", "Daniel J.", ""]]}, {"id": "1602.01254", "submitter": "Kaylea Haynes  Miss", "authors": "Kaylea Haynes, Paul Fearnhead and Idris A. Eckley", "title": "A computationally efficient nonparametric approach for changepoint\n  detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we build on an approach proposed by Zou et al. (2014) for\nnonpara- metric changepoint detection. This approach defines the best\nsegmentation for a data set as the one which minimises a penalised cost\nfunction, with the cost function defined in term of minus a non-parametric\nlog-likelihood for data within each segment. Min- imising this cost function is\npossible using dynamic programming, but their algorithm had a computational\ncost that is cubic in the length of the data set. To speed up computation, Zou\net al. (2014) resorted to a screening procedure which means that the estimated\nsegmentation is no longer guaranteed to be the global minimum of the cost\nfunction. We show that the screening procedure adversely affects the accuracy\nof the changepoint detection method, and show how a faster dynamic programming\nalgorithm, Pruned Exact Linear Time, PELT (Killick et al., 2012), can be used\nto find the optimal segmentation with a computational cost that can be close to\nlinear in the amount of data. PELT requires a penalty to avoid\nunder/over-fitting the model which can have a detrimental effect on the quality\nof the detected changepoints. To overcome this issue we use a relatively new\nmethod, Changepoints Over a Range of PenaltieS (CROPS) (Haynes et al., 2015),\nwhich finds all of the optimal segmentations for multiple penalty values over a\ncontinuous range. We apply our method to detect changes in heart rate during\nphysical activity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 10:41:40 GMT"}], "update_date": "2016-02-04", "authors_parsed": [["Haynes", "Kaylea", ""], ["Fearnhead", "Paul", ""], ["Eckley", "Idris A.", ""]]}, {"id": "1602.01445", "submitter": "Tevfik Aktekin", "authors": "Tevfik Aktekin and Nicholas G. Polson and Refik Soyer", "title": "Sequential Bayesian Analysis of Multivariate Count Data", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new class of dynamic multivariate Poisson count models that\nallow for fast online updating and we refer to these models as multivariate\nPoisson-scaled beta (MPSB). The MPSB model allows for serial dependence in the\ncounts as well as dependence across multiple series with a random common\nenvironment. Other notable features include analytic forms for state\npropagation and predictive likelihood densities. Sequential updating occurs\nthrough the updating of the sufficient statistics for static model parameters,\nleading to a fully adapted particle learning algorithm and a new class of\npredictive likelihoods and marginal distributions which we refer to as the\n(dynamic) multivariate confluent hyper-geometric negative binomial distribution\n(MCHG-NB) and the the dynamic multivariate negative binomial (DMNB)\ndistribution. To illustrate our methodology, we use various simulation studies\nand count data on weekly non-durable goods consumer demand.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2016 20:18:41 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2016 19:37:03 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Aktekin", "Tevfik", ""], ["Polson", "Nicholas G.", ""], ["Soyer", "Refik", ""]]}, {"id": "1602.02219", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Babak Shahbaba, Hongkai Zhao", "title": "Variational Hamiltonian Monte Carlo via Score Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, the field of computational Bayesian statistics has been\ndivided into two main subfields: variational methods and Markov chain Monte\nCarlo (MCMC). In recent years, however, several methods have been proposed\nbased on combining variational Bayesian inference and MCMC simulation in order\nto improve their overall accuracy and computational efficiency. This marriage\nof fast evaluation and flexible approximation provides a promising means of\ndesigning scalable Bayesian inference methods. In this paper, we explore the\npossibility of incorporating variational approximation into a state-of-the-art\nMCMC method, Hamiltonian Monte Carlo (HMC), to reduce the required gradient\ncomputation in the simulation of Hamiltonian flow, which is the bottleneck for\nmany applications of HMC in big data problems. To this end, we use a {\\it\nfree-form} approximation induced by a fast and flexible surrogate function\nbased on single-hidden layer feedforward neural networks. The surrogate\nprovides sufficiently accurate approximation while allowing for fast\nexploration of parameter space, resulting in an efficient approximate inference\nalgorithm. We demonstrate the advantages of our method on both synthetic and\nreal data problems.\n", "versions": [{"version": "v1", "created": "Sat, 6 Feb 2016 05:36:02 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 23:28:24 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zhang", "Cheng", ""], ["Shahbaba", "Babak", ""], ["Zhao", "Hongkai", ""]]}, {"id": "1602.02539", "submitter": "Simon Wood", "authors": "Simon N Wood", "title": "Just Another Gibbs Additive Modeller: Interfacing JAGS and mgcv", "comments": "Submitted to the Journal of Statistical Software", "journal-ref": null, "doi": "10.18637/jss.v075.i07", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BUGS language offers a very flexible way of specifying complex\nstatistical models for the purposes of Gibbs sampling, while its JAGS variant\noffers very convenient R integration via the rjags package. However, including\nsmoothers in JAGS models can involve some quite tedious coding, especially for\nmultivariate or adaptive smoothers. Further, if an additive smooth structure is\nrequired then some care is needed, in order to centre smooths appropriately,\nand to find appropriate starting values. R package mgcv implements a wide range\nof smoothers, all in a manner appropriate for inclusion in JAGS code, and\nautomates centring and other smooth setup tasks. The purpose of this note is to\ndescribe an interface between mgcv and JAGS, based around an R function,\n`jagam', which takes a generalized additive model (GAM) as specified in mgcv\nand automatically generates the JAGS model code and data required for inference\nabout the model via Gibbs sampling. Although the auto-generated JAGS code can\nbe run as is, the expectation is that the user would wish to modify it in order\nto add complex stochastic model components readily specified in JAGS. A simple\ninterface is also provided for visualisation and further inference about the\nestimated smooth components using standard mgcv functionality. The methods\ndescribed here will be un-necessarily inefficient if all that is required is\nfully Bayesian inference about a standard GAM, rather than the full flexibility\nof JAGS. In that case the BayesX package would be more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 12:12:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wood", "Simon N", ""]]}, {"id": "1602.02575", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson, Chenlei Leng", "title": "DECOrrelated feature space partitioning for distributed sparse\n  regression", "comments": "Correct legend errors in Figure 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting statistical models is computationally challenging when the sample\nsize or the dimension of the dataset is huge. An attractive approach for\ndown-scaling the problem size is to first partition the dataset into subsets\nand then fit using distributed algorithms. The dataset can be partitioned\neither horizontally (in the sample space) or vertically (in the feature space).\nWhile the majority of the literature focuses on sample space partitioning,\nfeature space partitioning is more effective when $p\\gg n$. Existing methods\nfor partitioning features, however, are either vulnerable to high correlations\nor inefficient in reducing the model dimension. In this paper, we solve these\nproblems through a new embarrassingly parallel framework named DECO for\ndistributed variable selection and parameter estimation. In DECO, variables are\nfirst partitioned and allocated to $m$ distributed workers. The decorrelated\nsubset data within each worker are then fitted via any algorithm designed for\nhigh-dimensional problems. We show that by incorporating the decorrelation\nstep, DECO can achieve consistent variable selection and parameter estimation\non each subset with (almost) no assumptions. In addition, the convergence rate\nis nearly minimax optimal for both sparse and weakly sparse models and does NOT\ndepend on the partition number $m$. Extensive numerical experiments are\nprovided to illustrate the performance of the new framework.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 14:17:38 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 13:18:57 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1602.02606", "submitter": "Jean-Michel Marin", "authors": "Julien Stoehr, Jean-Michel Marin and Pierre Pudlo", "title": "Hidden Gibbs random fields model selection using Block Likelihood\n  Information Criterion", "comments": null, "journal-ref": "Stat (2016) 5:158-172", "doi": "10.1002/sta4.112", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing model selection between Gibbs random fields is a very challenging\ntask. Indeed, due to the Markovian dependence structure, the normalizing\nconstant of the fields cannot be computed using standard analytical or\nnumerical methods. Furthermore, such unobserved fields cannot be integrated out\nand the likelihood evaluztion is a doubly intractable problem. This forms a\ncentral issue to pick the model that best fits an observed data. We introduce a\nnew approximate version of the Bayesian Information Criterion. We partition the\nlattice into continuous rectangular blocks and we approximate the probability\nmeasure of the hidden Gibbs field by the product of some Gibbs distributions\nover the blocks. On that basis, we estimate the likelihood and derive the Block\nLikelihood Information Criterion (BLIC) that answers model choice questions\nsuch as the selection of the dependency structure or the number of latent\nstates. We study the performances of BLIC for those questions. In addition, we\npresent a comparison with ABC algorithms to point out that the novel criterion\noffers a better trade-off between time efficiency and reliable results.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:20:47 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stoehr", "Julien", ""], ["Marin", "Jean-Michel", ""], ["Pudlo", "Pierre", ""]]}, {"id": "1602.02616", "submitter": "Thomas Bonis", "authors": "Thomas Bonis", "title": "Guarantees in Wasserstein Distance for the Langevin Monte Carlo\n  Algorithm", "comments": "Updated and merged with arXiv article 1506.06966", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a distribution $\\target$ using the\nLangevin Monte Carlo algorithm and provide rate of convergences for this\nalgorithm in terms of Wasserstein distance of order $2$. Our result holds as\nlong as the continuous diffusion process associated with the algorithm\nconverges exponentially fast to the target distribution along with some\ntechnical assumptions. While such an exponential convergence holds for example\nin the log-concave measure case, it also holds for the more general case of\nasymptoticaly log-concave measures. Our results thus extends the known rates of\nconvergence in total variation and Wasserstein distances which have only been\nobtained in the log-concave case. Moreover, using a sharper approximation bound\nof the continuous process, we obtain better asymptotic rates than traditional\nresults. We also look into variations of the Langevin Monte Carlo algorithm\nusing other discretization schemes. In a first time, we look into the use of\nthe Ozaki's discretization but are unable to obtain any significative\nimprovement in terms of convergence rates compared to the Euler's scheme. We\nthen provide a (sub-optimal) way to study more general schemes, however our\napproach only holds for the log-concave case.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 15:49:07 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2016 11:19:23 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bonis", "Thomas", ""]]}, {"id": "1602.02736", "submitter": "Alen Alexanderian", "authors": "Bilal Saad and Alen Alexanderian and Serge Prudhomme and Omar M. Knio", "title": "Probabilistic modeling and global sensitivity analysis for $CO_2$\n  storage in geological formations: a spectral approach", "comments": "25 pages; fixed a minor latex issue, and added biblio info", "journal-ref": "Applied Mathematical Modelling, 53 (2018), pp. 584--601", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work focuses on the simulation of $CO_2$ storage in deep underground\nformations under uncertainty and seeks to understand the impact of\nuncertainties in reservoir properties on $CO_2$ leakage. To simulate the\nprocess, a non-isothermal two-phase two-component flow system with equilibrium\nphase exchange is used. Since model evaluations are computationally intensive,\ninstead of traditional Monte Carlo methods, we rely on polynomial chaos (PC)\nexpansions for representation of the stochastic model response. A non-intrusive\napproach is used to determine the PC coefficients. We establish the accuracy of\nthe PC representations within a reasonable error threshold through systematic\nconvergence studies. In addition to characterizing the distributions of model\nobservables, we compute probabilities of excess $CO_2$ leakage. Moreover, we\nconsider the injection rate as a design parameter and compute an optimum\ninjection rate that ensures that the risk of excess pressure buildup at the\nleaky well remains below acceptable levels. We also provide a comprehensive\nanalysis of sensitivities of $CO_2$ leakage, where we compute the contributions\nof the random parameters, and their interactions, to the variance by computing\nfirst, second, and total order Sobol' indices.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2016 20:41:23 GMT"}, {"version": "v2", "created": "Thu, 23 Mar 2017 20:51:19 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 14:41:45 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 03:22:17 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Saad", "Bilal", ""], ["Alexanderian", "Alen", ""], ["Prudhomme", "Serge", ""], ["Knio", "Omar M.", ""]]}, {"id": "1602.03572", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, F. Louzada", "title": "Effective Sample Size for Importance Sampling based on discrepancy\n  measures", "comments": null, "journal-ref": "Signal Processing, Volume 131, Pages: 386-401, 2017", "doi": "10.1016/j.sigpro.2016.08.025", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Effective Sample Size (ESS) is an important measure of efficiency of\nMonte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance\nSampling (IS) techniques. In the IS context, an approximation $\\widehat{ESS}$\nof the theoretical ESS definition is widely applied, involving the inverse of\nthe sum of the squares of the normalized importance weights. This formula,\n$\\widehat{ESS}$, has become an essential piece within Sequential Monte Carlo\n(SMC) methods, to assess the convenience of a resampling step. From another\nperspective, the expression $\\widehat{ESS}$ is related to the Euclidean\ndistance between the probability mass described by the normalized weights and\nthe discrete uniform probability mass function (pmf). In this work, we derive\nother possible ESS functions based on different discrepancy measures between\nthese two pmfs. Several examples are provided involving, for instance, the\ngeometric mean of the weights, the discrete entropy (including theperplexity\nmeasure, already proposed in literature) and the Gini coefficient among others.\nWe list five theoretical requirements which a generic ESS function should\nsatisfy, allowing us to classify different ESS measures. We also compare the\nmost promising ones by means of numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 23:24:46 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 14:22:52 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2016 12:27:30 GMT"}, {"version": "v4", "created": "Sat, 5 Mar 2016 15:31:20 GMT"}, {"version": "v5", "created": "Sun, 25 Sep 2016 11:21:22 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Louzada", "F.", ""]]}, {"id": "1602.03658", "submitter": "Kainan Wang", "authors": "Kainan Wang, Tan Bui-Thanh, Omar Ghattas", "title": "A randomized maximum a posterior method for posterior sampling of high\n  dimensional nonlinear Bayesian inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomized maximum a posteriori (rMAP) method for generating\napproximate samples of posteriors in high dimensional Bayesian inverse problems\ngoverned by large-scale forward problems. We derive the rMAP approach by: 1)\ncasting the problem of computing the MAP point as a stochastic optimization\nproblem; 2) interchanging optimization and expectation; and 3) approximating\nthe expectation with a Monte Carlo method. For a specific randomized data and\nprior mean, rMAP reduces to the maximum likelihood approach (RML). It can also\nbe viewed as an iterative stochastic Newton method. An analysis of the\nconvergence of the rMAP samples is carried out for both linear and nonlinear\ninverse problems. Each rMAP sample requires solution of a PDE-constrained\noptimization problem; to solve these problems, we employ a state-of-the-art\ntrust region inexact Newton conjugate gradient method with sensitivity-based\nwarm starts. An approximate Metropolization approach is presented to reduce the\nbias in rMAP samples. Various numerical methods will be presented to\ndemonstrate the potential of the rMAP approach in posterior sampling of\nnonlinear Bayesian inverse problems in high dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 10:09:08 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Wang", "Kainan", ""], ["Bui-Thanh", "Tan", ""], ["Ghattas", "Omar", ""]]}, {"id": "1602.03692", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan", "title": "Maximum Likelihood Estimation of Triangular and Polygonal Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Triangular distributions are a well-known class of distributions that are\noften used as elementary example of a probability model. In the past,\nenumeration and order statistic-based methods have been suggested for the\nmaximum likelihood (ML) estimation of such distributions. A novel\nparametrization of triangular distributions is presented. The parametrization\nallows for the construction of an MM (minorization--maximization) algorithm for\nthe ML estimation of triangular distributions. The algorithm is shown to both\nmonotonically increase the likelihood evaluations, and be globally convergent.\nUsing the parametrization is then applied to construct an MM algorithm for the\nML estimation of polygonal distributions. This algorithm is shown to have the\nsame numerical properties as that of the triangular distribution. Numerical\nsimulation are provided to demonstrate the performances of the new algorithms\nagainst established enumeration and order statistics-based methods.\n", "versions": [{"version": "v1", "created": "Thu, 11 Feb 2016 12:10:25 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 23:48:07 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1602.03938", "submitter": "Simon Mak", "authors": "Simon Mak, V. Roshan Joseph", "title": "Minimax and minimax projection designs using clustering", "comments": "Under revision, Journal of Computational and Graphical Statistics\n  (JCGS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimax designs provide a uniform coverage of a design space $\\mathcal{X}\n\\subseteq \\mathbb{R}^p$ by minimizing the maximum distance from any point in\nthis space to its nearest design point. Although minimax designs have many\nuseful applications, e.g., for optimal sensor allocation or as space-filling\ndesigns for computer experiments, there has been little work in developing\nalgorithms for generating these designs, due to its computational complexity.\nIn this paper, a new hybrid algorithm combining particle swarm optimization and\nclustering is proposed for generating minimax designs on any convex and bounded\ndesign space. The computation time of this algorithm scales linearly in\ndimension $p$, meaning our method can generate minimax designs efficiently for\nhigh-dimensional regions. Simulation studies and a real-world example show that\nthe proposed algorithm provides improved minimax performance over existing\nmethods on a variety of design spaces. Finally, we introduce a new type of\nexperimental design called a minimax projection design, and show that this\nproposed design provides better minimax performance on projected subspaces of\n$\\mathcal{X}$ compared to existing designs. An efficient implementation of\nthese algorithms can be found in the R package minimaxdesign.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 01:04:54 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 17:46:50 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 21:03:10 GMT"}], "update_date": "2016-11-01", "authors_parsed": [["Mak", "Simon", ""], ["Joseph", "V. Roshan", ""]]}, {"id": "1602.03990", "submitter": "Li Ma", "authors": "Li Ma and Jacopo Soriano", "title": "Efficient functional ANOVA through wavelet-domain Markov groves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a wavelet-domain functional analysis of variance (fANOVA) method\nbased on a Bayesian hierarchical model. The factor effects are modeled through\na spike-and-slab mixture at each location-scale combination along with a\nnormal-inverse-Gamma (NIG) conjugate setup for the coefficients and errors. A\ngraphical model called the Markov grove (MG) is designed to jointly model the\nspike-and-slab statuses at all location-scale combinations, which incorporates\nthe clustering of each factor effect in the wavelet-domain thereby allowing\nborrowing of strength across location and scale. The posterior of this NIG-MG\nmodel is analytically available through a pyramid algorithm of the same\ncomputational complexity as Mallat's pyramid algorithm for discrete wavelet\ntransform, i.e., linear in both the number of observations and the number of\nlocations. Posterior probabilities of factor contributions can also be computed\nthrough pyramid recursion, and exact samples from the posterior can be drawn\nwithout MCMC. We investigate the performance of our method through extensive\nsimulation and show that it outperforms existing wavelet-domain fANOVA methods\nin a variety of common settings. We apply the method to analyzing the orthosis\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 09:31:34 GMT"}, {"version": "v2", "created": "Sun, 21 Feb 2016 03:56:08 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Ma", "Li", ""], ["Soriano", "Jacopo", ""]]}, {"id": "1602.04060", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over and Tim Friede", "title": "Discrete approximation of a mixture distribution via restricted\n  divergence", "comments": "16 pages, 4 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 26(1):217-222\n  (2017)", "doi": "10.1080/10618600.2016.1276840", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture distributions arise in many application areas, for example as\nmarginal distributions or convolutions of distributions. We present a method of\nconstructing an easily tractable discrete mixture distribution as an\napproximation to a mixture distribution with a large to infinite number,\ndiscrete or continuous, of components. The proposed DIRECT (Divergence\nRestricting Conditional Tesselation) algorithm is set up such that a\npre-specified precision, defined in terms of Kullback-Leibler divergence\nbetween true distribution and approximation, is guaranteed. Application of the\nalgorithm is demonstrated in two examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 14:08:42 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 08:42:26 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Friede", "Tim", ""]]}, {"id": "1602.04439", "submitter": "Sean Malory Mr", "authors": "Sean Malory and Chris Sherlock", "title": "Residual-Bridge Constructs for Conditioned Diffusions", "comments": "Includes 34 pages, 13 figures, 1 table, an extended simulation study\n  and a more detailed explanation of proposals", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new residual-bridge proposal for approximately simulating\nconditioned diffusions. This proposal is formed by applying the modified\ndiffusion bridge approximation of Durham and Gallant (2002) to the difference\nbetween the true diffusion and a second, approximate diffusion driven by the\nsame Brownian motion, and can be viewed as a natural extension to recent work\non residual-bridge constructs (Whitaker et al., 2016). This new proposal\nattempts to account for volatilities which are not constant and can therefore\nlead to gains in efficiency over the recently proposed residual-bridge\nconstructs in situations where the volatility varies considerably, as is often\nthe case for larger inter-observation times and for time-inhomogeneous\nvolatilities. These potential gains in efficiencies are illustrated via a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2016 11:04:11 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2016 09:57:19 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Malory", "Sean", ""], ["Sherlock", "Chris", ""]]}, {"id": "1602.04805", "submitter": "Jovana Mitrovic", "authors": "Jovana Mitrovic, Dino Sejdinovic, Yee Whye Teh", "title": "DR-ABC: Approximate Bayesian Computation with Kernel-Based Distribution\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact posterior inference in complex generative models is often\ndifficult or impossible due to an expensive to evaluate or intractable\nlikelihood function. Approximate Bayesian computation (ABC) is an inference\nframework that constructs an approximation to the true likelihood based on the\nsimilarity between the observed and simulated data as measured by a predefined\nset of summary statistics. Although the choice of appropriate problem-specific\nsummary statistics crucially influences the quality of the likelihood\napproximation and hence also the quality of the posterior sample in ABC, there\nare only few principled general-purpose approaches to the selection or\nconstruction of such summary statistics. In this paper, we develop a novel\nframework for this task using kernel-based distribution regression. We model\nthe functional relationship between data distributions and the optimal choice\n(with respect to a loss function) of summary statistics using kernel-based\ndistribution regression. We show that our approach can be implemented in a\ncomputationally and statistically efficient way using the random Fourier\nfeatures framework for large-scale kernel learning. In addition to that, our\nframework shows superior performance when compared to related methods on toy\nand real-world problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2016 20:55:57 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Mitrovic", "Jovana", ""], ["Sejdinovic", "Dino", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1602.05023", "submitter": "Alessio Spantini", "authors": "Youssef Marzouk, Tarek Moselhy, Matthew Parno, and Alessio Spantini", "title": "An introduction to sampling via measure transport", "comments": "To appear in Handbook of Uncertainty Quantification; R. Ghanem, D.\n  Higdon, and H. Owhadi, editors; Springer, 2016", "journal-ref": null, "doi": "10.1007/978-3-319-11259-6_23-1", "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the fundamentals of a measure transport approach to sampling. The\nidea is to construct a deterministic coupling---i.e., a transport map---between\na complex \"target\" probability measure of interest and a simpler reference\nmeasure. Given a transport map, one can generate arbitrarily many independent\nand unweighted samples from the target simply by pushing forward reference\nsamples through the map. We consider two different and complementary scenarios:\nfirst, when only evaluations of the unnormalized target density are available,\nand second, when the target distribution is known only through a finite\ncollection of samples. We show that in both settings the desired transports can\nbe characterized as the solutions of variational problems. We then address\npractical issues associated with the optimization--based construction of\ntransports: choosing finite-dimensional parameterizations of the map, enforcing\nmonotonicity, quantifying the error of approximate transports, and refining\napproximate transports by enriching the corresponding approximation spaces.\nApproximate transports can also be used to \"Gaussianize\" complex distributions\nand thus precondition conventional asymptotically exact sampling schemes. We\nplace the measure transport approach in broader context, describing connections\nwith other optimization--based samplers, with inference and density estimation\nschemes using optimal transport, and with alternative transformation--based\napproaches to simulation. We also sketch current work aimed at the construction\nof transport maps in high dimensions, exploiting essential features of the\ntarget distribution (e.g., conditional independence, low-rank structure). The\napproaches and algorithms presented here have direct applications to Bayesian\ncomputation and to broader problems of stochastic simulation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 14:10:57 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Marzouk", "Youssef", ""], ["Moselhy", "Tarek", ""], ["Parno", "Matthew", ""], ["Spantini", "Alessio", ""]]}, {"id": "1602.05128", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Christian A. Naesseth, Fredrik Lindsten, Brooks Paige,\n  Jan-Willem van de Meent, Arnaud Doucet, Frank Wood", "title": "Interacting Particle Markov Chain Monte Carlo", "comments": null, "journal-ref": "JMLR W&CP 48 : 2616-2625, 2016", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce interacting particle Markov chain Monte Carlo (iPMCMC), a PMCMC\nmethod based on an interacting pool of standard and conditional sequential\nMonte Carlo samplers. Like related methods, iPMCMC is a Markov chain Monte\nCarlo sampler on an extended space. We present empirical results that show\nsignificant improvements in mixing rates relative to both non-interacting PMCMC\nsamplers, and a single PMCMC sampler with an equivalent memory and\ncomputational budget. An additional advantage of the iPMCMC method is that it\nis suitable for distributed and multi-core architectures.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 18:36:19 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 18:05:09 GMT"}, {"version": "v3", "created": "Wed, 12 Apr 2017 14:10:58 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Rainforth", "Tom", ""], ["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Paige", "Brooks", ""], ["van de Meent", "Jan-Willem", ""], ["Doucet", "Arnaud", ""], ["Wood", "Frank", ""]]}, {"id": "1602.05208", "submitter": "Nathaniel Helwig", "authors": "Nathaniel E. Helwig and Ping Ma", "title": "Smoothing spline ANOVA for super-large samples: Scalable computation via\n  rounding parameters", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era of big data, researchers routinely collect and analyze\ndata of super-large sample sizes. Data-oriented statistical methods have been\ndeveloped to extract information from super-large data. Smoothing spline ANOVA\n(SSANOVA) is a promising approach for extracting information from noisy data;\nhowever, the heavy computational cost of SSANOVA hinders its wide application.\nIn this paper, we propose a new algorithm for fitting SSANOVA models to\nsuper-large sample data. In this algorithm, we introduce rounding parameters to\nmake the computation scalable. To demonstrate the benefits of the rounding\nparameters, we present a simulation study and a real data example using\nelectroencephalography data. Our results reveal that (using the rounding\nparameters) a researcher can fit nonparametric regression models to very large\nsamples within a few seconds using a standard laptop or tablet computer.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 21:21:05 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Helwig", "Nathaniel E.", ""], ["Ma", "Ping", ""]]}, {"id": "1602.05238", "submitter": "Jiming Jiang", "authors": "Jiming Jiang, P. Lahiri, Thuan Nguyen", "title": "A Unified Monte-Carlo Jackknife for Small Area Estimation after Model\n  Selection", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of measure of uncertainty in small area estimation\n(SAE) when a procedure of model selection is involved prior to the estimation.\nA unified Monte-Carlo jackknife method, called McJack, is proposed for\nestimating the logarithm of the mean squared prediction error. We prove the\nsecond-order unbiasedness of McJack, and demonstrate the performance of McJack\nin assessing uncertainty in SAE after model selection through empirical\ninvestigations that include simulation studies and real-data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2016 23:01:28 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Jiang", "Jiming", ""], ["Lahiri", "P.", ""], ["Nguyen", "Thuan", ""]]}, {"id": "1602.05762", "submitter": "Anna Gloria Bill\\'e Ph.D.", "authors": "Anna Gloria Bill\\'e, Cristina Salvioni, Roberto Benedetti", "title": "Modelling Spatial Regimes in Farms Technologies", "comments": null, "journal-ref": "Journal of Productivity Analysis 2018", "doi": "10.1007/s11123-018-0529-7", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit the information derived from geographical coordinates to\nendogenously identify spatial regimes in technologies that are the result of a\nvariety of complex, dynamic interactions among site-specific environmental\nvariables and farmer decision making about technology, which are often not\nobserved at the farm level. Controlling for unobserved heterogeneity is a\nfundamental challenge in empirical research, as failing to do so can produce\nmodel misspecification and preclude causal inference. In this article, we adopt\na two-step procedure to deal with unobserved spatial heterogeneity, while\naccounting for spatial dependence in a cross-sectional setting. The first step\nof the procedure takes explicitly unobserved spatial heterogeneity into account\nto endogenously identify subsets of farms that follow a similar local\nproduction econometric model, i.e. spatial production regimes. The second step\nconsists in the specification of a spatial autoregressive model with\nautoregressive disturbances and spatial regimes. The method is applied to two\nregional samples of olive growing farms in Italy. The main finding is that the\nidentification of spatial regimes can help drawing a more detailed picture of\nthe production environment and provide more accurate information to guide\nextension services and policy makers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 11:33:57 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 16:56:08 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 09:47:31 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Bill\u00e9", "Anna Gloria", ""], ["Salvioni", "Cristina", ""], ["Benedetti", "Roberto", ""]]}, {"id": "1602.05986", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison", "title": "A Poisson process model for Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulating samples from arbitrary probability distributions is a major\nresearch program of statistical computing. Recent work has shown promise in an\nold idea, that sampling from a discrete distribution can be accomplished by\nperturbing and maximizing its mass function. Yet, it has not been clearly\nexplained how this research project relates to more traditional ideas in the\nMonte Carlo literature. This chapter addresses that need by identifying a\nPoisson process model that unifies the perturbation and accept-reject views of\nMonte Carlo simulation. Many existing methods can be analyzed in this\nframework. The chapter reviews Poisson processes and defines a Poisson process\nmodel for Monte Carlo methods. This model is used to generalize the\nperturbation trick to infinite spaces by constructing Gumbel processes, random\nfunctions whose maxima are located at samples over infinite spaces. The model\nis also used to analyze A* sampling and OS*, two methods from distinct Monte\nCarlo families.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 21:57:12 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 14:30:40 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Maddison", "Chris J.", ""]]}, {"id": "1602.06030", "submitter": "Radford M. Neal", "authors": "Alexander Y. Shestopaloff and Radford M. Neal", "title": "Sampling latent states for high-dimensional non-linear state space\n  models with the embedded HMM method", "comments": "Revision has some changes to the paper, and now includes the program\n  used as ancillary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new scheme for selecting pool states for the embedded Hidden\nMarkov Model (HMM) Markov Chain Monte Carlo (MCMC) method. This new scheme\nallows the embedded HMM method to be used for efficient sampling in state space\nmodels where the state can be high-dimensional. Previously, embedded HMM\nmethods were only applied to models with a one-dimensional state space. We\ndemonstrate that using our proposed pool state selection scheme, an embedded\nHMM sampler can have similar performance to a well-tuned sampler that uses a\ncombination of Particle Gibbs with Backward Sampling (PGBS) and Metropolis\nupdates. The scaling to higher dimensions is made possible by selecting pool\nstates locally near the current value of the state sequence. The proposed pool\nstate selection scheme also allows each iteration of the embedded HMM sampler\nto take time linear in the number of the pool states, as opposed to quadratic\nas in the original embedded HMM sampler. We also consider a model with a\nmultimodal posterior, and show how a technique we term \"mirroring\" can be used\nto efficiently move between the modes.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 02:53:44 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 16:01:59 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Shestopaloff", "Alexander Y.", ""], ["Neal", "Radford M.", ""]]}, {"id": "1602.06218", "submitter": "Joseph Hart", "authors": "Joseph L. Hart, Alen Alexanderian, and Pierre A. Gremaud", "title": "Efficient computation of Sobol' indices for stochastic models", "comments": "Minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models are necessary for the realistic description of an\nincreasing number of applications. The ability to identify influential\nparameters and variables is critical to a thorough analysis and understanding\nof the underlying phenomena. We present a new global sensitivity analysis\napproach for stochastic models, i.e., models with both uncertain parameters and\nintrinsic stochasticity. Our method relies on an analysis of variance through a\ngeneralization of Sobol' indices and on the use of surrogate models. We show\nhow to efficiently compute the statistical properties of the resulting indices\nand illustrate the effectiveness of our approach by computing first order\nSobol' indices for two stochastic models.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 16:51:19 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 16:08:39 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Hart", "Joseph L.", ""], ["Alexanderian", "Alen", ""], ["Gremaud", "Pierre A.", ""]]}, {"id": "1602.06225", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye, Olivier Fercoq, Alexandre Gramfort, Joseph Salmon", "title": "GAP Safe Screening Rules for Sparse-Group-Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional settings, sparse structures are crucial for efficiency,\neither in term of memory, computation or performance. In some contexts, it is\nnatural to handle more refined structures than pure sparsity, such as for\ninstance group sparsity. Sparse-Group Lasso has recently been introduced in the\ncontext of linear regression to enforce sparsity both at the feature level and\nat the group level. We adapt to the case of Sparse-Group Lasso recent safe\nscreening rules that discard early in the solver irrelevant features/groups.\nSuch rules have led to important speed-ups for a wide range of iterative\nmethods. Thanks to dual gap computations, we provide new safe screening rules\nfor Sparse-Group Lasso and show significant gains in term of computing time for\na coordinate descent implementation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 17:08:34 GMT"}], "update_date": "2016-02-24", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Fercoq", "Olivier", ""], ["Gramfort", "Alexandre", ""], ["Salmon", "Joseph", ""]]}, {"id": "1602.06693", "submitter": "Maurizio Filippone", "authors": "Kurt Cutajar, Michael A. Osborne, John P. Cunningham, Maurizio\n  Filippone", "title": "Preconditioning Kernel Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational and storage complexity of kernel machines presents the\nprimary barrier to their scaling to large, modern, datasets. A common way to\ntackle the scalability issue is to use the conjugate gradient algorithm, which\nrelieves the constraints on both storage (the kernel matrix need not be stored)\nand computation (both stochastic gradients and parallelization can be used).\nEven so, conjugate gradient is not without its own issues: the conditioning of\nkernel matrices is often such that conjugate gradients will have poor\nconvergence in practice. Preconditioning is a common approach to alleviating\nthis issue. Here we propose preconditioned conjugate gradients for kernel\nmachines, and develop a broad range of preconditioners particularly useful for\nkernel matrices. We describe a scalable approach to both solving kernel\nmachines and learning their hyperparameters. We show this approach is exact in\nthe limit of iterations and outperforms state-of-the-art approximations for a\ngiven computational budget.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2016 09:21:48 GMT"}, {"version": "v2", "created": "Wed, 25 May 2016 08:05:52 GMT"}], "update_date": "2016-05-26", "authors_parsed": [["Cutajar", "Kurt", ""], ["Osborne", "Michael A.", ""], ["Cunningham", "John P.", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1602.07527", "submitter": "Iain Murray", "authors": "Iain Murray", "title": "Differentiation of the Cholesky decomposition", "comments": "18 pages, including 7 pages of code listings", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review strategies for differentiating matrix-based computations, and\nderive symbolic and algorithmic update rules for differentiating expressions\ncontaining the Cholesky decomposition. We recommend new `blocked' algorithms,\nbased on differentiating the Cholesky algorithm DPOTRF in the LAPACK library,\nwhich uses `Level 3' matrix-matrix operations from BLAS, and so is\ncache-friendly and easy to parallelize. For large matrices, the resulting\nalgorithms are the fastest way to compute Cholesky derivatives, and are an\norder of magnitude faster than the algorithms in common usage. In some\ncomputing environments, symbolically-derived updates are faster for small\nmatrices than those based on differentiating Cholesky algorithms. The symbolic\nand algorithmic approaches can be combined to get the best of both worlds.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 14:35:31 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Murray", "Iain", ""]]}, {"id": "1602.07587", "submitter": "Jean-Benoist Leger", "authors": "Jean-Benoist Leger", "title": "Blockmodels: A R-package for estimating in Latent Block Model and\n  Stochastic Block Model, with various probability functions, with or without\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of the topology of a graph, regular or bipartite one, can be done by\nclustering for regular ones or co-clustering for bipartite ones. The Stochastic\nBlock Model and the Latent Block Model are two models, which are very similar\nfor respectively regular and bipartite graphs, based on probabilistic models.\nInitially developed for binary graphs, these models have been extended to\nvalued networks with optional covariates on the edges. This paper present a\nimplementation of a Variational EM algorithm for Stochastic Block Model and\nLatent Block Model for some common probability functions, Bernoulli, Gaussian\nand Poisson, without or with covariates, with some standard flavors, like\nmultivariate extensions. This implementation allow automatic group number\nexploration and selection via the ICL criterion, and allow analyze networks\nwith thousands of nodes in a reasonable amount of time.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 16:41:33 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Leger", "Jean-Benoist", ""]]}, {"id": "1602.07640", "submitter": "Feng Liang", "authors": "Xichen Huang, Jin Wang, Feng Liang", "title": "A Variational Algorithm for Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an intense development on the estimation of a sparse\nregression coefficient vector in statistics, machine learning and related\nfields. In this paper, we focus on the Bayesian approach to this problem, where\nsparsity is incorporated by the so-called spike-and-slab prior on the\ncoefficients. Instead of replying on MCMC for posterior inference, we propose a\nfast and scalable algorithm based on variational approximation to the posterior\ndistribution. The updating scheme employed by our algorithm is different from\nthe one proposed by Carbonetto and Stephens (2012). Those changes seem crucial\nfor us to show that our algorithm can achieve asymptotic consistency even when\nthe feature dimension diverges exponentially fast with the sample size.\nEmpirical results have demonstrated the effectiveness and efficiency of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 19:16:36 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Huang", "Xichen", ""], ["Wang", "Jin", ""], ["Liang", "Feng", ""]]}, {"id": "1602.08154", "submitter": "Gregor Kastner", "authors": "Gregor Kastner, Sylvia Fr\\\"uhwirth-Schnatter, Hedibert Freitas Lopes", "title": "Efficient Bayesian Inference for Multivariate Factor Stochastic\n  Volatility Models", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics 26(4), 905-917\n  (2017)", "doi": "10.1080/10618600.2017.1322091", "report-no": null, "categories": "stat.CO econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss efficient Bayesian estimation of dynamic covariance matrices in\nmultivariate time series through a factor stochastic volatility model. In\nparticular, we propose two interweaving strategies (Yu and Meng, Journal of\nComputational and Graphical Statistics, 20(3), 531-570, 2011) to substantially\naccelerate convergence and mixing of standard MCMC approaches. Similar to\nmarginal data augmentation techniques, the proposed acceleration procedures\nexploit non-identifiability issues which frequently arise in factor models. Our\nnew interweaving strategies are easy to implement and come at almost no extra\ncomputational cost; nevertheless, they can boost estimation efficiency by\nseveral orders of magnitude as is shown in extensive simulation studies. To\nconclude, the application of our algorithm to a 26-dimensional exchange rate\ndata set illustrates the superior performance of the new approach for\nreal-world data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 00:01:27 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:07:01 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 13:11:14 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Kastner", "Gregor", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Lopes", "Hedibert Freitas", ""]]}, {"id": "1602.08734", "submitter": "Tim Salimans", "authors": "Tim Salimans", "title": "A Structured Variational Auto-encoder for Learning Deep Hierarchies of\n  Sparse Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we present a generative model of natural images consisting of a\ndeep hierarchy of layers of latent random variables, each of which follows a\nnew type of distribution that we call rectified Gaussian. These rectified\nGaussian units allow spike-and-slab type sparsity, while retaining the\ndifferentiability necessary for efficient stochastic gradient variational\ninference. To learn the parameters of the new model, we approximate the\nposterior of the latent variables with a variational auto-encoder. Rather than\nmaking the usual mean-field assumption however, the encoder parameterizes a new\ntype of structured variational approximation that retains the prior\ndependencies of the generative model. Using this structured posterior\napproximation, we are able to perform joint training of deep models with many\nlayers of latent random variables, without having to resort to stacking or\nother layerwise training procedures.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2016 16:10:40 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Salimans", "Tim", ""]]}, {"id": "1602.08787", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan, Pierre Orban, Pierre Bellec,\n  Andrew L Janke", "title": "Maximum Pseudolikelihood Estimation for Model-Based Clustering of Time\n  Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of autoregressions (MoAR) models provide a model-based approach to\nthe clustering of time series data. The maximum likelihood (ML) estimation of\nMoAR models requires the evaluation of products of large numbers of densities\nof normal random variables. In practical scenarios, these products converge to\nzero as the length of the time series increases, and thus the ML estimation of\nMoAR models becomes infeasible without the use of numerical tricks. We propose\na maximum pseudolikelihood (MPL) estimation approach as an alternative to the\nuse of numerical tricks. The MPL estimator is proved to be consistent and can\nbe computed via an EM (expectation--maximization) algorithm. Simulations are\nused to assess the performance of the MPL estimator against that of the ML\nestimator in cases where the latter was able to be calculated. An application\nto the clustering of time series data arising from a resting-state fMRI\nexperiment is presented as a demonstration of the methodology.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 00:11:22 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 03:23:32 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""], ["Orban", "Pierre", ""], ["Bellec", "Pierre", ""], ["Janke", "Andrew L", ""]]}, {"id": "1602.08849", "submitter": "Meng Hwee Victor Ong", "authors": "Meng Hwee Victor Ong, David J. Nott, Ajay Jasra", "title": "Flexible online multivariate regression with variational Bayes and the\n  matrix-variate Dirichlet process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flexible regression methods where interest centres on the way that the whole\ndistribution of a response vector changes with covariates are very useful in\nsome applications. A recently developed technique in this regard uses the\nmatrix-variate Dirichlet process as a prior for a mixing distribution on a\ncoefficient in a multivariate linear regression model. The method is\nattractive, particularly in the multivariate setting, for the convenient way\nthat it allows for borrowing strength across different component regressions\nand for its computational simplicity and tractability. The purpose of the\npresent article is to develop fast online variational Bayes approaches to\nfitting this model and to investigate how they perform compared to MCMC and\nbatch variational methods in a number of scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 08:03:48 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Ong", "Meng Hwee Victor", ""], ["Nott", "David J.", ""], ["Jasra", "Ajay", ""]]}]