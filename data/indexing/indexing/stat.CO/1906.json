[{"id": "1906.00031", "submitter": "Daniele Bigoni", "authors": "Michael C. Brennan and Daniele Bigoni and Olivier Zahm and Alessio\n  Spantini and Youssef Marzouk", "title": "Greedy inference with structure-exploiting lazy maps", "comments": "21 pages, 37 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for solving high-dimensional Bayesian inference\nproblems using \\emph{structure-exploiting} low-dimensional transport maps or\nflows. These maps are confined to a low-dimensional subspace (hence, lazy), and\nthe subspace is identified by minimizing an upper bound on the\nKullback--Leibler divergence (hence, structured). Our framework provides a\nprincipled way of identifying and exploiting low-dimensional structure in an\ninference problem. It focuses the expressiveness of a transport map along the\ndirections of most significant discrepancy from the posterior, and can be used\nto build deep compositions of lazy maps, where low-dimensional projections of\nthe parameters are iteratively transformed to match the posterior. We prove\nweak convergence of the generated sequence of distributions to the posterior,\nand we demonstrate the benefits of the framework on challenging inference\nproblems in machine learning and differential equations, using inverse\nautoregressive flows and polynomial maps as examples of the underlying density\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 18:52:21 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 06:20:34 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 21:37:44 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Brennan", "Michael C.", ""], ["Bigoni", "Daniele", ""], ["Zahm", "Olivier", ""], ["Spantini", "Alessio", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1906.00051", "submitter": "Fan Yang", "authors": "Zheng Tracy Ke, Lingzhou Xue and Fan Yang", "title": "Diagonally-Dominant Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of decomposing a large covariance matrix into the sum\nof a low-rank matrix and a diagonally dominant matrix, and we call this problem\nthe \"Diagonally-Dominant Principal Component Analysis (DD-PCA)\". DD-PCA is an\neffective tool for designing statistical methods for strongly correlated data.\nWe showcase the use of DD-PCA in two statistical problems: covariance matrix\nestimation, and global detection in multiple testing. Using the output of\nDD-PCA, we propose a new estimator for estimating a large covariance matrix\nwith factor structure. Thanks to a nice property of diagonally dominant\nmatrices, this estimator enjoys the advantage of simultaneous good estimation\nof the covariance matrix and the precision matrix (by a plain inversion). A\nplug-in of this estimator to linear discriminant analysis and portfolio\noptimization yields appealing performance in real data. We also propose two new\ntests for testing the global null hypothesis in multiple testing when the\n$z$-scores have a factor covariance structure. Both tests first use DD-PCA to\nadjust the individual $p$-values and then plug in the adjusted $p$-values to\nthe Higher Criticism (HC) test. These new tests significantly improve over the\nHC test and compare favorably with other existing tests. For computation of\nDD-PCA, we propose an iterative projection algorithm and an ADMM algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 20:02:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Xue", "Lingzhou", ""], ["Yang", "Fan", ""]]}, {"id": "1906.00198", "submitter": "Matias Cattaneo", "authors": "Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell", "title": "nprobust: Nonparametric Kernel-Based Estimation and Robust\n  Bias-Corrected Inference", "comments": null, "journal-ref": "Journal of Statistical Software, 91(8): 1-33, 2019", "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric kernel density and local polynomial regression estimators are\nvery popular in Statistics, Economics, and many other disciplines. They are\nroutinely employed in applied work, either as part of the main empirical\nanalysis or as a preliminary ingredient entering some other estimation or\ninference procedure. This article describes the main methodological and\nnumerical features of the software package nprobust, which offers an array of\nestimation and inference procedures for nonparametric kernel-based density and\nlocal polynomial regression methods, implemented in both the R and Stata\nstatistical platforms. The package includes not only classical bandwidth\nselection, estimation, and inference methods (Wand and Jones, 1995; Fan and\nGijbels, 1996), but also other recent developments in the statistics and\neconometrics literatures such as robust bias-corrected inference and coverage\nerror optimal bandwidth selection (Calonico, Cattaneo and Farrell, 2018, 2019).\nFurthermore, this article also proposes a simple way of estimating optimal\nbandwidths in practice that always delivers the optimal mean square error\nconvergence rate regardless of the specific evaluation point, that is, no\nmatter whether it is implemented at a boundary or interior point. Numerical\nperformance is illustrated using an empirical application and simulated data,\nwhere a detailed numerical comparison with other R packages is given.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:33:04 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Calonico", "Sebastian", ""], ["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""]]}, {"id": "1906.00202", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Max H. Farrell, Yingjie Feng", "title": "lspartition: Partitioning-Based Least Squares Regression", "comments": null, "journal-ref": "R Journal 12(1): 172-187, 2020", "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric partitioning-based least squares regression is an important\ntool in empirical work. Common examples include regressions based on splines,\nwavelets, and piecewise polynomials. This article discusses the main\nmethodological and numerical features of the R software package lspartition,\nwhich implements modern estimation and inference results for partitioning-based\nleast squares (series) regression estimation. This article discusses the main\nmethodological and numerical features of the R software package lspartition,\nwhich implements results for partitioning-based least squares (series)\nregression estimation and inference from Cattaneo and Farrell (2013) and\nCattaneo, Farrell, and Feng (2019). These results cover the multivariate\nregression function as well as its derivatives. First, the package provides\ndata-driven methods to choose the number of partition knots optimally,\naccording to integrated mean squared error, yielding optimal point estimation.\nSecond, robust bias correction is implemented to combine this point estimator\nwith valid inference. Third, the package provides estimates and inference for\nthe unknown function both pointwise and uniformly in the conditioning\nvariables. In particular, valid confidence bands are provided. Finally, an\nextension to two-sample analysis is developed, which can be used in\ntreatment-control comparisons and related problems\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 10:56:29 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 10:53:29 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""], ["Feng", "Yingjie", ""]]}, {"id": "1906.00348", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis", "title": "Clustering Multivariate Data using Factor Analytic Bayesian Mixtures\n  with an Unknown Number of Components", "comments": "Revised version (22 pages)", "journal-ref": "Statistics and Computing, 2019", "doi": "10.1007/s11222-019-09891-z", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on overfitting Bayesian mixtures of distributions offers a\npowerful framework for clustering multivariate data using a latent Gaussian\nmodel which resembles the factor analysis model. The flexibility provided by\noverfitting mixture models yields a simple and efficient way in order to\nestimate the unknown number of clusters and model parameters by Markov chain\nMonte Carlo (MCMC) sampling. The present study extends this approach by\nconsidering a set of eight parameterizations, giving rise to parsimonious\nrepresentations of the covariance matrix per cluster. A Gibbs sampler combined\nwith a prior parallel tempering scheme is implemented in order to approximately\nsample from the posterior distribution of the overfitting mixture. The\nparameterization and number of factors is selected according to the Bayesian\nInformation Criterion. Identifiability issues related to label switching are\ndealt by post-processing the simulated output with the Equivalence Classes\nRepresentatives algorithm. The contributed method and software are demonstrated\nand compared to similar models estimated using the Expectation-Maximization\nalgorithm on simulated and real datasets. The software is available online at\nhttps://CRAN.R-project.org/package=fabMix.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 05:26:50 GMT"}], "update_date": "2019-08-29", "authors_parsed": [["Papastamoulis", "Panagiotis", ""]]}, {"id": "1906.00507", "submitter": "Matthew Graham", "authors": "Matthew M. Graham, Alexandre H. Thiery", "title": "A scalable optimal-transport based local particle filter", "comments": "55 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering in spatially-extended dynamical systems is a challenging problem\nwith significant practical applications such as numerical weather prediction.\nParticle filters allow asymptotically consistent inference but require\ninfeasibly large ensemble sizes for accurate estimates in complex spatial\nmodels. Localisation approaches, which perform local state updates by\nexploiting low dependence between variables at distant points, have been\nsuggested as a potential resolution to this issue. Naively applying the\nresampling step of the particle filter locally however produces implausible\nspatially discontinuous states. The ensemble transform particle filter replaces\nresampling with an optimal-transport map and can be localised by computing maps\nfor every spatial mesh node. The resulting local ensemble transport particle\nfilter is however computationally intensive for dense meshes. We propose a new\noptimal-transport based local particle filter which computes a fixed number of\nmaps independent of the mesh resolution and interpolates these maps across\nspace, reducing the computation required and allowing it to be ensured\nparticles remain spatially smooth. We numerically illustrate that, at a reduced\ncomputational cost, we are able to achieve the same accuracy as the local\nensemble transport particle filter, and retain its improved robustness to\nnon-Gaussianity and ability to quantify uncertainty when compared to local\nensemble Kalman filters.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 00:01:58 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Graham", "Matthew M.", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1906.00587", "submitter": "Antonio Punzo", "authors": "Luca Bagnato, Antonio Punzo", "title": "Unconstrained representation of orthogonal matrices with application to\n  common principle components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical problems involve the estimation of a $\\left(d\\times\nd\\right)$ orthogonal matrix $\\textbf{Q}$. Such an estimation is often\nchallenging due to the orthonormality constraints on $\\textbf{Q}$. To cope with\nthis problem, we propose a very simple decomposition for orthogonal matrices\nwhich we abbreviate as PLR decomposition. It produces a one-to-one\ncorrespondence between $\\textbf{Q}$ and a $\\left(d\\times d\\right)$ unit lower\ntriangular matrix $\\textbf{L}$ whose $d\\left(d-1\\right)/2$ entries below the\ndiagonal are unconstrained real values. Once the decomposition is applied,\nregardless of the objective function under consideration, we can use any\nclassical unconstrained optimization method to find the minimum (or maximum) of\nthe objective function with respect to $\\textbf{L}$. For illustrative purposes,\nwe apply the PLR decomposition in common principle components analysis (CPCA)\nfor the maximum likelihood estimation of the common orthogonal matrix when a\nmultivariate leptokurtic-normal distribution is assumed in each group. Compared\nto the commonly used normal distribution, the leptokurtic-normal has an\nadditional parameter governing the excess kurtosis; this makes the estimation\nof $\\textbf{Q}$ in CPCA more robust against mild outliers. The usefulness of\nthe PLR decomposition in leptokurtic-normal CPCA is illustrated by two\nbiometric data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 05:58:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bagnato", "Luca", ""], ["Punzo", "Antonio", ""]]}, {"id": "1906.00618", "submitter": "Kevin Tian", "authors": "Arun Jambulapati, Aaron Sidford, Kevin Tian", "title": "A Direct $\\tilde{O}(1/\\epsilon)$ Iteration Parallel Algorithm for\n  Optimal Transport", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transportation, or computing the Wasserstein or ``earth mover's''\ndistance between two distributions, is a fundamental primitive which arises in\nmany learning and statistical settings. We give an algorithm which solves this\nproblem to additive $\\epsilon$ with $\\tilde{O}(1/\\epsilon)$ parallel depth, and\n$\\tilde{O}\\left(n^2/\\epsilon\\right)$ work. Barring a breakthrough on a\nlong-standing algorithmic open problem, this is optimal for first-order\nmethods. Blanchet et. al. '18, Quanrud '19 obtained similar runtimes through\nreductions to positive linear programming and matrix scaling. However, these\nreduction-based algorithms use complicated subroutines which may be deemed\nimpractical due to requiring solvers for second-order iterations (matrix\nscaling) or non-parallelizability (positive LP). The fastest practical\nalgorithms run in time $\\tilde{O}(\\min(n^2 / \\epsilon^2, n^{2.5} / \\epsilon))$\n(Dvurechensky et. al. '18, Lin et. al. '19). We bridge this gap by providing a\nparallel, first-order, $\\tilde{O}(1/\\epsilon)$ iteration algorithm without\nworse dependence on dimension, and provide preliminary experimental evidence\nthat our algorithm may enjoy improved practical performance. We obtain this\nruntime via a primal-dual extragradient method, motivated by recent theoretical\nimprovements to maximum flow (Sherman '17).\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 07:58:09 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Jambulapati", "Arun", ""], ["Sidford", "Aaron", ""], ["Tian", "Kevin", ""]]}, {"id": "1906.01200", "submitter": "Shengjia Zhao", "authors": "Jun-Ting Hsieh and Shengjia Zhao and Stephan Eismann and Lucia\n  Mirabella and Stefano Ermon", "title": "Learning Neural PDE Solvers with Convergence Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial differential equations (PDEs) are widely used across the physical and\ncomputational sciences. Decades of research and engineering went into designing\nfast iterative solution methods. Existing solvers are general purpose, but may\nbe sub-optimal for specific classes of problems. In contrast to existing\nhand-crafted solutions, we propose an approach to learn a fast iterative solver\ntailored to a specific domain. We achieve this goal by learning to modify the\nupdates of an existing solver using a deep neural network. Crucially, our\napproach is proven to preserve strong correctness and convergence guarantees.\nAfter training on a single geometry, our model generalizes to a wide variety of\ngeometries and boundary conditions, and achieves 2-3 times speedup compared to\nstate-of-the-art solvers.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 05:28:22 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Hsieh", "Jun-Ting", ""], ["Zhao", "Shengjia", ""], ["Eismann", "Stephan", ""], ["Mirabella", "Lucia", ""], ["Ermon", "Stefano", ""]]}, {"id": "1906.01235", "submitter": "Trevor Campbell", "authors": "Trevor Campbell and Xinglong Li", "title": "Universal Boosting Variational Inference", "comments": "In Advances in Neural Information Processing Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting variational inference (BVI) approximates an intractable probability\ndensity by iteratively building up a mixture of simple component distributions\none at a time, using techniques from sparse convex optimization to provide both\ncomputational scalability and approximation error guarantees. But the\nguarantees have strong conditions that do not often hold in practice, resulting\nin degenerate component optimization problems; and we show that the ad-hoc\nregularization used to prevent degeneracy in practice can cause BVI to fail in\nunintuitive ways. We thus develop universal boosting variational inference\n(UBVI), a BVI scheme that exploits the simple geometry of probability densities\nunder the Hellinger metric to prevent the degeneracy of other gradient-based\nBVI methods, avoid difficult joint optimizations of both component and weight,\nand simplify fully-corrective weight optimizations. We show that for any target\ndensity and any mixture component family, the output of UBVI converges to the\nbest possible approximation in the mixture family, even when the mixture family\nis misspecified. We develop a scalable implementation based on exponential\nfamily mixture components and standard stochastic optimization techniques.\nFinally, we discuss statistical benefits of the Hellinger distance as a\nvariational objective through bounds on posterior probability, moment, and\nimportance sampling errors. Experiments on multiple datasets and models show\nthat UBVI provides reliable, accurate posterior approximations.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 07:08:50 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 08:24:25 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Campbell", "Trevor", ""], ["Li", "Xinglong", ""]]}, {"id": "1906.01437", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Nhat Ho, Michael I. Jordan", "title": "On the Efficiency of the Sinkhorn and Greenkhorn Algorithms and Their\n  Acceleration for Optimal Transport", "comments": "A preliminary version [arXiv:1901.06482] of this paper, with a subset\n  of the results that are presented here, was presented at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new complexity results for several algorithms that approximately\nsolve the regularized optimal transport (OT) problem between two discrete\nprobability measures with at most $n$ atoms. First, we show that a greedy\nvariant of the classical Sinkhorn algorithm, known as the \\textit{Greenkhorn}\nalgorithm, achieves the complexity bound of\n$\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$, which improves the best known\nbound $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-3})$. Notably, this matches the\nbest known complexity bound of the Sinkhorn algorithm and explains the superior\nperformance of the Greenkhorn algorithm in practice. Furthermore, we generalize\nan adaptive primal-dual accelerated gradient descent (APDAGD) algorithm with\nmirror mapping $\\phi$ and show that the resulting \\textit{adaptive primal-dual\naccelerated mirror descent} (APDAMD) algorithm achieves the complexity bound of\n$\\widetilde{\\mathcal{O}}(n^2\\sqrt{\\delta}\\varepsilon^{-1})$ where $\\delta>0$\ndepends on $\\phi$. We point out that an existing complexity bound for the\nAPDAGD algorithm is not valid in general using a simple counterexample and then\nestablish the complexity bound of\n$\\widetilde{\\mathcal{O}}(n^{5/2}\\varepsilon^{-1})$ by exploiting the connection\nbetween the APDAMD and APDAGD algorithms. Moreover, we introduce accelerated\nSinkhorn and Greenkhorn algorithms that achieve the complexity bound of\n$\\widetilde{\\mathcal{O}}(n^{7/3}\\varepsilon^{-1})$, which improves on the\ncomplexity bounds $\\widetilde{\\mathcal{O}}(n^2\\varepsilon^{-2})$ of Sinkhorn\nand Greenkhorn algorithms in terms of $\\varepsilon$. Experimental results on\nsynthetic and real datasets demonstrate the favorable performance of new\nalgorithms in practice.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 05:33:05 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 07:16:49 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 04:21:58 GMT"}, {"version": "v4", "created": "Sun, 4 Aug 2019 18:15:07 GMT"}, {"version": "v5", "created": "Thu, 17 Oct 2019 23:25:15 GMT"}, {"version": "v6", "created": "Tue, 24 Mar 2020 02:50:47 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Lin", "Tianyi", ""], ["Ho", "Nhat", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1906.01468", "submitter": "David Dias", "authors": "Helder Rojas, David Dias", "title": "Stress Testing Network Reconstruction via Graphical Causal Model", "comments": "arXiv admin note: text overlap with arXiv:1809.07401", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An resilience optimal evaluation of financial portfolios implies having\nplausible hypotheses about the multiple interconnections between the\nmacroeconomic variables and the risk parameters. In this paper, we propose a\ngraphical model for the reconstruction of the causal structure that links the\nmultiple macroeconomic variables and the assessed risk parameters, it is this\nstructure that we call Stress Testing Network (STN). In this model, the\nrelationships between the macroeconomic variables and the risk parameter define\na \"relational graph\" among their time-series, where related time-series are\nconnected by an edge. Our proposal is based on the temporal causal models, but\nunlike, we incorporate specific conditions in the structure which correspond to\nintrinsic characteristics this type of networks. Using the proposed model and\ngiven the high-dimensional nature of the problem, we used regularization\nmethods to efficiently detect causality in the time-series and reconstruct the\nunderlying causal structure. In addition, we illustrate the use of model in\ncredit risk data of a portfolio. Finally, we discuss its uses and practical\nbenefits in stress testing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:16:24 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 14:29:57 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 12:37:05 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Rojas", "Helder", ""], ["Dias", "David", ""]]}, {"id": "1906.02014", "submitter": "Christopher Drovandi Dr", "authors": "Christopher Drovandi, Richard G Everitt, Andrew Golightly, Dennis\n  Prangle", "title": "Ensemble MCMC: Accelerating Pseudo-Marginal MCMC for State Space Models\n  using the Ensemble Kalman Filter", "comments": "minor edits, more extensive results, added web link to supporting\n  computer code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo (pMCMC) is now a popular method for\nperforming Bayesian statistical inference on challenging state space models\n(SSMs) with unknown static parameters. It uses a particle filter (PF) at each\niteration of an MCMC algorithm to unbiasedly estimate the likelihood for a\ngiven static parameter value. However, pMCMC can be computationally intensive\nwhen a large number of particles in the PF is required, such as when the data\nis highly informative, the model is misspecified and/or the time series is\nlong. In this paper we exploit the ensemble Kalman filter (EnKF) developed in\nthe data assimilation literature to speed up pMCMC. We replace the unbiased PF\nlikelihood with the biased EnKF likelihood estimate within MCMC to sample over\nthe space of the static parameter. On a wide class of different non-linear SSM\nmodels, we demonstrate that our new ensemble MCMC (eMCMC) method can\nsignificantly reduce the computational cost whilst maintaining reasonable\naccuracy. We also propose several extensions of the vanilla eMCMC algorithm to\nfurther improve computational efficiency. Computer code to implement our\nmethods on all the examples can be downloaded from\nhttps://github.com/cdrovandi/Ensemble-MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 13:09:10 GMT"}, {"version": "v2", "created": "Fri, 16 Aug 2019 14:46:05 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Drovandi", "Christopher", ""], ["Everitt", "Richard G", ""], ["Golightly", "Andrew", ""], ["Prangle", "Dennis", ""]]}, {"id": "1906.02435", "submitter": "Yuexiang Zhai", "authors": "Yuexiang Zhai, Zitong Yang, Zhenyu Liao, John Wright, and Yi Ma", "title": "Complete Dictionary Learning via $\\ell^4$-Norm Maximization over the\n  Orthogonal Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the fundamental problem of learning a complete\n(orthogonal) dictionary from samples of sparsely generated signals. Most\nexisting methods solve the dictionary (and sparse representations) based on\nheuristic algorithms, usually without theoretical guarantees for either\noptimality or complexity. The recent $\\ell^1$-minimization based methods do\nprovide such guarantees but the associated algorithms recover the dictionary\none column at a time. In this work, we propose a new formulation that maximizes\nthe $\\ell^4$-norm over the orthogonal group, to learn the entire dictionary. We\nprove that under a random data model, with nearly minimum sample complexity,\nthe global optima of the $\\ell^4$ norm are very close to signed permutations of\nthe ground truth. Inspired by this observation, we give a conceptually simple\nand yet effective algorithm based on \"matching, stretching, and projection\"\n(MSP). The algorithm provably converges locally at a superlinear (cubic) rate\nand cost per iteration is merely an SVD. In addition to strong theoretical\nguarantees, experiments show that the new algorithm is significantly more\nefficient and effective than existing methods, including KSVD and\n$\\ell^1$-based methods. Preliminary experimental results on mixed real imagery\ndata clearly demonstrate advantages of so learned dictionary over classic PCA\nbases.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 06:19:35 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 17:10:21 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 18:46:02 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 17:43:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhai", "Yuexiang", ""], ["Yang", "Zitong", ""], ["Liao", "Zhenyu", ""], ["Wright", "John", ""], ["Ma", "Yi", ""]]}, {"id": "1906.02446", "submitter": "Payam Siyari", "authors": "Payam Siyari, Bistra Dilkina, Constantine Dovrolis", "title": "Evolution of Hierarchical Structure & Reuse in iGEM Synthetic DNA\n  Sequences", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.04924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many complex systems, both in technology and nature, exhibit hierarchical\nmodularity: smaller modules, each of them providing a certain function, are\nused within larger modules that perform more complex functions. Previously, we\nhave proposed a modeling framework, referred to as Evo-Lexis, that provides\ninsight to some fundamental questions about evolving hierarchical systems.\n  The predictions of the Evo-Lexis model should be tested using real data from\nevolving systems in which the outputs can be well represented by sequences. In\nthis paper, we investigate the time series of iGEM synthetic DNA dataset\nsequences, and whether the resulting iGEM hierarchies exhibit the qualitative\nproperties predicted by the Evo-Lexis framework. Contrary to Evo-Lexis, in iGEM\nthe amount of reuse decreases during the timeline of the dataset. Although this\nresults in development of less cost-efficient and less deep Lexis-DAGs, the\ndataset exhibits a bias in reusing specific nodes more often than others. This\nresults in the Lexis-DAGs to take the shape of an hourglass with relatively\nhigh H-score values and stable set of core nodes. Despite the reuse bias and\nstability of the core set, the dataset presents a high amount of diversity\namong the targets which is in line with modeling of Evo-Lexis.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 07:15:33 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Siyari", "Payam", ""], ["Dilkina", "Bistra", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1906.02818", "submitter": "Francois Belletti", "authors": "Francois Belletti, Davis King, Kun Yang, Roland Nelet, Yusef Shafi,\n  Yi-Fan Chen, John Anderson", "title": "Tensor Processing Units for Financial Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Monte Carlo methods are critical to many routines in quantitative finance\nsuch as derivatives pricing, hedging and risk metrics. Unfortunately, Monte\nCarlo methods are very computationally expensive when it comes to running\nsimulations in high-dimensional state spaces where they are still a method of\nchoice in the financial industry. Recently, Tensor Processing Units (TPUs) have\nprovided considerable speedups and decreased the cost of running Stochastic\nGradient Descent (SGD) in Deep Learning. After highlighting computational\nsimilarities between training neural networks with SGD and simulating\nstochastic processes, we ask in the present paper whether TPUs are accurate,\nfast and simple enough to use for financial Monte Carlo. Through a theoretical\nreminder of the key properties of such methods and thorough empirical\nexperiments we examine the fitness of TPUs for option pricing, hedging and risk\nmetrics computation. In particular we demonstrate that, in spite of the use of\nmixed precision, TPUs still provide accurate estimators which are fast to\ncompute when compared to GPUs. We also show that the Tensorflow programming\nmodel for TPUs is elegant, expressive and simplifies automated differentiation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:11:05 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 20:40:12 GMT"}, {"version": "v3", "created": "Wed, 24 Jul 2019 17:34:20 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 19:14:34 GMT"}, {"version": "v5", "created": "Mon, 27 Jan 2020 22:20:16 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Belletti", "Francois", ""], ["King", "Davis", ""], ["Yang", "Kun", ""], ["Nelet", "Roland", ""], ["Shafi", "Yusef", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "1906.02840", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion, Tin Lok James Ng, Quan Vu and Maurizio\n  Filippone", "title": "Deep Compositional Spatial Models", "comments": "46 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial processes with nonstationary and anisotropic covariance structure are\noften used when modelling, analysing and predicting complex environmental\nphenomena. Such processes may often be expressed as ones that have stationary\nand isotropic covariance structure on a warped spatial domain. However, the\nwarping function is generally difficult to fit and not constrained to be\ninjective, often resulting in `space-folding.' Here, we propose modelling an\ninjective warping function through a composition of multiple elemental\ninjective functions in a deep-learning framework. We consider two cases; first,\nwhen these functions are known up to some weights that need to be estimated,\nand, second, when the weights in each layer are random. Inspired by recent\nmethodological and technological advances in deep learning and deep Gaussian\nprocesses, we employ approximate Bayesian methods to make inference with these\nmodels using graphics processing units. Through simulation studies in one and\ntwo dimensions we show that the deep compositional spatial models are quick to\nfit, and are able to provide better predictions and uncertainty quantification\nthan other deep stochastic models of similar complexity. We also show their\nremarkable capacity to model nonstationary, anisotropic spatial data using\nradiances from the MODIS instrument aboard the Aqua satellite.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 23:31:18 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 16:05:52 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Ng", "Tin Lok James", ""], ["Vu", "Quan", ""], ["Filippone", "Maurizio", ""]]}, {"id": "1906.03123", "submitter": "Waldyn Martinez", "authors": "Waldyn Martinez, J. Brian Gray", "title": "On the Current State of Research in Explaining Ensemble Performance\n  Using Margins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical evidence shows that ensembles, such as bagging, boosting, random\nand rotation forests, generally perform better in terms of their generalization\nerror than individual classifiers. To explain this performance, Schapire et al.\n(1998) developed an upper bound on the generalization error of an ensemble\nbased on the margins of the training data, from which it was concluded that\nlarger margins should lead to lower generalization error, everything else being\nequal. Many other researchers have backed this assumption and presented tighter\nbounds on the generalization error based on either the margins or functions of\nthe margins. For instance, Shen and Li (2010) provide evidence suggesting that\nthe generalization error of a voting classifier might be reduced by increasing\nthe mean and decreasing the variance of the margins. In this article we propose\nseveral techniques and empirically test whether the current state of research\nin explaining ensemble performance holds. We evaluate the proposed methods\nthrough experiments with real and simulated data sets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 14:23:29 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Martinez", "Waldyn", ""], ["Gray", "J. Brian", ""]]}, {"id": "1906.03222", "submitter": "Gabriel Hassler", "authors": "Gabriel Hassler, Max R. Tolkoff, William L. Allen, Lam Si Tung Ho,\n  Philippe Lemey, and Marc A. Suchard", "title": "Inferring phenotypic trait evolution on large trees with many incomplete\n  measurements", "comments": "29 pages, 7 figures, 2 tables, 3 supplementary sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative biologists are often interested in inferring covariation between\nmultiple biological traits sampled across numerous related taxa. To properly\nstudy these relationships, we must control for the shared evolutionary history\nof the taxa to avoid spurious inference. Existing control techniques almost\nuniversally scale poorly as the number of taxa increases. An additional\nchallenge arises as obtaining a full suite of measurements becomes increasingly\ndifficult with increasing taxa. This typically necessitates data imputation or\nintegration that further exacerbates scalability. We propose an inference\ntechnique that integrates out missing measurements analytically and scales\nlinearly with the number of taxa by using a post-order traversal algorithm\nunder a multivariate Brownian diffusion (MBD) model to characterize trait\nevolution. We further exploit this technique to extend the MBD model to account\nfor sampling error or non-heritable residual variance. We test these methods to\nexamine mammalian life history traits, prokaryotic genomic and phenotypic\ntraits, and HIV infection traits. We find computational efficiency increases\nthat top two orders-of-magnitude over current best practices. While we focus on\nthe utility of this algorithm in phylogenetic comparative methods, our approach\ngeneralizes to solve long-standing challenges in computing the likelihood for\nmatrix-normal and multivariate normal distributions with missing data at scale.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 16:41:55 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Hassler", "Gabriel", ""], ["Tolkoff", "Max R.", ""], ["Allen", "William L.", ""], ["Ho", "Lam Si Tung", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1906.03247", "submitter": "Waldyn Martinez", "authors": "Waldyn Martinez", "title": "Ensemble Pruning via Margin Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble models refer to methods that combine a typically large number of\nclassifiers into a compound prediction. The output of an ensemble method is the\nresult of fitting a base-learning algorithm to a given data set, and obtaining\ndiverse answers by reweighting the observations or by resampling them using a\ngiven probabilistic selection. A key challenge of using ensembles in\nlarge-scale multidimensional data lies in the complexity and the computational\nburden associated with them. The models created by ensembles are often\ndifficult, if not impossible, to interpret and their implementation requires\nmore computational power than single classifiers. Recent research effort in the\nfield has concentrated in reducing ensemble size, while maintaining their\npredictive accuracy. We propose a method to prune an ensemble solution by\noptimizing its margin distribution, while increasing its diversity. The\nproposed algorithm results in an ensemble that uses only a fraction of the\noriginal classifiers, with improved or similar generalization performance. We\nanalyze and test our method on both synthetic and real data sets. The\nsimulations show that the proposed method compares favorably to the original\nensemble solutions and to other existing ensemble pruning methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 17:22:31 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Martinez", "Waldyn", ""]]}, {"id": "1906.03329", "submitter": "Trevor Campbell", "authors": "Trevor Campbell and Boyan Beronov", "title": "Sparse Variational Inference: Bayesian Coresets from Scratch", "comments": "In Advances in Neural Information Processing Systems, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of automated inference algorithms in Bayesian statistics\nhas provided practitioners newfound access to fast, reproducible data analysis\nand powerful statistical models. Designing automated methods that are also both\ncomputationally scalable and theoretically sound, however, remains a\nsignificant challenge. Recent work on Bayesian coresets takes the approach of\ncompressing the dataset before running a standard inference algorithm,\nproviding both scalability and guarantees on posterior approximation error. But\nthe automation of past coreset methods is limited because they depend on the\navailability of a reasonable coarse posterior approximation, which is difficult\nto specify in practice. In the present work we remove this requirement by\nformulating coreset construction as sparsity-constrained variational inference\nwithin an exponential family. This perspective leads to a novel construction\nvia greedy optimization, and also provides a unifying information-geometric\nview of present and past methods. The proposed Riemannian coreset construction\nalgorithm is fully automated, requiring no problem-specific inputs aside from\nthe probabilistic model and dataset. In addition to being significantly easier\nto use than past methods, experiments demonstrate that past coreset\nconstructions are fundamentally limited by the fixed coarse posterior\napproximation; in contrast, the proposed algorithm is able to continually\nimprove the coreset, providing state-of-the-art Bayesian dataset summarization\nwith orders-of-magnitude reduction in KL divergence to the exact posterior.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 20:54:35 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 08:33:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Campbell", "Trevor", ""], ["Beronov", "Boyan", ""]]}, {"id": "1906.03564", "submitter": "Roberto Rivera", "authors": "Roberto Rivera", "title": "A Low Rank Gaussian Process Prediction Model for Very Large Datasets", "comments": null, "journal-ref": "In 2015 IEEE First International Conference on Big Data Computing\n  Service and Applications (pp. 308-313). IEEE", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction requires expensive computation to invert the spatial\ncovariance matrix it depends on and also has considerable storage needs. This\nwork concentrates on computationally efficient algorithms for prediction using\nvery large datasets. A recent prediction model for spatial data known as Fixed\nRank Kriging is much faster than the kriging and can be easily implemented with\nless assumptions about the process. However, Fixed Rank Kriging requires the\nestimation of a matrix which must be positive definite and the original\nestimation procedure cannot guarantee this property. We present a result that\nshows when a matrix subtraction of a given form will give a positive definite\nmatrix. Motivated by this result, we present an iterative Fixed Rank Kriging\nalgorithm that ensures positive definiteness of the matrix required for\nprediction and show that under mild conditions the algorithm numerically\nconverges. The modified Fixed Rank Kriging procedure is implemented to predict\nmissing chlorophyll observations for very large regions of ocean color.\nPredictions are compared to those made by other well known methods of spatial\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:37:53 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""]]}, {"id": "1906.03791", "submitter": "Alen Alexanderian", "authors": "Elizabeth Herman, Alen Alexanderian, and Arvind K. Saibaba", "title": "Randomization and reweighted $\\ell_1$-minimization for A-optimal design\n  of linear inverse problems", "comments": "27 Pages; Accepted for publication in SIAM Journal on Scientific\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal design of PDE-based Bayesian linear inverse problems with\ninfinite-dimensional parameters. We focus on the A-optimal design criterion,\ndefined as the average posterior variance and quantified by the trace of the\nposterior covariance operator. We propose using structure exploiting randomized\nmethods to compute the A-optimal objective function and its gradient, and\nprovide a detailed analysis of the error for the proposed estimators. To ensure\nsparse and binary design vectors, we develop a novel reweighted\n$\\ell_1$-minimization algorithm. We also introduce a modified A-optimal\ncriterion and present randomized estimators for its efficient computation. We\npresent numerical results illustrating the proposed methods on a model\ncontaminant source identification problem, where the inverse problem seeks to\nrecover the initial state of a contaminant plume, using discrete measurements\nof the contaminant in space and time.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 04:29:50 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 15:45:09 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Herman", "Elizabeth", ""], ["Alexanderian", "Alen", ""], ["Saibaba", "Arvind K.", ""]]}, {"id": "1906.03828", "submitter": "Dan Li", "authors": "Dan Li and Adam Clements and Christopher Drovandi", "title": "Efficient Bayesian estimation for GARCH-type models via Sequential Monte\n  Carlo", "comments": "Minor revisions; replaced the normal innovation of GARCH and\n  GJR-GARCH model with a Student-t innovation; some updates to the results\n  based on Student-t GARCH and GJR-GARCH (Section 6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advantages of sequential Monte Carlo (SMC) are exploited to develop\nparameter estimation and model selection methods for GARCH (Generalized\nAutoRegressive Conditional Heteroskedasticity) style models. It provides an\nalternative method for quantifying estimation uncertainty relative to classical\ninference. Even with long time series, it is demonstrated that the posterior\ndistribution of model parameters are non-normal, highlighting the need for a\nBayesian approach and an efficient posterior sampling method. Efficient\napproaches for both constructing the sequence of distributions in SMC, and\nleave-one-out cross-validation, for long time series data are also proposed.\nFinally, an unbiased estimator of the likelihood is developed for the Bad\nEnvironment-Good Environment model, a complex GARCH-type model, which permits\nexact Bayesian inference not previously available in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 07:59:26 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:05:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Li", "Dan", ""], ["Clements", "Adam", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1906.04063", "submitter": "Waldyn Martinez", "authors": "Waldyn Martinez, J. Brian Gray", "title": "On the Insufficiency of the Large Margins Theory in Explaining the\n  Performance of Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting and other ensemble methods combine a large number of weak\nclassifiers through weighted voting to produce stronger predictive models. To\nexplain the successful performance of boosting algorithms, Schapire et al.\n(1998) showed that AdaBoost is especially effective at increasing the margins\nof the training data. Schapire et al. (1998) also developed an upper bound on\nthe generalization error of any ensemble based on the margins of the training\ndata, from which it was concluded that larger margins should lead to lower\ngeneralization error, everything else being equal (sometimes referred to as the\n``large margins theory''). Tighter bounds have been derived and have reinforced\nthe large margins theory hypothesis. For instance, Wang et al. (2011) suggest\nthat specific margin instances, such as the equilibrium margin, can better\nsummarize the margins distribution. These results have led many researchers to\nconsider direct optimization of the margins to improve ensemble generalization\nerror with mixed results. We show that the large margins theory is not\nsufficient for explaining the performance of voting classifiers. We do this by\nillustrating how it is possible to improve upon the margin distribution of an\nensemble solution, while keeping the complexity fixed, yet not improve the test\nset performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 15:09:24 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Martinez", "Waldyn", ""], ["Gray", "J. Brian", ""]]}, {"id": "1906.04322", "submitter": "Jean-Fran\\c{c}ois B\\'egin", "authors": "Jean-Fran\\c{c}ois B\\'egin and Mathieu Boudreault", "title": "Likelihood Evaluation of Jump-Diffusion Models Using Deterministic\n  Nonlinear Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop a deterministic nonlinear filtering algorithm based\non a high-dimensional version of Kitagawa (1987) to evaluate the likelihood\nfunction of models that allow for stochastic volatility and jumps whose arrival\nintensity is also stochastic. We show numerically that the deterministic\nfiltering method is precise and much faster than the particle filter, in\naddition to yielding a smooth function over the parameter space. We then find\nthe maximum likelihood estimates of various models that include stochastic\nvolatility, jumps in the returns and variance, and also stochastic jump arrival\nintensity with the S&P 500 daily returns. During the Great Recession, the jump\narrival intensity increases significantly and contributes to the clustering of\nvolatility and negative returns.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 23:31:49 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 19:48:46 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["B\u00e9gin", "Jean-Fran\u00e7ois", ""], ["Boudreault", "Mathieu", ""]]}, {"id": "1906.04347", "submitter": "Scott Sisson", "authors": "G. S. Rodrigues and D. J. Nott and S. A. Sisson", "title": "Likelihood-free approximate Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods such as approximate Bayesian computation (ABC) have\nextended the reach of statistical inference to problems with computationally\nintractable likelihoods. Such approaches perform well for small-to-moderate\ndimensional problems, but suffer a curse of dimensionality in the number of\nmodel parameters. We introduce a likelihood-free approximate Gibbs sampler that\nnaturally circumvents the dimensionality issue by focusing on lower-dimensional\nconditional distributions. These distributions are estimated by flexible\nregression models either before the sampler is run, or adaptively during\nsampler implementation. As a result, and in comparison to Metropolis-Hastings\nbased approaches, we are able to fit substantially more challenging statistical\nmodels than would otherwise be possible. We demonstrate the sampler's\nperformance via two simulated examples, and a real analysis of Airbnb rental\nprices using a intractable high-dimensional multivariate non-linear state space\nmodel containing 13,140 parameters, which presents a real challenge to standard\nABC techniques.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 01:56:27 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Rodrigues", "G. S.", ""], ["Nott", "D. J.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1906.04410", "submitter": "Yutaka Shikano", "authors": "Kentaro Tamura, Yutaka Shikano", "title": "Quantum Random Numbers generated by the Cloud Superconducting Quantum\n  Computer", "comments": "21 pages, 5 figures, submitted to the paper in Book \"Mathematics,\n  Quantum Theory, and Cryptography\" Mathematics for Industry, Springer. In the\n  revised manuscript, the results of the simulator with the noise parameters\n  were added", "journal-ref": "International Symposium on Mathematics, Quantum Theory, and\n  Cryptography, Mathematics for Industry, vol 33 (Springer, Singapore, 2021) 17\n  -- 37", "doi": "10.1007/978-981-15-5191-8_6", "report-no": null, "categories": "quant-ph cs.CR cs.DC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cloud quantum computer is similar to a random number generator in that its\nphysical mechanism is inaccessible to its users. In this respect, a cloud\nquantum computer is a black box. In both devices, its users decide the device\ncondition from the output. A framework to achieve this exists in the field of\nrandom number generation in the form of statistical tests for random number\ngenerators. In the present study, we generated random numbers on a 20-qubit\ncloud quantum computer and evaluated the condition and stability of its qubits\nusing statistical tests for random number generators. As a result, we observed\nthat some qubits were more biased than others. Statistical tests for random\nnumber generators may provide a simple indicator of qubit condition and\nstability, enabling users to decide for themselves which qubits inside a cloud\nquantum computer to use.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 06:36:30 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 22:02:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tamura", "Kentaro", ""], ["Shikano", "Yutaka", ""]]}, {"id": "1906.04668", "submitter": "Fernando Alarid-Escudero", "authors": "Fernando Alarid-Escudero, Amy B. Knudsen, Jonathan Ozik, Nicholson\n  Collier, Karen M. Kuntz", "title": "Characterization and valuation of uncertainty of calibrated parameters\n  in stochastic decision models", "comments": "17 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the implications of different approaches to characterize\nuncertainty of calibrated parameters of stochastic decision models (DMs) in the\nquantified value of such uncertainty in decision making. We used a\nmicrosimulation DM of colorectal cancer (CRC) screening to conduct a\ncost-effectiveness analysis (CEA) of a 10-year colonoscopy screening. We\ncalibrated the natural history model of CRC to epidemiological data with\ndifferent degrees of uncertainty and obtained the joint posterior distribution\nof the parameters using a Bayesian approach. We conducted a probabilistic\nsensitivity analysis (PSA) on all the model parameters with different\ncharacterizations of uncertainty of the calibrated parameters and estimated the\nvalue of uncertainty of the different characterizations with a value of\ninformation analysis. All analyses were conducted using high performance\ncomputing resources running the Extreme-scale Model Exploration with Swift\n(EMEWS) framework. The posterior distribution had high correlation among some\nparameters. The parameters of the Weibull hazard function for the age of onset\nof adenomas had the highest posterior correlation of -0.958. Considering full\nposterior distributions and the maximum-a-posteriori estimate of the calibrated\nparameters, there is little difference on the spread of the distribution of the\nCEA outcomes with a similar expected value of perfect information (EVPI) of\n\\$653 and \\$685, respectively, at a WTP of \\$66,000/QALY. Ignoring correlation\non the posterior distribution of the calibrated parameters, produced the widest\ndistribution of CEA outcomes and the highest EVPI of \\$809 at the same WTP.\nDifferent characterizations of uncertainty of calibrated parameters have\nimplications on the expect value of reducing uncertainty on the CEA. Ignoring\ninherent correlation among calibrated parameters on a PSA overestimates the\nvalue of uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:47:32 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Alarid-Escudero", "Fernando", ""], ["Knudsen", "Amy B.", ""], ["Ozik", "Jonathan", ""], ["Collier", "Nicholson", ""], ["Kuntz", "Karen M.", ""]]}, {"id": "1906.04870", "submitter": "Yongyi Guo", "authors": "Jianqing Fan, Yongyi Guo, Kaizheng Wang", "title": "Communication-Efficient Accurate Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the data are stored in a distributed manner, direct application of\ntraditional statistical inference procedures is often prohibitive due to\ncommunication cost and privacy concerns. This paper develops and investigates\ntwo Communication-Efficient Accurate Statistical Estimators (CEASE),\nimplemented through iterative algorithms for distributed optimization. In each\niteration, node machines carry out computation in parallel and communicate with\nthe central processor, which then broadcasts aggregated information to node\nmachines for new updates. The algorithms adapt to the similarity among loss\nfunctions on node machines, and converge rapidly when each node machine has\nlarge enough sample size. Moreover, they do not require good initialization and\nenjoy linear converge guarantees under general conditions. The contraction rate\nof optimization errors is presented explicitly, with dependence on the local\nsample size unveiled. In addition, the improved statistical accuracy per\niteration is derived. By regarding the proposed method as a multi-step\nstatistical estimator, we show that statistical efficiency can be achieved in\nfinite steps in typical statistical applications. In addition, we give the\nconditions under which the one-step CEASE estimator is statistically efficient.\nExtensive numerical experiments on both synthetic and real data validate the\ntheoretical results and demonstrate the superior performance of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 00:41:12 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Yongyi", ""], ["Wang", "Kaizheng", ""]]}, {"id": "1906.05189", "submitter": "Alexandre Janon", "authors": "Alexandre Janon", "title": "Global optimization using Sobol indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and assess a new global (derivative-free) optimization algorithm,\ninspired by the LIPO algorithm, which uses variance-based sensitivity analysis\n(Sobol indices) to reduce the number of calls to the objective function. This\nmethod should be efficient to optimize costly functions satisfying the\nsparsity-of-effects principle.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:00:54 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Janon", "Alexandre", ""]]}, {"id": "1906.05346", "submitter": "Yin Xian", "authors": "Jian-Feng Cai, Lizhang Miao, Yang Wang and Yin Xian", "title": "Optimal low rank tensor recovery", "comments": "There is an error in the paper and need to be correct", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sample size requirement for exact recovery of a high order\ntensor of low rank from a subset of its entries. In the Tucker decomposition\nframework, we show that the Riemannian optimization algorithm with initial\nvalue obtained from a spectral method can reconstruct a tensor of size $n\\times\nn \\times\\cdots \\times n$ tensor of ranks $(r,\\cdots,r)$ with high probability\nfrom as few as $O((r^d+dnr)\\log(d))$ entries. In the case of order 3 tensor,\nthe entries can be asymptotically as few as $O(nr)$ for a low rank large\ntensor. We show the theoretical guarantee condition for the recovery. The\nanalysis relies on the tensor restricted isometry property (tensor RIP) and the\ncurvature of the low rank tensor manifold. Our algorithm is computationally\nefficient and easy to implement. Numerical results verify that the algorithms\nare able to recover a low rank tensor from minimum number of measurements. The\nexperiments on hyperspectral images recovery also show that our algorithm is\ncapable of real world signal processing problems.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:37:40 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 22:48:22 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 19:41:23 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Cai", "Jian-Feng", ""], ["Miao", "Lizhang", ""], ["Wang", "Yang", ""], ["Xian", "Yin", ""]]}, {"id": "1906.05399", "submitter": "Marcelo Costa Prof.", "authors": "Marcelo Azevedo Costa and Leandro Brioschi Mineti and Marcos Oliveira\n  Prates and Ramiro Ruiz Cardenas", "title": "Dynamic Time Scan Forecasting", "comments": "15 pages, 7 figures, working paper, version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time scan forecasting method relies on the premise that the most\nimportant pattern in a time series precedes the forecasting window, i.e., the\nlast observed values. Thus, a scan procedure is applied to identify similar\npatterns, or best matches, throughout the time series. As oppose to euclidean\ndistance, or any distance function, a similarity function is dynamically\nestimated in order to match previous values to the last observed values.\nGoodness-of-fit statistics are used to find the best matches. Using the\nrespective similarity functions, the observed values proceeding the best\nmatches are used to create a forecasting pattern, as well as forecasting\nintervals. Remarkably, the proposed method outperformed statistical and machine\nlearning approaches in a real case wind speed forecasting problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 22:03:09 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Costa", "Marcelo Azevedo", ""], ["Mineti", "Leandro Brioschi", ""], ["Prates", "Marcos Oliveira", ""], ["Cardenas", "Ramiro Ruiz", ""]]}, {"id": "1906.05575", "submitter": "Gentry White", "authors": "Gentry White and Dongchu Sun and Paul Speckman", "title": "Direct Sampling of Bayesian Thin-Plate Splines for Spatial Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radial basis functions are a common mathematical tool used to construct a\nsmooth interpolating function from a set of data points. A spatial prior based\non thin-plate spline radial basis functions can be easily implemented resulting\nin a posterior that can be sampled directly using Monte Carlo integration,\navoiding the computational burden and potential inefficiency of an Monte Carlo\nMarkov Chain (MCMC) sampling scheme. The derivation of the prior and sampling\nscheme are demonstrated.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 09:47:56 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["White", "Gentry", ""], ["Sun", "Dongchu", ""], ["Speckman", "Paul", ""]]}, {"id": "1906.05603", "submitter": "Michael Grayling", "authors": "Michael J Grayling, Graham M Wheeler", "title": "A review of available software for adaptive clinical trial design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background/Aims: The increasing expense of the drug development process has\nseen interest in the use of adaptive designs (ADs) grow substantially in recent\nyears. Accordingly, much research has been conducted to identify potential\nbarriers to increasing the use of ADs in practice, and several articles have\nargued that the availability of user-friendly software will be an important\nstep in making ADs easier to implement. Therefore, in this paper we present a\nreview of the current state of software availability for AD. Methods: We first\nreview articles from 31 journals published in 2013-17 that relate to\nmethodology for adaptive trials, in order to assess how often code and software\nfor implementing novel ADs is made available at the time of publication. We\ncontrast our findings against these journals' current policies on code\ndistribution. Secondly, we conduct additional searches of popular code\nrepositories, such as CRAN and GitHub, to identify further existing\nuser-contributed software for ADs. From this, we are able to direct interested\nparties towards solutions for their problem of interest by classifying\navailable code by type of adaptation. Results: Only 29% of included articles\nmade their code available in some form. In many instances, articles published\nin journals that had mandatory requirements on code provision still did not\nmake code available. There are several areas in which available software is\ncurrently limited or saturated. In particular, many packages are available to\naddress group sequential design, but comparatively little code is present in\nthe public domain to determine biomarker-guided ADs. Conclusions: There is much\nroom for improvement in the provision of software alongside AD publications.\nAdditionally, whilst progress has been made, well-established software for\nvarious types of trial adaptation remains sparsely available.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 11:27:10 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Grayling", "Michael J", ""], ["Wheeler", "Graham M", ""]]}, {"id": "1906.05669", "submitter": "Alexander Litvinenko", "authors": "Mike Espig, Wolfgang Hackbusch, Alexander Litvinenko, Hermann G.\n  Matthies, Elmar Zander", "title": "Post-Processing of High-Dimensional Data", "comments": "32 pages, 2 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific computations or measurements may result in huge volumes of data.\nOften these can be thought of representing a real-valued function on a\nhigh-dimensional domain, and can be conceptually arranged in the format of a\ntensor of high degree in some truncated or lossy compressed format. We look at\nsome common post-processing tasks which are not obvious in the compressed\nformat, as such huge data sets can not be stored in their entirety, and the\nvalue of an element is not readily accessible through simple look-up. The tasks\nwe consider are finding the location of maximum or minimum, or minimum and\nmaximum of a function of the data, or finding the indices of all elements in\nsome interval --- i.e. level sets, the number of elements with a value in such\na level set, the probability of an element being in a particular level set, and\nthe mean and variance of the total collection.\n  The algorithms to be described are fixed point iterations of particular\nfunctions of the tensor, which will then exhibit the desired result. For this,\nthe data is considered as an element of a high degree tensor space, although in\nan abstract sense, the algorithms are independent of the representation of the\ndata as a tensor. All that we require is that the data can be considered as an\nelement of an associative, commutative algebra with an inner product. Such an\nalgebra is isomorphic to a commutative sub-algebra of the usual matrix algebra,\nallowing the use of matrix algorithms to accomplish the mentioned tasks. We\nallow the actual computational representation to be a lossy compression, and we\nallow the algebra operations to be performed in an approximate fashion, so as\nto maintain a high compression level. One such example which we address\nexplicitly is the representation of data as a tensor with compression in the\nform of a low-rank representation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 13:33:09 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 07:38:37 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 14:34:03 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Espig", "Mike", ""], ["Hackbusch", "Wolfgang", ""], ["Litvinenko", "Alexander", ""], ["Matthies", "Hermann G.", ""], ["Zander", "Elmar", ""]]}, {"id": "1906.05944", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Alessandro Barp, Andrew B. Duncan, Mark\n  Girolami", "title": "Statistical Inference for Generative Models with Maximum Mean\n  Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While likelihood-based inference and its variants provide a statistically\nefficient and widely applicable approach to parametric inference, their\napplication to models involving intractable likelihoods poses challenges. In\nthis work, we study a class of minimum distance estimators for intractable\ngenerative models, that is, statistical models for which the likelihood is\nintractable, but simulation is cheap. The distance considered, maximum mean\ndiscrepancy (MMD), is defined through the embedding of probability measures\ninto a reproducing kernel Hilbert space. We study the theoretical properties of\nthese estimators, showing that they are consistent, asymptotically normal and\nrobust to model misspecification. A main advantage of these estimators is the\nflexibility offered by the choice of kernel, which can be used to trade-off\nstatistical efficiency and robustness. On the algorithmic side, we study the\ngeometry induced by MMD on the parameter space and use this to introduce a\nnovel natural gradient descent-like algorithm for efficient implementation of\nthese estimators. We illustrate the relevance of our theoretical results on\nseveral classes of models including a discrete-time latent Markov process and\ntwo multivariate stochastic differential equation models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 21:53:55 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Barp", "Alessandro", ""], ["Duncan", "Andrew B.", ""], ["Girolami", "Mark", ""]]}, {"id": "1906.05967", "submitter": "Yuling Jiao", "authors": "Jianfeng Cai and Yuling Jiao and Xiliang Lu and Juntao You", "title": "A stochastic alternating minimizing method for sparse phase retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse phase retrieval plays an important role in many fields of applied\nscience and thus attracts lots of attention. In this paper, we propose a\n\\underline{sto}chastic alte\\underline{r}nating \\underline{m}inimizing method\nfor \\underline{sp}arse ph\\underline{a}se \\underline{r}etrieval\n(\\textit{StormSpar}) algorithm which {emprically} is able to recover\n$n$-dimensional $s$-sparse signals from only $O(s\\,\\mathrm{log}\\, n)$ number of\nmeasurements without a desired initial value required by many existing methods.\nIn \\textit{StormSpar}, the hard-thresholding pursuit (HTP) algorithm is\nemployed to solve the sparse constraint least square sub-problems. The main\ncompetitive feature of \\textit{StormSpar} is that it converges globally\nrequiring optimal order of number of samples with random initialization.\nExtensive numerical experiments are given to validate the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 00:24:34 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Cai", "Jianfeng", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["You", "Juntao", ""]]}, {"id": "1906.06020", "submitter": "Guy Hawkins", "authors": "Minh-Ngoc Tran, Marcel Scharth, David Gunawan, Robert Kohn, Scott D.\n  Brown, Guy E. Hawkins", "title": "Robustly estimating the marginal likelihood for cognitive models via\n  importance sampling", "comments": "38 pages, 4 tables, 5 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Markov chain Monte Carlo (MCMC) extend the scope of\nBayesian inference to models for which the likelihood function is intractable.\nAlthough these developments allow us to estimate model parameters, other basic\nproblems such as estimating the marginal likelihood, a fundamental tool in\nBayesian model selection, remain challenging. This is an important scientific\nlimitation because testing psychological hypotheses with hierarchical models\nhas proven difficult with current model selection methods. We propose an\nefficient method for estimating the marginal likelihood for models where the\nlikelihood is intractable, but can be estimated unbiasedly. It is based on\nfirst running a sampling method such as MCMC to obtain samples for the model\nparameters, and then using these samples to construct the proposal density in\nan importance sampling (IS) framework with an unbiased estimate of the\nlikelihood. Our method has several attractive properties: it generates an\nunbiased estimate of the marginal likelihood, it is robust to the quality and\ntarget of the sampling method used to form the IS proposals, and it is\ncomputationally cheap to estimate the variance of the marginal likelihood\nestimator. We also obtain the convergence properties of the method and provide\nguidelines on maximizing computational efficiency. The method is illustrated in\ntwo challenging cases involving hierarchical models: identifying the form of\nindividual differences in an applied choice scenario, and evaluating the best\nparameterization of a cognitive model in a speeded decision making context.\nFreely available code to implement the methods is provided. Extensions to\nposterior moment estimation and parallelization are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 04:49:32 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 07:22:23 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Scharth", "Marcel", ""], ["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Brown", "Scott D.", ""], ["Hawkins", "Guy E.", ""]]}, {"id": "1906.06431", "submitter": "Yury Maximov", "authors": "Valerii Likhosherstov, Yury Maximov and Michael Chertkov", "title": "A New Family of Tractable Ising Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cond-mat.stat-mech physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of zero-field Ising models over N binary\nvariables/spins obtained by consecutive \"gluing\" of planar and $O(1)$-sized\ncomponents along with subsets of at most three vertices into a tree. The\npolynomial time algorithm of the dynamic programming type for solving exact\ninference (partition function computation) and sampling consists of a\nsequential application of an efficient (for planar) or brute-force (for\n$O(1)$-sized) inference and sampling to the components as a black box. To\nillustrate the utility of the new family of tractable graphical models, we\nfirst build an $O(N^{3/2})$ algorithm for inference and sampling of the\nK5-minor-free zero-field Ising models - an extension of the planar zero-field\nIsing models - which is neither genus- nor treewidth-bounded. Second, we\ndemonstrate empirically an improvement in the approximation quality of the\nNP-hard problem of the square-grid Ising model (with non-zero field) inference.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 23:15:35 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Likhosherstov", "Valerii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1906.06463", "submitter": "Jasjeet Sekhon", "authors": "S\\\"oren R. K\\\"unzel, Theo F. Saarinen, Edward W. Liu, Jasjeet S.\n  Sekhon", "title": "Linear Aggregation in Tree-based Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression trees and their ensemble methods are popular methods for\nnonparametric regression: they combine strong predictive performance with\ninterpretable estimators. To improve their utility for locally smooth response\nsurfaces, we study regression trees and random forests with linear aggregation\nfunctions. We introduce a new algorithm that finds the best axis-aligned split\nto fit linear aggregation functions on the corresponding nodes, and we offer a\nquasilinear time implementation. We demonstrate the algorithm's favorable\nperformance on real-world benchmarks and in an extensive simulation study, and\nwe demonstrate its improved interpretability using a large get-out-the-vote\nexperiment. We provide an open-source software package that implements several\ntree-based estimators with linear aggregation functions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 04:25:55 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 20:00:42 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 23:17:20 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 20:45:03 GMT"}, {"version": "v5", "created": "Sat, 23 Jan 2021 02:18:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Saarinen", "Theo F.", ""], ["Liu", "Edward W.", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1906.06529", "submitter": "Xinwei Ma", "authors": "Matias D. Cattaneo, Michael Jansson, Xinwei Ma", "title": "lpdensity: Local Polynomial Density Estimation and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimation and inference methods are widely used in empirical work.\nWhen the underlying distribution has compact support, conventional kernel-based\ndensity estimators are no longer consistent near or at the boundary because of\ntheir well-known boundary bias. Alternative smoothing methods are available to\nhandle boundary points in density estimation, but they all require additional\ntuning parameter choices or other typically ad hoc modifications depending on\nthe evaluation point and/or approach considered. This article discusses the R\nand Stata package lpdensity implementing a novel local polynomial density\nestimator proposed and studied in Cattaneo, Jansson, and Ma (2020, 2021), which\nis boundary adaptive and involves only one tuning parameter. The methods\nimplemented also cover local polynomial estimation of the cumulative\ndistribution function and density derivatives. In addition to point estimation\nand graphical procedures, the package offers consistent variance estimators,\nmean squared error optimal bandwidth selection, robust bias-corrected\ninference, and confidence bands construction, among other features. A\ncomparison with other density estimation packages available in R using a Monte\nCarlo experiment is provided.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 11:26:21 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:35:49 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 22:01:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Ma", "Xinwei", ""]]}, {"id": "1906.06580", "submitter": "Mike West", "authors": "Isaac Lavine, Michael Lindon, and Mike West", "title": "Adaptive Variable Selection for Sequential Prediction in Multivariate\n  Dynamic Models", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian model uncertainty analysis and forecasting in sequential\ndynamic modeling of multivariate time series. The perspective is that of a\ndecision-maker with a specific forecasting objective that guides thinking about\nrelevant models. Based on formal Bayesian decision-theoretic reasoning, we\ndevelop a time-adaptive approach to exploring, weighting, combining and\nselecting models that differ in terms of predictive variables included. The\nadaptivity allows for changes in the sets of favored models over time, and is\nguided by the specific forecasting goals. A synthetic example illustrates how\ndecision-guided variable selection differs from traditional Bayesian model\nuncertainty analysis and standard model averaging. An applied study in one\nmotivating application of long-term macroeconomic forecasting highlights the\nutility of the new approach in terms of improving predictions as well as its\nability to identify and interpret different sets of relevant models over time\nwith respect to specific, defined forecasting goals.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:53:50 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 14:03:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lavine", "Isaac", ""], ["Lindon", "Michael", ""], ["West", "Mike", ""]]}, {"id": "1906.06583", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron and J\\'er\\^ome Dedecker and Bertrand Michel", "title": "Linear regression with stationary errors : the R package slm", "comments": "31 pages, 11 figures, 5 tables. The associated R package 'slm' is\n  available on the CRAN website (https://cran.r-project.org/index.html) or on\n  the GitHub website (https://github.com/E-Caron/slm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package slm which stands for Stationary Linear\nModels. The package contains a set of statistical procedures for linear\nregression in the general context where the error process is strictly\nstationary with short memory. We work in the setting of Hannan (1973), who\nproved the asymptotic normality of the (normalized) least squares estimators\n(LSE) under very mild conditions on the error process. We propose different\nways to estimate the asymptotic covariance matrix of the LSE, and then to\ncorrect the type I error rates of the usual tests on the parameters (as well as\nconfidence intervals). The procedures are evaluated through different sets of\nsimulations, and two examples of real datasets are studied.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 16:09:48 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:51:36 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 18:45:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "1906.06722", "submitter": "Gregor Robinson", "authors": "Gregor Robinson and Ian Grooms", "title": "A fast tunable blurring algorithm for scattered data", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": "10.1137/19M1268781", "report-no": null, "categories": "stat.CO cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A blurring algorithm with linear time complexity can reduce the small-scale\ncontent of data observed at scattered locations in a spatially extended domain\nof arbitrary dimension. The method works by forming a Gaussian interpolant of\nthe input data, and then convolving the interpolant with a multiresolution\nGaussian approximation of the Green's function to a differential operator whose\nspectrum can be tuned for problem-specific considerations. Like conventional\nblurring algorithms, which the new algorithm generalizes to data measured at\nlocations other than a uniform grid, applications include deblurring and\nseparation of spatial scales. An example illustrates a possible application\ntoward enabling importance sampling approaches to data assimilation of\ngeophysical observations, which are often scattered over a spatial domain,\nsince blurring observations can make particle filters more effective at state\nestimation of large scales. Another example, motivated by data analysis of\ndynamics like ocean eddies that have strong separation of spatial scales, uses\nthe algorithm to decompose scattered oceanographic float measurements into\nlarge-scale and small-scale components.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 16:27:52 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:11:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Robinson", "Gregor", ""], ["Grooms", "Ian", ""]]}, {"id": "1906.06729", "submitter": "Zhiqiang Tan", "authors": "Ting Yang, Zhiqiang Tan", "title": "Hierarchical Total Variations and Doubly Penalized ANOVA Modeling for\n  Multivariate Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multivariate nonparametric regression, functional analysis-of-variance\n(ANOVA) modeling aims to capture the relationship between a response and\ncovariates by decomposing the unknown function into various components,\nrepresenting main effects, two-way interactions, etc. Such an approach has been\npursued explicitly in smoothing spline ANOVA modeling and implicitly in various\ngreedy methods such as MARS. We develop a new method for functional ANOVA\nmodeling, based on doubly penalized estimation using total-variation and\nempirical-norm penalties, to achieve sparse selection of component functions\nand their knots. For this purpose, we formulate a new class of hierarchical\ntotal variations, which measures total variations at different levels including\nmain effects and multi-way interactions, possibly after some order of\ndifferentiation. Furthermore, we derive suitable basis functions for\nmultivariate splines such that the hierarchical total variation can be\nrepresented as a regular Lasso penalty, and hence we extend a previous\nbackfitting algorithm to handle doubly penalized estimation for ANOVA modeling.\nWe present extensive numerical experiments on simulations and real data to\ncompare our method with existing methods including MARS, tree boosting, and\nrandom forest. The results are very encouraging and demonstrate considerable\ngains from our method in both prediction or classification accuracy and\nsimplicity of the fitted functions.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 17:09:30 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Yang", "Ting", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "1906.06914", "submitter": "Guillaume Dehaene P.", "authors": "Alexander Immer and Guillaume P. Dehaene", "title": "Variational Inference with Numerical Derivatives: variance reduction\n  through coupling", "comments": "Under review (NEURIPS 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Black Box Variational Inference (Ranganath et al. (2014)) algorithm\nprovides a universal method for Variational Inference, but taking advantage of\nspecial properties of the approximation family or of the target can improve the\nconvergence speed significantly. For example, if the approximation family is a\ntransformation family, such as a Gaussian, then switching to the\nreparameterization gradient (Kingma and Welling (2014)) often yields a major\nreduction in gradient variance. Ultimately, reducing the variance can reduce\nthe computational cost and yield better approximations.\n  We present a new method to extend the reparameterization trick to more\ngeneral exponential families including the Wishart, Gamma, and Student\ndistributions. Variational Inference with Numerical Derivatives (VIND)\napproximates the gradient with numerical derivatives and reduces its variance\nusing a tight coupling of the approximation family. The resulting algorithm is\nsimple to implement and can profit from widely known couplings. Our experiments\nconfirm that VIND effectively decreases the gradient variance and therefore\nimproves the posterior approximation in relevant cases. It thus provides an\nefficient yet simple Variational Inference method for computing non-Gaussian\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:30:29 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Immer", "Alexander", ""], ["Dehaene", "Guillaume P.", ""]]}, {"id": "1906.07177", "submitter": "Taylor Pospisil", "authors": "Taylor Pospisil and Ann B. Lee", "title": "(f)RFCDE: Random Forests for Conditional Density Estimation and\n  Functional Data", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.05753", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests is a common non-parametric regression technique which performs\nwell for mixed-type unordered data and irrelevant features, while being robust\nto monotonic variable transformations. Standard random forests, however, do not\nefficiently handle functional data and runs into a curse-of dimensionality when\npresented with high-resolution curves and surfaces. Furthermore, in settings\nwith heteroskedasticity or multimodality, a regression point estimate with\nstandard errors do not fully capture the uncertainty in our predictions. A more\ninformative quantity is the conditional density p(y | x) which describes the\nfull extent of the uncertainty in the response y given covariates x. In this\npaper we show how random forests can be efficiently leveraged for conditional\ndensity estimation, functional covariates, and multiple responses without\nincreasing computational complexity. We provide open-source software for all\nprocedures with R and Python versions that call a common C++ library.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 03:57:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Pospisil", "Taylor", ""], ["Lee", "Ann B.", ""]]}, {"id": "1906.07684", "submitter": "Michael Jauch", "authors": "Michael Jauch, Peter D. Hoff, and David B. Dunson", "title": "Monte Carlo simulation on the Stiefel manifold via polar expansion", "comments": "24 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications to Bayesian inference for statistical models with\northogonal matrix parameters, we present $\\textit{polar expansion},$ a general\napproach to Monte Carlo simulation from probability distributions on the\nStiefel manifold. To bypass many of the well-established challenges of\nsimulating from the distribution of a random orthogonal matrix\n$\\boldsymbol{Q},$ we construct a distribution for an unconstrained random\nmatrix $\\boldsymbol{X}$ such that $\\boldsymbol{Q}_X,$ the orthogonal component\nof the polar decomposition of $\\boldsymbol{X},$ is equal in distribution to\n$\\boldsymbol{Q}.$ The distribution of $\\boldsymbol{X}$ is amenable to Markov\nchain Monte Carlo (MCMC) simulation using standard methods, and an\napproximation to the distribution of $\\boldsymbol{Q}$ can be recovered from a\nMarkov chain on the unconstrained space. When combined with modern MCMC\nsoftware, polar expansion allows for routine and flexible posterior inference\nin models with orthogonal matrix parameters. We find that polar expansion with\nadaptive Hamiltonian Monte Carlo is an order of magnitude more efficient than\ncompeting MCMC approaches in a benchmark protein interaction network\napplication. We also propose a new approach to Bayesian functional principal\ncomponents analysis which we illustrate in a meteorological time series\napplication.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 16:55:42 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Jauch", "Michael", ""], ["Hoff", "Peter D.", ""], ["Dunson", "David B.", ""]]}, {"id": "1906.07828", "submitter": "Daniel Zilber", "authors": "Daniel Zilber, Matthias Katzfuss", "title": "Vecchia-Laplace approximations of generalized Gaussian processes for big\n  non-Gaussian spatial data", "comments": "26 pages, 10 figures, code available at\n  https://github.com/katzfuss-group/GPvecchia-Laplace", "journal-ref": "Computational Statistics & Data Analysis (2021), 153, 107081", "doi": "10.1016/j.csda.2020.107081", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Gaussian processes (GGPs) are highly flexible models that combine\nlatent GPs with potentially non-Gaussian likelihoods from the exponential\nfamily. GGPs can be used in a variety of settings, including GP classification,\nnonparametric count regression, modeling non-Gaussian spatial data, and\nanalyzing point patterns. However, inference for GGPs can be analytically\nintractable, and large datasets pose computational challenges due to the\ninversion of the GP covariance matrix. We propose a Vecchia-Laplace\napproximation for GGPs, which combines a Laplace approximation to the\nnon-Gaussian likelihood with a computationally efficient Vecchia approximation\nto the GP, resulting in a simple, general, scalable, and accurate methodology.\nWe provide numerical studies and comparisons on simulated and real spatial\ndata. Our methods are implemented in a freely available R package.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 21:58:30 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 17:49:32 GMT"}, {"version": "v3", "created": "Mon, 6 Jan 2020 20:39:42 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2020 20:18:45 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zilber", "Daniel", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "1906.08018", "submitter": "JInglai Li", "authors": "Xin Cai, Guang Lin, Jinglai Li", "title": "Bayesian inverse regression for dimension reduction with small datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider supervised dimension reduction problems, namely to identify a low\ndimensional projection of the predictors $\\-x$ which can retain the statistical\nrelationship between $\\-x$ and the response variable $y$. We follow the idea of\nthe sliced inverse regression (SIR) and the sliced average variance estimation\n(SAVE) type of methods, which is to use the statistical information of the\nconditional distribution $\\pi(\\-x|y)$ to identify the dimension reduction (DR)\nspace. In particular we focus on the task of computing this conditional\ndistribution without slicing the data. We propose a Bayesian framework to\ncompute the conditional distribution where the likelihood function is obtained\nusing the Gaussian process regression model. The conditional distribution\n$\\pi(\\-x|y)$ can then be computed directly via Monte Carlo sampling. We then\ncan perform DR by considering certain moment functions (e.g. the first or the\nsecond moment) of the samples of the posterior distribution. With numerical\nexamples, we demonstrate that the proposed method is especially effective for\nsmall data problems.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 10:49:56 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 20:50:25 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 20:20:16 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Cai", "Xin", ""], ["Lin", "Guang", ""], ["Li", "Jinglai", ""]]}, {"id": "1906.08110", "submitter": "Adolphus Wagala", "authors": "Adolphus Wagala, Graciela Gonzalez-Far{\\i}as, Rogelio Ramos, Oscar\n  Dalmau", "title": "PLS Generalized Linear Regression and Kernel Multilogit Algorithm (KMA)\n  for Microarray Data Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We implement extensions of the partial least squares generalized linear\nregression (PLSGLR) due to Bastien et al. (2005) through its combination with\nlogistic regression and linear discriminant analysis, to get a partial least\nsquares generalized linear regression-logistic regression model (PLSGLR-log),\nand a partial least squares generalized linear regression-linear discriminant\nanalysis model (PLSGLRDA). These two classification methods are then compared\nwith classical methodologies like the k-nearest neighbours (KNN), linear\ndiscriminant analysis (LDA), partial least squares discriminant analysis\n(PLSDA), ridge partial least squares (RPLS), and support vector machines(SVM).\nFurthermore, we implement the kernel multilogit algorithm (KMA) by Dalmau et\nal. (2015)and compare its performance with that of the other classifiers. The\nresults indicate that for both un-preprocessed and preprocessed data, the KMA\nhas the lowest classification error rates.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 14:17:39 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Wagala", "Adolphus", ""], ["Gonzalez-Far\u0131as", "Graciela", ""], ["Ramos", "Rogelio", ""], ["Dalmau", "Oscar", ""]]}, {"id": "1906.08147", "submitter": "Bernardo Nipoti", "authors": "Antonio Canale, Riccardo Corradin, Bernardo Nipoti", "title": "Importance conditional sampling for Pitman-Yor mixtures", "comments": "42 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric mixture models based on the Pitman-Yor process represent a\nflexible tool for density estimation and clustering. Natural generalization of\nthe popular class of Dirichlet process mixture models, they allow for more\nrobust inference on the number of components characterizing the distribution of\nthe data. We propose a new sampling strategy for such models, named importance\nconditional sampling (ICS), which combines appealing properties of existing\nmethods, including easy interpretability and straightforward quantification of\nposterior uncertainty. An extensive simulation study highlights the efficiency\nof the proposed method which, unlike other conditional samplers, is robust to\nthe specification of the parameters characterizing the Pitman-Yor process. The\nICS also proves more efficient than the marginal sampler, as soon as the sample\nsize is not small, and, importantly, the step to update latent parameters is\nfully parallelizable. We further show that the ICS approach can be naturally\nextended to other classes of computationally demanding models, such as\nnonparametric mixture models for partially exchangeable data. The behaviour of\nour method is illustrated by analysing a rich dataset from the Collaborative\nPerinatal Project.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 15:21:41 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 17:46:34 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Canale", "Antonio", ""], ["Corradin", "Riccardo", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1906.08198", "submitter": "Juan Domingo Gonzalez", "authors": "Juan D. Gonzalez, Victor J. Yohai, Ruben H. Zamar", "title": "Robust Clustering Using Tau-Scales", "comments": "40 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K means is a popular non-parametric clustering procedure introduced by\nSteinhaus (1956) and further developed by MacQueen (1967). It is known,\nhowever, that K means does not perform well in the presence of outliers.\nCuesta-Albertos et al (1997) introduced a robust alternative, trimmed K means,\nwhich can be tuned to be robust or efficient, but cannot achieve these two\nproperties simultaneously in an adaptive way. To overcome this limitation we\npropose a new robust clustering procedure called K Tau Centers, which is based\non the concept of Tau scale introduced by Yohai and Zamar (1988). We show that\nK Tau Centers performs well in extensive simulation studies and real data\nexamples. We also show that the centers found by the proposed method are\nconsistent estimators of the \"true\" centers defined as the minimizers of the\nthe objective function at the population level.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 16:18:39 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Gonzalez", "Juan D.", ""], ["Yohai", "Victor J.", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1906.08850", "submitter": "Topi Paananen", "authors": "Topi Paananen, Juho Piironen, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Implicitly Adaptive Importance Sampling", "comments": "Major revision: More comparisons to adaptive importance sampling with\n  parametric distributions", "journal-ref": "Stat Comput 31, 16 (2021)", "doi": "10.1007/s11222-020-09982-2", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance sampling is a class of techniques for finding good\nproposal distributions for importance sampling. Often the proposal\ndistributions are standard probability distributions whose parameters are\nadapted based on the mismatch between the current proposal and a target\ndistribution. In this work, we present an implicit adaptive importance sampling\nmethod that applies to complicated distributions which are not available in\nclosed form. The method iteratively matches the moments of a set of Monte Carlo\ndraws to weighted moments based on importance weights. We apply the method to\nBayesian leave-one-out cross-validation and show that it performs better than\nmany existing parametric adaptive importance sampling methods while being\ncomputationally inexpensive.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 21:16:35 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 12:16:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Paananen", "Topi", ""], ["Piironen", "Juho", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "1906.08884", "submitter": "Yuchao Liu", "authors": "Yuchao Liu, Ery Arias-Castro", "title": "A Multiscale Scan Statistic for Adaptive Submatrix Localization", "comments": "The original version was accepted by KDD2019 Research Track. Detail\n  of the proof is available at https://escholarship.org/uc/item/9wt627dg", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of localizing a submatrix with larger-than-usual\nentry values inside a data matrix, without the prior knowledge of the submatrix\nsize. We establish an optimization framework based on a multiscale scan\nstatistic, and develop algorithms in order to approach the optimizer. We also\nshow that our estimator only requires a signal strength of the same order as\nthe minimax estimator with oracle knowledge of the submatrix size, to exactly\nrecover the anomaly with high probability. We perform some simulations that\nshow that our estimator has superior performance compared to other estimators\nwhich do not require prior submatrix knowledge, while being comparatively\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 22:36:12 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Liu", "Yuchao", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "1906.09178", "submitter": "Michael Grayling", "authors": "Michael J Grayling, James MS Wason", "title": "A web application for the design of multi-arm clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-arm designs provide an effective means of evaluating several treatments\nwithin the same clinical trial. Given the large number of treatments now\navailable for testing in many disease areas, it has been argued that their\nutilisation should increase. However, for any given clinical trial there are\nnumerous possible multi-arm designs that could be used, and choosing between\nthem can be a difficult task. This task is complicated further by a lack of\navailable easy-to-use software for designing multi-arm trials. To aid the wider\nimplementation of multi-arm clinical trial designs, we have developed a web\napplication for sample size calculation when using a variety of popular\nmultiple comparison corrections. Furthermore, the application supports sample\nsize calculation to control several varieties of power, as well as the\ndetermination of optimised arm-wise allocation ratios. It is built using the\nShiny package in the R programming language, is free to access on any device\nwith an internet browser, and requires no programming knowledge to use. The\napplication provides the core information required by statisticians and\nclinicians to review the operating characteristics of a chosen multi-arm\nclinical trial design. We hope that it will assist with the future utilisation\nof such designs in practice.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 14:58:14 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Grayling", "Michael J", ""], ["Wason", "James MS", ""]]}, {"id": "1906.09388", "submitter": "Leming Qu", "authors": "Leming Qu and Yang Lu", "title": "Copula Density Estimation by Finite Mixture of Parametric Copula\n  Densities", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2019.1622720", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Copula density estimation method that is based on a finite mixture of\nheterogeneous parametric copula densities is proposed here. More specifically,\nthe mixture components are Clayton, Frank, Gumbel, T, and normal copula\ndensities, which are capable of capturing lower tail,strong central, upper\ntail, heavy tail, and symmetrical elliptical dependence, respectively. The\nmodel parameters are estimated by an interior-point algorithm for the\nconstrained maximum likelihood problem. The interior-point algorithm is\ncompared with the commonly used EM algorithm. Simulation and real data\napplication show that the proposed approach is effective to model complex\ndependencies for data in dimensions beyond two or three.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 05:11:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Qu", "Leming", ""], ["Lu", "Yang", ""]]}, {"id": "1906.10173", "submitter": "Peter Jacko", "authors": "Peter Jacko", "title": "The Finite-Horizon Two-Armed Bandit Problem with Binary Responses: A\n  Multidisciplinary Survey of the History, State of the Art, and Myths", "comments": "Submitted for publication in an academic journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the two-armed bandit problem, which often naturally\nappears per se or as a subproblem in some multi-armed generalizations, and\nserves as a starting point for introducing additional problem features. The\nconsideration of binary responses is motivated by its widespread applicability\nand by being one of the most studied settings. We focus on the undiscounted\nfinite-horizon objective, which is the most relevant in many applications. We\nmake an attempt to unify the terminology as this is different across\ndisciplines that have considered this problem, and present a unified model cast\nin the Markov decision process framework, with subject responses modelled using\nthe Bernoulli distribution, and the corresponding Beta distribution for\nBayesian updating. We give an extensive account of the history and state of the\nart of approaches from several disciplines, including design of experiments,\nBayesian decision theory, naive designs, reinforcement learning, biostatistics,\nand combination designs. We evaluate these designs, together with a few newly\nproposed, accurately computationally (using a newly written package in Julia\nprogramming language by the author) in order to compare their performance. We\nshow that conclusions are different for moderate horizons (typical in practice)\nthan for small horizons (typical in academic literature reporting computational\nresults). We further list and clarify a number of myths about this problem,\ne.g., we show that, computationally, much larger problems can be designed to\nBayes-optimality than what is commonly believed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:02:31 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Jacko", "Peter", ""]]}, {"id": "1906.10252", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens, David L. Buckeridge", "title": "Bayesian Clustering for Continuous-Time Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop clustering procedures for longitudinal trajectories based on a\ncontinuous-time hidden Markov model (CTHMM) and a generalized linear\nobservation model. Specifically in this paper, we carry out finite and infinite\nmixture model-based clustering for a CTHMM and achieve inference using Markov\nchain Monte Carlo (MCMC). For a finite mixture model with prior on the number\nof components, we implement reversible-jump MCMC to facilitate the\ntrans-dimensional move between different number of clusters. For a Dirichlet\nprocess mixture model, we utilize restricted Gibbs sampling split-merge\nproposals to expedite the MCMC algorithm. We employ proposed algorithms to the\nsimulated data as well as a real data example, and the results demonstrate the\ndesired performance of the new sampler.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:17:53 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 14:29:33 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 08:12:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""], ["Buckeridge", "David L.", ""]]}, {"id": "1906.10541", "submitter": "Qiang Liu", "authors": "Qiang Liu and Xin T. Tong", "title": "Accelerating Metropolis-within-Gibbs sampler with localized computations\n  of differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problem is ubiquitous in science and engineering, and Bayesian\nmethodologies are often used to infer the underlying parameters. For high\ndimensional temporal-spatial models, classical Markov chain Monte Carlo (MCMC)\nmethods are often slow to converge, and it is necessary to apply\nMetropolis-within-Gibbs (MwG) sampling on parameter blocks. However, the\ncomputation cost of each MwG iteration is typically $O(n^2)$, where $n$ is the\nmodel dimension. This can be too expensive in practice. This paper introduces a\nnew reduced computation method to bring down the computation cost to $O(n)$,\nfor the inverse initial value problem of a stochastic differential equation\n(SDE) with local interactions. The key observation is that each MwG proposal is\nonly different from the original iterate at one parameter block, and this\ndifference will only propagate within a local domain in the SDE computations.\nTherefore we can approximate the global SDE computation with a surrogate\nupdated only within the local domain for reduced computation cost. Both\ntheoretically and numerically, we show that the approximation errors can be\ncontrolled by the local domain size. We discuss how to implement the local\ncomputation scheme using Euler--Maruyama and 4th order Runge--Kutta methods. We\nnumerically demonstrate the performance of the proposed method with the Lorenz\n96 model and a linear stochastic flow model.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2019 08:17:39 GMT"}, {"version": "v2", "created": "Tue, 18 Feb 2020 06:45:42 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Liu", "Qiang", ""], ["Tong", "Xin T.", ""]]}, {"id": "1906.10564", "submitter": "Chris Oates", "authors": "Junyang Wang, Jon Cockayne, Chris J. Oates", "title": "A Role for Symmetry in the Bayesian Solution of Differential Equations", "comments": "A summary version of this manuscript appeared in the proceedings of\n  MaxEnt 2018 in London, UK; see arXiv:1805.07109", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of numerical methods, such as finite difference methods\nfor differential equations, as point estimators suggests that formal\nuncertainty quantification can also be performed in this context. Competing\nstatistical paradigms can be considered and Bayesian probabilistic numerical\nmethods (PNMs) are obtained when Bayesian statistical principles are deployed.\nBayesian PNM have the appealing property of being closed under composition,\nsuch that uncertainty due to different sources of discretisation in a numerical\nmethod can be jointly modelled and rigorously propagated. Despite recent\nattention, no exact Bayesian PNM for the numerical solution of ordinary\ndifferential equations (ODEs) has been proposed. This raises the fundamental\nquestion of whether exact Bayesian methods for (in general nonlinear) ODEs even\nexist. The purpose of this paper is to provide a positive answer for a limited\nclass of ODE. To this end, we work at a foundational level, where a novel\nBayesian PNM is proposed as a proof-of-concept. Our proposal is a synthesis of\nclassical Lie group methods, to exploit underlying symmetries in the gradient\nfield, and non-parametric regression in a transformed solution space for the\nODE. The procedure is presented in detail for first and second order ODEs and\nrelies on a certain strong technical condition -- existence of a solvable Lie\nalgebra -- being satisfied. Numerical illustrations are provided.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 15:59:27 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 13:05:14 GMT"}, {"version": "v3", "created": "Mon, 23 Sep 2019 11:11:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Oates", "Chris J.", ""]]}, {"id": "1906.10591", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Finn Lindgren, David Bolin, Anders Eklund and Mattias\n  Villani", "title": "Spatial 3D Mat\\'ern priors for fast whole-brain fMRI analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian whole-brain functional magnetic resonance imaging (fMRI) analysis\nwith three-dimensional spatial smoothing priors has been shown to produce\nstate-of-the-art activity maps without pre-smoothing the data. The proposed\ninference algorithms are computationally demanding however, and the proposed\nspatial priors have several less appealing properties, such as being improper\nand having infinite spatial range. We propose a statistical inference framework\nfor whole-brain fMRI analysis based on the class of Mat\\'ern covariance\nfunctions. The framework uses the Gaussian Markov random field (GMRF)\nrepresentation of possibly anisotropic spatial Mat\\'ern fields via the\nstochastic partial differential equation (SPDE) approach of Lindgren et al.\n(2011). This allows for more flexible and interpretable spatial priors, while\nmaintaining the sparsity required for fast inference in the high-dimensional\nwhole-brain setting. We develop an accelerated stochastic gradient descent\n(SGD) optimization algorithm for empirical Bayes (EB) inference of the spatial\nhyperparameters. Conditionally on the inferred hyperparameters, we make a fully\nBayesian treatment of the brain activity. The Mat\\'ern prior is applied to both\nsimulated and experimental task-fMRI data and clearly demonstrates that it is a\nmore reasonable choice than the previously used priors, using comparisons of\nactivity maps, prior simulation and cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:13:39 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 09:26:53 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindgren", "Finn", ""], ["Bolin", "David", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1906.10838", "submitter": "Guy Hawkins", "authors": "David Gunawan, Guy E. Hawkins, Robert Kohn, Minh-Ngoc Tran, Scott D.\n  Brown", "title": "Time-evolving psychological processes over repeated decisions", "comments": "50 pages, 12 figures, 2 tables, 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological experiments have participants repeat a simple task. This\nrepetition is often necessary in order to gain the statistical precision\nrequired to answer questions about quantitative theories of the psychological\nprocesses underlying performance. In such experiments, time-on-task can have\nsizable effects on performance, changing the psychological processes under\ninvestigation in interesting ways. These changes are often ignored, and the\nunderlying process is treated as static. We propose modern statistical\napproaches that are based on recent advances in particle Markov chain\nMonte-Carlo (MCMC) to extend a static model of decision-making to account for\ntime-varying changes in a psychologically plausible manner. Using data from\nthree highly-cited experiments we show that there are changes in performance\nwith time-on-task, and that these changes vary substantially over individuals.\nWe find strong evidence in favor of a hidden Markov switching process as an\nexplanation of time-varying effects. This embodies the psychological assumption\nthat participants switch between different cognitive states, representing\ndifferent modes of decision-making, and explains key long- and short-term\ndynamic effects in the data. The central idea of our approach can be applied\nquite generally to quantitative psychological theories, beyond the models and\ndata sets that we investigate.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:12:50 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 02:34:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gunawan", "David", ""], ["Hawkins", "Guy E.", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Brown", "Scott D.", ""]]}, {"id": "1906.11653", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Antonio Canale", "title": "Simultaneous Transformation and Rounding (STAR) Models for\n  Integer-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet powerful framework for modeling integer-valued data,\nsuch as counts, scores, and rounded data. The data-generating process is\ndefined by Simultaneously Transforming and Rounding (STAR) a continuous-valued\nprocess, which produces a flexible family of integer-valued distributions\ncapable of modeling zero-inflation, bounded or censored data, and over- or\nunderdispersion. The transformation is modeled as unknown for greater\ndistributional flexibility, while the rounding operation ensures a coherent\ninteger-valued data-generating process. An efficient MCMC algorithm is\ndeveloped for posterior inference and provides a mechanism for adaptation of\nsuccessful Bayesian models and algorithms for continuous data to the\ninteger-valued data setting. Using the STAR framework, we design a new Bayesian\nAdditive Regression Tree (BART) model for integer-valued data, which\ndemonstrates impressive predictive distribution accuracy for both synthetic\ndata and a large healthcare utilization dataset. For interpretable\nregression-based inference, we develop a STAR additive model, which offers\ngreater flexibility and scalability than existing integer-valued models. The\nSTAR additive model is applied to study the recent decline in Amazon river\ndolphins.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:56:39 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 15:27:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Canale", "Antonio", ""]]}, {"id": "1906.11920", "submitter": "Xiaojing Wang", "authors": "Xiaojing Wang, Jingang Miao, Yunting Sun", "title": "A Python Library For Empirical Calibration", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with biased data samples is a common task across many statistical\nfields. In survey sampling, bias often occurs due to unrepresentative samples.\nIn causal studies with observational data, the treated versus untreated group\nassignment is often correlated with covariates, i.e., not random. Empirical\ncalibration is a generic weighting method that presents a unified view on\ncorrecting or reducing the data biases for the tasks mentioned above. We\nprovide a Python library EC to compute the empirical calibration weights. The\nproblem is formulated as convex optimization and solved efficiently in the dual\nform. Compared to existing software, EC is both more efficient and robust. EC\nalso accommodates different optimization objectives, supports weight clipping,\nand allows inexact calibration, which improves usability. We demonstrate its\nusage across various experiments with both simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:18:29 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 18:58:10 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Wang", "Xiaojing", ""], ["Miao", "Jingang", ""], ["Sun", "Yunting", ""]]}, {"id": "1906.12074", "submitter": "Santosh Kumar", "authors": "Peter J. Forrester, Santosh Kumar", "title": "Recursion scheme for the largest $\\beta$-Wishart-Laguerre eigenvalue and\n  Landauer conductance in quantum transport", "comments": "Published version; 20 pages, 2 figures in the main text + 2 in the\n  Mathematica code towards the end", "journal-ref": "Journal of Physics A: Mathematical and Theoretical, Volume 52,\n  Page 42LT02, Year 2019", "doi": "10.1088/1751-8121/ab433c", "report-no": null, "categories": "math-ph cond-mat.mes-hall cond-mat.stat-mech math.MP stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest eigenvalue distribution of the Wishart-Laguerre ensemble, indexed\nby Dyson parameter $\\beta$ and Laguerre parameter $a$, is fundamental in\nmultivariate statistics and finds applications in diverse areas. Based on a\ngeneralization of the Selberg integral, we provide an effective recursion\nscheme to compute this distribution explicitly in both the original model, and\na fixed-trace variant, for $a,\\beta$ non-negative integers and finite matrix\nsize. For $\\beta = 2$ this circumvents known symbolic evaluation based on\ndeterminants which become impractical for large dimensions. Our exact results\nhave immediate applications in the areas of multiple channel communication and\nbipartite entanglement. Moreover, we are also led to the exact solution of a\nlong standing problem of finding a general result for Landauer conductance\ndistribution in a chaotic mesoscopic cavity with two ideal leads. Thus far,\nexact closed-form results for this were available only in the Fourier-Laplace\nspace or could be obtained on a case-by-case basis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:44:33 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 17:59:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Forrester", "Peter J.", ""], ["Kumar", "Santosh", ""]]}, {"id": "1906.12077", "submitter": "Remi Flamary", "authors": "Laurent Dragoni, R\\'emi Flamary, Karim Lounici, Patricia\n  Reynaud-Bouret", "title": "Large scale Lasso with windowed active set for convolutional spike\n  sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike sorting is a fundamental preprocessing step in neuroscience that is\ncentral to access simultaneous but distinct neuronal activities and therefore\nto better understand the animal or even human brain. But numerical complexity\nlimits studies that require processing large scale datasets in terms of number\nof electrodes, neurons, spikes and length of the recorded signals. We propose\nin this work a novel active set algorithm aimed at solving the Lasso for a\nclassical convolutional model. Our algorithm can be implemented efficiently on\nparallel architecture and has a linear complexity w.r.t. the temporal\ndimensionality which ensures scaling and will open the door to online spike\nsorting. We provide theoretical results about the complexity of the algorithm\nand illustrate it in numerical experiments along with results about the\naccuracy of the spike recovery and robustness to the regularization parameter.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:48:04 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Dragoni", "Laurent", ""], ["Flamary", "R\u00e9mi", ""], ["Lounici", "Karim", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "1906.12121", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean and Alberto De Luca and Chantal M. W. Tax and Max A.\n  Viergever and Alexander Leemans", "title": "Automated characterization of noise distributions in diffusion MRI data", "comments": "v3: Peer reviewed version v2: Manuscript as submitted to Medical\n  image analysis v1: Manuscript as submitted to Magnetic resonance in medicine", "journal-ref": "Medical Image Analysis, Volume 65, 2020, 101758, ISSN 1361-8415", "doi": "10.1016/j.media.2020.101758", "report-no": null, "categories": "eess.IV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the noise distribution in diffusion MRI is the centerpiece to\nquantify uncertainties arising from the acquisition process. Accurate\nestimation beyond textbook distributions often requires information about the\nacquisition process, which is usually not available. We introduce two new\nautomated methods using the moments and maximum likelihood equations of the\nGamma distribution to estimate all unknown parameters using only the magnitude\ndata. A rejection step is used to make the framework automatic and robust to\nartifacts. Simulations were created for two diffusion weightings with parallel\nimaging. Furthermore, MRI data of a water phantom with different combinations\nof parallel imaging were acquired. Finally, experiments on freely available\ndatasets are used to assess reproducibility when limited information about the\nacquisition protocol is available. Additionally, we demonstrated the\napplicability of the proposed methods for a bias correction and denoising task\non an in vivo dataset. A generalized version of the bias correction framework\nfor non integer degrees of freedom is also introduced. The proposed framework\nis compared with three other algorithms with datasets from three vendors,\nemploying different reconstruction methods. Simulations showed that assuming a\nRician distribution can lead to misestimation of the noise distribution in\nparallel imaging. Results showed that signal leakage in multiband can also lead\nto a misestimation of the noise distribution. Repeated acquisitions of in vivo\ndatasets show that the estimated parameters are stable and have lower\nvariability than compared methods. Results show that the proposed methods\nreduce the appearance of noise at high b-value. The proposed algorithms herein\ncan estimate both parameters of the noise distribution automatically, are\nrobust to signal leakage artifacts and perform best when used on acquired noise\nmaps.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:08:24 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 11:08:33 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 13:22:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["St-Jean", "Samuel", ""], ["De Luca", "Alberto", ""], ["Tax", "Chantal M. W.", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1906.12123", "submitter": "Darjus Hosszejni", "authors": "Darjus Hosszejni and Gregor Kastner", "title": "Modeling Univariate and Multivariate Stochastic Volatility in R with\n  stochvol and factorstochvol", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM q-fin.CP q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic volatility (SV) models are nonlinear state-space models that enjoy\nincreasing popularity for fitting and predicting heteroskedastic time series.\nHowever, due to the large number of latent quantities, their efficient\nestimation is non-trivial and software that allows to easily fit SV models to\ndata is rare. We aim to alleviate this issue by presenting novel\nimplementations of four SV models delivered in two R packages. Several unique\nfeatures are included and documented. As opposed to previous versions, stochvol\nis now capable of handling linear mean models, heavy-tailed SV, and SV with\nleverage. Moreover, we newly introduce factorstochvol which caters for\nmultivariate SV. Both packages offer a user-friendly interface through the\nconventional R generics and a range of tailor-made methods. Computational\nefficiency is achieved via interfacing R to C++ and doing the heavy work in the\nlatter. In the paper at hand, we provide a detailed discussion on Bayesian SV\nestimation and showcase the use of the new software through various examples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:35:36 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 10:25:56 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 15:57:14 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Hosszejni", "Darjus", ""], ["Kastner", "Gregor", ""]]}, {"id": "1906.12134", "submitter": "Gregor Kastner", "authors": "Gregor Kastner", "title": "Dealing with Stochastic Volatility in Time Series Using the R Package\n  stochvol", "comments": null, "journal-ref": "Journal of Statistical Software, 69(5), 1-30 (2016)", "doi": "10.18637/jss.v069.i05", "report-no": null, "categories": "stat.CO econ.EM q-fin.CP q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package stochvol provides a fully Bayesian implementation of\nheteroskedasticity modeling within the framework of stochastic volatility. It\nutilizes Markov chain Monte Carlo (MCMC) samplers to conduct inference by\nobtaining draws from the posterior distribution of parameters and latent\nvariables which can then be used for predicting future volatilities. The\npackage can straightforwardly be employed as a stand-alone tool; moreover, it\nallows for easy incorporation into other MCMC samplers. The main focus of this\npaper is to show the functionality of stochvol. In addition, it provides a\nbrief mathematical description of the model, an overview of the sampling\nschemes used, and several illustrative examples using exchange rate data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 11:16:00 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Kastner", "Gregor", ""]]}, {"id": "1906.12201", "submitter": "Julien  Chiquet Dr.", "authors": "Pierre Barbillon, Julien Chiquet, Timoth\\'ee Tabouy", "title": "missSBM: An R Package for Handling Missing Values in the Stochastic\n  Block Model", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Block Model (SBM) is a popular probabilistic model for random\ngraphs. It is commonly used for clustering network data by aggregating nodes\nthat share similar connectivity patterns into blocks. When fitting an SBM to a\nnetwork which is partially observed, it is important to take into account the\nunderlying process that generates the missing values, otherwise the inference\nmay be biased. This paper introduces missSBM, an R-package fitting the SBM when\nthe network is partially observed, i.e., the adjacency matrix contains not only\n1's or 0's encoding presence or absence of edges but also NA's encoding missing\ninformation between pairs of nodes. This package implements a set of algorithms\nfor fitting the binary SBM, possibly in the presence of external covariates, by\nperforming variational inference adapted to several observation processes. Our\nimplementation automatically explores different block numbers to select the\nmost relevant model according to the Integrated Classification Likelihood (ICL)\ncriterion. The ICL criterion can also help determine which observation process\nbetter corresponds to a given dataset. Finally, missSBM can be used to perform\nimputation of missing entries in the adjacency matrix. We illustrate the\npackage on a network data set consisting of interactions between political\nblogs sampled during the French presidential election in 2007.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:18:44 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 06:13:57 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 14:24:17 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Barbillon", "Pierre", ""], ["Chiquet", "Julien", ""], ["Tabouy", "Timoth\u00e9e", ""]]}, {"id": "1906.12215", "submitter": "Matthias Troffaes", "authors": "Nawapon Nakharutai and Matthias C. M. Troffaes and Camila C. S. Caiado", "title": "Improving and benchmarking of algorithms for decision making with lower\n  previsions", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.ijar.2019.06.008", "report-no": null, "categories": "math.OC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximality, interval dominance, and E-admissibility are three well-known\ncriteria for decision making under severe uncertainty using lower previsions.\nWe present a new fast algorithm for finding maximal gambles. We compare its\nperformance to existing algorithms, one proposed by Troffaes and Hable (2014),\nand one by Jansen, Augustin, and Schollmeyer (2017). To do so, we develop a new\nmethod for generating random decision problems with pre-specified ratios of\nmaximal and interval dominant gambles. Based on earlier work, we present\nefficient ways to find common feasible starting points in these algorithms. We\nthen exploit these feasible starting points to develop early stopping criteria\nfor the primal-dual interior point method, further improving efficiency. We\nfind that the primal-dual interior point method works best. We also investigate\nthe use of interval dominance to eliminate non-maximal gambles. This can make\nthe problem smaller, and we observe that this benefits Jansen et al.'s\nalgorithm, but perhaps surprisingly, not the other two algorithms. We find that\nour algorithm, without using interval dominance, outperforms all other\nalgorithms in all scenarios in our benchmarking.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:42:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Nakharutai", "Nawapon", ""], ["Troffaes", "Matthias C. M.", ""], ["Caiado", "Camila C. S.", ""]]}, {"id": "1906.12281", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli, Alain Durmus, Marcelo Pereyra, Ana F. Vidal", "title": "Efficient stochastic optimisation by unadjusted Langevin Monte Carlo.\n  Application to maximum marginal likelihood and empirical Bayesian estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic approximation methods play a central role in maximum likelihood\nestimation problems involving intractable likelihood functions, such as\nmarginal likelihoods arising in problems with missing or incomplete data, and\nin parametric empirical Bayesian estimation. Combined with Markov chain Monte\nCarlo algorithms, these stochastic optimisation methods have been successfully\napplied to a wide range of problems in science and industry. However, this\nstrategy scales poorly to large problems because of methodological and\ntheoretical difficulties related to using high-dimensional Markov chain Monte\nCarlo algorithms within a stochastic approximation scheme. This paper proposes\nto address these difficulties by using unadjusted Langevin algorithms to\nconstruct the stochastic approximation. This leads to a highly efficient\nstochastic optimisation methodology with favourable convergence properties that\ncan be quantified explicitly and easily checked. The proposed methodology is\ndemonstrated with three experiments, including a challenging application to\nhigh-dimensional statistical audio analysis and a sparse Bayesian logistic\nregression with random effects problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 16:05:29 GMT"}, {"version": "v2", "created": "Sat, 30 May 2020 13:08:08 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Pereyra", "Marcelo", ""], ["Vidal", "Ana F.", ""]]}, {"id": "1906.12309", "submitter": "Yang Ni", "authors": "Yang Ni, Yuan Ji, Peter Mueller", "title": "Consensus Monte Carlo for Random Subsets using Shared Anchors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a consensus Monte Carlo algorithm that scales existing Bayesian\nnonparametric models for clustering and feature allocation to big data. The\nalgorithm is valid for any prior on random subsets such as partitions and\nlatent feature allocation, under essentially any sampling model. Motivated by\nthree case studies, we focus on clustering induced by a Dirichlet process\nmixture sampling model, inference under an Indian buffet process prior with a\nbinomial sampling model, and with a categorical sampling model. We assess the\nproposed algorithm with simulation studies and show results for inference with\nthree datasets: an MNIST image dataset, a dataset of pancreatic cancer\nmutations, and a large set of electronic health records (EHR). Supplementary\nmaterials for this article are available online.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 16:57:33 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 17:31:31 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ni", "Yang", ""], ["Ji", "Yuan", ""], ["Mueller", "Peter", ""]]}]