[{"id": "1903.00617", "submitter": "Reza Hajargasht", "authors": "Reza Hajargasht", "title": "Approximation Properties of Variational Bayes for Vector Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a recent approximate method for Bayesian inference.\nIt has the merit of being a fast and scalable alternative to Markov Chain Monte\nCarlo (MCMC) but its approximation error is often unknown. In this paper, we\nderive the approximation error of VB in terms of mean, mode, variance,\npredictive density and KL divergence for the linear Gaussian multi-equation\nregression. Our results indicate that VB approximates the posterior mean\nperfectly. Factors affecting the magnitude of underestimation in posterior\nvariance and mode are revealed. Importantly, We demonstrate that VB estimates\npredictive densities accurately.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 03:24:16 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Hajargasht", "Reza", ""]]}, {"id": "1903.00870", "submitter": "Tiangang Cui", "authors": "Johnathan Bardsley and Tiangang Cui and Youssef Marzouk and Zheng Wang", "title": "Scalable optimization-based sampling on function space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization-based samplers such as randomize-then-optimize (RTO) [2] provide\nan efficient and parallellizable approach to solving large-scale Bayesian\ninverse problems. These methods solve randomly perturbed optimization problems\nto draw samples from an approximate posterior distribution. \"Correcting\" these\nsamples, either by Metropolization or importance sampling, enables\ncharacterization of the original posterior distribution. This paper focuses on\nthe scalability of RTO to problems with high- or infinite-dimensional\nparameters. We introduce a new subspace acceleration strategy that makes the\ncomputational complexity of RTO scale linearly with the parameter dimension.\nThis subspace perspective suggests a natural extension of RTO to a function\nspace setting. We thus formalize a function space version of RTO and establish\nsufficient conditions for it to produce a valid Metropolis-Hastings proposal,\nyielding dimension-independent sampling performance. Numerical examples\ncorroborate the dimension-independence of RTO and demonstrate sampling\nperformance that is also robust to small observational noise.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 09:35:36 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 04:15:34 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Bardsley", "Johnathan", ""], ["Cui", "Tiangang", ""], ["Marzouk", "Youssef", ""], ["Wang", "Zheng", ""]]}, {"id": "1903.00939", "submitter": "Sebastian Schmon", "authors": "Sebastian M Schmon, Arnaud Doucet, George Deligiannidis", "title": "Bernoulli Race Particle Filters", "comments": "19 pages", "journal-ref": "The 22nd International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2019)", "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the weights in a particle filter are not available analytically,\nstandard resampling methods cannot be employed. To circumvent this problem\nstate-of-the-art algorithms replace the true weights with non-negative unbiased\nestimates. This algorithm is still valid but at the cost of higher variance of\nthe resulting filtering estimates in comparison to a particle filter using the\ntrue weights. We propose here a novel algorithm that allows for resampling\naccording to the true intractable weights when only an unbiased estimator of\nthe weights is available. We demonstrate our algorithm on several examples.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 16:49:53 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Doucet", "Arnaud", ""], ["Deligiannidis", "George", ""]]}, {"id": "1903.01029", "submitter": "Yingying Xu", "authors": "Yingying Xu, Joon Lee, Joel A. Dubin", "title": "Similarity-based Random Survival Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting time-to-event outcomes in large databases can be a challenging but\nimportant task. One example of this is in predicting the time to a clinical\noutcome for patients in intensive care units (ICUs), which helps to support\ncritical medical treatment decisions. In this context, the time to an event of\ninterest could be, for example, survival time or time to recovery from a\ndisease/ailment observed within the ICU. The massive health datasets generated\nfrom the uptake of Electronic Health Records (EHRs) are quite heterogeneous as\npatients can be quite dissimilar in their relationship between the feature\nvector and the outcome, adding more noise than information to prediction. In\nthis paper, we propose a modified random forest method for survival data that\nidentifies similar cases in an attempt to improve accuracy for predicting\ntime-to-event outcomes; this methodology can be applied in various settings,\nincluding with ICU databases. We also introduce an adaptation of our\nmethodology in the case of dependent censoring. Our proposed method is\ndemonstrated in the Medical Information Mart for Intensive Care (MIMIC-III)\ndatabase, and, in addition, we present properties of our methodology through a\ncomprehensive simulation study. Introducing similarity to the random survival\nforest method indeed provides improved predictive accuracy compared to random\nsurvival forest alone across the various analyses we undertook.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:09:37 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 14:10:34 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Xu", "Yingying", ""], ["Lee", "Joon", ""], ["Dubin", "Joel A.", ""]]}, {"id": "1903.01138", "submitter": "Irene Tubikanec", "authors": "Evelyn Buckwar, Massimiliano Tamborrino, Irene Tubikanec", "title": "Spectral Density-Based and Measure-Preserving ABC for partially observed\n  diffusion processes. An illustration on Hamiltonian SDEs", "comments": "35 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) has become one of the major tools of\nlikelihood-free statistical inference in complex mathematical models.\nSimultaneously, stochastic differential equations (SDEs) have developed to an\nestablished tool for modelling time dependent, real world phenomena with\nunderlying random effects. When applying ABC to stochastic models, two major\ndifficulties arise. First, the derivation of effective summary statistics and\nproper distances is particularly challenging, since simulations from the\nstochastic process under the same parameter configuration result in different\ntrajectories. Second, exact simulation schemes to generate trajectories from\nthe stochastic model are rarely available, requiring the derivation of suitable\nnumerical methods for the synthetic data generation. To obtain summaries that\nare less sensitive to the intrinsic stochasticity of the model, we propose to\nbuild up the statistical method (e.g., the choice of the summary statistics) on\nthe underlying structural properties of the model. Here, we focus on the\nexistence of an invariant measure and we map the data to their estimated\ninvariant density and invariant spectral density. Then, to ensure that these\nmodel properties are kept in the synthetic data generation, we adopt\nmeasure-preserving numerical splitting schemes. The derived property-based and\nmeasure-preserving ABC method is illustrated on the broad class of partially\nobserved Hamiltonian type SDEs, both with simulated data and with real\nelectroencephalography (EEG) data. The proposed ingredients can be incorporated\ninto any type of ABC algorithm and directly applied to all SDEs that are\ncharacterised by an invariant distribution and for which a measure-preserving\nnumerical method can be derived.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 08:59:02 GMT"}, {"version": "v2", "created": "Fri, 5 Jul 2019 16:47:20 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Buckwar", "Evelyn", ""], ["Tamborrino", "Massimiliano", ""], ["Tubikanec", "Irene", ""]]}, {"id": "1903.01601", "submitter": "Behnam Malmir", "authors": "Behnam Malmir, Shing I Chang, Malgorzata Rys and Dylan Darter", "title": "Quantifying Gait Changes Using Microsoft Kinect and Sample Entropy", "comments": "This article is an updated version of a paper entitled 'Quantifying\n  Gait Changes Using Microsoft Kinect and Sample Entropy' presented at the 2018\n  Industrial and Systems Engineering Research Conference (ISERC) in Orlando,\n  Florida (May 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CG cs.HC stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study describes a method to quantify potential gait changes in human\nsubjects. Microsoft Kinect devices were used to provide and track coordinates\nof fifteen different joints of a subject over time. Three male subjects walk a\n10-foot path multiple times with and without motion-restricting devices. Their\nwalking patterns were recorded via two Kinect devices through frontal and\nsagittal planes. A modified sample entropy (SE) value was computed to quantify\nthe variability of the time series for each joint. The SE values with and\nwithout motion-restricting devices were used to compare the changes in each\njoint. The preliminary results of the experiments show that the proposed\nquantification method can detect differences in walking patterns with and\nwithout motion-restricting devices. The proposed method has the potential to be\napplied to track personal progress in physical therapy sessions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:16:23 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""], ["Rys", "Malgorzata", ""], ["Darter", "Dylan", ""]]}, {"id": "1903.01680", "submitter": "Daniel Andrade", "authors": "Daniel Andrade, Kenji Fukumizu, Yuzuru Okajima", "title": "Convex Covariate Clustering for Classification", "comments": "Under consideration at Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering, like covariate selection for classification, is an important step\nto compress and interpret the data. However, clustering of covariates is often\nperformed independently of the classification step, which can lead to\nundesirable clustering results that harm interpretability and compression rate.\nTherefore, we propose a method that can cluster covariates while taking into\naccount class label information of samples. We formulate the problem as a\nconvex optimization problem which uses both, a-priori similarity information\nbetween covariates, and information from class-labeled samples. Like ordinary\nconvex clustering [Chi and Lange, 2015], the proposed method offers a unique\nglobal minima making it insensitive to initialization. In order to solve the\nconvex problem, we propose a specialized alternating direction method of\nmultipliers (ADMM), which scales up to several thousands of variables.\nFurthermore, in order to circumvent computationally expensive cross-validation,\nwe propose a model selection criterion based on approximating the marginal\nlikelihood. Experiments on synthetic and real data confirm the usefulness of\nthe proposed clustering method and the selection criterion.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 05:36:43 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 02:21:23 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Andrade", "Daniel", ""], ["Fukumizu", "Kenji", ""], ["Okajima", "Yuzuru", ""]]}, {"id": "1903.02075", "submitter": "JInglai Li", "authors": "Qingping Zhou, Tengchao Yu, Xiaoqun Zhang, Jinglai Li", "title": "Bayesian inference and uncertainty quantification for image\n  reconstruction with Poisson data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a complete framework for performing infinite-dimensional Bayesian\ninference and uncertainty quantification for image reconstruction with Poisson\ndata. In particular, we address the following issues to make the Bayesian\nframework applicable in practice. We first introduce a positivity-preserving\nreparametrization, and we prove that under the reparametrization and a hybrid\nprior, the posterior distribution is well-posed in the infinite dimensional\nsetting. Second we provide a dimension-independent MCMC algorithm, based on the\npreconditioned Crank-Nicolson Langevin method, in which we use a primal-dual\nscheme to compute the offset direction. Third we give a method combining the\nmodel discrepancy method and maximum likelihood estimation to determine the\nregularization parameter in the hybrid prior. Finally we propose to use the\nobtained posterior distribution to detect artifacts in a recovered image. We\nprovide an example to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 21:56:23 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 10:55:06 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 10:57:36 GMT"}, {"version": "v4", "created": "Sun, 20 Oct 2019 18:08:16 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Zhou", "Qingping", ""], ["Yu", "Tengchao", ""], ["Zhang", "Xiaoqun", ""], ["Li", "Jinglai", ""]]}, {"id": "1903.02278", "submitter": "Diviyan Kalainathan", "authors": "Diviyan Kalainathan, Olivier Goudet", "title": "Causal Discovery Toolbox: Uncover causal relationships in Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper presents a new open source Python framework for causal discovery\nfrom observational data and domain background knowledge, aimed at causal graph\nand causal mechanism modeling. The 'cdt' package implements the end-to-end\napproach, recovering the direct dependencies (the skeleton of the causal graph)\nand the causal relationships between variables. It includes algorithms from the\n'Bnlearn' and 'Pcalg' packages, together with algorithms for pairwise causal\ndiscovery such as ANM. 'cdt' is available under the MIT License at\nhttps://github.com/Diviyan-Kalainathan/CausalDiscoveryToolbox.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 10:03:20 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Kalainathan", "Diviyan", ""], ["Goudet", "Olivier", ""]]}, {"id": "1903.02699", "submitter": "Alessandro Barp", "authors": "Alessandro Barp, Anthony Kennedy, Mark Girolami", "title": "Hamiltonian Monte Carlo on Symmetric and Homogeneous Spaces via\n  Symplectic Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hamiltonian Monte Carlo method generates samples by introducing a\nmechanical system that explores the target density. For distributions on\nmanifolds it is not always simple to perform the mechanics as a result of the\nlack of global coordinates, the constraints of the manifold, and the\nrequirement to compute the geodesic flow. In this paper we explain how to\nconstruct the Hamiltonian system on naturally reductive homogeneous spaces\nusing symplectic reduction, which lifts the HMC scheme to a matrix Lie group\nwith global coordinates and constant metric. This provides a general framework\nthat is applicable to many manifolds that arise in applications, such as\nhyperspheres, hyperbolic spaces, symmetric positive-definite matrices,\nGrassmannian, and Stiefel manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 02:38:20 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 00:35:59 GMT"}], "update_date": "2019-04-22", "authors_parsed": [["Barp", "Alessandro", ""], ["Kennedy", "Anthony", ""], ["Girolami", "Mark", ""]]}, {"id": "1903.02774", "submitter": "Katarzyna Reluga", "authors": "Katarzyna Reluga, Mar\\'ia Jos\\'e Lombard\\'ia, Stefan Andreas Sperlich", "title": "Simultaneous inference for mixed and small area parameters", "comments": "28 pages, 5 figures, a new version includes some changes regarding\n  the notation as well as methodological developments of the construction of\n  simultaneous prediction intervals and multiple tests", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address simultaneous inference for mixed parameters which are the key\ningredients in small area estimation. We assume linear mixed model framework.\nFirstly, we analyse statistical properties of a max-type statistic and use it\nto construct simultaneous prediction intervals as well as to implement multiple\ntesting procedure. Secondly, we derive bands based on the volume-of-tube\nformula. In addition, we adapt some of the simultaneous inference methods from\nregression and nonparametric curve estimation and compare them with our\napproaches. Simultaneous intervals are necessary to compare clusters since the\npresently available intervals are not statistically valid for such analysis.\nThe proposed testing procedures can be used to validate certain statements\nabout the set of mixed parameters or to test pairwise differences. Our proposal\nis accompanied by simulation experiments and a data example on small area\nhousehold incomes. Both of them demonstrate an excellent performance and\nutility of our techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:53:13 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 08:18:16 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Reluga", "Katarzyna", ""], ["Lombard\u00eda", "Mar\u00eda Jos\u00e9", ""], ["Sperlich", "Stefan Andreas", ""]]}, {"id": "1903.02787", "submitter": "Feng Li", "authors": "Yanfei Kang, Rob J Hyndman, Feng Li", "title": "GRATIS: GeneRAting TIme Series with diverse and controllable\n  characteristics", "comments": null, "journal-ref": "Statistical Analysis and Data Mining 2020", "doi": "10.1002/sam.11461", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion of time series data in recent years has brought a flourish of\nnew time series analysis methods, for forecasting, clustering, classification\nand other tasks. The evaluation of these new methods requires either collecting\nor simulating a diverse set of time series benchmarking data to enable reliable\ncomparisons against alternative approaches. We propose GeneRAting TIme Series\nwith diverse and controllable characteristics, named GRATIS, with the use of\nmixture autoregressive (MAR) models. We simulate sets of time series using MAR\nmodels and investigate the diversity and coverage of the generated time series\nin a time series feature space. By tuning the parameters of the MAR models,\nGRATIS is also able to efficiently generate new time series with controllable\nfeatures. In general, as a costless surrogate to the traditional data\ncollection approach, GRATIS can be used as an evaluation tool for tasks such as\ntime series forecasting and classification. We illustrate the usefulness of our\ntime series generation process through a time series forecasting application.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 09:29:31 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 13:07:19 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kang", "Yanfei", ""], ["Hyndman", "Rob J", ""], ["Li", "Feng", ""]]}, {"id": "1903.02964", "submitter": "Kody Law", "authors": "Ryan Bennink, Ajay Jasra, Kody J. H. Law, Pavel Lougovski", "title": "Estimation and uncertainty quantification for the output from quantum\n  simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating certain distributions over $\\{0,1\\}^d$ is\nconsidered here. The distribution represents a quantum system of $d$ qubits,\nwhere there are non-trivial dependencies between the qubits. A maximum entropy\napproach is adopted to reconstruct the distribution from exact moments or\nobserved empirical moments. The Robbins Monro algorithm is used to solve the\nintractable maximum entropy problem, by constructing an unbiased estimator of\nthe un-normalized target with a sequential Monte Carlo sampler at each\niteration. In the case of empirical moments, this coincides with a maximum\nlikelihood estimator. A Bayesian formulation is also considered in order to\nquantify posterior uncertainty. Several approaches are proposed in order to\ntackle this challenging problem, based on recently developed methodologies. In\nparticular, unbiased estimators of the gradient of the log posterior are\nconstructed and used within a provably convergent Langevin-based Markov chain\nMonte Carlo method. The methods are illustrated on classically simulated output\nfrom quantum simulators.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:55:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Bennink", "Ryan", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Lougovski", "Pavel", ""]]}, {"id": "1903.03028", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley and Sudipto Banerjee", "title": "Bayesian spatially varying coefficient models in the spBayes R package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and illustrates new functionality for fitting spatially\nvarying coefficients models in the spBayes (version 0.4-2) R package. The new\nspSVC function uses a computationally efficient Markov chain Monte Carlo\nalgorithm and extends current spBayes functions, that fit only space-varying\nintercept regression models, to fit independent or multivariate Gaussian\nprocess random effects for any set of columns in the regression design matrix.\nNewly added OpenMP parallelization options for spSVC are discussed and\nillustrated, as well as helper functions for joint and point-wise prediction\nand model fit diagnostics. The utility of the proposed models is illustrated\nusing a PM10 analysis over central Europe.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 16:28:14 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2019 16:36:57 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1903.03704", "submitter": "Pavel Sountsov", "authors": "Matthew Hoffman, Pavel Sountsov, Joshua V. Dillon, Ian Langmore,\n  Dustin Tran, Srinivas Vasudevan", "title": "NeuTra-lizing Bad Geometry in Hamiltonian Monte Carlo Using Neural\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hamiltonian Monte Carlo is a powerful algorithm for sampling from\ndifficult-to-normalize posterior distributions. However, when the geometry of\nthe posterior is unfavorable, it may take many expensive evaluations of the\ntarget distribution and its gradient to converge and mix. We propose neural\ntransport (NeuTra) HMC, a technique for learning to correct this sort of\nunfavorable geometry using inverse autoregressive flows (IAF), a powerful\nneural variational inference technique. The IAF is trained to minimize the KL\ndivergence from an isotropic Gaussian to the warped posterior, and then HMC\nsampling is performed in the warped space. We evaluate NeuTra HMC on a variety\nof synthetic and real problems, and find that it significantly outperforms\nvanilla HMC both in time to reach the stationary distribution and asymptotic\neffective-sample-size rates.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 00:23:26 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Hoffman", "Matthew", ""], ["Sountsov", "Pavel", ""], ["Dillon", "Joshua V.", ""], ["Langmore", "Ian", ""], ["Tran", "Dustin", ""], ["Vasudevan", "Srinivas", ""]]}, {"id": "1903.04478", "submitter": "Ali Taylan Cemgil", "authors": "Ali Taylan Cemgil, Mehmet Burak Kurutmaz, Sinan Yildirim, Melih\n  Barsbey, Umut Simsekli", "title": "Bayesian Allocation Model: Inference by Sequential Monte Carlo for\n  Nonnegative Tensor Factorizations and Topic Models using Polya Urns", "comments": "70 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic generative model, Bayesian allocation model (BAM),\nwhich establishes explicit connections between nonnegative tensor factorization\n(NTF), graphical models of discrete probability distributions and their\nBayesian extensions, and the topic models such as the latent Dirichlet\nallocation. BAM is based on a Poisson process, whose events are marked by using\na Bayesian network, where the conditional probability tables of this network\nare then integrated out analytically. We show that the resulting marginal\nprocess turns out to be a Polya urn, an integer valued self-reinforcing\nprocess. This urn processes, which we name a Polya-Bayes process, obey certain\nconditional independence properties that provide further insight about the\nnature of NTF. These insights also let us develop space efficient simulation\nalgorithms that respect the potential sparsity of data: we propose a class of\nsequential importance sampling algorithms for computing NTF and approximating\ntheir marginal likelihood, which would be useful for model selection. The\nresulting methods can also be viewed as a model scoring method for topic models\nand discrete Bayesian networks with hidden variables. The new algorithms have\nfavourable properties in the sparse data regime when contrasted with\nvariational algorithms that become more accurate when the total sum of the\nelements of the observed tensor goes to infinity. We illustrate the performance\non several examples and numerically study the behaviour of the algorithms for\nvarious data regimes.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:54:59 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Cemgil", "Ali Taylan", ""], ["Kurutmaz", "Mehmet Burak", ""], ["Yildirim", "Sinan", ""], ["Barsbey", "Melih", ""], ["Simsekli", "Umut", ""]]}, {"id": "1903.04754", "submitter": "Sayan Putatunda PhD", "authors": "Sayan Putatunda, Kiran Rama, Dayananda Ubrangala, Ravi Kondapalli", "title": "SmartEDA: An R Package for Automated Exploratory Data Analysis", "comments": "Pre-print submitted to the the Journal of Statistical Software", "journal-ref": "Journal of Open Source Software, 4(41), 1509 (2019)", "doi": "10.21105/joss.01509", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SmartEDA, which is an R package for performing\nExploratory data analysis (EDA). EDA is generally the first step that one needs\nto perform before developing any machine learning or statistical models. The\ngoal of EDA is to help someone perform the initial investigation to know more\nabout the data via descriptive statistics and visualizations. In other words,\nthe objective of EDA is to summarize and explore the data. The need for EDA\nbecame one of the factors that led to the development of various statistical\ncomputing packages over the years including the R programming language that is\na very popular and currently the most widely used software for statistical\ncomputing. However, EDA is a very tedious task, requires some manual effort and\nsome of the open source packages available in R are not just upto the mark. In\nthis paper, we propose a new open source package i.e. SmartEDA for R to address\nthe need for automation of exploratory data analysis. We discuss the various\nfeatures of SmartEDA and illustrate some of its applications for generating\nactionable insights using a couple of real-world datasets. We also perform a\ncomparative study of SmartEDA with respect to other packages available for\nexploratory data analysis in the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 07:29:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Putatunda", "Sayan", ""], ["Rama", "Kiran", ""], ["Ubrangala", "Dayananda", ""], ["Kondapalli", "Ravi", ""]]}, {"id": "1903.04797", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Elements of Sequential Monte Carlo", "comments": "Under review at Foundations and Trends in Machine Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in statistics and probabilistic machine learning is to compute\nprobability distributions and expectations. This is the fundamental problem of\nBayesian statistics and machine learning, which frames all inference as\nexpectations with respect to the posterior distribution. The key challenge is\nto approximate these intractable expectations. In this tutorial, we review\nsequential Monte Carlo (SMC), a random-sampling-based class of methods for\napproximate inference. First, we explain the basics of SMC, discuss practical\nissues, and review theoretical results. We then examine two of the main user\ndesign choices: the proposal distributions and the so called intermediate\ntarget distributions. We review recent results on how variational inference and\namortization can be used to learn efficient proposals and target distributions.\nNext, we discuss the SMC estimate of the normalizing constant, how this can be\nused for pseudo-marginal inference and inference evaluation. Throughout the\ntutorial we illustrate the use of SMC on various models commonly used in\nmachine learning, such as stochastic recurrent neural networks, probabilistic\ngraphical models, and probabilistic programs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 09:28:05 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1903.04881", "submitter": "John Muschelli III", "authors": "John Muschelli", "title": "ROC and AUC with a Binary Predictor: a Potentially Misleading Metric", "comments": "16 pages, 3 figures, code. J Classif (2019)", "journal-ref": null, "doi": "10.1007/s00357-019-09345-1", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In analysis of binary outcomes, the receiver operator characteristic (ROC)\ncurve is heavily used to show the performance of a model or algorithm. The ROC\ncurve is informative about the performance over a series of thresholds and can\nbe summarized by the area under the curve (AUC), a single number. When a\npredictor is categorical, the ROC curve has one less than number of categories\nas potential thresholds; when the predictor is binary there is only one\nthreshold. As the AUC may be used in decision-making processes on determining\nthe best model, it important to discuss how it agrees with the intuition from\nthe ROC curve. We discuss how the interpolation of the curve between thresholds\nwith binary predictors can largely change the AUC. Overall, we show using a\nlinear interpolation from the ROC curve with binary predictors corresponds to\nthe estimated AUC, which is most commonly done in software, which we believe\ncan lead to misleading results. We compare R, Python, Stata, and SAS software\nimplementations. We recommend using reporting the interpolation used and\ndiscuss the merit of using the step function interpolator, also referred to as\nthe \"pessimistic\" approach by Fawcett (2006).\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 12:57:58 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 21:28:48 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Muschelli", "John", ""]]}, {"id": "1903.05309", "submitter": "Song Li", "authors": "Song Li and Geoffrey K. F. Tso", "title": "Generalized Elliptical Slice Sampling with Regional Pseudo-priors", "comments": "29 pages, 12 figures, 2 tables. Under review at Communications in\n  Statistics - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a MCMC algorithm based on elliptical slice sampling\nwith the purpose to improve sampling efficiency. During sampling, a mixture\ndistribution is fitted periodically to previous samples. The components of the\nmixture distribution are called regional pseudo-priors because each component\nserves as the pseudo-prior for a subregion of the sampling space. Expectation\nmaximization algorithm, variational inference algorithm and stochastic\napproximation algorithm are used to estimate the parameters. Meanwhile,\nparallel computing is used to relieve the burden of computation. Ergodicity of\nthe proposed algorithm is proven mathematically. Experimental results on one\nsynthetic and two real-world dataset show that the proposed algorithm has the\nfollowing advantages: with the same starting points, the proposed algorithm can\nfind more distant modes; the proposed algorithm has lower rejection rates; when\ndoing Bayesian inference for uni-modal posterior distributions, the proposed\nalgorithm can give more accurate estimations; when doing Bayesian inference for\nmulti-modal posterior distributions, the proposed algorithm can find different\nmodes well, and the estimated means of the mixture distribution can provide\nadditional information for the location of modes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 04:40:08 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Li", "Song", ""], ["Tso", "Geoffrey K. F.", ""]]}, {"id": "1903.05367", "submitter": "Konstantin Posch", "authors": "Konstantin Posch, Maximilian Arbeiter, J\\\"urgen Pilz", "title": "A novel Bayesian approach for variable selection in linear regression\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Bayesian approach to the problem of variable selection in\nmultiple linear regression models. In particular, we present a hierarchical\nsetting which allows for direct specification of a-priori beliefs about the\nnumber of nonzero regression coefficients as well as a specification of beliefs\nthat given coefficients are nonzero. To guarantee numerical stability, we adopt\na $g$-prior with an additional ridge parameter for the unknown regression\ncoefficients. In order to simulate from the joint posterior distribution an\nintelligent random walk Metropolis-Hastings algorithm which is able to switch\nbetween different models is proposed. Testing our algorithm on real and\nsimulated data illustrates that it performs at least on par and often even\nbetter than other well-established methods. Finally, we prove that under some\nnominal assumptions, the presented approach is consistent in terms of model\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 09:01:43 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Posch", "Konstantin", ""], ["Arbeiter", "Maximilian", ""], ["Pilz", "J\u00fcrgen", ""]]}, {"id": "1903.05424", "submitter": "Ceren Vardar Acar Assistant Professor", "authors": "Buket Coskun, Ceren Vardar-Acar, Hakan Demirtas", "title": "A Generalized Correlated Random Walk Converging to Fractional Brownian\n  Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm to generate a fractional Brownian motion, with a\ngiven Hurst parameter, 1/2<H<1 using the correlated Bernoulli random variables\nwith parameter p; having a certain density. This density is constructed using\nthe link between the correlation of multivariate Gaussian random variables and\nthe correlation of their dichotomized binary variables and the relation between\nthe correlation coefficient and the persistence parameter. We prove that the\nnormalized sum of trajectories of this proposed random walk yields a Gaussian\nprocess whose scaling limit is the desired fractional Brownian motion with the\ngiven Hurst parameter, 1/2<H<1\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 11:41:56 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 16:18:32 GMT"}, {"version": "v3", "created": "Tue, 14 May 2019 08:46:28 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Coskun", "Buket", ""], ["Vardar-Acar", "Ceren", ""], ["Demirtas", "Hakan", ""]]}, {"id": "1903.05480", "submitter": "Adam Foster", "authors": "Adam Foster, Martin Jankowiak, Eli Bingham, Paul Horsfall, Yee Whye\n  Teh, Tom Rainforth, Noah Goodman", "title": "Variational Bayesian Optimal Experimental Design", "comments": "Published as a conference paper at the Thirty-third Conference on\n  Neural Information Processing Systems, Vancouver 2019.\n  https://papers.nips.cc/paper/9553-variational-bayesian-optimal-experimental-design.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal experimental design (BOED) is a principled framework for\nmaking efficient use of limited experimental resources. Unfortunately, its\napplicability is hampered by the difficulty of obtaining accurate estimates of\nthe expected information gain (EIG) of an experiment. To address this, we\nintroduce several classes of fast EIG estimators by building on ideas from\namortized variational inference. We show theoretically and empirically that\nthese estimators can provide significant gains in speed and accuracy over\nprevious approaches. We further demonstrate the practicality of our approach on\na number of end-to-end experiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 13:34:13 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 14:39:15 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 14:49:02 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Foster", "Adam", ""], ["Jankowiak", "Martin", ""], ["Bingham", "Eli", ""], ["Horsfall", "Paul", ""], ["Teh", "Yee Whye", ""], ["Rainforth", "Tom", ""], ["Goodman", "Noah", ""]]}, {"id": "1903.05715", "submitter": "Heather Battey Dr", "authors": "Henrique Helfer Hoeltgebaum and Heather Battey", "title": "HCmodelSets: An R package for specifying sets of well-fitting models in\n  regression with a large number of potential explanatory variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of regression with a large number of explanatory variables,\nCox and Battey (2017) emphasize that if there are alternative reasonable\nexplanations of the data that are statistically indistinguishable, one should\naim to specify as many of these explanations as is feasible. The standard\npractice, by contrast, is to report a single model effective for prediction.\nThe present paper illustrates the R implementation of the new ideas in the\npackage `HCmodelSets', using simple reproducible examples and real data.\nResults of some simulation experiments are also reported.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:15:42 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Hoeltgebaum", "Henrique Helfer", ""], ["Battey", "Heather", ""]]}, {"id": "1903.05726", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "A Multi-armed Bandit MCMC, with applications in sampling from doubly\n  intractable posterior", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms are widely used to sample from\ncomplicated distributions, especially to sample from the posterior distribution\nin Bayesian inference. However, MCMC is not directly applicable when facing the\ndoubly intractable problem. In this paper, we discussed and compared two\nexisting solutions -- Pseudo-marginal Monte Carlo and Exchange Algorithm. This\npaper also proposes a novel algorithm: Multi-armed Bandit MCMC (MABMC), which\nchooses between two (or more) randomized acceptance ratios in each step. MABMC\ncould be applied directly to incorporate Pseudo-marginal Monte Carlo and\nExchange algorithm, with higher average acceptance probability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:38:48 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:39:01 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "1903.06092", "submitter": "Richard Samworth", "authors": "Min Xu and Richard J. Samworth", "title": "High-dimensional nonparametric density estimation via symmetry and shape\n  constraints", "comments": "93 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of high-dimensional nonparametric density estimation by\ntaking the class of log-concave densities on $\\mathbb{R}^p$ and incorporating\nwithin it symmetry assumptions, which facilitate scalable estimation algorithms\nand can mitigate the curse of dimensionality. Our main symmetry assumption is\nthat the super-level sets of the density are $K$-homothetic (i.e. scalar\nmultiples of a convex body $K \\subseteq \\mathbb{R}^p$). When $K$ is known, we\nprove that the $K$-homothetic log-concave maximum likelihood estimator based on\n$n$ independent observations from such a density has a worst-case risk bound\nwith respect to, e.g., squared Hellinger loss, of $O(n^{-4/5})$, independent of\n$p$. Moreover, we show that the estimator is adaptive in the sense that if the\ndata generating density admits a special form, then a nearly parametric rate\nmay be attained. We also provide worst-case and adaptive risk bounds in cases\nwhere $K$ is only known up to a positive definite transformation, and where it\nis completely unknown and must be estimated nonparametrically. Our estimation\nalgorithms are fast even when $n$ and $p$ are on the order of hundreds of\nthousands, and we illustrate the strong finite-sample performance of our\nmethods on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 15:54:28 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Xu", "Min", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1903.06490", "submitter": "Achim Zeileis", "authors": "Achim Zeileis, Jason C. Fisher, Kurt Hornik, Ross Ihaka, Claire D.\n  McWhite, Paul Murrell, Reto Stauffer, Claus O. Wilke", "title": "colorspace: A Toolbox for Manipulating and Assessing Colors and Palettes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The R package colorspace provides a flexible toolbox for selecting individual\ncolors or color palettes, manipulating these colors, and employing them in\nstatistical graphics and data visualizations. In particular, the package\nprovides a broad range of color palettes based on the HCL\n(Hue-Chroma-Luminance) color space. The three HCL dimensions have been shown to\nmatch those of the human visual system very well, thus facilitating intuitive\nselection of color palettes through trajectories in this space. Using the HCL\ncolor model general strategies for three types of palettes are implemented: (1)\nQualitative for coding categorical information, i.e., where no particular\nordering of categories is available. (2) Sequential for coding ordered/numeric\ninformation, i.e., going from high to low (or vice versa). (3) Diverging for\ncoding ordered/numeric information around a central neutral value, i.e., where\ncolors diverge from neutral to two extremes. To aid selection and application\nof these palettes the package also contains scales for use with ggplot2, shiny\n(and tcltk) apps for interactive exploration, visualizations of palette\nproperties, accompanying manipulation utilities (like desaturation and\nlighten/darken), and emulation of color vision deficiencies.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 09:17:58 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Zeileis", "Achim", ""], ["Fisher", "Jason C.", ""], ["Hornik", "Kurt", ""], ["Ihaka", "Ross", ""], ["McWhite", "Claire D.", ""], ["Murrell", "Paul", ""], ["Stauffer", "Reto", ""], ["Wilke", "Claus O.", ""]]}, {"id": "1903.06568", "submitter": "Lukas Koch", "authors": "Lukas Koch", "title": "A response-matrix-centred approach to presenting cross-section\n  measurements", "comments": "26 pages, added reference to Phystat-nu", "journal-ref": null, "doi": "10.1088/1748-0221/14/09/P09013", "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current canonical approach to publishing cross-section data is to unfold\nthe reconstructed distributions. Detector effects like efficiency and smearing\nare undone mathematically, yielding distributions in true event properties.\nThis is an ill-posed problem, as even small statistical variations in the\nreconstructed data can lead to large changes in the unfolded spectra.\n  This work presents an alternative or complementary approach: the\nresponse-matrix-centred forward-folding approach. It offers a convenient way to\nforward-fold model expectations in truth space to reconstructed quantities.\nThese can then be compared to the data directly, similar to what is usually\ndone with full detector simulations within the experimental collaborations. For\nthis, the detector response (efficiency and smearing) is parametrised as a\nmatrix. The effects of the detector on the measurement of a given model is\nsimulated by simply multiplying the binned truth expectation values by this\nresponse matrix.\n  Systematic uncertainties in the detector response are handled by providing a\nset of matrices according to the prior distribution of the detector properties\nand marginalising over them. Background events can be included in the\nlikelihood calculation by giving background events their own bins in truth\nspace.\n  To facilitate a straight-forward use of response matrices, a new software\nframework has been developed: the Response Matrix Utilities (ReMU). ReMU is a\nPython package distributed via the Python Package Index. It only uses widely\navailable, standard scientific Python libraries and does not depend on any\ncustom experiment-specific software. It offers all methods needed to build\nresponse matrices from Monte Carlo data sets, use the response matrix to\nforward-fold truth-level model predictions, and compare the predictions to real\ndata using Bayesian or frequentist statistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 14:19:32 GMT"}, {"version": "v2", "created": "Thu, 21 Mar 2019 13:05:54 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Koch", "Lukas", ""]]}, {"id": "1903.06616", "submitter": "Matt Wand", "authors": "Tui H. Nolan, Marianne Menictas and Matt P. Wand", "title": "Streamlined Computing for Variational Inference with Higher Level Random\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and present explicit algorithms to facilitate streamlined computing\nfor variational inference for models containing higher level random effects.\nExisting literature, such as Lee and Wand (2016), is such that streamlined\nvariational inference is restricted to mean field variational Bayes algorithms\nfor two-level random effects models. Here we provide the following extensions:\n(1) explicit Gaussian response mean field variational Bayes algorithms for\nthree-level models, (2) explicit algorithms for the alternative variational\nmessage passing approach in the case of two-level and three-level models, and\n(3) an explanation of how arbitrarily high levels of nesting can be handled\nbased on the recently published matrix algebraic results of the authors. A\npay-off from (2) is simple extension to non-Gaussian response models. In\nsummary, we remove barriers for streamlining variational inference algorithms\nbased on either the mean field variational Bayes approach or the variational\nmessage passing approach when higher level random effects are present.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:56:40 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2020 22:43:22 GMT"}, {"version": "v3", "created": "Fri, 13 Mar 2020 03:16:26 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 04:50:10 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Nolan", "Tui H.", ""], ["Menictas", "Marianne", ""], ["Wand", "Matt P.", ""]]}, {"id": "1903.06964", "submitter": "Rui Jin", "authors": "Rui Jin, Aixin Tan", "title": "Fast Markov chain Monte Carlo for high dimensional Bayesian regression\n  models with shrinkage priors", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, many Bayesian shrinkage models have been developed for\nlinear regression problems where the number of covariates, $p$, is large.\nComputing the intractable posterior are often done with three-block Gibbs\nsamplers (3BG), based on representing the shrinkage priors as scale mixtures of\nNormal distributions. An alternative computing tool is a state of the art\nHamiltonian Monte Carlo (HMC) method, which can be easily implemented in the\nStan software. However, we found both existing methods to be inefficient and\noften impractical for large $p$ problems. Following the general idea of\nRajaratnam et al. (2018), we propose two-block Gibbs samplers (2BG) for three\ncommonly used shrinkage models, namely, the Bayesian group lasso, the Bayesian\nsparse group lasso and the Bayesian fused lasso models. We demonstrate with\nsimulated and real data examples that the Markov chains underlying 2BG's\nconverge much faster than that of 3BG's, and no worse than that of HMC. At the\nsame time, the computing costs of 2BG's per iteration are as low as that of\n3BG's, and can be several orders of magnitude lower than that of HMC. As a\nresult, the newly proposed 2BG is the only practical computing solution to do\nBayesian shrinkage analysis for datasets with large $p$. Further, we provide\ntheoretical justifications for the superior performance of 2BG's. We establish\ngeometric ergodicity (GE) of Markov chains associated with the 2BG for each of\nthe three Bayesian shrinkage models. We also prove, for most cases of the\nBayesian group lasso and the Bayesian sparse group lasso model, the Markov\noperators for the 2BG chains are trace-class. Whereas for all cases of all\nthree Bayesian shrinkage models, the Markov operator for the 3BG chains are not\neven Hilbert-Schmidt.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 17:49:30 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 21:26:40 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Jin", "Rui", ""], ["Tan", "Aixin", ""]]}, {"id": "1903.07373", "submitter": "Denis Belomestny", "authors": "D. Belomestny and E. Moulines and S. Samsonov", "title": "Variance reduction for MCMC methods via martingale representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an efficient variance reduction approach for\nadditive functionals of Markov chains relying on a novel discrete time\nmartingale representation. Our approach is fully non-asymptotic and does not\nrequire the knowledge of the stationary distribution (and even any type of\nergodicity) or specific structure of the underlying density. By rigorously\nanalyzing the convergence properties of the proposed algorithm, we show that\nits cost-to-variance product is indeed smaller than one of the naive algorithm.\nThe numerical performance of the new method is illustrated for the\nLangevin-type Markov Chain Monte Carlo (MCMC) methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 11:33:07 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2019 10:29:10 GMT"}, {"version": "v3", "created": "Mon, 11 May 2020 17:57:15 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Belomestny", "D.", ""], ["Moulines", "E.", ""], ["Samsonov", "S.", ""]]}, {"id": "1903.07594", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik", "title": "Combining Model and Parameter Uncertainty in Bayesian Neural Networks", "comments": "16 pages, 8 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian neural networks (BNNs) have recently regained a significant amount\nof attention in the deep learning community due to the development of scalable\napproximate Bayesian inference techniques. There are several advantages of\nusing Bayesian approach: Parameter and prediction uncertainty become easily\navailable, facilitating rigid statistical analysis. Furthermore, prior\nknowledge can be incorporated. However so far there have been no scalable\ntechniques capable of combining both model (structural) and parameter\nuncertainty. In this paper we introduce the concept of model uncertainty in\nBNNs and hence make inference in the joint space of models and parameters.\nMoreover, we suggest an adaptation of a scalable variational inference approach\nwith reparametrization of marginal inclusion probabilities to incorporate the\nmodel space constraints. Finally, we show that incorporating model uncertainty\nvia Bayesian model averaging and Bayesian model selection allows to drastically\nsparsify the structure of BNNs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 17:41:33 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 17:49:40 GMT"}, {"version": "v3", "created": "Sat, 25 May 2019 14:07:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""]]}, {"id": "1903.08008", "submitter": "Aki Vehtari", "authors": "Aki Vehtari, Andrew Gelman, Daniel Simpson, Bob Carpenter,\n  Paul-Christian B\\\"urkner", "title": "Rank-normalization, folding, and localization: An improved $\\widehat{R}$\n  for assessing convergence of MCMC", "comments": "Two small fixes. Published in Bayesian analysis\n  https://doi.org/10.1214/20-BA1221", "journal-ref": null, "doi": "10.1214/20-BA1221", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo is a key computational tool in Bayesian statistics,\nbut it can be challenging to monitor the convergence of an iterative stochastic\nalgorithm. In this paper we show that the convergence diagnostic $\\widehat{R}$\nof Gelman and Rubin (1992) has serious flaws. Traditional $\\widehat{R}$ will\nfail to correctly diagnose convergence failures when the chain has a heavy tail\nor when the variance varies across the chains. In this paper we propose an\nalternative rank-based diagnostic that fixes these problems. We also introduce\na collection of quantile-based local efficiency measures, along with a\npractical approach for computing Monte Carlo error estimates for quantiles. We\nsuggest that common trace plots should be replaced with rank plots from\nmultiple chains. Finally, we give recommendations for how these methods should\nbe used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:12:17 GMT"}, {"version": "v2", "created": "Thu, 16 Jan 2020 18:39:02 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 14:16:29 GMT"}, {"version": "v4", "created": "Thu, 17 Jun 2021 07:38:26 GMT"}, {"version": "v5", "created": "Tue, 22 Jun 2021 07:58:26 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Vehtari", "Aki", ""], ["Gelman", "Andrew", ""], ["Simpson", "Daniel", ""], ["Carpenter", "Bob", ""], ["B\u00fcrkner", "Paul-Christian", ""]]}, {"id": "1903.08412", "submitter": "Sumanta Das Dr", "authors": "Sumanta Kumar Das", "title": "Modeling Intelligent Decision Making Command And Control Agents: An\n  Application to Air Defense", "comments": null, "journal-ref": "IEEE Intelligent system, sept/oct 2014", "doi": "10.1109/MIS.2013.71", "report-no": null, "categories": "cs.AI stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper is a half-way between the agent technology and the mathematical\nreasoning to model tactical decision making tasks. These models are applied to\nair defense (AD) domain for command and control (C2). It also addresses the\nissues related to evaluation of agents. The agents are designed and implemented\nusing the agent-programming paradigm. The agents are deployed in an air combat\nsimulated environment for performing the tasks of C2 like electronic counter\ncounter measures, threat assessment, and weapon allocation. The simulated AD\nsystem runs without any human intervention, and represents state-of-the-art\nmodel for C2 autonomy. The use of agents as autonomous decision making entities\nis particularly useful in view of futuristic network centric warfare.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 09:52:17 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Das", "Sumanta Kumar", ""]]}, {"id": "1903.08829", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Marina Paez, Lizhen Lin and Zahra S. Razaee", "title": "Exact slice sampler for Hierarchical Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an exact slice sampler for Hierarchical Dirichlet process (HDP)\nand its associated mixture models (Teh et al., 2006). Although there are\nexisting MCMC algorithms for sampling from the HDP, a slice sampler has been\nmissing from the literature. Slice sampling is well-known for its desirable\nproperties including its fast mixing and its natural potential for\nparallelization. On the other hand, the hierarchical nature of HDPs poses\nchallenges to adopting a full-fledged slice sampler that automatically\ntruncates all the infinite measures involved without ad-hoc modifications. In\nthis work, we adopt the powerful idea of Bayesian variable augmentation to\naddress this challenge. By introducing new latent variables, we obtain a full\nfactorization of the joint distribution that is suitable for slice sampling.\nOur algorithm has several appealing features such as (1) fast mixing; (2)\nremaining exact while allowing natural truncation of the underlying\ninfinite-dimensional measures, as in (Kalli et al., 2011), resulting in updates\nof only a finite number of necessary atoms and weights in each iteration; and\n(3) being naturally suited to parallel implementations. The underlying\nprinciple for joint factorization of the full likelihood is simple and can be\napplied to many other settings, such as designing sampling algorithms for\ngeneral dependent Dirichlet process (DDP) models.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 04:51:22 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Amini", "Arash A.", ""], ["Paez", "Marina", ""], ["Lin", "Lizhen", ""], ["Razaee", "Zahra S.", ""]]}, {"id": "1903.08977", "submitter": "Martin Keller-Ressel", "authors": "Martin Keller-Ressel, Stephanie Nargang", "title": "Hydra: A method for strain-minimizing hyperbolic embedding of network-\n  and distance-based data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce hydra (hyperbolic distance recovery and approximation), a new\nmethod for embedding network- or distance-based data into hyperbolic space. We\nshow mathematically that hydra satisfies a certain optimality guarantee: It\nminimizes the `hyperbolic strain' between original and embedded data points.\nMoreover, it recovers points exactly, when they are located on a hyperbolic\nsubmanifold of the feature space. Testing on real network data we show that the\nembedding quality of hydra is competitive with existing hyperbolic embedding\nmethods, but achieved at substantially shorter computation time. An extended\nmethod, termed hydra+, outperforms existing methods in both computation time\nand embedding quality.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 13:16:01 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 13:20:41 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Keller-Ressel", "Martin", ""], ["Nargang", "Stephanie", ""]]}, {"id": "1903.09113", "submitter": "Behnam Malmir", "authors": "Behnam Malmir", "title": "Exploratory studies of human gait changes using depth cameras and\n  considering measurement errors", "comments": "73 pages, 26 figures, a thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research aims to quantify human walking patterns through depth cameras\nto (1) detect walking pattern changes of a person with and without a\nmotion-restricting device or a walking aid, and to (2) identify distinct\nwalking patterns from different persons of similar physical attributes.\nMicrosoft Kinect devices, often used for video games, were used to provide and\ntrack coordinates of 25 different joints of people over time to form a human\nskeleton. Then multiple machine learning (ML) models were applied to the SE\ndatasets from ten college-age subjects - five males and five females. In\nparticular, ML models were applied to classify subjects into two categories:\nnormal walking and abnormal walking (i.e. with motion-restricting devices). The\nbest ML model (K-nearest neighborhood) was able to predict 97.3% accuracy using\n10-fold cross-validation. Finally, ML models were applied to classify five gait\nconditions: walking normally, walking while wearing the ankle brace, walking\nwhile wearing the ACL brace, walking while using a cane, and walking while\nusing a walker. The best ML model was again the K-nearest neighborhood\nperforming at 98.7% accuracy rate.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:06:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Malmir", "Behnam", ""]]}, {"id": "1903.09321", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Yue Sheng", "title": "WONDER: Weighted one-shot distributed ridge regression in high\n  dimensions", "comments": "Gave the name \"Wonder\" to the algorithm, updated title, added\n  algorithm for general non-isotropic design", "journal-ref": null, "doi": null, "report-no": "Journal of Machine Learning Research 21(66) p. 1-52 2020. Short\n  version at ICML 2020", "categories": "math.ST cs.DC cs.LG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas, practitioners need to analyze large datasets that challenge\nconventional single-machine computing. To scale up data analysis, distributed\nand parallel computing approaches are increasingly needed.\n  Here we study a fundamental and highly important problem in this area: How to\ndo ridge regression in a distributed computing environment? Ridge regression is\nan extremely popular method for supervised learning, and has several optimality\nproperties, thus it is important to study. We study one-shot methods that\nconstruct weighted combinations of ridge regression estimators computed on each\nmachine. By analyzing the mean squared error in a high dimensional\nrandom-effects model where each predictor has a small effect, we discover\nseveral new phenomena.\n  1. Infinite-worker limit: The distributed estimator works well for very large\nnumbers of machines, a phenomenon we call \"infinite-worker limit\".\n  2. Optimal weights: The optimal weights for combining local estimators sum to\nmore than unity, due to the downward bias of ridge. Thus, all averaging methods\nare suboptimal.\n  We also propose a new Weighted ONe-shot DistributEd Ridge regression (WONDER)\nalgorithm. We test WONDER in simulation studies and using the Million Song\nDataset as an example. There it can save at least 100x in computation time,\nwhile nearly preserving test accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 02:26:29 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:25:58 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Dobriban", "Edgar", ""], ["Sheng", "Yue", ""]]}, {"id": "1903.09367", "submitter": "Peng Zhao", "authors": "Peng Zhao, Yun Yang and Qiao-Chu He", "title": "Implicit Regularization via Hadamard Product Over-Parametrization in\n  High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Hadamard product parametrization as a change-of-variable\n(over-parametrization) technique for solving least square problems in the\ncontext of linear regression. Despite the non-convexity and exponentially many\nsaddle points induced by the change-of-variable, we show that under certain\nconditions, this over-parametrization leads to implicit regularization: if we\ndirectly apply gradient descent to the residual sum of squares with\nsufficiently small initial values, then under proper early stopping rule, the\niterates converge to a nearly sparse rate-optimal solution with relatively\nbetter accuracy than explicit regularized approaches. In particular, the\nresulting estimator does not suffer from extra bias due to explicit penalties,\nand can achieve the parametric root-$n$ rate (independent of the dimension)\nunder proper conditions on the signal-to-noise ratio. We perform simulations to\ncompare our methods with high dimensional linear regression with explicit\nregularizations. Our results illustrate advantages of using implicit\nregularization via gradient descent after over-parametrization in sparse vector\nestimation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:56:04 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Yun", ""], ["He", "Qiao-Chu", ""]]}, {"id": "1903.09556", "submitter": "Filippo Pagani Mr", "authors": "Filippo Pagani, Martin Wiegand, Saralees Nadarajah", "title": "An n-dimensional Rosenbrock Distribution for MCMC Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Rosenbrock function is an ubiquitous benchmark problem for numerical\noptimisation, and variants have been proposed to test the performance of Markov\nChain Monte Carlo algorithms. In this work we discuss the two-dimensional\nRosenbrock density, its current $n$-dimensional extensions, and their\nadvantages and limitations. We then propose a new extension to arbitrary\ndimensions called the Hybrid Rosenbrock distribution, which is composed of\nconditional normal kernels arranged in such a way that preserves the key\nfeatures of the original kernel. Moreover, due to its structure, the Hybrid\nRosenbrock distribution is analytically tractable and possesses several\ndesirable properties, which make it an excellent test model for computational\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 15:29:02 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 14:28:37 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2020 13:07:54 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 16:09:15 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Pagani", "Filippo", ""], ["Wiegand", "Martin", ""], ["Nadarajah", "Saralees", ""]]}, {"id": "1903.09919", "submitter": "Ted Dunning", "authors": "Ted Dunning", "title": "Conservation of the $t$-digest Scale Invariant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A $t$-digest is a compact data structure that allows estimates of quantiles\nwhich increased accuracy near $q = 0$ or $q=1$. This is done by clustering\nsamples from $\\mathbb R$ subject to a constraint that the number of points\nassociated with any particular centroid is constrained so that the so-called\n$k$-size of the centroid is always $\\le 1$. The $k$-size is defined using a\nscale function that maps quantile $q$ to index $k$. Since the centroids are\nreal numbers, they can be ordered and thus the quantile range of a centroid can\nbe mapped into an interval in $k$ whose size is the $k$-size of that centroid.\nThe accuracy of quantile estimates made using a $t$-digest depends on the\ninvariance of this constraint even as new data is added or $t$-digests are\nmerged. This paper provides proofs of this invariance for four practically\nimportant scale functions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 04:18:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dunning", "Ted", ""]]}, {"id": "1903.09921", "submitter": "Ted Dunning", "authors": "Ted Dunning", "title": "The Size of a $t$-Digest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A $t$-digest is a compact data structure that allows estimates of quantiles\nwhich increased accuracy near $q = 0$ or $q=1$. This is done by clustering\nsamples from $\\mathbb R$ subject to a constraint that the number of points\nassociated with any particular centroid is constrained so that the so-called\n$k$-size of the centroid is always $\\le 1$. The $k$-size is defined using a\nscale function that maps quantile $q$ to index $k$. This paper provides bounds\non the sizes of $t$-digests created using any of four known scale functions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 04:25:26 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Dunning", "Ted", ""]]}, {"id": "1903.10184", "submitter": "Murray Pollock", "authors": "Paul A. Jenkins, Murray Pollock, Gareth O. Roberts and Michael\n  S{\\o}rensen", "title": "Simulating bridges using confluent diffusions", "comments": "Significant revision of prior submission, with an improved\n  methodology which is far broader in its applicability. Updated author\n  listing. 19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusions are a fundamental class of models in many fields, including\nfinance, engineering, and biology. Simulating diffusions is challenging as\ntheir sample paths are infinite-dimensional and their transition functions are\ntypically intractable. In statistical settings such as parameter inference for\ndiscretely observed diffusions, we require simulation techniques for diffusions\nconditioned on hitting a given endpoint, which introduces further complication.\nIn this paper we introduce a Markov chain Monte Carlo algorithm for simulating\nbridges of ergodic diffusions which (i) is exact in the sense that there is no\ndiscretisation error, (ii) has computational cost that is linear in the\nduration of the bridges, and (iii) provides bounds on local maxima and minima\nof the simulated trajectory. Our approach works directly on diffusion path\nspace, by constructing a proposal (which we term a confluence) that is then\ncorrected with an accept/reject step in a pseudo-marginal algorithm. Our method\nrequires only the simulation of unconditioned diffusion sample paths. We apply\nour approach to the simulation of Langevin diffusion bridges, a practical\nproblem arising naturally in many situations, such as statistical inference in\ndistributed settings.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 08:57:13 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 15:21:04 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1903.10221", "submitter": "Christopher Pooley Dr", "authors": "C. M. Pooley, S. C. Bishop, A. Doeschl-Wilson and G. Marion", "title": "Posterior-based proposals for speeding up Markov chain Monte Carlo", "comments": "54 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is widely used for Bayesian inference in\nmodels of complex systems. Performance, however, is often unsatisfactory in\nmodels with many latent variables due to so-called poor mixing, necessitating\ndevelopment of application specific implementations. This paper introduces\n\"posterior-based proposals\" (PBPs), a new type of MCMC update applicable to a\nhuge class of statistical models (whose conditional dependence structures are\nrepresented by directed acyclic graphs). PBPs generates large joint updates in\nparameter and latent variable space, whilst retaining good acceptance rates\n(typically 33%). Evaluation against other approaches (from standard Gibbs /\nrandom walk updates to state-of-the-art Hamiltonian and particle MCMC methods)\nwas carried out for widely varying model types: an individual-based model for\ndisease diagnostic test data, a financial stochastic volatility model, a mixed\nmodel used in statistical genetics and a population model used in ecology.\nWhilst different methods worked better or worse in different scenarios, PBPs\nwere found to be either near to the fastest or significantly faster than the\nnext best approach (by up to a factor of 10). PBPs therefore represent an\nadditional general purpose technique that can be usefully applied in a wide\nvariety of contexts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:16:08 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:07:20 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pooley", "C. M.", ""], ["Bishop", "S. C.", ""], ["Doeschl-Wilson", "A.", ""], ["Marion", "G.", ""]]}, {"id": "1903.10426", "submitter": "Cinzia Franceschini", "authors": "Cinzia Franceschini (1) and Nicola Loperfido (2) ((1) Universit\\`a\n  degli Studi della Tuscia, Dipartimento di Scienze Agrarie e Forestali\n  (DAFNE), Viterbo (Italy),(2) Universit\\`a degli Studi di Urbino \"Carlo Bo\",\n  Dipartimento di Economia, Societ\\`a e Politica (DESP), Urbino (PU), ITALY)", "title": "MaxSkew and MultiSkew: Two R Packages for Detecting, Measuring and\n  Removing Multivariate Skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skewness plays a relevant role in several multivariate statistical\ntechniques. Sometimes it is used to recover data features, as in cluster\nanalysis. In other circumstances, skewness impairs the performances of\nstatistical methods, as in the Hotelling's one-sample test. In both cases,\nthere is the need to check the symmetry of the underlying distribution, either\nby visual inspection or by formal testing. The R packages MaxSkew and MultiSkew\naddress these issues by measuring, testing and removing skewness from\nmultivariate data. Skewness is assessed by the third multivariate cumulant and\nits functions. The hypothesis of symmetry is tested either nonparametrically,\nwith the bootstrap, or parametrically, under the normality assumption. Skewness\nis removed or at least alleviated by projecting the data onto appropriate\nlinear subspaces. Usages of MaxSkew and MultiSkew are illustrated with the Iris\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:11:19 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Franceschini", "Cinzia", ""], ["Loperfido", "Nicola", ""]]}, {"id": "1903.10659", "submitter": "Vinayak Rao", "authors": "Qi Wang, Vinayak Rao, Yee Whye Teh", "title": "An Exact Auxiliary Variable Gibbs Sampler for a Class of Diffusions", "comments": "37 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) or diffusions are continuous-valued\ncontinuous-time stochastic processes widely used in the applied and\nmathematical sciences. Simulating paths from these processes is usually an\nintractable problem, and typically involves time-discretization approximations.\nWe propose an exact Markov chain Monte Carlo sampling algorithm that involves\nno such time-discretization error. Our sampler is applicable to the problem of\nprior simulation from an SDE, posterior simulation conditioned on noisy\nobservations, as well as parameter inference given noisy observations. Our work\nrecasts an existing rejection sampling algorithm for a class of diffusions as a\nlatent variable model, and then derives an auxiliary variable Gibbs sampling\nalgorithm that targets the associated joint distribution. At a high level, the\nresulting algorithm involves two steps: simulating a random grid of times from\nan inhomogeneous Poisson process, and updating the SDE trajectory conditioned\non this grid. Our work allows the vast literature of Monte Carlo sampling\nalgorithms from the Gaussian process literature to be brought to bear to\napplications involving diffusions. We study our method on synthetic and real\ndatasets, where we demonstrate superior performance over competing methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 02:52:39 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 19:23:29 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Qi", ""], ["Rao", "Vinayak", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1903.11187", "submitter": "Chi Feng", "authors": "Chi Feng and Youssef M. Marzouk", "title": "A layered multiple importance sampling scheme for focused optimal\n  Bayesian experimental design", "comments": "33 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new computational approach for \"focused\" optimal Bayesian\nexperimental design with nonlinear models, with the goal of maximizing expected\ninformation gain in targeted subsets of model parameters. Our approach\nconsiders uncertainty in the full set of model parameters, but employs a design\nobjective that can exploit learning trade-offs among different parameter\nsubsets. We introduce a new layered multiple importance sampling scheme that\nprovides consistent estimates of expected information gain in this focused\nsetting. This sampling scheme yields significant reductions in estimator bias\nand variance for a given computational effort, making optimal design more\ntractable for a wide range of computationally intensive problems.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 23:00:16 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Feng", "Chi", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1903.11200", "submitter": "Yangmei Zhou", "authors": "Yangmei Zhou, Weixin Yao", "title": "Maximum Likelihood Estimation of a Semiparametric Two-component Mixture\n  Model using Log-concave Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by studies in biological sciences to detect differentially\nexpressed genes, a semiparametric two-component mixture model with one known\ncomponent is being studied in this paper. Assuming the density of the unknown\ncomponent to be log-concave, which contains a very broad family of densities,\nwe develop a semiparametric maximum likelihood estimator and propose an EM\nalgorithm to compute it. Our new estimation method finds the mixing proportions\nand the distribution of the unknown component simultaneously. We establish the\nidentifiability of the proposed semiparametric mixture model and prove the\nexistence and consistency of the proposed estimators. We further compare our\nestimator with several existing estimators through simulation studies and apply\nour method to two real data sets from biological sciences and astronomy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:32:57 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhou", "Yangmei", ""], ["Yao", "Weixin", ""]]}, {"id": "1903.11372", "submitter": "Neo Christopher Chung", "authors": "Neo Christopher Chung, B{\\l}a\\.zej Miasojedow, Micha{\\l} Startek, Anna\n  Gambin", "title": "Jaccard/Tanimoto similarity test and estimation methods", "comments": null, "journal-ref": "BMC Bioinformatics (2019) 20(Suppl 15): 644", "doi": "10.1186/s12859-019-3118-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary data are used in a broad area of biological sciences. Using binary\npresence-absence data, we can evaluate species co-occurrences that help\nelucidate relationships among organisms and environments. To summarize\nsimilarity between occurrences of species, we routinely use the\nJaccard/Tanimoto coefficient, which is the ratio of their intersection to their\nunion. It is natural, then, to identify statistically significant\nJaccard/Tanimoto coefficients, which suggest non-random co-occurrences of\nspecies. However, statistical hypothesis testing using this similarity\ncoefficient has been seldom used or studied.\n  We introduce a hypothesis test for similarity for biological presence-absence\ndata, using the Jaccard/Tanimoto coefficient. Several key improvements are\npresented including unbiased estimation of expectation and centered\nJaccard/Tanimoto coefficients, that account for occurrence probabilities. We\nderived the exact and asymptotic solutions and developed the bootstrap and\nmeasurement concentration algorithms to compute statistical significance of\nbinary similarity. Comprehensive simulation studies demonstrate that our\nproposed methods produce accurate p-values and false discovery rates. The\nproposed estimation methods are orders of magnitude faster than the exact\nsolution. The proposed methods are implemented in an open source R package\ncalled jaccard (https://cran.r-project.org/package=jaccard).\n  We introduce a suite of statistical methods for the Jaccard/Tanimoto\nsimilarity coefficient, that enable straightforward incorporation of\nprobabilistic measures in analysis for species co-occurrences. Due to their\ngenerality, the proposed methods and implementations are applicable to a wide\nrange of binary data arising from genomics, biochemistry, and other areas of\nscience.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:22:31 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chung", "Neo Christopher", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Startek", "Micha\u0142", ""], ["Gambin", "Anna", ""]]}, {"id": "1903.11460", "submitter": "Chengjing Wang", "authors": "Peipei Tang, Chengjing Wang, Defeng Sun, and Kim-Chuan Toh", "title": "A sparse semismooth Newton based proximal majorization-minimization\n  algorithm for nonconvex square-root-loss regression problems", "comments": "34 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider high-dimensional nonconvex square-root-loss\nregression problems and introduce a proximal majorization-minimization (PMM)\nalgorithm for these problems. Our key idea for making the proposed PMM to be\nefficient is to develop a sparse semismooth Newton method to solve the\ncorresponding subproblems. By using the Kurdyka-{\\L}ojasiewicz property\nexhibited in the underlining problems, we prove that the PMM algorithm\nconverges to a d-stationary point. We also analyze the oracle property of the\ninitial subproblem used in our algorithm. Extensive numerical experiments are\npresented to demonstrate the high efficiency of the proposed PMM algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 14:51:35 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 12:03:39 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 05:59:27 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Tang", "Peipei", ""], ["Wang", "Chengjing", ""], ["Sun", "Defeng", ""], ["Toh", "Kim-Chuan", ""]]}, {"id": "1903.11576", "submitter": "Shiqian Ma", "authors": "Shixiang Chen, Shiqian Ma, Lingzhou Xue, Hui Zou", "title": "An Alternating Manifold Proximal Gradient Method for Sparse PCA and\n  Sparse CCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) and sparse canonical correlation\nanalysis (CCA) are two essential techniques from high-dimensional statistics\nand machine learning for analyzing large-scale data. Both problems can be\nformulated as an optimization problem with nonsmooth objective and nonconvex\nconstraints. Since non-smoothness and nonconvexity bring numerical\ndifficulties, most algorithms suggested in the literature either solve some\nrelaxations or are heuristic and lack convergence guarantees. In this paper, we\npropose a new alternating manifold proximal gradient method to solve these two\nhigh-dimensional problems and provide a unified convergence analysis. Numerical\nexperiment results are reported to demonstrate the advantages of our algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 17:44:00 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Chen", "Shixiang", ""], ["Ma", "Shiqian", ""], ["Xue", "Lingzhou", ""], ["Zou", "Hui", ""]]}, {"id": "1903.11697", "submitter": "Nicol\\'as Kuschinski", "authors": "Nicol\\'as E. Kuschinski, J. Andr\\'es Christen, Adriana Monroy,\n  Silvestre Alavez", "title": "Bayesian Experimental Design for Oral Glucose Tolerance Tests (OGTT)", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  OGTT is a common test, frequently used to diagnose insulin resistance or\ndiabetes, in which a patient's blood sugar is measured at various times over\nthe course of a few hours. Recent developments in the study of OGTT results\nhave framed it as an inverse problem which has been the subject of Bayesian\ninference. This is a powerful new tool for analyzing the results of an OGTT\ntest,and the question arises as to whether the test itself can be improved. It\nis of particular interest to discover whether the times at which a patient's\nglucose is measured can be changed to improve the effectiveness of the test.\nThe purpose of this paper is to explore the possibility of finding a better\nexperimental design, that is, a set of times to perform the test. We review the\ntheory of Bayesian experimental design and propose an estimator for the\nexpected utility of a design. We then study the properties of this estimator\nand propose a new method for quantifying the uncertainty in comparisons between\ndesigns. We implement this method to find a new design and the proposed design\nis compared favorably to the usual testing scheme.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:54:16 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Kuschinski", "Nicol\u00e1s E.", ""], ["Christen", "J. Andr\u00e9s", ""], ["Monroy", "Adriana", ""], ["Alavez", "Silvestre", ""]]}, {"id": "1903.11745", "submitter": "Yves Atchade F", "authors": "Yves F. Atchad\\'e", "title": "Approximate spectral gaps for Markov chains mixing times in high\n  dimensions", "comments": "27 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a concept of approximate spectral gap to analyze the\nmixing time of Markov Chain Monte Carlo (MCMC) algorithms for which the usual\nspectral gap is degenerate or almost degenerate. We use the idea to analyze a\nclass of MCMC algorithms to sample from mixtures of densities. As an\napplication we study the mixing time of a Gibbs sampler for variable selection\nin linear regression models. Under some regularity conditions on the signal and\nthe design matrix of the regression problem, we show that for well-chosen\ninitial distributions the mixing time of the Gibbs sampler is polynomial in the\ndimension of the space.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 00:58:45 GMT"}, {"version": "v2", "created": "Thu, 22 Aug 2019 18:01:16 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Atchad\u00e9", "Yves F.", ""]]}, {"id": "1903.11908", "submitter": "V\\'ictor Elvira", "authors": "Mateu Sbert and V\\'ictor Elvira", "title": "Generalizing the Balance Heuristic Estimator in Multiple Importance\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel and generic family of multiple importance\nsampling estimators. We first revisit the celebrated balance heuristic\nestimator, a widely used Monte Carlo technique for the approximation of\nintractable integrals. Then, we establish a generalized framework for the\ncombination of samples simulated from multiple proposals. We show that the\nnovel framework contains the balance heuristic as a particular case. In\naddition, we study the optimal choice of the free parameters in such a way the\nvariance of the resulting estimator is minimized. A theoretical variance study\nshows the optimal solution is always better than the balance heuristic\nestimator (except in degenerate cases where both are the same). As a side\nresult of this analysis, we also provide new upper bounds for the balance\nheuristic estimator. Finally, we show the gap in the variance of both\nestimators by means of five numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 11:59:29 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 09:18:48 GMT"}, {"version": "v3", "created": "Sat, 6 Apr 2019 13:41:11 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Sbert", "Mateu", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "1903.12044", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Joaqu\\'in M\\'iguez", "title": "Convergence rates for optimised adaptive importance samplers", "comments": "Revised version, new results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate\nexpectations with respect to some target distribution which \\textit{adapt}\nthemselves to obtain better estimators over a sequence of iterations. Although\nit is straightforward to show that they have the same $\\mathcal{O}(1/\\sqrt{N})$\nconvergence rate as standard importance samplers, where $N$ is the number of\nMonte Carlo samples, the behaviour of adaptive importance samplers over the\nnumber of iterations has been left relatively unexplored. In this work, we\ninvestigate an adaptation strategy based on convex optimisation which leads to\na class of adaptive importance samplers termed \\textit{optimised adaptive\nimportance samplers} (OAIS). These samplers rely on the iterative minimisation\nof the $\\chi^2$-divergence between an exponential-family proposal and the\ntarget. The analysed algorithms are closely related to the class of adaptive\nimportance samplers which minimise the variance of the weight function. We\nfirst prove non-asymptotic error bounds for the mean squared errors (MSEs) of\nthese algorithms, which explicitly depend on the number of iterations and the\nnumber of samples together. The non-asymptotic bounds derived in this paper\nimply that when the target belongs to the exponential family, the $L_2$ errors\nof the optimised samplers converge to the optimal rate of\n$\\mathcal{O}(1/\\sqrt{N})$ and the rate of convergence in the number of\niterations are explicitly provided. When the target does not belong to the\nexponential family, the rate of convergence is the same but the asymptotic\n$L_2$ error increases by a factor $\\sqrt{\\rho^\\star} > 1$, where $\\rho^\\star -\n1$ is the minimum $\\chi^2$-divergence between the target and an\nexponential-family proposal.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 15:21:53 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 09:17:45 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 19:56:08 GMT"}, {"version": "v4", "created": "Thu, 7 May 2020 01:08:45 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1903.12078", "submitter": "Ziyu Liu", "authors": "Ziyu Liu, Shihong Wei, James C. Spall", "title": "Error Analysis for the Particle Filter: Methods and Theoretical Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The particle filter is a popular Bayesian filtering algorithm for use in\ncases where the state-space model is nonlinear and/or the random terms (initial\nstate or noises) are non-Gaussian distributed. We study the behavior of the\nerror in the particle filter algorithm as the number of particles gets large.\nAfter a decomposition of the error into two terms, we show that the difference\nbetween the estimator and the conditional mean is asymptotically normal when\nthe resampling is done at every step in the filtering process. Two\nnonlinear/non-Gaussian examples are tested to verify this conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:45:22 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Liu", "Ziyu", ""], ["Wei", "Shihong", ""], ["Spall", "James C.", ""]]}, {"id": "1903.12322", "submitter": "Liam Hodgkinson", "authors": "Liam Hodgkinson, Robert Salomone, Fred Roosta", "title": "Implicit Langevin Algorithms for Sampling From Log-concave Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For sampling from a log-concave density, we study implicit integrators\nresulting from $\\theta$-method discretization of the overdamped Langevin\ndiffusion stochastic differential equation. Theoretical and algorithmic\nproperties of the resulting sampling methods for $ \\theta \\in [0,1] $ and a\nrange of step sizes are established. Our results generalize and extend prior\nworks in several directions. In particular, for $\\theta\\ge1/2$, we prove\ngeometric ergodicity and stability of the resulting methods for all step sizes.\nWe show that obtaining subsequent samples amounts to solving a strongly-convex\noptimization problem, which is readily achievable using one of numerous\nexisting methods. Numerical examples supporting our theoretical analysis are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 29 Mar 2019 02:13:16 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 07:31:46 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Hodgkinson", "Liam", ""], ["Salomone", "Robert", ""], ["Roosta", "Fred", ""]]}]