[{"id": "0909.0329", "submitter": "Bertrand Iooss", "authors": "Matthieu Petelet (CEA-DEN), Bertrand Iooss (M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques), Olivier Asserin\n  (CEA-DEN), Alexandre Loredo (EA1859)", "title": "Latin hypercube sampling with inequality constraints", "comments": null, "journal-ref": "AStA Advances in Statistical Analysis 3 (2010) 11-21", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some studies requiring predictive and CPU-time consuming numerical models,\nthe sampling design of the model input variables has to be chosen with caution.\nFor this purpose, Latin hypercube sampling has a long history and has shown its\nrobustness capabilities. In this paper we propose and discuss a new algorithm\nto build a Latin hypercube sample (LHS) taking into account inequality\nconstraints between the sampled variables. This technique, called constrained\nLatin hypercube sampling (cLHS), consists in doing permutations on an initial\nLHS to honor the desired monotonic constraints. The relevance of this approach\nis shown on a real example concerning the numerical welding simulation, where\nthe inequality constraints are caused by the physical decreasing of some\nmaterial properties in function of the temperature.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2009 06:27:14 GMT"}, {"version": "v2", "created": "Sun, 7 Feb 2010 14:18:20 GMT"}, {"version": "v3", "created": "Thu, 23 Sep 2010 12:00:03 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Petelet", "Matthieu", "", "CEA-DEN"], ["Iooss", "Bertrand", "", "M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques"], ["Asserin", "Olivier", "", "CEA-DEN"], ["Loredo", "Alexandre", "", "EA1859"]]}, {"id": "0909.0389", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "Monte Carlo Methods in Statistics", "comments": "Entry submitted to the International Handbook of Statistical Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are now an essential part of the statistician's toolbox,\nto the point of being more familiar to graduate students than the measure\ntheoretic notions upon which they are based! We recall in this note some of the\nadvances made in the design of Monte Carlo techniques towards their use in\nStatistics, referring to Robert and Casella (2004,2010) for an in-depth\ncoverage.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2009 11:25:53 GMT"}], "update_date": "2009-09-03", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "0909.0856", "submitter": "Chris Sherlock", "authors": "Chris Sherlock, Gareth Roberts", "title": "Optimal scaling of the random walk Metropolis on elliptically symmetric\n  unimodal targets", "comments": "Published in at http://dx.doi.org/10.3150/08-BEJ176 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2009, Vol. 15, No. 3, 774-798", "doi": "10.3150/08-BEJ176", "report-no": "IMS-BEJ-BEJ176", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling of proposals for Metropolis algorithms is an important practical\nproblem in MCMC implementation. Criteria for scaling based on empirical\nacceptance rates of algorithms have been found to work consistently well across\na broad range of problems. Essentially, proposal jump sizes are increased when\nacceptance rates are high and decreased when rates are low. In recent years,\nconsiderable theoretical support has been given for rules of this type which\nwork on the basis that acceptance rates around 0.234 should be preferred. This\nhas been based on asymptotic results that approximate high dimensional\nalgorithm trajectories by diffusions. In this paper, we develop a novel\napproach to understanding 0.234 which avoids the need for diffusion limits. We\nderive explicit formulae for algorithm efficiency and acceptance rates as\nfunctions of the scaling parameter. We apply these to the family of\nelliptically symmetric target densities, where further illuminating explicit\nresults are possible. Under suitable conditions, we verify the 0.234 rule for a\nnew class of target densities. Moreover, we can characterise cases where 0.234\nfails to hold, either because the target density is too diffuse in a sense we\nmake precise, or because the eccentricity of the target density is too severe,\nagain in a sense we make precise. We provide numerical verifications of our\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 4 Sep 2009 11:16:58 GMT"}], "update_date": "2009-09-07", "authors_parsed": [["Sherlock", "Chris", ""], ["Roberts", "Gareth", ""]]}, {"id": "0909.1234", "submitter": "Rodrigo Labouriau", "authors": "Gabriel C. G. de Abreu, Rodrigo Labouriau and David Edwards", "title": "High-dimensional Graphical Model Search with gRapHD R Package", "comments": "20 pages with 8 figures", "journal-ref": "Journal of Statistical Software, Vol. 37, Issue 1, Nov. 2010", "doi": "10.18637/jss.V037", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package gRapHD for efficient selection of\nhigh-dimensional undirected graphical models. The package provides tools for\nselecting trees, forests and decomposable models minimizing information\ncriteria such as AIC or BIC, and for displaying the independence graphs of the\nmodels. It has also some useful tools for analysing graphical structures. It\nsupports the use of discrete, continuous, or both types of variables\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2009 12:20:10 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2009 10:29:31 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2010 10:33:07 GMT"}, {"version": "v4", "created": "Wed, 22 Sep 2010 08:36:10 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["de Abreu", "Gabriel C. G.", ""], ["Labouriau", "Rodrigo", ""], ["Edwards", "David", ""]]}, {"id": "0909.4073", "submitter": "Jie Yang", "authors": "Liping Tong, Jie Yang, Richard S. Cooper", "title": "Efficient Calculation of P-value and Power for Quadratic Form Statistics\n  in Multilocus Association Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the asymptotic and approximate distributions of a large class of\ntest statistics with quadratic forms used in association studies. The\nstatistics of interest do not necessarily follow a chi-square distribution and\ntake the general form $D=X^T A X$, where $X$ follows the multivariate normal\ndistribution, and $A$ is a general similarity matrix which may or may not be\npositive semi-definite. We show that $D$ can be written as a linear combination\nof independent chi-square random variables, whose distribution can be\napproximated by a chi-square or the difference of two chi-square distributions.\nIn the setting of association testing, our methods are especially useful in two\nsituations. First, for a genome screen, the required significance level is much\nsmaller than 0.05 due to multiple comparisons, and estimation of p-values using\npermutation procedures is particularly challenging. An efficient and accurate\nestimation procedure would therefore be useful. Second, in a candidate gene\nstudy based on haplotypes when phase is unknown a computationally expensive\nmethod-the EM algorithm-is usually required to infer haplotype frequencies.\nBecause the EM algorithm is needed for each permutation, this results in a\nsubstantial computational burden, which can be eliminated with our mathematical\nsolution. We assess the practical utility of our method using extensive\nsimulation studies based on two example statistics and apply it to find the\nsample size needed for a typical candidate gene association study when phase\ninformation is not available. Our method can be applied to any quadratic form\nstatistic and therefore should be of general interest.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2009 20:56:19 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Tong", "Liping", ""], ["Yang", "Jie", ""], ["Cooper", "Richard S.", ""]]}, {"id": "0909.4129", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "Efficient Simulation of a Bivariate Exponential Conditionals\n  Distribution", "comments": "only 5 pages!", "journal-ref": "Computational Statistics & Data Analysis 52 (2008) 2273-2276", "doi": "10.1016/j.csda.2007.10.013", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bivariate distribution with exponential conditionals (BEC) is introduced\nby Arnold and Strauss [Bivariate distributions with exponential conditionals,\nJ. Amer. Statist. Assoc. 83 (1988) 522--527]. This work presents a simple and\nfast algorithm for simulating random variates from this density.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2009 03:47:31 GMT"}], "update_date": "2009-09-24", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "0909.5026", "submitter": "Taiji Suzuki", "authors": "Taiji Suzuki and Ryota Tomioka", "title": "SpicyMKL", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new optimization algorithm for Multiple Kernel Learning (MKL)\ncalled SpicyMKL, which is applicable to general convex loss functions and\ngeneral types of regularization. The proposed SpicyMKL iteratively solves\nsmooth minimization problems. Thus, there is no need of solving SVM, LP, or QP\ninternally. SpicyMKL can be viewed as a proximal minimization method and\nconverges super-linearly. The cost of inner minimization is roughly\nproportional to the number of active kernels. Therefore, when we aim for a\nsparse kernel combination, our algorithm scales well against increasing number\nof kernels. Moreover, we give a general block-norm formulation of MKL that\nincludes non-sparse regularizations, such as elastic-net and \\ellp -norm\nregularizations. Extending SpicyMKL, we propose an efficient optimization\nmethod for the general regularization framework. Experimental results show that\nour algorithm is faster than existing methods especially when the number of\nkernels is large (> 1000).\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2009 07:45:29 GMT"}, {"version": "v2", "created": "Mon, 9 May 2011 03:06:22 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Suzuki", "Taiji", ""], ["Tomioka", "Ryota", ""]]}, {"id": "0909.5262", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Nicholas G. Polson", "title": "Particle learning of Gaussian process models for sequential design and\n  optimization", "comments": "18 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simulation-based method for the online updating of Gaussian\nprocess regression and classification models. Our method exploits sequential\nMonte Carlo to produce a fast sequential design algorithm for these models\nrelative to the established MCMC alternative. The latter is less ideal for\nsequential design since it must be restarted and iterated to convergence with\nthe inclusion of each new design point. We illustrate some attractive ensemble\naspects of our SMC approach, and show how active learning heuristics may be\nimplemented via particles to optimize a noisy function or to explore\nclassification boundaries online.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 04:25:10 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2010 10:24:46 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2010 12:20:42 GMT"}], "update_date": "2010-07-07", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "0909.5369", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "On the relevance of the Bayesian approach to Statistics", "comments": "This paper is written in conjunction with the 3rd Bayesian\n  econometrics meeting that took place at the Rimini Centre for Economic\n  Analysis on July 01-02, 2009. A version will eventually be published in the\n  Review of Economic Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue here about the relevance and the ultimate unity of the Bayesian\napproach in a neutral and agnostic manner. Our main theme is that Bayesian data\nanalysis is an effective tool for handling complex models, as proven by the\nincreasing proportion of Bayesian studies in the applied sciences. We disregard\nin this essay the philosophical debates on the deeper meaning of probability\nand on the random nature of parameters as things of the past that do a\ndisservice to the approach and are incomprehensible to most bystanders.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2009 19:30:09 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2010 18:07:05 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2010 10:45:01 GMT"}], "update_date": "2010-03-26", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "0909.5673", "submitter": "Christian P. Robert", "authors": "Christian P. Robert, Kerrie L. Mengersen and Carla Chen", "title": "Model choice versus model criticism", "comments": "This is a comment on the recent paper by Ratmann, Andrieu, Wiuf, and\n  Richardson (PNAS, 106), submitted too late for PNAS to consider it", "journal-ref": null, "doi": "10.1073/pnas.0911260107", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The new perspectives on ABC and Bayesian model criticisms presented in\nRatmann et al.(2009) are challenging standard approaches to Bayesian model\nchoice. We discuss here some issues arising from the authors' approach,\nincluding prior influence, model assessment and criticism, and the meaning of\nerror in ABC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2009 17:35:43 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2009 03:50:56 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Robert", "Christian P.", ""], ["Mengersen", "Kerrie L.", ""], ["Chen", "Carla", ""]]}]