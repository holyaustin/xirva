[{"id": "1309.0428", "submitter": "Nebojsa Dedovic M", "authors": "Snezana Matic-Kekic, Nebojsa Dedovic, Beba Mutavdzic", "title": "Nonsensical Excel and Statistica Default Output and Algorithm for an\n  Adequate Display", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to present an algorithm that determines the\nnecessary and sufficient number of significant digits in the coefficients of a\npolynomial trend to achieve a pre-specified precision for the polynomial trend.\nThus, the obtained coefficients should be presented in the default output when\nfitting the experimental data with the polynomial trend. Namely, when finding\nthe best fitting function for a certain type of data, there is a real\npossibility of making significant errors when using the default output of the\nExcel 2003, 2007, 2010, and 2013 and Statistica for Windows 2007-2012 software\npackages. Conversely, the software package Mathematica (version 6) has been\nvery good at dealing with precision problems, although the default output\nsometimes shows more significant digits than necessary.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2013 23:14:58 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Matic-Kekic", "Snezana", ""], ["Dedovic", "Nebojsa", ""], ["Mutavdzic", "Beba", ""]]}, {"id": "1309.1192", "submitter": "Karl Broman", "authors": "Karl W. Broman", "title": "Fourteen years of R/qtl: Just barely sustainable", "comments": "Previously submission to First Workshop on Sustainable Software for\n  Science: Practice and Experiences (WSSSPE),\n  http://wssspe.researchcomputing.org.uk; revised for submission to the Journal\n  of Open Research Software, http://openresearchsoftware.metajnl.com/", "journal-ref": "Broman, K.W. 2014. Fourteen Years of R/qtl: Just Barely\n  Sustainable. Journal of Open Research Software 2(1):e11", "doi": "10.5334/jors.at", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  R/qtl is an R package for mapping quantitative trait loci (genetic loci that\ncontribute to variation in quantitative traits) in experimental crosses. Its\ndevelopment began in 2000. There have been 38 software releases since 2001. The\nlatest release contains 35k lines of R code and 24k lines of C code, plus 15k\nlines of code for the documentation. Challenges in the development and\nmaintenance of the software are discussed. A key to the success of R/qtl is\nthat it remains a central tool for the chief developer's own research work, and\nso its maintenance is of selfish importance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2013 21:37:06 GMT"}, {"version": "v2", "created": "Sun, 6 Apr 2014 18:01:18 GMT"}], "update_date": "2014-07-10", "authors_parsed": [["Broman", "Karl W.", ""]]}, {"id": "1309.1246", "submitter": "Rieko  Sakurai", "authors": "Rieko Sakurai, Toshio Sakata", "title": "Holonomic Decent Minimization Method for Restricted Maximum Likelihood\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the school of Takemura and Takayama have developed a quite\ninteresting minimization method called holonomic gradient descent method (HGD).\nIt works by a mixed use of Pfaffian differential equation satisfied by an\nobjective holonomic function and an iterative optimization method. They\nsuccessfully applied the method to several maximum likelihood estimation (MLE)\nproblems, which have been intractable in the past. On the other hand, in\nstatistical models, it is not rare that parameters are constrained and\ntherefore the MLE with constraints has been surely one of fundamental topics in\nstatistics. In this paper we develop HGD with constraints for MLE .\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 07:20:28 GMT"}], "update_date": "2013-09-06", "authors_parsed": [["Sakurai", "Rieko", ""], ["Sakata", "Toshio", ""]]}, {"id": "1309.1296", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "Regression with an infinite number of observations applied to estimating\n  the parameters of the stable distribution using the empirical characteristic\n  function", "comments": null, "journal-ref": "Communications in Statistics - Theory and Methods, 2016, 45(11),\n  pp. 3323-3331", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A function of the empirical characteristic function,exists for the stable\ndistribution, which leads to a linear regression and can be used to estimate\nthe parameters. Two approaches are often used, one to find optimal values of t,\nbut these points are dependent on the unknown parameters. And using a fixed\nnumber of values for t. In this work the results when all points in an interval\nis used, thus where least squares using an infinite number of observations,is\napproximated. It was found that this procedure performs good in small samples.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 09:57:33 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1309.1369", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Y. Aravkin, Anna Choromanska, Tony Jebara, and Dimitri\n  Kanevsky", "title": "Semistochastic Quadratic Bound Methods", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partition functions arise in a variety of settings, including conditional\nrandom fields, logistic regression, and latent gaussian models. In this paper,\nwe consider semistochastic quadratic bound (SQB) methods for maximum likelihood\ninference based on partition function optimization. Batch methods based on the\nquadratic bound were recently proposed for this class of problems, and\nperformed favorably in comparison to state-of-the-art techniques.\nSemistochastic methods fall in between batch algorithms, which use all the\ndata, and stochastic gradient type methods, which use small random selections\nat each iteration. We build semistochastic quadratic bound-based methods, and\nprove both global convergence (to a stationary point) under very weak\nassumptions, and linear convergence rate under stronger assumptions on the\nobjective. To make the proposed methods faster and more stable, we consider\ninexact subproblem minimization and batch-size selection schemes. The efficacy\nof SQB methods is demonstrated via comparison with several state-of-the-art\ntechniques on commonly used datasets.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2013 15:12:11 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 02:42:50 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2014 21:00:34 GMT"}, {"version": "v4", "created": "Mon, 17 Feb 2014 22:18:34 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Aravkin", "Aleksandr Y.", ""], ["Choromanska", "Anna", ""], ["Jebara", "Tony", ""], ["Kanevsky", "Dimitri", ""]]}, {"id": "1309.1754", "submitter": "Sayantan Banerjee", "authors": "Sayantan Banerjee and Subhashis Ghosal", "title": "Bayesian estimation of a sparse precision matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a sparse precision matrix of a\nmultivariate Gaussian distribution, including the case where the dimension $p$\nis large. Gaussian graphical models provide an important tool in describing\nconditional independence through presence or absence of the edges in the\nunderlying graph. A popular non-Bayesian method of estimating a graphical\nstructure is given by the graphical lasso. In this paper, we consider a\nBayesian approach to the problem. We use priors which put a mixture of a point\nmass at zero and certain absolutely continuous distribution on off-diagonal\nelements of the precision matrix. Hence the resulting posterior distribution\ncan be used for graphical structure learning. The posterior convergence rate of\nthe precision matrix is obtained. The posterior distribution on the model space\nis extremely cumbersome to compute. We propose a fast computational method for\napproximating the posterior probabilities of various graphs using the Laplace\napproximation approach by expanding the posterior density around the posterior\nmode, which is the graphical lasso by our choice of the prior distribution. We\nalso provide estimates of the accuracy in the approximation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2013 19:55:36 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 01:45:34 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Banerjee", "Sayantan", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1309.1799", "submitter": "Yajuan Si", "authors": "Yajuan Si, Natesh S. Pillai, Andrew Gelman", "title": "Bayesian Nonparametric Weighted Sampling Inference", "comments": "Published at http://dx.doi.org/10.1214/14-BA924 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 3, 605-625", "doi": "10.1214/14-BA924", "report-no": "VTeX-BA-BA924", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has historically been a challenge to perform Bayesian inference in a\ndesign-based survey context. The present paper develops a Bayesian model for\nsampling inference in the presence of inverse-probability weights. We use a\nhierarchical approach in which we model the distribution of the weights of the\nnonsampled units in the population and simultaneously include them as\npredictors in a nonparametric Gaussian process regression. We use simulation\nstudies to evaluate the performance of our procedure and compare it to the\nclassical design-based estimator. We apply our method to the Fragile Family and\nChild Wellbeing Study. Our studies find the Bayesian nonparametric finite\npopulation estimator to be more robust than the classical design-based\nestimator without loss in efficiency, which works because we induce\nregularization for small cells and thus this is a way of automatically\nsmoothing the highly variable weights.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 01:14:31 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 21:52:49 GMT"}, {"version": "v3", "created": "Mon, 13 Oct 2014 21:50:16 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2015 05:49:15 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Pillai", "Natesh S.", ""], ["Gelman", "Andrew", ""]]}, {"id": "1309.1901", "submitter": "Paul McNicholas", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "Variational Bayes Approximations for Clustering via Mixtures of Normal\n  Inverse Gaussian Distributions", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0165-7", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for model-based clustering using a finite mixture of\nnormal inverse Gaussian (NIG) distributions is achieved through variational\nBayes approximations. Univariate NIG mixtures and multivariate NIG mixtures are\nconsidered. The use of variational Bayes approximations here is a substantial\ndeparture from the traditional EM approach and alleviates some of the\nassociated computational complexities and uncertainties. Our variational\nalgorithm is applied to simulated and real data. The paper concludes with\ndiscussion and suggestions for future work.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 20:29:26 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1309.1906", "submitter": "Matthew Pratola", "authors": "Matthew T. Pratola and Hugh A. Chipman and James R. Gattiker and David\n  M. Higdon and Robert McCulloch and William N. Rust", "title": "Parallel Bayesian Additive Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Additive Regression Trees (BART) is a Bayesian approach to flexible\nnon-linear regression which has been shown to be competitive with the best\nmodern predictive methods such as those based on bagging and boosting. BART\noffers some advantages. For example, the stochastic search Markov Chain Monte\nCarlo (MCMC) algorithm can provide a more complete search of the model space\nand variation across MCMC draws can capture the level of uncertainty in the\nusual Bayesian way. The BART prior is robust in that reasonable results are\ntypically obtained with a default prior specification. However, the publicly\navailable implementation of the BART algorithm in the R package BayesTree is\nnot fast enough to be considered interactive with over a thousand observations,\nand is unlikely to even run with 50,000 to 100,000 observations. In this paper\nwe show how the BART algorithm may be modified and then computed using single\nprogram, multiple data (SPMD) parallel computation implemented using the\nMessage Passing Interface (MPI) library. The approach scales nearly linearly in\nthe number of processor cores, enabling the practitioner to perform statistical\ninference on massive datasets. Our approach can also handle datasets too\nmassive to fit on any single data repository.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2013 21:02:01 GMT"}], "update_date": "2013-09-10", "authors_parsed": [["Pratola", "Matthew T.", ""], ["Chipman", "Hugh A.", ""], ["Gattiker", "James R.", ""], ["Higdon", "David M.", ""], ["McCulloch", "Robert", ""], ["Rust", "William N.", ""]]}, {"id": "1309.2375", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz and Tong Zhang", "title": "Accelerated Proximal Stochastic Dual Coordinate Ascent for Regularized\n  Loss Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a proximal version of the stochastic dual coordinate ascent\nmethod and show how to accelerate the method using an inner-outer iteration\nprocedure. We analyze the runtime of the framework and obtain rates that\nimprove state-of-the-art results for various key machine learning optimization\nproblems including SVM, logistic regression, ridge regression, Lasso, and\nmulticlass SVM. Experiments validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 05:39:25 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 06:06:09 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Zhang", "Tong", ""]]}, {"id": "1309.2388", "submitter": "Mark Schmidt", "authors": "Mark Schmidt (SIERRA, LIENS), Nicolas Le Roux (SIERRA, LIENS), Francis\n  Bach (SIERRA, LIENS)", "title": "Minimizing Finite Sums with the Stochastic Average Gradient", "comments": "Revision from January 2015 submission. Major changes: updated\n  literature follow and discussion of subsequent work, additional Lemma showing\n  the validity of one of the formulas, somewhat simplified presentation of\n  Lyapunov bound, included code needed for checking proofs rather than the\n  polynomials generated by the code, added error regions to the numerical\n  experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the stochastic average gradient (SAG) method for optimizing the\nsum of a finite number of smooth convex functions. Like stochastic gradient\n(SG) methods, the SAG method's iteration cost is independent of the number of\nterms in the sum. However, by incorporating a memory of previous gradient\nvalues the SAG method achieves a faster convergence rate than black-box SG\nmethods. The convergence rate is improved from O(1/k^{1/2}) to O(1/k) in\ngeneral, and when the sum is strongly-convex the convergence rate is improved\nfrom the sub-linear O(1/k) to a linear convergence rate of the form O(p^k) for\np \\textless{} 1. Further, in many cases the convergence rate of the new method\nis also faster than black-box deterministic gradient methods, in terms of the\nnumber of gradient evaluations. Numerical experiments indicate that the new\nalgorithm often dramatically outperforms existing SG and deterministic gradient\nmethods, and that the performance may be further improved through the use of\nnon-uniform sampling strategies.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 06:49:15 GMT"}, {"version": "v2", "created": "Wed, 11 May 2016 06:51:31 GMT"}], "update_date": "2016-05-12", "authors_parsed": [["Schmidt", "Mark", "", "SIERRA, LIENS"], ["Roux", "Nicolas Le", "", "SIERRA, LIENS"], ["Bach", "Francis", "", "SIERRA, LIENS"]]}, {"id": "1309.2627", "submitter": "Xiaoming Lu", "authors": "Lu Xiaoming and Fan Zhaozhi", "title": "Weighted quantile regression for longitudinal data", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a powerful statistical methodology that complements\nthe classical linear regression by examining how covariates influence the\nlocation, scale, and shape of the entire response distribution and offering a\nglobal view of the statistical landscape. In this paper we propose a new\nquantile regression model for longitudinal data. The proposed approach\nincorporates the correlation structure between repeated measures to enhance the\nefficiency of the inference. In order to use the Newton-Raphson iteration\nmethod to obtain convergent estimates, the estimating functions are redefined\nas smoothed functions which are differentiable with respect to regression\nparameters. Our proposed method for quantile regression provides consistent\nestimates with asymptotically normal distributions. Simulation studies are\ncarried out to evaluate the performance of the proposed method. As an\nillustration, the proposed method was applied to a real-life data that contains\nself-reported labor pain for women in two groups.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2013 19:48:21 GMT"}], "update_date": "2013-09-11", "authors_parsed": [["Xiaoming", "Lu", ""], ["Zhaozhi", "Fan", ""]]}, {"id": "1309.2918", "submitter": "Nick Whiteley", "authors": "Nick Whiteley, Anthony Lee, Kari Heine", "title": "On the role of interaction in sequential Monte Carlo algorithms", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ666 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 1, 494-529", "doi": "10.3150/14-BEJ666", "report-no": "IMS-BEJ-BEJ666", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general form of sequential Monte Carlo algorithm defined in\nterms of a parameterized resampling mechanism. We find that a suitably\ngeneralized notion of the Effective Sample Size (ESS), widely used to monitor\nalgorithm degeneracy, appears naturally in a study of its convergence\nproperties. We are then able to phrase sufficient conditions for time-uniform\nconvergence in terms of algorithmic control of the ESS, in turn achievable by\nadaptively modulating the interaction between particles. This leads us to\nsuggest novel algorithms which are, in senses to be made precise, provably\nstable and yet designed to avoid the degree of interaction which hinders\nparallelization of standard algorithms. As a byproduct, we prove time-uniform\nconvergence of the popular adaptive resampling particle filter.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2013 19:20:37 GMT"}, {"version": "v2", "created": "Fri, 12 Sep 2014 14:42:03 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 06:46:52 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Whiteley", "Nick", ""], ["Lee", "Anthony", ""], ["Heine", "Kari", ""]]}, {"id": "1309.3075", "submitter": "Filip Bielejec", "authors": "Filip Bielejec, Philippe Lemey, Guy Baele, Andrew Rambaut, Marc A\n  Suchard", "title": "Inferring Heterogeneous Evolutionary Processes Through Time: from\n  sequence substitution to phylogeography", "comments": "30 pages, 6 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular phylogenetic and phylogeographic reconstructions generally assume\ntime-homogeneous substitution processes. Motivated by computational\nconvenience, this assumption sacrifices biological realism and offers little\nopportunity to uncover the temporal dynamics in evolutionary histories. Here,\nwe extend and generalize an evolutionary approach that relaxes the\ntime-homogeneous process assumption by allowing the specification of different\ninfinitesimal substitution rate matrices across different time intervals,\ncalled epochs, along the evolutionary history. We focus on an epoch model\nimplementation in a Bayesian inference framework that offers great modeling\nflexibility in drawing inference about any discrete data type characterized as\na continuous-time Markov chain, including phylogeographic traits. To alleviate\nthe computational burden that the additional temporal heterogeneity imposes, we\nadopt a massively parallel approach that achieves both fine- and coarse-grain\nparallelization of the computations across branches that accommodate epoch\ntransitions, making extensive use of graphics processing units. Through\nsynthetic examples, we assess model performance in recovering evolutionary\nparameters from data generated according to different evolutionary scenarios\nthat comprise different numbers of epochs for both nucleotide and codon\nsubstitution processes. We illustrate the usefulness of our inference framework\nin two different applications to empirical data sets: the selection dynamics on\nwithin-host HIV populations throughout infection and the seasonality of global\ninfluenza circulation. In both cases, our epoch model captures key features of\ntemporal heterogeneity that remained difficult to test using ad hoc procedures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 09:38:43 GMT"}], "update_date": "2013-09-13", "authors_parsed": [["Bielejec", "Filip", ""], ["Lemey", "Philippe", ""], ["Baele", "Guy", ""], ["Rambaut", "Andrew", ""], ["Suchard", "Marc A", ""]]}, {"id": "1309.3217", "submitter": "Xiyun Jiao", "authors": "David A. van Dyk and Xiyun Jiao", "title": "Metropolis-Hastings within Partially Collapsed Gibbs Samplers", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics (2015), 24,\n  301-327", "doi": "10.1080/10618600.2014.930041", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Partially Collapsed Gibbs (PCG) sampler offers a new strategy for\nimproving the convergence of a Gibbs sampler. PCG achieves faster convergence\nby reducing the conditioning in some of the draws of its parent Gibbs sampler.\nAlthough this can significantly improve convergence, care must be taken to\nensure that the stationary distribution is preserved. The conditional\ndistributions sampled in a PCG sampler may be incompatible and permuting their\norder may upset the stationary distribution of the chain. Extra care must be\ntaken when Metropolis-Hastings (MH) updates are used in some or all of the\nupdates. Reducing the conditioning in an MH within Gibbs sampler can change the\nstationary distribution, even when the PCG sampler would work perfectly if MH\nwere not used. In fact, a number of samplers of this sort that have been\nadvocated in the literature do not actually have the target stationary\ndistributions. In this article, we illustrate the challenges that may arise\nwhen using MH within a PCG sampler and develop a general strategy for using\nsuch updates while maintaining the desired stationary distribution. Theoretical\narguments provide guidance when choosing between different MH within PCG\nsampling schemes. Finally we illustrate the MH within PCG sampler and its\ncomputational advantage using several examples from our applied work.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 16:55:57 GMT"}, {"version": "v2", "created": "Fri, 7 Mar 2014 19:40:54 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["van Dyk", "David A.", ""], ["Jiao", "Xiyun", ""]]}, {"id": "1309.3250", "submitter": "Monir Hajiaghayi", "authors": "Monir Hajiaghayi, Bonnie Kirkpatrick, Liangliang Wang, and Alexandre\n  Bouchard-C\\^ot\\'e", "title": "Efficient Continuous-Time Markov Chain Estimation", "comments": "19 pages, 7 figures, 2 tables, 6 Algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems of practical interest rely on Continuous-time Markov\nchains~(CTMCs) defined over combinatorial state spaces, rendering the\ncomputation of transition probabilities, and hence probabilistic inference,\ndifficult or impossible with existing methods. For problems with countably\ninfinite states, where classical methods such as matrix exponentiation are not\napplicable, the main alternative has been particle Markov chain Monte Carlo\nmethods imputing both the holding times and sequences of visited states. We\npropose a particle-based Monte Carlo approach where the holding times are\nmarginalized analytically. We demonstrate that in a range of realistic\ninferential setups, our scheme dramatically reduces the variance of the Monte\nCarlo approximation and yields more accurate parameter posterior approximations\ngiven a fixed computational budget. These experiments are performed on both\nsynthetic and real datasets, drawing from two important examples of CTMCs\nhaving combinatorial state spaces: string-valued mutation models in\nphylogenetics and nucleic acid folding pathways.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 19:30:14 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 19:30:34 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Hajiaghayi", "Monir", ""], ["Kirkpatrick", "Bonnie", ""], ["Wang", "Liangliang", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1309.3271", "submitter": "Yin-Zhe Ma", "authors": "Yin-Zhe Ma, Aaron Berndsen", "title": "How to combine correlated data sets -- A Bayesian hyperparameter matrix\n  method", "comments": "13 pages, 7 figures. Astronomy and Computing, 2014", "journal-ref": "Astronomy and Computing, 5 (2014) 45-56", "doi": "10.1016/j.ascom.2014.04.005", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a \"hyperparameter matrix\" statistical method for performing the\njoint analyses of multiple correlated astronomical data sets, in which the\nweights of data sets are determined by their own statistical properties. This\nmethod is a generalization of the hyperparameter method constructed by Lahav et\nal. (2000) and Hobson, Bridle, & Lahav (2002) which was designed to combine\nindependent data sets. The advantage of our method is to treat correlations\nbetween multiple data sets and gives appropriate relevant weights of multiple\ndata sets with mutual correlations. We define a new \"element-wise\" product,\nwhich greatly simplifies the likelihood function with hyperparameter matrix. We\nrigorously prove the simplified formula of the joint likelihood and show that\nit recovers the original hyperparameter method in the limit of no covariance\nbetween data sets. We then illustrate the method by applying it to a\ndemonstrative toy model of fitting a straight line to two sets of data. We show\nthat the hyperparameter matrix method can detect unaccounted systematic errors\nor underestimated errors in the data sets. Additionally, the ratio of Bayes'\nfactors provides a distinct indicator of the necessity of including\nhyperparameters. Our example shows that the likelihood we construct for joint\nanalyses of correlated data sets can be widely applied to many astrophysical\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 20:00:00 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 20:27:13 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ma", "Yin-Zhe", ""], ["Berndsen", "Aaron", ""]]}, {"id": "1309.3295", "submitter": "Nicholas James", "authors": "Nicholas A. James and David S. Matteson", "title": "ecp: An R Package for Nonparametric Multiple Change Point Analysis of\n  Multivariate Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many different ways in which change point analysis can be\nperformed, from purely parametric methods to those that are distribution free.\nThe ecp package is designed to perform multiple change point analysis while\nmaking as few assumptions as possible. While many other change point methods\nare applicable only for univariate data, this R package is suitable for both\nunivariate and multivariate observations. Estimation can be based upon either a\nhierarchical divisive or agglomerative algorithm. Divisive estimation\nsequentially identifies change points via a bisection algorithm. The\nagglomerative algorithm estimates change point locations by determining an\noptimal segmentation. Both approaches are able to detect any type of\ndistributional change within the data. This provides an advantage over many\nexisting change point algorithms which are only able to detect changes within\nthe marginal distributions.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2013 20:38:42 GMT"}, {"version": "v2", "created": "Sat, 23 Nov 2013 22:00:58 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["James", "Nicholas A.", ""], ["Matteson", "David S.", ""]]}, {"id": "1309.3386", "submitter": "Huei-Wen Teng", "authors": "Huei-Wen Teng, Ming-Hsuan Kang and Cheng-Der Fuh", "title": "On spherical Monte Carlo simulations for multivariate normal\n  probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The calculation of multivariate normal probabilities is of great importance\nin many statistical and economic applications. This paper proposes a spherical\nMonte Carlo method with both theoretical analysis and numerical simulation.\nFirst, the multivariate normal probability is rewritten via an inner radial\nintegral and an outer spherical integral by the spherical transformation. For\nthe outer spherical integral, we apply an integration rule by randomly rotating\na predetermined set of well-located points. To find the desired set, we derive\nan upper bound for the variance of the Monte Carlo estimator and propose a set\nwhich is related to the kissing number problem in sphere packings. For the\ninner radial integral, we employ the idea of antithetic variates and identify\ncertain conditions so that variance reduction is guaranteed. Extensive Monte\nCarlo experiments on some probabilities calculation confirm these claims.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 07:40:16 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Teng", "Huei-Wen", ""], ["Kang", "Ming-Hsuan", ""], ["Fuh", "Cheng-Der", ""]]}, {"id": "1309.3490", "submitter": "Jozsef Bakosi", "authors": "J. Bakosi and J.R. Ristorcelli", "title": "A stochastic diffusion process for Lochner's generalized Dirichlet\n  distribution", "comments": "Journal of Mathematical Physics, 2013. arXiv admin note: text overlap\n  with arXiv:1303.0217", "journal-ref": "Journal of Mathematical Physics, 54(10), 2013", "doi": "10.1063/1.4822416", "report-no": "LA-UR 13-21573", "categories": "math-ph math.MP math.PR physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of potential solutions of Fokker-Planck equations is used to\ndevelop a transport equation for the joint probability of N stochastic\nvariables with Lochner's generalized Dirichlet distribution (R.H. Lochner, A\nGeneralized Dirichlet Distribution in Bayesian Life Testing, Journal of the\nRoyal Statistical Society, Series B, 37(1):pp. 103-113, 1975) as its asymptotic\nsolution. Individual samples of a discrete ensemble, obtained from the system\nof stochastic differential equations, equivalent to the Fokker-Planck equation\ndeveloped here, satisfy a unit-sum constraint at all times and ensure a bounded\nsample space, similarly to the process developed in (J. Bakosi, J.R.\nRistorcelli, A stochastic diffusion process for the Dirichlet distribution,\nInt. J. Stoch. Anal., Article ID, 842981, 2013) for the Dirichlet distribution.\nConsequently, the generalized Dirichlet diffusion process may be used to\nrepresent realizations of a fluctuating ensemble of N variables subject to a\nconservation principle. Compared to the Dirichlet distribution and process, the\nadditional parameters of the generalized Dirichlet distribution allow a more\ngeneral class of physical processes to be modeled with a more general\ncovariance matrix.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2013 15:45:41 GMT"}], "update_date": "2013-10-02", "authors_parsed": [["Bakosi", "J.", ""], ["Ristorcelli", "J. R.", ""]]}, {"id": "1309.4289", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Bo Zhou, Babak Shahbaba", "title": "Spherical Hamiltonian Monte Carlo for Constrained Target Distributions", "comments": null, "journal-ref": "Proceedings of The 31st International Conference on Machine\n  Learning, pp. 629-637, 2014", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Markov Chain Monte Carlo (MCMC) method for constrained\ntarget distributions. Our method first maps the $D$-dimensional constrained\ndomain of parameters to the unit ball ${\\bf B}_0^D(1)$. Then, it augments the\nresulting parameter space to the $D$-dimensional sphere, ${\\bf S}^D$. The\nboundary of ${\\bf B}_0^D(1)$ corresponds to the equator of ${\\bf S}^D$. This\nchange of domains enables us to implicitly handle the original constraints\nbecause while the sampler moves freely on the sphere, it proposes states that\nare within the constraints imposed on the original parameter space. To improve\nthe computational efficiency of our algorithm, we split the Lagrangian dynamics\ninto several parts such that a part of the dynamics can be handled analytically\nby finding the geodesic flow on the sphere. We apply our method to several\nexamples including truncated Gaussian, Bayesian Lasso, Bayesian bridge\nregression, and a copula model for identifying synchrony among multiple\nneurons. Our results show that the proposed method can provide a natural and\nefficient framework for handling several types of constraints on target\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 12:56:49 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Lan", "Shiwei", ""], ["Zhou", "Bo", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1309.4402", "submitter": "Martin M\\\"achler", "authors": "Marius Hofert and Martin M\\\"achler", "title": "Parallel and other simulations in R made easy: An end-to-end study", "comments": "The first 19 pages = user manual; Rest is a description of the\n  implementation and technical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown how to set up, conduct, and analyze large simulation studies with\nthe new R package simsalapar = simulations simplified and launched parallel. A\nsimulation study typically starts with determining a collection of input\nvariables and their values on which the study depends, such as sample sizes,\ndimensions, types and degrees of dependence, estimation methods, etc.\nComputations are desired for all com- binations of these variables. If\nconducting these computations sequentially is too time- consuming, parallel\ncomputing can be applied over all combinations of select variables. The final\nresult object of a simulation study is typically an array. From this array,\nsum- mary statistics can be derived and presented in terms of (flat contingency\nor LATEX) tables or visualized in terms of (matrix-like) figures. The R package\nsimsalapar provides several tools to achieve the above tasks. Warnings and\nerrors are dealt with correctly, various seeding methods are available, and run\ntime is measured. Furthermore, tools for analyzing the results via tables or\ngraphics are pro- vided. In contrast to rather minimal examples typically found\nin R packages or vignettes, an end-to-end, not-so-minimal simulation problem\nfrom the realm of quantitative risk management is given. The concepts presented\nand solutions provided by simsalapar may be of interest to students,\nresearchers, and practitioners as a how-to for conducting real- istic,\nlarge-scale simulation studies in R. Also, the development of the package\nrevealed useful improvements to R itself, which are available in R 3.0.0.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2013 17:34:13 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Hofert", "Marius", ""], ["M\u00e4chler", "Martin", ""]]}, {"id": "1309.5117", "submitter": "Hui Zhao", "authors": "Hui Zhao and Paul Marriott", "title": "Diagnostics for Variational Bayes approximations", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) has shown itself to be a powerful approximation method\nin many application areas. This paper describes some diagnostics methods which\ncan assess how well the VB approximates the true posterior, particularly with\nregards to its covariance structure. The methods proposed also allow us to\ngenerate simple corrections when the approximation error is large. It looks at\njoint, marginal and conditional aspects of the approximate posterior and shows\nhow to apply these techniques in both simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2013 22:59:44 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Zhao", "Hui", ""], ["Marriott", "Paul", ""]]}, {"id": "1309.5122", "submitter": "Hui Zhao", "authors": "Hui Zhao and Paul Marriott", "title": "Variational Bayes inference and Dirichlet process priors", "comments": "submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how the variational Bayes method provides a computational\nefficient technique in the context of hierarchical modelling using Dirichlet\nprocess priors, in particular without requiring conjugate prior assumption. It\nshows, using the so called parameter separation parameterization, a simple\ncriterion under which the variational method works well. Based on this\nframework, its provides a full variational solution for the Dirichlet process.\nThe numerical results show that the method is very computationally efficient\nwhen compared to MCMC. Finally, we propose an empirical method to estimate the\ntruncation level for the truncated Dirichlet process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 00:43:50 GMT"}], "update_date": "2013-09-23", "authors_parsed": [["Zhao", "Hui", ""], ["Marriott", "Paul", ""]]}, {"id": "1309.5124", "submitter": "Brandon Oselio", "authors": "Brandon Oselio and Alex Kulesza and Alfred O. Hero III", "title": "Multi-layer graph analysis for dynamic social networks", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": "10.1109/JSTSP.2014.2328312", "report-no": null, "categories": "cs.SI physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern social networks frequently encompass multiple distinct types of\nconnectivity information; for instance, explicitly acknowledged friend\nrelationships might complement behavioral measures that link users according to\ntheir actions or interests. One way to represent these networks is as\nmulti-layer graphs, where each layer contains a unique set of edges over the\nsame underlying vertices (users). Edges in different layers typically have\nrelated but distinct semantics; depending on the application multiple layers\nmight be used to reduce noise through averaging, to perform multifaceted\nanalyses, or a combination of the two. However, it is not obvious how to extend\nstandard graph analysis techniques to the multi-layer setting in a flexible\nway. In this paper we develop latent variable models and methods for mining\nmulti-layer networks for connectivity patterns based on noisy data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2013 01:06:43 GMT"}, {"version": "v2", "created": "Mon, 12 May 2014 02:53:41 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Oselio", "Brandon", ""], ["Kulesza", "Alex", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1309.5489", "submitter": "Hui Jiang", "authors": "Hui Jiang, John C. Mu, Kun Yang, Chao Du, Luo Lu, Wing Hung Wong", "title": "Computational Aspects of Optional P\\'{o}lya Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optional P\\'{o}lya Tree (OPT) is a flexible non-parametric Bayesian model for\ndensity estimation. Despite its merits, the computation for OPT inference is\nchallenging. In this paper we present time complexity analysis for OPT\ninference and propose two algorithmic improvements. The first improvement,\nnamed Limited-Lookahead Optional P\\'{o}lya Tree (LL-OPT), aims at greatly\naccelerate the computation for OPT inference. The second improvement modifies\nthe output of OPT or LL-OPT and produces a continuous piecewise linear density\nestimate. We demonstrate the performance of these two improvements using\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 15:18:34 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Jiang", "Hui", ""], ["Mu", "John C.", ""], ["Yang", "Kun", ""], ["Du", "Chao", ""], ["Lu", "Luo", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1309.5524", "submitter": "Youssef Marzouk", "authors": "Jinglai Li and Youssef M. Marzouk", "title": "Adaptive construction of surrogates for the Bayesian solution of inverse\n  problems", "comments": "23 pages, 10 figures", "journal-ref": "SIAM Journal on Scientific Computing, 36: A1163--A1186 (2014)", "doi": "10.1137/130938189", "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach to inverse problems typically relies on posterior\nsampling approaches, such as Markov chain Monte Carlo, for which the generation\nof each sample requires one or more evaluations of the parameter-to-observable\nmap or forward model. When these evaluations are computationally intensive,\napproximations of the forward model are essential to accelerating sample-based\ninference. Yet the construction of globally accurate approximations for\nnonlinear forward models can be computationally prohibitive and in fact\nunnecessary, as the posterior distribution typically concentrates on a small\nfraction of the support of the prior distribution. We present a new approach\nthat uses stochastic optimization to construct polynomial approximations over a\nsequence of measures adaptively determined from the data, eventually\nconcentrating on the posterior distribution. The approach yields substantial\ngains in efficiency and accuracy over prior-based surrogates, as demonstrated\nvia application to inverse problems in partial differential equations.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2013 19:39:37 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Li", "Jinglai", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1309.5616", "submitter": "Anat Reiner-Benaim", "authors": "Anat Reiner-Benaim", "title": "Scan statistic tail probability assessment based on process covariance\n  and window size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A scan statistic is examined for the purpose of testing the existence of a\nglobal peak in a random process with dependent variables of any distribution.\nThe scan statistic tail probability is obtained based on the covariance of the\nmoving sums process, thereby accounting for the spatial nature of the data as\nwell as the size of the searching window. Exact formulas linking this\ncovariance to the window size and the correlation coefficient are developed\nunder general, common and auto covariance structures of the variables in the\noriginal process. The implementation and applicability of the formulas are\ndemonstrated on families of multiple processes of t-statistics. A sensitivity\nanalysis provides further insight into the variant interaction of the tail\nprobability with the influence parameters. An R code for the tail probability\ncomputation is offered within the supplementary material.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2013 16:39:27 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Reiner-Benaim", "Anat", ""]]}, {"id": "1309.5808", "submitter": "Ulf Schepsmeier", "authors": "Ulf Schepsmeier", "title": "Efficient goodness-of-fit tests in multi-dimensional vine copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new goodness-of-fit test for regular vine (R-vine) copula\nmodels, a flexible class of multivariate copulas based on a pair-copula\nconstruction (PCC). The test arises from the information matrix ratio. The\ncorresponding test statistic is derived and its asymptotic normality is proven.\nThe test's power is investigated and compared to 14 other goodness-of-fit\ntests, adapted from the bivariate copula case, in a high dimensional setting.\nThe extensive simulation study shows the excellent performance with respect to\nsize and power as well as the superiority of the information matrix ratio based\ntest against most other goodness-of-fit tests. The best performing tests are\napplied to a portfolio of stock indices and their related volatility indices\nvalidating different R-vine specifications.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 14:01:32 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Schepsmeier", "Ulf", ""]]}, {"id": "1309.5977", "submitter": "Alexander Rakhlin", "authors": "Hariharan Narayanan, Alexander Rakhlin", "title": "Efficient Sampling from Time-Varying Log-Concave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally efficient random walk on a convex body which\nrapidly mixes and closely tracks a time-varying log-concave distribution. We\ndevelop general theoretical guarantees on the required number of steps; this\nnumber can be calculated on the fly according to the distance from and the\nshape of the next distribution. We then illustrate the technique on several\nexamples. Within the context of exponential families, the proposed method\nproduces samples from a posterior distribution which is updated as data arrive\nin a streaming fashion. The sampling technique can be used to track\ntime-varying truncated distributions, as well as to obtain samples from a\nchanging mixture model, fitted in a streaming fashion to data. In the setting\nof linear optimization, the proposed method has oracle complexity with best\nknown dependence on the dimension for certain geometries. In the context of\nonline learning and repeated games, the algorithm is an efficient method for\nimplementing no-regret mixture forecasting strategies. Remarkably, in some of\nthese examples, only one step of the random walk is needed to track the next\ndistribution.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 20:40:09 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Narayanan", "Hariharan", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "1309.6208", "submitter": "Eric Frichot", "authors": "Eric Frichot, Fran\\c{c}ois Mathieu, Th\\'eo Trouillon, Guillaume\n  Bouchard, Olivier Fran\\c{c}ois", "title": "Fast Inference of Admixture Coefficients Using Sparse Non-negative\n  Matrix Factorization Algorithms", "comments": "31 pages, 5 figures, 3 tables, 2 supplementary tables, 4\n  supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference of individual admixture coefficients, which is important for\npopulation genetic and association studies, is commonly performed using\ncompute-intensive likelihood algorithms. With the availability of large\npopulation genomic data sets, fast versions of likelihood algorithms have\nattracted considerable attention. Reducing the computational burden of\nestimation algorithms remains, however, a major challenge. Here, we present a\nfast and efficient method for estimating individual admixture coefficients\nbased on sparse non-negative matrix factorization algorithms. We implemented\nour method in the computer program sNMF, and applied it to human and plant\ngenomic data sets. The performances of sNMF were then compared to the\nlikelihood algorithm implemented in the computer program ADMIXTURE. Without\nloss of accuracy, sNMF computed estimates of admixture coefficients within\nrun-times approximately 10 to 30 times faster than those of ADMIXTURE.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2013 15:19:38 GMT"}], "update_date": "2013-09-25", "authors_parsed": [["Frichot", "Eric", ""], ["Mathieu", "Fran\u00e7ois", ""], ["Trouillon", "Th\u00e9o", ""], ["Bouchard", "Guillaume", ""], ["Fran\u00e7ois", "Olivier", ""]]}, {"id": "1309.6290", "submitter": "Aravindh Krishnamoorthy", "authors": "Aravindh Krishnamoorthy", "title": "Coefficient Matrices Computation of Structural Vector Autoregressive\n  Model", "comments": "2pp; pre-publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the Large Inverse Cholesky (LIC) method, an\nefficient method for computing the coefficient matrices of a Structural Vector\nAutoregressive (SVAR) model.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2013 15:18:14 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2013 15:01:08 GMT"}, {"version": "v3", "created": "Mon, 10 Feb 2014 18:43:25 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Krishnamoorthy", "Aravindh", ""]]}, {"id": "1309.6699", "submitter": "Natesh Pillai", "authors": "Natesh S. Pillai, Aaron Smith", "title": "Finite Sample Properties of Adaptive Markov Chains via Curvature", "comments": "Revised version of the earlier manuscript. In this version we only\n  focus on the equi-energy sampler", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Markov chains are an important class of Monte Carlo methods for\nsampling from probability distributions. The time evolution of adaptive\nalgorithms depends on past samples, and thus these algorithms are\nnon-Markovian. Although there has been previous work establishing conditions\nfor their ergodicity, not much is known theoretically about their finite sample\nproperties. In this paper, using a notion of discrete Ricci curvature for\nMarkov kernels introduced by Ollivier, we establish concentration inequalities\nand finite sample bounds for a class of adaptive Markov chains. After\nestablishing some general results, we give quantitative bounds for\n`multi-level' adaptive algorithms such as the equi-energy sampler. We also\nprovide the first rigorous proofs that the finite sample properties of an\nequi-energy sampler are superior to those of related parallel tempering and\nMetropolis-Hastings samplers after a learning period comparable to their mixing\ntimes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 01:14:02 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 02:10:20 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1309.6745", "submitter": "Marcel Scharth", "authors": "Marcel Scharth and Robert Kohn", "title": "Particle Efficient Importance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient importance sampling (EIS) method is a general principle for the\nnumerical evaluation of high-dimensional integrals that uses the sequential\nstructure of target integrands to build variance minimising importance\nsamplers. Despite a number of successful applications in high dimensions, it is\nwell known that importance sampling strategies are subject to an exponential\ngrowth in variance as the dimension of the integration increases. We solve this\nproblem by recognising that the EIS framework has an offline sequential Monte\nCarlo interpretation. The particle EIS method is based on non-standard\nresampling weights that take into account the look-ahead construction of the\nimportance sampler. We apply the method for a range of univariate and bivariate\nstochastic volatility specifications. We also develop a new application of the\nEIS approach to state space models with Student's t state innovations. Our\nresults show that the particle EIS method strongly outperforms both the\nstandard EIS method and particle filters for likelihood evaluation in high\ndimensions. Moreover, the ratio between the variances of the particle EIS and\nparticle filter methods remains stable as the time series dimension increases.\nWe illustrate the efficiency of the method for Bayesian inference using the\nparticle marginal Metropolis-Hastings and importance sampling squared\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 07:46:46 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Scharth", "Marcel", ""], ["Kohn", "Robert", ""]]}, {"id": "1309.6897", "submitter": "Andrew Butler", "authors": "Andrew Butler, Thomas D. Humphries, Pritam Ranjan, Ronald D. Haynes", "title": "Efficient Optimization of the Likelihood Function in Gaussian Process\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Process (GP) models are popular statistical surrogates used for\nemulating computationally expensive computer simulators. The quality of a GP\nmodel fit can be assessed by a goodness of fit measure based on optimized\nlikelihood. Finding the global maximum of the likelihood function for a GP\nmodel is typically very challenging as the likelihood surface often has\nmultiple local optima, and an explicit expression for the gradient of the\nlikelihood function is typically unavailable. Previous methods for optimizing\nthe likelihood function (e.g. MacDonald et al. (2013)) have proven to be robust\nand accurate, though relatively inefficient. We propose several likelihood\noptimization techniques, including two modified multi-start local search\ntechniques, based on the method implemented by MacDonald et al. (2013), that\nare equally as reliable, and significantly more efficient. A hybridization of\nthe global search algorithm Dividing Rectangles (DIRECT) with the local\noptimization algorithm BFGS provides a comparable GP model quality for a\nfraction of the computational cost, and is the preferred optimization technique\nwhen computational resources are limited. We use several test functions and a\nreal application from an oil reservoir simulation to test and compare the\nperformance of the proposed methods with the one implemented by MacDonald et\nal. (2013) in the R library GPfit. The proposed method is implemented in a\nMatlab package, GPMfit.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 13:54:31 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Butler", "Andrew", ""], ["Humphries", "Thomas D.", ""], ["Ranjan", "Pritam", ""], ["Haynes", "Ronald D.", ""]]}, {"id": "1309.6919", "submitter": "Or Zuk", "authors": "Or Zuk, Amnon Amir, Amit Zeisel, Ohad Shamir and Noam Shental", "title": "Accurate Profiling of Microbial Communities from Massively Parallel\n  Sequencing using Convex Optimization", "comments": "To appear in SPIRE 13", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.GN q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Microbial Community Reconstruction ({\\bf MCR}) Problem, which\nis fundamental for microbiome analysis. In this problem, the goal is to\nreconstruct the identity and frequency of species comprising a microbial\ncommunity, using short sequence reads from Massively Parallel Sequencing (MPS)\ndata obtained for specified genomic regions. We formulate the problem\nmathematically as a convex optimization problem and provide sufficient\nconditions for identifiability, namely the ability to reconstruct species\nidentity and frequency correctly when the data size (number of reads) grows to\ninfinity. We discuss different metrics for assessing the quality of the\nreconstructed solution, including a novel phylogenetically-aware metric based\non the Mahalanobis distance, and give upper-bounds on the reconstruction error\nfor a finite number of reads under different metrics. We propose a scalable\ndivide-and-conquer algorithm for the problem using convex optimization, which\nenables us to handle large problems (with $\\sim10^6$ species). We show using\nnumerical simulations that for realistic scenarios, where the microbial\ncommunities are sparse, our algorithm gives solutions with high accuracy, both\nin terms of obtaining accurate frequency, and in terms of species phylogenetic\nresolution.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2013 14:30:13 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Zuk", "Or", ""], ["Amir", "Amnon", ""], ["Zeisel", "Amit", ""], ["Shamir", "Ohad", ""], ["Shental", "Noam", ""]]}, {"id": "1309.7098", "submitter": "Kyle Treleaven", "authors": "Kyle Treleaven, Emilio Frazzoli", "title": "An Explicit Formulation of the Earth Movers Distance with Continuous\n  Road Map Distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Earth movers distance (EMD) is a measure of distance between probability\ndistributions which is at the heart of mass transportation theory. Recent\nresearch has shown that the EMD plays a crucial role in studying the potential\nimpact of Demand-Responsive Transportation (DRT) and Mobility-on-Demand (MoD)\nsystems, which are growing paradigms for one-way vehicle sharing where people\ndrive (or are driven by) shared vehicles from a point of origin to a point of\ndestination. While the ubiquitous physical transportation setting is the road\nnetwork, characterized by systems of roads connected together by interchanges,\nmost analytical works about vehicle sharing represent distances between points\nin a plane using the simple Euclidean metric. Instead, we consider the EMD when\nthe ground metric is taken from a class of one-dimensional, continuous metric\nspaces, reminiscent of road networks. We produce an explicit formulation of the\nEarth movers distance given any finite road network R. The result generalizes\nthe EMD with a Euclidean R1 ground metric, which had remained one of the only\nknown non-discrete cases with an explicit formula. Our formulation casts the\nEMD as the optimal value of a finite-dimensional, real-valued optimization\nproblem, with a convex objective function and linear constraints. In the\nspecial case that the input distributions have piece-wise uniform (constant)\ndensity, the problem reduces to one whose objective function is convex\nquadratic. Both forms are amenable to modern mathematical programming\ntechniques.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 01:34:17 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 17:00:37 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Treleaven", "Kyle", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1309.7209", "submitter": "Chris Sherlock", "authors": "Chris Sherlock, Alexandre H. Thiery, Gareth O. Roberts, Jeffrey S.\n  Rosenthal", "title": "On the efficiency of pseudo-marginal random walk Metropolis algorithms", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1278 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 238-275", "doi": "10.1214/14-AOS1278", "report-no": "IMS-AOS-AOS1278", "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the behaviour of the pseudo-marginal random walk Metropolis\nalgorithm, where evaluations of the target density for the accept/reject\nprobability are estimated rather than computed precisely. Under relatively\ngeneral conditions on the target distribution, we obtain limiting formulae for\nthe acceptance rate and for the expected squared jump distance, as the\ndimension of the target approaches infinity, under the assumption that the\nnoise in the estimate of the log-target is additive and is independent of the\nposition. For targets with independent and identically distributed components,\nwe also obtain a limiting diffusion for the first component. We then consider\nthe overall efficiency of the algorithm, in terms of both speed of mixing and\ncomputational time. Assuming the additive noise is Gaussian and is inversely\nproportional to the number of unbiased estimates that are used, we prove that\nthe algorithm is optimally efficient when the variance of the noise is\napproximately 3.283 and the acceptance rate is approximately 7.001%. We also\nfind that the optimal scaling is insensitive to the noise and that the optimal\nvariance of the noise is insensitive to the scaling. The theory is illustrated\nwith a simulation study using the particle marginal random walk Metropolis.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2013 11:47:25 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 02:08:11 GMT"}, {"version": "v3", "created": "Tue, 30 Dec 2014 08:21:25 GMT"}], "update_date": "2014-12-31", "authors_parsed": [["Sherlock", "Chris", ""], ["Thiery", "Alexandre H.", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1309.7721", "submitter": "Adrien Ickowicz", "authors": "Ross Sparks, Adrien Ickowicz", "title": "Spatio-Temporal Disease Surveillance: Forward Selection Scan Statistic", "comments": "19 pages, 4 figures, submitted to Journal of applied statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scan statistic sets the benchmark for spatio-temporal surveillance\nmethods with its popularity. In its simplest form it scans the target area and\ntime to find regions with disease count higher than expected. If the shape and\nsize of the disease outbreaks are known, then to detect it sufficiently early\nthe scan statistic can design its search area to be efficient for this shape\nand size. A plan that is efficient at detecting a range of disease outbreak\nshapes and sizes is important because these vary from one outbreak to the next\nand are generally never known in advance. This paper offers a forward selection\nscan statistic that reduces the computational effort on the usual single window\nscan plan, while still offering greater flexibility in signalling outbreaks of\nvarying shapes. The approach starts by dividing the target geographical regions\ninto a lattice. Secondly it smooths the time series of lattice cell counts\nusing multivariate exponential weighted moving averages. Thirdly, these EWMA\ncell counts are spatially smoothed to reduce spatial noise and leave the\nspatial signal. The fourth step uses forward selection approach to scanning\nmutually exclusive and exhaustive rectangular regions of dynamic dimensions. In\nthe fifth step, it prunes away all insignificant scanned regions where counts\nare not significantly higher than expected. An outbreak is signaled if at least\none region remains after pruning. If all regions are pruned away - including\nthe scan of the target region, then no outbreak is signaled.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 04:55:12 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Sparks", "Ross", ""], ["Ickowicz", "Adrien", ""]]}, {"id": "1309.7807", "submitter": "Hans R. K\\\"{u}nsch", "authors": "Hans R. K\\\"unsch", "title": "Particle filters", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJSP07 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 4, 1391-1403", "doi": "10.3150/12-BEJSP07", "report-no": "IMS-BEJ-BEJSP07", "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a short review of Monte Carlo methods for approximating filter\ndistributions in state space models. The basic algorithm and different\nstrategies to reduce imbalance of the weights are discussed. Finally, methods\nfor more difficult problems like smoothing and parameter estimation and\napplications outside the state space model context are presented.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2013 12:06:02 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["K\u00fcnsch", "Hans R.", ""]]}]