[{"id": "2011.00144", "submitter": "Samarth Gupta", "authors": "Samarth Gupta, Saurabh Amin", "title": "Integer Programming-based Error-Correcting Output Code Design for Robust\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-Correcting Output Codes (ECOCs) offer a principled approach for\ncombining simple binary classifiers into multiclass classifiers. In this paper,\nwe investigate the problem of designing optimal ECOCs to achieve both nominal\nand adversarial accuracy using Support Vector Machines (SVMs) and binary deep\nlearning models. In contrast to previous literature, we present an Integer\nProgramming (IP) formulation to design minimal codebooks with desirable error\ncorrecting properties. Our work leverages the advances in IP solvers to\ngenerate codebooks with optimality guarantees. To achieve tractability, we\nexploit the underlying graph-theoretic structure of the constraint set in our\nIP formulation. This enables us to use edge clique covers to substantially\nreduce the constraint set. Our codebooks achieve a high nominal accuracy\nrelative to standard codebooks (e.g., one-vs-all, one-vs-one, and dense/sparse\ncodes). We also estimate the adversarial accuracy of our ECOC-based classifiers\nin a white-box setting. Our IP-generated codebooks provide non-trivial\nrobustness to adversarial perturbations even without any adversarial training.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 23:35:18 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Gupta", "Samarth", ""], ["Amin", "Saurabh", ""]]}, {"id": "2011.00680", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang", "title": "Modern Monte Carlo Methods for Efficient Uncertainty Quantification and\n  Propagation: A Survey", "comments": "This review paper has been accepted by WIREs Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty quantification (UQ) includes the characterization, integration,\nand propagation of uncertainties that result from stochastic variations and a\nlack of knowledge or data in the natural world. Monte Carlo (MC) method is a\nsampling-based approach that has widely used for quantification and propagation\nof uncertainties. However, the standard MC method is often time-consuming if\nthe simulation-based model is computationally intensive. This article gives an\noverview of modern MC methods to address the existing challenges of the\nstandard MC in the context of UQ. Specifically, multilevel Monte Carlo (MLMC)\nextending the concept of control variates achieves a significant reduction of\nthe computational cost by performing most evaluations with low accuracy and\ncorresponding low cost, and relatively few evaluations at high accuracy and\ncorrespondingly high cost. Multifidelity Monte Carlo (MFMC) accelerates the\nconvergence of standard Monte Carlo by generalizing the control variates with\ndifferent models having varying fidelities and varying computational costs.\nMultimodel Monte Carlo method (MMMC), having a different setting of MLMC and\nMFMC, aims to address the issue of uncertainty quantification and propagation\nwhen data for characterizing probability distributions are limited. Multimodel\ninference combined with importance sampling is proposed for quantifying and\nefficiently propagating the uncertainties resulting from small datasets. All of\nthese three modern MC methods achieve a significant improvement of\ncomputational efficiency for probabilistic UQ, particularly uncertainty\npropagation. An algorithm summary and the corresponding code implementation are\nprovided for each of the modern Monte Carlo methods. The extension and\napplication of these methods are discussed in detail.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 02:02:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Zhang", "Jiaxin", ""]]}, {"id": "2011.00787", "submitter": "Santosh Kumar", "authors": "Peter J. Forrester and Santosh Kumar", "title": "Differential recurrences for the distribution of the trace of the\n  $\\beta$-Jacobi ensemble", "comments": "28 pages, 1 figure, Mathematica codes & Codes description included as\n  ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph cond-mat.mes-hall math.MP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Examples of the $\\beta$-Jacobi ensemble specify the joint distribution of the\ntransmission eigenvalues in scattering problems. In this context, there has\nbeen interest in the distribution of the trace, as the trace corresponds to the\nconductance. Earlier, in the case $\\beta = 1$, the trace statistic was isolated\nin studies of covariance matrices in multivariate statistics, where it is\nreferred to as Pillai's $V$ statistic. In this context, Davis showed that for\n$\\beta = 1$ the trace statistic, and its Fourier-Laplace transform, can be\ncharacterised by $(N+1) \\times (N+1)$ matrix differential equations. For the\nFourier-Laplace transform, this leads to a vector recurrence for the moments.\nHowever, for the distribution itself the characterisation provided was\nincomplete, as the connection problem of determining the linear combination of\nFrobenius type solutions that correspond to the statistic was not solved. We\nsolve this connection problem for Jacobi parameter $b$ and Dyson index $\\beta$\nnon-negative integers. For the other Jacobi parameter $a$ also a non-negative\ninteger, the power series portion of each Frobenius solution terminates to a\npolynomial, and the matrix differential equation gives a recurrence for their\ncomputation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 07:35:49 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 13:44:49 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Forrester", "Peter J.", ""], ["Kumar", "Santosh", ""]]}, {"id": "2011.00898", "submitter": "Christian M\\\"uller", "authors": "L\\'eo Simpson, Patrick L. Combettes, Christian L. M\\\"uller", "title": "c-lasso -- a Python package for constrained sparse and robust regression\n  and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce c-lasso, a Python package that enables sparse and robust linear\nregression and classification with linear equality constraints. The underlying\nstatistical forward model is assumed to be of the following form: \\[ y = X\n\\beta + \\sigma \\epsilon \\qquad \\textrm{subject to} \\qquad C\\beta=0 \\] Here, $X\n\\in \\mathbb{R}^{n\\times d}$is a given design matrix and the vector $y \\in\n\\mathbb{R}^{n}$ is a continuous or binary response vector. The matrix $C$ is a\ngeneral constraint matrix. The vector $\\beta \\in \\mathbb{R}^{d}$ contains the\nunknown coefficients and $\\sigma$ an unknown scale. Prominent use cases are\n(sparse) log-contrast regression with compositional data $X$, requiring the\nconstraint $1_d^T \\beta = 0$ (Aitchion and Bacon-Shone 1984) and the\nGeneralized Lasso which is a special case of the described problem (see, e.g,\n(James, Paulson, and Rusmevichientong 2020), Example 3). The c-lasso package\nprovides estimators for inferring unknown coefficients and scale (i.e.,\nperspective M-estimators (Combettes and M\\\"uller 2020a)) of the form \\[\n\\min_{\\beta \\in \\mathbb{R}^d, \\sigma \\in \\mathbb{R}_{0}} f\\left(X\\beta -\ny,{\\sigma} \\right) + \\lambda \\left\\lVert \\beta\\right\\rVert_1 \\qquad\n\\textrm{subject to} \\qquad C\\beta = 0 \\] for several convex loss functions\n$f(\\cdot,\\cdot)$. This includes the constrained Lasso, the constrained scaled\nLasso, and sparse Huber M-estimators with linear equality constraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 11:16:27 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Simpson", "L\u00e9o", ""], ["Combettes", "Patrick L.", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "2011.01197", "submitter": "Krzysztof Szajowski", "authors": "Maria Katarzyna Stachowiak and Krzysztof J\\'ozef Szajowski", "title": "Cross-entropy method in application to SIRC model", "comments": "22 pages, 9 figures", "journal-ref": "https://www.mdpi.com/1999-4893/13/11/281", "doi": "10.3390/a13110281", "report-no": null, "categories": "math.OC math.PR stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The study considers the usage of a probabilistic optimization method called\nCross-Entropy (CE). This is the version of the Monte Carlo method created by\nReuven Rubinstein (1997). It was developed in the context of determining rare\nevents. Here we will present the way in which the CE method can be used for\nproblems of optimization of epidemiological models, and more specifically the\noptimization of the SIRC (Susceptible - Infectious - Recovered - Cross-immune)\nmodel based on the functions supervising the care of specific groups in the\nmodel. With the help of weighted sampling, an attempt was made to find the\nfastest and most accurate version of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 18:41:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Stachowiak", "Maria Katarzyna", ""], ["Szajowski", "Krzysztof J\u00f3zef", ""]]}, {"id": "2011.01379", "submitter": "Angeliki Papana", "authors": "Angeliki Papana, Elsa Siggiridou, Dimitris Kugiumtzis", "title": "Detecting direct causality in multivariate time series: A comparative\n  study", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nlin.CD stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of Granger causality is increasingly being applied for the\ncharacterization of directional interactions in different applications. A\nmultivariate framework for estimating Granger causality is essential in order\nto account for all the available information from multivariate time series.\nHowever, the inclusion of non-informative or non-significant variables creates\nestimation problems related to the 'curse of dimensionality'. To deal with this\nissue, direct causality measures using variable selection and dimension\nreduction techniques have been introduced. In this comparative work, the\nperformance of an ensemble of bivariate and multivariate causality measures in\nthe time domain is assessed, focusing on dimension reduction causality\nmeasures. In particular, different types of high-dimensional coupled discrete\nsystems are used (involving up to 100 variables) and the robustness of the\ncausality measures to time series length and different noise types is examined.\nThe results of the simulation study highlight the superiority of the dimension\nreduction measures, especially for high-dimensional systems.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2020 23:21:57 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Papana", "Angeliki", ""], ["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "2011.01484", "submitter": "Kimmo Suotsalo", "authors": "Kimmo Suotsalo, Yingying Xu, Jukka Corander, Johan Pensar", "title": "High-dimensional structure learning of sparse vector autoregressive\n  models using fractional marginal pseudo-likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning vector autoregressive models from multivariate time series is\nconventionally approached through least squares or maximum likelihood\nestimation. These methods typically assume a fully connected model which\nprovides no direct insight to the model structure and may lead to highly noisy\nestimates of the parameters. Because of these limitations, there has been an\nincreasing interest towards methods that produce sparse estimates through\npenalized regression. However, such methods are computationally intensive and\nmay become prohibitively time-consuming when the number of variables in the\nmodel increases. In this paper we adopt an approximate Bayesian approach to the\nlearning problem by combining fractional marginal likelihood and\npseudo-likelihood. We propose a novel method, PLVAR, that is both faster and\nproduces more accurate estimates than the state-of-the-art methods based on\npenalized regression. We prove the consistency of the PLVAR estimator and\ndemonstrate the attractive performance of the method on both simulated and\nreal-world data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 05:12:31 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Suotsalo", "Kimmo", ""], ["Xu", "Yingying", ""], ["Corander", "Jukka", ""], ["Pensar", "Johan", ""]]}, {"id": "2011.01661", "submitter": "Subhadip Maji", "authors": "Indranil Basu and Subhadip Maji", "title": "Multicollinearity Correction and Combined Feature Effect in Shapley\n  Values", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model interpretability is one of the most intriguing problems in most of the\nMachine Learning models, particularly for those that are mathematically\nsophisticated. Computing Shapley Values are arguably the best approach so far\nto find the importance of each feature in a model, at the row level. In other\nwords, Shapley values represent the importance of a feature for a particular\nrow, especially for Classification or Regression problems. One of the biggest\nlimitations of Shapley vales is that, Shapley value calculations assume all the\nfeatures are uncorrelated (independent of each other), this assumption is often\nincorrect. To address this problem, we present a unified framework to calculate\nShapley values with correlated features. To be more specific, we do an\nadjustment (Matrix formulation) of the features while calculating Independent\nShapley values for the rows. Moreover, we have given a Mathematical proof\nagainst the said adjustments. With these adjustments, Shapley values\n(Importance) for the features become independent of the correlations existing\nbetween them. We have also enhanced this adjustment concept for more than\nfeatures. As the Shapley values are additive, to calculate combined effect of\ntwo features, we just have to add their individual Shapley values. This is\nagain not right if one or more of the features (used in the combination) are\ncorrelated with the other features (not in the combination). We have addressed\nthis problem also by extending the correlation adjustment for one feature to\nmultiple features in the said combination for which Shapley values are\ndetermined. Our implementation of this method proves that our method is\ncomputationally efficient also, compared to original Shapley method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 12:28:42 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Basu", "Indranil", ""], ["Maji", "Subhadip", ""]]}, {"id": "2011.01799", "submitter": "Camille Chapdelaine", "authors": "Sylvaine Picard, Camille Chapdelaine, Cyril Cappi, Laurent Gardes,\n  Eric Jenn, Baptiste Lef\\`evre, Thomas Soumarmon", "title": "Ensuring Dataset Quality for Machine Learning Certification", "comments": null, "journal-ref": "The 10th IEEE International Workshop on Software Certification\n  (WoSoCer 2020)", "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of dataset quality in the context of\nMachine Learning (ML)-based critical systems. We briefly analyse the\napplicability of some existing standards dealing with data and show that the\nspecificities of the ML context are neither properly captured nor taken into\nac-count. As a first answer to this concerning situation, we propose a dataset\nspecification and verification process, and apply it on a signal recognition\nsystem from the railway domain. In addi-tion, we also give a list of\nrecommendations for the collection and management of datasets. This work is one\nstep towards the dataset engineering process that will be required for ML to be\nused on safety critical systems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2020 15:45:43 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Picard", "Sylvaine", ""], ["Chapdelaine", "Camille", ""], ["Cappi", "Cyril", ""], ["Gardes", "Laurent", ""], ["Jenn", "Eric", ""], ["Lef\u00e8vre", "Baptiste", ""], ["Soumarmon", "Thomas", ""]]}, {"id": "2011.02328", "submitter": "Hai Dang Dau", "authors": "Hai-Dang Dau and Nicolas Chopin", "title": "Waste-free Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard way to move particles in a SMC sampler is to apply several steps\nof a MCMC (Markov chain Monte Carlo) kernel. Unfortunately, it is not clear how\nmany steps need to be performed for optimal performance. In addition, the\noutput of the intermediate steps are discarded and thus wasted somehow. We\npropose a new, waste-free SMC algorithm which uses the outputs of all these\nintermediate MCMC steps as particles. We establish that its output is\nconsistent and asymptotically normal. We use the expression of the asymptotic\nvariance to develop various insights on how to implement the algorithm in\npractice. We develop in particular a method to estimate, from a single run of\nthe algorithm, the asymptotic variance of any particle estimate. We show\nempirically, through a range of numerical examples, that waste-free SMC tends\nto outperform standard SMC samplers, and especially so in situations where the\nmixing of the considered MCMC kernels decreases across iterations (as in\ntempering or rare event problems).\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 14:58:29 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Dau", "Hai-Dang", ""], ["Chopin", "Nicolas", ""]]}, {"id": "2011.02396", "submitter": "Zhenhuan(Neyo) Yang", "authors": "Zhenhuan Yang, Baojian Zhou, Yunwen Lei, Yiming Ying", "title": "Stochastic Hard Thresholding Algorithms for AUC Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to develop stochastic hard thresholding algorithms for\nthe important problem of AUC maximization in imbalanced classification. The\nmain challenge is the pairwise loss involved in AUC maximization. We overcome\nthis obstacle by reformulating the U-statistics objective function as an\nempirical risk minimization (ERM), from which a stochastic hard thresholding\nalgorithm (\\texttt{SHT-AUC}) is developed. To our best knowledge, this is the\nfirst attempt to provide stochastic hard thresholding algorithms for AUC\nmaximization with a per-iteration cost $\\O(b d)$ where $d$ and $b$ are the\ndimension of the data and the minibatch size, respectively. We show that the\nproposed algorithm enjoys the linear convergence rate up to a tolerance error.\nIn particular, we show, if the data is generated from the Gaussian\ndistribution, then its convergence becomes slower as the data gets more\nimbalanced. We conduct extensive experiments to show the efficiency and\neffectiveness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2020 16:49:29 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yang", "Zhenhuan", ""], ["Zhou", "Baojian", ""], ["Lei", "Yunwen", ""], ["Ying", "Yiming", ""]]}, {"id": "2011.02672", "submitter": "Shushu Zhang", "authors": "Shushu Zhang, Vivak Patel", "title": "Stochastic Approximation for High-frequency Observations in Data\n  Assimilation", "comments": "21 pages, 5 figures, submitted to SIAM/ASA Journal on Uncertainty\n  Quantification (JUQ)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing penetration of high-frequency sensors across a number of\nbiological and physical systems, the abundance of the resulting observations\noffers opportunities for higher statistical accuracy of down-stream estimates,\nbut their frequency results in a plethora of computational problems in data\nassimilation tasks. The high-frequency of these observations has been\ntraditionally dealt with by using data modification strategies such as\naccumulation, averaging, and sampling. However, these data modification\nstrategies will reduce the quality of the estimates, which may be untenable for\nmany systems. Therefore, to ensure high-quality estimates, we adapt stochastic\napproximation methods to address the unique challenges of high-frequency\nobservations in data assimilation. As a result, we are able to produce\nestimates that leverage all of the observations in a manner that avoids the\naforementioned computational problems and preserves the statistical accuracy of\nthe estimates.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 06:02:27 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Zhang", "Shushu", ""], ["Patel", "Vivak", ""]]}, {"id": "2011.02761", "submitter": "Kirankumar Shiragur", "authors": "Nima Anari, Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "Instance Based Approximations to Profile Maximum Likelihood", "comments": "Accepted at Thirty-fourth Conference on Neural Information Processing\n  Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a new efficient algorithm for approximately\ncomputing the profile maximum likelihood (PML) distribution, a prominent\nquantity in symmetric property estimation. We provide an algorithm which\nmatches the previous best known efficient algorithms for computing approximate\nPML distributions and improves when the number of distinct observed frequencies\nin the given instance is small. We achieve this result by exploiting new\nsparsity structure in approximate PML distributions and providing a new matrix\nrounding algorithm, of independent interest. Leveraging this result, we obtain\nthe first provable computationally efficient implementation of PseudoPML, a\ngeneral framework for estimating a broad class of symmetric properties.\nAdditionally, we obtain efficient PML-based estimators for distributions with\nsmall profile entropy, a natural instance-based complexity measure. Further, we\nprovide a simpler and more practical PseudoPML implementation that matches the\nbest-known theoretical guarantees of such an estimator and evaluate this method\nempirically.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 11:17:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Anari", "Nima", ""], ["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2011.03121", "submitter": "Naoki Awaya", "authors": "Naoki Awaya, Li Ma", "title": "Hidden Markov P\\'olya trees for high-dimensional distributions", "comments": "50 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The P\\'olya tree (PT) process is a general-purpose Bayesian nonparametric\nmodel that has found wide application in a range of inference problems. The PT\nhas a simple analytic form and the resulting posterior computation boils down\nto straight-forward beta-binomial conjugate updates along a partition tree over\nthe sample space. Recent development in PT models shows that performance of\nthese models can be substantially improved by (i) incorporating latent state\nvariables that characterize local features of the underlying distributions and\n(ii) allowing the partition tree to adapt to the structure of the underlying\ndistribution. Despite these advances, however, some important limitations of\nthe PT that remain include---(i) the sensitivity in the posterior inference\nwith respect to the choice of the partition points, and (ii) the lack of\ncomputational scalability to multivariate problems beyond a small number\n($<10$) of dimensions. We consider a modeling strategy for PT models that\nincorporates a very flexible prior on the partition tree along with latent\nstates that can be first-order dependent (i.e., following a Markov process),\nand introduce a hybrid algorithm that combines sequential Monte Carlo (SMC) and\nrecursive message passing for posterior inference that can readily accommodate\nPT models with or without latent states as well as flexible partition points in\nproblems up to 100 dimensions. Moreover, we investigate the large sample\nproperties of the tree structures and latent states under the posterior model.\nWe carry out extensive numerical experiments in the context of density\nestimation and two-sample testing, which show that flexible partitioning can\nsubstantially improve the performance of PT models in both inference tasks. We\ndemonstrate an application to a flow cytometry data set with 19 dimensions and\nover 200,000 observations.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2020 22:14:03 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 04:23:23 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Awaya", "Naoki", ""], ["Ma", "Li", ""]]}, {"id": "2011.03176", "submitter": "Krishnakumar Balasubramanian", "authors": "Ye He, Krishnakumar Balasubramanian, Murat A. Erdogdu", "title": "On the Ergodicity, Bias and Asymptotic Normality of Randomized Midpoint\n  Sampling Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The randomized midpoint method, proposed by [SL19], has emerged as an optimal\ndiscretization procedure for simulating the continuous time Langevin\ndiffusions. Focusing on the case of strong-convex and smooth potentials, in\nthis paper, we analyze several probabilistic properties of the randomized\nmidpoint discretization method for both overdamped and underdamped Langevin\ndiffusions. We first characterize the stationary distribution of the discrete\nchain obtained with constant step-size discretization and show that it is\nbiased away from the target distribution. Notably, the step-size needs to go to\nzero to obtain asymptotic unbiasedness. Next, we establish the asymptotic\nnormality for numerical integration using the randomized midpoint method and\nhighlight the relative advantages and disadvantages over other discretizations.\nOur results collectively provide several insights into the behavior of the\nrandomized midpoint discretization method, including obtaining confidence\nintervals for numerical integrations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 03:39:23 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["He", "Ye", ""], ["Balasubramanian", "Krishnakumar", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "2011.03305", "submitter": "Xudong Li", "authors": "Zhensheng Yu, Xuyu Chen, Xudong Li", "title": "A dynamic programming approach for generalized nearly isotonic\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape restricted statistical estimation problems have been extensively\nstudied, with many important practical applications in signal processing,\nbioinformatics, and machine learning. In this paper, we propose and study a\ngeneralized nearly isotonic optimization (GNIO) model, which recovers, as\nspecial cases, many classic problems in shape constrained statistical\nregression, such as isotonic regression, nearly isotonic regression and\nunimodal regression problems. We develop an efficient and easy-to-implement\ndynamic programming algorithm for solving the proposed model whose recursion\nnature is carefully uncovered and exploited. For special $\\ell_2$-GNIO\nproblems, implementation details and the optimal ${\\cal O}(n)$ running time\nanalysis of our algorithm are discussed. Numerical experiments, including the\ncomparison between our approach and the powerful commercial solver Gurobi for\nsolving $\\ell_1$-GNIO and $\\ell_2$-GNIO problems, on both simulated and real\ndata sets are presented to demonstrate the high efficiency and robustness of\nour proposed algorithm in solving large scale GNIO problems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2020 12:11:53 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Yu", "Zhensheng", ""], ["Chen", "Xuyu", ""], ["Li", "Xudong", ""]]}, {"id": "2011.04155", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Xibin Zhang", "title": "Bayesian bandwidth estimation for local linear fitting in nonparametric\n  regression models", "comments": "25 pages, 6 figures, to appear at Studies in Nonlinear Dynamics &\n  Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian sampling approach to bandwidth estimation for\nthe local linear estimator of the regression function in a nonparametric\nregression model. In the Bayesian sampling approach, the error density is\napproximated by a location-mixture density of Gaussian densities with means the\nindividual errors and variance a constant parameter. This mixture density has\nthe form of a kernel density estimator of errors and is referred to as the\nkernel-form error density (c.f., Zhang et al., 2014). While Zhang et al. (2014)\nuse the local constant (also known as the Nadaraya- Watson) estimator to\nestimate the regression function, we extend this to the local linear estimator,\nwhich produces more accurate estimation. The proposed investigation is\nmotivated by the lack of data-driven methods for simultaneously choosing\nbandwidths in the local linear estimator of the regression function and\nkernel-form error density. Treating bandwidths as parameters, we derive an\napproximate (pseudo) likelihood and a posterior. A simulation study shows that\nthe proposed bandwidth estimation outperforms the rule-of-thumb and\ncross-validation methods under the criterion of integrated squared errors. The\nproposed bandwidth estimation method is validated through a nonparametric\nregression model involving firm ownership concentration, and a model involving\nstate-price density estimation.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 02:26:34 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Shang", "Han Lin", ""], ["Zhang", "Xibin", ""]]}, {"id": "2011.04342", "submitter": "Neil Chada", "authors": "Neil K. Chada, Ajay Jasra, Fangyuan Yu", "title": "Multilevel Ensemble Kalman-Bucy Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the linear filtering problem in continuous-time.\nWe develop and apply multilevel Monte Carlo (MLMC) strategies for ensemble\nKalman-Bucy filters (EnKBFs). These filters can be viewed as approximations of\nconditional McKean-Vlasov-type diffusion processes. They are also interpreted\nas the continuous-time analogue of the \\textit{ensemble Kalman filter}, which\nhas proven to be successful due to its applicability and computational cost. We\nprove that an ideal version of our multilevel EnKBF can achieve a mean square\nerror (MSE) of $\\mathcal{O}(\\epsilon^2), \\ \\epsilon>0$ with a cost of order\n$\\mathcal{O}(\\epsilon^{-2}\\log(\\epsilon)^2)$. In order to prove this result we\nprovide a Monte Carlo convergence and approximation bounds associated to\ntime-discretized EnKBFs. This implies a reduction in cost compared to the\n(single level) EnKBF which requires a cost of $\\mathcal{O}(\\epsilon^{-3})$ to\nachieve an MSE of $\\mathcal{O}(\\epsilon^2)$. We test our theory on a linear\nproblem, which we motivate through high-dimensional examples of order $\\sim\n\\mathcal{O}(10^4)$ and $\\mathcal{O}(10^5)$.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 11:10:51 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 11:03:49 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chada", "Neil K.", ""], ["Jasra", "Ajay", ""], ["Yu", "Fangyuan", ""]]}, {"id": "2011.04486", "submitter": "Emma Simpson", "authors": "Emma S. Simpson, Thomas Opitz, Jennifer L. Wadsworth", "title": "High-dimensional modeling of spatial and spatio-temporal conditional\n  extremes using INLA and the SPDE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional extremes framework allows for event-based stochastic modeling\nof dependent extremes, and has recently been extended to spatial and\nspatio-temporal settings. After standardizing the marginal distributions and\napplying an appropriate linear normalization, certain non-stationary Gaussian\nprocesses can be used as asymptotically-motivated models for the process\nconditioned on threshold exceedances at a fixed reference location and time. In\nthis work, we adopt a Bayesian perspective by implementing estimation through\nthe integrated nested Laplace approximation (INLA), allowing for novel and\nflexible semi-parametric specifications of the Gaussian mean function. By using\nGauss-Markov approximations of the Mat\\'ern covariance function (known as the\nStochastic Partial Differential Equation approach) at a latent stage of the\nmodel, likelihood-based inference becomes feasible even with thousands of\nobserved locations. We explain how constraints on the spatial and\nspatio-temporal Gaussian processes, arising from the conditioning mechanism,\ncan be implemented through the latent variable approach without losing the\ncomputationally convenient Markov property. We discuss tools for the comparison\nof models via their posterior distributions, and illustrate the flexibility of\nthe approach with gridded Red Sea surface temperature data at over 6,000\nobserved locations. Posterior sampling is exploited to study the probability\ndistribution of cluster functionals of spatial and spatio-temporal extreme\nepisodes.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 15:11:47 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 09:42:27 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Simpson", "Emma S.", ""], ["Opitz", "Thomas", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2011.04532", "submitter": "Louis Raynal", "authors": "Louis Raynal, Sixing Chen, Antonietta Mira, Jukka-Pekka Onnela", "title": "Scalable Approximate Bayesian Computation for Growing Network Models via\n  Extrapolated and Sampled Summaries", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a simulation-based likelihood-free\nmethod applicable to both model selection and parameter estimation. ABC\nparameter estimation requires the ability to forward simulate datasets from a\ncandidate model, but because the sizes of the observed and simulated datasets\nusually need to match, this can be computationally expensive. Additionally,\nsince ABC inference is based on comparisons of summary statistics computed on\nthe observed and simulated data, using computationally expensive summary\nstatistics can lead to further losses in efficiency. ABC has recently been\napplied to the family of mechanistic network models, an area that has\ntraditionally lacked tools for inference and model choice. Mechanistic models\nof network growth repeatedly add nodes to a network until it reaches the size\nof the observed network, which may be of the order of millions of nodes. With\nABC, this process can quickly become computationally prohibitive due to the\nresource intensive nature of network simulations and evaluation of summary\nstatistics. We propose two methodological developments to enable the use of ABC\nfor inference in models for large growing networks. First, to save time needed\nfor forward simulating model realizations, we propose a procedure to\nextrapolate (via both least squares and Gaussian processes) summary statistics\nfrom small to large networks. Second, to reduce computation time for evaluating\nsummary statistics, we use sample-based rather than census-based summary\nstatistics. We show that the ABC posterior obtained through this approach,\nwhich adds two additional layers of approximation to the standard ABC, is\nsimilar to a classic ABC posterior. Although we deal with growing network\nmodels, both extrapolated summaries and sampled summaries are expected to be\nrelevant in other ABC settings where the data are generated incrementally.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 16:13:47 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Raynal", "Louis", ""], ["Chen", "Sixing", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2011.04562", "submitter": "Arnaud Poinas", "authors": "Arnaud Poinas, R\\'emi Bardenet", "title": "On proportional volume sampling for experimental design in general\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal design for linear regression is a fundamental task in statistics. For\nfinite design spaces, recent progress has shown that random designs drawn using\nproportional volume sampling (PVS) lead to approximation guarantees for\nA-optimal design. PVS strikes the balance between design nodes that jointly\nfill the design space, while marginally staying in regions of high mass under\nthe solution of a relaxed convex version of the original problem. In this\npaper, we examine some of the statistical implications of a new variant of PVS\nfor (possibly Bayesian) optimal design. Using point process machinery, we treat\nthe case of a generic Polish design space. We show that not only are the\nA-optimality approximation guarantees preserved, but we obtain similar\nguarantees for D-optimal design that tighten recent results. Moreover, we show\nthat PVS can be sampled in polynomial time. Unfortunately, in spite of its\nelegance and tractability, we demonstrate on a simple example that the\npractical implications of general PVS are likely limited. In the second part of\nthe paper, we focus on applications and investigate the use of PVS as a\nsubroutine for stochastic search heuristics. We demonstrate that PVS is a\nrobust addition to the practitioner's toolbox, especially when the regression\nfunctions are nonstandard and the design space, while low-dimensional, has a\ncomplicated shape (e.g., nonlinear boundaries, several connected components).\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 17:07:26 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:35:55 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Poinas", "Arnaud", ""], ["Bardenet", "R\u00e9mi", ""]]}, {"id": "2011.04829", "submitter": "Philip Greengard", "authors": "Philip Greengard, Andrew Gelman, Aki Vehtari", "title": "A Fast Linear Regression via SVD and Marginalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a numerical scheme for evaluating the posterior moments of\nBayesian linear regression models with partial pooling of the coefficients. The\nprincipal analytical tool of the evaluation is a change of basis from\ncoefficient space to the space of singular vectors of the matrix of predictors.\nAfter this change of basis and an analytical integration, we reduce the problem\nof finding moments of a density over k + m dimensions, to finding moments of an\nm-dimensional density, where k is the number of coefficients and k + m is the\ndimension of the posterior. Moments can then be computed using, for example,\nMCMC, the trapezoid rule, or adaptive Gaussian quadrature. An evaluation of the\nSVD of the matrix of predictors is the dominant computational cost and is\nperformed once during the precomputation stage. We demonstrate numerical\nresults of the algorithm. The scheme described in this paper generalizes\nnaturally to multilevel and multi-group hierarchical regression models where\nnormal-normal parameters appear.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2020 23:30:04 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Greengard", "Philip", ""], ["Gelman", "Andrew", ""], ["Vehtari", "Aki", ""]]}, {"id": "2011.05417", "submitter": "Jonathan Leake", "authors": "Jonathan Leake and Colin S. McSwiggen and Nisheeth K. Vishnoi", "title": "Sampling Matrices from Harish-Chandra-Itzykson-Zuber Densities with\n  Applications to Quantum Inference and Differential Privacy", "comments": "Full version of a paper appearing in STOC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC math.PR math.RT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two $n \\times n$ Hermitian matrices $Y$ and $\\Lambda$, the\nHarish-Chandra-Itzykson-Zuber (HCIZ) distribution on the unitary group\n$\\text{U}(n)$ is $e^{\\text{tr}(U\\Lambda U^*Y)}d\\mu(U)$, where $\\mu$ is the Haar\nmeasure on $\\text{U}(n)$. The density $e^{\\text{tr}(U\\Lambda U^*Y)}$ is known\nas the HCIZ density. Random unitary matrices distributed according to the HCIZ\ndensity are important in various settings in physics and random matrix theory.\nHowever, the basic question of efficient sampling from the HCIZ distribution\nhas remained open. We present two efficient algorithms to sample matrices from\ndistributions that are close to the HCIZ distribution. The first algorithm\noutputs samples that are $\\xi$-close in total variation distance and requires\npolynomially many arithmetic operations in $\\log 1/\\xi$ and the number of bits\nneeded to encode $Y$ and $\\Lambda$. The second algorithm comes with a stronger\nguarantee that the samples are $\\xi$-close in infinity divergence, but the\nnumber of arithmetic operations depends polynomially on $1/\\xi$, the number of\nbits needed to encode $Y$ and $\\Lambda$, and the differences of the largest and\nthe smallest eigenvalues of $Y$ and $\\Lambda$.\n  HCIZ densities can also be viewed as exponential densities on\n$\\text{U}(n)$-orbits, and these densities have been studied in statistics,\nmachine learning, and theoretical computer science. Thus our results have the\nfollowing applications: 1) an efficient algorithm to sample from complex\nversions of matrix Langevin distributions studied in statistics, 2) an\nefficient algorithm to sample from continuous max-entropy distributions on\nunitary orbits, which implies an efficient algorithm to sample a pure quantum\nstate from the entropy-maximizing ensemble representing a given density matrix,\nand 3) an efficient algorithm for differentially private rank-$k$\napproximation, with improved utility bounds for $k>1$.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2020 21:55:16 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:47:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Leake", "Jonathan", ""], ["McSwiggen", "Colin S.", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2011.05988", "submitter": "HaiYing Wang", "authors": "HaiYing Wang and Jae Kwang Kim", "title": "Maximum sampled conditional likelihood for informative subsampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsampling is a computationally effective approach to extract information\nfrom massive data sets when computing resources are limited. After a subsample\nis taken from the full data, most available methods use an inverse probability\nweighted objective function to estimate the model parameters. This type of\nweighted estimator does not fully utilize information in the selected\nsubsample. In this paper, we propose to use the maximum sampled conditional\nlikelihood estimator (MSCLE) based on the sampled data. We established the\nasymptotic normality of the MSCLE and prove that its asymptotic variance\ncovariance matrix is the smallest among a class of asymptotically unbiased\nestimators, including the inverse probability weighted estimator. We further\ndiscuss the asymptotic results with the L-optimal subsampling probabilities and\nillustrate the estimation procedure with generalized linear models. Numerical\nexperiments are provided to evaluate the practical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 16:01:17 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 01:38:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "HaiYing", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2011.06052", "submitter": "Shanyin Tong", "authors": "Shanyin Tong, Anirudh Subramanyam, Vishwas Rao", "title": "Optimization under rare chance constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chance constraints provide a principled framework to mitigate the risk of\nhigh-impact extreme events by modifying the controllable properties of a\nsystem. The low probability and rare occurrence of such events, however, impose\nsevere sampling and computational requirements on classical solution methods\nthat render them impractical. This work proposes a novel sampling-free method\nfor solving rare chance constrained optimization problems affected by\nuncertainties that follow general Gaussian mixture distributions. By\nintegrating modern developments in large deviation theory with tools from\nconvex analysis and bilevel optimization, we propose tractable formulations\nthat can be solved by off-the-shelf solvers. Our formulations enjoy several\nadvantages compared to classical methods: their size and complexity is\nindependent of event rarity, they do not require linearity or convexity\nassumptions on system constraints, and under easily verifiable conditions,\nserve as safe conservative approximations or asymptotically exact\nreformulations of the true problem. Computational experiments on linear,\nnonlinear and PDE-constrained problems from applications in portfolio\nmanagement, structural engineering and fluid dynamics illustrate the broad\napplicability of our method and its advantages over classical sampling-based\napproaches in terms of both accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2020 20:09:50 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 20:59:59 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Tong", "Shanyin", ""], ["Subramanyam", "Anirudh", ""], ["Rao", "Vishwas", ""]]}, {"id": "2011.06427", "submitter": "Sadra Rahimi Kari", "authors": "S. Rahimi Kari", "title": "Realization of Stochastic Neural Networks and Its Potential Applications", "comments": "6 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.NI eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successive Cancellation Decoders have come a long way since the\nimplementation of traditional SC decoders, but there still is a potential for\nimprovement. The main struggle over the years was to find an optimal algorithm\nto implement them. Most of the proposed algorithms are not practical enough to\nbe implemented in real-life. In this research, we aim to introduce the\nEfficiency of stochastic neural networks as an SC decoder and Find the possible\nways of improving its performance and practicality. In this paper, after a\nbrief introduction to stochastic neurons and SNNs, we introduce methods to\nrealize Stochastic NNs on both deterministic and stochastic platforms.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:01:07 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Kari", "S. Rahimi", ""]]}, {"id": "2011.06444", "submitter": "Mario Beraha", "authors": "Mario Beraha, Raffaele Argiento, Jesper M{\\o}ller, Alessandra\n  Guglielmi", "title": "MCMC computations for Bayesian mixture models using repulsive point\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repulsive mixture models have recently gained popularity for Bayesian cluster\ndetection. Compared to more traditional mixture models, repulsive mixture\nmodels produce a smaller number of well separated clusters. The most commonly\nused methods for posterior inference either require to fix a priori the number\nof components or are based on reversible jump MCMC computation. We present a\ngeneral framework for mixture models, when the prior of the `cluster centres'\nis a finite repulsive point process depending on a hyperparameter, specified by\na density which may depend on an intractable normalizing constant. By\ninvestigating the posterior characterization of this class of mixture models,\nwe derive a MCMC algorithm which avoids the well-known difficulties associated\nto reversible jump MCMC computation. In particular, we use an ancillary\nvariable method, which eliminates the problem of having intractable normalizing\nconstants in the Hastings ratio. The ancillary variable method relies on a\nperfect simulation algorithm, and we demonstrate this is fast because the\nnumber of components is typically small. In several simulation studies and an\napplication on sociological data, we illustrate the advantage of our new\nmethodology over existing methods, and we compare the use of a determinantal or\na repulsive Gibbs point process prior model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 15:39:01 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 10:03:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Beraha", "Mario", ""], ["Argiento", "Raffaele", ""], ["M\u00f8ller", "Jesper", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2011.06446", "submitter": "Yueming Lyu", "authors": "Yueming Lyu, Yuan Yuan and Ivor W. Tsang", "title": "Subgroup-based Rank-1 Lattice Quasi-Monte Carlo", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) is an essential tool for integral approximation,\nBayesian inference, and sampling for simulation in science, etc. In the QMC\narea, the rank-1 lattice is important due to its simple operation, and nice\nproperties for point set construction. However, the construction of the\ngenerating vector of the rank-1 lattice is usually time-consuming because of an\nexhaustive computer search. To address this issue, we propose a simple\nclosed-form rank-1 lattice construction method based on group theory. Our\nmethod reduces the number of distinct pairwise distance values to generate a\nmore regular lattice. We theoretically prove a lower and an upper bound of the\nminimum pairwise distance of any non-degenerate rank-1 lattice. Empirically,\nour methods can generate a near-optimal rank-1 lattice compared with the\nKorobov exhaustive search regarding the $l_1$-norm and $l_2$-norm minimum\ndistance. Moreover, experimental results show that our method achieves superior\napproximation performance on benchmark integration test problems and kernel\napproximation problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 03:42:30 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Lyu", "Yueming", ""], ["Yuan", "Yuan", ""], ["Tsang", "Ivor W.", ""]]}, {"id": "2011.06501", "submitter": "Fabien Panloup", "authors": "Piotr Sobczyk, Stanislaw Wilczynski, Malgorzata Bogdan, Piotr Graczyk,\n  Julie Josse, Fabien Panloup, Val\\'erie Seegers, Mateusz Staniak", "title": "VARCLUST: clustering variables using dimensionality reduction", "comments": "24 pages, 34 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  VARCLUST algorithm is proposed for clustering variables under the assumption\nthat variables in a given cluster are linear combinations of a small number of\nhidden latent variables, corrupted by the random noise. The entire clustering\ntask is viewed as the problem of selection of the statistical model, which is\ndefined by the number of clusters, the partition of variables into these\nclusters and the 'cluster dimensions', i.e. the vector of dimensions of linear\nsubspaces spanning each of the clusters. The optimal model is selected using\nthe approximate Bayesian criterion based on the Laplace approximations and\nusing a non-informative uniform prior on the number of clusters. To solve the\nproblem of the search over a huge space of possible models we propose an\nextension of the ClustOfVar algorithm which was dedicated to subspaces of\ndimension only 1, and which is similar in structure to the $K$-centroid\nalgorithm. We provide a complete methodology with theoretical guarantees,\nextensive numerical experimentations, complete data analyses and\nimplementation. Our algorithm assigns variables to appropriate clusterse based\non the consistent Bayesian Information Criterion (BIC), and estimates the\ndimensionality of each cluster by the PEnalized SEmi-integrated Likelihood\nCriterion (PESEL), whose consistency we prove. Additionally, we prove that each\niteration of our algorithm leads to an increase of the Laplace approximation to\nthe model posterior probability and provide the criterion for the estimation of\nthe number of clusters. Numerical comparisons with other algorithms show that\nVARCLUST may outperform some popular machine learning tools for sparse subspace\nclustering. We also report the results of real data analysis including TCGA\nbreast cancer data and meteorological data. The proposed method is implemented\nin the publicly available R package varclust.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 17:11:50 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 15:00:07 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Sobczyk", "Piotr", ""], ["Wilczynski", "Stanislaw", ""], ["Bogdan", "Malgorzata", ""], ["Graczyk", "Piotr", ""], ["Josse", "Julie", ""], ["Panloup", "Fabien", ""], ["Seegers", "Val\u00e9rie", ""], ["Staniak", "Mateusz", ""]]}, {"id": "2011.06682", "submitter": "Sanjeena Subedi", "authors": "Yuan Fang and Sanjeena Subedi", "title": "Clustering microbiome data using mixtures of logistic normal multinomial\n  models", "comments": "51 pages, 11 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discrete data such as counts of microbiome taxa resulting from\nnext-generation sequencing are routinely encountered in bioinformatics. Taxa\ncount data in microbiome studies are typically high-dimensional,\nover-dispersed, and can only reveal relative abundance therefore being treated\nas compositional. Analyzing compositional data presents many challenges because\nthey are restricted on a simplex. In a logistic normal multinomial model, the\nrelative abundance is mapped from a simplex to a latent variable that exists on\nthe real Euclidean space using the additive log-ratio transformation. While a\nlogistic normal multinomial approach brings in flexibility for modeling the\ndata, it comes with a heavy computational cost as the parameter estimation\ntypically relies on Bayesian techniques. In this paper, we develop a novel\nmixture of logistic normal multinomial models for clustering microbiome data.\nAdditionally, we utilize an efficient framework for parameter estimation using\nvariational Gaussian approximations (VGA). Adopting a variational Gaussian\napproximation for the posterior of the latent variable reduces the\ncomputational overhead substantially. The proposed method is illustrated on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 23:16:24 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Fang", "Yuan", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "2011.06879", "submitter": "Jacob Leander", "authors": "Jacob Leander, Joachim Almquist, Anna Johnning, Julia Larsson, Mats\n  Jirstrand", "title": "NLMEModeling: A Wolfram Mathematica Package for Nonlinear Mixed Effects\n  Modeling of Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear mixed effects modeling is a powerful tool when analyzing data from\nseveral entities in an experiment. In this paper, we present NLMEModeling, a\npackage for mixed effects modeling in Wolfram Mathematica. NLMEModeling\nsupports mixed effects modeling of dynamical systems where the underlying\ndynamics are described by either ordinary or stochastic differential equations\ncombined with a flexible observation error model. Moreover, NLMEModeling is a\nuser-friendly package with functionality for model validation, visual\npredictive checks and simulation capabilities. The package is freely available\nand provides a flexible add-on to Wolfram Mathematica.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 12:28:45 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Leander", "Jacob", ""], ["Almquist", "Joachim", ""], ["Johnning", "Anna", ""], ["Larsson", "Julia", ""], ["Jirstrand", "Mats", ""]]}, {"id": "2011.06898", "submitter": "Gregor Zens", "authors": "Sylvia Fr\\\"uhwirth-Schnatter, Gregor Zens, Helga Wagner", "title": "Ultimate P\\'olya Gamma Samplers -- Efficient MCMC for possibly\n  imbalanced binary and categorical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Modeling binary and categorical data is one of the most commonly encountered\ntasks of applied statisticians and econometricians. While Bayesian methods in\nthis context have been available for decades now, they often require a high\nlevel of familiarity with Bayesian statistics or suffer from issues such as low\nsampling efficiency. To contribute to the accessibility of Bayesian models for\nbinary and categorical data, we introduce novel latent variable representations\nbased on P\\'olya Gamma random variables for a range of commonly encountered\ndiscrete choice models. From these latent variable representations, new Gibbs\nsampling algorithms for binary, binomial and multinomial logistic regression\nmodels are derived. All models allow for a conditionally Gaussian likelihood\nrepresentation, rendering extensions to more complex modeling frameworks such\nas state space models straight-forward. However, sampling efficiency may still\nbe an issue in these data augmentation based estimation frameworks. To\ncounteract this, MCMC boosting strategies are developed and discussed in\ndetail. The merits of our approach are illustrated through extensive\nsimulations and a real data application.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2020 13:29:53 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 06:45:57 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 04:14:53 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Fr\u00fchwirth-Schnatter", "Sylvia", ""], ["Zens", "Gregor", ""], ["Wagner", "Helga", ""]]}, {"id": "2011.07559", "submitter": "Mehrdad Naderi Dr", "authors": "Mehrdad Naderi, Elham Mirfarah, Matthew Bernhardt and Ding-Geng Chen", "title": "Semiparametric inference for the scale-mixture of normal partial linear\n  regression model with censored data", "comments": "17 page, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the framework of censored data modeling, the classical linear regression\nmodel that assumes normally distributed random errors has received increasing\nattention in recent years, mainly for mathematical and computational\nconvenience. However, practical studies have often criticized this linear\nregression model due to its sensitivity to departure from the normality and\nfrom the partial nonlinearity. This paper proposes to solve these potential\nissues simultaneously in the context of the partial linear regression model by\nassuming that the random errors follow a scale-mixture of normal (SMN) family\nof distributions. The proposed method allows us to model data with great\nflexibility, accommodating heavy tails, and outliers. By implementing the\nB-spline function and using the convenient hierarchical representation of the\nSMN distributions, a computationally analytical EM-type algorithm is developed\nto perform maximum likelihood inference of the model parameters. Various\nsimulation studies are conducted to investigate the finite sample properties as\nwell as the robustness of the model in dealing with the heavy-tails distributed\ndatasets. Real-word data examples are finally analyzed for illustrating the\nusefulness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 15:38:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Naderi", "Mehrdad", ""], ["Mirfarah", "Elham", ""], ["Bernhardt", "Matthew", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2011.07579", "submitter": "Daniel Romero", "authors": "Daniel Romero, Siavash Mollaebrahim, Baltasar Beferull-Lozano, C\\'esar\n  Asensio-Marco", "title": "Fast Graph Filters for Decentralized Subspace Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of inference problems with sensor networks involve projecting a\nmeasured signal onto a given subspace. In existing decentralized approaches,\nsensors communicate with their local neighbors to obtain a sequence of iterates\nthat asymptotically converges to the desired projection. In contrast, the\npresent paper develops methods that produce these projections in a finite and\napproximately minimal number of iterations. Building upon tools from graph\nsignal processing, the problem is cast as the design of a graph filter which,\nin turn, is reduced to the design of a suitable graph shift operator.\nExploiting the eigenstructure of the projection and shift matrices leads to an\nobjective whose minimization yields approximately minimum-order graph filters.\nTo cope with the fact that this problem is not convex, the present work\nintroduces a novel convex relaxation of the number of distinct eigenvalues of a\nmatrix based on the nuclear norm of a Kronecker difference. To tackle the case\nwhere there exists no graph filter capable of implementing a certain subspace\nprojection with a given network topology, a second optimization criterion is\npresented to approximate the desired projection while trading the number of\niterations for approximation error. Two algorithms are proposed to optimize the\naforementioned criteria based on the alternating-direction method of\nmultipliers. An exhaustive simulation study demonstrates that the obtained\nfilters can effectively obtain subspace projections markedly faster than\nexisting algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 16:56:41 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Romero", "Daniel", ""], ["Mollaebrahim", "Siavash", ""], ["Beferull-Lozano", "Baltasar", ""], ["Asensio-Marco", "C\u00e9sar", ""]]}, {"id": "2011.07614", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "A Picture's Worth a Thousand Words: Visualizing n-dimensional Overlap in\n  Logistic Regression Models with Empirical Likelihood", "comments": "Contains 1 pdf file consisting of 22 pages, 12 tables, 10 figures,\n  and 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this note, conditions for the existence and uniqueness of the maximum\nlikelihood estimate for multidimensional predictor, binary response models are\nintroduced from a sensitivity testing point of view. The well known condition\nof Silvapulle is translated to be an empirical likelihood maximization which,\nwith existing R code, mechanizes the process of assessing overlap status. The\ntranslation shifts the meaning of overlap, defined by geometrical properties of\nthe two-predictor groups, from the intersection of their convex cones is\nnon-empty to the more understandable requirement that the convex hull of their\ndifferences contains zero. The code is applied to reveal the character of\noverlap by examining minimal overlapping structures and cataloging them in\ndimensions fewer than four. Rules to generate minimal higher dimensional\nstructures which account for overlap are provided. Supplementary materials are\navailable online.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2020 19:39:56 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.07721", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri and Subhroshekhar Ghosh and David J. Nott and Kim Cuc\n  Pham", "title": "On a Variational Approximation based Empirical Likelihood ABC Method", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.01675", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientifically well-motivated statistical models in natural,\nengineering, and environmental sciences are specified through a generative\nprocess. However, in some cases, it may not be possible to write down the\nlikelihood for these models analytically. Approximate Bayesian computation\n(ABC) methods allow Bayesian inference in such situations. The procedures are\nnonetheless typically computationally intensive. Recently, computationally\nattractive empirical likelihood-based ABC methods have been suggested in the\nliterature. All of these methods rely on the availability of several suitable\nanalytically tractable estimating equations, and this is sometimes problematic.\nWe propose an easy-to-use empirical likelihood ABC method in this article.\nFirst, by using a variational approximation argument as a motivation, we show\nthat the target log-posterior can be approximated as a sum of an expected joint\nlog-likelihood and the differential entropy of the data generating density. The\nexpected log-likelihood is then estimated by an empirical likelihood where the\nonly inputs required are a choice of summary statistic, it's observed value,\nand the ability to simulate the chosen summary statistics for any parameter\nvalue under the model. The differential entropy is estimated from the simulated\nsummaries using traditional methods. Posterior consistency is established for\nthe method, and we discuss the bounds for the required number of simulated\nsummaries in detail. The performance of the proposed method is explored in\nvarious examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 21:24:26 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Chaudhuri", "Sanjay", ""], ["Ghosh", "Subhroshekhar", ""], ["Nott", "David J.", ""], ["Pham", "Kim Cuc", ""]]}, {"id": "2011.07866", "submitter": "Benjamin Guedj", "authors": "Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey", "title": "Cluster-Specific Predictions with Multi-Task Gaussian Processes", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A model involving Gaussian processes (GPs) is introduced to simultaneously\nhandle multi-task learning, clustering, and prediction for multiple functional\ndata. This procedure acts as a model-based clustering method for functional\ndata as well as a learning step for subsequent predictions for new tasks. The\nmodel is instantiated as a mixture of multi-task GPs with common mean\nprocesses. A variational EM algorithm is derived for dealing with the\noptimisation of the hyper-parameters along with the hyper-posteriors'\nestimation of latent variables and processes. We establish explicit formulas\nfor integrating the mean processes and the latent clustering variables within a\npredictive distribution, accounting for uncertainty on both aspects. This\ndistribution is defined as a mixture of cluster-specific GP predictions, which\nenhances the performances when dealing with group-structured data. The model\nhandles irregular grid of observations and offers different hypotheses on the\ncovariance structure for sharing additional information across tasks. The\nperformances on both clustering and prediction tasks are assessed through\nvarious simulated scenarios and real datasets. The overall algorithm, called\nMagmaClust, is publicly available as an R package.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:08:59 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 13:45:02 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Leroy", "Arthur", ""], ["Latouche", "Pierre", ""], ["Guedj", "Benjamin", ""], ["Gey", "Servane", ""]]}, {"id": "2011.07913", "submitter": "Paula Saavedra-Nieves", "authors": "A. Saavedra-Nieves and P. Saavedra-Nieves", "title": "On systems of quotas based on bankruptcy with a priori unions:\n  estimating random arrival-style rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper addresses a sampling procedure for estimating extensions of the\nrandom arrival rule to those bankruptcy situations where there exist a priori\nunions. It is based on simple random sampling with replacement and it adapts an\nestimation method of the Owen value for transferable utility games with a\npriori unions, especially useful when the set of involved agents is\nsufficiently large. We analyse the theoretical statistical properties of the\nresulting estimator as well as we provide some bounds for the incurred error.\nIts performance is evaluated on two well-studied examples in literature where\nthis allocation rule can be exactly obtained. Finally, we apply this sampling\nmethod to provide a new quota system for the milk market in Galicia (Spain) to\ncheck the role of different territorial structures when they are taken as a\npriori unions. The resulting quotas estimator is also compared with two\nclassical rules in bankruptcy literature.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 13:00:50 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Saavedra-Nieves", "A.", ""], ["Saavedra-Nieves", "P.", ""]]}, {"id": "2011.08282", "submitter": "Deepak Nag Ayyala", "authors": "Deepak Nag Ayyala, Santu Ghosh and Daniel F. Linder", "title": "Covariance matrix testing in high dimension using random projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation and hypothesis tests for the covariance matrix in high dimensions\nis a challenging problem as the traditional multivariate asymptotic theory is\nno longer valid. When the dimension is larger than or increasing with the\nsample size, standard likelihood based tests for the covariance matrix have\npoor performance. Existing high dimensional tests are either computationally\nexpensive or have very weak control of type I error. In this paper, we propose\na test procedure, CRAMP, for testing hypotheses involving one or more\ncovariance matrices using random projections. Projecting the high dimensional\ndata randomly into lower dimensional subspaces alleviates of the curse of\ndimensionality, allowing for the use of traditional multivariate tests. An\nextensive simulation study is performed to compare CRAMP against\nasymptotics-based high dimensional test procedures. An application of the\nproposed method to two gene expression data sets is presented.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 21:15:14 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ayyala", "Deepak Nag", ""], ["Ghosh", "Santu", ""], ["Linder", "Daniel F.", ""]]}, {"id": "2011.08360", "submitter": "Yuetian Luo", "authors": "Yuetian Luo, Wen Huang, Xudong Li, Anru R. Zhang", "title": "Recursive Importance Sketching for Rank Constrained Least Squares:\n  Algorithms and High-order Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new {\\it \\underline{R}ecursive} {\\it\n\\underline{I}mportance} {\\it \\underline{S}ketching} algorithm for {\\it\n\\underline{R}ank} constrained least squares {\\it \\underline{O}ptimization}\n(RISRO). As its name suggests, the algorithm is based on a new sketching\nframework, recursive importance sketching. Several existing algorithms in the\nliterature can be reinterpreted under the new sketching framework and RISRO\noffers clear advantages over them. RISRO is easy to implement and\ncomputationally efficient, where the core procedure in each iteration is only\nsolving a dimension reduced least squares problem. Different from numerous\nexisting algorithms with locally geometric convergence rate, we establish the\nlocal quadratic-linear and quadratic rate of convergence for RISRO under some\nmild conditions. In addition, we discover a deep connection of RISRO to\nRiemannian manifold optimization on fixed rank matrices. The effectiveness of\nRISRO is demonstrated in two applications in machine learning and statistics:\nlow-rank matrix trace regression and phase retrieval. Simulation studies\ndemonstrate the superior numerical performance of RISRO.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 01:32:59 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 00:30:34 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luo", "Yuetian", ""], ["Huang", "Wen", ""], ["Li", "Xudong", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2011.08417", "submitter": "Pedro Ramos", "authors": "Pedro L. Ramos, Daniel C. F. Guzman, Alex L. Mota, Francisco A.\n  Rodrigues, Francisco Louzada", "title": "Sampling with censored data: a practical guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this review, we present a simple guide for researchers to obtain\npseudo-random samples with censored data. We focus our attention on the most\ncommon types of censored data, such as type I, type II, and random censoring.\nWe discussed the necessary steps to sample pseudo-random values from long-term\nsurvival models where an additional cure fraction is informed. For illustrative\npurposes, these techniques are applied in the Weibull distribution. The\nalgorithms and codes in R are presented, enabling the reproducibility of our\nstudy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 04:33:49 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Guzman", "Daniel C. F.", ""], ["Mota", "Alex L.", ""], ["Rodrigues", "Francisco A.", ""], ["Louzada", "Francisco", ""]]}, {"id": "2011.08578", "submitter": "Jakiw Pidstrigach", "authors": "Jakiw Pidstrigach", "title": "Convergence of Preconditioned Hamiltonian Monte Carlo on Hilbert Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the preconditioned Hamiltonian Monte Carlo\n(pHMC) algorithm defined directly on an infinite-dimensional Hilbert space. In\nthis context, and under a condition reminiscent of strong log-concavity of the\ntarget measure, we prove convergence bounds for adjusted pHMC in the standard\n1-Wasserstein distance. The arguments rely on a synchronous coupling of two\ncopies of pHMC, which is controlled by adapting elements from arXiv:1805.00452.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 11:53:29 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Pidstrigach", "Jakiw", ""]]}, {"id": "2011.08644", "submitter": "Sebastian Schmon", "authors": "Sebastian M Schmon, Patrick W Cannon, Jeremias Knoblauch", "title": "Generalized Posteriors in Approximate Bayesian Computation", "comments": "Accepted at Advances in Approximate Bayesian Inference, AABI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex simulators have become a ubiquitous tool in many scientific\ndisciplines, providing high-fidelity, implicit probabilistic models of natural\nand social phenomena. Unfortunately, they typically lack the tractability\nrequired for conventional statistical analysis. Approximate Bayesian\ncomputation (ABC) has emerged as a key method in simulation-based inference,\nwherein the true model likelihood and posterior are approximated using samples\nfrom the simulator. In this paper, we draw connections between ABC and\ngeneralized Bayesian inference (GBI). First, we re-interpret the accept/reject\nstep in ABC as an implicitly defined error model. We then argue that these\nimplicit error models will invariably be misspecified. While ABC posteriors are\noften treated as a necessary evil for approximating the standard Bayesian\nposterior, this allows us to re-interpret ABC as a potential robustification\nstrategy. This leads us to suggest the use of GBI within ABC, a use case we\nexplore empirically.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 14:08:59 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 18:16:36 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Cannon", "Patrick W", ""], ["Knoblauch", "Jeremias", ""]]}, {"id": "2011.08922", "submitter": "Rados{\\l}aw Kycia", "authors": "Agnieszka Niemczynowicz, Gabriela Bia{\\l}osk\\'orska, Joanna\n  Nie\\.zurawska-Zaj\\k{a}c, Rados{\\l}aw A. Kycia", "title": "TreeGen -- a Monte Carlo generator for data frames", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The typical problem in Data Science is creating a structure that encodes the\noccurrence frequency of unique elements in rows and relations between different\nrows of a data frame. We present the probability tree abstract data structure,\nan extension of the decision tree, that facilitates more than two choices with\nassigned probabilities. Such a tree represents statistical relations between\ndifferent rows of the data frame. The Probability Tree algorithmic structure is\nsupplied with the Generator module that is a Monte Carlo generator that\ntraverses through the tree. These two components are implemented in TreeGen\nPython package. The package can be used in increasing data multiplicity,\ncompressing data preserving its statistical information, constructing\nhierarchical models, exploring data, and in feature extraction.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2020 20:22:15 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Niemczynowicz", "Agnieszka", ""], ["Bia\u0142osk\u00f3rska", "Gabriela", ""], ["Nie\u017curawska-Zaj\u0105c", "Joanna", ""], ["Kycia", "Rados\u0142aw A.", ""]]}, {"id": "2011.09317", "submitter": "Nicola Branchini", "authors": "Nicola Branchini and V\\'ictor Elvira", "title": "Optimized Auxiliary Particle Filters: adapting mixture proposals via\n  convex optimization", "comments": "Accepted version at Uncertainty in Artificial Intelligence (UAI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auxiliary particle filters (APFs) are a class of sequential Monte Carlo (SMC)\nmethods for Bayesian inference in state-space models. In their original\nderivation, APFs operate in an extended state space using an auxiliary variable\nto improve inference. In this work, we propose optimized auxiliary particle\nfilters, a framework where the traditional APF auxiliary variables are\ninterpreted as weights in an importance sampling mixture proposal. Under this\ninterpretation, we devise a mechanism for proposing the mixture weights that is\ninspired by recent advances in multiple and adaptive importance sampling. In\nparticular, we propose to select the mixture weights by formulating a convex\noptimization problem, with the aim of approximating the filtering posterior at\neach timestep. Further, we propose a weighting scheme that generalizes previous\nresults on the APF (Pitt et al. 2012), proving unbiasedness and consistency of\nour estimators. Our framework demonstrates significantly improved estimates on\na range of metrics compared to state-of-the-art particle filters at similar\ncomputational complexity in challenging and widely used dynamical models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 14:38:02 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 13:08:56 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Branchini", "Nicola", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "2011.09341", "submitter": "Andi Wang", "authors": "Christophe Andrieu, Paul Dobson, Andi Q. Wang", "title": "Subgeometric hypocoercivity for piecewise-deterministic Markov process\n  Monte Carlo methods", "comments": "33 pages, 1 figure. Minor revisions made", "journal-ref": "Electron. J. Probab. 26 1 - 26, 2021", "doi": "10.1214/21-EJP643", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the hypocoercivity framework for piecewise-deterministic Markov\nprocess (PDMP) Monte Carlo established in [Andrieu et. al. (2018)] to\nheavy-tailed target distributions, which exhibit subgeometric rates of\nconvergence to equilibrium. We make use of weak Poincar\\'e inequalities, as\ndeveloped in the work of [Grothaus and Wang (2019)], the ideas of which we\nadapt to the PDMPs of interest. On the way we report largely\npotential-independent approaches to bounding explicitly solutions of the\nPoisson equation of the Langevin diffusion and its first and second\nderivatives, required here to control various terms arising in the application\nof the hypocoercivity result.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 15:28:00 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:05:13 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Andrieu", "Christophe", ""], ["Dobson", "Paul", ""], ["Wang", "Andi Q.", ""]]}, {"id": "2011.09544", "submitter": "Alexander Foss", "authors": "Alexander H. Foss, Richard B. Lehoucq, W. Zachary Stuart, J. Derek\n  Tucker, Jonathan W. Berry", "title": "A Deterministic Hitting-Time Moment Approach to Seed-set Expansion over\n  a Graph", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce HITMIX, a new technique for network seed-set expansion, i.e.,\nthe problem of identifying a set of graph vertices related to a given seed-set\nof vertices. We use the moments of the graph's hitting-time distribution to\nquantify the relationship of each non-seed vertex to the seed-set. This\ninvolves a deterministic calculation for the hitting-time moments that is\nscalable in the number of graph edges and so avoids directly sampling a Markov\nchain over the graph. The moments are used to fit a mixture model to estimate\nthe probability that each non-seed vertex should be grouped with the seed set.\nThis membership probability enables us to sort the non-seeds and threshold in a\nstatistically-justified way. To the best of our knowledge, HITMIX is the first\nfull statistical model for seed-set expansion that can give vertex-level\nmembership probabilities. While HITMIX is a global method, its linear\ncomputation complexity in practice enables computations on large graphs. We\nhave a high-performance implementation, and we present computational results on\nstochastic blockmodels and a small-world network from the SNAP repository. The\nstate of the art in this problem is a collection of recently developed local\nmethods, and we show that distinct advantages in solution quality are available\nif our global method can be used. In practice, we expect to be able to run\nHITMIX if the graph can be stored in memory.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 20:54:10 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Foss", "Alexander H.", ""], ["Lehoucq", "Richard B.", ""], ["Stuart", "W. Zachary", ""], ["Tucker", "J. Derek", ""], ["Berry", "Jonathan W.", ""]]}, {"id": "2011.09549", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "The Pearson Bayes factor: An analytic formula for computing evidential\n  value from minimal summary statistics", "comments": "Accepted for publication in Biometrical Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In Bayesian hypothesis testing, evidence for a statistical model is\nquantified by the Bayes factor, which represents the relative likelihood of\nobserved data under that model compared to another competing model. In general,\ncomputing Bayes factors is difficult, as computing the marginal likelihood of\ndata under a given model requires integrating over a prior distribution of\nmodel parameters. In this paper, I capitalize on a particular choice of prior\ndistribution that allows the Bayes factor to be expressed without integral\nrepresentation and I develop a simple formula -- the Pearson Bayes factor --\nthat requires only minimal summary statistics commonly reported in scientific\npapers, such as the $t$ or $F$ score and the degrees of freedom. In addition to\npresenting this new result, I provide several examples of its use and report a\nsimulation study validating its performance. Importantly, the Pearson Bayes\nfactor gives applied researchers the ability to compute exact Bayes factors\nfrom minimal summary data, and thus easily assess the evidential value of any\ndata for which these summary statistics are provided, even when the original\ndata is not available.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2020 21:17:25 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 11:29:52 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "2011.09638", "submitter": "Genshiro Kitagawa", "authors": "G. Kitagawa (The University of Tokyo and Meiji University)", "title": "Computation of the Gradient and the Hessian of the Log-likelihood of the\n  State-space Model by the Kalman Filter", "comments": "15 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": "Meiji University MIMS-RPB Statistics & Data Science Series (SDS-17)", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood estimates of an ARMA model can be obtained by the\nKalman filter based on the state-space representation of the model. This paper\npresents an algorithm for computing gradient of the log-likelihood by an\nextending the Kalman filter without resorting to the numerical difference.\nThree examples of seasonal adjustment model and ARMA model are presented to\nexemplified the specification of structural matrices and initial matrices. An\nextension of the algorithm to compute the Hessian matrix is also shown.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 04:04:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Kitagawa", "G.", "", "The University of Tokyo and Meiji University"]]}, {"id": "2011.09680", "submitter": "Michael Choi", "authors": "Michael C.H. Choi", "title": "On the convergence of an improved discrete simulated annealing via\n  landscape modification", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.OC math.PR stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose new Metropolis-Hastings and simulated annealing\nalgorithms on finite state space via modifying the energy landscape. The core\nidea of landscape modification relies on introducing a parameter $c$, in which\nthe landscape is modified once the algorithm is above this threshold parameter.\nWe illustrate the power and benefits of landscape modification by investigating\nits effect on the classical Curie-Weiss model with Glauber dynamics and\nexternal magnetic field in the subcritical regime. This leads to a\nlandscape-modified mean-field equation, and with appropriate choice of $c$ the\nfree energy landscape can be transformed from a double-well into a single-well,\nwhile the location of the global minimum is preserved on the modified\nlandscape. Consequently, running algorithms on the modified landscape can\nimprove the convergence to the ground-state in the Curie-Weiss model. In the\nsetting of simulated annealing, we demonstrate that landscape modification can\nyield improved mean tunneling time between global minima, and give convergence\nguarantee using an improved logarithmic cooling schedule with reduced critical\nheight. Finally, we discuss connections between landscape modification and\nother acceleration techniques such as Catoni's energy transformation algorithm,\npreconditioning, importance sampling and quantum annealing. We stress that the\ntechnique developed in this paper is applicable to any difference-based\ndiscrete optimization algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 06:06:38 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:11:58 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Choi", "Michael C. H.", ""]]}, {"id": "2011.09682", "submitter": "Sabrina Enriquez", "authors": "Sabrina Enriquez, Fushing Hsieh", "title": "Categorical exploratory data analysis on goodness-of-fit issues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If the aphorism \"All models are wrong\"- George Box, continues to be true in\ndata analysis, particularly when analyzing real-world data, then we should\nannotate this wisdom with visible and explainable data-driven patterns. Such\nannotations can critically shed invaluable light on validity as well as\nlimitations of statistical modeling as a data analysis approach. In an effort\nto avoid holding our real data to potentially unattainable or even unrealistic\ntheoretical structures, we propose to utilize the data analysis paradigm called\nCategorical Exploratory Data Analysis (CEDA). We illustrate the merits of this\nproposal with two real-world data sets from the perspective of goodness-of-fit.\nIn both data sets, the Normal distribution's bell shape seemingly fits rather\nwell by first glance. We apply CEDA to bring out where and how each data fits\nor deviates from the model shape via several important distributional aspects.\nWe also demonstrate that CEDA affords a version of tree-based p-value, and\ncompare it with p-values based on traditional statistical approaches. Along our\ndata analysis, we invest computational efforts in making graphic display to\nilluminate the advantages of using CEDA as one primary way of data analysis in\nData Science education.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 06:11:06 GMT"}, {"version": "v2", "created": "Fri, 4 Dec 2020 01:41:15 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Enriquez", "Sabrina", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2011.09833", "submitter": "Sowmya Chandrasekaran", "authors": "Sowmya Chandrasekaran, Margarita Rebolledo, Thomas Bartz-Beielstein", "title": "EventDetectR -- An Open-Source Event Detection System", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  EventDetectR: An efficient Event Detection System (EDS) capable of detecting\nunexpected water quality conditions. This approach uses multiple algorithms to\nmodel the relationship between various multivariate water quality signals. Then\nthe residuals of the models were utilized in constructing the event detection\nalgorithm, which provides a continuous measure of the probability of an event\nat every time step. The proposed framework was tested for water contamination\nevents with industrial data from automated water quality sensors. The results\nshowed that the framework is reliable with better performance and is highly\nsuitable for event detection.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2020 11:26:17 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chandrasekaran", "Sowmya", ""], ["Rebolledo", "Margarita", ""], ["Bartz-Beielstein", "Thomas", ""]]}, {"id": "2011.09853", "submitter": "Hamed Majidifard", "authors": "Hamed Majidifard, Behnam Jahangiri, Punyaslok Rath, Amir H. Alavi,\n  William G. Buttlar", "title": "A Deep Learning Approach to Predict Hamburg Rutting Curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rutting continues to be one of the principal distresses in asphalt pavements\nworldwide. This type of distress is caused by permanent deformation and shear\nfailure of the asphalt mix under the repetition of heavy loads. The Hamburg\nwheel tracking test (HWTT) is a widely used testing procedure designed to\naccelerate, and to simulate the rutting phenomena in the laboratory. Rut depth,\nas one of the outputs of the HWTT, is dependent on a number of parameters\nrelated to mix design and testing conditions. This study introduces a new model\nfor predicting the rutting depth of asphalt mixtures using a deep learning\ntechnique - the convolution neural network (CNN). A database containing a\ncomprehensive collection of HWTT results was used to develop a CNN-based\nmachine learning prediction model. The database includes 10,000 rutting depth\ndata points measured across a large variety of asphalt mixtures. The model has\nbeen formulated in terms of known influencing mixture variables such as asphalt\nbinder high temperature performance grade, mixture type, aggregate size,\naggregate gradation, asphalt content, total asphalt binder recycling content,\nand testing parameters, including testing temperature and number of wheel\npasses. A rigorous validation process was used to assess the accuracy of the\nmodel to predict total rut depth and the HWTT rutting curve. A sensitivity\nanalysis is presented, which evaluates the effect of the investigated variables\non rutting depth predictions by the CNN model. The model can be used as a tool\nto estimate the rut depth in asphalt mixtures when laboratory testing is not\nfeasible, or for cost saving, pre-design trials.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2020 22:10:54 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Majidifard", "Hamed", ""], ["Jahangiri", "Behnam", ""], ["Rath", "Punyaslok", ""], ["Alavi", "Amir H.", ""], ["Buttlar", "William G.", ""]]}, {"id": "2011.09985", "submitter": "Peng Chen", "authors": "Peng Chen and Omar Ghattas", "title": "Taylor approximation for chance constrained optimization problems\n  governed by partial differential equations with high-dimensional random\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a fast and scalable optimization method to solve chance or\nprobabilistic constrained optimization problems governed by partial\ndifferential equations (PDEs) with high-dimensional random parameters. To\naddress the critical computational challenges of expensive PDE solution and\nhigh-dimensional uncertainty, we construct surrogates of the constraint\nfunction by Taylor approximation, which relies on efficient computation of the\nderivatives, low rank approximation of the Hessian, and a randomized algorithm\nfor eigenvalue decomposition. To tackle the difficulty of the\nnon-differentiability of the inequality chance constraint, we use a smooth\napproximation of the discontinuous indicator function involved in the chance\nconstraint, and apply a penalty method to transform the inequality constrained\noptimization problem to an unconstrained one. Moreover, we design a\ngradient-based optimization scheme that gradually increases smoothing and\npenalty parameters to achieve convergence, for which we present an efficient\ncomputation of the gradient of the approximate cost functional by the Taylor\napproximation. Based on numerical experiments for a problem in optimal\ngroundwater management, we demonstrate the accuracy of the Taylor\napproximation, its ability to greatly accelerate constraint evaluations, the\nconvergence of the continuation optimization scheme, and the scalability of the\nproposed method in terms of the number of PDE solves with increasing random\nparameter dimension from one thousand to hundreds of thousands.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 17:36:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Chen", "Peng", ""], ["Ghattas", "Omar", ""]]}, {"id": "2011.10088", "submitter": "Ben Swallow", "authors": "Giada Sacchi and Ben Swallow", "title": "Parallel tempering as a mechanism for facilitating inference in\n  hierarchical hidden Markov models", "comments": null, "journal-ref": null, "doi": "10.3389/fevo.2021.623731", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The study of animal behavioural states inferred through hidden Markov models\nand similar state switching models has seen a significant increase in\npopularity in recent years. The ability to account for varying levels of\nbehavioural scale has become possible through hierarchical hidden Markov\nmodels, but additional levels lead to higher complexity and increased\ncorrelation between model components. Maximum likelihood approaches to\ninference using the EM algorithm and direct optimisation of likelihoods are\nmore frequently used, with Bayesian approaches being less favoured due to\ncomputational demands. Given these demands, it is vital that efficient\nestimation algorithms are developed when Bayesian methods are preferred. We\nstudy the use of various approaches to improve convergence times and mixing in\nMarkov chain Monte Carlo methods applied to hierarchical hidden Markov models,\nincluding parallel tempering as an inference facilitation mechanism. The method\nshows promise for analysing complex stochastic models with high levels of\ncorrelation between components, but our results show that it requires careful\ntuning in order to maximise that potential.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 20:08:56 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 09:22:10 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Sacchi", "Giada", ""], ["Swallow", "Ben", ""]]}, {"id": "2011.10529", "submitter": "Ali Abdi", "authors": "Iman Habibi, Effat S Emamian, Osvaldo Simeone and Ali Abdi", "title": "Computation capacities of a broad class of signaling networks are higher\n  than their communication capacities", "comments": "51 pages, 8 figures", "journal-ref": "Phys. Biol. 16 064001 (2019)", "doi": "10.1088/1478-3975/ab4345", "report-no": null, "categories": "q-bio.MN cs.IT math.IT q-bio.SC stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Due to structural and functional abnormalities or genetic variations and\nmutations, there may be dysfunctional molecules within an intracellular\nsignaling network that do not allow the network to correctly regulate its\noutput molecules, such as transcription factors. This disruption in signaling\ninterrupts normal cellular functions and may eventually develop some\npathological conditions. In this paper, computation capacity of signaling\nnetworks is introduced as a fundamental limit on signaling capability and\nperformance of such networks. The computation capacity measures the maximum\nnumber of computable inputs, that is, the maximum number of input values for\nwhich the correct functional output values can be recovered from the erroneous\nnetwork outputs, when the network contains some dysfunctional molecules. This\ncontrasts with the conventional communication capacity that measures instead\nthe maximum number of input values that can be correctly distinguished based on\nthe erroneous network outputs.\n  The computation capacity is higher than the communication capacity, if the\nnetwork response function is not a one-to-one function of the input signals. By\nexplicitly incorporating the effect of signaling errors that result in the\nnetwork dysfunction, the computation capacity provides more information about\nthe network and its malfunction. Two examples of signaling networks are studied\nhere, one regulating caspase3 and another regulating NFkB, for which\ncomputation and communication capacities are analyzed. Higher computation\ncapacities are observed for both networks. One biological implication of this\nfinding is that signaling networks may have more capacity than that specified\nby the conventional communication capacity metric. The effect of feedback is\nalso studied. In summary, this paper reports findings on a new fundamental\nfeature of the signaling capability of cell signaling networks.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:59:02 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Habibi", "Iman", ""], ["Emamian", "Effat S", ""], ["Simeone", "Osvaldo", ""], ["Abdi", "Ali", ""]]}, {"id": "2011.10589", "submitter": "Tony Pourmohamad", "authors": "Tony Pourmohamad", "title": "CompModels: A suite of computer model test functions for Bayesian\n  optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CompModels package for R provides a suite of computer model test\nfunctions that can be used for computer model prediction/emulation, uncertainty\nquantification, and calibration, but in particular, the sequential optimization\nof computer models. The package is a mix of real-world physics problems, known\nmathematical functions, and black-box functions that have been converted into\ncomputer models with the goal of Bayesian (i.e., sequential) optimization in\nmind. Likewise, the package contains computer models that represent either the\nconstrained or unconstrained optimization case, each with varying levels of\ndifficulty. In this paper, we illustrate the use of the package with both\nreal-world examples and black-box functions by solving constrained optimization\nproblems via Bayesian optimization. Ultimately, the package is shown to provide\nusers with a source of computer model test functions that are reproducible,\nshareable, and that can be used for benchmarking of novel optimization methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 19:00:30 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 16:16:54 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Pourmohamad", "Tony", ""]]}, {"id": "2011.10715", "submitter": "Kyungmin Kim", "authors": "Seungjae Jung, Kyung-Min Kim, Hanock Kwak and Young-Jin Park", "title": "A Worrying Analysis of Probabilistic Time-series Models for Sales\n  Forecasting", "comments": "NeurIPS 2020 workshop (I Can't Believe It's Not Better,\n  ICBINB@NeurIPS 2020). All authors contributed equally to this research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic time-series models become popular in the forecasting field as\nthey help to make optimal decisions under uncertainty. Despite the growing\ninterest, a lack of thorough analysis hinders choosing what is worth applying\nfor the desired task. In this paper, we analyze the performance of three\nprominent probabilistic time-series models for sales forecasting. To remove the\nrole of random chance in architecture's performance, we make two experimental\nprinciples; 1) Large-scale dataset with various cross-validation sets. 2) A\nstandardized training and hyperparameter selection. The experimental results\nshow that a simple Multi-layer Perceptron and Linear Regression outperform the\nprobabilistic models on RMSE without any feature engineering. Overall, the\nprobabilistic models fail to achieve better performance on point estimation,\nsuch as RMSE and MAPE, than comparably simple baselines. We analyze and discuss\nthe performances of probabilistic time-series models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 03:31:23 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Jung", "Seungjae", ""], ["Kim", "Kyung-Min", ""], ["Kwak", "Hanock", ""], ["Park", "Young-Jin", ""]]}, {"id": "2011.10863", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Hanmo Li", "title": "Gaussian orthogonal latent factor processes for large incomplete\n  matrices of correlated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Gaussian orthogonal latent factor processes for modeling and\npredicting large correlated data. To handle the computational challenge, we\nfirst decompose the likelihood function of the Gaussian random field with a\nmulti-dimensional input domain into a product of densities at the orthogonal\ncomponents with lower-dimensional inputs. The continuous-time Kalman filter is\nimplemented to compute the likelihood function efficiently without making\napproximations. We also show that the posterior distribution of the factor\nprocesses is independent, as a consequence of prior independence of factor\nprocesses and orthogonal factor loading matrix. For studies with large sample\nsizes, we propose a flexible way to model the mean, and we derive the marginal\nposterior distribution to solve identifiability issues in sampling these\nparameters. Both simulated and real data applications confirm the outstanding\nperformance of this method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 20:39:38 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 19:28:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Gu", "Mengyang", ""], ["Li", "Hanmo", ""]]}, {"id": "2011.10971", "submitter": "Hongqiao Wang", "authors": "Ying Zhou, Hongqiao Wang", "title": "Inferring the unknown parameters in Differential Equation by Gaussian\n  Process Regression with Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential Equation (DE) is a commonly used modeling method in various\nscientific subjects such as finance and biology. The parameters in DE models\noften have interesting scientific interpretations, but their values are often\nunknown and need to be estimated from the measurements of the DE. In this work,\nwe propose a Bayesian inference framework to solve the problem of estimating\nthe parameters of the DE model, from the given noisy and scarce observations of\nthe solution only. A key issue in this problem is to robustly estimate the\nderivatives of a function from noisy observations of only the function values\nat given location points, under the assumption of a physical model in the form\nof differential equation governing the function and its derivatives. To address\nthe key issue, we use the Gaussian Process Regression with Constraint (GPRC)\nmethod which jointly model the solution, the derivatives, and the parametric\ndifferential equation, to estimate the solution and its derivatives. For\nnonlinear differential equations, a Picard-iteration-like approximation of\nlinearization method is used so that the GPRC can be still iteratively\napplicable. A new potential which combines the data and equation information,\nis proposed and used in the likelihood for our inference. With numerical\nexamples, we illustrate that the proposed method has competitive performance\nagainst existing approaches for estimating the unknown parameters in DEs.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2020 09:12:56 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Zhou", "Ying", ""], ["Wang", "Hongqiao", ""]]}, {"id": "2011.11177", "submitter": "Paul Roediger", "authors": "Paul A. Roediger", "title": "Gonogo: An R Implementation of Test Methods to Perform, Analyze and\n  Simulate Sensitivity Experiments", "comments": "This documentation is 58 pages in length and contains 31 figures, 40\n  tables and 2 flow diagrams. The subject of much of the paper, the gonogo.R\n  file, contains 118 functions plus 2 constants and is available online", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME stat.ML stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work provides documentation for a suite of R functions contained in\ngonogo.R. The functions provide sensitivity testing practitioners and\nresearchers with an ability to conduct, analyze and simulate various\nsensitivity experiments involving binary responses and a single stimulus level\n(e.g., drug dosage, drop height, velocity, etc.). Included are the modern Neyer\nand 3pod adaptive procedures, as well as the Bruceton and Langlie. The latter\ntwo benchmark procedures are capable of being performed according to\ngeneralized up-down transformed-response rules. Each procedure is designated\nphase-one of a three-phase experiment. The goal of phase-one is to achieve\noverlapping data. The two additional (and optional) refinement phases utilize\nthe D-optimal criteria and the Robbins-Monro-Joseph procedure. The goals of the\ntwo refinement phases are to situate testing in the vicinity of the median and\ntails of the latent response distribution, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 02:28:29 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Roediger", "Paul A.", ""]]}, {"id": "2011.11592", "submitter": "Tracie Shing", "authors": "Tracie L. Shing, John S. Preisser, Richard C. Zink", "title": "GEECORR: A SAS macro for regression models of correlated binary\n  responses and within-cluster correlation using generalized estimating\n  equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A SAS macro, GEECORR, has been developed for the analysis of correlated\nbinary data based on the Prentice (1988) estimating equations method that\nextends the Liang and Zeger (1986) generalized estimating equations (GEE)\nmethod to include additional estimating equations for the pairwise correlation\nbetween binary variates. This extension allows for flexible modeling of both\nthe marginal mean and within-cluster correlation as a function of their\nrespective covariate risk factors. This paper provides an overview of the\nextended estimating equations method, describes the features and capabilities\nof the GEECORR macro, and applies the GEECORR macro to three different\ndatasets. In addition, this paper describes the more detailed fitting algorithm\nproposed by Prentice (1988), of which a variation has been implemented in the\nGEECORR macro. We provide a small simulation study to demonstrate the\nefficiency of the detailed method for estimating correlation parameters.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 18:10:00 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Shing", "Tracie L.", ""], ["Preisser", "John S.", ""], ["Zink", "Richard C.", ""]]}, {"id": "2011.11710", "submitter": "Mihai Alexandru Petrovici", "authors": "Elena Kreutzer, Walter M. Senn, Mihai A. Petrovici", "title": "Natural-gradient learning for spiking neurons", "comments": "Joint senior authorship: Walter M. Senn and Mihai A. Petrovici", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.NE math.DG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In many normative theories of synaptic plasticity, weight updates implicitly\ndepend on the chosen parametrization of the weights. This problem relates, for\nexample, to neuronal morphology: synapses which are functionally equivalent in\nterms of their impact on somatic firing can differ substantially in spine size\ndue to their different positions along the dendritic tree. Classical theories\nbased on Euclidean gradient descent can easily lead to inconsistencies due to\nsuch parametrization dependence. The issues are solved in the framework of\nRiemannian geometry, in which we propose that plasticity instead follows\nnatural gradient descent. Under this hypothesis, we derive a synaptic learning\nrule for spiking neurons that couples functional efficiency with the\nexplanation of several well-documented biological phenomena such as dendritic\ndemocracy, multiplicative scaling and heterosynaptic plasticity. We therefore\nsuggest that in its search for functional synaptic plasticity, evolution might\nhave come up with its own version of natural gradient descent.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 20:26:37 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Kreutzer", "Elena", ""], ["Senn", "Walter M.", ""], ["Petrovici", "Mihai A.", ""]]}, {"id": "2011.11846", "submitter": "Tien Dung Nguyen", "authors": "Tien-Dung Nguyen, Bogdan Gabrys and Katarzyna Musial", "title": "AutoWeka4MCPS-AVATAR: Accelerating Automated Machine Learning Pipeline\n  Composition and Optimisation", "comments": "arXiv admin note: substantial text overlap with arXiv:2001.11158", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated machine learning pipeline (ML) composition and optimisation aim at\nautomating the process of finding the most promising ML pipelines within\nallocated resources (i.e., time, CPU and memory). Existing methods, such as\nBayesian-based and genetic-based optimisation, which are implemented in\nAuto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them.\nTherefore, the pipeline composition and optimisation of these methods\nfrequently require a tremendous amount of time that prevents them from\nexploring complex pipelines to find better predictive models. To further\nexplore this research challenge, we have conducted experiments showing that\nmany of the generated pipelines are invalid in the first place, and attempting\nto execute them is a waste of time and resources. To address this issue, we\npropose a novel method to evaluate the validity of ML pipelines, without their\nexecution, using a surrogate model (AVATAR). The AVATAR generates a knowledge\nbase by automatically learning the capabilities and effects of ML algorithms on\ndatasets' characteristics. This knowledge base is used for a simplified mapping\nfrom an original ML pipeline to a surrogate model which is a Petri net based\npipeline. Instead of executing the original ML pipeline to evaluate its\nvalidity, the AVATAR evaluates its surrogate model constructed by capabilities\nand effects of the ML pipeline components and input/output simplified mappings.\nEvaluating this surrogate model is less resource-intensive than the execution\nof the original pipeline. As a result, the AVATAR enables the pipeline\ncomposition and optimisation methods to evaluate more pipelines by quickly\nrejecting invalid pipelines. We integrate the AVATAR into the sequential\nmodel-based algorithm configuration (SMAC). Our experiments show that when SMAC\nemploys AVATAR, it finds better solutions than on its own.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2020 14:05:49 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Nguyen", "Tien-Dung", ""], ["Gabrys", "Bogdan", ""], ["Musial", "Katarzyna", ""]]}, {"id": "2011.12036", "submitter": "Fabio Centofanti", "authors": "Fabio Centofanti, Antonio Lepore, Alessandra Menafoglio, Biagio\n  Palumbo, Simone Vantini", "title": "Adaptive Smoothing Spline Estimator for the Function-on-Function Linear\n  Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an adaptive smoothing spline (AdaSS) estimator for\nthe function-on-function linear regression model where each value of the\nresponse, at any domain point, depends on the full trajectory of the predictor.\nThe AdaSS estimator is obtained by the optimization of an objective function\nwith two spatially adaptive penalties, based on initial estimates of the\npartial derivatives of the regression coefficient function. This allows the\nproposed estimator to adapt more easily to the true coefficient function over\nregions of large curvature and not to be undersmoothed over the remaining part\nof the domain. A novel evolutionary algorithm is developed ad hoc to obtain the\noptimization tuning parameters. Extensive Monte Carlo simulations have been\ncarried out to compare the AdaSS estimator with competitors that have already\nappeared in the literature before. The results show that our proposal mostly\noutperforms the competitor in terms of estimation and prediction accuracy.\nLastly, those advantages are illustrated also on two real-data benchmark\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 11:29:12 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Centofanti", "Fabio", ""], ["Lepore", "Antonio", ""], ["Menafoglio", "Alessandra", ""], ["Palumbo", "Biagio", ""], ["Vantini", "Simone", ""]]}, {"id": "2011.12560", "submitter": "Christophe Ley", "authors": "Sla{\\dj}ana Babi\\'c and Christophe Ley and Marko Palangeti\\'c", "title": "Elliptical Symmetry Tests in \\proglang{R}", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of elliptical symmetry has an important role in many\ntheoretical developments and applications, hence it is of primary importance to\nbe able to test whether that assumption actually holds true or not. Various\ntests have been proposed in the literature for this problem. To the best of our\nknowledge, none of them has been implemented in R. The focus of this paper is\nthe implementation of several well-known tests for elliptical symmetry together\nwith some recent tests. We demonstrate the testing procedures with a real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 07:55:45 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 11:25:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Babi\u0107", "Sla\u0111ana", ""], ["Ley", "Christophe", ""], ["Palangeti\u0107", "Marko", ""]]}, {"id": "2011.13057", "submitter": "Giles Hooker", "authors": "Zi Ye, Giles Hooker and Stephen P. Ellner", "title": "Generalized Single Index Models and Jensen Effects on Reproduction and\n  Survival", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental variability often has substantial impacts on natural\npopulations and communities through its effects on the performance of\nindividuals. Because organisms' responses to environmental conditions are often\nnonlinear (e.g., decreasing performance on both sides of an optimal\ntemperature), the mean response is often different from the response in the\nmean environment. Ye et. al. 2020, proposed testing for the presence of such\nvariance effects on individual or population growth rates by estimating the\n\"Jensen Effect\", the difference in average growth rates under varying versus\nfixed environments, in functional single index models for environmental effects\non growth. In this paper, we extend this analysis to effect of environmental\nvariance on reproduction and survival, which have count and binary outcomes. In\nthe standard generalized linear models used to analyze such data the direction\nof the Jensen Effect is tacitly assumed a priori by the model's link function.\nHere we extend the methods of Ye et. al. 2020 using a generalized single index\nmodel to test whether this assumed direction is contradicted by the data. We\nshow that our test has reasonable power under mild alternatives, but requires\nsample sizes that are larger than are often available. We demonstrate our\nmethods on a long-term time series of plant ground cover on the Idaho steppe.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 23:03:13 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Ye", "Zi", ""], ["Hooker", "Giles", ""], ["Ellner", "Stephen P.", ""]]}, {"id": "2011.13083", "submitter": "Jaewoo Park", "authors": "Benjamin Seiyon Lee and Jaewoo Park", "title": "A Scalable Partitioned Approach to Model Massive Nonstationary\n  Non-Gaussian Spatial Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonstationary non-Gaussian spatial data are common in many disciplines,\nincluding climate science, ecology, epidemiology, and social sciences. Examples\ninclude count data on disease incidence and binary satellite data on cloud mask\n(cloud/no-cloud). Modeling such datasets as stationary spatial processes can be\nunrealistic since they are collected over large heterogeneous domains (i.e.,\nspatial behavior differs across subregions). Although several approaches have\nbeen developed for nonstationary spatial models, these have focused primarily\non Gaussian responses. In addition, fitting nonstationary models for large\nnon-Gaussian datasets is computationally prohibitive. To address these\nchallenges, we propose a scalable algorithm for modeling such data by\nleveraging parallel computing in modern high-performance computing systems. We\npartition the spatial domain into disjoint subregions and fit locally\nnonstationary models using a carefully curated set of spatial basis functions.\nThen, we combine the local processes using a novel neighbor-based weighting\nscheme. Our approach scales well to massive datasets (e.g., 1 million samples)\nand can be implemented in nimble, a popular software environment for Bayesian\nhierarchical modeling. We demonstrate our method to simulated examples and two\nlarge real-world datasets pertaining to infectious diseases and remote sensing.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 01:17:57 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Lee", "Benjamin Seiyon", ""], ["Park", "Jaewoo", ""]]}, {"id": "2011.13960", "submitter": "Abhinandan Dalal", "authors": "Navonil Deb, Abhinandan Dalal and Gopal Krishna Basak", "title": "Finding Optimal Cancer Treatment using Markov Decision Process to\n  Improve Overall Health and Quality of Life", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-bio.QM q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Decision Processes and Dynamic Treatment Regimes have grown\nincreasingly popular in the treatment of diseases, including cancer. However,\ncancer treatment often impacts quality of life drastically, and people often\nfail to take treatments that are sustainable, affordable and can be adhered to.\nIn this paper, we emphasize the usage of ambient factors like profession,\nradioactive exposure, food habits on the treatment choice, keeping in mind that\nthe aim is not just to relieve the patient of his disease, but rather to\nmaximize his overall physical, social and mental well being. We delineate a\ngeneral framework which can directly incorporate a net benefit function from a\nphysician as well as patient's utility, and can incorporate the varying\nprobabilities of exposure and survival of patients of varying medical profiles.\nWe also show by simulations that the optimal choice of actions often is\nsensitive to extraneous factors, like the financial status of a person (as a\nproxy for the affordability of treatment), and that these actions should be\nwelcome keeping in mind the overall quality of life.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 19:06:04 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Deb", "Navonil", ""], ["Dalal", "Abhinandan", ""], ["Basak", "Gopal Krishna", ""]]}, {"id": "2011.14238", "submitter": "Amy Zhang", "authors": "Amy X. Zhang, Le Bao, Michael J. Daniels", "title": "Approximate Cross-validated Mean Estimates for Bayesian Hierarchical\n  Regression Models", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel procedure for obtaining cross-validated predictive\nestimates for Bayesian hierarchical regression models (BHRMs). Bayesian\nhierarchical models are popular for their ability to model complex dependence\nstructures and provide probabilistic uncertainty estimates, but can be\ncomputationally expensive to run. Cross-validation (CV) is therefore not a\ncommon practice to evaluate the predictive performance of BHRMs. Our method\ncircumvents the need to re-run computationally costly estimation methods for\neach cross-validation fold and makes CV more feasible for large BHRMs. By\nconditioning on the variance-covariance parameters, we shift the CV problem\nfrom probability-based sampling to a simple and familiar optimization problem.\nIn many cases, this produces estimates which are equivalent to full CV. We\nprovide theoretical results and demonstrate its efficacy on publicly available\ndata and in simulations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 00:00:20 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 20:45:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Amy X.", ""], ["Bao", "Le", ""], ["Daniels", "Michael J.", ""]]}, {"id": "2011.14279", "submitter": "Lizhen Nie", "authors": "Lizhen Nie and Veronika Ro\\v{c}kov\\'a", "title": "Bayesian Bootstrap Spike-and-Slab LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impracticality of posterior sampling has prevented the widespread\nadoption of spike-and-slab priors in high-dimensional applications. To\nalleviate the computational burden, optimization strategies have been proposed\nthat quickly find local posterior modes. Trading off uncertainty quantification\nfor computational speed, these strategies have enabled spike-and-slab\ndeployments at scales that would be previously unfeasible. We build on one\nrecent development in this strand of work: the Spike-and-Slab LASSO procedure\nof Ro\\v{c}kov\\'{a} and George (2018). Instead of optimization, however, we\nexplore multiple avenues for posterior sampling, some traditional and some new.\nIntrigued by the speed of Spike-and-Slab LASSO mode detection, we explore the\npossibility of sampling from an approximate posterior by performing MAP\noptimization on many independently perturbed datasets. To this end, we explore\nBayesian bootstrap ideas and introduce a new class of jittered Spike-and-Slab\nLASSO priors with random shrinkage targets. These priors are a key constituent\nof the Bayesian Bootstrap Spike-and-Slab LASSO (BB-SSL) method proposed here.\nBB-SSL turns fast optimization into approximate posterior sampling. Beyond its\nscalability, we show that BB-SSL has a strong theoretical support. Indeed, we\nfind that the induced pseudo-posteriors contract around the truth at a\nnear-optimal rate in sparse normal-means and in high-dimensional regression. We\ncompare our algorithm to the traditional Stochastic Search Variable Selection\n(under Laplace priors) as well as many state-of-the-art methods for shrinkage\npriors. We show, both in simulations and on real data, that our method fares\nsuperbly in these comparisons, often providing substantial computational gains.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2020 04:39:43 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:55:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nie", "Lizhen", ""], ["Ro\u010dkov\u00e1", "Veronika", ""]]}, {"id": "2011.14698", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri and Andrew Duncan and George Thorne and Geoffrey Parks\n  and Mark Girolami", "title": "Bayesian Assessments of Aeroengine Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Aeroengine performance is determined by temperature and pressure profiles\nalong various axial stations within an engine. Given limited sensor\nmeasurements along an axial station, we require a statistically principled\napproach to inferring these profiles. In this paper, we detail a Bayesian\nmethodology for interpolating the spatial temperature or pressure profile at a\nsingle axial station within an aeroengine. The profile is represented as a\nspatial Gaussian random field on an annulus, with circumferential variations\nmodelled using a Fourier basis and a square exponential kernel respectively. In\nthe scenario where precise frequencies comprising the temperature field are\nunknown, we utilise a sparsity-promoting prior on the frequencies to encourage\nsparse representations. The main quantity of interest, the spatial area average\nis readily obtained in closed form, and we demonstrate how to naturally\ndecompose the posterior uncertainty into terms characterising insufficient\nsampling and sensor measurement error respectively. Finally, we demonstrate how\nthis framework can be employed to enable more tailored design of experiments.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 11:25:05 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Seshadri", "Pranay", ""], ["Duncan", "Andrew", ""], ["Thorne", "George", ""], ["Parks", "Geoffrey", ""], ["Girolami", "Mark", ""]]}]