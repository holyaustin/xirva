[{"id": "1009.0893", "submitter": "Charles R. Doss", "authors": "Charles R. Doss, Marc A. Suchard, Ian Holmes, Midori Kato-Maeda,\n  Vladimir N. Minin", "title": "Fitting birth-death processes to panel data with applications to\n  bacterial DNA fingerprinting", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS673 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 4, 2315-2335", "doi": "10.1214/13-AOAS673", "report-no": "IMS-AOAS-AOAS673", "categories": "stat.CO q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time linear birth-death-immigration (BDI) processes are frequently\nused in ecology and epidemiology to model stochastic dynamics of the population\nof interest. In clinical settings, multiple birth-death processes can describe\ndisease trajectories of individual patients, allowing for estimation of the\neffects of individual covariates on the birth and death rates of the process.\nSuch estimation is usually accomplished by analyzing patient data collected at\nunevenly spaced time points, referred to as panel data in the biostatistics\nliterature. Fitting linear BDI processes to panel data is a nontrivial\noptimization problem because birth and death rates can be functions of many\nparameters related to the covariates of interest. We propose a novel\nexpectation--maximization (EM) algorithm for fitting linear BDI models with\ncovariates to panel data. We derive a closed-form expression for the joint\ngenerating function of some of the BDI process statistics and use this\ngenerating function to reduce the E-step of the EM algorithm, as well as\ncalculation of the Fisher information, to one-dimensional integration. This\nanalytical technique yields a computationally efficient and robust optimization\nalgorithm that we implemented in an open-source R package. We apply our method\nto DNA fingerprinting of Mycobacterium tuberculosis, the causative agent of\ntuberculosis, to study intrapatient time evolution of IS6110 copy number, a\ngenetic marker frequently used during estimation of epidemiological clusters of\nMycobacterium tuberculosis infections. Our analysis reveals previously\nundocumented differences in IS6110 birth-death rates among three major lineages\nof Mycobacterium tuberculosis, which has important implications for\nepidemiologists that use IS6110 for DNA fingerprinting of Mycobacterium\ntuberculosis.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 05:23:50 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 23:13:44 GMT"}, {"version": "v3", "created": "Sun, 16 Dec 2012 23:30:46 GMT"}, {"version": "v4", "created": "Tue, 13 Aug 2013 21:47:17 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2014 14:07:26 GMT"}], "update_date": "2014-01-13", "authors_parsed": [["Doss", "Charles R.", ""], ["Suchard", "Marc A.", ""], ["Holmes", "Ian", ""], ["Kato-Maeda", "Midori", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1009.1216", "submitter": "Nicolas Bousquet", "authors": "Alberto Pasanisi and Shuai Fu and Nicolas Bousquet", "title": "Estimating Discrete Markov Models From Various Incomplete Data Schemes", "comments": "26 pages - preprint accepted in 20th February 2012 for publication in\n  Computational Statistics and Data Analysis (please cite the journal's paper)", "journal-ref": "Computational Statistics and Data Analysis - Volume 56, Issue 9,\n  September 2012, Pages 2609-2625", "doi": "10.1016/j.csda.2012.02.027", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameters of a discrete stationary Markov model are transition\nprobabilities between states. Traditionally, data consist in sequences of\nobserved states for a given number of individuals over the whole observation\nperiod. In such a case, the estimation of transition probabilities is\nstraightforwardly made by counting one-step moves from a given state to\nanother. In many real-life problems, however, the inference is much more\ndifficult as state sequences are not fully observed, namely the state of each\nindividual is known only for some given values of the time variable. A review\nof the problem is given, focusing on Monte Carlo Markov Chain (MCMC) algorithms\nto perform Bayesian inference and evaluate posterior distributions of the\ntransition probabilities in this missing-data framework. Leaning on the\ndependence between the rows of the transition matrix, an adaptive MCMC\nmechanism accelerating the classical Metropolis-Hastings algorithm is then\nproposed and empirically studied.\n", "versions": [{"version": "v1", "created": "Tue, 7 Sep 2010 07:49:21 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2012 14:04:08 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Pasanisi", "Alberto", ""], ["Fu", "Shuai", ""], ["Bousquet", "Nicolas", ""]]}, {"id": "1009.1444", "submitter": "D\\'avid Papp", "authors": "D\\'avid Papp", "title": "Optimal designs for rational function regression", "comments": "25 pages. Previous version updated with more details in the theory\n  and additional examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal non-sequential designs for a large class of (linear and\nnonlinear) regression models involving polynomials and rational functions with\nheteroscedastic noise also given by a polynomial or rational weight function.\nThe proposed method treats D-, E-, A-, and $\\Phi_p$-optimal designs in a\nunified manner, and generates a polynomial whose zeros are the support points\nof the optimal approximate design, generalizing a number of previously known\nresults of the same flavor. The method is based on a mathematical optimization\nmodel that can incorporate various criteria of optimality and can be solved\nefficiently by well established numerical optimization methods. In contrast to\nprevious optimization-based methods proposed for similar design problems, it\nalso has theoretical guarantee of its algorithmic efficiency; in fact, the\nrunning times of all numerical examples considered in the paper are negligible.\nThe stability of the method is demonstrated in an example involving high degree\npolynomials. After discussing linear models, applications for finding locally\noptimal designs for nonlinear regression models involving rational functions\nare presented, then extensions to robust regression designs, and trigonometric\nregression are shown. As a corollary, an upper bound on the size of the support\nset of the minimally-supported optimal designs is also found. The method is of\nconsiderable practical importance, with the potential for instance to impact\ndesign software development. Further study of the optimality conditions of the\nmain optimization model might also yield new theoretical insights.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 02:44:03 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2011 17:28:56 GMT"}], "update_date": "2011-08-30", "authors_parsed": [["Papp", "D\u00e1vid", ""]]}, {"id": "1009.1909", "submitter": "Ting Kei Pong", "authors": "Zhaosong Lu, Ting Kei Pong", "title": "Computing Optimal Experimental Designs via Interior Point Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study optimal experimental design problems with a broad\nclass of smooth convex optimality criteria, including the classical A-, D- and\np th mean criterion. In particular, we propose an interior point (IP) method\nfor them and establish its global convergence. Furthermore, by exploiting the\nstructure of the Hessian matrix of the aforementioned optimality criteria, we\nderive an explicit formula for computing its rank. Using this result, we then\nshow that the Newton direction arising in the IP method can be computed\nefficiently via Sherman-Morrison-Woodbury formula when the size of the moment\nmatrix is small relative to the sample size. Finally, we compare our IP method\nwith the widely used multiplicative algorithm introduced by Silvey et al. [29].\nThe computational results show that the IP method generally outperforms the\nmultiplicative algorithm both in speed and solution quality.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 22:47:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2012 00:25:10 GMT"}, {"version": "v3", "created": "Fri, 12 Oct 2012 23:55:15 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Lu", "Zhaosong", ""], ["Pong", "Ting Kei", ""]]}, {"id": "1009.1914", "submitter": "Anthony Lee", "authors": "Anthony Lee, Francois Caron, Arnaud Doucet, Chris Holmes", "title": "A Hierarchical Bayesian Framework for Constructing Sparsity-inducing\n  Priors", "comments": "Submitted for publication; corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection techniques have become increasingly popular amongst\nstatisticians due to an increased number of regression and classification\napplications involving high-dimensional data where we expect some predictors to\nbe unimportant. In this context, Bayesian variable selection techniques\ninvolving Markov chain Monte Carlo exploration of the posterior distribution\nover models can be prohibitively computationally expensive and so there has\nbeen attention paid to quasi-Bayesian approaches such as maximum a posteriori\n(MAP) estimation using priors that induce sparsity in such estimates. We focus\non this latter approach, expanding on the hierarchies proposed to date to\nprovide a Bayesian interpretation and generalization of state-of-the-art\npenalized optimization approaches and providing simultaneously a natural way to\ninclude prior information about parameters within this framework. We give\nexamples of how to use this hierarchy to compute MAP estimates for linear and\nlogistic regression as well as sparse precision-matrix estimates in Gaussian\ngraphical models. In addition, an adaptive group lasso method is derived using\nthe framework.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 23:28:44 GMT"}, {"version": "v2", "created": "Thu, 16 Sep 2010 20:20:37 GMT"}], "update_date": "2010-09-20", "authors_parsed": [["Lee", "Anthony", ""], ["Caron", "Francois", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Chris", ""]]}, {"id": "1009.2022", "submitter": "Luis Mendo", "authors": "Luis Mendo", "title": "Estimation of a probability in inverse binomial sampling under\n  normalized linear-linear and inverse-linear loss", "comments": "4 figures", "journal-ref": "Test (2012), 21, pp. 656--675", "doi": "10.1007/s11749-011-0267-x", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential estimation of the success probability $p$ in inverse binomial\nsampling is considered in this paper. For any estimator $\\hat p$, its quality\nis measured by the risk associated with normalized loss functions of\nlinear-linear or inverse-linear form. These functions are possibly asymmetric,\nwith arbitrary slope parameters $a$ and $b$ for $\\hat p<p$ and $\\hat p>p$\nrespectively. Interest in these functions is motivated by their significance\nand potential uses, which are briefly discussed. Estimators are given for which\nthe risk has an asymptotic value as $p$ tends to $0$, and which guarantee that,\nfor any $p$ in $(0,1)$, the risk is lower than its asymptotic value. This\nallows selecting the required number of successes, $r$, to meet a prescribed\nquality irrespective of the unknown $p$. In addition, the proposed estimators\nare shown to be approximately minimax when $a/b$ does not deviate too much from\n$1$, and asymptotically minimax as $r$ tends to infinity when $a=b$.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 14:35:47 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 12:20:19 GMT"}, {"version": "v3", "created": "Wed, 18 Dec 2013 10:03:38 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Mendo", "Luis", ""]]}, {"id": "1009.2260", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Computing the confidence levels for a root-mean-square test of\n  goodness-of-fit, II", "comments": "14 pages, 3 figures (each with two parts), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends our earlier article, \"Computing the confidence levels for\na root-mean-square test of goodness-of-fit;\" unlike in the earlier article, the\nmodels in the present paper involve parameter estimation -- both the null and\nalternative hypotheses in the associated tests are composite. We provide\nefficient black-box algorithms for calculating the asymptotic confidence levels\nof a variant on the classic chi-squared test. In some circumstances, it is also\nfeasible to compute the exact confidence levels via Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Sun, 12 Sep 2010 19:22:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jan 2011 17:38:28 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2011 17:37:48 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1009.2300", "submitter": "Chenlei Leng", "authors": "Chenlei Leng, Minh Ngoc Tran and David Nott", "title": "Bayesian Adaptive Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Bayesian adaptive Lasso (BaLasso) for variable selection and\ncoefficient estimation in linear regression. The BaLasso is adaptive to the\nsignal level by adopting different shrinkage for different coefficients.\nFurthermore, we provide a model selection machinery for the BaLasso by\nassessing the posterior conditional mode estimates, motivated by the\nhierarchical Bayesian interpretation of the Lasso. Our formulation also permits\nprediction using a model averaging strategy. We discuss other variants of this\nnew approach and provide a unified framework for variable selection using\nflexible penalties. Empirical evidence of the attractiveness of the method is\ndemonstrated via extensive simulation studies and data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Sep 2010 06:02:37 GMT"}], "update_date": "2010-09-14", "authors_parsed": [["Leng", "Chenlei", ""], ["Tran", "Minh Ngoc", ""], ["Nott", "David", ""]]}, {"id": "1009.2698", "submitter": "Michael Amrein", "authors": "Michael Amrein and Hans R. K\\\"unsch", "title": "Approximate variances for tapered spectral estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approximation of the asymptotic variance that removes a certain\ndiscontinuity in the usual formula for the raw and the smoothed periodogram in\ncase a data taper is used. It is based on an approximation of the covariance of\nthe (tapered) periodogram at two arbitrary frequencies. Exact computations of\nthe variances for a Gaussian white noise and an AR(4) process show that the\napproximation is more accurate than the usual formula.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 15:37:23 GMT"}, {"version": "v2", "created": "Mon, 24 Jan 2011 17:26:33 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Amrein", "Michael", ""], ["K\u00fcnsch", "Hans R.", ""]]}, {"id": "1009.2707", "submitter": "Karim Lounici", "authors": "Pierre Alquier, Karim Lounici", "title": "Pac-bayesian bounds for sparse regression estimation with exponential\n  weights", "comments": "19 pages", "journal-ref": "Electronic Journal of Statistics, Vol 5(2011), 127-145", "doi": "10.1214/11-EJS601", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sparse regression model where the number of parameters $p$ is\nlarger than the sample size $n$. The difficulty when considering\nhigh-dimensional problems is to propose estimators achieving a good compromise\nbetween statistical and computational performances. The BIC estimator for\ninstance performs well from the statistical point of view \\cite{BTW07} but can\nonly be computed for values of $p$ of at most a few tens. The Lasso estimator\nis solution of a convex minimization problem, hence computable for large value\nof $p$. However stringent conditions on the design are required to establish\nfast rates of convergence for this estimator. Dalalyan and Tsybakov\n\\cite{arnak} propose a method achieving a good compromise between the\nstatistical and computational aspects of the problem. Their estimator can be\ncomputed for reasonably large $p$ and satisfies nice statistical properties\nunder weak assumptions on the design. However, \\cite{arnak} proposes sparsity\noracle inequalities in expectation for the empirical excess risk only. In this\npaper, we propose an aggregation procedure similar to that of \\cite{arnak} but\nwith improved statistical performances. Our main theoretical result is a\nsparsity oracle inequality in probability for the true excess risk for a\nversion of exponential weight estimator. We also propose a MCMC method to\ncompute our estimator for reasonably large values of $p$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Sep 2010 16:17:29 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 14:53:23 GMT"}], "update_date": "2011-03-15", "authors_parsed": [["Alquier", "Pierre", ""], ["Lounici", "Karim", ""]]}, {"id": "1009.3072", "submitter": "Ian Dryden", "authors": "Kim Kenobi and Ian L. Dryden", "title": "Bayesian matching of unlabelled point sets using Procrustes and\n  configuration models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of matching unlabelled point sets using Bayesian inference is\nconsidered. Two recently proposed models for the likelihood are compared, based\non the Procrustes size-and-shape and the full configuration. Bayesian inference\nis carried out for matching point sets using Markov chain Monte Carlo\nsimulation. An improvement to the existing Procrustes algorithm is proposed\nwhich improves convergence rates, using occasional large jumps in the burn-in\nperiod. The Procrustes and configuration methods are compared in a simulation\nstudy and using real data, where it is of interest to estimate the strengths of\nmatches between protein binding sites. The performance of both methods is\ngenerally quite similar, and a connection between the two models is made using\na Laplace approximation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Sep 2010 01:44:17 GMT"}], "update_date": "2010-09-17", "authors_parsed": [["Kenobi", "Kim", ""], ["Dryden", "Ian L.", ""]]}, {"id": "1009.3507", "submitter": "Matthew Schofield", "authors": "Matthew R. Schofield, Richard J. Barker", "title": "Hierarchical Modeling of Abundance in Closed Population\n  Capture-Recapture Models Under Heterogeneity", "comments": null, "journal-ref": "Environmental and Ecological Statistics, September 2014, Volume\n  21, Issue 3, pp 435-451", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical modeling of abundance in space or time using closed-population\nmark-recapture under heterogeneity (model M$_{h}$) presents two challenges: (i)\nfinding a flexible likelihood in which abundance appears as an explicit\nparameter and (ii) fitting the hierarchical model for abundance. The first\nchallenge arises because abundance not only indexes the population size, it\nalso determines the dimension of the capture probabilities in heterogeneity\nmodels. A common approach is to use data augmentation to include these capture\nprobabilities directly into the likelihood and fit the model using Bayesian\ninference via Markov chain Monte Carlo (MCMC). Two such examples of this\napproach are (i) explicit trans-dimensional MCMC, and (ii) superpopulation data\naugmentation. The superpopulation approach has the advantage of simple\nspecification that is easily implemented in BUGS and related software. However,\nit reparameterizes the model so that abundance is no longer included, except as\na derived quantity. This is a drawback when hierarchical models for abundance,\nor related parameters, are desired. Here, we analytically compare the two\napproaches and show that they are more closely related than might appear\nsuperficially. We exploit this relationship to specify the model in a way that\nallows us to include abundance as a parameter and that facilitates hierarchical\nmodeling using readily available software such as BUGS. We use this approach to\nmodel trends in grizzly bear abundance in Yellowstone National Park from\n1986-1998.\n", "versions": [{"version": "v1", "created": "Fri, 17 Sep 2010 21:14:45 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 00:04:14 GMT"}], "update_date": "2015-04-10", "authors_parsed": [["Schofield", "Matthew R.", ""], ["Barker", "Richard J.", ""]]}, {"id": "1009.4241", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Heng Lian", "title": "Gaussian process single-index models as emulators for computer\n  experiments", "comments": "23 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A single-index model (SIM) provides for parsimonious multi-dimensional\nnonlinear regression by combining parametric (linear) projection with\nunivariate nonparametric (non-linear) regression models. We show that a\nparticular Gaussian process (GP) formulation is simple to work with and ideal\nas an emulator for some types of computer experiment as it can outperform the\ncanonical separable GP regression model commonly used in this setting. Our\ncontribution focuses on drastically simplifying, re-interpreting, and then\ngeneralizing a recently proposed fully Bayesian GP-SIM combination, and then\nillustrating its favorable performance on synthetic data and a real-data\ncomputer experiment. Two R packages, both released on CRAN, have been augmented\nto facilitate inference under our proposed model(s).\n", "versions": [{"version": "v1", "created": "Wed, 22 Sep 2010 01:09:03 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2011 02:34:39 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 16:39:18 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lian", "Heng", ""]]}, {"id": "1009.4959", "submitter": "Jan Mandel", "authors": "Ashok Krishnamurthy, Loren Cobb, Jan Mandel, and Jonathan Beezley", "title": "Bayesian Tracking of Emerging Epidemics Using Ensemble Optimal\n  Statistical Interpolation (EnOSI)", "comments": "15 pages, 5 figures. JSM 2010", "journal-ref": null, "doi": null, "report-no": "UCD CCM Report 291", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of the optimal statistical interpolation (OSI) data\nassimilation method for the statistical tracking of emerging epidemics and to\nstudy the spatial dynamics of a disease. The epidemic models that we used for\nthis study are spatial variants of the common susceptible-infectious-removed\n(S-I-R) compartmental model of epidemiology. The spatial S-I-R epidemic model\nis illustrated by application to simulated spatial dynamic epidemic data from\nthe historic \"Black Death\" plague of 14th century Europe. Bayesian statistical\ntracking of emerging epidemic diseases using the OSI as it unfolds is\nillustrated for a simulated epidemic wave originating in Santa Fe, New Mexico.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 00:41:33 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Krishnamurthy", "Ashok", ""], ["Cobb", "Loren", ""], ["Mandel", "Jan", ""], ["Beezley", "Jonathan", ""]]}, {"id": "1009.5103", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Maria De Iorio and Marc Chadeau-Hyam", "title": "The Time Machine: A Simulation Approach for Stochastic Trees", "comments": "22 Pages, 5 Figures", "journal-ref": null, "doi": "10.1098/rspa.2010.0497", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper we consider a simulation technique for stochastic\ntrees. One of the most important areas in computational genetics is the\ncalculation and subsequent maximization of the likelihood function associated\nto such models. This typically consists of using importance sampling (IS) and\nsequential Monte Carlo (SMC) techniques. The approach proceeds by simulating\nthe tree, backward in time from observed data, to a most recent common ancestor\n(MRCA). However, in many cases, the computational time and variance of\nestimators are often too high to make standard approaches useful. In this paper\nwe propose to stop the simulation, subsequently yielding biased estimates of\nthe likelihood surface. The bias is investigated from a theoretical point of\nview. Results from simulation studies are also given to investigate the balance\nbetween loss of accuracy, saving in computing time and variance reduction.\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 15:40:21 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Jasra", "Ajay", ""], ["De Iorio", "Maria", ""], ["Chadeau-Hyam", "Marc", ""]]}, {"id": "1009.5177", "submitter": "Julien Bect", "authors": "Julien Bect and David Ginsbourger and Ling Li and Victor Picheny and\n  Emmanuel Vazquez", "title": "Sequential design of computer experiments for the estimation of a\n  probability of failure", "comments": "This is an author-generated postprint version. The published version\n  is available at http://www.springerlink.com", "journal-ref": "Statistics and Computing, 22(3):773-793, 2012", "doi": "10.1007/s11222-011-9241-4", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of estimating the volume of the excursion\nset of a function $f:\\mathbb{R}^d \\to \\mathbb{R}$ above a given threshold,\nunder a probability measure on $\\mathbb{R}^d$ that is assumed to be known. In\nthe industrial world, this corresponds to the problem of estimating a\nprobability of failure of a system. When only an expensive-to-simulate model of\nthe system is available, the budget for simulations is usually severely limited\nand therefore classical Monte Carlo methods ought to be avoided. One of the\nmain contributions of this article is to derive SUR (stepwise uncertainty\nreduction) strategies from a Bayesian-theoretic formulation of the problem of\nestimating a probability of failure. These sequential strategies use a Gaussian\nprocess model of $f$ and aim at performing evaluations of $f$ as efficiently as\npossible to infer the value of the probability of failure. We compare these\nstrategies to other strategies also based on a Gaussian process model for\nestimating a probability of failure.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 07:41:59 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 20:02:53 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Bect", "Julien", ""], ["Ginsbourger", "David", ""], ["Li", "Ling", ""], ["Picheny", "Victor", ""], ["Vazquez", "Emmanuel", ""]]}]