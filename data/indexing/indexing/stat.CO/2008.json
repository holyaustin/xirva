[{"id": "2008.00532", "submitter": "Suvra Pal", "authors": "Suvra Pal", "title": "A Simplified Stochastic EM Algorithm for Cure Rate Model with Negative\n  Binomial Competing Risks: An Application to Breast Cancer Data", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a long-term survival model under competing risks is\nconsidered. The unobserved number of competing risks is assumed to follow a\nnegative binomial distribution that can capture both over- and\nunder-dispersion. Considering the latent competing risks as missing data, a\nvariation of the well-known expectation maximization (EM) algorithm, called the\nstochastic EM algorithm (SEM), is developed. It is shown that the SEM algorithm\navoids calculation of complicated expectations, which is a major advantage of\nthe SEM algorithm over the EM algorithm. The proposed procedure also allows the\nobjective function to be split into two simpler functions, one corresponding to\nthe parameters associated with the cure rate and the other corresponding to the\nparameters associated with the progression times. The advantage of this\napproach is that each simple function, with lower parameter dimension, can be\nmaximized independently. An extensive Monte Carlo simulation study is carried\nout to compare the performances of the SEM and EM algorithms. Finally, a breast\ncancer survival data is analyzed and it is shown that the SEM algorithm\nperforms better than the EM algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 17:45:14 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 03:10:49 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 23:20:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Pal", "Suvra", ""]]}, {"id": "2008.00553", "submitter": "Henrik Bengtsson", "authors": "Henrik Bengtsson", "title": "A Unifying Framework for Parallel and Distributed Processing in R using\n  Futures", "comments": "19 pages, 1 figure, accepted The R Journal, 2021", "journal-ref": null, "doi": "10.32614/RJ-2021-048", "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A future is a programming construct designed for concurrent and asynchronous\nevaluation of code, making it particularly useful for parallel processing. The\nfuture package implements the Future API for programming with futures in R.\nThis minimal API provides sufficient constructs for implementing parallel\nversions of well-established, high-level map-reduce APIs. The future ecosystem\nsupports exception handling, output and condition relaying, parallel random\nnumber generation, and automatic identification of globals lowering the\nthreshold to parallelize code. The Future API bridges parallel frontends with\nparallel backends following the philosophy that end-users are the ones who\nchoose the parallel backend while the developer focuses on what to parallelize.\nA variety of backends exist and third-party contributions meeting the\nspecifications, which ensure that the same code works on all backends, are\nautomatically supported. The future framework solves several problems not\naddressed by other parallel frameworks in R.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:53:52 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 02:16:02 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 17:35:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Bengtsson", "Henrik", ""]]}, {"id": "2008.00961", "submitter": "Mohammed Alser", "authors": "Mohammed Alser, Z\\\"ulal Bing\\\"ol, Damla Senol Cali, Jeremie Kim,\n  Saugata Ghose, Can Alkan, Onur Mutlu", "title": "Accelerating Genome Analysis: A Primer on an Ongoing Journey", "comments": "This is an extended and updated version of a paper published in IEEE\n  Micro, vol. 40, no. 5, pp. 65-75, 1 Sept.-Oct. 2020,\n  https://doi.org/10.1109/MM.2020.3013728", "journal-ref": "IEEE Micro, Volume: 40, Issue: 5, Sept.-Oct. 1 2020", "doi": "10.1109/MM.2020.3013728", "report-no": null, "categories": "cs.AR q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome analysis fundamentally starts with a process known as read mapping,\nwhere sequenced fragments of an organism's genome are compared against a\nreference genome. Read mapping is currently a major bottleneck in the entire\ngenome analysis pipeline, because state-of-the-art genome sequencing\ntechnologies are able to sequence a genome much faster than the computational\ntechniques employed to analyze the genome. We describe the ongoing journey in\nsignificantly improving the performance of read mapping. We explain\nstate-of-the-art algorithmic methods and hardware-based acceleration\napproaches. Algorithmic approaches exploit the structure of the genome as well\nas the structure of the underlying hardware. Hardware-based acceleration\napproaches exploit specialized microarchitectures or various execution\nparadigms (e.g., processing inside or near memory). We conclude with the\nchallenges of adopting these hardware-accelerated read mappers.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 21:27:31 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 08:37:43 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Alser", "Mohammed", ""], ["Bing\u00f6l", "Z\u00fclal", ""], ["Cali", "Damla Senol", ""], ["Kim", "Jeremie", ""], ["Ghose", "Saugata", ""], ["Alkan", "Can", ""], ["Mutlu", "Onur", ""]]}, {"id": "2008.02046", "submitter": "Peter Rousseeuw", "authors": "Joachim Schreurs, Iwein Vranckx, Mia Hubert, Johan A.K. Suykens, Peter\n  J. Rousseeuw", "title": "Outlier detection in non-elliptical data by kernel MRCD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The minimum regularized covariance determinant method (MRCD) is a robust\nestimator for multivariate location and scatter, which detects outliers by\nfitting a robust covariance matrix to the data. Its regularization ensures that\nthe covariance matrix is well-conditioned in any dimension. The MRCD assumes\nthat the non-outlying observations are roughly elliptically distributed, but\nmany datasets are not of that form. Moreover, the computation time of MRCD\nincreases substantially when the number of variables goes up, and nowadays\ndatasets with many variables are common. The proposed Kernel Minimum\nRegularized Covariance Determinant (KMRCD) estimator addresses both issues. It\nis not restricted to elliptical data because it implicitly computes the MRCD\nestimates in a kernel induced feature space. A fast algorithm is constructed\nthat starts from kernel-based initial estimates and exploits the kernel trick\nto speed up the subsequent computations. Based on the KMRCD estimates, a rule\nis proposed to flag outliers. The KMRCD algorithm performs well in simulations,\nand is illustrated on real-life data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 11:09:08 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 22:23:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Schreurs", "Joachim", ""], ["Vranckx", "Iwein", ""], ["Hubert", "Mia", ""], ["Suykens", "Johan A. K.", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "2008.02204", "submitter": "Sumi Seo", "authors": "Yi Li, Sumi Seo, Kyu Ha Lee", "title": "Bayesian Survival Analysis Using Gamma Processes with Adaptive Time\n  Partition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian semi-parametric analyses of time-to-event data, non-parametric\nprocess priors are adopted for the baseline hazard function or the cumulative\nbaseline hazard function for a given finite partition of the time axis.\nHowever, it would be controversial to suggest a general guideline to construct\nan optimal time partition. While a great deal of research has been done to\nrelax the assumption of the fixed split times for other non-parametric\nprocesses, to our knowledge, no methods have been developed for a gamma process\nprior, which is one of the most widely used in Bayesian survival analysis. In\nthis paper, we propose a new Bayesian framework for proportional hazards models\nwhere the cumulative baseline hazard function is modeled a priori by a gamma\nprocess. A key feature of the proposed framework is that the number and\nposition of interval cutpoints are treated as random and estimated based on\ntheir posterior distributions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:07:22 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Li", "Yi", ""], ["Seo", "Sumi", ""], ["Lee", "Kyu Ha", ""]]}, {"id": "2008.02386", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis, Piyush Pandita, Sayan Ghosh, Valeria Andreoli,\n  Thomas Vandeputte, Liping Wang", "title": "Bayesian learning of orthogonal embeddings for multi-fidelity Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach to identify optimal transformations that map\nmodel input points to low dimensional latent variables. The \"projection\"\nmapping consists of an orthonormal matrix that is considered a priori unknown\nand needs to be inferred jointly with the GP parameters, conditioned on the\navailable training data. The proposed Bayesian inference scheme relies on a\ntwo-step iterative algorithm that samples from the marginal posteriors of the\nGP parameters and the projection matrix respectively, both using Markov Chain\nMonte Carlo (MCMC) sampling. In order to take into account the orthogonality\nconstraints imposed on the orthonormal projection matrix, a Geodesic Monte\nCarlo sampling algorithm is employed, that is suitable for exploiting\nprobability measures on manifolds. We extend the proposed framework to\nmulti-fidelity models using GPs including the scenarios of training multiple\noutputs together. We validate our framework on three synthetic problems with a\nknown lower-dimensional subspace. The benefits of our proposed framework, are\nillustrated on the computationally challenging three-dimensional aerodynamic\noptimization of a last-stage blade for an industrial gas turbine, where we\nstudy the effect of an 85-dimensional airfoil shape parameterization on two\noutput quantities of interest, specifically on the aerodynamic efficiency and\nthe degree of reaction.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 22:28:53 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Pandita", "Piyush", ""], ["Ghosh", "Sayan", ""], ["Andreoli", "Valeria", ""], ["Vandeputte", "Thomas", ""], ["Wang", "Liping", ""]]}, {"id": "2008.02455", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "Exact Convergence Rate Analysis of the Independent Metropolis-Hastings\n  Algorithms", "comments": "25 pages, 2 figures, 1 table. Typos fixed. Theorem 2 and 3 in the\n  previous version are strengthened and combined as Theorem 2 in the new\n  version. Some changes in the Introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known difficult problem regarding Metropolis-Hastings algorithms is to\nget sharp bounds on their convergence rates. Moreover, a fundamental but often\noverlooked problem in Markov chain theory is to study the convergence rates for\ndifferent initializations. In this paper, we study the two issues mentioned\nabove of the Independent Metropolis-Hastings (IMH) algorithms on both general\nand discrete state spaces. We derive the exact convergence rate and prove that\nthe IMH algorithm's different deterministic initializations have the same\nconvergence rate. Surprisingly, under mild conditions, we get the exact\nconvergence speed for IMH algorithms on general state spaces, which is the\nfirst `exact convergence' result for general state space MCMC algorithms to the\nauthor's best knowledge. Connections with the Random Walk Metropolis-Hastings\n(RWMH) algorithm are also discussed, which solve a conjecture proposed by\nAtchad\\'{e} and Perron \\cite{atchade2007geometric} using a counterexample.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:49:57 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 08:18:28 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 23:47:59 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 01:07:52 GMT"}, {"version": "v5", "created": "Mon, 21 Dec 2020 00:13:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "2008.02497", "submitter": "Jaewoo Park", "authors": "Jaewoo Park", "title": "Bayesian Indirect Inference for Models with Intractable Normalizing\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for doubly intractable distributions is challenging because the\nintractable normalizing functions of these models include parameters of\ninterest. Previous auxiliary variable MCMC algorithms are infeasible for\nmulti-dimensional models with large data sets because they depend on expensive\nauxiliary variable simulation at each iteration. We develop a fast Bayesian\nindirect algorithm by replacing an expensive auxiliary variable simulation from\na probability model with a computationally cheap simulation from a surrogate\nmodel. We learn the relationship between the surrogate model parameters and the\nprobability model parameters using Gaussian process approximations. We apply\nour methods to challenging simulated and real data examples, and illustrate\nthat the algorithm addresses both computational and inferential challenges for\ndoubly intractable distributions. Especially for a large social network model\nwith 10 parameters, we show that our method can reduce computing time from\nabout 2 weeks to 5 hours, compared to the previous method. Our method allows\npractitioners to carry out Bayesian inference for more complex models with\nlarger data sets than before.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:43:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Park", "Jaewoo", ""]]}, {"id": "2008.02906", "submitter": "Kengo Kamatani", "authors": "Alexandros Beskos and Kengo Kamatani", "title": "MCMC Algorithms for Posteriors on Matrix Spaces", "comments": "35 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Markov chain Monte Carlo (MCMC) algorithms for target distributions\ndefined on matrix spaces. Such an important sampling problem has yet to be\nanalytically explored. We carry out a major step in covering this gap by\ndeveloping the proper theoretical framework that allows for the identification\nof ergodicity properties of typical MCMC algorithms, relevant in such a\ncontext. Beyond the standard Random-Walk Metropolis (RWM) and preconditioned\nCrank--Nicolson (pCN), a contribution of this paper in the development of a\nnovel algorithm, termed the `Mixed' pCN (MpCN). RWM and pCN are shown not to be\ngeometrically ergodic for an important class of matrix distributions with heavy\ntails. In contrast, MpCN has very good empirical performance within this class.\nGeometric ergodicity for MpCN is not fully proven in this work, as some\nremaining drift conditions are quite challenging to obtain owing to the\ncomplexity of the state space. We do, however, make a lot of progress towards a\nproof, and show in detail the last steps left for future work. We illustrate\nthe computational performance of the various algorithms through simulation\nstudies, first for the trivial case of an Inverse-Wishart target, and then for\na challenging model arising in financial statistics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 00:05:38 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 15:37:49 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Beskos", "Alexandros", ""], ["Kamatani", "Kengo", ""]]}, {"id": "2008.03067", "submitter": "Antonia Mey", "authors": "Antonia S. J. S. Mey, Bryce Allen, Hannah E. Bruce Macdonald, John D.\n  Chodera, Maximilian Kuhn, Julien Michel, David L. Mobley, Levi N. Naden,\n  Samarjeet Prasad, Andrea Rizzi, Jenke Scheen, Michael R. Shirts, Gary\n  Tresadern, Huafeng Xu", "title": "Best Practices for Alchemical Free Energy Calculations", "comments": "48 pages, 14 figures", "journal-ref": null, "doi": "10.33011/livecoms.2.1.18378", "report-no": null, "categories": "q-bio.BM stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Alchemical free energy calculations are a useful tool for predicting free\nenergy differences associated with the transfer of molecules from one\nenvironment to another. The hallmark of these methods is the use of \"bridging\"\npotential energy functions representing \\emph{alchemical} intermediate states\nthat cannot exist as real chemical species. The data collected from these\nbridging alchemical thermodynamic states allows the efficient computation of\ntransfer free energies (or differences in transfer free energies) with orders\nof magnitude less simulation time than simulating the transfer process\ndirectly. While these methods are highly flexible, care must be taken in\navoiding common pitfalls to ensure that computed free energy differences can be\nrobust and reproducible for the chosen force field, and that appropriate\ncorrections are included to permit direct comparison with experimental data. In\nthis paper, we review current best practices for several popular application\ndomains of alchemical free energy calculations, including relative and absolute\nsmall molecule binding free energy calculations to biomolecular targets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:01:31 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 14:27:19 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 07:41:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Mey", "Antonia S. J. S.", ""], ["Allen", "Bryce", ""], ["Macdonald", "Hannah E. Bruce", ""], ["Chodera", "John D.", ""], ["Kuhn", "Maximilian", ""], ["Michel", "Julien", ""], ["Mobley", "David L.", ""], ["Naden", "Levi N.", ""], ["Prasad", "Samarjeet", ""], ["Rizzi", "Andrea", ""], ["Scheen", "Jenke", ""], ["Shirts", "Michael R.", ""], ["Tresadern", "Gary", ""], ["Xu", "Huafeng", ""]]}, {"id": "2008.03098", "submitter": "Vasyl Hafych", "authors": "Vasyl Hafych, Philipp Eller, Oliver Schulz and Allen Caldwell", "title": "Parallelizing MCMC Sampling via Space Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient sampling of many-dimensional and multimodal density functions is a\ntask of great interest in many research fields. We describe an algorithm that\nallows parallelizing inherently serial Markov chain Monte Carlo (MCMC) sampling\nby partitioning the space of the function parameters into multiple subspaces\nand sampling each of them independently. The samples of the different subspaces\nare then reweighted by their integral values and stitched back together. This\napproach allows reducing sampling wall-clock time by parallel operation. It\nalso improves sampling of multimodal target densities and results in less\ncorrelated samples. Finally, the approach yields an estimate of the integral of\nthe target density function.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 11:52:02 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hafych", "Vasyl", ""], ["Eller", "Philipp", ""], ["Schulz", "Oliver", ""], ["Caldwell", "Allen", ""]]}, {"id": "2008.03132", "submitter": "Oliver Schulz", "authors": "Oliver Schulz and Frederik Beaujean and Allen Caldwell and Cornelius\n  Grunwald and Vasyl Hafych and Kevin Kr\\\"oninger and Salvatore La Cagnina and\n  Lars R\\\"ohrig and Lolian Shtembari", "title": "BAT.jl -- A Julia-based tool for Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM cs.LG hep-ex physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the development of a multi-purpose software for Bayesian\nstatistical inference, BAT.jl, written in the Julia language. The major design\nconsiderations and implemented algorithms are summarized here, together with a\ntest suite that ensures the proper functioning of the algorithms. We also give\nan extended example from the realm of physics that demonstrates the\nfunctionalities of BAT.jl.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 12:55:52 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Schulz", "Oliver", ""], ["Beaujean", "Frederik", ""], ["Caldwell", "Allen", ""], ["Grunwald", "Cornelius", ""], ["Hafych", "Vasyl", ""], ["Kr\u00f6ninger", "Kevin", ""], ["La Cagnina", "Salvatore", ""], ["R\u00f6hrig", "Lars", ""], ["Shtembari", "Lolian", ""]]}, {"id": "2008.03626", "submitter": "Loc Tran H", "authors": "Loc Hoang Tran, Linh Hoang Tran", "title": "Directed hypergraph neural network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To deal with irregular data structure, graph convolution neural networks have\nbeen developed by a lot of data scientists. However, data scientists just have\nconcentrated primarily on developing deep neural network method for un-directed\ngraph. In this paper, we will present the novel neural network method for\ndirected hypergraph. In the other words, we will develop not only the novel\ndirected hypergraph neural network method but also the novel directed\nhypergraph based semi-supervised learning method. These methods are employed to\nsolve the node classification task. The two datasets that are used in the\nexperiments are the cora and the citeseer datasets. Among the classic directed\ngraph based semi-supervised learning method, the novel directed hypergraph\nbased semi-supervised learning method, the novel directed hypergraph neural\nnetwork method that are utilized to solve this node classification task, we\nrecognize that the novel directed hypergraph neural network achieves the\nhighest accuracies.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 01:39:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Tran", "Loc Hoang", ""], ["Tran", "Linh Hoang", ""]]}, {"id": "2008.03689", "submitter": "Huang Huang", "authors": "Huang Huang, Ying Sun, Marc G. Genton", "title": "Visualization of Covariance Structures for Multivariate Spatio-Temporal\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of multivariate space-time data collected from monitoring\nnetworks and satellites or generated from numerical models has brought much\nattention to multivariate spatio-temporal statistical models, where the\ncovariance function plays a key role in modeling, inference, and prediction.\nFor multivariate space-time data, understanding the spatio-temporal\nvariability, within and across variables, is essential in employing a realistic\ncovariance model. Meanwhile, the complexity of generic covariances often makes\nmodel fitting very challenging, and simplified covariance structures, including\nsymmetry and separability, can reduce the model complexity and facilitate the\ninference procedure. However, a careful examination of these properties is\nneeded in real applications. In the work presented here, we formally define\nthese properties for multivariate spatio-temporal random fields and use\nfunctional data analysis techniques to visualize them, hence providing\nintuitive interpretations. We then propose a rigorous rank-based testing\nprocedure to conclude whether the simplified properties of covariance are\nsuitable for the underlying multivariate space-time data. The good performance\nof our method is illustrated through synthetic data, for which we know the true\nstructure. We also investigate the covariance of bivariate wind speed, a key\nvariable in renewable energy, over a coastal and an inland area in Saudi\nArabia.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 08:33:38 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Huang", "Huang", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""]]}, {"id": "2008.04099", "submitter": "David Frazier", "authors": "David T. Frazier, Christopher Drovandi, Ruben Loaiza-Maya", "title": "Robust Approximate Bayesian Computation: An Adjustment Approach", "comments": "arXiv admin note: text overlap with arXiv:1904.04551", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to approximate Bayesian computation (ABC) that\nseeks to cater for possible misspecification of the assumed model. This new\napproach can be equally applied to rejection-based ABC and to popular\nregression adjustment ABC. We demonstrate that this new approach mitigates the\npoor performance of regression adjusted ABC that can eventuate when the model\nis misspecified. In addition, this new adjustment approach allows us to detect\nwhich features of the observed data can not be reliably reproduced by the\nassumed model. A series of simulated and empirical examples illustrate this new\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:05:54 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""], ["Loaiza-Maya", "Ruben", ""]]}, {"id": "2008.04348", "submitter": "Wei Zheng", "authors": "Xiangshun Kong and Wei Zheng", "title": "Design based incomplete U-statistics", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-statistics are widely used in fields such as economics, machine learning,\nand statistics. However, while they enjoy desirable statistical properties,\nthey have an obvious drawback in that the computation becomes impractical as\nthe data size $n$ increases. Specifically, the number of combinations, say $m$,\nthat a U-statistic of order $d$ has to evaluate is $O(n^d)$. Many efforts have\nbeen made to approximate the original U-statistic using a small subset of\ncombinations since Blom (1976), who referred to such an approximation as an\nincomplete U-statistic. To the best of our knowledge, all existing methods\nrequire $m$ to grow at least faster than $n$, albeit more slowly than $n^d$, in\norder for the corresponding incomplete U-statistic to be asymptotically\nefficient in terms of the mean squared error. In this paper, we introduce a new\ntype of incomplete U-statistic that can be asymptotically efficient, even when\n$m$ grows more slowly than $n$. In some cases, $m$ is only required to grow\nfaster than $\\sqrt{n}$. Our theoretical and empirical results both show\nsignificant improvements in the statistical efficiency of the new incomplete\nU-statistic.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 18:20:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Kong", "Xiangshun", ""], ["Zheng", "Wei", ""]]}, {"id": "2008.04847", "submitter": "Hadi Meidani", "authors": "Amir Kazemi and Hadi Meidani", "title": "IGANI: Iterative Generative Adversarial Networks for Imputation with\n  Application to Traffic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasing use of sensor data in intelligent transportation systems calls for\naccurate imputation algorithms that can enable reliable traffic management in\nthe occasional absence of data. As one of the effective imputation approaches,\ngenerative adversarial networks (GANs) are implicit generative models that can\nbe used for data imputation, which is formulated as an unsupervised learning\nproblem. This work introduces a novel iterative GAN architecture, called\nIterative Generative Adversarial Networks for Imputation (IGANI), for data\nimputation. IGANI imputes data in two steps and maintains the invertibility of\nthe generative imputer, which will be shown to be a sufficient condition for\nthe convergence of the proposed GAN-based imputation. The performance of our\nproposed method is evaluated on (1) the imputation of traffic speed data\ncollected in the city of Guangzhou in China, and the training of short-term\ntraffic prediction models using imputed data, and (2) the imputation of\nmulti-variable traffic data of highways in Portland-Vancouver metropolitan\nregion which includes volume, occupancy, and speed with different missing rates\nfor each of them. It is shown that our proposed algorithm mostly produces more\naccurate results compared to those of previous GAN-based imputation\narchitectures.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 16:46:02 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 19:47:56 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 16:37:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kazemi", "Amir", ""], ["Meidani", "Hadi", ""]]}, {"id": "2008.05021", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar, Mookyong Son, Shrijita Bhattacharya, Tapabrata Maiti", "title": "A Fast and Calibrated Computer Model Emulator: An Empirical Bayes\n  Approach", "comments": null, "journal-ref": "Stat Comput 31, 49 (2021)", "doi": "10.1007/s11222-021-10024-8", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mathematical models implemented on a computer have become the driving force\nbehind the acceleration of the cycle of scientific processes. This is because\ncomputer models are typically much faster and economical to run than physical\nexperiments. In this work, we develop an empirical Bayes approach to\npredictions of physical quantities using a computer model, where we assume that\nthe computer model under consideration needs to be calibrated and is\ncomputationally expensive. We propose a Gaussian process emulator and a\nGaussian process model for the systematic discrepancy between the computer\nmodel and the underlying physical process. This allows for closed-form and\neasy-to-compute predictions given by a conditional distribution induced by the\nGaussian processes. We provide a rigorous theoretical justification of the\nproposed approach by establishing posterior consistency of the estimated\nphysical process. The computational efficiency of the methods is demonstrated\nin an extensive simulation study and a real data example. The newly established\napproach makes enhanced use of computer models both from practical and\ntheoretical standpoints.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 22:26:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:59:41 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Son", "Mookyong", ""], ["Bhattacharya", "Shrijita", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2008.05310", "submitter": "Sirio Legramanti", "authors": "Sirio Legramanti", "title": "Variational Bayes for Gaussian Factor Models under the Cumulative\n  Shrinkage Process", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative shrinkage process is an increasing shrinkage prior that can be\nemployed within models in which additional terms are supposed to play a\nprogressively negligible role. A natural application is to Gaussian factor\nmodels, where such a process has proved effective in inducing parsimonious\nrepresentations while providing accurate inference on the data covariance\nmatrix. The cumulative shrinkage process came with an adaptive Gibbs sampler\nthat tunes the number of latent factors throughout iterations, which makes it\nfaster than the non-adaptive Gibbs sampler. In this work we propose a\nvariational algorithm for Gaussian factor models endowed with a cumulative\nshrinkage process. Such a strategy provides comparable inference with respect\nto the adaptive Gibbs sampler and further reduces runtime\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 13:36:19 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Legramanti", "Sirio", ""]]}, {"id": "2008.05506", "submitter": "Raphael Saavedra", "authors": "Guilherme Bodin, Raphael Saavedra, Cristiano Fernandes, Alexandre\n  Street", "title": "ScoreDrivenModels.jl: a Julia Package for Generalized Autoregressive\n  Score Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-driven models, also known as generalized autoregressive score models,\nrepresent a class of observation-driven time series models. They possess\npowerful properties, such as the ability to model different conditional\ndistributions and to consider time-varying parameters within a flexible\nframework. In this paper, we present ScoreDrivenModels.jl, an open-source Julia\npackage for modeling, forecasting, and simulating time series using the\nframework of score-driven models. The package is flexible with respect to model\ndefinition, allowing the user to specify the lag structure and which parameters\nare time-varying or constant. It is also possible to consider several\ndistributions, including Beta, Exponential, Gamma, Lognormal, Normal, Poisson,\nStudent's t, and Weibull. The provided interface is flexible, allowing\ninterested users to implement any desired distribution and parametrization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 18:12:30 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 10:15:58 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Bodin", "Guilherme", ""], ["Saavedra", "Raphael", ""], ["Fernandes", "Cristiano", ""], ["Street", "Alexandre", ""]]}, {"id": "2008.05793", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli, Alain Durmus, Ana F. Vidal, Marcelo Pereyra", "title": "Maximum likelihood estimation of regularisation parameters in\n  high-dimensional inverse problems: an empirical Bayesian approach. Part II:\n  Theoretical Analysis", "comments": "SIIMS 2020 - 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed theoretical analysis of the three stochastic\napproximation proximal gradient algorithms proposed in our companion paper [49]\nto set regularization parameters by marginal maximum likelihood estimation. We\nprove the convergence of a more general stochastic approximation scheme that\nincludes the three algorithms of [49] as special cases. This includes\nasymptotic and non-asymptotic convergence results with natural and easily\nverifiable conditions, as well as explicit bounds on the convergence rates.\nImportantly, the theory is also general in that it can be applied to other\nintractable optimisation problems. A main novelty of the work is that the\nstochastic gradient estimates of our scheme are constructed from inexact\nproximal Markov chain Monte Carlo samplers. This allows the use of samplers\nthat scale efficiently to large problems and for which we have precise\ntheoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:10:00 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Vidal", "Ana F.", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2008.05926", "submitter": "Berent {\\AA}nund Str{\\o}mnes Lunde", "authors": "Berent {\\AA}nund Str{\\o}mnes Lunde, Tore Selland Kleppe, Hans Julius\n  Skaug", "title": "An information criterion for automatic gradient tree boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An information theoretic approach to learning the complexity of\nclassification and regression trees and the number of trees in gradient tree\nboosting is proposed. The optimism (test loss minus training loss) of the\ngreedy leaf splitting procedure is shown to be the maximum of a\nCox-Ingersoll-Ross process, from which a generalization-error based information\ncriterion is formed. The proposed procedure allows fast local model selection\nwithout cross validation based hyper parameter tuning, and hence efficient and\nautomatic comparison among the large number of models performed during each\nboosting iteration. Relative to xgboost, speedups on numerical experiments\nranges from around 10 to about 1400, at similar predictive-power measured in\nterms of test-loss.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:24:27 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Lunde", "Berent \u00c5nund Str\u00f8mnes", ""], ["Kleppe", "Tore Selland", ""], ["Skaug", "Hans Julius", ""]]}, {"id": "2008.06368", "submitter": "Fabian Wagner", "authors": "Fabian Wagner, Jonas Latz, Iason Papaioannou, Elisabeth Ullmann", "title": "Error analysis for probabilities of rare events with approximate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of the probability of rare events is an important task in\nreliability and risk assessment. We consider failure events that are expressed\nin terms of a limit-state function, which depends on the solution of a partial\ndifferential equation (PDE). In many applications, the PDE cannot be solved\nanalytically. We can only evaluate an approximation of the exact PDE solution.\nTherefore, the probability of rare events is estimated with respect to an\napproximation of the limit-state function. This leads to an approximation error\nin the estimate of the probability of rare events. Indeed, we prove an error\nbound for the approximation error of the probability of failure, which behaves\nlike the discretization accuracy of the PDE multiplied by an approximation of\nthe probability of failure, the first order reliability method (FORM) estimate.\nThis bound requires convexity of the failure domain. For non-convex failure\ndomains, we prove an error bound for the relative error of the FORM estimate.\nHence, we derive a relationship between the required accuracy of the\nprobability of rare events estimate and the PDE discretization level. This\nrelationship can be used to guide practicable reliability analyses and, for\ninstance, multilevel methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 13:37:02 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 08:41:50 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 11:53:57 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Wagner", "Fabian", ""], ["Latz", "Jonas", ""], ["Papaioannou", "Iason", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "2008.06858", "submitter": "Denis Belomestny", "authors": "D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov, S. Samsonov", "title": "Variance reduction for dependent sequences with applications to\n  Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel and practical variance reduction approach\nfor additive functionals of dependent sequences. Our approach combines the use\nof control variates with the minimisation of an empirical variance estimate. We\nanalyse finite sample properties of the proposed method and derive finite-time\nbounds of the excess asymptotic variance to zero. We apply our methodology to\nStochastic Gradient MCMC (SGMCMC) methods for Bayesian inference on large data\nsets and combine it with existing variance reduction methods for SGMCMC. We\npresent empirical results carried out on a number of benchmark examples showing\nthat our variance reduction method achieves significant improvement as compared\nto state-of-the-art methods at the expense of a moderate increase of\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 08:33:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""]]}, {"id": "2008.06888", "submitter": "JinGuo Liu", "authors": "Jin-Guo Liu, Lei Wang and Pan Zhang", "title": "Tropical Tensor Network for Ground States of Spin Glasses", "comments": "code: https://github.com/TensorBFS/TropicalTensors.jl", "journal-ref": "Phys. Rev. Lett. 126, 090506 (2021)", "doi": "10.1103/PhysRevLett.126.090506", "report-no": null, "categories": "cond-mat.stat-mech quant-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unified exact tensor network approach to compute the ground\nstate energy, identify the optimal configuration, and count the number of\nsolutions for spin glasses. The method is based on tensor networks with the\nTropical Algebra defined on the semiring. Contracting the tropical tensor\nnetwork gives the ground state energy; differentiating through the tensor\nnetwork contraction gives the ground state configuration; mixing the tropical\nalgebra and the ordinary algebra counts the ground state degeneracy. The\napproach brings together the concepts from graphical models, tensor networks,\ndifferentiable programming, and quantum circuit simulation, and easily utilizes\nthe computational power of graphical processing units (GPUs). For applications,\nwe compute the exact ground state energy of Ising spin glasses on square\nlattice up to 1024 spins, on cubic lattice up to 216 spins, and on 3 regular\nrandom graphs up to 220 spins, on a single GPU; We obtain exact ground state\nenergy of (+/-)J Ising spin glass on the chimera graph of D-Wave quantum\nannealer of 512 qubits in less than 100 seconds and investigate the exact value\nof the residual entropy of (+/-)J spin glasses on the chimera graph; Finally,\nwe investigate ground-state energy and entropy of 3-state Potts glasses on\nsquare lattices up to size 18 x 18. Our approach provides baselines and\nbenchmarks for exact algorithms for spin glasses and combinatorial optimization\nproblems, and for evaluating heuristic algorithms and mean-field theories.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 11:50:23 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 19:18:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Liu", "Jin-Guo", ""], ["Wang", "Lei", ""], ["Zhang", "Pan", ""]]}, {"id": "2008.07214", "submitter": "Yunxiao Chen", "authors": "Siliang Zhang and Yunxiao Chen", "title": "Computation for Latent Variable Model Estimation: A Unified Stochastic\n  Proximal Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models have been playing a central role in psychometrics and\nrelated fields. In many modern applications, the inference based on latent\nvariable models involves one or several of the following features: (1) the\npresence of many latent variables, (2) the observed and latent variables being\ncontinuous, discrete, or a combination of both, (3) constraints on parameters,\nand (4) penalties on parameters to impose model parsimony. The estimation often\ninvolves maximizing an objective function based on a marginal\nlikelihood/pseudo-likelihood, possibly with constraints and/or penalties on\nparameters. Solving this optimization problem is highly non-trivial, due to the\ncomplexities brought by the features mentioned above. Although several\nefficient algorithms have been proposed, there lacks a unified computational\nframework that takes all these features into account. In this paper, we fill\nthe gap. Specifically, we provide a unified formulation for the optimization\nproblem and then propose a quasi-Newton stochastic proximal algorithm.\nTheoretical properties of the proposed algorithms are established. The\ncomputational efficiency and robustness are shown by simulation studies under\nvarious settings for latent variable model estimation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 10:49:29 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:00:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Siliang", ""], ["Chen", "Yunxiao", ""]]}, {"id": "2008.07733", "submitter": "Edgar Merkle", "authors": "Edgar C. Merkle and Ellen Fitzsimmons and James Uanhoro and Ben\n  Goodrich", "title": "Efficient Bayesian Structural Equation Modeling in Stan", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models comprise a large class of popular statistical\nmodels, including factor analysis models, certain mixed models, and extensions\nthereof. Model estimation is complicated by the fact that we typically have\nmultiple interdependent response variables and multiple latent variables (which\nmay also be called random effects or hidden variables), often leading to slow\nand inefficient MCMC samples. In this paper, we describe and illustrate a\ngeneral, efficient approach to Bayesian SEM estimation in Stan, contrasting it\nwith previous implementations in R package blavaan (Merkle & Rosseel, 2018).\nAfter describing the approaches in detail, we conduct a practical comparison\nunder multiple scenarios. The comparisons show that the new approach is clearly\nbetter. We also discuss ways that the approach may be extended to other models\nthat are of interest to psychometricians.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 04:17:54 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Merkle", "Edgar C.", ""], ["Fitzsimmons", "Ellen", ""], ["Uanhoro", "James", ""], ["Goodrich", "Ben", ""]]}, {"id": "2008.07803", "submitter": "Hamza M. Ruzayqat", "authors": "Alexandros Beskos, Dan Crisan, Ajay Jasra, Nikolas Kantas, Hamza\n  Ruzayqat", "title": "Score-Based Parameter Estimation for a Class of Continuous-Time State\n  Space Models", "comments": "32 pages, 32 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of parameter estimation for a class of\ncontinuous-time state space models. In particular, we explore the case of a\npartially observed diffusion, with data also arriving according to a diffusion\nprocess. Based upon a standard identity of the score function, we consider two\nparticle filter based methodologies to estimate the score function. Both\nmethods rely on an online estimation algorithm for the score function of\n$\\mathcal{O}(N^2)$ cost, with $N\\in\\mathbb{N}$ the number of particles. The\nfirst approach employs a simple Euler discretization and standard particle\nsmoothers and is of cost $\\mathcal{O}(N^2 + N\\Delta_l^{-1})$ per unit time,\nwhere $\\Delta_l=2^{-l}$, $l\\in\\mathbb{N}_0$, is the time-discretization step.\nThe second approach is new and based upon a novel diffusion bridge\nconstruction. It yields a new backward type Feynman-Kac formula in\ncontinuous-time for the score function and is presented along with a particle\nmethod for its approximation. Considering a time-discretization, the cost is\n$\\mathcal{O}(N^2\\Delta_l^{-1})$ per unit time. To improve computational costs,\nwe then consider multilevel methodologies for the score function. We illustrate\nour parameter estimation method via stochastic gradient approaches in several\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 08:59:49 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 07:11:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Beskos", "Alexandros", ""], ["Crisan", "Dan", ""], ["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""], ["Ruzayqat", "Hamza", ""]]}, {"id": "2008.07843", "submitter": "Matthias Sachs", "authors": "Gregory Herschlag, Jonathan C. Mattingly, Matthias Sachs, Evan Wyse", "title": "Non-reversible Markov chain Monte Carlo for sampling of districting maps", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the degree of partisan districting (Gerrymandering) in a\nstatistical framework typically requires an ensemble of districting plans which\nare drawn from a prescribed probability distribution that adheres to a\nrealistic and non-partisan criteria. In this article we introduce novel\nnon-reversible Markov chain Monte-Carlo (MCMC) methods for the sampling of such\ndistricting plans which have improved mixing properties in comparison to\npreviously used (reversible) MCMC algorithms. In doing so we extend the current\nframework for construction of non-reversible Markov chains on discrete sampling\nspaces by considering a generalization of skew detailed balance. We provide a\ndetailed description of the proposed algorithms and evaluate their performance\nin numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:34:09 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Herschlag", "Gregory", ""], ["Mattingly", "Jonathan C.", ""], ["Sachs", "Matthias", ""], ["Wyse", "Evan", ""]]}, {"id": "2008.07859", "submitter": "Giles Hooker", "authors": "Giles Hooker and Hanlin Shang", "title": "Selecting the Derivative of a Functional Covariate in Scalar-on-Function\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents tests to formally choose between regression models using\ndifferent derivatives of a functional covariate in scalar-on-function\nregression. We demonstrate that for linear regression, models using different\nderivatives can be nested within a model that includes point-impact effects at\nthe end-points of the observed functions. Contrasts can then be employed to\ntest the specification of different derivatives. When nonlinear regression\nmodels are defined, we apply a $J$ test to determine the statistical\nsignificance of the nonlinear structure between a functional covariate and a\nscalar response. The finite-sample performance of these methods is verified in\nsimulation, and their practical application is demonstrated using a chemometric\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:15:25 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Hooker", "Giles", ""], ["Shang", "Hanlin", ""]]}, {"id": "2008.08044", "submitter": "Deborshee Sen", "authors": "Deborshee Sen and Theodore Papamarkou and David Dunson", "title": "Bayesian neural networks and dimensionality reduction", "comments": "29 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conducting non-linear dimensionality reduction and feature learning, it is\ncommon to suppose that the data lie near a lower-dimensional manifold. A class\nof model-based approaches for such problems includes latent variables in an\nunknown non-linear regression function; this includes Gaussian process latent\nvariable models and variational auto-encoders (VAEs) as special cases. VAEs are\nartificial neural networks (ANNs) that employ approximations to make\ncomputation tractable; however, current implementations lack adequate\nuncertainty quantification in estimating the parameters, predictive densities,\nand lower-dimensional subspace, and can be unstable and lack interpretability\nin practice. We attempt to solve these problems by deploying Markov chain Monte\nCarlo sampling algorithms (MCMC) for Bayesian inference in ANN models with\nlatent variables. We address issues of identifiability by imposing constraints\non the ANN parameters as well as by using anchor points. This is demonstrated\non simulated and real data examples. We find that current MCMC sampling schemes\nface fundamental challenges in neural networks involving latent variables,\nmotivating new research directions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:11:07 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 15:47:32 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Sen", "Deborshee", ""], ["Papamarkou", "Theodore", ""], ["Dunson", "David", ""]]}, {"id": "2008.08051", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "On dropping the first Sobol' point", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) points are a substitute for plain Monte Carlo (MC)\npoints that greatly improve integration accuracy under mild assumptions on the\nproblem. Because QMC can give errors that are $o(1/n)$ as $n\\to\\infty$,\nchanging even one point can change the estimate by an amount much larger than\nthe error would have been and worsen the convergence rate. As a result, certain\npractices that fit quite naturally and intuitively with MC points are very\ndetrimental to QMC performance. These include thinning, burn-in, and taking\nsample sizes such as powers of $10$, other than the ones for which the QMC\npoints were designed. This article looks at the effects of a common practice in\nwhich one skips the first point of a Sobol' sequence. The retained points\nordinarily fail to be a digital net and when scrambling is applied, skipping\nover the first point can increase the numerical error by a factor proportional\nto $\\sqrt{n}$ where $n$ is the number of function evaluations used.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:29:48 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 22:20:47 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "2008.08080", "submitter": "Raphael Sonabend", "authors": "Raphael Sonabend, Franz J. Kir\\'aly, Andreas Bender, Bernd Bischl,\n  Michel Lang", "title": "mlr3proba: An R Package for Machine Learning in Survival Analysis", "comments": "Submitted to Bioinformatics", "journal-ref": null, "doi": "10.1093/bioinformatics/btab039", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning has become increasingly popular over the last few\ndecades, so too has the number of machine learning interfaces for implementing\nthese models. Whilst many R libraries exist for machine learning, very few\noffer extended support for survival analysis. This is problematic considering\nits importance in fields like medicine, bioinformatics, economics, engineering,\nand more. mlr3proba provides a comprehensive machine learning interface for\nsurvival analysis and connects with mlr3's general model tuning and\nbenchmarking facilities to provide a systematic infrastructure for survival\nmodeling and evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:21:24 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 11:41:25 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Sonabend", "Raphael", ""], ["Kir\u00e1ly", "Franz J.", ""], ["Bender", "Andreas", ""], ["Bischl", "Bernd", ""], ["Lang", "Michel", ""]]}, {"id": "2008.08176", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "Mixed Portmanteau Tests for Simultaneous Linear and Nonlinear Dependency\n  in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Omnibus portmanteau tests, for detecting simultaneous linear and nonlinear\ndependence structures in time series, are proposed. The tests are based on\ncombining the autocorrelation function of the conditional residuals, the\nautocorrelation function of the conditional square residuals, and the\ncross-correlation function between the conditional residuals and their squares.\nThe quasi maximum likelihood estimate is used to derive the asymptotic\ndistribution as a chi-squared distribution under a general class of time series\nmodels including ARMA, arch, and other linear and nonlinear models. The\nsimulation results show that the proposed tests successfully control the Type I\nerror probability and tend to be more powerful than other tests in many cases.\nThe efficacy of the proposed tests is demonstrated through the analysis of\nFacebook Inc., daily log returns.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 22:48:24 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 22:26:50 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2008.08240", "submitter": "Julius Juodakis", "authors": "Julius Juodakis and Stephen Marsland", "title": "Epidemic changepoint detection in the presence of nuisance changes", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time series problems feature epidemic changes - segments where a\nparameter deviates from a background baseline. The number and location of such\nchanges can be estimated in a principled way by existing detection methods,\nproviding that the background level is stable and known. However, practical\ndata often contains nuisance changes in background level, which interfere with\nstandard estimation techniques. Furthermore, such changes often differ from the\ntarget segments only in duration, and appear as false alarms in the detection\nresults. To solve these issues, we propose a two-level detector that models and\nseparates nuisance and signal changes. As part of this method, we developed a\nnew, efficient approach to simultaneously estimate unknown, but fixed,\nbackground level and detect epidemic changes. The analytic and computational\nproperties of the proposed methods are established, including consistency and\nconvergence. We demonstrate via simulations that our two-level detector\nprovides accurate estimation of changepoints under a nuisance process, while\nother state-of-the-art detectors fail. Using real-world genomic and demographic\ndatasets, we demonstrate that our method can identify and localise target\nevents while separating out seasonal variations and experimental artefacts.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:32:51 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Juodakis", "Julius", ""], ["Marsland", "Stephen", ""]]}, {"id": "2008.09239", "submitter": "Jing Liu", "authors": "Jing Liu, Aditya Deshmukh, Venugopal V. Veeravalli", "title": "Robust Mean Estimation in High Dimensions via $\\ell_0$ Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robust mean estimation problem in high dimensions, where $\\alpha\n<0.5$ fraction of the data points can be arbitrarily corrupted. Motivated by\ncompressive sensing, we formulate the robust mean estimation problem as the\nminimization of the $\\ell_0$-`norm' of the outlier indicator vector, under\nsecond moment constraints on the inlier data points. We prove that the global\nminimum of this objective is order optimal for the robust mean estimation\nproblem, and we propose a general framework for minimizing the objective. We\nfurther leverage the $\\ell_1$ and $\\ell_p$ $(0<p<1)$, minimization techniques\nin compressive sensing to provide computationally tractable solutions to the\n$\\ell_0$ minimization problem. Both synthetic and real data experiments\ndemonstrate that the proposed algorithms significantly outperform\nstate-of-the-art robust mean estimation methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:19:48 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Liu", "Jing", ""], ["Deshmukh", "Aditya", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2008.09360", "submitter": "Mathias Rousset", "authors": "Pierre Monmarch\\'e (LJLL (UMR\\_7598), LCT), Mathias Rousset (IRMAR,\n  SIMSMART, UNIV-RENNES), Pierre-Andr\\'e Zitt (LAMA, UNIV GUSTAVE EIFFEL)", "title": "Exact targeting of Gibbs distributions using velocity-jump processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces and studies a new family of velocity jump Markov\nprocesses directly amenable to exact simulation with the following two\nproperties: i) trajectories converge in law when a time-step parameter vanishes\ntowards a given Langevin or Hamil-tonian dynamics; ii) the stationary\ndistribution of the process is always exactly given by the product of a\nGaussian (for velocities) by any target log-density whose gradient is pointwise\ncomputabe together with some additional explicit appropriate upper bound. The\nprocess does not exhibit any velocity reflections (jump sizes can be\ncontrolled) and is suitable for the 'factorization method'. We provide a\nrigorous mathematical proof of: i) the small time-step convergence towards\nHamiltonian/Langevin dynamics, as well as ii) the exponentially fast\nconvergence towards the target distribution when suitable noise on velocity is\npresent. Numerical implementation is detailed and illustrated.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 08:14:27 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 07:18:21 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Monmarch\u00e9", "Pierre", "", "LJLL"], ["Rousset", "Mathias", "", "IRMAR,\n  SIMSMART, UNIV-RENNES"], ["Zitt", "Pierre-Andr\u00e9", "", "LAMA, UNIV GUSTAVE EIFFEL"]]}, {"id": "2008.09589", "submitter": "Amir Shahmoradi", "authors": "Amir Shahmoradi, Fatemeh Bagheri", "title": "ParaDRAM: A Cross-Language Toolbox for Parallel High-Performance\n  Delayed-Rejection Adaptive Metropolis Markov Chain Monte Carlo Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE astro-ph.IM physics.data-an stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present ParaDRAM, a high-performance Parallel Delayed-Rejection Adaptive\nMetropolis Markov Chain Monte Carlo software for optimization, sampling, and\nintegration of mathematical objective functions encountered in scientific\ninference. ParaDRAM is currently accessible from several popular programming\nlanguages including C/C++, Fortran, MATLAB, Python and is part of the ParaMonte\nopen-source project with the following principal design goals: 1. full\nautomation of Monte Carlo simulations, 2. interoperability of the core library\nwith as many programming languages as possible, thus, providing a unified\nApplication Programming Interface and Monte Carlo simulation environment across\nall programming languages, 3. high-performance 4. parallelizability and\nscalability of simulations from personal laptops to supercomputers, 5.\nvirtually zero-dependence on external libraries, 6. fully-deterministic\nreproducibility of simulations, 7. automatic comprehensive reporting and\npost-processing of the simulation results. We present and discuss several novel\ntechniques implemented in ParaDRAM to automatically and dynamically ensure the\ngood-mixing and the diminishing-adaptation of the resulting pseudo-Markov\nchains from ParaDRAM. We also discuss the implementation of an efficient data\nstorage method used in ParaDRAM that reduces the average memory and storage\nrequirements of the algorithm by, a factor of 4 for simple simulation problems,\nto an order of magnitude and more for sampling complex high-dimensional\nmathematical objective functions. Finally, we discuss how the design goals of\nParaDRAM can help users readily and efficiently solve a variety of machine\nlearning and scientific inference problems on a wide range of computing\nplatforms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 17:29:24 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Shahmoradi", "Amir", ""], ["Bagheri", "Fatemeh", ""]]}, {"id": "2008.09633", "submitter": "Renato J Cintra", "authors": "A. Borges Jr., R. J. Cintra, D. F. G. Coelho, V. S. Dimitrov", "title": "Low-complexity Architecture for AR(1) Inference", "comments": "7 pages, 3 tables, 4 figures", "journal-ref": "Electronics Letters 56 (14), 732-734, 2020", "doi": "10.1049/el.2019.4030", "report-no": null, "categories": "eess.SP cs.AR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Letter, we propose a low-complexity estimator for the correlation\ncoefficient based on the signed $\\operatorname{AR}(1)$ process. The introduced\napproximation is suitable for implementation in low-power hardware\narchitectures. Monte Carlo simulations reveal that the proposed estimator\nperforms comparably to the competing methods in literature with maximum error\nin order of $10^{-2}$. However, the hardware implementation of the introduced\nmethod presents considerable advantages in several relevant metrics, offering\nmore than 95% reduction in dynamic power and doubling the maximum operating\nfrequency when compared to the reference method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:16:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Borges", "A.", "Jr."], ["Cintra", "R. J.", ""], ["Coelho", "D. F. G.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "2008.09650", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki and Tom\\'a\\v{s} Mrkvi\\v{c}ka", "title": "Comparison of non-parametric global envelopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a simulation study to compare different non-parametric\nglobal envelopes that are refinements of the rank envelope proposed by\nMyllym\\\"aki et al. (2017, Global envelope tests for spatial processes, J. R.\nStatist. Soc. B 79, 381-404, doi: 10.1111/rssb.12172). The global envelopes are\nconstructed for a set of functions or vectors. For a large number of vectors,\nall the refinements lead to the same outcome as the global rank envelope. For\nsmaller numbers of vectors the refinement playes a role, where different\nrefinements are sensitive to different types of extremeness of a vector among\nthe set of vectors. The performance of the different alternatives are compared\nin a simulation study with respect to the numbers of available vectors, the\ndimensionality of the vectors, the amount of dependence between the vector\nelements and the expected type of extremeness.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 18:57:47 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""]]}, {"id": "2008.10437", "submitter": "Jake Grainger", "authors": "Jake P. Grainger, Adam M. Sykulski, Philip Jonathan and Kevin Ewans", "title": "Estimating the parameters of ocean wave spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind-generated waves are often treated as stochastic processes. There is\nparticular interest in their spectral density functions, which are often\nexpressed in some parametric form. Such spectral density functions are used as\ninputs when modelling structural response or other engineering concerns.\nTherefore, accurate and precise recovery of the parameters of such a form, from\nobserved wave records, is important. Current techniques are known to struggle\nwith recovering certain parameters, especially the peak enhancement factor and\nspectral tail decay. We introduce an approach from the statistical literature,\nknown as the de-biased Whittle likelihood, and address some practical concerns\nregarding its implementation in the context of wind-generated waves. We\ndemonstrate, through numerical simulation, that the de-biased Whittle\nlikelihood outperforms current techniques, such as least squares fitting, both\nin terms of accuracy and precision of the recovered parameters. We also provide\na method for estimating the uncertainty of parameter estimates. We perform an\nexample analysis on a data-set recorded off the coast of New Zealand, to\nillustrate some of the extra practical concerns that arise when estimating the\nparameters of spectra from observed data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:42:54 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 10:55:40 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Grainger", "Jake P.", ""], ["Sykulski", "Adam M.", ""], ["Jonathan", "Philip", ""], ["Ewans", "Kevin", ""]]}, {"id": "2008.10547", "submitter": "William Stephenson", "authors": "William T. Stephenson, Madeleine Udell, Tamara Broderick", "title": "Approximate Cross-Validation with Low-Rank Data in High Dimensions", "comments": "19 pages, 6 figures", "journal-ref": "Advances in Neural Information Processing Systems 33 (NeurIPS\n  2020)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in machine learning are driven by a challenging\ntrifecta: large data size $N$; high dimensions; and expensive algorithms. In\nthis setting, cross-validation (CV) serves as an important tool for model\nassessment. Recent advances in approximate cross validation (ACV) provide\naccurate approximations to CV with only a single model fit, avoiding\ntraditional CV's requirement for repeated runs of expensive algorithms.\nUnfortunately, these ACV methods can lose both speed and accuracy in high\ndimensions -- unless sparsity structure is present in the data. Fortunately,\nthere is an alternative type of simplifying structure that is present in most\ndata: approximate low rank (ALR). Guided by this observation, we develop a new\nalgorithm for ACV that is fast and accurate in the presence of ALR data. Our\nfirst key insight is that the Hessian matrix -- whose inverse forms the\ncomputational bottleneck of existing ACV methods -- is ALR. We show that,\ndespite our use of the \\emph{inverse} Hessian, a low-rank approximation using\nthe largest (rather than the smallest) matrix eigenvalues enables fast,\nreliable ACV. Our second key insight is that, in the presence of ALR data,\nerror in existing ACV methods roughly grows with the (approximate, low) rank\nrather than with the (full, high) dimension. These insights allow us to prove\ntheoretical guarantees on the quality of our proposed algorithm -- along with\nfast-to-compute upper bounds on its error. We demonstrate the speed and\naccuracy of our method, as well as the usefulness of our bounds, on a range of\nreal and simulated data sets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 16:34:05 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Stephenson", "William T.", ""], ["Udell", "Madeleine", ""], ["Broderick", "Tamara", ""]]}, {"id": "2008.11155", "submitter": "Andr\\'e Mas", "authors": "Cl\\'{e]ment Carr\\'e and Andr\\'e Mas", "title": "Prediction of Hilbertian autoregressive processes : a Recurrent Neural\n  Network approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autoregressive Hilbertian model (ARH) was introduced in the early 90's by\nDenis Bosq. It was the subject of a vast literature and gave birth to numerous\nextensions. The model generalizes the classical multidimensional autoregressive\nmodel, widely used in Time Series Analysis. It was successfully applied in\nnumerous fields such as finance, industry, biology. We propose here to compare\nthe classical prediction methodology based on the estimation of the\nautocorrelation operator with a neural network learning approach. The latter is\nbased on a popular version of Recurrent Neural Networks : the Long Short Term\nMemory networks. The comparison is carried out through simulations and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:43:24 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Carr\u00e9", "Cl\\'{e]ment", ""], ["Mas", "Andr\u00e9", ""]]}, {"id": "2008.12098", "submitter": "Benjamin Baumer", "authors": "Audrey M. Bertin, Benjamin S. Baumer", "title": "Creating optimal conditions for reproducible data analysis in R with\n  'fertile'", "comments": "17 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advancement of scientific knowledge increasingly depends on ensuring that\ndata-driven research is reproducible: that two people with the same data obtain\nthe same results. However, while the necessity of reproducibility is clear,\nthere are significant behavioral and technical challenges that impede its\nwidespread implementation, and no clear consensus on standards of what\nconstitutes reproducibility in published research. We present fertile, an R\npackage that focuses on a series of common mistakes programmers make while\nconducting data science projects in R, primarily through the RStudio integrated\ndevelopment environment. fertile operates in two modes: proactively (to prevent\nreproducibility mistakes from happening in the first place), and retroactively\n(analyzing code that is already written for potential problems). Furthermore,\nfertile is designed to educate users on why their mistakes are problematic and\nhow to fix them.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 18:55:19 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Bertin", "Audrey M.", ""], ["Baumer", "Benjamin S.", ""]]}, {"id": "2008.12625", "submitter": "Berent {\\AA}nund Str{\\o}mnes Lunde", "authors": "Berent {\\AA}nund Str{\\o}mnes Lunde, Tore Selland Kleppe", "title": "agtboost: Adaptive and Automatic Gradient Tree Boosting Computations", "comments": "16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  agtboost is an R package implementing fast gradient tree boosting\ncomputations in a manner similar to other established frameworks such as\nxgboost and LightGBM, but with significant decreases in computation time and\nrequired mathematical and technical knowledge. The package automatically takes\ncare of split/no-split decisions and selects the number of trees in the\ngradient tree boosting ensemble, i.e., agtboost adapts the complexity of the\nensemble automatically to the information in the data. All of this is done\nduring a single training run, which is made possible by utilizing developments\nin information theory for tree algorithms {\\tt arXiv:2008.05926v1 [stat.ME]}.\nagtboost also comes with a feature importance function that eliminates the\ncommon practice of inserting noise features. Further, a useful model validation\nfunction performs the Kolmogorov-Smirnov test on the learned distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:42:19 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lunde", "Berent \u00c5nund Str\u00f8mnes", ""], ["Kleppe", "Tore Selland", ""]]}, {"id": "2008.12662", "submitter": "Radu V. Craiu", "authors": "Radu V. Craiu and Xiao-Li Meng", "title": "Double Happiness: Enhancing the Coupled Gains of L-lag Coupling via\n  Control Variates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently proposed L-lag coupling for unbiased Markov chain Monte Carlo\n(MCMC) calls for a joint celebration by MCMC practitioners and theoreticians.\nFor practitioners, it circumvents the thorny issue of deciding the burn-in\nperiod or when to terminate an MCMC sampling process, and opens the door for\nsafe parallel implementation. For theoreticians, it provides a powerful tool to\nestablish elegant and easily estimable bounds on the exact error of an MCMC\napproximation at any finite number of iterates. A serendipitous observation\nabout the bias-correcting term leads us to introduce naturally available\ncontrol variates into the L-lag coupling estimators. In turn, this extension\nenhances the coupled gains of L-lag coupling, because it results in more\nefficient unbiased estimators, as well as a better bound on the total variation\nerror of MCMC iterations, albeit the gains diminish as L increases.\nSpecifically, the new upper bound is theoretically guaranteed to never exceed\nthe one given previously. We also argue that L-lag coupling represents a\ncoupling for the future, breaking from the coupling-from-the-past type of\nperfect sampling, by reducing the generally unachievable requirement of being\nperfect to one of being unbiased, a worthwhile trade-off for ease of\nimplementation in most practical situations. The theoretical analysis is\nsupported by numerical experiments that show tighter bounds and a gain in\nefficiency when control variates are introduced.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:00:47 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 22:55:11 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2021 16:04:38 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 00:21:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Craiu", "Radu V.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "2008.12682", "submitter": "Johannes Resin", "authors": "Johannes Resin", "title": "A Simple Algorithm for Exact Multinomial Tests", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new method for computing acceptance regions of exact\nmultinomial tests. From this an algorithm is derived, which finds exact\np-values for tests of simple multinomial hypotheses. Using concepts from\ndiscrete convex analysis, the method is proven to be exact for various popular\ntest statistics, including Pearson's chi-square and the log-likelihood ratio.\nThe proposed algorithm improves greatly on the naive approach using full\nenumeration of the sample space. However, its use is limited to multinomial\ndistributions with a small number of categories, as the runtime grows\nexponentially in the number of possible outcomes.\n  The method is applied in a simulation study and uses of multinomial tests in\nforecast evaluation are outlined. Additionally, properties of a test statistic\nusing probability ordering, referred to as the \"exact multinomial test\" by some\nauthors, are investigated and discussed. The algorithm is implemented in the\naccompanying R package ExactMultinom.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 14:36:10 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Resin", "Johannes", ""]]}, {"id": "2008.12857", "submitter": "Austin Cole", "authors": "D. Austin Cole, Ryan Christianson, Robert B. Gramacy", "title": "Locally induced Gaussian processes for large-scale simulation\n  experiments", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) serve as flexible surrogates for complex surfaces,\nbut buckle under the cubic cost of matrix decompositions with big training data\nsizes. Geospatial and machine learning communities suggest pseudo-inputs, or\ninducing points, as one strategy to obtain an approximation easing that\ncomputational burden. However, we show how placement of inducing points and\ntheir multitude can be thwarted by pathologies, especially in large-scale\ndynamic response surface modeling tasks. As remedy, we suggest porting the\ninducing point idea, which is usually applied globally, over to a more local\ncontext where selection is both easier and faster. In this way, our proposed\nmethodology hybridizes global inducing point and data subset-based local GP\napproximation. A cascade of strategies for planning the selection of local\ninducing points is provided, and comparisons are drawn to related methodology\nwith emphasis on computer surrogate modeling applications. We show that local\ninducing points extend their global and data-subset component parts on the\naccuracy--computational efficiency frontier. Illustrative examples are provided\non benchmark data and a large-scale real-simulation satellite drag\ninterpolation problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:37:46 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:57:43 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Cole", "D. Austin", ""], ["Christianson", "Ryan", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "2008.13424", "submitter": "Prosha Rahman", "authors": "Prosha A. Rahman, Boris Beranger, Matthew Roughan, Scott A. Sisson", "title": "Likelihood-based inference for modelling packet transit from thinned\n  flow summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The substantial growth of network traffic speed and volume presents practical\nchallenges to network data analysis. Packet thinning and flow aggregation\nprotocols such as NetFlow reduce the size of datasets by providing structured\ndata summaries, but conversely this impedes statistical inference. Methods\nwhich aim to model patterns of traffic propagation typically do not account for\nthe packet thinning and summarisation process into the analysis, and are often\nsimplistic, e.g.~method-of-moments. As a result, they can be of limited\npractical use.\n  We introduce a likelihood-based analysis which fully incorporates packet\nthinning and NetFlow summarisation into the analysis. As a result, inferences\ncan be made for models on the level of individual packets while only observing\nthinned flow summary information. We establish consistency of the resulting\nmaximum likelihood estimator, derive bounds on the volume of traffic which\nshould be observed to achieve required levels of estimator accuracy, and\nidentify an ideal family of models. The robust performance of the estimator is\nexamined through simulated analyses and an application on a publicly available\ntrace dataset containing over 36m packets over a 1 minute period.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:27:10 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rahman", "Prosha A.", ""], ["Beranger", "Boris", ""], ["Roughan", "Matthew", ""], ["Sisson", "Scott A.", ""]]}, {"id": "2008.13480", "submitter": "Christian Agrell", "authors": "Andreas Hafver, Christian Agrell, Erik Vanem", "title": "Environmental contours as Voronoi cells", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental contours are widely used as basis for design of structures\nexposed to environmental loads. The basic idea of the method is to decouple the\nenvironmental description from the structural response. This is done by\nestablishing an envelope of environmental conditions, such that any structure\ntolerating loads on this envelope will have a failure probability smaller than\na prescribed value. Specifically, given an $n$-dimensional random variable\n$\\mathbf{X}$ and a target probability of failure $p_{e}$, an environmental\ncontour is the boundary of a set $\\mathcal{B} \\subset \\mathbb{R}^{n}$ with the\nfollowing property: For any failure set $\\mathcal{F} \\subset \\mathbb{R}^{n}$,\nif $\\mathcal{F}$ does not intersect the interior of $\\mathcal{B}$, then the\nprobability of failure, $P(\\mathbf{X} \\in \\mathcal{F})$, is bounded above by\n$p_{e}$. As is common for many real-world applications, we work under the\nassumption that failure sets are convex.\n  In this paper, we show that such environmental contours may be regarded as\nboundaries of Voronoi cells. This geometric interpretation leads to new\ntheoretical insights and suggests a simple novel construction algorithm that\nguarantees the desired probabilistic properties. The method is illustrated with\nexamples in two and three dimensions, but the results extend to environmental\ncontours in arbitrary dimensions. Inspired by the Voronoi-Delaunay duality in\nthe numerical discrete scenario, we are also able to derive an analytical\nrepresentation where the environmental contour is considered as a\ndifferentiable manifold, and a criterion for its existence is established.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 10:46:02 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hafver", "Andreas", ""], ["Agrell", "Christian", ""], ["Vanem", "Erik", ""]]}]