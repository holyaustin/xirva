[{"id": "1912.00694", "submitter": "Rapha\\\"el Huser", "authors": "Rapha\\\"el Huser", "title": "Editorial: EVA 2019 data competition on spatio-temporal prediction of\n  Red Sea surface temperature extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large, non-stationary spatio-temporal data are ubiquitous in modern\nstatistical applications, and the modeling of spatio-temporal extremes is\ncrucial for assessing risks in environmental sciences among others. While the\nmodeling of extremes is challenging in itself, the prediction of rare events at\nunobserved spatial locations and time points is even more difficult. In this\neditorial, we describe the data competition that was organized for the 11th\ninternational conference on Extreme-Value Analysis (EVA 2019), for which\nseveral teams modeled and predicted Red Sea surface temperature extremes over\nspace and time. After introducing the dataset and the goal of the competition,\nwe disclose the final ranking of the teams, and we finally discuss some\ninteresting outcomes and future challenges.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 11:43:36 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Huser", "Rapha\u00ebl", ""]]}, {"id": "1912.01089", "submitter": "Zhengze Zhou", "authors": "Zhengze Zhou, Lucas Mentch, Giles Hooker", "title": "$V$-statistics and Variance Estimation", "comments": "This version supersedes the previous technical report titled\n  \"Asymptotic Normality and Variance Estimation For Supervised Ensembles\".\n  Extensive simulations are added and we also provide a more detailed\n  discussion on the bias phenomenon in variance estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for analyzing asymptotics of\n$V$-statistics. Previous literature on limiting distribution mainly focuses on\nthe cases when $n \\to \\infty$ with fixed kernel size $k$. Under some regularity\nconditions, we demonstrate asymptotic normality when $k$ grows with $n$ by\nutilizing existing results for $U$-statistics. The key in our approach lies in\na mathematical reduction to $U$-statistics by designing an equivalent kernel\nfor $V$-statistics. We also provide a unified treatment on variance estimation\nfor both $U$- and $V$-statistics by observing connections to existing methods\nand proposing an empirically more accurate estimator. Ensemble methods such as\nrandom forests, where multiple base learners are trained and aggregated for\nprediction purposes, serve as a running example throughout the paper because\nthey are a natural and flexible application of $V$-statistics.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2019 21:42:19 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 02:08:01 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Zhou", "Zhengze", ""], ["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1912.01234", "submitter": "Armin K\\\"uper", "authors": "Armin K\\\"uper and Steffen Waldherr", "title": "Numerical Gaussian process Kalman filtering", "comments": "6 pages, 3 figures, this work has been accepted by IFAC for\n  publication (\\copyright 2020 IFAC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we introduce numerical Gaussian process Kalman filtering\n(GPKF). Numerical Gaussian processes have recently been developed to simulate\nspatiotemporal models. The contribution of this paper is to embed numerical\nGaussian processes into the recursive Kalman filter equations. This embedding\nenables us to do Kalman filtering on infinite-dimensional systems using\nGaussian processes. This is possible because i) we are obtaining a linear model\nfrom numerical Gaussian processes, and ii) the states of this model are by\ndefinition Gaussian distributed random variables. Convenient properties of the\nnumerical GPKF are that no spatial discretization of the model is necessary,\nand manual setting up of the Kalman filter, that is fine-tuning the process and\nmeasurement noise levels by hand is not required, as they are learned online\nfrom the data stream. We showcase the capability of the numerical GPKF in a\nsimulation study of the advection equation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 08:09:27 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 13:27:03 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["K\u00fcper", "Armin", ""], ["Waldherr", "Steffen", ""]]}, {"id": "1912.01376", "submitter": "Haziq Jamil", "authors": "Haziq Jamil, Wicher Bergsma", "title": "iprior: An R Package for Regression Modelling using I-priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is an overview of the R package iprior, which implements a unified\nmethodology for fitting parametric and nonparametric regression models,\nincluding additive models, multilevel models, and models with one or more\nfunctional covariates. Based on the principle of maximum entropy, an I-prior is\nan objective Gaussian process prior for the regression function with covariance\nkernel equal to its Fisher information. The regression function is estimated by\nits posterior mean under the I-prior, and hyperparameters are estimated via\nmaximum marginal likelihood. Estimation of I-prior models is simple and\ninference straightforward, while small and large sample predictive performances\nare comparative, and often better, to similar leading state-of-the-art models.\nWe illustrate the use of the iprior package by analysing a simulated toy data\nset as well as three real-data examples, in particular, a multilevel data set,\na longitudinal data set, and a dataset involving a functional covariate.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2019 06:11:57 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Jamil", "Haziq", ""], ["Bergsma", "Wicher", ""]]}, {"id": "1912.01517", "submitter": "Denis Talbot", "authors": "Diop S. Arona, Duchesne Thierry, Cumming Steven, Diop Awa, Talbot\n  Denis", "title": "Confounding Adjustment Methods for Multi-level Treatment Comparisons\n  Under Lack of Positivity and Unknown Model Specification", "comments": "15 pages, 6 tables", "journal-ref": "Journal of Applied Statistics by Taylor & Francis, 2021", "doi": "10.1080/02664763.2021.1911966", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imbalances in covariates between treatment groups are frequent in\nobservational studies and can lead to biased comparisons. Various adjustment\nmethods can be employed to correct these biases in the context of multi-level\ntreatments ($>$ 2). However, analytical challenges, such as positivity\nviolations and incorrect model specification, may affect their ability to yield\nunbiased estimates. Adjustment methods that present the best potential to deal\nwith those challenges were identified: the overlap weights, augmented overlap\nweights, bias-corrected matching and targeted maximum likelihood. A simple\nvariance estimator for the overlap weight estimators that can naturally be\ncombined with machine learning algorithms is proposed. In a simulation study,\nwe investigated the empirical performance of these methods as well as those of\nsimpler alternatives, standardization, inverse probability weighting and\nmatching. Our proposed variance estimator performed well, even at a sample size\nof 500. Adjustment methods that included an outcome modeling component\nperformed better than those that only modeled the treatment mechanism.\nAdditionally, a machine learning implementation was observed to efficiently\ncompensate for the unknown model specification for the former methods, but not\nthe latter. Based on these results, the wildfire data were analyzed using the\naugmented overlap weight estimator. With respect to effectiveness of alternate\nfire-suppression interventions, the results were counter-intuitive, indeed the\nopposite of what would be expected on subject-matter grounds. This suggests the\npresence in the data of unmeasured confounding bias.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 16:43:12 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 14:25:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Arona", "Diop S.", ""], ["Thierry", "Duchesne", ""], ["Steven", "Cumming", ""], ["Awa", "Diop", ""], ["Denis", "Talbot", ""]]}, {"id": "1912.01691", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli and Agnes Desolneux and Alain Durmus and Bruno\n  Galerne and Arthur Leclaire", "title": "Maximum entropy methods for texture synthesis: theory and practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the rise of convolutional neural network techniques in\nexemplar-based image synthesis. These methods often rely on the minimization of\nsome variational formulation on the image space for which the minimizers are\nassumed to be the solutions of the synthesis problem. In this paper we\ninvestigate, both theoretically and experimentally, another framework to deal\nwith this problem using an alternate sampling/minimization scheme. First, we\nuse results from information geometry to assess that our method yields a\nprobability measure which has maximum entropy under some constraints in\nexpectation. Then, we turn to the analysis of our method and we show, using\nrecent results from the Markov chain literature, that its error can be\nexplicitly bounded with constants which depend polynomially in the dimension\neven in the non-convex setting. This includes the case where the constraints\nare defined via a differentiable neural network. Finally, we present an\nextensive experimental study of the model, including a comparison with\nstate-of-the-art methods and an extension to style transfer.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2019 21:36:44 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Desolneux", "Agnes", ""], ["Durmus", "Alain", ""], ["Galerne", "Bruno", ""], ["Leclaire", "Arthur", ""]]}, {"id": "1912.02026", "submitter": "Denis Allard", "authors": "Denis Allard, Xavier Emery, C\\'eline Lacaux, Christian Lantu\\'ejoul", "title": "Simulating space-time random fields with nonseparable Gneiting-type\n  covariance functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two algorithms are proposed to simulate space-time Gaussian random fields\nwith a covariance function belonging to an extended Gneiting class, the\ndefinition of which depends on a completely monotone function associated with\nthe spatial structure and a conditionally negative definite function associated\nwith the temporal structure. In both cases, the simulated random field is\nconstructed as a weighted sum of cosine waves, with a Gaussian spatial\nfrequency vector and a uniform phase. The difference lies in the way to handle\nthe temporal component. The first algorithm relies on a spectral decomposition\nin order to simulate a temporal frequency conditional upon the spatial one,\nwhile in the second algorithm the temporal frequency is replaced by an\nintrinsic random field whose variogram is proportional to the conditionally\nnegative definite function associated with the temporal structure. Both\nalgorithms are scalable as their computational cost is proportional to the\nnumber of space-time locations, which may be unevenly spaced in space and/or in\ntime. They are illustrated and validated through synthetic examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2019 14:45:41 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Allard", "Denis", ""], ["Emery", "Xavier", ""], ["Lacaux", "C\u00e9line", ""], ["Lantu\u00e9joul", "Christian", ""]]}, {"id": "1912.02382", "submitter": "Benjamin Seiyon Lee", "authors": "Ben Seiyon Lee and Murali Haran", "title": "PICAR: An Efficient Extendable Approach for Fitting Hierarchical Spatial\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical spatial models are very flexible and popular for a vast array of\napplications in areas such as ecology, social science, public health, and\natmospheric science. It is common to carry out Bayesian inference for these\nmodels via Markov chain Monte Carlo (MCMC). Each iteration of the MCMC\nalgorithm is computationally expensive due to costly matrix operations. In\naddition, the MCMC algorithm needs to be run for more iterations because the\nstrong cross-correlations among the spatial latent variables result in slow\nmixing Markov chains. To address these computational challenges, we propose a\nprojection-based intrinsic conditional autoregression (PICAR) approach, which\nis a discretized and dimension-reduced representation of the underlying spatial\nrandom field using empirical basis functions on a triangular mesh. Our approach\nexhibits fast mixing as well as a considerable reduction in computational cost\nper iteration. PICAR is computationally efficient and scales well to high\ndimensions. It is also automated and easy to implement for a wide array of\nuser-specified hierarchical spatial models. We show, via simulation studies,\nthat our approach performs well in terms of parameter inference and prediction.\nWe provide several examples to illustrate the applicability of our method,\nincluding (i) a high-dimensional cloud cover dataset that showcases its\ncomputational efficiency, (ii) a spatially varying coefficient model that\ndemonstrates the ease of implementation of PICAR in the probabilistic\nprogramming languages stan and nimble, and (iii) a watershed survey example\nthat illustrates how PICAR applies to models that are not amenable to efficient\ninference via existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 04:59:30 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 22:23:51 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lee", "Ben Seiyon", ""], ["Haran", "Murali", ""]]}, {"id": "1912.02517", "submitter": "Yasin Asar", "authors": "Yasin Asar and R. Arabi Belaghi", "title": "Inference for Two Lomax Populations Under Joint Type-II Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lomax distribution has been widely used in economics, business and actuarial\nsciences. Due to its importance, we consider the statistical inference of this\nmodel under joint type-II censoring scenario. In order to estimate the\nparameters, we derive the Newton-Raphson(NR) procedure and we observe that most\nof the times in the simulation NR algorithm does not converge. Consequently, we\nmake use of the expectation-maximization (EM) algorithm. Moreover, Bayesian\nestimations are also provided based on squared error, linear-exponential and\ngeneralized entropy loss functions together with the importance sampling method\ndue to the structure of posterior density function. In the sequel, we perform a\nMonte Carlo simulation experiment to compare the performances of the listed\nmethods. Mean squared error values, averages of estimated values as well as\ncoverage probabilities and average interval lengths are considered to compare\nthe performances of different methods. The approximate confidence intervals,\nbootstrap-p and bootstrap-t confidence intervals are computed for EM\nestimations. Also, Bayesian coverage probabilities and credible intervals are\nobtained. Finally, we consider the Bladder Cancer data to illustrate the\napplicability of the methods covered in the paper.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 11:49:54 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Asar", "Yasin", ""], ["Belaghi", "R. Arabi", ""]]}, {"id": "1912.02867", "submitter": "Dominika Mik\\v{s}ov\\'a", "authors": "Dominika Mik\\v{s}ov\\'a, Christopher Rieser, Peter Filzmoser", "title": "Identification of mineralization in geochemistry along a transect based\n  on the spatial curvature of log-ratios", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting subcropping mineralizations but also deeply buried mineralizations\nis one important goal in geochemical exploration. The identification of useful\nindicators for mineralization is a difficult task as mineralization might be\ninfluenced by many factors, such as location, investigated media, depth, etc.\nWe propose a statistical method which indicates chemical elements related to\nmineralization along a transect. Moreover, the method determines along a\ntransect the potential area of the deposit. The identification is based on\nGeneral Additive Models (GAMs) for the element concentrations across the\nspatial coordinate(s). The log-ratios of the GAM fits are taken to compute the\ncurvature, where high and narrow curvature is supposed to indicate the\nmineralization area. By defining a measure for the quantification of high\ncurvature, the log-ratios can be ranked, and elements can be identified that\nare indicative of the anomaly patterns.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2019 20:41:12 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mik\u0161ov\u00e1", "Dominika", ""], ["Rieser", "Christopher", ""], ["Filzmoser", "Peter", ""]]}, {"id": "1912.02982", "submitter": "Matthew Graham", "authors": "Matthew M. Graham and Alexandre H. Thiery and Alexandros Beskos", "title": "Manifold Markov chain Monte Carlo methods for Bayesian inference in a\n  wide class of diffusion models", "comments": "Updated with additional numerical experiments and improvements to\n  methodology. 50 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for nonlinear diffusions, observed at discrete times, is a\nchallenging task that has prompted the development of a number of algorithms,\nmainly within the computational statistics community. We propose a new\ndirection, and accompanying methodology, borrowing ideas from statistical\nphysics and computational chemistry, for inferring the posterior distribution\nof latent diffusion paths and model parameters, given observations of the\nprocess. Joint configurations of the underlying process noise and of\nparameters, mapping onto diffusion paths consistent with observations, form an\nimplicitly defined manifold. Then, by making use of a constrained Hamiltonian\nMonte Carlo algorithm on the embedded manifold, we are able to perform\ncomputationally efficient inference for an extensive class of discretely\nobserved diffusion models. Critically, in contrast with other approaches\nproposed in the literature, our methodology is highly automated, requiring\nminimal user intervention and applying alike in a range of settings, including:\nelliptic or hypo-elliptic systems; observations with or without noise; linear\nor non-linear observation operators. Exploiting Markovianity, we propose a\nvariant of the method with complexity that scales linearly in the resolution of\npath discretisation and the number of observation times.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 05:55:16 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 12:02:09 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Graham", "Matthew M.", ""], ["Thiery", "Alexandre H.", ""], ["Beskos", "Alexandros", ""]]}, {"id": "1912.03253", "submitter": "Daniel Sanz-Alonso", "authors": "M. P. Calvo, D. Sanz-Alonso, J. M. Sanz-Serna", "title": "HMC: avoiding rejections by not using leapfrog and some results on the\n  acceptance rate", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The leapfrog integrator is routinely used within the Hamiltonian Monte Carlo\nmethod and its variants. We give strong numerical evidence that alternative,\neasy to implement algorithms yield fewer rejections with a given computational\neffort. When the dimensionality of the target distribution is high, the number\nof accepted proposals may be multiplied by a factor of three or more. This\nincrease in the number of accepted proposals is not achieved by impairing any\npositive features of the sampling. We also establish new non-asymptotic and\nasymptotic results on the monotonic relationship between the expected\nacceptance rate and the expected energy error. These results further validate\nthe derivation of one of the integrators we consider and are of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2019 17:53:15 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 16:43:11 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 15:20:27 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Calvo", "M. P.", ""], ["Sanz-Alonso", "D.", ""], ["Sanz-Serna", "J. M.", ""]]}, {"id": "1912.03662", "submitter": "Duyeol Lee", "authors": "Duyeol Lee, Kai Zhang, and Michael R. Kosorok", "title": "The Binary Expansion Randomized Ensemble Test (BERET)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the binary expansion testing framework was introduced to test the\nindependence of two continuous random variables by utilizing symmetry\nstatistics that are complete sufficient statistics for dependence. We develop a\nnew test based on an ensemble approach that uses the sum of squared symmetry\nstatistics and distance correlation. Simulation studies suggest that this\nmethod improves the power while preserving the clear interpretation of the\nbinary expansion testing. We extend this method to tests of independence of\nrandom vectors in arbitrary dimension. Through random projections, the proposed\nbinary expansion randomized ensemble test transforms the multivariate\nindependence testing problem into a univariate problem. Simulation studies and\ndata example analyses show that the proposed method provides relatively robust\nperformance compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2019 11:54:57 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 10:25:20 GMT"}, {"version": "v3", "created": "Sun, 17 May 2020 23:37:36 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 20:08:37 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lee", "Duyeol", ""], ["Zhang", "Kai", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1912.03805", "submitter": "Scott Sisson", "authors": "Tom Whitaker and Boris Beranger and Scott A. Sisson", "title": "Logistic regression models for aggregated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression models are a popular and effective method to predict the\nprobability of categorical response data. However inference for these models\ncan become computationally prohibitive for large datasets. Here we adapt ideas\nfrom symbolic data analysis to summarise the collection of predictor variables\ninto histogram form, and perform inference on this summary dataset. We develop\nideas based on composite likelihoods to derive an efficient one-versus-rest\napproximate composite likelihood model for histogram-based random variables,\nconstructed from low-dimensional marginal histograms obtained from the full\nhistogram. We demonstrate that this procedure can achieve comparable\nclassification rates compared to the standard full data multinomial analysis\nand against state-of-the-art subsampling algorithms for logistic regression,\nbut at a substantially lower computational cost. Performance is explored\nthrough simulated examples, and analyses of large supersymmetry and satellite\ncrop classification datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2019 01:15:49 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 07:46:45 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Whitaker", "Tom", ""], ["Beranger", "Boris", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1912.04565", "submitter": "Christopher Ting", "authors": "Masaaki Kijima and Christopher Ting", "title": "Market Price of Trading Liquidity Risk and Market Depth", "comments": "46 Pages, 12 Figures, To appear in the International Journal of\n  Theoretical and Applied Finance", "journal-ref": null, "doi": "10.1142/S0219024919500456", "report-no": null, "categories": "q-fin.TR econ.EM q-fin.CP q-fin.MF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Price impact of a trade is an important element in pre-trade and post-trade\nanalyses. We introduce a framework to analyze the market price of liquidity\nrisk, which allows us to derive an inhomogeneous Bernoulli ordinary\ndifferential equation. We obtain two closed form solutions, one of which\nreproduces the linear function of the order flow in Kyle (1985) for informed\ntraders. However, when traders are not as asymmetrically informed, an S-shape\nfunction of the order flow is obtained. We perform an empirical intra-day\nanalysis on Nikkei futures to quantify the price impact of order flow and\ncompare our results with industry's heuristic price impact functions. Our model\nof order flow yields a rich framework for not only to estimate the liquidity\nrisk parameters, but also to provide a plausible cause of why volatility and\ncorrelation are stochastic in nature. Finally, we find that the market depth\nencapsulates the market price of liquidity risk.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 08:20:26 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Kijima", "Masaaki", ""], ["Ting", "Christopher", ""]]}, {"id": "1912.04681", "submitter": "Samuel Power", "authors": "Samuel Power, Jacob Vorstrup Goldman", "title": "Accelerated Sampling on Discrete Spaces with Non-Reversible Markov\n  Processes", "comments": "31 pages, 8 figures, code available at\n  https://github.com/jvorstrupgoldman/tabu_dc_dzz", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of MCMC sampling from a distribution defined on a\ndiscrete space. Building on recent insights provided in [Zan19], we devise a\nclass of efficient continuous-time, non-reversible algorithms which make active\nuse of the structure of the underlying space. Particular emphasis is placed on\nhow symmetries and other group-theoretic notions can be used to improve\nexploration of the space. We test our algorithms on a range of examples from\nstatistics, computational physics, machine learning, and cryptography, which\nshow improvement on alternative algorithms. We provide practical\nrecommendations on how to design and implement these algorithms, and close with\nremarks on the outlook for both discrete sampling and continuous-time Monte\nCarlo more broadly.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 13:39:44 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:28:11 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Power", "Samuel", ""], ["Goldman", "Jacob Vorstrup", ""]]}, {"id": "1912.04753", "submitter": "Zhipeng Gui", "authors": "Yuan Wang, Zhipeng Gui, Huayi Wu, Dehua Peng, Jinghang Wu, Zousen Cui", "title": "Optimizing and accelerating space-time Ripley's K function based on\n  Apache Spark for distributed spatiotemporal point pattern analysis", "comments": "35 pages, 23 figures, Future Generation Computer Systems", "journal-ref": "Future Generation Computer Systems, 2020", "doi": "10.1016/j.future.2019.11.036", "report-no": null, "categories": "stat.CO cs.CG cs.DC cs.DS cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With increasing point of interest (POI) datasets available with fine-grained\nspatial and temporal attributes, space-time Ripley's K function has been\nregarded as a powerful approach to analyze spatiotemporal point process.\nHowever, space-time Ripley's K function is computationally intensive for\npoint-wise distance comparisons, edge correction and simulations for\nsignificance testing. Parallel computing technologies like OpenMP, MPI and CUDA\nhave been leveraged to accelerate the K function, and related experiments have\ndemonstrated the substantial acceleration. Nevertheless, previous works have\nnot extended optimization of Ripley's K function from space dimension to\nspace-time dimension. Without sophisticated spatiotemporal query and\npartitioning mechanisms, extra computational overhead can be problematic.\nMeanwhile, these researches were limited by the restricted scalability and\nrelative expensive programming cost of parallel frameworks and impeded their\napplications for large POI dataset and Ripley's K function variations. This\npaper presents a distributed computing method to accelerate space-time Ripley's\nK function upon state-of-the-art distributed computing framework Apache Spark,\nand four strategies are adopted to simplify calculation procedures and\naccelerate distributed computing respectively. Based on the optimized method, a\nweb-based visual analytics framework prototype has been developed. Experiments\nprove the feasibility and time efficiency of the proposed method, and also\ndemonstrate its value on promoting applications of space-time Ripley's K\nfunction in ecology, geography, sociology, economics, urban transportation and\nother fields.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2019 15:15:37 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Wang", "Yuan", ""], ["Gui", "Zhipeng", ""], ["Wu", "Huayi", ""], ["Peng", "Dehua", ""], ["Wu", "Jinghang", ""], ["Cui", "Zousen", ""]]}, {"id": "1912.05153", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nhat Ho, Martin J. Wainwright, Peter L. Bartlett, Michael\n  I. Jordan", "title": "Sampling for Bayesian Mixture Models: MCMC with Polynomial-Time Mixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from the power posterior distribution in\nBayesian Gaussian mixture models, a robust version of the classical posterior.\nThis power posterior is known to be non-log-concave and multi-modal, which\nleads to exponential mixing times for some standard MCMC algorithms. We\nintroduce and study the Reflected Metropolis-Hastings Random Walk (RMRW)\nalgorithm for sampling. For symmetric two-component Gaussian mixtures, we prove\nthat its mixing time is bounded as $d^{1.5}(d + \\Vert \\theta_{0}\n\\Vert^2)^{4.5}$ as long as the sample size $n$ is of the order $d (d + \\Vert\n\\theta_{0} \\Vert^2)$. Notably, this result requires no conditions on the\nseparation of the two means. En route to proving this bound, we establish some\nnew results of possible independent interest that allow for combining\nPoincar\\'{e} inequalities for conditional and marginal densities.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 07:48:49 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mou", "Wenlong", ""], ["Ho", "Nhat", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1912.05503", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Emanuel Parzen", "title": "Nonparametric Universal Copula Modeling", "comments": "A perspective on \"60 years of Copula\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle the ubiquitous problem of \"dependence learning,\" copulas are\nquickly becoming a pervasive tool across a wide range of data-driven\ndisciplines encompassing neuroscience, finance, econometrics, genomics, social\nscience, machine learning, healthcare and many more. Copula (or connection)\nfunctions were invented in 1959 by Abe Sklar in response to a query of Maurice\nFrechet. After 60 years, where do we stand now? This article provides a history\nof the key developments and offers a unified perspective.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2019 18:07:54 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Parzen", "Emanuel", ""]]}, {"id": "1912.05737", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif, Pierre Alquier", "title": "Finite sample properties of parametric MMD estimation: robustness to\n  misspecification and dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in statistics aim at designing a universal estimation procedure,\nthat is, an estimator that would converge to the best approximation of the\n(unknown) data generating distribution in a model, without any assumption on\nthis distribution. This question is of major interest, in particular because\nthe universality property leads to the robustness of the estimator. In this\npaper, we tackle the problem of universal estimation using a minimum distance\nestimator presented in Briol et al. (2019) based on the Maximum Mean\nDiscrepancy. We show that the estimator is robust to both dependence and to the\npresence of outliers in the dataset. Finally, we provide a theoretical study of\nthe stochastic gradient descent algorithm used to compute the estimator, and we\nsupport our findings with numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 02:28:13 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 20:52:14 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 02:11:15 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 02:00:55 GMT"}, {"version": "v5", "created": "Thu, 4 Mar 2021 08:52:02 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1912.06030", "submitter": "Paramita Chakraborty", "authors": "Paramita Chakraborty, Chong Ma, John Grego and James Lynch", "title": "Exploratory data analysis for large-scale multiple testing problems and\n  its application in gene expression studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large scale multiple testing problems, a two-class empirical Bayes\napproach can be used to control the false discovery rate (Fdr) for the entire\narray of hypotheses under study. A sample splitting step is incorporated to\nmodify that approach where one part of the data is used for model fitting and\nthe other part for detecting the significant cases by a screening technique\nfeaturing the empirical Bayes mode of Fdr control. Cases with high detection\nfrequency across repeated random sample splits are considered true discoveries.\nA critical detection frequency is set to control the overall false discovery\nrate. The proposed method helps to balance out unwanted sources of variation\nand addresses potential statistical overfitting of the core empirical model by\ncross-validation through resampling. Further, concurrent detection frequencies\nare used to provide visual tools to explore the inter-relationship between\nsignificant cases. The methodology is illustrated using a microarray data set,\nRNA-sequencing data set, and several simulation studies. A power analysis is\npresented to understand the efficiency of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 15:27:48 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Chakraborty", "Paramita", ""], ["Ma", "Chong", ""], ["Grego", "John", ""], ["Lynch", "James", ""]]}, {"id": "1912.06073", "submitter": "He Jia", "authors": "He Jia, Uro\\v{s} Seljak", "title": "Normalizing Constant Estimation with Gaussianized Bridge Sampling", "comments": "Accepted by AABI 2019 Proceedings", "journal-ref": "Proceedings of The 2nd Symposium on Advances in Approximate\n  Bayesian Inference, PMLR 118:1-14, 2020", "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing constant (also called partition function, Bayesian evidence, or\nmarginal likelihood) is one of the central goals of Bayesian inference, yet\nmost of the existing methods are both expensive and inaccurate. Here we develop\na new approach, starting from posterior samples obtained with a standard Markov\nChain Monte Carlo (MCMC). We apply a novel Normalizing Flow (NF) approach to\nobtain an analytic density estimator from these samples, followed by Optimal\nBridge Sampling (OBS) to obtain the normalizing constant. We compare our method\nwhich we call Gaussianized Bridge Sampling (GBS) to existing methods such as\nNested Sampling (NS) and Annealed Importance Sampling (AIS) on several\nexamples, showing our method is both significantly faster and substantially\nmore accurate than these methods, and comes with a reliable error estimation.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 16:50:03 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Jia", "He", ""], ["Seljak", "Uro\u0161", ""]]}, {"id": "1912.06137", "submitter": "Thierry Denoeux", "authors": "Thierry Denoeux", "title": "Calibrated model-based evidential clustering using bootstrapping", "comments": null, "journal-ref": "Information Sciences, Vol. 528, pages 17-45, 2020", "doi": "10.1016/j.ins.2020.04.014", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidential clustering is an approach to clustering in which\ncluster-membership uncertainty is represented by a collection of\nDempster-Shafer mass functions forming an evidential partition. In this paper,\nwe propose to construct these mass functions by bootstrapping finite mixture\nmodels. In the first step, we compute bootstrap percentile confidence intervals\nfor all pairwise probabilities (the probabilities for any two objects to belong\nto the same class). We then construct an evidential partition such that the\npairwise belief and plausibility degrees approximate the bounds of the\nconfidence intervals. This evidential partition is calibrated, in the sense\nthat the pairwise belief-plausibility intervals contain the true probabilities\n\"most of the time\", i.e., with a probability close to the defined confidence\nlevel. This frequentist property is verified by simulation, and the practical\napplicability of the method is demonstrated using several real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2019 08:40:39 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 02:07:19 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Denoeux", "Thierry", ""]]}, {"id": "1912.06342", "submitter": "Runxiong Wu", "authors": "Runxiong Wu and Xin Chen", "title": "MM Algorithms for Distance Covariance based Sufficient Dimension\n  Reduction and Sufficient Variable Selection", "comments": "26 pages, 4 figures", "journal-ref": "Computational Statistics & Data Analysis, 2021", "doi": "10.1016/j.csda.2020.107089", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction (SDR) using distance covariance (DCOV) was\nrecently proposed as an approach to dimension-reduction problems. Compared with\nother SDR methods, it is model-free without estimating link function and does\nnot require any particular distributions on predictors (see Sheng and Yin,\n2013, 2016). However, the DCOV-based SDR method involves optimizing a nonsmooth\nand nonconvex objective function over the Stiefel manifold. To tackle the\nnumerical challenge, we novelly formulate the original objective function\nequivalently into a DC (Difference of Convex functions) program and construct\nan iterative algorithm based on the majorization-minimization (MM) principle.\nAt each step of the MM algorithm, we inexactly solve the quadratic subproblem\non the Stiefel manifold by taking one iteration of Riemannian Newton's method.\nThe algorithm can also be readily extended to sufficient variable selection\n(SVS) using distance covariance. We establish the convergence property of the\nproposed algorithm under some regularity conditions. Simulation studies show\nour algorithm drastically improves the computation efficiency and is robust\nacross various settings compared with the existing method. Supplemental\nmaterials for this article are available.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 07:08:15 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 08:50:08 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Runxiong", ""], ["Chen", "Xin", ""]]}, {"id": "1912.06622", "submitter": "Jing Yu", "authors": "Jing Yu and Mihai Anitescu", "title": "Solving Optimal Experimental Design with Sequential Quadratic\n  Programming and Chebyshev Interpolation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimization algorithm to compute the optimal sensor locations\nin experimental design in the formulation of Bayesian inverse problems, where\nthe parameter-to-observable mapping is described through an integral equation\nand its discretization results in a continuously indexed matrix whose size\ndepends on the mesh size n. By approximating the gradient and Hessian of the\nobjective design criterion from Chebyshev interpolation, we solve a sequence of\nquadratic programs and achieve the complexity $\\mathcal{O}(n\\log^2(n))$. An\nerror analysis guarantees the integrality gap shrinks to zero as $n\\to\\infty$,\nand we apply the algorithm on a two-dimensional advection-diffusion equation,\nto determine the LIDAR's optimal sensing directions for data collection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 17:34:03 GMT"}, {"version": "v2", "created": "Thu, 26 Dec 2019 16:03:58 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Yu", "Jing", ""], ["Anitescu", "Mihai", ""]]}, {"id": "1912.06635", "submitter": "Joris Bierkens", "authors": "Joris Bierkens, Pierre Nyquist, Mikola C. Schlottke", "title": "Large deviations for the empirical measure of the zig-zag process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The zig-zag process is a piecewise deterministic Markov process in position\nand velocity space. The process can be designed to have an arbitrary Gibbs type\nmarginal probability density for its position coordinate, which makes it\nsuitable for Monte Carlo simulation of continuous probability distributions. An\nimportant question in assessing the efficiency of this method is how fast the\nempirical measure converges to the stationary distribution of the process. In\nthis paper we provide a partial answer to this question by characterizing the\nlarge deviations of the empirical measure from the stationary distribution.\nBased on the Feng-Kurtz approach, we develop an abstract framework aimed at\nencompassing piecewise deterministic Markov processes in position-velocity\nspace. We derive explicit conditions for the zig-zag process to allow the\nDonsker-Varadhan variational formulation of the rate function, both for a\ncompact setting (the torus) and one-dimensional Euclidean space. Finally we\nderive an explicit expression for the Donsker-Varadhan functional for the case\nof a compact state space and use this form of the rate function to address a\nkey question concerning the optimal choice of the switching rate of the zig-zag\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2019 18:04:27 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 11:04:05 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bierkens", "Joris", ""], ["Nyquist", "Pierre", ""], ["Schlottke", "Mikola C.", ""]]}, {"id": "1912.07346", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Rocio Titiunik, Gonzalo Vazquez-Bare", "title": "Analysis of Regression Discontinuity Designs with Multiple Cutoffs or\n  Multiple Scores", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \\texttt{Stata} (and \\texttt{R}) package \\texttt{rdmulti},\nwhich includes three commands (\\texttt{rdmc}, \\texttt{rdmcplot}, \\texttt{rdms})\nfor analyzing Regression Discontinuity (RD) designs with multiple cutoffs or\nmultiple scores. The command \\texttt{rdmc} applies to non-cumulative and\ncumulative multi-cutoff RD settings. It calculates pooled and cutoff-specific\nRD treatment effects, and provides robust bias-corrected inference procedures.\nPost estimation and inference is allowed. The command \\texttt{rdmcplot} offers\nRD plots for multi-cutoff settings. Finally, the command \\texttt{rdms} concerns\nmulti-score settings, covering in particular cumulative cutoffs and two running\nvariables contexts. It also calculates pooled and cutoff-specific RD treatment\neffects, provides robust bias-corrected inference procedures, and allows for\npost-estimation estimation and inference. These commands employ the\n\\texttt{Stata} (and \\texttt{R}) package \\texttt{rdrobust} for plotting,\nestimation, and inference. Companion \\texttt{R} functions with the same syntax\nand capabilities are provided.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 13:38:08 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 21:07:43 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Titiunik", "Rocio", ""], ["Vazquez-Bare", "Gonzalo", ""]]}, {"id": "1912.07466", "submitter": "Karl Schurter", "authors": "Joris Pinkse and Karl Schurter", "title": "Estimation of Auction Models with Shape Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce several new estimation methods that leverage shape constraints\nin auction models to estimate various objects of interest, including the\ndistribution of a bidder's valuations, the bidder's ex ante expected surplus,\nand the seller's counterfactual revenue. The basic approach applies broadly in\nthat (unlike most of the literature) it works for a wide range of auction\nformats and allows for asymmetric bidders. Though our approach is not\nrestrictive, we focus our analysis on first--price, sealed--bid auctions with\nindependent private valuations. We highlight two nonparametric estimation\nstrategies, one based on a least squares criterion and the other on a maximum\nlikelihood criterion. We also provide the first direct estimator of the\nstrategy function. We establish several theoretical properties of our methods\nto guide empirical analysis and inference. In addition to providing the\nasymptotic distributions of our estimators, we identify ways in which\nmethodological choices should be tailored to the objects of their interest. For\nobjects like the bidders' ex ante surplus and the seller's counterfactual\nexpected revenue with an additional symmetric bidder, we show that our\ninput--parameter--free estimators achieve the semiparametric efficiency bound.\nFor objects like the bidders' inverse strategy function, we provide an easily\nimplementable boundary--corrected kernel smoothing and transformation method in\norder to ensure the squared error is integrable over the entire support of the\nvaluations. An extensive simulation study illustrates our analytical results\nand demonstrates the respective advantages of our least--squares and maximum\nlikelihood estimators in finite samples. Compared to estimation strategies\nbased on kernel density estimation, the simulations indicate that the smoothed\nversions of our estimators enjoy a large degree of robustness to the choice of\nan input parameter.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 16:01:16 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Pinkse", "Joris", ""], ["Schurter", "Karl", ""]]}, {"id": "1912.07761", "submitter": "David Degras", "authors": "David Degras", "title": "Sparse Group Fused Lasso for Model Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the sparse group fused lasso (SGFL) as a statistical\nframework for segmenting sparse regression models with multivariate time\nseries. To compute solutions of the SGFL, a nonsmooth and nonseparable convex\nprogram, we develop a hybrid optimization method that is fast, requires no\ntuning parameter selection, and is guaranteed to converge to a global\nminimizer. In numerical experiments, the hybrid method compares favorably to\nstate-of-the-art techniques with respect to computation time and numerical\naccuracy; benefits are particularly substantial in high dimension. The method's\nstatistical performance is satisfactory in recovering nonzero regression\ncoefficients and excellent in change point detection. An application to air\nquality data is presented. The hybrid method is implemented in the R package\nsparseGFL available on the author's Github page.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 23:52:34 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 17:21:37 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Degras", "David", ""]]}, {"id": "1912.07775", "submitter": "Lijing Ma", "authors": "Lijing Ma, Andrew Grant, Georgy Sofronov", "title": "Multiple Change Point Detection and Validation in Autoregressive Time\n  Series Data", "comments": "Changepoint detection, Autoregressive time series, Likelihood ratio\n  scan statistics, Multiple testing problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is quite common that the structure of a time series changes abruptly.\nIdentifying these change points and describing the model structure in the\nsegments between these change points is of interest. In this paper, time series\ndata is modelled assuming each segment is an autoregressive time series with\npossibly different autoregressive parameters. This is achieved using two main\nsteps. The first step is to use a likelihood ratio scan based estimation\ntechnique to identify these potential change points to segment the time series.\nOnce these potential change points are identified, modified parametric spectral\ndiscrimination tests are used to validate the proposed segments. A numerical\nstudy is conducted to demonstrate the performance of the proposed method across\nvarious scenarios and compared against other contemporary techniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 01:16:48 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["Ma", "Lijing", ""], ["Grant", "Andrew", ""], ["Sofronov", "Georgy", ""]]}, {"id": "1912.08311", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Bhargav Srinivasa Desikan", "title": "Kernel-Based Ensemble Learning in Python", "comments": "11 pages", "journal-ref": "Information 2020, 11(2)", "doi": "10.3390/info11020063", "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new supervised learning algorithm, for classification and\nregression problems where two or more preliminary predictors are available. We\nintroduce \\texttt{KernelCobra}, a non-linear learning strategy for combining an\narbitrary number of initial predictors. \\texttt{KernelCobra} builds on the\nCOBRA algorithm introduced by \\citet{biau2016cobra}, which combined estimators\nbased on a notion of proximity of predictions on the training data. While the\nCOBRA algorithm used a binary threshold to declare which training data were\nclose and to be used, we generalize this idea by using a kernel to better\nencapsulate the proximity information. Such a smoothing kernel provides more\nrepresentative weights to each of the training points which are used to build\nthe aggregate and final predictor, and \\texttt{KernelCobra} systematically\noutperforms the COBRA algorithm. While COBRA is intended for regression,\n\\texttt{KernelCobra} deals with classification and regression.\n\\texttt{KernelCobra} is included as part of the open source Python package\n\\texttt{Pycobra} (0.2.4 and onward), introduced by \\citet{guedj2018pycobra}.\nNumerical experiments assess the performance (in terms of pure prediction and\ncomputational complexity) of \\texttt{KernelCobra} on real-life and synthetic\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 23:23:00 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Guedj", "Benjamin", ""], ["Desikan", "Bhargav Srinivasa", ""]]}, {"id": "1912.08434", "submitter": "Javier Felip Leon", "authors": "Javier Felip and Nilesh Ahuja and Omesh Tickoo", "title": "Tree pyramidal adaptive importance sampling", "comments": "20 pages + 13 pages of additional result plots and evaluation details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Tree-Pyramidal Adaptive Importance Sampling (TP-AIS), a\nnovel iterated sampling method that outperforms state-of-the-art approaches\nlike deterministic mixture population Monte Carlo (DM-PMC), mixture population\nMonte Carlo (M-PMC) and layered adaptive importance sampling (LAIS). TP-AIS\niteratively builds a proposal distribution parameterized by a tree pyramid,\nwhere each tree leaf spans a subspace that represents its importance density.\nAfter each new sample operation, a set of tree leaves are subdivided for\nimproving the approximation of the proposal distribution to the target density.\nUnlike the rest of the methods in the literature, TP-AIS is parameter free and\nrequires no tuning to achieve its best performance. We evaluate TP-AIS with\ndifferent complexity randomized target probability density functions (PDF) and\nalso analyze its application to different dimensions. The results are compared\nto state-of-the-art iterative importance sampling approaches and other baseline\nMCMC approaches using Normalized Effective Sample Size (N-ESS), Jensen-Shannon\nDivergence, and time complexity.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2019 08:05:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 18:55:24 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Felip", "Javier", ""], ["Ahuja", "Nilesh", ""], ["Tickoo", "Omesh", ""]]}, {"id": "1912.08718", "submitter": "Karl Granstr\\\"om", "authors": "Karl Granstr\\\"om, Lennart Svensson, Yuxuan Xia, Jason Williams,\n  \\'Angel F. Garc\\'ia-Fern\\'andez", "title": "Poisson Multi-Bernoulli Mixtures for Sets of Trajectories", "comments": "arXiv admin note: text overlap with arXiv:1812.05131", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.RO eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the standard point target model with Poisson birth process, the Poisson\nMulti-Bernoulli Mixture (PMBM) is a conjugate multi-target density. The PMBM\nfilter for sets of targets has been shown to have state-of-the-art performance\nand a structure similar to the Multiple Hypothesis Tracker (MHT). In this paper\nwe consider a recently developed formulation of multiple target tracking as a\nrandom finite set (RFS) of trajectories, and present three important and\ninteresting results. First, we show that, for the standard point target model,\nthe PMBM density is conjugate also for sets of trajectories. Second, based on\nthis we develop PMBM trackers (trajectory RFS filters) that efficiently\nestimate the set of trajectories. Third, we establish that for the standard\npoint target model the multi-trajectory density is PMBM for trajectories in any\ntime window, given measurements in any (possibly non-overlapping) time window.\nIn addition, the PMBM trackers are evaluated in a simulation study, and shown\nto yield state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2019 10:09:21 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Granstr\u00f6m", "Karl", ""], ["Svensson", "Lennart", ""], ["Xia", "Yuxuan", ""], ["Williams", "Jason", ""], ["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""]]}, {"id": "1912.09185", "submitter": "Zhenyu Zhang", "authors": "Zhenyu Zhang, Akihiko Nishimura, Paul Bastide, Xiang Ji, Rebecca P.\n  Payne, Philip Goulder, Philippe Lemey, Marc A. Suchard", "title": "Large-scale inference of correlation among mixed-type biological traits\n  with phylogenetic multivariate probit models", "comments": "24 pages, 6 figures, 2 tables. Version accepted by Annals of Applied\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring concerted changes among biological traits along an evolutionary\nhistory remains an important yet challenging problem. Besides adjusting for\nspurious correlation induced from the shared history, the task also requires\nsufficient flexibility and computational efficiency to incorporate multiple\ncontinuous and discrete traits as data size increases. To accomplish this, we\njointly model mixed-type traits by assuming latent parameters for binary\noutcome dimensions at the tips of an unknown tree informed by molecular\nsequences. This gives rise to a phylogenetic multivariate probit model. With\nlarge sample sizes, posterior computation under this model is problematic, as\nit requires repeated sampling from a high-dimensional truncated normal\ndistribution. Current best practices employ multiple-try rejection sampling\nthat suffers from slow-mixing and a computational cost that scales\nquadratically in sample size. We develop a new inference approach that exploits\n1) the bouncy particle sampler (BPS) based on piecewise deterministic Markov\nprocesses to simultaneously sample all truncated normal dimensions, and 2)\nnovel dynamic programming that reduces the cost of likelihood and gradient\nevaluations for BPS to linear in sample size. In an application with 535 HIV\nviruses and 24 traits that necessitates sampling from a 12,840-dimensional\ntruncated normal, our method makes it possible to estimate the across-trait\ncorrelation and detect factors that affect the pathogen's capacity to cause\ndisease. This inference framework is also applicable to a broader class of\ncovariance structures beyond comparative biology.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 13:33:52 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 16:04:04 GMT"}, {"version": "v3", "created": "Tue, 31 Dec 2019 15:40:31 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 22:24:40 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Zhang", "Zhenyu", ""], ["Nishimura", "Akihiko", ""], ["Bastide", "Paul", ""], ["Ji", "Xiang", ""], ["Payne", "Rebecca P.", ""], ["Goulder", "Philip", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1912.09229", "submitter": "Giulio Morina", "authors": "Giulio Morina, Krzysztof Latuszynski, Piotr Nayar, Alex Wendland", "title": "From the Bernoulli Factory to a Dice Enterprise via Perfect Sampling of\n  Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a $p$-coin that lands heads with unknown probability $p$, we wish to\nproduce an $f(p)$-coin for a given function $f: (0,1) \\rightarrow (0,1)$. This\nproblem is commonly known as the Bernoulli Factory and results on its\nsolvability and complexity have already been obtained. Nevertheless, generic\nways to design a practical Bernoulli Factory for a given function $f$ exist\nonly in a few special cases. We present a constructive way to build an\nefficient Bernoulli Factory when $f(p)$ is a rational function with\ncoefficients in $\\mathbb{R}$. Moreover, we extend the Bernoulli Factory problem\nto a more general setting where we have access to an $m$-sided die and we wish\nto roll a $v$-sided one; i.e., we consider rational functions between open\nprobability simplices. Our construction consists of rephrasing the original\nproblem as simulating from the stationary distribution of a certain class of\nMarkov chains - a task that we show can be achieved using perfect simulation\ntechniques with the original $m$-sided die as the only source of randomness. In\nthe Bernoulli Factory case, the number of tosses needed by the algorithm has\nexponential tails and its expected value can be bounded uniformly in $p$. En\nroute to optimizing the algorithm we show a fact of independent interest: every\nfinite, integer valued, random variable will eventually become log-concave\nafter convolving with enough Bernoulli trials.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 14:40:49 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 14:22:16 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Morina", "Giulio", ""], ["Latuszynski", "Krzysztof", ""], ["Nayar", "Piotr", ""], ["Wendland", "Alex", ""]]}, {"id": "1912.09334", "submitter": "Uwe Petersohn", "authors": "Uwe Petersohn, Thomas Dedek, Sandra Zimmer, Hans Biskupski", "title": "Causal statistical modeling and calculation of distribution functions of\n  classification features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical system models provide the basis for the examination of various\nsorts of distributions. Classification distributions are a very common and\nversatile form of statistics in e.g. real economic, social, and IT systems. The\nstatistical distributions of classification features can be applied in\ndetermining the a priori probabilities in Bayesian networks. We investigate a\nstatistical model of classification distributions based on finding the critical\npoint of a specialized form of entropy. A distribution function for\nclassification features is derived, with the two parameters $n_0$, minimal\nclass, and $\\bar{N}$, average number of classes. Efficient algorithms for the\ncomputation of the class probabilities and the approximation of real frequency\ndistributions are developed and applied to examples from different domains. The\nmethod is compared to established distributions like Zipf's law. The majority\nof examples can be approximated with a sufficient quality ($3-5\\%$).\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2019 16:13:09 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Petersohn", "Uwe", ""], ["Dedek", "Thomas", ""], ["Zimmer", "Sandra", ""], ["Biskupski", "Hans", ""]]}, {"id": "1912.09733", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin", "title": "An adaptive simulated annealing EM algorithm for inference on\n  non-homogeneous hidden Markov models", "comments": "8 pages, 6 figures, 4 tables. Accepted version of the article\n  published in AIIPCC 2019", "journal-ref": null, "doi": "10.1145/3371425.3371641", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-homogeneous hidden Markov models (NHHMM) are a subclass of dependent\nmixture models used for semi-supervised learning, where both transition\nprobabilities between the latent states and mean parameter of the probability\ndistribution of the responses (for a given state) depend on the set of $p$\ncovariates. A priori we do not know which (and how) covariates influence the\ntransition probabilities and the mean parameters. This induces a complex\ncombinatorial optimization problem for model selection with $4^p$ potential\nconfigurations. To address the problem, in this article we propose an adaptive\n(A) simulated annealing (SA) expectation maximization (EM) algorithm (ASA-EM)\nfor joint optimization of models and their parameters with respect to a\ncriterion of interest.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 10:03:53 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Hubin", "Aliaksandr", ""]]}, {"id": "1912.09759", "submitter": "Mark Van Der Loo", "authors": "Mark P.J. van der Loo and Edwin de Jonge", "title": "Data Validation Infrastructure for R", "comments": "To be published in the Journal of Statistical Software", "journal-ref": "Journal of Statistical Software 97 (10) (2021)", "doi": "10.18637/jss.v097.i10", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Checking data quality against domain knowledge is a common activity that\npervades statistical analysis from raw data to output. The R package 'validate'\nfacilitates this task by capturing and applying expert knowledge in the form of\nvalidation rules: logical restrictions on variables, records, or data sets that\nshould be satisfied before they are considered valid input for further\nanalysis. In the validate package, validation rules are objects of computation\nthat can be manipulated, investigated, and confronted with data or versions of\na data set. The results of a confrontation are then available for further\ninvestigation, summarization or visualization. Validation rules can also be\nendowed with metadata and documentation and they may be stored or retrieved\nfrom external sources such as text files or tabular formats. This data\nvalidation infrastructure thus allows for systematic, user-defined definition\nof data quality requirements that can be reused for various versions of a data\nset or by data correction algorithms that are parameterized by validation\nrules.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2019 11:13:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["van der Loo", "Mark P. J.", ""], ["de Jonge", "Edwin", ""]]}, {"id": "1912.10396", "submitter": "Kae-Wen (Kevin) Chern", "authors": "Alexandre Bouchard-C\\^ot\\'e, Kevin Chern, Davor Cubranic, Sahand\n  Hosseini, Justin Hume, Matteo Lepur, Zihui Ouyang, Giorgio Sgarbi", "title": "Blang: Bayesian declarative modelling of general data structures and\n  inference via algorithms based on distribution continua", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Bayesian inference problem where a variable of interest does not\ntake values in a Euclidean space. These \"non-standard\" data structures are in\nreality fairly common. They are frequently used in problems involving latent\ndiscrete factor models, networks, and domain specific problems such as sequence\nalignments and reconstructions, pedigrees, and phylogenies. In principle,\nBayesian inference should be particularly well-suited in such scenarios, as the\nBayesian paradigm provides a principled way to obtain confidence assessment for\nrandom variables of any type. However, much of the recent work on making\nBayesian analysis more accessible and computationally efficient has focused on\ninference in Euclidean spaces.\n  In this paper, we introduce Blang, a domain specific language and library\naimed at bridging this gap. Blang allows users to perform Bayesian analysis on\narbitrary data types while using a declarative syntax similar to BUGS. Blang is\naugmented with intuitive language additions to create data types of the user's\nchoosing. To perform inference at scale on such arbitrary state spaces, Blang\nleverages recent advances in sequential Monte Carlo and non-reversible Markov\nchain Monte Carlo methods.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2019 08:05:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 19:24:39 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Chern", "Kevin", ""], ["Cubranic", "Davor", ""], ["Hosseini", "Sahand", ""], ["Hume", "Justin", ""], ["Lepur", "Matteo", ""], ["Ouyang", "Zihui", ""], ["Sgarbi", "Giorgio", ""]]}, {"id": "1912.10981", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio and Michela Cameletti and Marta Blangiardo", "title": "Missing data analysis and imputation via latent Gaussian Markov random\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we recast the problem of missing values in the covariates of a\nregression model as a latent Gaussian Markov random field (GMRF) model in a\nfully Bayesian framework. Our proposed approach is based on the definition of\nthe covariate imputation sub-model as a latent effect with a GMRF structure. We\nshow how this formulation works for continuous covariates and provide some\ninsight on how this could be extended to categorical covariates.\n  The resulting Bayesian hierarchical model naturally fits within the\nintegrated nested Laplace approximation (INLA) framework, which we use for\nmodel fitting. Hence, our work fills an important gap in the INLA methodology\nas it allows to treat models with missing values in the covariates.\n  As in any other fully Bayesian framework, by relying on INLA for model\nfitting it is possible to formulate a joint model for the data, the imputed\ncovariates and their missingness mechanism. In this way, we are able to tackle\nthe more general problem of assessing the missingness mechanism by conducting a\nsensitivity analysis on the different alternatives to model the non-observed\ncovariates.\n  Finally, we illustrate the proposed approach with two examples on modeling\nhealth risk factors and disease mapping. Here, we rely on two different\nimputation mechanisms based on a typical multiple linear regression and a\nspatial model, respectively. Given the speed of model fitting with INLA we are\nable to fit joint models in a short time, and to easily conduct sensitivity\nanalyses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 17:16:28 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Cameletti", "Michela", ""], ["Blangiardo", "Marta", ""]]}, {"id": "1912.11028", "submitter": "Katarzyna Reluga", "authors": "Katarzyna Reluga, Mar\\'ia-Jos\\'e Lombard\\'ia, Stefan Sperlich", "title": "Simultaneous Inference for Empirical Best Predictors with a Poverty\n  Study in Small Areas", "comments": "46 pages, 20 figures; simulations and data analysis expanded,\n  additional remarks added", "journal-ref": null, "doi": "10.1080/01621459.2021.1942014", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, generalized linear mixed models are broadly used in many fields.\nHowever, the development of tools for performing simultaneous inference has\nbeen largely neglected in this domain. A framework for joint inference is\nindispensable to carry out statistically valid multiple comparisons of\nparameters of interest between all or several clusters. We therefore develop\nsimultaneous confidence intervals and multiple testing procedures for empirical\nbest predictors under generalized linear mixed models. In addition, we\nimplement our methodology to study widely employed examples of mixed models,\nthat is, the unit-level binomial, the area-level Poisson-gamma and the\narea-level Poisson-lognormal mixed models. The asymptotic results are\naccompanied by extensive simulations. A case study on predicting poverty rates\nillustrates applicability and advantages of our simultaneous inference tools.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:49:04 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 12:16:19 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Reluga", "Katarzyna", ""], ["Lombard\u00eda", "Mar\u00eda-Jos\u00e9", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1912.11029", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis and Iason Papaioannou and Daniel Straub and Fabio\n  Nobile", "title": "Sparse Polynomial Chaos expansions using Variational Relevance Vector\n  Machines", "comments": "Submitted to Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2020.109498", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenges for non-intrusive methods for Polynomial Chaos modeling lie in\nthe computational efficiency and accuracy under a limited number of model\nsimulations. These challenges can be addressed by enforcing sparsity in the\nseries representation through retaining only the most important basis terms. In\nthis work, we present a novel sparse Bayesian learning technique for obtaining\nsparse Polynomial Chaos expansions which is based on a Relevance Vector Machine\nmodel and is trained using Variational Inference. The methodology shows great\npotential in high-dimensional data-driven settings using relatively few data\npoints and achieves user-controlled sparse levels that are comparable to other\nmethods such as compressive sensing. The proposed approach is illustrated on\ntwo numerical examples, a synthetic response function that is explored for\nvalidation purposes and a low-carbon steel plate with random Young's modulus\nand random loading, which is modeled by stochastic finite element with 38 input\nrandom variables.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 18:49:55 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Papaioannou", "Iason", ""], ["Straub", "Daniel", ""], ["Nobile", "Fabio", ""]]}, {"id": "1912.11070", "submitter": "Ivan Panin I.", "authors": "Ivan I. Panin", "title": "Risk of estimators for Sobol' sensitivity indices based on metamodels", "comments": null, "journal-ref": "Electron. J. Statist. 15 (2021), no. 1, 235--281", "doi": "10.1214/20-EJS1793", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sobol' sensitivity indices allow to quantify the respective effects of random\ninput variables and their combinations on the variance of mathematical model\noutput. We focus on the problem of Sobol' indices estimation via a metamodeling\napproach where we replace the true mathematical model with a sample-based\napproximation to compute sensitivity indices. We propose a new method for\nindices quality control and obtain asymptotic and non-asymptotic risk bounds\nfor Sobol' indices estimates based on a general class of metamodels. Our\nanalysis is closely connected with the problem of nonparametric function\nfitting using the orthogonal system of functions in the random design setting.\nIt considers the relation between the metamodel quality and the error of the\ncorresponding estimator for Sobol' indices and shows the possibility of fast\nconvergence rates in the case of noiseless observations. The theoretical\nresults are complemented with numerical experiments for the approximations\nbased on multivariate Legendre and Trigonometric polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 19:20:53 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2021 18:34:12 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Panin", "Ivan I.", ""]]}, {"id": "1912.11119", "submitter": "Zhu Wang", "authors": "Zhu Wang", "title": "MM for Penalized Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized estimation can conduct variable selection and parameter estimation\nsimultaneously. The general framework is to minimize a loss function subject to\na penalty designed to generate sparse variable selection. The\nmajorization-minimization (MM) algorithm is a computational scheme for\nstability and simplicity, and the MM algorithm has been widely applied in\npenalized estimation. Much of the previous work have focused on convex loss\nfunctions such as generalized linear models. When data are contaminated with\noutliers, robust loss functions can generate more reliable estimates. Recent\nliterature has witnessed a growing impact of nonconvex loss-based methods,\nwhich can generate robust estimation for data contaminated with outliers. This\narticle investigates MM algorithm for penalized estimation, provide innovative\noptimality conditions and establish convergence theory with both convex and\nnonconvex loss functions. With respect to applications, we focus on several\nnonconvex loss functions, which were formerly studied in machine learning for\nregression and classification problems. Performance of the proposed algorithms\nare evaluated on simulated and real data including healthcare costs and cancer\nclinical status. Efficient implementations of the algorithms are available in\nthe R package mpath in CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 21:40:08 GMT"}, {"version": "v2", "created": "Sat, 5 Sep 2020 22:57:02 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Wang", "Zhu", ""]]}, {"id": "1912.11144", "submitter": "Dirk Eddelbuettel", "authors": "Dirk Eddelbuettel", "title": "Parallel Computing With R: A Brief Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel computing has established itself as another standard method for\napplied research and data analysis. The R system, being internally constrained\nto mostly singly-threaded operations, can nevertheless be used along with\ndifferent parallel computing approaches. This brief review covers OpenMP and\nIntel TBB at the cpu- and compiler level, moves to process-parallel approaches\nbefore discussing message-passing parallelism and big data technologies for\nparallel processing such as Spark, Docker and Kubernetes before concluding with\na focus on the future package integrating many of these approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2019 23:28:04 GMT"}, {"version": "v2", "created": "Sun, 29 Dec 2019 20:03:00 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 21:37:52 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Eddelbuettel", "Dirk", ""]]}, {"id": "1912.11703", "submitter": "Yan Liu", "authors": "Minggen Lu, Yan Liu, Chin-Shang Li, Jianguo Sun", "title": "An efficient penalized estimation approach for a semi-parametric linear\n  transformation model with interval-censored data", "comments": "20 pages, 5 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider efficient estimation of flexible transformation models with\ninterval-censored data. To reduce the dimension of semi-parametric models, the\nunknown monotone transformation function is approximated via monotone splines.\nA penalization technique is used to provide more computationally efficient\nestimation of all parameters. To accomplish model fitting, a computationally\nefficient nested iterative expectation-maximization (EM) based algorithm is\ndeveloped for estimation, and an easily implemented variance-covariance\napproach is proposed for inference on regression parameters. Theoretically, we\nshow that the estimator of the transformation function achieves the optimal\nrate of convergence and the estimators of regression parameters are\nasymptotically normal and efficient. The penalized procedure is assessed\nthrough extensive numerical experiments and further illustrated via two real\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2019 18:11:36 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Lu", "Minggen", ""], ["Liu", "Yan", ""], ["Li", "Chin-Shang", ""], ["Sun", "Jianguo", ""]]}, {"id": "1912.11740", "submitter": "Michael Byrd", "authors": "Michael Byrd and Monnie McGee", "title": "A Simple Correction Procedure for High-Dimensional Generalized Linear\n  Models with Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional generalized linear models when the covariates\nare contaminated by measurement error. Estimates from errors-in-variables\nregression models are well-known to be biased in traditional low-dimensional\nsettings if the error is unincorporated. Such models have recently become of\ninterest when regularizing penalties are added to the estimation procedure.\nUnfortunately, correcting for the mismeasurements can add undue computational\ndifficulties onto the optimization, which a new tool set for practitioners to\nsuccessfully use the models. We investigate a general procedure that utilizes\nthe recently proposed Imputation-Regularized Optimization algorithm for\nhigh-dimensional errors-in-variables models, which we implement for continuous,\nbinary, and count response type. Crucially, our method allows for off-the-shelf\nlinear regression methods to be employed in the presence of contaminated\ncovariates. We apply our correction to gene microarray data, and illustrate\nthat it results in a great reduction in the number of false positives whilst\nstill retaining most true positives.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 01:13:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 21:49:47 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Byrd", "Michael", ""], ["McGee", "Monnie", ""]]}, {"id": "1912.11914", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Inverses of Matern Covariances on Grids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct a study of the aliased spectral densities of Mat\\'ern covariance\nfunctions on a regular grid of points, providing clarity on the properties of a\npopular approximation based on stochastic partial differential equations; while\nothers have shown that it can approximate the covariance function well, we find\nthat it assigns too much power at high frequencies and does not provide\nincreasingly accurate approximations to the inverse as the grid spacing goes to\nzero, except in the one-dimensional exponential covariance case. We provide\nnumerical results to support our theory, and in a simulation study, we\ninvestigate the implications for parameter estimation, finding that the SPDE\napproximation tends to overestimate spatial range parameters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Dec 2019 18:36:06 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 14:44:05 GMT"}, {"version": "v3", "created": "Mon, 1 Mar 2021 19:22:37 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1912.12353", "submitter": "Kevin He", "authors": "Kevin He, Ji Zhu, Jian Kang, Yi Li", "title": "Minorization-Maximization-based Steepest Ascent for Large-scale Survival\n  Analysis with Time-Varying Effects: Application to the National Kidney\n  Transplant Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The time-varying effects model is a flexible and powerful tool for modeling\nthe dynamic changes of covariate effects. However, in survival analysis, its\ncomputational burden increases quickly as the number of sample sizes or\npredictors grows. Traditional methods that perform well for moderate sample\nsizes and low-dimensional data do not scale to massive data. Analysis of\nnational kidney transplant data with a massive sample size and large number of\npredictors defy any existing statistical methods and software. In view of these\ndifficulties, we propose a Minorization-Maximization-based steepest ascent\nprocedure for estimating the time-varying effects. Leveraging the block\nstructure formed by the basis expansions, the proposed procedure iteratively\nupdates the optimal block-wise direction along which the approximate increase\nin the log-partial likelihood is maximized. The resulting estimates ensure the\nascent property and serve as refinements of the previous step. The performance\nof the proposed method is examined by simulations and applications to the\nanalysis of national kidney transplant data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:07:06 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["He", "Kevin", ""], ["Zhu", "Ji", ""], ["Kang", "Jian", ""], ["Li", "Yi", ""]]}, {"id": "1912.12356", "submitter": "Aritra Halder", "authors": "Aritra Halder, Shariq Mohammed, Kun Chen and Dipak Dey", "title": "Spatial risk estimation in Tweedie compound Poisson double generalized\n  linear models", "comments": "34 pages, 10 figures and 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie exponential dispersion family constitutes a fairly rich sub-class of\nthe celebrated exponential family. In particular, a member, compound Poisson\ngamma (CP-g) model has seen extensive use over the past decade for modeling\nmixed response featuring exact zeros with a continuous response from a gamma\ndistribution. This paper proposes a framework to perform residual analysis on\nCP-g double generalized linear models for spatial uncertainty quantification.\nApproximations are introduced to proposed framework making the procedure\nscalable, without compromise in accuracy of estimation and model complexity;\naccompanied by sensitivity analysis to model mis-specification. Proposed\nframework is applied to modeling spatial uncertainty in insurance loss costs\narising from automobile collision coverage. Scalability is demonstrated by\nchoosing sizable spatial reference domains comprised of groups of states within\nthe United States of America.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2019 22:25:53 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 20:43:00 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Halder", "Aritra", ""], ["Mohammed", "Shariq", ""], ["Chen", "Kun", ""], ["Dey", "Dipak", ""]]}, {"id": "1912.12404", "submitter": "David Warne", "authors": "David J. Warne (1), Ruth E. Baker (2), Matthew J. Simpson (1) ((1)\n  Queensland University of Technology, (2) University of Oxford)", "title": "A practical guide to pseudo-marginal methods for computational inference\n  in systems biology", "comments": null, "journal-ref": null, "doi": "10.1016/j.jtbi.2020.110255", "report-no": null, "categories": "q-bio.MN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many stochastic models of interest in systems biology, such as those\ndescribing biochemical reaction networks, exact quantification of parameter\nuncertainty through statistical inference is intractable. Likelihood-free\ncomputational inference techniques enable parameter inference when the\nlikelihood function for the model is intractable but the generation of many\nsample paths is feasible through stochastic simulation of the forward problem.\nThe most common likelihood-free method in systems biology is approximate\nBayesian computation that accepts parameters that result in low discrepancy\nbetween stochastic simulations and measured data. However, it can be difficult\nto assess how the accuracy of the resulting inferences are affected by the\nchoice of acceptance threshold and discrepancy function. The pseudo-marginal\napproach is an alternative likelihood-free inference method that utilises a\nMonte Carlo estimate of the likelihood function. This approach has several\nadvantages, particularly in the context of noisy, partially observed,\ntime-course data typical in biochemical reaction network studies. Specifically,\nthe pseudo-marginal approach facilitates exact inference and uncertainty\nquantification, and may be efficiently combined with particle filters for low\nvariance, high-accuracy likelihood estimation. In this review, we provide a\npractical introduction to the pseudo-marginal approach using inference for\nbiochemical reaction networks as a series of case studies. Implementations of\nkey algorithms and examples are provided using the Julia programming language;\na high performance, open source programming language for scientific computing.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 05:16:22 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Warne", "David J.", ""], ["Baker", "Ruth E.", ""], ["Simpson", "Matthew J.", ""]]}, {"id": "1912.12446", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers and Peter J. Rousseeuw", "title": "Handling cellwise outliers by sparse regression and robust covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a data-analytic method for detecting cellwise outliers. Given a\nrobust covariance matrix, outlying cells (entries) in a row are found by the\ncellHandler technique which combines lasso regression with a stepwise\napplication of constructed cutoff values. The penalty term of the lasso has a\nphysical interpretation as the total distance that suspicious cells need to\nmove in order to bring their row into the fold. For estimating a cellwise\nrobust covariance matrix we construct a detection-imputation method which\nalternates between flagging outlying cells and updating the covariance matrix\nas in the EM algorithm. The proposed methods are illustrated by simulations and\non real data about volatile organic compounds in children.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2019 11:48:39 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 11:16:49 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1912.12755", "submitter": "Eugene Geis", "authors": "Eugene Geis", "title": "Stochastic Approximation EM for Exploratory Item Factor Analysis", "comments": "131 pages, 57 figures, A dissertation for completion of PhD in\n  psychometrics at Rutgers Graduate School of Education", "journal-ref": "Statistics in Medicine, Volume 38, Issue 21, page 3997 (2019)", "doi": "10.7282/t3-7k3j-6x67 10.1002/sim.8217", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic approximation EM algorithm (SAEM) is described for the\nestimation of item and person parameters given test data coded as dichotomous\nor ordinal variables. The method hinges upon the eigenanalysis of missing\nvariables sampled as augmented data; the augmented data approach was introduced\nby Albert's seminal work applying Gibbs sampling to Item Response Theory in\n1992. Similar to maximum likelihood factor analysis, the factor structure in\nthis Bayesian approach depends only on sufficient statistics, which are\ncomputed from the missing latent data. A second feature of the SAEM algorithm\nis the use of the Robbins-Monro procedure for establishing convergence.\nContrary to Expectation Maximization methods where costly integrals must be\ncalculated, this method is well-suited for highly multidimensional data, and an\nannealing method is implemented to prevent convergence to a local maximum\nlikelihood. Multiple calculations of errors applied within this framework of\nMarkov Chain Monte Carlo are presented to delineate the uncertainty of\nparameter estimates. Given the nature of EFA (exploratory factor analysis), an\nalgorithm is formalized leveraging the Tracy-Widom distribution for the\nretention of factors extracted from an eigenanalysis of the sufficient\nstatistic of the covariance of the augmented data matrix. Simulation conditions\nof dichotomous and polytomous data, from one to ten dimensions of factor\nloadings, are used to assess statistical accuracy and to gauge computational\ntime of the EFA approach of this IRT-specific implementation of the SAEM\nalgorithm. Finally, three applications of this methodology are also reported\nthat demonstrate the effectiveness of the method for enabling timely analyses\nas well as substantive interpretations when this method is applied to real\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2019 23:09:54 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Geis", "Eugene", ""]]}, {"id": "1912.13076", "submitter": "Alex Reinhart", "authors": "Alex Reinhart and Christopher R. Genovese", "title": "Expanding the scope of statistical computing: Training statisticians to\n  be software engineers", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, statistical computing courses have taught the syntax of a\nparticular programming language or specific statistical computation methods.\nSince the publication of Nolan and Temple Lang (2010), we have seen a greater\nemphasis on data wrangling, reproducible research, and visualization. This\nshift better prepares students for careers working with complex datasets and\nproducing analyses for multiple audiences. But, we argue, statisticians are now\noften called upon to develop statistical software, not just analyses, such as R\npackages implementing new analysis methods or machine learning systems\nintegrated into commercial products. This demands different skills.\n  We describe a graduate course that we developed to meet this need by focusing\non four themes: programming practices; software design; important algorithms\nand data structures; and essential tools and methods. Through code review and\nrevision, and a semester-long software project, students practice all the\nskills of software engineering. The course allows students to expand their\nunderstanding of computing as applied to statistical problems while building\nexpertise in the kind of software development that is increasingly the province\nof the working statistician. We see this as a model for the future evolution of\nthe computing curriculum in statistics and data science.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 20:23:51 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 13:57:39 GMT"}, {"version": "v3", "created": "Thu, 29 Oct 2020 00:53:54 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Reinhart", "Alex", ""], ["Genovese", "Christopher R.", ""]]}, {"id": "1912.13132", "submitter": "Florian Gerber", "authors": "Florian Gerber and Douglas W. Nychka", "title": "Parallel cross-validation: a scalable fitting method for Gaussian\n  process models", "comments": "11 pages, 6 Figs. Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) models are widely used to analyze spatially referenced\ndata and to predict values at locations without observations. In contrast to\nmany algorithmic procedures, GP models are based on a statistical framework,\nwhich enables uncertainty quantification of the model structure and\npredictions. Both the evaluation of the likelihood and the prediction involve\nsolving linear systems. Hence, the computational costs are large and limit the\namount of data that can be handled. While there are many approximation\nstrategies that lower the computational cost of GP models, they often provide\nonly sub-optimal support for the parallel computing capabilities of current\n(high-performance) computing environments. We aim at bridging this gap with a\nparameter estimation and prediction method that is designed to be\nparallelizable. More precisely, we divide the spatial domain into overlapping\nsubsets and use cross-validation (CV) to estimate the covariance parameters in\nparallel. We present simulation studies, which assess the accuracy of the\nparameter estimates and predictions. Moreover, we show that our implementation\nhas good weak and strong parallel scaling properties. For illustration, we fit\nan exponential covariance model to a scientifically relevant canopy height\ndataset with 5 million observations. Using 512 processor cores in parallel\nbrings the evaluation time of one covariance parameter configuration to less\nthan 1.5 minutes. The parallel CV method can be easily extended to include\napproximate likelihood methods, multivariate and spatio-temporal data, as well\nas non-stationary covariance models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 00:57:43 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Gerber", "Florian", ""], ["Nychka", "Douglas W.", ""]]}, {"id": "1912.13170", "submitter": "Jeremy Heng", "authors": "Espen Bernton, Jeremy Heng, Arnaud Doucet, Pierre E. Jacob", "title": "Schr\\\"odinger Bridge Samplers", "comments": "53 pages and 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a reference Markov process with initial distribution $\\pi_{0}$ and\ntransition kernels $\\{M_{t}\\}_{t\\in[1:T]}$, for some $T\\in\\mathbb{N}$. Assume\nthat you are given distribution $\\pi_{T}$, which is not equal to the marginal\ndistribution of the reference process at time $T$. In this scenario,\nSchr\\\"odinger addressed the problem of identifying the Markov process with\ninitial distribution $\\pi_{0}$ and terminal distribution equal to $\\pi_{T}$\nwhich is the closest to the reference process in terms of Kullback--Leibler\ndivergence. This special case of the so-called Schr\\\"odinger bridge problem can\nbe solved using iterative proportional fitting, also known as the Sinkhorn\nalgorithm. We leverage these ideas to develop novel Monte Carlo schemes, termed\nSchr\\\"odinger bridge samplers, to approximate a target distribution $\\pi$ on\n$\\mathbb{R}^{d}$ and to estimate its normalizing constant. This is achieved by\niteratively modifying the transition kernels of the reference Markov chain to\nobtain a process whose marginal distribution at time $T$ becomes closer to\n$\\pi_T = \\pi$, via regression-based approximations of the corresponding\niterative proportional fitting recursion. We report preliminary experiments and\nmake connections with other problems arising in the optimal transport, optimal\ncontrol and physics literatures.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 04:49:30 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Bernton", "Espen", ""], ["Heng", "Jeremy", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1912.13336", "submitter": "Melina Freitag", "authors": "Melina A. Freitag", "title": "Numerical Linear Algebra in Data Assimilation", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation is a method that combines observations (that is, real world\ndata) of a state of a system with model output for that system in order to\nimprove the estimate of the state of the system and thereby the model output.\nThe model is usually represented by a discretised partial differential\nequation. The data assimilation problem can be formulated as a large scale\nBayesian inverse problem. Based on this interpretation we will derive the most\nimportant variational and sequential data assimilation approaches, in\nparticular three-dimensional and four-dimensional variational data assimilation\n(3D-Var and 4D-Var) and the Kalman filter. We will then consider more advanced\nmethods which are extensions of the Kalman filter and variational data\nassimilation and pay particular attention to their advantages and\ndisadvantages. The data assimilation problem usually results in a very large\noptimisation problem and/or a very large linear system to solve (due to\ninclusion of time and space dimensions). Therefore, the second part of this\narticle aims to review advances and challenges, in particular from the\nnumerical linear algebra perspective, within the various data assimilation\napproaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 15:04:07 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:51:58 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Freitag", "Melina A.", ""]]}]