[{"id": "1803.00360", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "Computing Bayes factors to measure evidence from experiments: An\n  extension of the BIC approximation", "comments": "to appear in Biometrical Letters", "journal-ref": "Biometrical Letters 55 (2018) 31-43", "doi": "10.2478/bile-2018-0003", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference affords scientists with powerful tools for testing\nhypotheses. One of these tools is the Bayes factor, which indexes the extent to\nwhich support for one hypothesis over another is updated after seeing the data.\nPart of the hesitance to adopt this approach may stem from an unfamiliarity\nwith the computational tools necessary for computing Bayes factors. Previous\nwork has shown that closed form approximations of Bayes factors are relatively\neasy to obtain for between- groups methods, such as an analysis of variance or\nt-test. In this paper, I extend this approximation to develop a formula for the\nBayes factor that directly uses information that is typically reported for\nANOVAs (e.g., the F ratio and degrees of freedom). After giving two examples of\nits use, I report the results of simulations which show that even with minimal\ninput, this approximate Bayes factor produces similar results to existing\nsoftware solutions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 13:44:59 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 22:43:43 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1803.00426", "submitter": "Paul van Mulbregt", "authors": "Paul van Mulbregt", "title": "Computing the Cumulative Distribution Function and Quantiles of the\n  limit of the Two-sided Kolmogorov-Smirnov Statistic", "comments": "20 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1802.06966", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative distribution and quantile functions for the two-sided one\nsample Kolmogorov-Smirnov probability distributions are used for\ngoodness-of-fit testing. The CDF is notoriously difficult to explicitly\ndescribe and to compute, and for large sample size use of the limiting\ndistribution is an attractive alternative, with its lower computational\nrequirements. No closed form solution for the computation of the quantiles is\nknown. Computing the quantile function by a numeric root-finder for any\nspecific probability may require multiple evaluations of both the CDF and its\nderivative. Approximations to both the CDF and its derivative can be used to\nreduce the computational demands. We show that the approximations in use inside\nthe open source SciPy python software result in increased computation, not just\nreduced accuracy, and cause convergence failures in the root-finding. Then we\nprovide alternate algorithms which restore accuracy and efficiency across the\nwhole domain.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 17:02:28 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["van Mulbregt", "Paul", ""]]}, {"id": "1803.00472", "submitter": "Jose Ameijeiras-Alonso", "authors": "Jose Ameijeiras-Alonso, Rosa M. Crujeiras, Alberto Rodr\\'iguez-Casal", "title": "Multimode: An R Package for Mode Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several applied fields, multimodality assessment is a crucial task as a\nprevious exploratory tool or for determining the suitability of certain\ndistributions. The goal of this paper is to present the utilities of the R\npackage multimode, which collects different exploratory and testing\nnonparametric approaches for determining the number of modes and their\nestimated location. Specifically, some graphical tools, allowing for the\nidentification of mode patterns, based on the kernel density estimation are\nprovided (SiZer map, mode tree or mode forest). Several formal testing\nprocedures for determining the number of modes are described in this paper and\nimplemented in the multimode package, including methods based on the ideas of\nthe critical bandwidth, the excess mass or using a combination of both. This\npackage also includes a function for estimating the modes locations and\ndifferent classical data examples that have been considered in mode testing\nliterature.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 15:51:52 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Ameijeiras-Alonso", "Jose", ""], ["Crujeiras", "Rosa M.", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1803.01328", "submitter": "Mingyuan Zhou", "authors": "Hao Zhang, Bo Chen, Dandan Guo, Mingyuan Zhou", "title": "WHAI: Weibull Hybrid Autoencoding Inference for Deep Topic Modeling", "comments": "ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To train an inference network jointly with a deep generative topic model,\nmaking it both scalable to big corpora and fast in out-of-sample prediction, we\ndevelop Weibull hybrid autoencoding inference (WHAI) for deep latent Dirichlet\nallocation, which infers posterior samples via a hybrid of stochastic-gradient\nMCMC and autoencoding variational Bayes. The generative network of WHAI has a\nhierarchy of gamma distributions, while the inference network of WHAI is a\nWeibull upward-downward variational autoencoder, which integrates a\ndeterministic-upward deep neural network, and a stochastic-downward deep\ngenerative model based on a hierarchy of Weibull distributions. The Weibull\ndistribution can be used to well approximate a gamma distribution with an\nanalytic Kullback-Leibler divergence, and has a simple reparameterization via\nthe uniform noise, which help efficiently compute the gradients of the evidence\nlower bound with respect to the parameters of the inference network. The\neffectiveness and efficiency of WHAI are illustrated with experiments on big\ncorpora.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 09:53:59 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 14:57:35 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Bo", ""], ["Guo", "Dandan", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1803.01454", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh and Rahul Mazumder", "title": "Fast Best Subset Selection: Coordinate Descent and Local Combinatorial\n  Optimization Algorithms", "comments": "To appear in Operations Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $L_0$-regularized least squares problem (a.k.a. best subsets) is central\nto sparse statistical learning and has attracted significant attention across\nthe wider statistics, machine learning, and optimization communities. Recent\nwork has shown that modern mixed integer optimization (MIO) solvers can be used\nto address small to moderate instances of this problem. In spite of the\nusefulness of $L_0$-based estimators and generic MIO solvers, there is a steep\ncomputational price to pay when compared to popular sparse learning algorithms\n(e.g., based on $L_1$ regularization). In this paper, we aim to push the\nfrontiers of computation for a family of $L_0$-regularized problems with\nadditional convex penalties. We propose a new hierarchy of necessary optimality\nconditions for these problems. We develop fast algorithms, based on coordinate\ndescent and local combinatorial optimization, that are guaranteed to converge\nto solutions satisfying these optimality conditions. From a statistical\nviewpoint, an interesting story emerges. When the signal strength is high, our\ncombinatorial optimization algorithms have an edge in challenging statistical\nsettings. When the signal is lower, pure $L_0$ benefits from additional convex\nregularization. We empirically demonstrate that our family of $L_0$-based\nestimators can outperform the state-of-the-art sparse learning algorithms in\nterms of a combination of prediction, estimation, and variable selection\nmetrics under various regimes (e.g., different signal strengths, feature\ncorrelations, number of samples and features). Our new open-source sparse\nlearning toolkit L0Learn (available on CRAN and Github) reaches up to a\nthree-fold speedup (with $p$ up to $10^6$) when compared to competing toolkits\nsuch as glmnet and ncvreg.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 01:41:12 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 03:50:59 GMT"}, {"version": "v3", "created": "Sat, 25 Jan 2020 04:40:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1803.01801", "submitter": "Paulo Hubert", "authors": "Paulo Hubert, Linilson Padovese, Julio Stern", "title": "Fast Implementation of a Bayesian Unsupervised Segmentation Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, we have proposed an unsupervised algorithm for audio\nsignal segmentation entirely based on Bayesian methods. In its first\nimplementation, however, the method showed poor computational performance. In\nthis paper we address this question by describing a fast parallel\nimplementation using the Cython library for Python; we use open GSL methods for\nstandard mathematical functions, and the OpenMP framework for parallelization.\nWe also offer a detailed analysis on the sensibility of the algorithm to its\ndifferent parameters, and show its application to real-life subacquatic signals\nobtained off the brazilian South coast. Our code and data are available freely\non github.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 17:47:18 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 17:06:40 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Hubert", "Paulo", ""], ["Padovese", "Linilson", ""], ["Stern", "Julio", ""]]}, {"id": "1803.01999", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi", "title": "ABC and Indirect Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  Indirect inference (II) is a classical likelihood-free approach that\npre-dates the main developments of ABC and relies on simulation from a\nparametric model of interest to determine point estimates of the parameters. It\nis not surprising then that some likelihood-free Bayesian approaches have\nharnessed the II literature. This chapter provides an introduction to II and\ndetails the connections between ABC and II. A particular focus is placed on the\nuse of an auxiliary model with a tractable likelihood function, an approach\ncommonly adopted in the II literature, to facilitate likelihood-free Bayesian\ninferences.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 03:15:50 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Drovandi", "Christopher C", ""]]}, {"id": "1803.02032", "submitter": "Adam Gustafson", "authors": "Adam Gustafson, Hariharan Narayanan", "title": "John's Walk", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CG cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an affine-invariant random walk for drawing uniform random samples\nfrom a convex body $\\mathcal{K} \\subset \\mathbb{R}^n$ that uses maximum volume\ninscribed ellipsoids, known as John's ellipsoids, for the proposal\ndistribution. Our algorithm makes steps using uniform sampling from the John's\nellipsoid of the symmetrization of $\\mathcal{K}$ at the current point. We show\nthat from a warm start, the random walk mixes in $\\widetilde{O}(n^7)$ steps\nwhere the log factors depend only on constants associated with the warm start\nand desired total variation distance to uniformity. We also prove polynomial\nmixing bounds starting from any fixed point $x$ such that for any chord $pq$ of\n$\\mathcal{K}$ containing $x$, $\\left|\\log \\frac{|p-x|}{|q-x|}\\right|$ is\nbounded above by a polynomial in $n$.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 07:01:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 19:36:44 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Gustafson", "Adam", ""], ["Narayanan", "Hariharan", ""]]}, {"id": "1803.02272", "submitter": "Alain Franc", "authors": "Pierre Blanchard (1, 2), Philippe Chaumeil (3, 1), Jean-Marc Frigerio\n  (3, 1), Fr\\'ed\\'eric Rimet (4), Franck Salin (1, 3), Sylvie Th\\'erond (5),\n  Olivier Coulaud (2), Alain Franc (PLEIADE, BioGeCo) ((1) PLEIADE, (2)\n  HiePACS, (3) BioGeCo, (4) CARRTEL, (5) IDRIS)", "title": "A geometric view of Biodiversity: scaling to metagenomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed a new efficient dimensionality reduction algorithm in order\nto investigate new ways of accurately characterizing the biodiversity, namely\nfrom a geometric point of view, scaling with large environmental sets produced\nby NGS ($\\sim 10^5$ sequences). The approach is based on Multidimensional\nScaling (MDS) that allows for mapping items on a set of $n$ points into a low\ndimensional euclidean space given the set of pairwise distances. We compute all\npairwise distances between reads in a given sample, run MDS on the distance\nmatrix, and analyze the projection on first axis, by visualization tools. We\nhave circumvented the quadratic complexity of computing pairwise distances by\nimplementing it on a hyperparallel computer (Turing, a Blue Gene Q), and the\ncubic complexity of the spectral decomposition by implementing a dense random\nprojection based algorithm. We have applied this data analysis scheme on a set\nof $10^5$ reads, which are amplicons of a diatom environmental sample from Lake\nGeneva. Analyzing the shape of the point cloud paves the way for a geometric\nanalysis of biodiversity, and for accurately building OTUs (Operational\nTaxonomic Units), when the data set is too large for implementing unsupervised,\nhierarchical, high-dimensional clustering.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 16:02:26 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Blanchard", "Pierre", "", "PLEIADE, BioGeCo"], ["Chaumeil", "Philippe", "", "PLEIADE, BioGeCo"], ["Frigerio", "Jean-Marc", "", "PLEIADE, BioGeCo"], ["Rimet", "Fr\u00e9d\u00e9ric", "", "PLEIADE, BioGeCo"], ["Salin", "Franck", "", "PLEIADE, BioGeCo"], ["Th\u00e9rond", "Sylvie", "", "PLEIADE, BioGeCo"], ["Coulaud", "Olivier", "", "PLEIADE, BioGeCo"], ["Franc", "Alain", "", "PLEIADE, BioGeCo"]]}, {"id": "1803.02704", "submitter": "Felix Bestehorn", "authors": "Felix Bestehorn and Maike Bestehorn and Markus Bestehorn and Christian\n  Kirches", "title": "A deterministic balancing score algorithm to avoid common pitfalls of\n  propensity score matching", "comments": "25 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching (PSM) is the de-facto standard for estimating\ncausal effects in observational studies. We show that PSM and its\nimplementations are susceptible to several major drawbacks and illustrate these\nfindings using a case study with $17,427$ patients. We derive four formal\nproperties an optimal statistical matching algorithm should meet, and propose\nDeterministic Balancing Score exact Matching (DBSeM) which meets the\naforementioned properties for an exact matching. Furthermore, we investigate\none of the main problems of PSM, that is that common PSM results in one valid\nset of matched pairs or a bootstrapped PSM in a selection of possible valid\nsets of matched pairs. For exact matchings we provide the mathematical proof,\nthat DBSeM, as a result, delivers the expected value of all valid sets of\nmatched pairs for the investigated dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 15:08:08 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 06:52:13 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 15:43:47 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 15:06:06 GMT"}, {"version": "v5", "created": "Fri, 17 May 2019 12:53:28 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Bestehorn", "Felix", ""], ["Bestehorn", "Maike", ""], ["Bestehorn", "Markus", ""], ["Kirches", "Christian", ""]]}, {"id": "1803.03333", "submitter": "Maikol Sol\\'is", "authors": "Maikol Sol\\'is", "title": "Nonparametric estimation of the first order Sobol indices with bootstrap\n  bandwidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that $Y = \\psi(X_1, \\ldots, X_p)$, where $(X_1,\\ldots, X_p)^\\top$ are\nrandom inputs, $Y$ is the output, and $\\psi(\\cdot)$ is an unknown link\nfunction. The Sobol indices gauge the sensitivity of each $X$ against $Y$ by\nestimating the regression curve's variability between them. In this paper, we\nestimate these curves with a kernel-based method. The method allows to estimate\nthe first order indices when the link between the independent and dependent\nvariables is unknown. The kernel-based methods need a bandwidth to average the\nobservations. For finite samples, the cross-validation method is famous to\ndecide this bandwidth. However, it produces a structural bias. To remedy this,\nwe propose a bootstrap procedure which reconstruct the model residuals and\nre-estimate the non-parametric regression curve. With the new set of curves,\nthe procedure corrects the bias in the Sobol index. To test the developed\nmethod, we implemented simulated numerical examples with complex functions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 23:22:38 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 23:29:51 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 21:56:05 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Sol\u00eds", "Maikol", ""]]}, {"id": "1803.03364", "submitter": "Konstantin Zuev M", "authors": "Keegan Mendonca, Vasileios E. Kontosakos, Athanasios A. Pantelous, and\n  Konstantin M. Zuev", "title": "Efficient Pricing of Barrier Options on High Volatility Assets using\n  Subset Simulation", "comments": "41 pages, 9 figures, 3 tables, available at SSRN:\n  https://ssrn.com/abstract=3132336", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.CP stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Barrier options are one of the most widely traded exotic options on stock\nexchanges. In this paper, we develop a new stochastic simulation method for\npricing barrier options and estimating the corresponding execution\nprobabilities. We show that the proposed method always outperforms the standard\nMonte Carlo approach and becomes substantially more efficient when the\nunderlying asset has high volatility, while it performs better than multilevel\nMonte Carlo for special cases of barrier options and underlying assets. These\ntheoretical findings are confirmed by numerous simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 02:49:08 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 00:44:53 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Mendonca", "Keegan", ""], ["Kontosakos", "Vasileios E.", ""], ["Pantelous", "Athanasios A.", ""], ["Zuev", "Konstantin M.", ""]]}, {"id": "1803.03765", "submitter": "Haakon Bakka", "authors": "Haakon Bakka", "title": "How to solve the stochastic partial differential equation that gives a\n  Mat\\'ern random field using the finite element method", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial teaches parts of the finite element method (FEM), and solves a\nstochastic partial differential equation (SPDE). The contents herein are\nconsidered \"known\" in the numerics literature, but for statisticians it is very\ndifficult to find a resource for learning these ideas in a timely manner\n(without doing a year's worth of courses in numerics). The goal of this\ntutorial is to be pedagogical and explain the computations/theory to a\nstatistician. This is not a practical tutorial, there is little computer code,\nand no data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 06:02:04 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 11:32:38 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Bakka", "Haakon", ""]]}, {"id": "1803.03858", "submitter": "Sara Algeri", "authors": "Sara Algeri and David A. van Dyk", "title": "Testing One Hypothesis Multiple Times: The Multidimensional Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of new rare signals in data, the detection of a sudden\nchange in a trend, and the selection of competing models, are among the most\nchallenging problems in statistical practice. These challenges can be tackled\nusing a test of hypothesis where a nuisance parameter is present only under the\nalternative, and a computationally efficient solution can be obtained by the\n\"Testing One Hypothesis Multiple times\" (TOHM) method. In the one-dimensional\nsetting, a fine discretization of the space of the non-identifiable parameter\nis specified, and a global p-value is obtained by approximating the\ndistribution of the supremum of the resulting stochastic process. In this\npaper, we propose a computationally efficient inferential tool to perform TOHM\nin the multidimensional setting. Here, the approximations of interest typically\ninvolve the expected Euler Characteristics (EC) of the excursion set of the\nunderlying random field. We introduce a simple algorithm to compute the EC in\nmultiple dimensions and for arbitrary large significance levels. This leads to\nan highly generalizable computational tool to perform inference under\nnon-standard regularity conditions.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 19:49:26 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2019 17:39:19 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Algeri", "Sara", ""], ["van Dyk", "David A.", ""]]}, {"id": "1803.04084", "submitter": "Yun-Jhong Wu", "authors": "Yun-Jhong Wu, Elizaveta Levina, Ji Zhu", "title": "Link prediction for egocentrically sampled networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction in networks is typically accomplished by estimating or\nranking the probabilities of edges for all pairs of nodes. In practice,\nespecially for social networks, the data are often collected by egocentric\nsampling, which means selecting a subset of nodes and recording all of their\nedges. This sampling mechanism requires different prediction tools than the\ntypical assumption of links missing at random. We propose a new computationally\nefficient link prediction algorithm for egocentrically sampled networks, which\nestimates the underlying probability matrix by estimating its row space. For\nnetworks created by sampling rows, our method outperforms many popular link\nprediction and graphon estimation techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 01:37:53 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Wu", "Yun-Jhong", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1803.04246", "submitter": "Colin  Gillespie", "authors": "Richard J. Boys and Holly F. Ainsworth and Colin S. Gillespie", "title": "Bayesian inference for a partially observed birth-death process using\n  data on proportions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic kinetic models are often used to describe complex biological\nprocesses. Typically these models are analytically intractable and have unknown\nparameters which need to be estimated from observed data. Ideally we would have\nmeasurements on all interacting chemical species in the process, observed\ncontinuously in time. However, in practice, measurements are taken only at a\nrelatively few time-points. In some situations, only very limited observation\nof the process is available, such as when experimenters can only observe noisy\nobservations on the proportion of cells that are alive. This makes the\ninference task even more problematic. We consider a range of data-poor\nscenarios and investigate the performance of various computationally intensive\nBayesian algorithms in determining the posterior distribution using data on\nproportions from a simple birth-death process.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:47:16 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Boys", "Richard J.", ""], ["Ainsworth", "Holly F.", ""], ["Gillespie", "Colin S.", ""]]}, {"id": "1803.04254", "submitter": "Colin  Gillespie", "authors": "Colin S. Gillespie and Richard J. Boys", "title": "Efficient construction of Bayes optimal designs for stochastic process\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic process models are now commonly used to analyse complex\nbiological, ecological and industrial systems. Increasingly there is a need to\ndeliver accurate estimates of model parameters and assess model fit by\noptimizing the timing of measurement of these processes. Standard methods to\nconstruct Bayes optimal designs, such as the well known \\Muller algorithm, are\ncomputationally intensive even for relatively simple models. A key issue is\nthat, in determining the merit of a design, the utility function typically\nrequires summaries of many parameter posterior distributions, each determined\nvia a computer-intensive scheme such as MCMC. This paper describes a fast and\ncomputationally efficient scheme to determine optimal designs for stochastic\nprocess models. The algorithm compares favourably with other methods for\ndetermining optimal designs and can require up to an order of magnitude fewer\nutility function evaluations for the same accuracy in the optimal design\nsolution. It benefits from being embarrassingly parallel and is ideal for\nrunning on multi-core computers. The method is illustrated by determining\ndifferent sized optimal designs for three problems of increasing complexity.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 13:55:08 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 08:00:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Gillespie", "Colin S.", ""], ["Boys", "Richard J.", ""]]}, {"id": "1803.04582", "submitter": "Dalia Chakrabarty Dr.", "authors": "Kangrui Wang and Dalia Chakrabarty", "title": "Deep Bayesian Supervised Learning given Hypercuboidally-shaped,\n  Discontinuous Data, using Compound Tensor-Variate & Scalar-Variate Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We undertake Bayesian learning of the high-dimensional functional\nrelationship between a system parameter vector and an observable, that is in\ngeneral tensor-valued. The ultimate aim is Bayesian inverse prediction of the\nsystem parameters, at which test data is recorded. We attempt such learning\ngiven hypercuboidally-shaped data that displays strong discontinuities,\nrendering learning challenging. We model the sought high-dimensional function,\nwith a tensor-variate Gaussian Process (GP), and use three independent ways for\nlearning covariance matrices of the resulting likelihood, which is\nTensor-Normal. We demonstrate that the discontinuous data demands that\nimplemented covariance kernels be non-stationary--achieved by modelling each\nkernel hyperparameter, as a function of the sample function of the invoked\ntensor-variate GP. Each such function can be shown to be temporally-evolving,\nand treated as a realisation from a distinct scalar-variate GP, with covariance\ndescribed adaptively by collating information from a historical set of samples\nof chosen sample-size. We prove that deep-learning using 2-\"layers\", suffice,\nwhere the outer-layer comprises the tensor-variate GP, compounded with multiple\nscalar-variate GPs in the \"inner-layer\", and undertake inference with\nMetropolis-within-Gibbs. We apply our method to a cuboidally-shaped,\ndiscontinuous, real dataset, and subsequently perform forward prediction to\ngenerate data from our model, given our results--to perform model-checking.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 00:16:18 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2018 07:34:16 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Wang", "Kangrui", ""], ["Chakrabarty", "Dalia", ""]]}, {"id": "1803.04739", "submitter": "Evangelos Evangelou", "authors": "Evangelos Evangelou and Vivekananda Roy", "title": "Estimation and prediction for spatial generalized linear mixed models\n  with parametric links via reparameterized importance sampling", "comments": null, "journal-ref": "Spatial Statistics, 2019", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial generalized linear mixed models (SGLMMs) are popular for analyzing\nnon-Gaussian spatial data. These models assume a prescribed link function that\nrelates the underlying spatial field with the mean response. There are\ncircumstances, such as when the data contain outlying observations, where the\nuse of a prescribed link function can result in poor fit, which can be improved\nby using a parametric link function. Some popular link functions, such as the\nBox-Cox, are unsuitable because they are inconsistent with the Gaussian\nassumption of the spatial field. We present sensible choices of parametric link\nfunctions which possess desirable properties. It is important to estimate the\nparameters of the link function, rather than assume a known value. To that end,\nwe present a generalized importance sampling (GIS) estimator based on multiple\nMarkov chains for empirical Bayes analysis of SGLMMs. The GIS estimator,\nalthough more efficient than the simple importance sampling, can be highly\nvariable when used to estimate the parameters of certain link functions. Using\nsuitable reparameterizations of the Monte Carlo samples, we propose modified\nGIS estimators that do not suffer from high variability. We use Laplace\napproximation for choosing the multiple importance densities in the GIS\nestimator. Finally, we develop a methodology for selecting models with\nappropriate link function family, which extends to choosing a spatial\ncorrelation function as well. We present an ensemble prediction of the mean\nresponse by appropriately weighting the estimates from different models. The\nproposed methodology is illustrated using simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 11:47:57 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 17:18:32 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 07:23:15 GMT"}, {"version": "v4", "created": "Tue, 8 Jan 2019 12:40:47 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Evangelou", "Evangelos", ""], ["Roy", "Vivekananda", ""]]}, {"id": "1803.04947", "submitter": "Matthew Dixon", "authors": "Matthew Dixon and Tyler Ward", "title": "Takeuchi's Information Criteria as a form of Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Takeuchi's Information Criteria (TIC) is a linearization of maximum\nlikelihood estimator bias which shrinks the model parameters towards the\nmaximum entropy distribution, even when the model is mis-specified. In\nstatistical machine learning, $L_2$ regularization (a.k.a. ridge regression)\nalso introduces a parameterized bias term with the goal of minimizing\nout-of-sample entropy, but generally requires a numerical solver to find the\nregularization parameter. This paper presents a novel regularization approach\nbased on TIC; the approach does not assume a data generation process and\nresults in a higher entropy distribution through more efficient sample noise\nsuppression. The resulting objective function can be directly minimized to\nestimate and select the best model, without the need to select a regularization\nparameter, as in ridge regression. Numerical results applied to a synthetic\nhigh dimensional dataset generated from a logistic regression model demonstrate\nsuperior model performance when using the TIC based regularization over a $L_1$\nand a $L_2$ penalty term.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 17:31:17 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 22:40:48 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Dixon", "Matthew", ""], ["Ward", "Tyler", ""]]}, {"id": "1803.05165", "submitter": "Thomas Lumley", "authors": "Thomas Lumley", "title": "Fast generalised linear models by database sampling and one-step\n  polishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, I show how to fit a generalised linear model to $N$\nobservations on $p$ variables stored in a relational database, using one\nsampling query and one aggregation queries, as long as $N^{\\frac{1}{2}+\\delta}$\nobservations can be stored in memory. The resulting estimator is fully\nefficient and asymptotically equivalent to the maximum likelihood estimator,\nand so its variance can be estimated from the Fisher information in the usual\nway. A proof-of-concept implementation uses R with MonetDB and with SQLite, and\ncould easily be adapted to other popular databases. I illustrate the approach\nwith examples of taxi-trip data in New York City and factors related to car\ncolour in New Zealand.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 08:29:13 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Lumley", "Thomas", ""]]}, {"id": "1803.05554", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Tamara Broderick and Caroline Uhler", "title": "Minimal I-MAP MCMC for Scalable Structure Discovery in Causal DAG Models", "comments": "Proceedings of the 30th International Conference on Machine Learning.\n  2018, to appear. 16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a Bayesian network (BN) from data can be useful for decision-making\nor discovering causal relationships. However, traditional methods often fail in\nmodern applications, which exhibit a larger number of observed variables than\ndata points. The resulting uncertainty about the underlying network as well as\nthe desire to incorporate prior information recommend a Bayesian approach to\nlearning the BN, but the highly combinatorial structure of BNs poses a striking\nchallenge for inference. The current state-of-the-art methods such as order\nMCMC are faster than previous methods but prevent the use of many natural\nstructural priors and still have running time exponential in the maximum\nindegree of the true directed acyclic graph (DAG) of the BN. We here propose an\nalternative posterior approximation based on the observation that, if we\nincorporate empirical conditional independence tests, we can focus on a\nhigh-probability DAG associated with each order of the vertices. We show that\nour method allows the desired flexibility in prior specification, removes\ntiming dependence on the maximum indegree and yields provably good posterior\napproximations; in addition, we show that it achieves superior accuracy,\nscalability, and sampler mixing on several datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 00:53:25 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 23:44:43 GMT"}, {"version": "v3", "created": "Sun, 24 Jun 2018 15:52:36 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""], ["Uhler", "Caroline", ""]]}, {"id": "1803.05664", "submitter": "David R\\\"ugamer", "authors": "Benjamin S\\\"afken, David R\\\"ugamer, Thomas Kneib and Sonja Greven", "title": "Conditional Model Selection in Mixed-Effects Models with cAIC4", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection in mixed models based on the conditional distribution is\nappropriate for many practical applications and has been a focus of recent\nstatistical research. In this paper we introduce the R-package cAIC4 that\nallows for the computation of the conditional Akaike Information Criterion\n(cAIC). Computation of the conditional AIC needs to take into account the\nuncertainty of the random effects variance and is therefore not\nstraightforward. We introduce a fast and stable implementation for the\ncalculation of the cAIC for linear mixed models estimated with lme4 and\nadditive mixed models estimated with gamm4 . Furthermore, cAIC4 offers a\nstepwise function that allows for a fully automated stepwise selection scheme\nfor mixed models based on the conditional AIC. Examples of many possible\napplications are presented to illustrate the practical impact and easy handling\nof the package.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 09:57:21 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 15:21:02 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["S\u00e4fken", "Benjamin", ""], ["R\u00fcgamer", "David", ""], ["Kneib", "Thomas", ""], ["Greven", "Sonja", ""]]}, {"id": "1803.06010", "submitter": "Shannon McCurdy", "authors": "Shannon R. McCurdy", "title": "Ridge Regression and Provable Deterministic Ridge Leverage Score\n  Sampling", "comments": "24 pages, 15 figures. Minor changes such as typos fixed, some\n  background discussion added, references added", "journal-ref": "Advances in Neural Information Processing Systems (NeurIPS 2018),\n  Montreal, Canada", "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge leverage scores provide a balance between low-rank approximation and\nregularization, and are ubiquitous in randomized linear algebra and machine\nlearning. Deterministic algorithms are also of interest in the moderately big\ndata regime, because deterministic algorithms provide interpretability to the\npractitioner by having no failure probability and always returning the same\nresults.\n  We provide provable guarantees for deterministic column sampling using ridge\nleverage scores. The matrix sketch returned by our algorithm is a column subset\nof the original matrix, yielding additional interpretability. Like the\nrandomized counterparts, the deterministic algorithm provides (1 + {\\epsilon})\nerror column subset selection, (1 + {\\epsilon}) error projection-cost\npreservation, and an additive-multiplicative spectral bound. We also show that\nunder the assumption of power-law decay of ridge leverage scores, this\ndeterministic algorithm is provably as accurate as randomized algorithms.\n  Lastly, ridge regression is frequently used to regularize ill-posed linear\nleast-squares problems. While ridge regression provides shrinkage for the\nregression coefficients, many of the coefficients remain small but non-zero.\nPerforming ridge regression with the matrix sketch returned by our algorithm\nand a particular regularization parameter forces coefficients to zero and has a\nprovable (1 + {\\epsilon}) bound on the statistical risk. As such, it is an\ninteresting alternative to elastic net regularization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:35:55 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 21:06:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["McCurdy", "Shannon R.", ""]]}, {"id": "1803.06287", "submitter": "Ranjan Maitra", "authors": "Karl T. Pazdernik and Ranjan Maitra and Douglas Nychka and Stephen\n  Sain", "title": "Reduced Basis Kriging for Big Spatial Fields", "comments": "Sankhya, Series A, accepted for publication", "journal-ref": "Sankhya, Series A, 80:2:280--300, 2018", "doi": "10.1007/s13171-018-0129-7", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, a common method for prediction over a Gaussian random\nfield (GRF) is maximum likelihood estimation combined with kriging. For massive\ndata sets, kriging is computationally intensive, both in terms of CPU time and\nmemory, and so fixed rank kriging has been proposed as a solution. The method\nhowever still involves operations on large matrices, so we develop an\nalteration to this method by utilizing the approximations made in fixed rank\nkriging combined with restricted maximum likelihood estimation and sparse\nmatrix methodology. Experiments show that our methodology can provide\nadditional gains in computational efficiency over fixed-rank kriging without\nloss of accuracy in prediction. The methodology is applied to climate data\narchived by the United States National Climate Data Center, with very good\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 03:37:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2018 12:11:14 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Pazdernik", "Karl T.", ""], ["Maitra", "Ranjan", ""], ["Nychka", "Douglas", ""], ["Sain", "Stephen", ""]]}, {"id": "1803.06295", "submitter": "Charanraj Thimmisetty", "authors": "Charanraj A. Thimmisetty, Wenju Zhao, Xiao Chen, Charles H. Tong,\n  Joshua A. White", "title": "High-dimensional Stochastic Inversion via Adjoint Models and Machine\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing stochastic inversion on a computationally expensive forward\nsimulation model with a high-dimensional uncertain parameter space (e.g. a\nspatial random field) is computationally prohibitive even with gradient\ninformation provided. Moreover, the `nonlinear' mapping from parameters to\nobservables generally gives rise to non-Gaussian posteriors even with Gaussian\npriors, thus hampering the use of efficient inversion algorithms designed for\nmodels with Gaussian assumptions. In this paper, we propose a novel Bayesian\nstochastic inversion methodology, characterized by a tight coupling between a\ngradient-based Langevin Markov Chain Monte Carlo (LMCMC) method and a kernel\nprincipal component analysis (KPCA). This approach addresses the\n`curse-of-dimensionality' via KPCA to identify a low-dimensional feature space\nwithin the high-dimensional and nonlinearly correlated spatial random field.\nMoreover, non-Gaussian full posterior probability distribution functions are\nestimated via an efficient LMCMC method on both the projected low-dimensional\nfeature space and the recovered high-dimensional parameter space. We\ndemonstrate this computational framework by integrating and adapting recent\ndevelopments such as data-driven statistics-on-manifolds constructions and\nreduction-through-projection techniques to solve inverse problems in linear\nelasticity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 16:21:40 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Thimmisetty", "Charanraj A.", ""], ["Zhao", "Wenju", ""], ["Chen", "Xiao", ""], ["Tong", "Charles H.", ""], ["White", "Joshua A.", ""]]}, {"id": "1803.06328", "submitter": "Tom Rainforth", "authors": "Tom Rainforth", "title": "Nesting Probabilistic Programs", "comments": "Published at UAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the notion of nesting probabilistic programming queries and\ninvestigate the resulting statistical implications. We demonstrate that while\nquery nesting allows the definition of models which could not otherwise be\nexpressed, such as those involving agents reasoning about other agents,\nexisting systems take approaches which lead to inconsistent estimates. We show\nhow to correct this by delineating possible ways one might want to nest queries\nand asserting the respective conditions required for convergence. We further\nintroduce a new online nested Monte Carlo estimator that makes it substantially\neasier to ensure these conditions are met, thereby providing a simple framework\nfor designing statistically correct inference engines. We prove the correctness\nof this online estimator and show that, when using the recommended setup, its\nasymptotic variance is always better than that of the equivalent fixed\nestimator, while its bias is always within a factor of two.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 17:30:35 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 14:48:32 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Rainforth", "Tom", ""]]}, {"id": "1803.06387", "submitter": "Xi Chen", "authors": "Xi Chen, Mike Hobson, Saptarshi Das, Paul Gelderblom", "title": "Improving the efficiency and robustness of nested sampling using\n  posterior repartitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real-world Bayesian inference applications, prior assumptions regarding\nthe parameters of interest may be unrepresentative of their actual values for a\ngiven dataset. In particular, if the likelihood is concentrated far out in the\nwings of the assumed prior distribution, this can lead to extremely inefficient\nexploration of the resulting posterior by nested sampling algorithms, with\nunnecessarily high associated computational costs. Simple solutions such as\nbroadening the prior range in such cases might not be appropriate or possible\nin real-world applications, for example when one wishes to assume a single\nstandardised prior across the analysis of a large number of datasets for which\nthe true values of the parameters of interest may vary. This work therefore\nintroduces a posterior repartitioning (PR) method for nested sampling\nalgorithms, which addresses the problem by redefining the likelihood and prior\nwhile keeping their product fixed, so that the posterior inferences and\nevidence estimates remain unchanged but the efficiency of the nested sampling\nprocess is significantly increased. Numerical results show that the PR method\nprovides a simple yet powerful refinement for nested sampling algorithms to\naddress the issue of unrepresentative priors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 20:08:53 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 13:48:35 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 14:46:14 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Chen", "Xi", ""], ["Hobson", "Mike", ""], ["Das", "Saptarshi", ""], ["Gelderblom", "Paul", ""]]}, {"id": "1803.06505", "submitter": "Radu Stoica", "authors": "R. S. Stoica, M. Deaconu, L. Hurtado", "title": "A simulated annealing procedure based on the ABC Shadow algorithm for\n  statistical inference of point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a new algorithm for sampling posteriors of unnormalised probability\ndensities, called ABC Shadow, was proposed in [8]. This talk introduces a\nglobal optimisation procedure based on the ABC Shadow simulation dynamics.\nFirst the general method is explained, and then results on simulated and real\ndata are presented. The method is rather general, in the sense that it applies\nfor probability densities that are continuously differentiable with respect to\ntheir parameters\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 13:45:29 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Stoica", "R. S.", ""], ["Deaconu", "M.", ""], ["Hurtado", "L.", ""]]}, {"id": "1803.06518", "submitter": "Eric Chi", "authors": "Eric C. Chi and Brian R. Gaines and Will Wei Sun and Hua Zhou and Jian\n  Yang", "title": "Provable Convex Co-clustering of Tensors", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research, 21(214):1-58, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis is a fundamental tool for pattern discovery of complex\nheterogeneous data. Prevalent clustering methods mainly focus on vector or\nmatrix-variate data and are not applicable to general-order tensors, which\narise frequently in modern scientific and business applications. Moreover,\nthere is a gap between statistical guarantees and computational efficiency for\nexisting tensor clustering solutions due to the nature of their non-convex\nformulations. In this work, we bridge this gap by developing a provable convex\nformulation of tensor co-clustering. Our convex co-clustering (CoCo) estimator\nenjoys stability guarantees and its computational and storage costs are\npolynomial in the size of the data. We further establish a non-asymptotic error\nbound for the CoCo estimator, which reveals a surprising \"blessing of\ndimensionality\" phenomenon that does not exist in vector or matrix-variate\ncluster analysis. Our theoretical findings are supported by extensive simulated\nstudies. Finally, we apply the CoCo estimator to the cluster analysis of\nadvertisement click tensor data from a major online company. Our clustering\nresults provide meaningful business insights to improve advertising\neffectiveness.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:15:28 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 16:53:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chi", "Eric C.", ""], ["Gaines", "Brian R.", ""], ["Sun", "Will Wei", ""], ["Zhou", "Hua", ""], ["Yang", "Jian", ""]]}, {"id": "1803.06536", "submitter": "Yuanzhi Huang", "authors": "Yuanzhi Huang, Steven Gilmour, Kalliopi Mylona, Peter Goos", "title": "Optimal Design of Experiments for Nonlinear Response Surface Models", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12313", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many chemical and biological experiments involve multiple treatment factors\nand often it is convenient to fit a nonlinear model in these factors. This\nnonlinear model can be mechanistic, empirical or a hybrid of the two. Motivated\nby experiments in chemical engineering, we focus on D-optimal design for\nmultifactor nonlinear response surfaces in general. In order to find and study\noptimal designs, we first implement conventional point and coordinate exchange\nalgorithms. Next, we develop a novel multiphase optimisation method to\nconstruct D-optimal designs with improved properties. The benefits of this\nmethod are demonstrated by application to two experiments involving nonlinear\nregression models. The designs obtained are shown to be considerably more\ninformative than designs obtained using traditional design optimality\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 16:40:05 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2018 10:58:34 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Huang", "Yuanzhi", ""], ["Gilmour", "Steven", ""], ["Mylona", "Kalliopi", ""], ["Goos", "Peter", ""]]}, {"id": "1803.06645", "submitter": "Christopher Drovandi Dr", "authors": "Christopher C Drovandi, Clara Grazian, Kerrie Mengersen, Christian\n  Robert", "title": "Approximating the Likelihood in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter will appear in the forthcoming Handbook of Approximate Bayesian\nComputation (2018).\n  The conceptual and methodological framework that underpins approximate\nBayesian computation (ABC) is targetted primarily towards problems in which the\nlikelihood is either challenging or missing. ABC uses a simulation-based\nnon-parametric estimate of the likelihood of a summary statistic and assumes\nthat the generation of data from the model is computationally cheap. This\nchapter reviews two alternative approaches for estimating the intractable\nlikelihood, with the goal of reducing the necessary model simulations to\nproduce an approximate posterior. The first of these is a Bayesian version of\nthe synthetic likelihood (SL), initially developed by Wood (2010), which uses a\nmultivariate normal approximation to the summary statistic likelihood. Using\nthe parametric approximation as opposed to the non-parametric approximation of\nABC, it is possible to reduce the number of model simulations required. The\nsecond likelihood approximation method we consider in this chapter is based on\nthe empirical likelihood (EL), which is a non-parametric technique and involves\nmaximising a likelihood constructed empirically under a set of moment\nconstraints. Mengersen et al (2013) adapt the EL framework so that it can be\nused to form an approximate posterior for problems where ABC can be applied,\nthat is, for models with intractable likelihoods. However, unlike ABC and the\nBayesian SL (BSL), the Bayesian EL (BCel) approach can be used to completely\navoid model simulations in some cases. The BSL and BCel methods are illustrated\non models of varying complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 11:43:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Drovandi", "Christopher C", ""], ["Grazian", "Clara", ""], ["Mengersen", "Kerrie", ""], ["Robert", "Christian", ""]]}, {"id": "1803.06673", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson and Ravi Varadhan", "title": "Damped Anderson acceleration with restarts and monotonicity control for\n  accelerating EM and EM-like algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is a well-known iterative method\nfor computing maximum likelihood estimates from incomplete data. Despite its\nnumerous advantages, a main drawback of the EM algorithm is its frequently\nobserved slow convergence which often hinders the application of EM algorithms\nin high-dimensional problems or in other complex settings.To address the need\nfor more rapidly convergent EM algorithms, we describe a new class of\nacceleration schemes that build on the Anderson acceleration technique for\nspeeding fixed-point iterations. Our approach is effective at greatly\naccelerating the convergence of EM algorithms and is automatically scalable to\nhigh dimensional settings. Through the introduction of periodic algorithm\nrestarts and a damping factor, our acceleration scheme provides faster and more\nrobust convergence when compared to un-modified Anderson acceleration while\nalso improving global convergence. Crucially, our method works as an\n\"off-the-shelf\" method in that it may be directly used to accelerate any EM\nalgorithm without relying on the use of any model-specific features or\ninsights. Through a series of simulation studies involving five representative\nproblems, we show that our algorithm is substantially faster than the existing\nstate-of-art acceleration schemes.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:01:22 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 13:59:42 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1803.06675", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Jacob Bien", "title": "Rare Feature Selection in High Dimensions", "comments": "42 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in modern prediction problems for many predictor variables to be\ncounts of rarely occurring events. This leads to design matrices in which many\ncolumns are highly sparse. The challenge posed by such \"rare features\" has\nreceived little attention despite its prevalence in diverse areas, ranging from\nnatural language processing (e.g., rare words) to biology (e.g., rare species).\nWe show, both theoretically and empirically, that not explicitly accounting for\nthe rareness of features can greatly reduce the effectiveness of an analysis.\nWe next propose a framework for aggregating rare features into denser features\nin a flexible manner that creates better predictors of the response. Our\nstrategy leverages side information in the form of a tree that encodes feature\nsimilarity.\n  We apply our method to data from TripAdvisor, in which we predict the\nnumerical rating of a hotel based on the text of the associated review. Our\nmethod achieves high accuracy by making effective use of rare words; by\ncontrast, the lasso is unable to identify highly predictive words if they are\ntoo rare. A companion R package, called rare, implements our new estimator,\nusing the alternating direction method of multipliers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:15:49 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:30:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1803.07418", "submitter": "Yang Feng", "authors": "Emre Demirkaya, Yang Feng, Pallavi Basu, Jinchi Lv", "title": "Large-Scale Model Selection with Misspecification", "comments": "38 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1412.7468", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:10:12 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Demirkaya", "Emre", ""], ["Feng", "Yang", ""], ["Basu", "Pallavi", ""], ["Lv", "Jinchi", ""]]}, {"id": "1803.07587", "submitter": "Joshua Lukemire", "authors": "Joshua Lukemire, Yikai Wang, Amit Verma, Ying Guo", "title": "HINT: A Hierarchical Independent Component Analysis Toolbox for\n  Investigating Brain Functional Networks using Neuroimaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a popular tool for investigating\nbrain organization in neuroscience research. In fMRI studies, an important goal\nis to study how brain networks are modulated by subjects' clinical and\ndemographic variables. Existing ICA methods and toolboxes don't incorporate\nsubjects' covariates effects in ICA estimation of brain networks, which\npotentially leads to loss in accuracy and statistical power in detecting brain\nnetwork differences between subjects' groups. We introduce a Matlab toolbox,\nHINT (Hierarchical INdependent component analysis Toolbox), that provides a\nhierarchical covariate-adjusted ICA (hc-ICA) for modeling and testing covariate\neffects and generates model-based estimates of brain networks on both the\npopulation- and individual-level. HINT provides a user-friendly Matlab GUI that\nallows users to easily load images, specify covariate effects, monitor model\nestimation via an EM algorithm, specify hypothesis tests, and visualize\nresults. HINT also has a command line interface which allows users to\nconveniently run and reproduce the analysis with a script. HINT implements a\nnew multi-level probabilistic ICA model for group ICA. It provides a\nstatistically principled ICA modeling framework for investigating covariate\neffects on brain networks. HINT can also generate and visualize model-based\nnetwork estimates for user-specified subject groups, which greatly facilitates\ngroup comparisons.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 18:22:06 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 19:40:48 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 23:10:21 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 18:42:16 GMT"}, {"version": "v5", "created": "Mon, 6 Apr 2020 19:54:43 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Lukemire", "Joshua", ""], ["Wang", "Yikai", ""], ["Verma", "Amit", ""], ["Guo", "Ying", ""]]}, {"id": "1803.07715", "submitter": "Emily Morris", "authors": "Emily Morris, Kevin He, Yanming Li, Yi Li, and Jian Kang", "title": "SurvBoost: An R Package for High-Dimensional Variable Selection in the\n  Stratified Proportional Hazards Model via Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional variable selection in the proportional hazards (PH) model\nhas many successful applications in different areas. In practice, data may\ninvolve confounding variables that do not satisfy the PH assumption, in which\ncase the stratified proportional hazards (SPH) model can be adopted to control\nthe confounding effects by stratification of the confounding variable, without\ndirectly modeling the confounding effects. However, there is lack of\ncomputationally efficient statistical software for high-dimensional variable\nselection in the SPH model. In this work, an R package, SurvBoost, is developed\nto implement the gradient boosting algorithm for fitting the SPH model with\nhigh-dimensional covariate variables and other confounders. Extensive\nsimulation studies demonstrate that in many scenarios SurvBoost can achieve a\nbetter selection accuracy and reduce computational time substantially compared\nto the existing R package that implements boosting algorithms without\nstratification. The proposed R package is also illustrated by an analysis of\nthe gene expression data with survival outcome in The Cancer Genome Atlas\n(TCGA) study. In addition, a detailed hands-on tutorial for SurvBoost is\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 01:53:02 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Morris", "Emily", ""], ["He", "Kevin", ""], ["Li", "Yanming", ""], ["Li", "Yi", ""], ["Kang", "Jian", ""]]}, {"id": "1803.07813", "submitter": "Krzysztof Domino", "authors": "Krzysztof Domino, Adam Glos", "title": "Introducing higher order correlations to marginals' subset of\n  multivariate data by means of Archimedean copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the algorithm that alters the subset of marginals\nof multivariate standard distributed data into such modelled by an Archimedean\ncopula. Proposed algorithm leaves a correlation matrix almost unchanged, but\nintroduces a higher order correlation into a subset of marginals. Our data\ntransformation algorithm can be used to analyse whether particular machine\nlearning algorithm, especially a dimensionality reduction one, utilises higher\norder correlations or not. We present an exemplary application on two features\nselection algorithms, mention that features selection is one of the approaches\nto dimensionality reduction. To measure higher order correlation, we use\nmultivariate higher order cumulants, hence to utilises higher order\ncorrelations be to use the Joint Skewness Band Selection (JSBS) algorithm that\nuses third-order multivariate cumulant. We show the robust performance of the\nJSBS in contrary to the poor performance of the Maximum Ellipsoid Volume (MEV)\nalgorithm that does not utilise such higher order correlations. With this\nresult, we confirm the potential application of our data generation algorithm\nto analyse a performance of various dimensionality reduction algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 09:42:39 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 10:44:30 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Domino", "Krzysztof", ""], ["Glos", "Adam", ""]]}, {"id": "1803.07984", "submitter": "Dan Li", "authors": "Dan Li and Sonia Martinez", "title": "Online data assimilation in distributionally robust optimization", "comments": "Appeared in CDC 2018", "journal-ref": null, "doi": "10.1109/CDC.2018.8619159", "report-no": null, "categories": "math.OC cs.SY eess.SP eess.SY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a class of real-time decision making problems to\nminimize the expected value of a function that depends on a random variable\n$\\xi$ under an unknown distribution $\\mathbb{P}$. In this process, samples of\n$\\xi$ are collected sequentially in real time, and the decisions are made,\nusing the real-time data, to guarantee out-of-sample performance. We approach\nthis problem in a distributionally robust optimization framework and propose a\nnovel Online Data Assimilation Algorithm for this purpose. This algorithm\nguarantees the out-of-sample performance in high probability, and gradually\nimproves the quality of the data-driven decisions by incorporating the\nstreaming data. We show that the Online Data Assimilation Algorithm guarantees\nconvergence under the streaming data, and a criteria for termination of the\nalgorithm after certain number of data has been collected.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 16:09:44 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 20:48:19 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 22:35:38 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Dan", ""], ["Martinez", "Sonia", ""]]}, {"id": "1803.08021", "submitter": "Shusen Wang", "authors": "Miles E. Lopes, Shusen Wang, Michael W. Mahoney", "title": "Error Estimation for Randomized Least-Squares Algorithms via the\n  Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the course of the past decade, a variety of randomized algorithms have\nbeen proposed for computing approximate least-squares (LS) solutions in\nlarge-scale settings. A longstanding practical issue is that, for any given\ninput, the user rarely knows the actual error of an approximate solution\n(relative to the exact solution). Likewise, it is difficult for the user to\nknow precisely how much computation is needed to achieve the desired error\ntolerance. Consequently, the user often appeals to worst-case error bounds that\ntend to offer only qualitative guidance. As a more practical alternative, we\npropose a bootstrap method to compute a posteriori error estimates for\nrandomized LS algorithms. These estimates permit the user to numerically assess\nthe error of a given solution, and to predict how much work is needed to\nimprove a \"preliminary\" solution. In addition, we provide theoretical\nconsistency results for the method, which are the first such results in this\ncontext (to the best of our knowledge). From a practical standpoint, the method\nalso has considerable flexibility, insofar as it can be applied to several\npopular sketching algorithms, as well as a variety of error metrics. Moreover,\nthe extra step of error estimation does not add much cost to an underlying\nsketching algorithm. Finally, we demonstrate the effectiveness of the method\nwith empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:19:41 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 05:01:36 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Lopes", "Miles E.", ""], ["Wang", "Shusen", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1803.08027", "submitter": "Ranjan Maitra", "authors": "Ranjan Maitra", "title": "Efficient Bandwidth Estimation in Two-dimensional Filtered\n  Backprojection Reconstruction", "comments": "12 pages, 7 figures, submitted to IEEE Transactions on Image\n  Processing", "journal-ref": null, "doi": "10.1109/TIP.2019.2919428", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized cross-validation approach to estimate the reconstruction filter\nbandwidth in two-dimensional Filtered Backprojection is presented. The method\nwrites the reconstruction equation in equivalent backprojected filtering form,\nderives results on eigendecomposition of symmetric two-dimensional circulant\nmatrices and applies them to make bandwidth estimation a computationally\nefficient operation within the context of standard backprojected filtering\nreconstruction. Performance evaluations on a wide range of simulated emission\ntomography experiments give promising results. The superior performance holds\nat both low and high total expected counts, pointing to the method's\napplicability even in weaker signal-noise situations. The approach also applies\nto the more general class of elliptically symmetric filters, with\nreconstruction performance often better than even that obtained with the true\noptimal radially symmetric filter.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 17:26:41 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 11:13:50 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2018 16:20:11 GMT"}, {"version": "v4", "created": "Sat, 9 Feb 2019 18:32:06 GMT"}, {"version": "v5", "created": "Sun, 21 Apr 2019 15:35:53 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Maitra", "Ranjan", ""]]}, {"id": "1803.08424", "submitter": "Yuan Tang", "authors": "Yuan Tang", "title": "Autoplotly - Automatic Generation of Interactive Visualizations for\n  Popular Statistical Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoplotly package provides functionalities to automatically generate\ninteractive visualizations for many popular statistical results supported by\nggfortify package with plotly and ggplot2 style. The generated visualizations\ncan also be easily extended using ggplot2 and plotly syntax while staying\ninteractive.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 21:36:16 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Tang", "Yuan", ""]]}, {"id": "1803.08522", "submitter": "Alessandro Zocca Dr", "authors": "John Moriarty, Jure Vogrinc, Alessandro Zocca", "title": "Frequency violations from random disturbances: an MCMC approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequency stability of power systems is increasingly challenged by\nvarious types of disturbances. In particular, the increasing penetration of\nrenewable energy sources is increasing the variability of power generation and\nat the same time reducing system inertia against disturbances. In this paper we\nare particularly interested in understanding how rate of change of frequency\n(RoCoF) violations could arise from unusually large power disturbances. We\ndevise a novel specialization, named ghost sampling, of the Metropolis-Hastings\nMarkov Chain Monte Carlo method that is tailored to efficiently sample rare\npower disturbances leading to nodal frequency violations. Generating a\nrepresentative random sample addresses important statistical questions such as\n\"which generator is most likely to be disconnected due to a RoCoF violation?\"\nor \"what is the probability of having simultaneous RoCoF violations, given that\na violation occurs?\" Our method can perform conditional sampling from any joint\ndistribution of power disturbances including, for instance, correlated and\nnon-Gaussian disturbances, features which have both been recently shown to be\nsignificant in security analyses.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 18:02:27 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Moriarty", "John", ""], ["Vogrinc", "Jure", ""], ["Zocca", "Alessandro", ""]]}, {"id": "1803.09121", "submitter": "Jiaxin Zhang", "authors": "Jiaxin Zhang, Michael D. Shields", "title": "Probability measure changes in Monte Carlo simulation", "comments": "30 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of Bayesian inference is often to infer, from data, a\nprobability measure for a random variable that can be used as input for Monte\nCarlo simulation. When datasets for Bayesian inference are small, a principle\nchallenge is that, as additional data are collected, the probability measure\ninferred from Bayesian inference may change significantly. That is, the\noriginal probability density inferred from Bayesian inference may differ\nconsiderably from the updated probability density both in its model form and\nparameters. In such cases, expensive Monte Carlo simulations may have already\nbeen performed using the original distribution and it is infeasible to start\nagain and perform a new Monte Carlo analysis using the updated density due to\nthe large added computational cost. In this work, we discuss four strategies\nfor updating Mote Carlo simulations for such a change in probability measure:\n1. Importance sampling reweighting; 2. A sample augmenting strategy; 3. A\nsample filtering strategy; and 4. A mixed augmenting-filtering strategy. The\nefficiency of each strategy is compared and the ultimate aim is to achieve the\nchange in distribution with a minimal number of added computational\nsimulations. The comparison results show that when the change in measure is\nsmall importance sampling reweighting can be very effective. Otherwise, a\nproposed novel mixed augmenting-filtering algorithm can robustly and\nefficiently accommodate a measure change in Monte Carlo simulation that\nminimizes the impact on the sample set and saves a large amount of additional\ncomputational cost. The strategy is then applied for uncertainty quantification\nin the buckling strength of a simple plate given ongoing data collection to\nestimate uncertainty in the yield stress.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 15:02:20 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 00:50:49 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Zhang", "Jiaxin", ""], ["Shields", "Michael D.", ""]]}, {"id": "1803.09365", "submitter": "Joseph Marion", "authors": "Joseph Marion and Scott C. Schmidler", "title": "Finite Sample Complexity of Sequential Monte Carlo Estimators", "comments": "Correcting an error in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present bounds for the finite sample error of sequential Monte Carlo\nsamplers on static spaces. Our approach explicitly relates the performance of\nthe algorithm to properties of the chosen sequence of distributions and mixing\nproperties of the associated Markov kernels. This allows us to give the first\nfinite sample comparison to other Monte Carlo schemes. We obtain bounds for the\ncomplexity of sequential Monte Carlo approximations for a variety of target\ndistributions including finite spaces, product measures, and log-concave\ndistributions including Bayesian logistic regression. The bounds obtained are\nwithin a logarithmic factor of similar bounds obtainable for Markov chain Monte\nCarlo.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 23:15:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 12:35:50 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Marion", "Joseph", ""], ["Schmidler", "Scott C.", ""]]}, {"id": "1803.09460", "submitter": "Giacomo Zanella", "authors": "Omiros Papaspiliopoulos, Gareth O. Roberts and Giacomo Zanella", "title": "Scalable inference for crossed random effects models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the complexity of Gibbs samplers for inference in crossed random\neffect models used in modern analysis of variance. We demonstrate that for\ncertain designs the plain vanilla Gibbs sampler is not scalable, in the sense\nthat its complexity is worse than proportional to the number of parameters and\ndata. We thus propose a simple modification leading to a collapsed Gibbs\nsampler that is provably scalable. Although our theory requires some\nbalancedness assumptions on the data designs, we demonstrate in simulated and\nreal datasets that the rates it predicts match remarkably the correct rates in\ncases where the assumptions are violated. We also show that the collapsed Gibbs\nsampler, extended to sample further unknown hyperparameters, outperforms\nsignificantly alternative state of the art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:15:54 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Roberts", "Gareth O.", ""], ["Zanella", "Giacomo", ""]]}, {"id": "1803.09514", "submitter": "Charu Sharma", "authors": "Charu Sharma (Shiv Nadar University, UP), Amber Habib (Shiv Nadar\n  University, UP), Sunil Bowry (Shiv Nadar University, UP)", "title": "Cluster analysis of stocks using price movements of high frequency data\n  from National Stock Exchange", "comments": "presented in conference IPECS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to develop new techniques to describe joint behavior of\nstocks, beyond regression and correlation. For example, we want to identify the\nclusters of the stocks that move together. Our work is based on applying Kernel\nPrincipal Component Analysis(KPCA) and Functional Principal Component\nAnalysis(FPCA) to high frequency data from NSE. Since we dealt with high\nfrequency data with a tick size of 30 seconds, FPCA seems to be an ideal\nchoice. FPCA is a functional variant of PCA where each sample point is\nconsidered to be a function in Hilbert space L^2. On the other hand, KPCA is an\nextension of PCA using kernel methods. Results obtained from FPCA and Gaussian\nKernel PCA seems to be in synergy but with a lag. There were two prominent\nclusters that showed up in our analysis, one corresponding to the banking\nsector and another corresponding to the IT sector. The other smaller clusters\nwere seen from the automobile industry and the energy sector. IT sector was\nseen interacting with these small clusters. The learning gained from these\ninteractions is substantial as one can use it significantly to develop trading\nstrategies for intraday traders.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 11:22:56 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sharma", "Charu", "", "Shiv Nadar University, UP"], ["Habib", "Amber", "", "Shiv Nadar\n  University, UP"], ["Bowry", "Sunil", "", "Shiv Nadar University, UP"]]}, {"id": "1803.09527", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu, Arnaud Doucet, Sinan Y{\\i}ld{\\i}r{\\i}m and Nicolas\n  Chopin", "title": "On the utility of Metropolis-Hastings with asymmetric acceptance ratio", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-Hastings algorithm allows one to sample asymptotically from\nany probability distribution $\\pi$. There has been recently much work devoted\nto the development of variants of the MH update which can handle scenarios\nwhere such an evaluation is impossible, and yet are guaranteed to sample from\n$\\pi$ asymptotically. The most popular approach to have emerged is arguably the\npseudo-marginal MH algorithm which substitutes an unbiased estimate of an\nunnormalised version of $\\pi$ for $\\pi$. Alternative pseudo-marginal algorithms\nrelying instead on unbiased estimates of the MH acceptance ratio have also been\nproposed. These algorithms can have better properties than standard PM\nalgorithms. Convergence properties of both classes of algorithms are known to\ndepend on the variability of the estimators involved and reduced variability is\nguaranteed to decrease the asymptotic variance of ergodic averages and will\nshorten the burn-in period, or convergence to equilibrium, in most scenarios of\ninterest. A simple approach to reduce variability, amenable to parallel\ncomputations, consists of averaging independent estimators. However, while\naveraging estimators of $\\pi$ in a pseudo-marginal algorithm retains the\nguarantee of sampling from $\\pi$ asymptotically, naive averaging of acceptance\nratio estimates breaks detailed balance, leading to incorrect results. We\npropose an original methodology which allows for a correct implementation of\nthis idea. We establish theoretical properties which parallel those available\nfor standard PM algorithms and discussed above. We demonstrate the interest of\nthe approach on various inference problems. In particular we show that\nconvergence to equilibrium can be significantly shortened, therefore offering\nthe possibility to reduce a user's waiting time in a generic fashion when a\nparallel computing architecture is available.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 11:54:29 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Andrieu", "Christophe", ""], ["Doucet", "Arnaud", ""], ["Y\u0131ld\u0131r\u0131m", "Sinan", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1803.10161", "submitter": "Wilson Ye Chen", "authors": "Wilson Ye Chen, Lester Mackey, Jackson Gorham, Fran\\c{c}ois-Xavier\n  Briol, Chris J. Oates", "title": "Stein Points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in computational statistics and machine learning is to\napproximate a posterior distribution $p(x)$ with an empirical measure supported\non a set of representative points $\\{x_i\\}_{i=1}^n$. This paper focuses on\nmethods where the selection of points is essentially deterministic, with an\nemphasis on achieving accurate approximation when $n$ is small. To this end, we\npresent `Stein Points'. The idea is to exploit either a greedy or a conditional\ngradient method to iteratively minimise a kernel Stein discrepancy between the\nempirical measure and $p(x)$. Our empirical results demonstrate that Stein\nPoints enable accurate approximation of the posterior at modest computational\ncost. In addition, theoretical results are provided to establish convergence of\nthe method.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 16:12:33 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 13:14:22 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 18:34:04 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 16:30:55 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Mackey", "Lester", ""], ["Gorham", "Jackson", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris J.", ""]]}, {"id": "1803.10290", "submitter": "Stefan Van Aelst", "authors": "Holger Cevallos-Valdiviezo and Stefan Van Aelst", "title": "Fast Computation of Robust Subspace Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is often an important step in the analysis of\nhigh-dimensional data. PCA is a popular technique to find the best\nlow-dimensional approximation of high-dimensional data. However, classical PCA\nis very sensitive to atypical data. Robust methods to estimate the\nlow-dimensional subspace that best approximates the regular data have been\nproposed. However, for high-dimensional data his algorithms become\ncomputationally expensive. Alternative algorithms for the robust subspace\nestimators are proposed that are better suited to compute the solution for\nhigh-dimensional problems. The main ingredients of the new algorithms are\ntwofold. First, the principal directions of the subspace are estimated directly\nby iterating the estimating equations corresponding to the estimators. Second,\nto reduce computation time even further five robust deterministic values are\nproposed to initialize the algorithms instead of using random starting values.\nIt is shown that the new algorithms yield robust solutions and the computation\ntime is largely reduced, especially for high-dimensional data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 19:49:56 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 11:39:16 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Cevallos-Valdiviezo", "Holger", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1803.10656", "submitter": "Jean-Baptiste Blanchard", "authors": "J-B. Blanchard and G. Damblin and J-M. Martinez and G. Arnaud and F.\n  Gaudier", "title": "The Uranie platform: an Open-source software for optimisation,\n  meta-modelling and uncertainty analysis", "comments": "35 pages, submitted to CPC (elsevier)", "journal-ref": "EPJN 2019", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The high-performance computing resources and the constant improvement of both\nnumerical simulation accuracy and the experimental measurements with which they\nare confronted, bring a new compulsory step to strengthen the credence given to\nthe simulation results: uncertainty quantification. This can have different\nmeanings, according to the requested goals (rank uncertainty sources, reduce\nthem, estimate precisely a critical threshold or an optimal working point) and\nit could request mathematical methods with greater or lesser complexity. This\npaper introduces the Uranie platform, an Open-source framework which is\ncurrently developed at the Alternative Energies and Atomic Energy Commission\n(CEA), in the nuclear energy division, in order to deal with uncertainty\npropagation, surrogate models, optimisation issues, code calibration... This\nplatform benefits from both its dependencies, but also from personal\ndevelopments, to offer an efficient data handling model, a C++ and Python\ninterpreter, advanced graphical tools, several parallelisation solutions...\nThese methods are very generic and can then be applied to many kinds of code\n(as Uranie considers them as black boxes) so to many fields of physics as well.\nIn this paper, the example of thermal exchange between a plate-sheet and a\nfluid is introduced to show how Uranie can be used to perform a large range of\nanalysis. The code used to produce the figures of this paper can be found in\nhttps://sourceforge.net/projects/uranie/ along with the sources of the\nplatform.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 14:41:45 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Blanchard", "J-B.", ""], ["Damblin", "G.", ""], ["Martinez", "J-M.", ""], ["Arnaud", "G.", ""], ["Gaudier", "F.", ""]]}, {"id": "1803.10749", "submitter": "Erlis Ruli", "authors": "Erlis Ruli", "title": "On Model Selection with Summary Statistics", "comments": "6 pages, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many authors have cast doubts on the validity of ABC model choice.\nIt has been shown that the use of sufficient statistic in ABC model selection\nleads, apart from few exceptional cases in which the sufficient statistic is\nalso cross-model sufficient, to unreliable results. In a single model context\nand given a sufficient summary statistic, we show that it is possible to fully\nrecover the posterior normalising constant, without using the likelihood\nfunction. The idea can be applied, in an approximate way, to more realistic\nscenarios in which the sufficient statistic is not unavailable but a \"good\"\nsummary statistic for estimation is available.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 07:06:31 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ruli", "Erlis", ""]]}, {"id": "1803.10884", "submitter": "Adam Gustafson", "authors": "Adam Gustafson, Matthew Hirn, Kitty Mohammed, Hariharan Narayanan, and\n  Jason Xu", "title": "Structural Risk Minimization for $C^{1,1}(\\mathbb{R}^d)$ Regression", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One means of fitting functions to high-dimensional data is by providing\nsmoothness constraints. Recently, the following smooth function approximation\nproblem was proposed: given a finite set $E \\subset \\mathbb{R}^d$ and a\nfunction $f: E \\rightarrow \\mathbb{R}$, interpolate the given information with\na function $\\widehat{f} \\in \\dot{C}^{1, 1}(\\mathbb{R}^d)$ (the class of\nfirst-order differentiable functions with Lipschitz gradients) such that\n$\\widehat{f}(a) = f(a)$ for all $a \\in E$, and the value of\n$\\mathrm{Lip}(\\nabla \\widehat{f})$ is minimal. An algorithm is provided that\nconstructs such an approximating function $\\widehat{f}$ and estimates the\noptimal Lipschitz constant $\\mathrm{Lip}(\\nabla \\widehat{f})$ in the noiseless\nsetting.\n  We address statistical aspects of reconstructing the approximating function\n$\\widehat{f}$ from a closely-related class $C^{1, 1}(\\mathbb{R}^d)$ given\nsamples from noisy data. We observe independent and identically distributed\nsamples $y(a) = f(a) + \\xi(a)$ for $a \\in E$, where $\\xi(a)$ is a noise term\nand the set $E \\subset \\mathbb{R}^d$ is fixed and known. We obtain uniform\nbounds relating the empirical risk and true risk over the class\n$\\mathcal{F}_{\\widetilde{M}} = \\{f \\in C^{1, 1}(\\mathbb{R}^d) \\mid\n\\mathrm{Lip}(\\nabla f) \\leq \\widetilde{M}\\}$, where the quantity\n$\\widetilde{M}$ grows with the number of samples at a rate governed by the\nmetric entropy of the class $C^{1, 1}(\\mathbb{R}^d)$. Finally, we provide an\nimplementation using Vaidya's algorithm, supporting our results via numerical\nexperiments on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 00:19:45 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:49:35 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Gustafson", "Adam", ""], ["Hirn", "Matthew", ""], ["Mohammed", "Kitty", ""], ["Narayanan", "Hariharan", ""], ["Xu", "Jason", ""]]}, {"id": "1803.11033", "submitter": "Chang-Yun Lin Dr.", "authors": "Chang-Yun Lin", "title": "Generalized Bayesian D criterion for single-stratum and multistratum\n  designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DuMouchel and Jones (1994) proposed the Bayesian D criterion by modifying the\nD-optimality approach to reduce dependence of the selected design on an assumed\nmodel. This criterion has been applied to select various single-stratum designs\nfor completely randomized experiments when the number of effects is greater\nthan the sample size. In many industrial experiments, complete randomization is\nsometimes expensive or infeasible and, hence, designs used for the experiments\noften have multistratum structures. However, the original Bayesian D criterion\nwas developed under the framework of single-stratum structures and cannot be\napplied to select multistratum designs. In this paper, we study how to extend\nthe Bayesian approach for more complicated experiments and develop the\ngeneralized Bayesian D criterion, which generalizes the original Bayesian D\ncriterion and can be applied to select single-stratum and multistratum designs\nfor various experiments when the number of effects is greater than the rank of\nthe model matrix.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 12:44:41 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Lin", "Chang-Yun", ""]]}, {"id": "1803.11354", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis and R. M. Huggins", "title": "Two-stage approaches to the analysis of occupancy data II. The\n  heterogeneous model and conditional likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models involve both the probability a site is occupied and the\nprobability occupancy is detected. The homogeneous occupancy model, where the\noccupancy and detection probabilities are the same at each site, admits an\northogonal parameter transformation that yields a two-stage process to\ncalculate the maximum likelihood estimates so that it is not necessary to\nsimultaneously estimate the occupancy and detection probabilities. The\ntwo-stage approach is examined here for the heterogeneous occupancy model where\nthe occupancy and detection probabilities now depend on covariates that may\nvary between sites and over time. There is no longer an orthogonal\ntransformation but this approach effectively reduces the parameter space and\nallows fuller use of the R functionality. This permits use of existing vector\ngeneralised linear models methods to fit models for detection and allows the\ndevelopment of an iterative weighted least squares approach to fit models for\noccupancy. Efficiency is examined in a simulation study and the full maximum\nlikelihood and two-stage approaches are compared on several data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:22:54 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 05:49:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Huggins", "R. M.", ""]]}]