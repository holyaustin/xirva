[{"id": "1201.0306", "submitter": "Xiaodong Lin", "authors": "Xiaodong Lin, Minh Pham, Andrzej Ruszczynski", "title": "Alternating Linearization for Structured Regularization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the alternating linearization method for proximal decomposition to\nstructured regularization problems, in particular, to the generalized lasso\nproblems. The method is related to two well-known operator splitting methods,\nthe Douglas--Rachford and the Peaceman--Rachford method, but it has descent\nproperties with respect to the objective function. This is achieved by\nemploying a special update test, which decides whether it is beneficial to make\na Peaceman--Rachford step, any of the two possible Douglas--Rachford steps, or\nnone. The convergence mechanism of the method is related to that of bundle\nmethods of nonsmooth optimization. We also discuss implementation for very\nlarge problems, with the use of specialized algorithms and sparse data\nstructures. Finally, we present numerical results for several synthetic and\nreal-world examples, including a three-dimensional fused lasso problem, which\nillustrate the scalability, efficacy, and accuracy of the method.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 19:30:44 GMT"}, {"version": "v2", "created": "Wed, 19 Mar 2014 05:40:02 GMT"}, {"version": "v3", "created": "Mon, 24 Mar 2014 14:31:59 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Lin", "Xiaodong", ""], ["Pham", "Minh", ""], ["Ruszczynski", "Andrzej", ""]]}, {"id": "1201.0480", "submitter": "Arnaud  Doucet", "authors": "Bernard Bercu, Pierre Del Moral and Arnaud Doucet", "title": "Fluctuations of Interacting Markov Chain Monte Carlo Methods", "comments": "Second revision of the INRIA-RR-6438 technical report (available\n  February 2008) at http://hal.inria.fr/docs/00/23/92/48/PDF/RR-6438.pdf To\n  appear in Stochastic Processes and Their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multivariate central limit theorem for a general class of\ninteracting Markov chain Monte Carlo algorithms used to solve nonlinear\nmeasure-valued equations. These algorithms generate stochastic processes which\nbelong to the class of nonlinear Markov chains interacting with their empirical\noccupation measures. We develop an original theoretical analysis based on\nresolvent operators and semigroup techniques to analyze the fluctuations of\ntheir occupation measures around their limiting values.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2012 15:15:58 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Bercu", "Bernard", ""], ["Del Moral", "Pierre", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1201.0646", "submitter": "Luca Martino", "authors": "Luca Martino and Jesse Read", "title": "On the flexibility of the design of Multiple Try Metropolis schemes", "comments": null, "journal-ref": "Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823,\n  2013", "doi": "10.1007/s00180-013-0429-2", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiple Try Metropolis (MTM) method is a generalization of the classical\nMetropolis-Hastings algorithm in which the next state of the chain is chosen\namong a set of samples, according to normalized weights. In the literature,\nseveral extensions have been proposed. In this work, we show and remark upon\nthe flexibility of the design of MTM-type methods, fulfilling the detailed\nbalance condition. We discuss several possibilities and show different\nnumerical results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 14:38:14 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2012 10:57:38 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2013 14:28:35 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""]]}, {"id": "1201.1314", "submitter": "Christian P. Robert", "authors": "Christophe Andrieu, Simon Barthelme, Nicolas Chopin, Julien Cornebise,\n  Arnaud Doucet, Mark Girolami, Ioannis Kosmidis, Ajay Jasra, Anthony Lee,\n  Jean-Michel Marin, Pierre Pudlo, Christian P. Robert, Mohammed Sedki. and\n  Sumeetpal S. Singh", "title": "Some discussions of D. Fearnhead and D. Prangle's Read Paper\n  \"Constructing summary statistics for approximate Bayesian computation:\n  semi-automatic approximate Bayesian computation\"", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is a collection of comments on the Read Paper of Fearnhead and\nPrangle (2011), to appear in the Journal of the Royal Statistical Society\nSeries B, along with a reply from the authors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 22:02:07 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Andrieu", "Christophe", ""], ["Barthelme", "Simon", ""], ["Chopin", "Nicolas", ""], ["Cornebise", "Julien", ""], ["Doucet", "Arnaud", ""], ["Girolami", "Mark", ""], ["Kosmidis", "Ioannis", ""], ["Jasra", "Ajay", ""], ["Lee", "Anthony", ""], ["Marin", "Jean-Michel", ""], ["Pudlo", "Pierre", ""], ["Robert", "Christian P.", ""], ["Sedki.", "Mohammed", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1201.1320", "submitter": "Alessandro Soranzo", "authors": "A. Soranzo, E. Epure", "title": "Simply Explicitly Invertible Approximations to 4 Decimals of Error\n  Function and Normal Cumulative Distribution Function", "comments": "3 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We improve the Modified Winitzki's Approximation of the error function\n$erf(x)\\cong \\sqrt{1-e^{-x^2\\frac{\\frac{4}{\\pi}+0.147x^2}{1+0.147x^2}}}$ which\nhas error $|\\varepsilon (x)| < 1.25 \\cdot 10^{-4}$ $\\forall x \\ge 0$ till\nreaching 4 decimals of precision with $|\\varepsilon (x)| < 2.27 \\cdot 10^{-5}$;\nalso reducing slightly the relative error. Old formula and ours are both\nexplicitly invertible, essentially solving a biquadratic equation, after\nobvious substitutions. Then we derive approximations to 4 decimals of normal\ncumulative distribution function $\\Phi (x)$, of erfc$(x)$ and of the $Q$\nfunction (or cPhi).\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 22:43:16 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Soranzo", "A.", ""], ["Epure", "E.", ""]]}, {"id": "1201.1421", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Testing the significance of assuming homogeneity in\n  contingency-tables/cross-tabulations", "comments": "14 pages, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model for homogeneity of proportions in a two-way\ncontingency-table/cross-tabulation is the same as the model of independence,\nexcept that the probabilistic process generating the data is viewed as fixing\nthe column totals (but not the row totals). When gauging the consistency of\nobserved data with the assumption of independence, recent work has illustrated\nthat the Euclidean/Frobenius/Hilbert-Schmidt distance is often far more\nstatistically powerful than the classical statistics such as chi-square, the\nlog-likelihood-ratio (G), the Freeman-Tukey/Hellinger distance, and other\nmembers of the Cressie-Read power-divergence family. The present paper\nindicates that the Euclidean/Frobenius/Hilbert-Schmidt distance can be more\npowerful for gauging the consistency of observed data with the assumption of\nhomogeneity, too.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 14:06:43 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "1201.1431", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "An introduction to how chi-square and classical exact tests often wildly\n  misreport significance and how the remedy lies in computers", "comments": "41 pages, 25 figures, 7 tables. arXiv admin note: near complete text\n  overlap with arXiv:1108.4126", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goodness-of-fit tests based on the Euclidean distance often outperform\nchi-square and other classical tests (including the standard exact tests) by at\nleast an order of magnitude when the model being tested for goodness-of-fit is\na discrete probability distribution that is not close to uniform. The present\narticle discusses numerous examples of this. Goodness-of-fit tests based on the\nEuclidean metric are now practical and convenient: although the actual values\ntaken by the Euclidean distance and similar goodness-of-fit statistics are\nseldom humanly interpretable, black-box computer programs can rapidly calculate\ntheir precise significance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 15:20:40 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2012 21:20:35 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1201.1433", "submitter": "Gopal Basak", "authors": "Gopal K. Basak and Arunangshu Biswas", "title": "Langevin type limiting processes for Adaptive MCMC", "comments": "It has 22 pages including 3 new figures comparing SMCMC and AMCMC.\n  Also, includes diffusion approximation for multivariate target density", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Markov Chain Monte Carlo (AMCMC) is a class of MCMC algorithms where\nthe proposal distribution changes at every iteration of the chain. In this case\nit is important to verify that such a Markov Chain indeed has a stationary\ndistribution. In this paper we discuss a diffusion approximation to a discrete\ntime AMCMC. This diffusion approximation is different when compared to the\ndiffusion approximation as in Gelman, Gilks and Roberts (1997) where the state\nspace increases in dimension to infinity. In our approach the time parameter is\nsped up in such a way that the limiting distribution (as the mesh size goes to\n0) is to a non-trivial continuous time diffusion process.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 15:35:22 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2012 15:12:30 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 13:38:08 GMT"}, {"version": "v4", "created": "Fri, 4 Sep 2015 15:10:40 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Basak", "Gopal K.", ""], ["Biswas", "Arunangshu", ""]]}, {"id": "1201.1578", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi, Djamel Meraghni, Abdelhakim Necir, Djabrane Yahia", "title": "A Bias-reduced Estimator for the Mean of a Heavy-tailed Distribution\n  with an Infinite Second Moment", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use bias-reduced estimators of high quantiles, of heavy-tailed\ndistributions, to introduce a new estimator of the mean in the case of infinite\nsecond moment. The asymptotic normality of the proposed estimator is\nestablished and checked, in a simulation study, by four of the most popular\ngoodness-of-fit tests for different sample sizes. Moreover, we compare, in\nterms of bias and mean squared error, our estimator with Peng's estimator\n(Peng, 2001) and we evaluate the accuracy of some resulting confidence\nintervals.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2012 17:40:49 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2012 19:10:14 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Brahimi", "Brahim", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""], ["Yahia", "Djabrane", ""]]}, {"id": "1201.1623", "submitter": "Sergio G\\'omez", "authors": "Sergio Gomez, Justo Montiel, David Torres, Alberto Fernandez", "title": "MultiDendrograms: Variable-Group Agglomerative Hierarchical Clusterings", "comments": "Article upgraded to MultiDendrograms 3.0. Software available at\n  http://deim.urv.cat/~sgomez/multidendrograms.php", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR math.ST physics.comp-ph physics.data-an q-fin.CP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MultiDendrograms is a Java-written application that computes agglomerative\nhierarchical clusterings of data. Starting from a distances (or weights)\nmatrix, MultiDendrograms is able to calculate its dendrograms using the most\ncommon agglomerative hierarchical clustering methods. The application\nimplements a variable-group algorithm that solves the non-uniqueness problem\nfound in the standard pair-group algorithm. This problem arises when two or\nmore minimum distances between different clusters are equal during the\nagglomerative process, because then different output clusterings are possible\ndepending on the criterion used to break ties between distances.\nMultiDendrograms solves this problem implementing a variable-group algorithm\nthat groups more than two clusters at the same time when ties occur.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2012 11:03:42 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 15:44:04 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Gomez", "Sergio", ""], ["Montiel", "Justo", ""], ["Torres", "David", ""], ["Fernandez", "Alberto", ""]]}, {"id": "1201.1893", "submitter": "Yanan Fan Dr", "authors": "D. J. Nott, Y. Fan and S. A. Sisson", "title": "Discussions on Fernhead and Prangle (2012)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two contributions to the discussion of Fearnhead P. and D. Prangle (2012).\nConstructing summary statistics for approximate Bayesian computation:\nSemi-automatic approx- imate Bayesian computation, J. Roy. Statist. Soc. B, 74\n(3).\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2012 20:24:52 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Nott", "D. J.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1201.2337", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Nial Friel", "title": "Bayesian model selection for exponential random graph models", "comments": "30 pages; Accepted to appear in Social Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models are a class of widely used exponential family\nmodels for social networks. The topological structure of an observed network is\nmodelled by the relative prevalence of a set of local sub-graph configurations\ntermed network statistics. One of the key tasks in the application of these\nmodels is which network statistics to include in the model. This can be thought\nof as statistical model selection problem. This is a very challenging\nproblem---the posterior distribution for each model is often termed \"doubly\nintractable\" since computation of the likelihood is rarely available, but also,\nthe evidence of the posterior is, as usual, intractable. The contribution of\nthis paper is the development of a fully Bayesian model selection method based\non a reversible jump Markov chain Monte Carlo algorithm extension of Caimo and\nFriel (2011) which estimates the posterior probability for each competing\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 15:56:03 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2012 11:02:26 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 09:39:06 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Caimo", "Alberto", ""], ["Friel", "Nial", ""]]}, {"id": "1201.2375", "submitter": "Jorge Figueroa jifiguer", "authors": "Jorge I. Figueroa-Zu\\~niga, Reinaldo B. Arellano-Valle and Silvia L.\n  P. Ferrari", "title": "Mixed Beta Regression: A Bayesian Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds on recent research that focuses on regression modeling of\ncontinuous bounded data, such as proportions measured on a continuous scale.\nSpecifically, it deals with beta regression models with mixed effects from a\nBayesian approach. We use a suitable parameterization of the beta law in terms\nof its mean and a precision parameter, and allow both parameters to be modeled\nthrough regression structures that may involve fixed and random effects.\nSpecification of prior distributions is discussed, computational implementation\nvia Gibbs sampling is provided, and illustrative examples are presented.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 18:53:45 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2012 15:53:08 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2012 22:36:48 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2012 12:05:34 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Figueroa-Zu\u00f1iga", "Jorge I.", ""], ["Arellano-Valle", "Reinaldo B.", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1201.2770", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Nial Friel", "title": "Bergm: Bayesian Exponential Random Graphs in R", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the main featuress of the Bergm package for the\nopen-source R software which provides a comprehensive framework for Bayesian\nanalysis for exponential random graph models: tools for parameter estimation,\nmodel selection and goodness-of-fit diagnostics. We illustrate the capabilities\nof this package describing the algorithms through a tutorial analysis of two\nwell-known network datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 09:04:47 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 15:58:44 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 16:45:25 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Caimo", "Alberto", ""], ["Friel", "Nial", ""]]}, {"id": "1201.3528", "submitter": "Hua Zhou", "authors": "Hua Zhou and Artin Armagan and David B. Dunson", "title": "Path Following and Empirical Bayes Model Selection for Sparse Regression", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years, a rich variety of regularization procedures have been\nproposed for high dimensional regression problems. However, tuning parameter\nchoice and computational efficiency in ultra-high dimensional problems remain\nvexing issues. The routine use of $\\ell_1$ regularization is largely\nattributable to the computational efficiency of the LARS algorithm, but similar\nefficiency for better behaved penalties has remained elusive. In this article,\nwe propose a highly efficient path following procedure for combination of any\nconvex loss function and a broad class of penalties. From a Bayesian\nperspective, this algorithm rapidly yields maximum a posteriori estimates at\ndifferent hyper-parameter values. To bypass the inefficiency and potential\ninstability of cross validation, we propose an empirical Bayes procedure for\nrapidly choosing the optimal model and corresponding hyper-parameter value.\nThis approach applies to any penalty that corresponds to a proper prior\ndistribution on the regression coefficients. While we mainly focus on sparse\nestimation of generalized linear models, the method extends to more general\nregularizations such as polynomial trend filtering after reparameterization.\nThe proposed algorithm scales efficiently to large $p$ and/or $n$. Solution\npaths of 10,000 dimensional examples are computed within one minute on a laptop\nfor various generalized linear models (GLM). Operating characteristics are\nassessed through simulation studies and the methods are applied to several real\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 15:18:25 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Zhou", "Hua", ""], ["Armagan", "Artin", ""], ["Dunson", "David B.", ""]]}, {"id": "1201.3571", "submitter": "Hua Zhou", "authors": "Hua Zhou and Yichao Wu", "title": "A Generic Path Algorithm for Regularized Statistical Estimation", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Regularization is widely used in statistics and machine learning to prevent\noverfitting and gear solution towards prior information. In general, a\nregularized estimation problem minimizes the sum of a loss function and a\npenalty term. The penalty term is usually weighted by a tuning parameter and\nencourages certain constraints on the parameters to be estimated. Particular\nchoices of constraints lead to the popular lasso, fused-lasso, and other\ngeneralized $l_1$ penalized regression methods. Although there has been a lot\nof research in this area, developing efficient optimization methods for many\nnonseparable penalties remains a challenge. In this article we propose an exact\npath solver based on ordinary differential equations (EPSODE) that works for\nany convex loss function and can deal with generalized $l_1$ penalties as well\nas more complicated regularization such as inequality constraints encountered\nin shape-restricted regressions and nonparametric density estimation. In the\npath following process, the solution path hits, exits, and slides along the\nvarious constraints and vividly illustrates the tradeoffs between goodness of\nfit and model parsimony. In practice, the EPSODE can be coupled with AIC, BIC,\n$C_p$ or cross-validation to select an optimal tuning parameter. Our\napplications to generalized $l_1$ regularized generalized linear models,\nshape-restricted regressions, Gaussian graphical models, and nonparametric\ndensity estimation showcase the potential of the EPSODE algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 17:42:46 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Zhou", "Hua", ""], ["Wu", "Yichao", ""]]}, {"id": "1201.3593", "submitter": "Hua Zhou", "authors": "Hua Zhou and Kenneth Lange", "title": "Path Following in the Exact Penalty Method of Convex Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Classical penalty methods solve a sequence of unconstrained problems that put\ngreater and greater stress on meeting the constraints. In the limit as the\npenalty constant tends to $\\infty$, one recovers the constrained solution. In\nthe exact penalty method, squared penalties are replaced by absolute value\npenalties, and the solution is recovered for a finite value of the penalty\nconstant. In practice, the kinks in the penalty and the unknown magnitude of\nthe penalty constant prevent wide application of the exact penalty method in\nnonlinear programming. In this article, we examine a strategy of path following\nconsistent with the exact penalty method. Instead of performing optimization at\na single penalty constant, we trace the solution as a continuous function of\nthe penalty constant. Thus, path following starts at the unconstrained solution\nand follows the solution path as the penalty constant increases. In the\nprocess, the solution path hits, slides along, and exits from the various\nconstraints. For quadratic programming, the solution path is piecewise linear\nand takes large jumps from constraint to constraint. For a general convex\nprogram, the solution path is piecewise smooth, and path following operates by\nnumerically solving an ordinary differential equation segment by segment. Our\ndiverse applications to a) projection onto a convex set, b) nonnegative least\nsquares, c) quadratically constrained quadratic programming, d) geometric\nprogramming, and e) semidefinite programming illustrate the mechanics and\npotential of path following. The final detour to image denoising demonstrates\nthe relevance of path following to regularized estimation in inverse problems.\nIn regularized estimation, one follows the solution path as the penalty\nconstant decreases from a large value.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 18:55:37 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Zhou", "Hua", ""], ["Lange", "Kenneth", ""]]}, {"id": "1201.3767", "submitter": "Nikolas Kantas", "authors": "Ajay Jasra and Nikolas Kantas", "title": "Bayesian Parameter Inference for Partially Observed Stopped Processes", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider Bayesian parameter inference associated to\npartially-observed stochastic processes that start from a set B0 and are\nstopped or killed at the first hitting time of a known set A. Such processes\noccur naturally within the context of a wide variety of applications. The\nassociated posterior distributions are highly complex and posterior parameter\ninference requires the use of advanced Markov chain Monte Carlo (MCMC)\ntechniques. Our approach uses a recently introduced simulation methodology,\nparticle Markov chain Monte Carlo (PMCMC) (Andrieu et. al. 2010 [1]), where\nsequential Monte Carlo (SMC) approximations (see Doucet et. al. 2001 [18] and\nLiu 2001 [27]) are embedded within MCMC. However, when the parameter of\ninterest is fixed, standard SMC algorithms are not always appropriate for many\nstopped processes. In Chen et. al. [11] and Del Moral 2004 [15] the authors\nintroduce SMC approximations of multi-level Feynman-Kac formulae, which can\nlead to more efficient algorithms. This is achieved by devising a sequence of\nnested sets from B0 to A and then perform the resampling step only when the\nsamples of the process reach intermediate level sets in the sequence.\nNaturally, the choice of the intermediate level sets is critical to the\nperformance of such a scheme. In this paper, we demonstrate that multi-level\nSMC algorithms can be used as a proposal in PMCMC. In addition, we propose a\nflexible strategy that adapts the level sets for different parameter proposals.\nOur methodology is illustrated on the coalescent model with migration.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 12:22:52 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""]]}, {"id": "1201.3973", "submitter": "Babak Shahbaba", "authors": "Shiwei Lan and Babak Shahbaba", "title": "Split HMC for Gaussian Process Models", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss an extension of the Split Hamiltonian Monte Carlo\n(Split HMC) method for Gaussian process model (GPM). This method is based on\nsplitting the Hamiltonian in a way that allows much of the movement around the\nstate space to be done at low computational cost. To this end, we approximate\nthe negative log density (i.e., the energy function) of the distribution of\ninterest by a quadratic function U0 for which Hamiltonian dynamics can be\nsolved analytically. The overall energy function U is then written as U0 + U1,\nwhere U1 is the approximation error. The Hamiltonian is then split into two\nparts; one part is based on U0 is handled analytically, the other part is based\non U1 for which we approximate Hamiltonian's equations by discretizing time. We\nuse simulated and real data to compare the performance of our method to the\nstandard HMC. We find that splitting the Hamiltonian for GP models could lead\nto substantial improvement (up to 10 folds) of sampling efficiency, which is\nmeasured in terms of the amount of time required for producing an independent\nsample with high acceptance probability from posterior distributions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 05:08:45 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2012 14:04:55 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Lan", "Shiwei", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1201.4239", "submitter": "Gabriele Martinelli", "authors": "Gabriele Martinelli, Jo Eidsvik and Ragnar Hauge", "title": "Dynamic Decision Making for Graphical Models Applied to Oil Exploration", "comments": "This paper has been withdrawn by the authors. 22 pages, 7 figures,\n  submitted", "journal-ref": null, "doi": "10.1016/j.ejor.2013.04.057", "report-no": "Technical Report in Statistics N. 12/2011, Dept. of Mathematical\n  Sciences, NTNU", "categories": "stat.AP cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the authors. We present a framework for\nsequential decision making in problems described by graphical models. The\nsetting is given by dependent discrete random variables with associated costs\nor revenues. In our examples, the dependent variables are the potential\noutcomes (oil, gas or dry) when drilling a petroleum well. The goal is to\ndevelop an optimal selection strategy that incorporates a chosen utility\nfunction within an approximated dynamic programming scheme. We propose and\ncompare different approximations, from simple heuristics to more complex\niterative schemes, and we discuss their computational properties. We apply our\nstrategies to oil exploration over multiple prospects modeled by a directed\nacyclic graph, and to a reservoir drilling decision problem modeled by a Markov\nrandom field. The results show that the suggested strategies clearly improve\nthe simpler intuitive constructions, and this is useful when selecting\nexploration policies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2012 09:46:59 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 14:31:37 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Martinelli", "Gabriele", ""], ["Eidsvik", "Jo", ""], ["Hauge", "Ragnar", ""]]}, {"id": "1201.4724", "submitter": "Gregory Nuel", "authors": "G. Nuel", "title": "Tutorial on Exact Belief Propagation in Bayesian Networks: from Messages\n  to Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian networks, exact belief propagation is achieved through message\npassing algorithms. These algorithms (ex: inward and outward) provide only a\nrecursive definition of the corresponding messages. In contrast, when working\non hidden Markov models and variants, one classically first defines explicitly\nthese messages (forward and backward quantities), and then derive all results\nand algorithms. In this paper, we generalize the hidden Markov model approach\nby introducing an explicit definition of the messages in Bayesian networks,\nfrom which we derive all the relevant properties and results including the\nrecursive algorithms that allow to compute these messages. Two didactic\nexamples (the precipitation hidden Markov model and the pedigree Bayesian\nnetwork) are considered along the paper to illustrate the new formalism and\nstandalone R source code is provided in the appendix.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 14:16:44 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Nuel", "G.", ""]]}, {"id": "1201.5229", "submitter": "Sean Sedwards", "authors": "Cyrille J\\'egourel, Axel Legay, and Sean Sedwards", "title": "Cross-entropy optimisation of importance sampling parameters for\n  statistical model checking", "comments": "16 pages, 8 figures, LNCS style", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.CE cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical model checking avoids the exponential growth of states associated\nwith probabilistic model checking by estimating properties from multiple\nexecutions of a system and by giving results within confidence bounds. Rare\nproperties are often very important but pose a particular challenge for\nsimulation-based approaches, hence a key objective under these circumstances is\nto reduce the number and length of simulations necessary to produce a given\nlevel of confidence. Importance sampling is a well-established technique that\nachieves this, however to maintain the advantages of statistical model checking\nit is necessary to find good importance sampling distributions without\nconsidering the entire state space.\n  Motivated by the above, we present a simple algorithm that uses the notion of\ncross-entropy to find the optimal parameters for an importance sampling\ndistribution. In contrast to previous work, our algorithm uses a low\ndimensional vector of parameters to define this distribution and thus avoids\nthe often intractable explicit representation of a transition matrix. We show\nthat our parametrisation leads to a unique optimum and can produce many orders\nof magnitude improvement in simulation efficiency. We demonstrate the efficacy\nof our methodology by applying it to models from reliability engineering and\nbiochemistry.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 10:22:58 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["J\u00e9gourel", "Cyrille", ""], ["Legay", "Axel", ""], ["Sedwards", "Sean", ""]]}, {"id": "1201.5907", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Alfred O. Hero", "title": "Kullback Proximal Algorithms for Maximum Likelihood Estimation", "comments": "6 figures", "journal-ref": "IEEE Transactions on Information Theory, (2000) 46 no.5, pp.\n  1800--1810", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated algorithms for maximum likelihood image reconstruction are\nessential for emerging applications such as 3D tomography, dynamic tomographic\nimaging, and other high dimensional inverse problems. In this paper, we\nintroduce and analyze a class of fast and stable sequential optimization\nmethods for computing maximum likelihood estimates and study its convergence\nproperties. These methods are based on a {\\it proximal point algorithm}\nimplemented with the Kullback-Liebler (KL) divergence between posterior\ndensities of the complete data as a proximal penalty function. When the\nproximal relaxation parameter is set to unity one obtains the classical\nexpectation maximization (EM) algorithm. For a decreasing sequence of\nrelaxation parameters, relaxed versions of EM are obtained which can have much\nfaster asymptotic convergence without sacrifice of monotonicity. We present an\nimplementation of the algorithm using Mor\\'{e}'s {\\it Trust Region} update\nstrategy. For illustration the method is applied to a non-quadratic inverse\nproblem with Poisson distributed data.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 22:51:39 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1201.5912", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien and Alfred O. Hero", "title": "On EM algorithms and their proximal generalizations", "comments": null, "journal-ref": "ESAIM: Probability and Statistics (2008) 12 pp. 308--326", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the celebrated EM algorithm from the point of view\nof proximal point algorithms. More precisely, we study a new type of\ngeneralization of the EM procedure introduced in \\cite{Chretien&Hero:98} and\ncalled Kullback-proximal algorithms. The proximal framework allows us to prove\nnew results concerning the cluster points. An essential contribution is a\ndetailed analysis of the case where some cluster points lie on the boundary of\nthe parameter space.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 23:12:51 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1201.5913", "submitter": "Stephane Chretien", "authors": "Gilles Celeux, St\\'ephane Chr\\'etien, Florence Forbes", "title": "A Component-wise EM Algorithm for Mixtures", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics. (2001), 10 no.4\n  pp. 697--712", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some situations, EM algorithm shows slow convergence problems. One\npossible reason is that standard procedures update the parameters\nsimultaneously. In this paper we focus on finite mixture estimation. In this\nframework, we propose a component-wise EM, which updates the parameters\nsequentially. We give an interpretation of this procedure as a proximal point\nalgorithm and use it to prove the convergence. Illustrative numerical\nexperiments show how our algorithm compares to EM and a version of the SAGE\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 23:19:11 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Celeux", "Gilles", ""], ["Chr\u00e9tien", "St\u00e9phane", ""], ["Forbes", "Florence", ""]]}, {"id": "1201.6140", "submitter": "Nicolas Chopin", "authors": "Nicolas Chopin", "title": "Fast simulation of truncated Gaussian distributions", "comments": null, "journal-ref": "Statistics and Computing 2011, Volume 21, Number 2, 275-288", "doi": "10.1007/s11222-009-9168-1", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simulating a Gaussian vector X, conditional on the\nfact that each component of X belongs to a finite interval [a_i,b_i], or a\nsemi-finite interval [a_i,+infty). In the one-dimensional case, we design a\ntable-based algorithm that is computationally faster than alternative\nalgorithms. In the two-dimensional case, we design an accept-reject algorithm.\nAccording to our calculations and our numerical studies, the acceptance rate of\nthis algorithm is bounded from below by 0.5 for semi-finite truncation\nintervals, and by 0.47 for finite intervals. Extension to 3 or more dimensions\nis discussed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2012 09:39:35 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Chopin", "Nicolas", ""]]}]