[{"id": "1311.0317", "submitter": "Paul McNicholas", "authors": "Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne and Paula M.\n  Murray", "title": "Parsimonious Shifted Asymmetric Laplace Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 22:14:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Franczak", "Brian C.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""], ["Murray", "Paula M.", ""]]}, {"id": "1311.0656", "submitter": "Ioannis Ntzoufras", "authors": "Silia Vitoratou, Ioannis Ntzoufras and Irini Moustaki", "title": "Explaining the behavior of joint and marginal Monte Carlo estimators in\n  latent variable models with independence assumptions", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-014-9495-8", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In latent variable models the parameter estimation can be implemented by\nusing the joint or the marginal likelihood, based on independence or\nconditional independence assumptions. The same dilemma occurs within the\nBayesian framework with respect to the estimation of the Bayesian marginal (or\nintegrated) likelihood, which is the main tool for model comparison and\naveraging. In most cases, the Bayesian marginal likelihood is a high\ndimensional integral that cannot be computed analytically and a plethora of\nmethods based on Monte Carlo integration (MCI) are used for its estimation. In\nthis work, it is shown that the joint MCI approach makes subtle use of the\nproperties of the adopted model, leading to increased error and bias in finite\nsettings. The sources and the components of the error associated with\nestimators under the two approaches are identified here and provided in exact\nforms. Additionally, the effect of the sample covariation on the Monte Carlo\nestimators is examined. In particular, even under independence assumptions the\nsample covariance will be close to (but not exactly) zero which surprisingly\nhas a severe effect on the estimated values and their variability. To address\nthis problem, an index of the sample's divergence from independence is\nintroduced as a multivariate extension of covariance. The implications\naddressed here are important in the majority of practical problems appearing in\nBayesian inference of multi-parameter models with analogous structures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 11:57:09 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Vitoratou", "Silia", ""], ["Ntzoufras", "Ioannis", ""], ["Moustaki", "Irini", ""]]}, {"id": "1311.0674", "submitter": "Konstantinos Perrakis", "authors": "K. Perrakis, I. Ntzoufras and E.G. Tsionas", "title": "On the use of marginal posteriors in marginal likelihood estimation via\n  importance-sampling", "comments": null, "journal-ref": "Computational Statistics & Data Analysis Volume 77, September\n  2014, Pages 54-69", "doi": "10.1016/j.csda.2014.03.004", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the efficiency of a marginal likelihood estimator where the\nproduct of the marginal posterior distributions is used as an\nimportance-sampling function. The approach is generally applicable to\nmulti-block parameter vector settings, does not require additional Markov Chain\nMonte Carlo (MCMC) sampling and is not dependent on the type of MCMC scheme\nused to sample from the posterior. The proposed approach is applied to normal\nregression models, finite normal mixtures and longitudinal Poisson models, and\nleads to accurate marginal likelihood estimates.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 12:30:46 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2014 18:05:36 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Perrakis", "K.", ""], ["Ntzoufras", "I.", ""], ["Tsionas", "E. G.", ""]]}, {"id": "1311.0686", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Particle Metropolis-Hastings using gradient and Hessian information", "comments": "27 pages, 5 figures, 2 tables. The final publication is available at\n  Springer via: http://dx.doi.org/10.1007/s11222-014-9510-0", "journal-ref": "Statistics and Computing, Volume 25, Issue 1, pp 81-92, 2015", "doi": "10.1007/s11222-014-9510-0", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Metropolis-Hastings (PMH) allows for Bayesian parameter inference in\nnonlinear state space models by combining Markov chain Monte Carlo (MCMC) and\nparticle filtering. The latter is used to estimate the intractable likelihood.\nIn its original formulation, PMH makes use of a marginal MCMC proposal for the\nparameters, typically a Gaussian random walk. However, this can lead to a poor\nexploration of the parameter space and an inefficient use of the generated\nparticles.\n  We propose a number of alternative versions of PMH that incorporate gradient\nand Hessian information about the posterior into the proposal. This information\nis more or less obtained as a byproduct of the likelihood estimation. Indeed,\nwe show how to estimate the required information using a fixed-lag particle\nsmoother, with a computational cost growing linearly in the number of\nparticles. We conclude that the proposed methods can: (i) decrease the length\nof the burn-in phase, (ii) increase the mixing of the Markov chain at the\nstationary phase, and (iii) make the proposal distribution scale invariant\nwhich simplifies tuning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:10:31 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 08:27:32 GMT"}, {"version": "v3", "created": "Mon, 16 Jun 2014 12:07:03 GMT"}, {"version": "v4", "created": "Thu, 18 Sep 2014 22:28:55 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1311.0689", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin and Fredrik Lindsten", "title": "Particle filter-based Gaussian process optimisation for parameter\n  inference", "comments": "Accepted for publication in proceedings of the 19th World Congress of\n  the International Federation of Automatic Control (IFAC), Cape Town, South\n  Africa, August 2014. 6 pages, 4 figures", "journal-ref": null, "doi": "10.3182/20140824-6-ZA-1003.00278", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for maximum likelihood-based parameter inference in\nnonlinear and/or non-Gaussian state space models. The method is an iterative\nprocedure with three steps. At each iteration a particle filter is used to\nestimate the value of the log-likelihood function at the current parameter\niterate. Using these log-likelihood estimates, a surrogate objective function\nis created by utilizing a Gaussian process model. Finally, we use a heuristic\nprocedure to obtain a revised parameter iterate, providing an automatic\ntrade-off between exploration and exploitation of the surrogate model. The\nmethod is profiled on two state space models with good performance both\nconsidering accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 13:14:27 GMT"}, {"version": "v2", "created": "Mon, 31 Mar 2014 07:12:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1311.0907", "submitter": "Vinayak Rao", "authors": "Lizhen Lin, Vinayak Rao and David B. Dunson", "title": "Bayesian nonparametric inference on the Stiefel manifold", "comments": "Split the original paper into two paper, with details of computation\n  included in arXiv:1406.6652", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stiefel manifold $V_{p,d}$ is the space of all $d \\times p$ orthonormal\nmatrices, with the $d-1$ hypersphere and the space of all orthogonal matrices\nconstituting special cases. In modeling data lying on the Stiefel manifold,\nparametric distributions such as the matrix Langevin distribution are often\nused; however, model misspecification is a concern and it is desirable to have\nnonparametric alternatives. Current nonparametric methods are Fr\\'echet mean\nbased. We take a fully generative nonparametric approach, which relies on\nmixing parametric kernels such as the matrix Langevin. The proposed kernel\nmixtures can approximate a large class of distributions on the Stiefel\nmanifold, and we develop theory showing posterior consistency. While there\nexists work developing general posterior consistency results, extending these\nresults to this particular manifold requires substantial new theory. Posterior\ninference is illustrated on a real-world dataset of near-Earth objects.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 21:43:29 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 15:01:24 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Lin", "Lizhen", ""], ["Rao", "Vinayak", ""], ["Dunson", "David B.", ""]]}, {"id": "1311.1129", "submitter": "Simon Byrne", "authors": "Simon Byrne, Mark Girolami, Persi Diaconis, Christof Seiler, Susan\n  Holmes, Ian L. Dryden, John T. Kent, Marcelo Pereyra, Babak Shahbaba, Shiwei\n  Lan, Jeffrey Streets, Daniel Simpson", "title": "Discussion of \"Geodesic Monte Carlo on Embedded Manifolds\"", "comments": "Discussion of arXiv:1301.6064. To appear in the Scandinavian Journal\n  of Statistics. 18 pages", "journal-ref": "Scandinavian Journal of Statistics (2014) 41 (1), pages 1-21", "doi": "10.1111/sjos.12081", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contributed discussion and rejoinder to \"Geodesic Monte Carlo on Embedded\nManifolds\" (arXiv:1301.6064)\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 17:17:15 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Byrne", "Simon", ""], ["Girolami", "Mark", ""], ["Diaconis", "Persi", ""], ["Seiler", "Christof", ""], ["Holmes", "Susan", ""], ["Dryden", "Ian L.", ""], ["Kent", "John T.", ""], ["Pereyra", "Marcelo", ""], ["Shahbaba", "Babak", ""], ["Lan", "Shiwei", ""], ["Streets", "Jeffrey", ""], ["Simpson", "Daniel", ""]]}, {"id": "1311.1835", "submitter": "Demetris Christopoulos", "authors": "Demetris T. Christopoulos", "title": "Linear Regression without computing pseudo-inverse matrix", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting a method of linear regression based on Gram-Schmidt\northogonal projection that does not compute a pseudo-inverse matrix. This is\nuseful when we want to make several regressions with random data vectors for\nsimulation purposes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 09:41:27 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Christopoulos", "Demetris T.", ""]]}, {"id": "1311.1888", "submitter": "Marco Selig", "authors": "Marco Selig and Torsten En{\\ss}lin", "title": "D$^3$PO - Denoising, Deconvolving, and Decomposing Photon Observations", "comments": "22 pages, 8 figures, 2 tables, accepted by Astronomy & Astrophysics;\n  refereed version, 1 figure added, results unchanged, software available at\n  http://www.mpa-garching.mpg.de/ift/d3po/", "journal-ref": "A&A 574, A74 (2015)", "doi": "10.1051/0004-6361/201323006", "report-no": null, "categories": "astro-ph.IM cs.IT math.IT physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of astronomical images is a non-trivial task. The D3PO algorithm\naddresses the inference problem of denoising, deconvolving, and decomposing\nphoton observations. Its primary goal is the simultaneous but individual\nreconstruction of the diffuse and point-like photon flux given a single photon\ncount image, where the fluxes are superimposed. In order to discriminate\nbetween these morphologically different signal components, a probabilistic\nalgorithm is derived in the language of information field theory based on a\nhierarchical Bayesian parameter model. The signal inference exploits prior\ninformation on the spatial correlation structure of the diffuse component and\nthe brightness distribution of the spatially uncorrelated point-like sources. A\nmaximum a posteriori solution and a solution minimizing the Gibbs free energy\nof the inference problem using variational Bayesian methods are discussed.\nSince the derivation of the solution is not dependent on the underlying\nposition space, the implementation of the D3PO algorithm uses the NIFTY package\nto ensure applicability to various spatial grids and at any resolution. The\nfidelity of the algorithm is validated by the analysis of simulated data,\nincluding a realistic high energy photon count image showing a 32 x 32 arcmin^2\nobservation with a spatial resolution of 0.1 arcmin. In all tests the D3PO\nalgorithm successfully denoised, deconvolved, and decomposed the data into a\ndiffuse and a point-like signal estimate for the respective photon flux\ncomponents.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 07:17:05 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 11:53:05 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Selig", "Marco", ""], ["En\u00dflin", "Torsten", ""]]}, {"id": "1311.1890", "submitter": "Daniel Rudolf", "authors": "Josef Dick, Daniel Rudolf", "title": "Discrepancy estimates for variance bounding Markov chain quasi-Monte\n  Carlo", "comments": "24 pages", "journal-ref": "Electron. J. Probab. 19 (2014), no. 105, 1-24", "doi": "10.1214/EJP.v19-3132", "report-no": null, "categories": "stat.CO math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) simulations are modeled as driven by true\nrandom numbers. We consider variance bounding Markov chains driven by a\ndeterministic sequence of numbers. The star-discrepancy provides a measure of\nefficiency of such Markov chain quasi-Monte Carlo methods. We define a\npull-back discrepancy of the driver sequence and state a close relation to the\nstar-discrepancy of the Markov chain-quasi Monte Carlo samples. We prove that\nthere exists a deterministic driver sequence such that the discrepancies\ndecrease almost with the Monte Carlo rate $n^{1/2}$. As for MCMC simulations, a\nburn-in period can also be taken into account for Markov chain quasi-Monte\nCarlo to reduce the influence of the initial state. In particular, our\ndiscrepancy bound leads to an estimate of the error for the computation of\nexpectations. To illustrate our theory we provide an example for the Metropolis\nalgorithm based on a ball walk. Furthermore, under additional assumptions we\nprove the existence of a driver sequence such that the discrepancy of the\ncorresponding deterministic Markov chain sample decreases with order\n$n^{-1+\\delta}$ for every $\\delta>0$.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 07:47:13 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 16:30:34 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Dick", "Josef", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1311.2002", "submitter": "Nevena Maric", "authors": "Mark Huber and Nevena Maric", "title": "Multivariate distributions with fixed marginals and correlations", "comments": "Compared to the journal version, here Lemma 4 and Theorem 3, for case\n  n=4, are slightly corrected", "journal-ref": "Journal of Applied Probability 52(2) (2015), 602-608", "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of drawing random variates $(X_1,\\ldots,X_n)$ from a\ndistribution where the marginal of each $X_i$ is specified, as well as the\ncorrelation between every pair $X_i$ and $X_j$. For given marginals, the\nFr\\'echet-Hoeffding bounds put a lower and upper bound on the correlation\nbetween $X_i$ and $X_j$. Any achievable correlation between $X_i$ and $X_j$ is\na convex combinations of these bounds. The value $\\lambda(X_i,X_j) \\in [0,1]$\nof this convex combination is called here the convexity parameter of\n$(X_i,X_j),$ with $\\lambda(X_i,X_j) = 1$ corresponding to the upper bound and\nmaximal correlation. For given marginal distributions functions\n$F_1,\\ldots,F_n$ of $(X_1,\\ldots,X_n)$ we show that $\\lambda(X_i,X_j) =\n\\lambda_{ij}$ if and only if there exist symmetric Bernoulli random variables\n$(B_1,\\ldots,B_n)$ (that is $\\{0,1\\}$ random variables with mean 1/2) such that\n$\\lambda(B_i,B_j) = \\lambda_{ij}$. In addition, we characterize completely the\nset of convexity parameters for symmetric Bernoulli marginals in two, three and\nfour dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 15:29:33 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 23:47:07 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Huber", "Mark", ""], ["Maric", "Nevena", ""]]}, {"id": "1311.2166", "submitter": "Ari Pakman", "authors": "Ari Pakman and Liam Paninski", "title": "Auxiliary-variable Exact Hamiltonian Monte Carlo Samplers for Binary\n  Distributions", "comments": "11 pages, 4 figures. Proceedings of the 27th Annual Conference Neural\n  Information Processing Systems (NIPS), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to sample from generic binary distributions, based\non an exact Hamiltonian Monte Carlo algorithm applied to a piecewise continuous\naugmentation of the binary distribution of interest. An extension of this idea\nto distributions over mixtures of binary and possibly-truncated Gaussian or\nexponential variables allows us to sample from posteriors of linear and probit\nregression models with spike-and-slab priors and truncated parameters. We\nillustrate the advantages of these algorithms in several examples in which they\noutperform the Metropolis or Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Sat, 9 Nov 2013 12:59:17 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 14:40:38 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1311.2281", "submitter": "Andres Christen", "authors": "Marcos Capistr\\'an, J. Andr\\'es Christen and Sophie Donnet", "title": "Bayesian Analysis of ODE's: solver optimal accuracy and Bayes factors", "comments": "28 pages, 6 figures", "journal-ref": "Journal of Uncertainty Quantification, 2016, V. 4, p. 829-849", "doi": "10.1137/140976777", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most relevant cases in the Bayesian analysis of ODE inverse problems, a\nnumerical solver needs to be used. Therefore, we cannot work with the exact\ntheoretical posterior distribution but only with an approximate posterior\nderiving from the error in the numerical solver. To compare a numerical and the\ntheoretical posterior distributions we propose to use Bayes Factors (BF),\nconsidering both of them as models for the data at hand. We prove that the\ntheoretical vs a numerical posterior BF tends to 1, in the same order (of the\nstep size used) as the numerical forward map solver does. For higher order\nsolvers (eg. Runge-Kutta) the Bayes Factor is already nearly 1 for step sizes\nthat would take far less computational effort. Considerable CPU time may be\nsaved by using coarser solvers that nevertheless produce practically error free\nposteriors. Two examples are presented where nearly 90% CPU time is saved while\nall inference results are identical to using a solver with a much finer time\nstep.\n", "versions": [{"version": "v1", "created": "Sun, 10 Nov 2013 15:41:19 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Capistr\u00e1n", "Marcos", ""], ["Christen", "J. Andr\u00e9s", ""], ["Donnet", "Sophie", ""]]}, {"id": "1311.2335", "submitter": "Selin Damla Ahipasaoglu", "authors": "Selin Damla Ahipasaoglu", "title": "A First-Order Algorithm for the A-Optimal Experimental Design Problem: A\n  Mathematical Programming Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyse a first-order algorithm for the A-optimal experimental\ndesign problem. The problem is first presented as a special case of a\nparametric family of optimal design problems for which duality results and\noptimality conditions are given. Then, two first-order (Frank-Wolfe type)\nalgorithms are presented, accompanied by a detailed time-complexity analysis of\nthe algorithms and computational results on various sized problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 02:38:44 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Ahipasaoglu", "Selin Damla", ""]]}, {"id": "1311.2376", "submitter": "Giorgio Ottaviani", "authors": "Giorgio Ottaviani, Pierre-Jean Spaenlehauer and Bernd Sturmfels", "title": "Exact Solutions in Structured Low-Rank Approximation", "comments": "22 pages; theorem numbering fits with the journal version", "journal-ref": "SIAM Journal on Matrix Analysis and Applications, 35 (4) (2014),\n  1521-1542", "doi": null, "report-no": null, "categories": "math.OC cs.SC math.AG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured low-rank approximation is the problem of minimizing a weighted\nFrobenius distance to a given matrix among all matrices of fixed rank in a\nlinear space of matrices. We study exact solutions to this problem by way of\ncomputational algebraic geometry. A particular focus lies on Hankel matrices,\nSylvester matrices and generic linear spaces.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 08:22:26 GMT"}, {"version": "v2", "created": "Thu, 24 Apr 2014 15:37:16 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 17:31:11 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Ottaviani", "Giorgio", ""], ["Spaenlehauer", "Pierre-Jean", ""], ["Sturmfels", "Bernd", ""]]}, {"id": "1311.2994", "submitter": "Scott Sisson", "authors": "J. Lee and Y. Fan and S. A. Sisson", "title": "Bayesian threshold selection for extremal models using measures of\n  surprise", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical extreme value theory is concerned with the use of asymptotically\nmotivated models to describe the extreme values of a process. A number of\ncommonly used models are valid for observed data that exceed some high\nthreshold. However, in practice a suitable threshold is unknown and must be\ndetermined for each analysis. While there are many threshold selection methods\nfor univariate extremes, there are relatively few that can be applied in the\nmultivariate setting. In addition, there are only a few Bayesian-based methods,\nwhich are naturally attractive in the modelling of extremes due to data\nscarcity. The use of Bayesian measures of surprise to determine suitable\nthresholds for extreme value models is proposed. Such measures quantify the\nlevel of support for the proposed extremal model and threshold, without the\nneed to specify any model alternatives. This approach is easily implemented for\nboth univariate and multivariate extremes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 00:57:12 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 11:43:04 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Lee", "J.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1311.3190", "submitter": "Amit Moscovich-Eiger", "authors": "Amit Moscovich, Boaz Nadler, Clifford Spiegelman", "title": "On the exact Berk-Jones statistics and their p-value calculation", "comments": "29 pages, 3 figures, pdflatex; Minor revision", "journal-ref": "Electronic Journal of Statistics 10 (2016) 2329-2354", "doi": "10.1214/16-EJS1172", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous goodness-of-fit testing is a classical problem in statistics.\nDespite having low power for detecting deviations at the tail of a\ndistribution, the most popular test is based on the Kolmogorov-Smirnov\nstatistic. While similar variance-weighted statistics, such as Anderson-Darling\nand the Higher Criticism statistic give more weight to tail deviations, as\nshown in various works, they still mishandle the extreme tails.\n  As a viable alternative, in this paper we study some of the statistical\nproperties of the exact $M_n$ statistics of Berk and Jones. We derive the\nasymptotic null distributions of $M_n, M_n^+, M_n^-$, and further prove their\nconsistency as well as asymptotic optimality for a wide range of rare-weak\nmixture models. Additionally, we present a new computationally efficient method\nto calculate $p$-values for any supremum-based one-sided statistic, including\nthe one-sided $M_n^+,M_n^-$ and $R_n^+,R_n^-$ statistics of Berk and Jones and\nthe Higher Criticism statistic. We illustrate our theoretical analysis with\nseveral finite-sample simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:19:58 GMT"}, {"version": "v2", "created": "Sun, 18 May 2014 17:34:05 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 16:49:58 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2015 11:26:37 GMT"}, {"version": "v5", "created": "Thu, 24 Mar 2016 15:20:16 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Moscovich", "Amit", ""], ["Nadler", "Boaz", ""], ["Spiegelman", "Clifford", ""]]}, {"id": "1311.3830", "submitter": "Josef Dick", "authors": "Josef Dick", "title": "Applications of geometric discrepancy in numerical analysis and\n  statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss various connections between geometric discrepancy\nmeasures, such as discrepancy with respect to convex sets (and convex sets with\nsmooth boundary in particular), and applications to numerical analysis and\nstatistics, like point distributions on the sphere, the acceptance-rejection\nalgorithm and certain Markov chain Monte Carlo algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 12:44:43 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Dick", "Josef", ""]]}, {"id": "1311.4117", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim and Sumeetpal Singh and Thomas Dean and Ajay Jasra", "title": "Parameter Estimation in Hidden Markov Models with Intractable\n  Likelihoods Using Sequential Monte Carlo", "comments": "22 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sequential Monte Carlo based algorithms for maximum likelihood\nestimation of the static parameters in hidden Markov models with an intractable\nlikelihood using ideas from approximate Bayesian computation. The static\nparameter estimation algorithms are gradient based and cover both offline and\nonline estimation. We demonstrate their performance by estimating the\nparameters of three intractable models, namely the alpha-stable distribution,\ng-and-k distribution, and the stochastic volatility model with alpha-stable\nreturns, using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 05:27:56 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Yildirim", "Sinan", ""], ["Singh", "Sumeetpal", ""], ["Dean", "Thomas", ""], ["Jasra", "Ajay", ""]]}, {"id": "1311.4555", "submitter": "Stefan Wager", "authors": "Stefan Wager, Trevor Hastie, and Bradley Efron", "title": "Confidence Intervals for Random Forests: The Jackknife and the\n  Infinitesimal Jackknife", "comments": "To appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variability of predictions made by bagged learners and random\nforests, and show how to estimate standard errors for these methods. Our work\nbuilds on variance estimates for bagging proposed by Efron (1992, 2012) that\nare based on the jackknife and the infinitesimal jackknife (IJ). In practice,\nbagged predictors are computed using a finite number B of bootstrap replicates,\nand working with a large B can be computationally expensive. Direct\napplications of jackknife and IJ estimators to bagging require B on the order\nof n^{1.5} bootstrap replicates to converge, where n is the size of the\ntraining set. We propose improved versions that only require B on the order of\nn replicates. Moreover, we show that the IJ estimator requires 1.7 times less\nbootstrap replicates than the jackknife to achieve a given accuracy. Finally,\nwe study the sampling distributions of the jackknife and IJ variance estimates\nthemselves. We illustrate our findings with multiple experiments and simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:03:31 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 00:06:16 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Wager", "Stefan", ""], ["Hastie", "Trevor", ""], ["Efron", "Bradley", ""]]}, {"id": "1311.4780", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Chong Wang, Eric Xing", "title": "Asymptotically Exact, Embarrassingly Parallel MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication costs, resulting from synchronization requirements during\nlearning, can greatly slow down many parallel machine learning algorithms. In\nthis paper, we present a parallel Markov chain Monte Carlo (MCMC) algorithm in\nwhich subsets of data are processed independently, with very little\ncommunication. First, we arbitrarily partition data onto multiple machines.\nThen, on each machine, any classical MCMC method (e.g., Gibbs sampling) may be\nused to draw samples from a posterior distribution given the data subset.\nFinally, the samples from each machine are combined to form samples from the\nfull posterior. This embarrassingly parallel algorithm allows each machine to\nact independently on a subset of the data (without communication) until the\nfinal combination stage. We prove that our algorithm generates asymptotically\nexact samples and empirically demonstrate its ability to parallelize burn-in\nand sampling in several models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 15:23:04 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 04:25:50 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Chong", ""], ["Xing", "Eric", ""]]}, {"id": "1311.5343", "submitter": "Laura Vinckenbosch", "authors": "Laura Vinckenbosch, C\\'eline Lacaux, Samy Tindel, Magalie Thomassin,\n  Tiphaine Obara", "title": "Monte Carlo methods for light propagation in biological tissues", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Light propagation in turbid media is driven by the equation of radiative\ntransfer. We give a formal probabilistic representation of its solution in the\nframework of biological tissues and we implement algorithms based on Monte\nCarlo methods in order to estimate the quantity of light that is received by an\nhomogeneous tissue when emitted by an optic fiber. A variance reduction method\nis studied and implemented, as well as a Markov chain Monte Carlo method based\non the Metropolis-Hastings algorithm. The resulting estimating methods are then\ncompared to the so-called Wang-Prahl (or Wang) method. Finally, the formal\nrepresentation allows to derive a non-linear optimization algorithm close to\nLevenberg-Marquardt that is used for the estimation of the scattering and\nabsorption coefficients of the tissue from measurements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 09:38:27 GMT"}, {"version": "v2", "created": "Wed, 15 Oct 2014 11:02:36 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Vinckenbosch", "Laura", ""], ["Lacaux", "C\u00e9line", ""], ["Tindel", "Samy", ""], ["Thomassin", "Magalie", ""], ["Obara", "Tiphaine", ""]]}, {"id": "1311.5699", "submitter": "Jere Koskela", "authors": "Jere Koskela, Paul A. Jenkins and Dario Spano", "title": "Computational inference beyond Kingman's coalescent", "comments": "20 pages, 5 figures", "journal-ref": "J. Appl. Probab. 52(2), p. 519-537, 2015", "doi": "10.1239/jap/1437658613", "report-no": null, "categories": "math.PR q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full likelihood inference under Kingman's coalescent is a computationally\nchallenging problem to which importance sampling (IS) and the product of\napproximate conditionals (PAC) method have been applied successfully. Both\nmethods can be expressed in terms of families of intractable conditional\nsampling distributions (CSDs), and rely on principled approximations for\naccurate inference. Recently, more general $\\Lambda$- and $\\Xi$-coalescents\nhave been observed to provide better modelling fits to some genetic data sets.\nWe derive families of approximate CSDs for finite sites $\\Lambda$- and\n$\\Xi$-coalescents, and use them to obtain \"approximately optimal\" IS and PAC\nalgorithms for $\\Lambda$-coalescents, yielding substantial gains in efficiency\nover existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 10:25:16 GMT"}, {"version": "v2", "created": "Mon, 9 Jun 2014 15:26:52 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2015 11:08:29 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2015 15:38:16 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Koskela", "Jere", ""], ["Jenkins", "Paul A.", ""], ["Spano", "Dario", ""]]}, {"id": "1311.5777", "submitter": "Paul Jenkins", "authors": "Paul A. Jenkins", "title": "Exact simulation of the sample paths of a diffusion with a finite\n  entrance boundary", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion processes arise in many fields, and so simulating the path of a\ndiffusion is an important problem. It is usually necessary to make some sort of\napproximation via model-discretization, but a recently introduced class of\nalgorithms, known as the exact algorithm and based on retrospective rejection\nsampling ideas, obviate the need for such discretization. In this paper I\nextend the exact algorithm to apply to a class of diffusions with a finite\nentrance boundary. The key innovation is that for these models the Bessel\nprocess is a more suitable candidate process than the more usually chosen\nBrownian motion. The algorithm is illustrated by an application to a general\ndiffusion model of population growth, where it simulates paths efficiently,\nwhile previous algorithms are impracticable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 15:16:15 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Jenkins", "Paul A.", ""]]}, {"id": "1311.5947", "submitter": "Chunhua Shen", "authors": "Guosheng Lin, Chunhua Shen, Anton van den Hengel, David Suter", "title": "Fast Training of Effective Multi-class Boosting Using Coordinate Descent\n  Optimization", "comments": "Appeared in Proc. Asian Conf. Computer Vision 2012. Code can be\n  downloaded at http://goo.gl/WluhrQ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wepresentanovelcolumngenerationbasedboostingmethod for multi-class\nclassification. Our multi-class boosting is formulated in a single optimization\nproblem as in Shen and Hao (2011). Different from most existing multi-class\nboosting methods, which use the same set of weak learners for all the classes,\nwe train class specified weak learners (i.e., each class has a different set of\nweak learners). We show that using separate weak learner sets for each class\nleads to fast convergence, without introducing additional computational\noverhead in the training procedure. To further make the training more efficient\nand scalable, we also propose a fast co- ordinate descent method for solving\nthe optimization problem at each boosting iteration. The proposed coordinate\ndescent method is conceptually simple and easy to implement in that it is a\nclosed-form solution for each coordinate update. Experimental results on a\nvariety of datasets show that, compared to a range of existing multi-class\nboosting meth- ods, the proposed method has much faster convergence rate and\nbetter generalization performance in most cases. We also empirically show that\nthe proposed fast coordinate descent algorithm needs less training time than\nthe MultiBoost algorithm in Shen and Hao (2011).\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 02:30:14 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Lin", "Guosheng", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""], ["Suter", "David", ""]]}, {"id": "1311.6000", "submitter": "Christian P. Robert", "authors": "Jeong Eun Lee (Auckland University of Technology) and Christian P.\n  Robert (Universite Paris-Dauphine and University of Warwick)", "title": "Importance sampling schemes for evidence approximation in mixture models", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood is a central tool for drawing Bayesian inference\nabout the number of components in mixture models. It is often approximated\nsince the exact form is unavailable. A bias in the approximation may be due to\nan incomplete exploration by a simulated Markov chain (e.g., a Gibbs sequence)\nof the collection of posterior modes, a phenomenon also known as lack of label\nswitching, as all possible label permutations must be simulated by a chain in\norder to converge and hence overcome the bias. In an importance sampling\napproach, imposing label switching to the importance function results in an\nexponential increase of the computational cost with the number of components.\nIn this paper, two importance sampling schemes are proposed through choices for\nthe importance function; a MLE proposal and a Rao-Blackwellised importance\nfunction. The second scheme is called dual importance sampling. We demonstrate\nthat this dual importance sampling is a valid estimator of the evidence and\nmoreover show that the statistical efficiency of estimates increases. To reduce\nthe induced high demand in computation, the original importance function is\napproximated but a suitable approximation can produce an estimate with the same\nprecision and with reduced computational workload.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 14:01:31 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 11:02:54 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Lee", "Jeong Eun", "", "Auckland University of Technology"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine and University of Warwick"]]}, {"id": "1311.6403", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Kaspar Rufibach and Dominic Schuhmacher", "title": "Maximum-Likelihood Estimation of a Log-Concave Density based on Censored\n  Data", "comments": null, "journal-ref": "Electronic Journal of Statistics 8 (2014), 1405-1437", "doi": "10.1214/14-EJS930", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric maximum-likelihood estimation of a log-concave\ndensity in case of interval-censored, right-censored and binned data. We allow\nfor the possibility of a subprobability density with an additional mass at\n$+\\infty$, which is estimated simultaneously. The existence of the estimator is\nproved under mild conditions and various theoretical aspects are given, such as\ncertain shape and consistency properties. An EM algorithm is proposed for the\napproximate computation of the estimator and its performance is illustrated in\ntwo examples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 18:47:27 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 16:01:19 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 20:25:55 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Duembgen", "Lutz", ""], ["Rufibach", "Kaspar", ""], ["Schuhmacher", "Dominic", ""]]}, {"id": "1311.6486", "submitter": "Gustaf Hendeby", "authors": "Saikat Saha and Gustaf Hendeby", "title": "Online inference in Markov modulated nonlinear dynamic systems: a\n  Rao-Blackwellized particle filtering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov modulated (switching) state space is an important model paradigm\nin applied statistics. In this article, we specifically consider Markov\nmodulated nonlinear state-space models and address the online Bayesian\ninference problem for such models. In particular, we propose a new\nRao-Blackwellized particle filter for the inference task which is our main\ncontribution here. The detailed descriptions including an algorithmic summary\nare subsequently presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 21:21:57 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Saha", "Saikat", ""], ["Hendeby", "Gustaf", ""]]}, {"id": "1311.6529", "submitter": "Noah Simon", "authors": "Noah Simon, Jerome Friedman, Trevor Hastie", "title": "A Blockwise Descent Algorithm for Group-penalized Multiresponse and\n  Multinomial Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we purpose a blockwise descent algorithm for group-penalized\nmultiresponse regression. Using a quasi-newton framework we extend this to\ngroup-penalized multinomial regression. We give a publicly available\nimplementation for these in R, and compare the speed of this algorithm to a\ncompeting algorithm --- we show that our implementation is an order of\nmagnitude faster than its competitor, and can solve gene-expression-sized\nproblems in real time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 00:57:30 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Simon", "Noah", ""], ["Friedman", "Jerome", ""], ["Hastie", "Trevor", ""]]}, {"id": "1311.7286", "submitter": "Erlis Ruli Dr.", "authors": "Erlis Ruli, Nicola Sartori, Laura Ventura", "title": "Approximate Bayesian Computation with composite score functions", "comments": "Statistics and Computing (final version)", "journal-ref": null, "doi": "10.1007/s11222-015-9551-z", "report-no": "STCO-D-14-00149R2", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both Approximate Bayesian Computation (ABC) and composite likelihood methods\nare useful for Bayesian and frequentist inference, respectively, when the\nlikelihood function is intractable. We propose to use composite likelihood\nscore functions as summary statistics in ABC in order to obtain accurate\napproximations to the posterior distribution. This is motivated by the use of\nthe score function of the full likelihood, and extended to general unbiased\nestimating functions in complex models. Moreover, we show that if the composite\nscore is suitably standardised, the resulting ABC procedure is invariant to\nreparameterisations and automatically adjusts the curvature of the composite\nlikelihood, and of the corresponding posterior distribution. The method is\nillustrated through examples with simulated data, and an application to\nmodelling of spatial extreme rainfall data is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 11:52:41 GMT"}, {"version": "v2", "created": "Thu, 22 May 2014 10:19:16 GMT"}, {"version": "v3", "created": "Sat, 21 Feb 2015 10:16:53 GMT"}, {"version": "v4", "created": "Tue, 24 Feb 2015 19:55:05 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Ruli", "Erlis", ""], ["Sartori", "Nicola", ""], ["Ventura", "Laura", ""]]}, {"id": "1311.7525", "submitter": "Demetris Christopoulos", "authors": "Demetris T. Christopoulos", "title": "Polynomial regression using trapezoidal rule for computing Legendre\n  coefficients", "comments": "13 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting a method for computing the Fourier coefficients of a given\npolynomial regression by using the trapezoidal rule for numerical integration.\nAs function basis we use the orthogonal Legendre polynomials. The results are\naccurate and stable compared to Forsythe's method.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 11:34:37 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Christopoulos", "Demetris T.", ""]]}, {"id": "1311.7656", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Suvrit Sra", "title": "Statistical estimation for optimization problems on graphs", "comments": "Paper for the NIPS Workshop on Discrete Optimization for Machine\n  Learning (DISCML) (2011): Uncertainty, Generalization and Feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large graphs abound in machine learning, data mining, and several related\nareas. A useful step towards analyzing such graphs is that of obtaining certain\nsummary statistics - e.g., or the expected length of a shortest path between\ntwo nodes, or the expected weight of a minimum spanning tree of the graph, etc.\nThese statistics provide insight into the structure of a graph, and they can\nhelp predict global properties of a graph. Motivated thus, we propose to study\nstatistical properties of structured subgraphs (of a given graph), in\nparticular, to estimate the expected objective function value of a\ncombinatorial optimization problem over these subgraphs. The general task is\nvery difficult, if not unsolvable; so for concreteness we describe a more\nspecific statistical estimation problem based on spanning trees. We hope that\nour position paper encourages others to also study other types of graphical\nstructures for which one can prove nontrivial statistical estimates.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:21:13 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Sra", "Suvrit", ""]]}]