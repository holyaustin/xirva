[{"id": "1511.00108", "submitter": "Satoshi Kuriki", "authors": "Satoshi Kuriki, Kunihiko Takahashi, Hisayuki Hara", "title": "Recursive computation for evaluating the exact $p$-values of temporal\n  and spatial scan statistics", "comments": "23 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $V$ be a finite set of indices, and let $B_i$, $i=1,\\ldots,m$, be subsets\nof $V$ such that $V=\\bigcup_{i=1}^{m}B_i$. Let $X_i$, $i\\in V$, be independent\nrandom variables, and let $X_{B_i}=(X_j)_{j\\in B_i}$. In this paper, we propose\na recursive computation method to calculate the conditional expectation\n$E\\bigl[\\prod_{i=1}^m\\chi_i(X_{B_i}) \\,|\\, N\\bigr]$ with $N=\\sum_{i\\in V}X_i$\ngiven, where $\\chi_i$ is an arbitrary function. Our method is based on the\nrecursive summation/integration technique using the Markov property in\nstatistics. To extract the Markov property, we define an undirected graph whose\ncliques are $B_j$, and obtain its chordal extension, from which we present the\nexpressions of the recursive formula. This methodology works for a class of\ndistributions including the Poisson distribution (that is, the conditional\ndistribution is the multinomial). This problem is motivated from the evaluation\nof the multiplicity-adjusted $p$-value of scan statistics in spatial\nepidemiology. As an illustration of the approach, we present the real data\nanalyses to detect temporal and spatial clustering.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 10:36:41 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kuriki", "Satoshi", ""], ["Takahashi", "Kunihiko", ""], ["Hara", "Hisayuki", ""]]}, {"id": "1511.00146", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Reza Babanezhad, Wu Lin, Mark Schmidt, Masashi\n  Sugiyama", "title": "Faster Stochastic Variational Inference using Proximal-Gradient Methods\n  with General Divergence Functions", "comments": "Published in UAI 2016. We have made the following change in this\n  revision: instead of expressing convergence rate results in terms of the\n  iterate difference, we state them in terms of the iterate distance divided by\n  the step-size (a measure of first-order optimality). We also removed some\n  claims about the performance with a fixed step size", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several recent works have explored stochastic gradient methods for\nvariational inference that exploit the geometry of the variational-parameter\nspace. However, the theoretical properties of these methods are not\nwell-understood and these methods typically only apply to\nconditionally-conjugate models. We present a new stochastic method for\nvariational inference which exploits the geometry of the variational-parameter\nspace and also yields simple closed-form updates even for non-conjugate models.\nWe also give a convergence-rate analysis of our method and many other previous\nmethods which exploit the geometry of the space. Our analysis generalizes\nexisting convergence results for stochastic mirror-descent on non-convex\nobjectives by using a more general class of divergence functions. Beyond giving\na theoretical justification for a variety of recent methods, our experiments\nshow that new algorithms derived in this framework lead to state of the art\nresults on a variety of problems. Further, due to its generality, we expect\nthat our theoretical analysis could also apply to other applications.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 15:56:32 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 23:47:06 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 00:47:22 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Babanezhad", "Reza", ""], ["Lin", "Wu", ""], ["Schmidt", "Mark", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1511.00326", "submitter": "Zdravko Botev", "authors": "Kevin Lam, Zdravko Botev", "title": "The Dynamic Splitting Method with an application to portfolio credit\n  risk", "comments": "Honours thesis of Kevin Lam under the supervision of Zdravko Botev at\n  UNSW", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider the problem of accurately measuring the credit risk of a\nportfolio consisting of loss exposures such as loans, bonds and other financial\nassets. We are particularly interested in the probability of large portfolio\nlosses. We describe the popular models in the credit risk framework including\nfactor models and copula models. To this end, we revisit the most efficient\nprobability estimation algorithms within current copula credit risk literature,\nnamely importance sampling. We illustrate the workings and developments of\nthese algorithms for large portfolio loss probability estimation and quantile\nestimation. We then propose a modification to the dynamic splitting method\nwhich allows application to the credit risk models described. Our proposed\nalgorithm for the unbiased estimation of rare-event probabilities, exploits the\nquasi-monotonic property of functions to embed a static simulation problem\nwithin a time-dependent Markov process. A study of our proposed algorithm is\nthen conducted through numerical experiments with its performance benchmarked\nagainst current popular importance sampling algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 22:41:19 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Lam", "Kevin", ""], ["Botev", "Zdravko", ""]]}, {"id": "1511.01609", "submitter": "Benjamin Risk", "authors": "Benjamin B. Risk, David S. Matteson, David Ruppert", "title": "Linear Non-Gaussian Component Analysis via Maximum Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is popular in many applications,\nincluding cognitive neuroscience and signal processing. Due to computational\nconstraints, principal component analysis is used for dimension reduction prior\nto ICA (PCA+ICA), which could remove important information. The problem is that\ninteresting independent components (ICs) could be mixed in several principal\ncomponents that are discarded and then these ICs cannot be recovered. We\nformulate a linear non-Gaussian component model with Gaussian noise components.\nTo estimate this model, we propose likelihood component analysis (LCA), in\nwhich dimension reduction and latent variable estimation are achieved\nsimultaneously. Our method orders components by their marginal likelihood\nrather than ordering components by variance as in PCA. We present a parametric\nLCA using the logistic density and a semi-parametric LCA using tilted Gaussians\nwith cubic B-splines. Our algorithm is scalable to datasets common in\napplications (e.g., hundreds of thousands of observations across hundreds of\nvariables with dozens of latent components). In simulations, latent components\nare recovered that are discarded by PCA+ICA methods. We apply our method to\nmultivariate data and demonstrate that LCA is a useful data visualization and\ndimension reduction tool that reveals features not apparent from PCA or\nPCA+ICA. We also apply our method to an fMRI experiment from the Human\nConnectome Project and identify artifacts missed by PCA+ICA. We present\ntheoretical results on identifiability of the linear non-Gaussian component\nmodel and consistency of LCA.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 05:12:27 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 21:50:34 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 01:25:49 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""], ["Ruppert", "David", ""]]}, {"id": "1511.01707", "submitter": "Johan Dahlin PhD", "authors": "Johan Dahlin and Thomas B. Sch\\\"on", "title": "Getting Started with Particle Metropolis-Hastings for Inference in\n  Nonlinear Dynamical Models", "comments": "41 pages, 7 figures. In press for Journal of Statistical Software.\n  Source code for R, Python and MATLAB available at:\n  https://github.com/compops/pmh-tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial provides a gentle introduction to the particle\nMetropolis-Hastings (PMH) algorithm for parameter inference in nonlinear\nstate-space models together with a software implementation in the statistical\nprogramming language R. We employ a step-by-step approach to develop an\nimplementation of the PMH algorithm (and the particle filter within) together\nwith the reader. This final implementation is also available as the package\npmhtutorial in the CRAN repository. Throughout the tutorial, we provide some\nintuition as to how the algorithm operates and discuss some solutions to\nproblems that might occur in practice. To illustrate the use of PMH, we\nconsider parameter inference in a linear Gaussian state-space model with\nsynthetic data and a nonlinear stochastic volatility model with real-world\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 11:59:36 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2015 19:58:56 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 07:40:04 GMT"}, {"version": "v4", "created": "Thu, 31 Mar 2016 06:53:18 GMT"}, {"version": "v5", "created": "Thu, 6 Jul 2017 11:49:01 GMT"}, {"version": "v6", "created": "Sun, 27 Aug 2017 23:14:39 GMT"}, {"version": "v7", "created": "Fri, 20 Oct 2017 00:24:07 GMT"}, {"version": "v8", "created": "Tue, 12 Mar 2019 13:09:43 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1511.01863", "submitter": "Anders Eklund", "authors": "Anders Eklund, Thomas Nichols, Hans Knutsson", "title": "Can parametric statistical methods be trusted for fMRI based group\n  studies?", "comments": null, "journal-ref": "PNAS (2016), vol. 113 no. 28, 7900 - 7905", "doi": "10.1073/pnas.1602413113", "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most widely used task fMRI analyses use parametric methods that depend on\na variety of assumptions. While individual aspects of these fMRI models have\nbeen evaluated, they have not been evaluated in a comprehensive manner with\nempirical data. In this work, a total of 2 million random task fMRI group\nanalyses have been performed using resting state fMRI data, to compute\nempirical familywise error rates for the software packages SPM, FSL and AFNI,\nas well as a standard non-parametric permutation method. While there is some\nvariation, for a nominal familywise error rate of 5% the parametric statistical\nmethods are shown to be conservative for voxel-wise inference and invalid for\ncluster-wise inference; in particular, cluster size inference with a cluster\ndefining threshold of p = 0.01 generates familywise error rates up to 60%. We\nconduct a number of follow up analyses and investigations that suggest the\ncause of the invalid cluster inferences is spatial auto correlation functions\nthat do not follow the assumed Gaussian shape. By comparison, the\nnon-parametric permutation test, which is based on a small number of\nassumptions, is found to produce valid results for voxel as well as cluster\nwise inference. Using real task data, we compare the results between one\nparametric method and the permutation test, and find stark differences in the\nconclusions drawn between the two using cluster inference. These findings speak\nto the need of validating the statistical methods being used in the\nneuroimaging field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:34:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Eklund", "Anders", ""], ["Nichols", "Thomas", ""], ["Knutsson", "Hans", ""]]}, {"id": "1511.01942", "submitter": "Mark Schmidt", "authors": "Reza Babanezhad, Mohamed Osama Ahmed, Alim Virani, Mark Schmidt, Jakub\n  Kone\\v{c}n\\'y, Scott Sallinen", "title": "Stop Wasting My Gradients: Practical SVRG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and analyze several strategies for improving the performance of\nstochastic variance-reduced gradient (SVRG) methods. We first show that the\nconvergence rate of these methods can be preserved under a decreasing sequence\nof errors in the control variate, and use this to derive variants of SVRG that\nuse growing-batch strategies to reduce the number of gradient calculations\nrequired in the early iterations. We further (i) show how to exploit support\nvectors to reduce the number of gradient computations in the later iterations,\n(ii) prove that the commonly-used regularized SVRG iteration is justified and\nimproves the convergence rate, (iii) consider alternate mini-batch selection\nstrategies, and (iv) consider the generalization error of the method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 22:45:12 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Babanezhad", "Reza", ""], ["Ahmed", "Mohamed Osama", ""], ["Virani", "Alim", ""], ["Schmidt", "Mark", ""], ["Kone\u010dn\u00fd", "Jakub", ""], ["Sallinen", "Scott", ""]]}, {"id": "1511.02204", "submitter": "Rahul Mazumder", "authors": "Robert M. Freund and Paul Grigas and Rahul Mazumder", "title": "An Extended Frank-Wolfe Method with \"In-Face\" Directions, and its\n  Application to Low-Rank Matrix Completion", "comments": "25 pages, 3 tables and 2 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated principally by the low-rank matrix completion problem, we present\nan extension of the Frank-Wolfe method that is designed to induce near-optimal\nsolutions on low-dimensional faces of the feasible region. This is accomplished\nby a new approach to generating ``in-face\" directions at each iteration, as\nwell as through new choice rules for selecting between in-face and ``regular\"\nFrank-Wolfe steps. Our framework for generating in-face directions generalizes\nthe notion of away-steps introduced by Wolfe. In particular, the in-face\ndirections always keep the next iterate within the minimal face containing the\ncurrent iterate. We present computational guarantees for the new method that\ntrade off efficiency in computing near-optimal solutions with upper bounds on\nthe dimension of minimal faces of iterates. We apply the new method to the\nmatrix completion problem, where low-dimensional faces correspond to low-rank\nmatrices. We present computational results that demonstrate the effectiveness\nof our methodological approach at producing nearly-optimal solutions of very\nlow rank. On both artificial and real datasets, we demonstrate significant\nspeed-ups in computing very low-rank nearly-optimal solutions as compared to\neither the Frank-Wolfe method or its traditional away-step variant.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 19:31:52 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1511.02284", "submitter": "JInglai Li", "authors": "Tian Gao, Jinglai Li", "title": "A Derivative-Free Trust-Region Algorithm for Reliability-Based\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we present a derivative-free trust-region (TR) algorithm for\nreliability based optimization (RBO) problems. The proposed algorithm consists\nof solving a set of subproblems, in which simple surrogate models of the\nreliability constraints are constructed and used in solving the subproblems.\nTaking advantage of the special structure of the RBO problems, we employ a\nsample reweighting method to evaluate the failure probabilities, which\nconstructs the surrogate for the reliability constraints by performing only a\nsingle full reliability evaluation in each iteration. With numerical\nexperiments, we illustrate that the proposed algorithm is competitive against\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 02:20:32 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2016 02:43:18 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 12:06:46 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Gao", "Tian", ""], ["Li", "Jinglai", ""]]}, {"id": "1511.02386", "submitter": "Dustin Tran", "authors": "Rajesh Ranganath, Dustin Tran, David M. Blei", "title": "Hierarchical Variational Models", "comments": "Appears in International Conference on Machine Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box variational inference allows researchers to easily prototype and\nevaluate an array of models. Recent advances allow such algorithms to scale to\nhigh dimensions. However, a central question remains: How to specify an\nexpressive variational distribution that maintains efficient computation? To\naddress this, we develop hierarchical variational models (HVMs). HVMs augment a\nvariational approximation with a prior on its parameters, which allows it to\ncapture complex structure for both discrete and continuous latent variables.\nThe algorithm we develop is black box, can be used for any HVM, and has the\nsame computational efficiency as the original approximation. We study HVMs on a\nvariety of deep discrete latent variable models. HVMs generalize other\nexpressive variational distributions and maintains higher fidelity to the\nposterior.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 19:01:48 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 21:16:38 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ranganath", "Rajesh", ""], ["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1511.02543", "submitter": "Roger Grosse", "authors": "Roger B. Grosse, Zoubin Ghahramani, and Ryan P. Adams", "title": "Sandwiching the marginal likelihood using bidirectional Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the marginal likelihood (ML) of a model requires marginalizing out\nall of the parameters and latent variables, a difficult high-dimensional\nsummation or integration problem. To make matters worse, it is often hard to\nmeasure the accuracy of one's ML estimates. We present bidirectional Monte\nCarlo, a technique for obtaining accurate log-ML estimates on data simulated\nfrom a model. This method obtains stochastic lower bounds on the log-ML using\nannealed importance sampling or sequential Monte Carlo, and obtains stochastic\nupper bounds by running these same algorithms in reverse starting from an exact\nposterior sample. The true value can be sandwiched between these two stochastic\nbounds with high probability. Using the ground truth log-ML estimates obtained\nfrom our method, we quantitatively evaluate a wide variety of existing ML\nestimators on several latent variable models: clustering, a low rank\napproximation, and a binary attributes model. These experiments yield insights\ninto how to accurately estimate marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 23:55:36 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Grosse", "Roger B.", ""], ["Ghahramani", "Zoubin", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1511.02703", "submitter": "Jeroen Wouters", "authors": "Jeroen Wouters (Phys-ENS), Freddy Bouchet (Phys-ENS)", "title": "Rare event computation in deterministic chaotic systems using\n  genealogical particle analysis", "comments": null, "journal-ref": null, "doi": "10.1088/1751-8113/49/37/374002", "report-no": null, "categories": "cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the use of rare event computation techniques to\nestimate small over-threshold probabilities of observables in determin-istic\ndynamical systems. We demonstrate that the genealogical particle analysis\nalgorithms can be successfully applied to a toy model of atmospheric dynamics,\nthe Lorenz '96 model. We furthermore use the Ornstein-Uhlenbeck system to\nillustrate a number of implementation issues. We also show how a time-dependent\nobjective function based on the fluctuation path to a high threshold can\ngreatly improve the performance of the estimator compared to a fixed-in-time\nobjective function.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:47:00 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 08:53:59 GMT"}], "update_date": "2016-09-21", "authors_parsed": [["Wouters", "Jeroen", "", "Phys-ENS"], ["Bouchet", "Freddy", "", "Phys-ENS"]]}, {"id": "1511.02930", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa and Pavel N. Krivitsky and Aleksandra B. Slavkovi\\'c", "title": "Sharing Social Network Data: Differentially Private Estimation of\n  Exponential-Family Random Graph Models", "comments": "Updated, 39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CR cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a real-life problem of sharing social network data that contain\nsensitive personal information, we propose a novel approach to release and\nanalyze synthetic graphs in order to protect privacy of individual\nrelationships captured by the social network while maintaining the validity of\nstatistical results. A case study using a version of the Enron e-mail corpus\ndataset demonstrates the application and usefulness of the proposed techniques\nin solving the challenging problem of maintaining privacy \\emph{and} supporting\nopen access to network data to ensure reproducibility of existing studies and\ndiscovering new scientific insights that can be obtained by analyzing such\ndata. We use a simple yet effective randomized response mechanism to generate\nsynthetic networks under $\\epsilon$-edge differential privacy, and then use\nlikelihood based inference for missing data and Markov chain Monte Carlo\ntechniques to fit exponential-family random graph models to the generated\nsynthetic networks.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 23:36:30 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 16:48:20 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Karwa", "Vishesh", ""], ["Krivitsky", "Pavel N.", ""], ["Slavkovi\u0107", "Aleksandra B.", ""]]}, {"id": "1511.03046", "submitter": "Francois Bachoc", "authors": "Fran\\c{c}ois Bachoc (IMT, GdR MASCOT-NUM), Jean-Marc Martinez (MoVe),\n  Karim Ammar (LPEC)", "title": "Improvement of code behaviour in a design of experiments by metamodeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now common practice in nuclear engineering to base extensive studies on\nnumerical computer models. These studies require to run computer codes in\npotentially thousands of numerical configurations and without expert individual\ncontrols on the computational and physical aspects of each simulations.In this\npaper, we compare different statistical metamodeling techniques and show how\nmetamodels can help to improve the global behaviour of codes in these extensive\nstudies. We consider the metamodeling of the Germinal thermalmechanical code by\nKriging, kernel regression and neural networks. Kriging provides the most\naccurate predictions while neural networks yield the fastest metamodel\nfunctions. All three metamodels can conveniently detect strong computation\nfailures. It is however significantly more challenging to detect code\ninstabilities, that is groups of computations that are all valid, but\nnumerically inconsistent with one another. For code instability detection, we\nfind that Kriging provides the most useful tools.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 10:13:45 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT, GdR MASCOT-NUM"], ["Martinez", "Jean-Marc", "", "MoVe"], ["Ammar", "Karim", "", "LPEC"]]}, {"id": "1511.03095", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Luca Martino, David Luengo, M\\'onica F. Bugallo", "title": "Generalized Multiple Importance Sampling", "comments": null, "journal-ref": "Statistical Science, Volume 34, Number 1 (2019), 129-155", "doi": "10.1214/18-STS668", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance Sampling methods are broadly used to approximate posterior\ndistributions or some of their moments. In its standard approach, samples are\ndrawn from a single proposal distribution and weighted properly. However, since\nthe performance depends on the mismatch between the targeted and the proposal\ndistributions, several proposal densities are often employed for the generation\nof samples. Under this Multiple Importance Sampling (MIS) scenario, many works\nhave addressed the selection or adaptation of the proposal distributions,\ninterpreting the sampling and the weighting steps in different ways. In this\npaper, we establish a general framework for sampling and weighing procedures\nwhen more than one proposal are available. The most relevant MIS schemes in the\nliterature are encompassed within the new framework, and, moreover novel valid\nschemes appear naturally. All the MIS schemes are compared and ranked in terms\nof the variance of the associated estimators. Finally, we provide illustrative\nexamples which reveal that, even with a good choice of the proposal densities,\na careful interpretation of the sampling and weighting procedures can make a\nsignificant difference in the performance of the method.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 13:11:07 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 12:50:38 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 20:07:09 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""], ["Luengo", "David", ""], ["Bugallo", "M\u00f3nica F.", ""]]}, {"id": "1511.03145", "submitter": "Clara Grazian", "authors": "Clara Grazian and Christian Robert", "title": "Jeffreys priors for mixture estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Jeffreys priors usually are well-defined for the parameters of mixtures\nof distributions, they are not available in closed form. Furthermore, they\noften are improper priors. Hence, they have never been used to draw inference\non the mixture parameters. We study in this paper the implementation and the\nproperties of Jeffreys priors in several mixture settings, show that the\nassociated posterior distributions most often are improper, and then propose a\nnoninformative alternative for the analysis of mixtures.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 15:35:30 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 16:43:54 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Grazian", "Clara", ""], ["Robert", "Christian", ""]]}, {"id": "1511.03463", "submitter": "Dimitris Kugiumtzis", "authors": "Elsa Siggiridou and Dimitris Kugiumtzis", "title": "Granger Causality in Multi-variate Time Series using a Time Ordered\n  Restricted Vector Autoregressive Model", "comments": "15 pages, 4 tables, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2500893", "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality has been used for the investigation of the inter-dependence\nstructure of the underlying systems of multi-variate time series. In\nparticular, the direct causal effects are commonly estimated by the conditional\nGranger causality index (CGCI). In the presence of many observed variables and\nrelatively short time series, CGCI may fail because it is based on vector\nautoregressive models (VAR) involving a large number of coefficients to be\nestimated. In this work, the VAR is restricted by a scheme that modifies the\nrecently developed method of backward-in-time selection (BTS) of the lagged\nvariables and the CGCI is combined with BTS. Further, the proposed approach is\ncompared favorably to other restricted VAR representations, such as the\ntop-down strategy, the bottom-up strategy, and the least absolute shrinkage and\nselection operator (LASSO), in terms of sensitivity and specificity of CGCI.\nThis is shown by using simulations of linear and nonlinear, low and\nhigh-dimensional systems and different time series lengths. For nonlinear\nsystems, CGCI from the restricted VAR representations are compared with\nanalogous nonlinear causality indices. Further, CGCI in conjunction with BTS\nand other restricted VAR representations is applied to multi-channel scalp\nelectroencephalogram (EEG) recordings of epileptic patients containing\nepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,\ncould track the changes in brain connectivity before, during and after\nepileptiform discharges, which was not possible using the full VAR\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:35:21 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1511.03475", "submitter": "Richard Wilkinson", "authors": "Philip B. Holden, Neil R. Edwards, James Hensman, Richard D. Wilkinson", "title": "ABC for climate: dealing with expensive simulators", "comments": "To appear in the forthcoming Handbook of Approximate Bayesian\n  Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is due to appear as a chapter of the forthcoming Handbook of\nApproximate Bayesian Computation (ABC) by S. Sisson, L. Fan, and M. Beaumont.\nWe describe the challenge of calibrating climate simulators, and discuss the\ndifferences in emphasis in climate science compared to many of the more\ntraditional ABC application areas. The primary difficulty is how to do\ninference with a computationally expensive simulator which we can only afford\nto run a small number of times, and we describe how Gaussian process emulators\nare used as surrogate models in this case. We introduce the idea of history\nmatching, which is a non-probabilistic calibration method, which divides the\nparameter space into (not im)plausible and implausible regions. History\nmatching can be shown to be a special case of ABC, but with a greater emphasis\non defining realistic simulator discrepancy bounds, and using these to define\ntolerances and metrics. We describe a design approach for choosing parameter\nvalues at which to run the simulator, and illustrate the approach on a toy\nclimate model, showing that with careful design we can find the plausible\nregion with a very small number of model evaluations. Finally, we describe how\ncalibrated GENIE-1 (an earth system model of intermediate complexity)\npredictions have been used, and why it is important to accurately characterise\nparametric uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 12:26:35 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Holden", "Philip B.", ""], ["Edwards", "Neil R.", ""], ["Hensman", "James", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "1511.03895", "submitter": "Pau Closas", "authors": "Pau Closas and Antoni Guillamon", "title": "Sequential estimation of intrinsic activity and synaptic input in single\n  neurons by particle filtering with optimal importance density", "comments": "Submitted for publication in the Special Issue on Advanced Signal\n  Processing in Brain Networks of the IEEE Journal on Selected Topics in Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of inferring the signals and parameters\nthat cause neural activity to occur. The ultimate challenge being to unveil\nbrain's connectivity, here we focus on a microscopic vision of the problem,\nwhere single neurons (potentially connected to a network of peers) are at the\ncore of our study. The sole observation available are noisy, sampled voltage\ntraces obtained from intracellular recordings. We design algorithms and\ninference methods using the tools provided by stochastic filtering, that allow\na probabilistic interpretation and treatment of the problem. Using particle\nfiltering we are able to reconstruct traces of voltages and estimate the time\ncourse of auxiliary variables. By extending the algorithm, through PMCMC\nmethodology, we are able to estimate hidden physiological parameters as well,\nlike intrinsic conductances or reversal potentials. Last, but not least, the\nmethod is applied to estimate synaptic conductances arriving at a target cell,\nthus reconstructing the synaptic excitatory/inhibitory input traces. Notably,\nthese estimations have a bound-achieving performance even in spiking regimes.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 13:47:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Closas", "Pau", ""], ["Guillamon", "Antoni", ""]]}, {"id": "1511.03977", "submitter": "Fabian Dunker", "authors": "Fabian Dunker", "title": "Nonparametric instrumental variable regression and quantile regression\n  with full independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of endogeneity in statistics and econometrics is often handled by\nintroducing instrumental variables (IV) which are assumed to be mean\nindependent of some regressors or other observables. When full independence of\nIV's and observables is assumed, nonparametric IV regression models and\nnonparametric demand models lead to nonlinear integral equations with unknown\nintegral kernels. We prove convergence rates for the mean integrated square\nerror of the iteratively regularized Newton method applied to these problems.\nCompared to related results we derive stronger convergence results that rely on\nweaker nonlinearity restrictions. We demonstrate in numerical simulations for a\nnonparametric IV regression that the method produces better results than the\nstandard model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:26:47 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 23:42:46 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Dunker", "Fabian", ""]]}, {"id": "1511.04220", "submitter": "Leonidas Pitsoulis", "authors": "G. Zioutas, C. Chatzinakos, T.D. Nguyen and L. Pitsoulis", "title": "Optimization techniques for multivariate least trimmed absolute\n  deviation estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset an outlier can be defined as an observation that it is\nunlikely to follow the statistical properties of the majority of the data.\nComputation of the location estimate of is fundamental in data analysis, and it\nis well known in statistics that classical methods, such as taking the sample\naverage, can be greatly affected by the presence of outliers in the data. Using\nthe median instead of the mean can partially resolve this issue but not\ncompletely. For the univariate case, a robust version of the median is the\nLeast Trimmed Absolute Deviation (LTAD) robust estimator introduced\nin~\\cite{Tableman1994}, which has desirable asymptotic properties such as\nrobustness, consistently, high breakdown and normality. There are different\ngeneralizations of the LTAD for multivariate data, depending on the choice of\nnorm. In~\\cite{ChaPitZiou:2015} we present such a generalization using the\nEuclidean norm and propose a solution technique for the resulting combinatorial\noptimization problem, based on a necessary condition, that results in a highly\nconvergent local search algorithm. In this subsequent work we use the $L^1$\nnorm to generalize the LTAD to higher dimensions, and show that the resulting\nmixed integer programming problem has an integral relaxation, after applying an\nappropriate data transformation. Moreover, we utilize the structure of the\nproblem to show that the resulting LP's can be solved efficiently using a\nsubgradient optimization approach. The robust statistical properties of the\nproposed estimator are verified by extensive computational results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 10:09:44 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Zioutas", "G.", ""], ["Chatzinakos", "C.", ""], ["Nguyen", "T. D.", ""], ["Pitsoulis", "L.", ""]]}, {"id": "1511.04334", "submitter": "Peter Neal Dr", "authors": "Peter Neal and Clement Lee", "title": "Optimal scaling of the independence sampler: Theory and Practice", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independence sampler is one of the most commonly used MCMC algorithms\nusually as a component of a Metropolis-within-Gibbs algorithm. The common focus\nfor the independence sampler is on the choice of proposal distribution to\nobtain an as high as possible acceptance rate. In this paper we have a somewhat\ndifferent focus concentrating on the use of the independence sampler for\nupdating augmented data in a Bayesian framework where a natural proposal\ndistribution for the independence sampler exists. Thus we concentrate on the\nproportion of the augmented data to update to optimise the independence\nsampler. Generic guidelines for optimising the independence sampler are\nobtained for independent and identically distributed product densities\nmirroring findings for the random walk Metropolis algorithm. The generic\nguidelines are shown to be informative beyond the narrow confines of idealised\nproduct densities in two epidemic examples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:06:16 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2016 10:42:34 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Neal", "Peter", ""], ["Lee", "Clement", ""]]}, {"id": "1511.04485", "submitter": "St\\'ephane Guerrier", "authors": "Stephane Guerrier and Maria-Pia Victoria-Feser", "title": "A Prediction Divergence Criterion for Model Selection", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of model selection is inevitable in an increasingly large number\nof applications involving partial theoretical knowledge and vast amounts of\ninformation, like in medicine, biology or economics. The associated techniques\nare intended to determine which variables are \"important\" to \"explain a\nphenomenon under investigation. The terms \"important\" and \"explain\" can have\nvery different meanings according to the context and, in fact, model selection\ncan be applied to any situation where one tries to balance variability with\ncomplexity. In this paper, we introduce a new class of error measures and of\nmodel selection criteria, to which many well know selection criteria belong.\nMoreover, this class enables us to derive a novel criterion, based on a\ndivergence measure between the predictions produced by two nested models,\ncalled the Prediction Divergence Criterion (PDC). Our selection procedure is\ndeveloped for linear regression models, but has the potential to be extended to\nother models. We demonstrate that, under some regularity conditions, it is\nasymptotically loss efficient and can also be consistent. In the linear case,\nthe PDC is a counterpart to Mallow's Cp but with a lower asymptotic probability\nof overfitting. In a case study and by means of simulations, the PDC is shown\nto be particularly well suited in \"sparse\" settings with correlated covariates\nwhich we believe to be common in real applications.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 00:45:23 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Guerrier", "Stephane", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1511.04992", "submitter": "George Deligiannidis", "authors": "George Deligiannidis, Arnaud Doucet, Michael K. Pitt", "title": "The Correlated Pseudo-Marginal Method", "comments": "78 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo-marginal algorithm is a popular variant of the\nMetropolis--Hastings scheme which allows us to sample asymptotically from a\ntarget probability density $\\pi$, when we are only able to estimate an\nunnormalized version of $\\pi$ pointwise unbiasedly. It has found numerous\napplications in Bayesian statistics as there are many scenarios where the\nlikelihood function is intractable but can be estimated unbiasedly using Monte\nCarlo samples. Using many samples will typically result in averages computed\nunder this chain with lower asymptotic variances than the corresponding\naverages that use fewer samples. For a fixed computing time, it has been shown\nin several recent contributions that an efficient implementation of the\npseudo-marginal method requires the variance of the log-likelihood ratio\nestimator appearing in the acceptance probability of the algorithm to be of\norder 1, which in turn usually requires scaling the number $N$ of Monte Carlo\nsamples linearly with the number $T$ of data points. We propose a modification\nof the pseudo-marginal algorithm, termed the correlated pseudo-marginal\nalgorithm, which is based on a novel log-likelihood ratio estimator computed\nusing the difference of two positively correlated log-likelihood estimators. We\nshow that the parameters of this scheme can be selected such that the variance\nof this estimator is order $1$ as $N,T\\rightarrow\\infty$ whenever\n$N/T\\rightarrow 0$. By combining these results with the Bernstein-von Mises\ntheorem, we provide an analysis of the performance of the correlated\npseudo-marginal algorithm in the large $T$ regime. In our numerical examples,\nthe efficiency of computations is increased relative to the standard\npseudo-marginal algorithm by more than 20 fold for values of $T$ of a few\nhundreds to more than 100 fold for values of $T$ of around 10,000-20,000.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 15:41:57 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 15:52:14 GMT"}, {"version": "v3", "created": "Fri, 8 Jul 2016 10:29:36 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 12:19:33 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""], ["Pitt", "Michael K.", ""]]}, {"id": "1511.05309", "submitter": "Yehezkel Resheff", "authors": "Yehezkel S. Resheff, Daphna Weinshall", "title": "Optimized Linear Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in real-world datasets, especially in high dimensional data, some\nfeature values are missing. Since most data analysis and statistical methods do\nnot handle gracefully missing values, the first step in the analysis requires\nthe imputation of missing values. Indeed, there has been a long standing\ninterest in methods for the imputation of missing values as a pre-processing\nstep. One recent and effective approach, the IRMI stepwise regression\nimputation method, uses a linear regression model for each real-valued feature\non the basis of all other features in the dataset. However, the proposed\niterative formulation lacks convergence guarantee. Here we propose a closely\nrelated method, stated as a single optimization problem and a block\ncoordinate-descent solution which is guaranteed to converge to a local minimum.\nExperiments show results on both synthetic and benchmark datasets, which are\ncomparable to the results of the IRMI method whenever it converges. However,\nwhile in the set of experiments described here IRMI often does not converge,\nthe performance of our methods is shown to be markedly superior in comparison\nwith other methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 08:26:40 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2016 17:46:18 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 13:28:52 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Resheff", "Yehezkel S.", ""], ["Weinshall", "Daphna", ""]]}, {"id": "1511.05355", "submitter": "Juan A. Cuesta-Albertos", "authors": "Pedro C. \\'Alvarez-Esteban, E. del Barrio, J.A. Cuesta-Albertos and C.\n  Matr\\'an", "title": "A fixed-point approach to barycenters in Wasserstein space", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{P}_{2,ac}$ be the set of Borel probabilities on $\\mathbb{R}^d$\nwith finite second moment and absolutely continuous with respect to Lebesgue\nmeasure. We consider the problem of finding the barycenter (or Fr\\'echet mean)\nof a finite set of probabilities $\\nu_1,\\ldots,\\nu_k \\in \\mathcal{P}_{2,ac}$\nwith respect to the $L_2-$Wasserstein metric. For this task we introduce an\noperator on $\\mathcal{P}_{2,ac}$ related to the optimal transport maps pushing\nforward any $\\mu \\in \\mathcal{P}_{2,ac}$ to $\\nu_1,\\ldots,\\nu_k$. Under very\ngeneral conditions we prove that the barycenter must be a fixed point for this\noperator and introduce an iterative procedure which consistently approximates\nthe barycenter. The procedure allows effective computation of barycenters in\nany location-scatter family, including the Gaussian case. In such cases the\nbarycenter must belong to the family, thus it is characterized by its mean and\ncovariance matrix. While its mean is just the weighted mean of the means of the\nprobabilities, the covariance matrix is characterized in terms of their\ncovariance matrices $\\Sigma_1,\\dots,\\Sigma_k$ through a nonlinear matrix\nequation. The performance of the iterative procedure in this case is\nillustrated through numerical simulations, which show fast convergence towards\nthe barycenter.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 11:41:02 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 10:17:43 GMT"}, {"version": "v3", "created": "Fri, 22 Apr 2016 06:46:34 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["\u00c1lvarez-Esteban", "Pedro C.", ""], ["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matr\u00e1n", "C.", ""]]}, {"id": "1511.05464", "submitter": "Joseph  Salmon", "authors": "Igor Colin and Aur\\'elien Bellet and Joseph Salmon and St\\'ephan\n  Cl\\'emen\\c{c}on", "title": "Extending Gossip Algorithms to Distributed Estimation of U-Statistics", "comments": "to be presented at NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and robust algorithms for decentralized estimation in networks are\nessential to many distributed systems. Whereas distributed estimation of sample\nmean statistics has been the subject of a good deal of attention, computation\nof $U$-statistics, relying on more expensive averaging over pairs of\nobservations, is a less investigated area. Yet, such data functionals are\nessential to describe global properties of a statistical population, with\nimportant examples including Area Under the Curve, empirical variance, Gini\nmean difference and within-cluster point scatter. This paper proposes new\nsynchronous and asynchronous randomized gossip algorithms which simultaneously\npropagate data across the network and maintain local estimates of the\n$U$-statistic of interest. We establish convergence rate bounds of $O(1/t)$ and\n$O(\\log t / t)$ for the synchronous and asynchronous cases respectively, where\n$t$ is the number of iterations, with explicit data and network dependent\nterms. Beyond favorable comparisons in terms of rate analysis, numerical\nexperiments provide empirical evidence the proposed algorithms surpasses the\npreviously introduced approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 16:49:52 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Colin", "Igor", ""], ["Bellet", "Aur\u00e9lien", ""], ["Salmon", "Joseph", ""], ["Cl\u00e9men\u00e7on", "St\u00e9phan", ""]]}, {"id": "1511.05483", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin, Fredrik Lindsten, Joel Kronander and Thomas B. Sch\\\"on", "title": "Accelerating pseudo-marginal Metropolis-Hastings by correlating\n  auxiliary variables", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-marginal Metropolis-Hastings (pmMH) is a powerful method for Bayesian\ninference in models where the posterior distribution is analytical intractable\nor computationally costly to evaluate directly. It operates by introducing\nadditional auxiliary variables into the model and form an extended target\ndistribution, which then can be evaluated point-wise. In many cases, the\nstandard Metropolis-Hastings is then applied to sample from the extended target\nand the sought posterior can be obtained by marginalisation. However, in some\nimplementations this approach suffers from poor mixing as the auxiliary\nvariables are sampled from an independent proposal. We propose a modification\nto the pmMH algorithm in which a Crank-Nicolson (CN) proposal is used instead.\nThis results in that we introduce a positive correlation in the auxiliary\nvariables. We investigate how to tune the CN proposal and its impact on the\nmixing of the resulting pmMH sampler. The conclusion is that the proposed\nmodification can have a beneficial effect on both the mixing of the Markov\nchain and the computational cost for each iteration of the pmMH algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:35:57 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Kronander", "Joel", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1511.05604", "submitter": "Edgar Merkle", "authors": "Edgar C. Merkle and Yves Rosseel", "title": "blavaan: Bayesian structural equation models via parameter expansion", "comments": null, "journal-ref": "Journal of Statistical Software (2018), 85(4), 1-30", "doi": "10.18637/jss.v085.i04", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes blavaan, an R package for estimating Bayesian\nstructural equation models (SEMs) via JAGS and for summarizing the results. It\nalso describes a novel parameter expansion approach for estimating specific\ntypes of models with residual covariances, which facilitates estimation of\nthese models in JAGS. The methodology and software are intended to provide\nusers with a general means of estimating Bayesian SEMs, both classical and\nnovel, in a straightforward fashion. Users can estimate Bayesian versions of\nclassical SEMs with lavaan syntax, they can obtain state-of-the-art Bayesian\nfit measures associated with the models, and they can export JAGS code to\nmodify the SEMs as desired. These features and more are illustrated by example,\nand the parameter expansion approach is explained in detail.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 22:18:44 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 17:13:32 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Merkle", "Edgar C.", ""], ["Rosseel", "Yves", ""]]}, {"id": "1511.05838", "submitter": "JInglai Li", "authors": "Zixi Hu, Zhewei Yao, Jinglai Li", "title": "On an adaptive preconditioned Crank-Nicolson MCMC algorithm for infinite\n  dimensional Bayesian inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and engineering problems require to perform Bayesian\ninferences for unknowns of infinite dimension. In such problems, many standard\nMarkov Chain Monte Carlo (MCMC) algorithms become arbitrary slow under the mesh\nrefinement, which is referred to as being dimension dependent. To this end, a\nfamily of dimensional independent MCMC algorithms, known as the preconditioned\nCrank-Nicolson (pCN) methods, were proposed to sample the infinite dimensional\nparameters. In this work we develop an adaptive version of the pCN algorithm,\nwhere the covariance operator of the proposal distribution is adjusted based on\nsampling history to improve the simulation efficiency. We show that the\nproposed algorithm satisfies an important ergodicity condition under some mild\nassumptions. Finally we provide numerical examples to demonstrate the\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:39:48 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 08:25:07 GMT"}, {"version": "v3", "created": "Fri, 1 Apr 2016 14:58:03 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Hu", "Zixi", ""], ["Yao", "Zhewei", ""], ["Li", "Jinglai", ""]]}, {"id": "1511.06196", "submitter": "Daniel  Sanz-Alonso", "authors": "S. Agapiou, O. Papaspiliopoulos, D. Sanz-Alonso, A. M. Stuart", "title": "Importance Sampling: Intrinsic Dimension and Computational Cost", "comments": "Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The basic idea of importance sampling is to use independent samples from a\nproposal measure in order to approximate expectations with respect to a target\nmeasure. It is key to understand how many samples are required in order to\nguarantee accurate approximations. Intuitively, some notion of distance between\nthe target and the proposal should determine the computational cost of the\nmethod. A major challenge is to quantify this distance in terms of parameters\nor statistics that are pertinent for the practitioner. The subject has\nattracted substantial interest from within a variety of communities. The\nobjective of this paper is to overview and unify the resulting literature by\ncreating an overarching framework. A general theory is presented, with a focus\non the use of importance sampling in Bayesian inverse problems and filtering.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:06:53 GMT"}, {"version": "v2", "created": "Sun, 8 Jan 2017 10:51:25 GMT"}, {"version": "v3", "created": "Sat, 14 Jan 2017 21:03:09 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Agapiou", "S.", ""], ["Papaspiliopoulos", "O.", ""], ["Sanz-Alonso", "D.", ""], ["Stuart", "A. M.", ""]]}, {"id": "1511.06286", "submitter": "Anthony Lee", "authors": "Pieralberto Guarniero, Adam M. Johansen and Anthony Lee", "title": "The iterated auxiliary particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an offline, iterated particle filter to facilitate statistical\ninference in general state space hidden Markov models. Given a model and a\nsequence of observations, the associated marginal likelihood L is central to\nlikelihood-based inference for unknown statistical parameters. We define a\nclass of \"twisted\" models: each member is specified by a sequence of positive\nfunctions psi and has an associated psi-auxiliary particle filter that provides\nunbiased estimates of L. We identify a sequence psi* that is optimal in the\nsense that the psi*-auxiliary particle filter's estimate of L has zero\nvariance. In practical applications, psi* is unknown so the psi*-auxiliary\nparticle filter cannot straightforwardly be implemented. We use an iterative\nscheme to approximate psi*, and demonstrate empirically that the resulting\niterated auxiliary particle filter significantly outperforms the bootstrap\nparticle filter in challenging settings. Applications include parameter\nestimation using a particle Markov chain Monte Carlo algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 18:09:13 GMT"}, {"version": "v2", "created": "Wed, 15 Jun 2016 19:14:57 GMT"}], "update_date": "2016-06-16", "authors_parsed": [["Guarniero", "Pieralberto", ""], ["Johansen", "Adam M.", ""], ["Lee", "Anthony", ""]]}, {"id": "1511.06499", "submitter": "Dustin Tran", "authors": "Dustin Tran, Rajesh Ranganath, David M. Blei", "title": "The Variational Gaussian Process", "comments": "Appears in International Conference on Learning Representations, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful tool for approximate inference, and it\nhas been recently applied for representation learning with deep generative\nmodels. We develop the variational Gaussian process (VGP), a Bayesian\nnonparametric variational family, which adapts its shape to match complex\nposterior distributions. The VGP generates approximate posterior samples by\ngenerating latent inputs and warping them through random non-linear mappings;\nthe distribution over random mappings is learned during inference, enabling the\ntransformed outputs to adapt to varying complexity. We prove a universal\napproximation theorem for the VGP, demonstrating its representative power for\nlearning any model. For inference we present a variational objective inspired\nby auto-encoders and perform black box inference over a wide class of models.\nThe VGP achieves new state-of-the-art results for unsupervised learning,\ninferring models such as the deep latent Gaussian model and the recently\nproposed DRAW.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 06:01:23 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:56:01 GMT"}, {"version": "v3", "created": "Wed, 23 Mar 2016 23:11:38 GMT"}, {"version": "v4", "created": "Sun, 17 Apr 2016 22:14:13 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1511.06925", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura, David Dunson", "title": "Recycling intermediate steps to improve Hamiltonian Monte Carlo", "comments": "22 pages, 16 figures (+ Supplement 6 pages, 4 figures)", "journal-ref": "Bayesian Analysis (2020)", "doi": "10.1214/19-BA1171", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) and related algorithms have become routinely\nused in Bayesian computation. In this article, we present a simple and provably\naccurate method to improve the efficiency of HMC and related algorithms with\nessentially no extra computational cost. This is achieved by {recycling the\nintermediate states along simulated trajectories of Hamiltonian dynamics.\nStandard algorithms use only the end points of trajectories, wastefully\ndiscarding all the intermediate states. Compared to the alternative methods for\nutilizing the intermediate states, our algorithm is simpler to apply in\npractice and requires little programming effort beyond the usual\nimplementations of HMC and related algorithms. Our algorithm applies\nstraightforwardly to the no-U-turn sampler, arguably the most popular variant\nof HMC. Through a variety of experiments, we demonstrate that our recycling\nalgorithm yields substantial computational efficiency gains.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 21:08:09 GMT"}, {"version": "v2", "created": "Sun, 19 Feb 2017 03:01:34 GMT"}, {"version": "v3", "created": "Tue, 2 Jul 2019 04:13:03 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Dunson", "David", ""]]}, {"id": "1511.07304", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Luke Bornn", "title": "Convergence Results for a Class of Time-Varying Simulated Annealing\n  Algorithms", "comments": "25 pages (final version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a set of conditions which ensure the almost sure convergence of a\nclass of simulated annealing algorithms on a bounded set\n$\\mathcal{X}\\subset\\mathbb{R}^d$ based on a time-varying Markov kernel. The\nclass of algorithms considered in this work encompasses the one studied in\nBelisle (1992) and Yang (2000) as well as its derandomized version recently\nproposed by Gerber and Bornn (2016). To the best of our knowledge, the results\nwe derive are the first examples of almost sure convergence results for\nsimulated annealing based on a time-varying kernel. In addition, the\nassumptions on the Markov kernel and on the cooling schedule have the advantage\nof being trivial to verify in practice.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 16:40:36 GMT"}, {"version": "v2", "created": "Mon, 5 Sep 2016 10:13:59 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 23:05:12 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Gerber", "Mathieu", ""], ["Bornn", "Luke", ""]]}, {"id": "1511.07482", "submitter": "Artur Gramacki", "authors": "Artur Gramacki and Jaros{\\l}aw Gramacki", "title": "FFT-Based Fast Bandwidth Selector for Multivariate Kernel Density\n  Estimation", "comments": "35 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of multivariate kernel density estimation (KDE) depends\nstrongly on the choice of bandwidth matrix. The high computational cost\nrequired for its estimation provides a big motivation to develop fast and\naccurate methods. One of such methods is based on the Fast Fourier Transform.\nHowever, the currently available implementation works very well only for the\nunivariate KDE and it's multivariate extension suffers from a very serious\nlimitation as it can accurately operate only with diagonal bandwidth matrices.\nA more general solution is presented where the above mentioned limitation is\nrelaxed. Moreover, the presented solution can by easily adopted also for the\ntask of efficient computation of integrated density derivative functionals\ninvolving an arbitrary derivative order. Consequently, bandwidth selection for\nkernel density derivative estimation is also supported. The practical usability\nof the new solution is demonstrated by comprehensive numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 22:00:53 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 07:43:49 GMT"}, {"version": "v3", "created": "Thu, 12 May 2016 15:48:40 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Gramacki", "Artur", ""], ["Gramacki", "Jaros\u0142aw", ""]]}, {"id": "1511.07492", "submitter": "Bruno Sudret", "authors": "Katerina Konakli and Bruno Sudret", "title": "Polynomial meta-models with canonical low-rank approximations: numerical\n  insights and comparison to sparse polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.06.005", "report-no": "RSUQ-2015-007", "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need for uncertainty analysis of complex computational models has\nled to an expanding use of meta-models across engineering and sciences. The\nefficiency of meta-modeling techniques relies on their ability to provide\nstatistically-equivalent analytical representations based on relatively few\nevaluations of the original model. Polynomial chaos expansions (PCE) have\nproven a powerful tool for developing meta-models in a wide range of\napplications; the key idea thereof is to expand the model response onto a basis\nmade of multivariate polynomials obtained as tensor products of appropriate\nunivariate polynomials. The classical PCE approach nevertheless faces the\n\"curse of dimensionality\", namely the exponential increase of the basis size\nwith increasing input dimension. To address this limitation, the sparse PCE\ntechnique has been proposed, in which the expansion is carried out on only a\nfew relevant basis terms that are automatically selected by a suitable\nalgorithm. An alternative for developing meta-models with polynomial functions\nin high-dimensional problems is offered by the newly emerged low-rank\napproximations (LRA) approach. By exploiting the tensor-product structure of\nthe multivariate basis, LRA can provide polynomial representations in highly\ncompressed formats. Through extensive numerical investigations, we herein first\nshed light on issues relating to the construction of canonical LRA with a\nparticular greedy algorithm involving a sequential updating of the polynomial\ncoefficients along separate dimensions. Canonical LRA exhibit smaller errors\nthan sparse PCE in cases when the number of available model evaluations is\nsmall with respect to the input dimension. By introducing the conditional\ngeneralization error, we further demonstrate that canonical LRA tend to\noutperform sparse PCE in the prediction of extreme model responses.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 22:21:42 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2016 11:37:03 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Konakli", "Katerina", ""], ["Sudret", "Bruno", ""]]}, {"id": "1511.07837", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu and Xiaojun Chen", "title": "Generalized Conjugate Gradient Methods for $\\ell_1$ Regularized Convex\n  Quadratic Programming with Finite Convergence", "comments": "36 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conjugate gradient (CG) method is an efficient iterative method for\nsolving large-scale strongly convex quadratic programming (QP). In this paper\nwe propose some generalized CG (GCG) methods for solving the\n$\\ell_1$-regularized (possibly not strongly) convex QP that terminate at an\noptimal solution in a finite number of iterations. At each iteration, our\nmethods first identify a face of an orthant and then either perform an exact\nline search along the direction of the negative projected minimum-norm\nsubgradient of the objective function or execute a CG subroutine that conducts\na sequence of CG iterations until a CG iterate crosses the boundary of this\nface or an approximate minimizer of over this face or a subface is found. We\ndetermine which type of step should be taken by comparing the magnitude of some\ncomponents of the minimum-norm subgradient of the objective function to that of\nits rest components. Our analysis on finite convergence of these methods makes\nuse of an error bound result and some key properties of the aforementioned\nexact line search and the CG subroutine. We also show that the proposed methods\nare capable of finding an approximate solution of the problem by allowing some\ninexactness on the execution of the CG subroutine. The overall arithmetic\noperation cost of our GCG methods for finding an $\\epsilon$-optimal solution\ndepends on $\\epsilon$ in $O(\\log(1/\\epsilon))$, which is superior to the\naccelerated proximal gradient method [2,23] that depends on $\\epsilon$ in\n$O(1/\\sqrt{\\epsilon})$. In addition, our GCG methods can be extended\nstraightforwardly to solve box-constrained convex QP with finite convergence.\nNumerical results demonstrate that our methods are very favorable for solving\nill-conditioned problems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 19:28:09 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 22:16:30 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 19:23:49 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Lu", "Zhaosong", ""], ["Chen", "Xiaojun", ""]]}, {"id": "1511.08528", "submitter": "Ming Gu", "authors": "Christopher Melgaard, Ming Gu", "title": "Gaussian Elimination with Randomized Complete Pivoting", "comments": "5 figures, 33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian elimination with partial pivoting (GEPP) has long been among the\nmost widely used methods for computing the LU factorization of a given matrix.\nHowever, this method is also known to fail for matrices that induce large\nelement growth during the factorization process. In this paper, we propose a\nnew scheme, Gaussian elimination with randomized complete pivoting (GERCP) for\nthe efficient and reliable LU factorization of a given matrix. GERCP satisfies\nGECP (Gaussian elimination with complete pivoting) style element growth bounds\nwith high probability, yet costs only marginally higher than GEPP. Our\nnumerical experimental results strongly suggest that GERCP is as reliable as\nGECP and as efficient as GEPP for computing the LU factorization.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 23:25:44 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Melgaard", "Christopher", ""], ["Gu", "Ming", ""]]}, {"id": "1511.09123", "submitter": "Ka-Chun Wong", "authors": "Ka-Chun Wong", "title": "A Short Survey on Data Clustering Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CV cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly increasing data, clustering algorithms are important tools for\ndata analytics in modern research. They have been successfully applied to a\nwide range of domains; for instance, bioinformatics, speech recognition, and\nfinancial analysis. Formally speaking, given a set of data instances, a\nclustering algorithm is expected to divide the set of data instances into the\nsubsets which maximize the intra-subset similarity and inter-subset\ndissimilarity, where a similarity measure is defined beforehand. In this work,\nthe state-of-the-arts clustering algorithms are reviewed from design concept to\nmethodology; Different clustering paradigms are discussed. Advanced clustering\nalgorithms are also discussed. After that, the existing clustering evaluation\nmetrics are reviewed. A summary with future insights is provided at the end.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 08:02:37 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Wong", "Ka-Chun", ""]]}]