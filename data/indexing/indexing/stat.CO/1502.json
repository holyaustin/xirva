[{"id": "1502.00239", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, I. V. Tchervensky, V. S. Dimitrov, M. P. Mintchev", "title": "Optimal Wavelets for Electrogastrography", "comments": "6 pages, 4 figures, 2 tables, corrected Eq. (3)", "journal-ref": "26th Annual International Conference of the IEEE Engineering in\n  Medicine and Biology Society (IEMBS), vol. 1, pp. 329--332, 2004", "doi": "10.1109/IEMBS.2004.1403159", "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching a wavelet to class of signals can be of interest in feature\ndetection and classification based on wavelet representation. The aim of this\nwork is to provide a quantitative approach to the problem of matching a wavelet\nto electrogastrographic (EGG) signals. Visually inspected EGG recordings from\nsixteen dogs and six volunteers were submitted to wavelet analysis.\nApproximated wavelet-based versions of EGG signals were calculated using Pollen\nparameterization of 6-tap wavelet filters and wavelet compression techniques.\nWavelet parameterization values that minimize the approximation error of\ncompressed EGG signals were sought and considered optimal. The wavelets\ngenerated from the optimal parameterization values were remarkably similar to\nthe standard Daubechies-3 wavelet.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 12:24:26 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Cintra", "R. J.", ""], ["Tchervensky", "I. V.", ""], ["Dimitrov", "V. S.", ""], ["Mintchev", "M. P.", ""]]}, {"id": "1502.00277", "submitter": "Renato J Cintra", "authors": "H. M. de Oliveira, R. G. F. T\\'avora, R. J. Cintra, R. M. Campello de\n  Souza", "title": "Fast Finite Field Hartley Transforms Based on Hadamard Decomposition", "comments": "6 pages, 3 tables, fixed typos, submitted to the Sixth International\n  Symposium on Communication Theory and Applications (ISCTA'01), 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new transform over finite fields, the finite field Hartley transform\n(FFHT), was recently introduced and a number of promising applications on the\ndesign of efficient multiple access systems and multilevel spread spectrum\nsequences were proposed. The FFHT exhibits interesting symmetries, which are\nexploited to derive tailored fast transform algorithms. The proposed fast\nalgorithms are based on successive decompositions of the FFHT by means of\nHadamard-Walsh transforms (HWT). The introduced decompositions meet the lower\nbound on the multiplicative complexity for all the cases investigated. The\ncomplexity of the new algorithms is compared with that of traditional\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 16:08:55 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["de Oliveira", "H. M.", ""], ["T\u00e1vora", "R. G. F.", ""], ["Cintra", "R. J.", ""], ["de Souza", "R. M. Campello", ""]]}, {"id": "1502.00318", "submitter": "Nicholas Horton", "authors": "Nicholas J. Horton and Benjamin S. Baumer and Hadley Wickham", "title": "Setting the stage for data science: integration of data management\n  skills in introductory and second courses in statistics", "comments": null, "journal-ref": null, "doi": "10.1080/09332480.2015.1042739", "report-no": null, "categories": "stat.CO cs.CY stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many have argued that statistics students need additional facility to express\nstatistical computations. By introducing students to commonplace tools for data\nmanagement, visualization, and reproducible analysis in data science and\napplying these to real-world scenarios, we prepare them to think statistically.\nIn an era of increasingly big data, it is imperative that students develop\ndata-related capacities, beginning with the introductory course. We believe\nthat the integration of these precursors to data science into our\ncurricula-early and often-will help statisticians be part of the dialogue\nregarding \"Big Data\" and \"Big Questions\".\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 21:43:51 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Horton", "Nicholas J.", ""], ["Baumer", "Benjamin S.", ""], ["Wickham", "Hadley", ""]]}, {"id": "1502.00555", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "A Discrete Tchebichef Transform Approximation for Image and Video Coding", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": "IEEE Signal Processing Letters, vol. 22, issue 8, pp. 1137-1141,\n  2015", "doi": "10.1109/LSP.2015.2389899", "report-no": null, "categories": "stat.ME cs.CV cs.MM cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a low-complexity approximation for the discrete\nTchebichef transform (DTT). The proposed forward and inverse transforms are\nmultiplication-free and require a reduced number of additions and bit-shifting\noperations. Numerical compression simulations demonstrate the efficiency of the\nproposed transform for image and video coding. Furthermore, Xilinx Virtex-6\nFPGA based hardware realization shows 44.9% reduction in dynamic power\nconsumption and 64.7% lower area when compared to the literature.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 14:07:44 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1502.01148", "submitter": "Christian P. Robert", "authors": "Peter J. Green (Bristol), Krzysztof {\\L}atuszy\\'nski (Warwick),\n  Marcelo Pereyra (Bristol) and Christian P. Robert (Paris-Dauphine and\n  Warwick)", "title": "Bayesian computation: a perspective on the current state, and sampling\n  backwards and forwards", "comments": "30 pages (incl. 8 pages of references), 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decades have seen enormous improvements in computational inference\nbased on statistical models, with continual enhancement in a wide range of\ncomputational tools, in competition. In Bayesian inference, first and foremost,\nMCMC techniques continue to evolve, moving from random walk proposals to\nLangevin drift, to Hamiltonian Monte Carlo, and so on, with both theoretical\nand algorithmic inputs opening wider access to practitioners. However, this\nimpressive evolution in capacity is confronted by an even steeper increase in\nthe complexity of the models and datasets to be addressed. The difficulties of\nmodelling and then handling ever more complex datasets most likely call for a\nnew type of tool for computational inference that dramatically reduce the\ndimension and size of the raw data while capturing its essential aspects.\nApproximate models and algorithms may thus be at the core of the next\ncomputational revolution.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 10:38:42 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 11:51:19 GMT"}, {"version": "v3", "created": "Sat, 9 May 2015 08:17:58 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Green", "Peter J.", "", "Bristol"], ["\u0141atuszy\u0144ski", "Krzysztof", "", "Warwick"], ["Pereyra", "Marcelo", "", "Bristol"], ["Robert", "Christian P.", "", "Paris-Dauphine and\n  Warwick"]]}, {"id": "1502.01252", "submitter": "Joanna Polanska", "authors": "Andrzej Polanski, Michal Marczyk, Monika Pietrowska, Piotr Widlak,\n  Joanna Polanska", "title": "Signal Partitioning Algorithm for Highly Efficient Gaussian Mixture\n  Modeling in Mass Spectrometry", "comments": "24 pages, 6 figures", "journal-ref": "PLOS ONE (2015), 10(7): e0134256", "doi": "10.1371/journal.pone.0134256", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture - modeling of mass spectra is an approach with many potential\napplications including peak detection and quantification, smoothing,\nde-noising, feature extraction and spectral signal compression. However,\nexisting algorithms do not allow for automatic analyses of whole spectra.\nTherefore, despite highlighting potential advantages of mixture modeling of\nmass spectra of peptide/protein mixtures and some preliminary results presented\nin several papers, the mixture modeling approach was so far not developed to\nthe stage enabling systematic comparisons with existing software packages for\nproteomic mass spectra analyses. In this paper we present an efficient\nalgorithm for Gaussian mixture modeling of proteomic mass spectra of different\ntypes (e.g., MALDI-ToF profiling, MALDI-IMS). The main idea is automatic\npartitioning of protein mass spectral signal into fragments. The obtained\nfragments are separately decomposed into Gaussian mixture models. The\nparameters of the mixture models of fragments are then aggregated to form the\nmixture model of the whole spectrum. We compare the elaborated algorithm to\nexisting algorithms for peak detection and we demonstrate improvements of peak\ndetection efficiency obtained by using Gaussian mixture modeling. We also show\napplications of the elaborated algorithm to real proteomic datasets of low and\nhigh resolution.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 16:36:09 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 11:02:53 GMT"}], "update_date": "2015-08-04", "authors_parsed": [["Polanski", "Andrzej", ""], ["Marczyk", "Michal", ""], ["Pietrowska", "Monika", ""], ["Widlak", "Piotr", ""], ["Polanska", "Joanna", ""]]}, {"id": "1502.01377", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, V. S. Dimitrov", "title": "The Arithmetic Cosine Transform: Exact and Approximate Algorithms", "comments": "17 pages, 3 figures", "journal-ref": "IEEE Transactions on Signal Processing, vol. 58, no. 6, pp.\n  3076-3085, June 2010", "doi": "10.1109/TSP.2010.2045781", "report-no": null, "categories": "cs.NA math.NA stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of transform method --- the\narithmetic cosine transform (ACT). We provide the central mathematical\nproperties of the ACT, necessary in designing efficient and accurate\nimplementations of the new transform method. The key mathematical tools used in\nthe paper come from analytic number theory, in particular the properties of the\nRiemann zeta function. Additionally, we demonstrate that an exact signal\ninterpolation is achievable for any block-length. Approximate calculations were\nalso considered. The numerical examples provided show the potential of the ACT\nfor various digital signal processing applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 22:10:21 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Cintra", "R. J.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1502.01400", "submitter": "Marcelo Pereyra", "authors": "Marcelo Pereyra and Steve McLaughlin", "title": "Fast unsupervised Bayesian image segmentation with adaptive spatial\n  regularisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian estimation technique for hidden\nPotts-Markov random fields with unknown regularisation parameters, with\napplication to fast unsupervised K-class image segmentation. The technique is\nderived by first removing the regularisation parameter from the Bayesian model\nby marginalisation, followed by a small-variance-asymptotic (SVA) analysis in\nwhich the spatial regularisation and the integer-constrained terms of the Potts\nmodel are decoupled. The evaluation of this SVA Bayesian estimator is then\nrelaxed into a problem that can be computed efficiently by iteratively solving\na convex total-variation denoising problem and a least-squares clustering\n(K-means) problem, both of which can be solved straightforwardly, even in\nhigh-dimensions, and with parallel computing techniques. This leads to a fast\nfully unsupervised Bayesian image segmentation methodology in which the\nstrength of the spatial regularisation is adapted automatically to the observed\nimage during the inference procedure, and that can be easily applied in large\n2D and 3D scenarios or in applications requiring low computing times.\nExperimental results on real images, as well as extensive comparisons with\nstate-of-the-art algorithms, confirm that the proposed methodology offer\nextremely fast convergence and produces accurate segmentation results, with the\nimportant additional advantage of self-adjusting regularisation parameters.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 00:35:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2016 14:34:34 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 08:53:06 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Pereyra", "Marcelo", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1502.01477", "submitter": "Bruno Sudret", "authors": "G. Deman, K. Konakli, B. Sudret, J. Kerrou, P. Perrochet, H.\n  Benabderrahmane", "title": "Using sparse polynomial chaos expansions for the global sensitivity\n  analysis of groundwater lifetime expectancy in a multi-layered\n  hydrogeological model", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2015.11.005", "report-no": "RSUQ-2015-001", "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study makes use of polynomial chaos expansions to compute Sobol' indices\nwithin the frame of a global sensitivity analysis of hydro-dispersive\nparameters in a simplified vertical cross-section of a segment of the\nsubsurface of the Paris Basin. Applying conservative ranges, the uncertainty in\n78 input variables is propagated upon the mean lifetime expectancy of water\nmolecules departing from a specific location within a highly confining layer\nsituated in the middle of the model domain. Lifetime expectancy is a\nhydrogeological performance measure pertinent to safety analysis with respect\nto subsurface contaminants, such as radionuclides. The sensitivity analysis\nindicates that the variability in the mean lifetime expectancy can be\nsufficiently explained by the uncertainty in the petrofacies, \\ie the sets of\nporosity and hydraulic conductivity, of only a few layers of the model. The\nobtained results provide guidance regarding the uncertainty modeling in future\ninvestigations employing detailed numerical models of the subsurface of the\nParis Basin. Moreover, the study demonstrates the high efficiency of sparse\npolynomial chaos expansions in computing Sobol' indices for high-dimensional\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 09:46:22 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 12:42:53 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2015 22:43:14 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Deman", "G.", ""], ["Konakli", "K.", ""], ["Sudret", "B.", ""], ["Kerrou", "J.", ""], ["Perrochet", "P.", ""], ["Benabderrahmane", "H.", ""]]}, {"id": "1502.01527", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine, University of\n  Warwick, and CREST)", "title": "Some comments about A. Ronald Gallant's \"Reflections on the Probability\n  Space Induced by Moment Conditions with Implications for Bayesian Inference\"", "comments": "6 pages, submitted to the Journal of Financial Econometrics for the\n  discussion of Gallant (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is commenting on Ronald Gallant's (2015) reflections on the\nconstruction of Bayesian prior distributions from moment conditions. The main\nconclusion is that the paper does not deliver a working principle that could\njustify inference based on such priors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 13:10:43 GMT"}], "update_date": "2015-02-06", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, University of\n  Warwick, and CREST"]]}, {"id": "1502.01908", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Johan Dahlin and Thomas B. Sch\\\"on", "title": "Marginalizing Gaussian Process Hyperparameters using Sequential Monte\n  Carlo", "comments": "Accepted to the 6th IEEE international workshop on computational\n  advances in multi-sensor adaptive processing (CAMSAP), Cancun, Mexico,\n  December 2015", "journal-ref": null, "doi": "10.1109/CAMSAP.2015.7383840", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression is a popular method for non-parametric\nprobabilistic modeling of functions. The Gaussian process prior is\ncharacterized by so-called hyperparameters, which often have a large influence\non the posterior model and can be difficult to tune. This work provides a\nmethod for numerical marginalization of the hyperparameters, relying on the\nrigorous framework of sequential Monte Carlo. Our method is well suited for\nonline problems, and we demonstrate its ability to handle real-world problems\nwith several dimensions and compare it to other marginalization methods. We\nalso conclude that our proposed method is a competitive alternative to the\ncommonly used point estimates maximizing the likelihood, both in terms of\ncomputational load and its ability to handle multimodal posteriors.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 14:52:01 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 11:05:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Svensson", "Andreas", ""], ["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.01955", "submitter": "Robert Wolstenholme", "authors": "R. J. Wolstenholme and A. T. Walden", "title": "An Efficient Approach to Graphical Modelling of Time Series", "comments": "Replaced a typo in section 4.1; it is deemed an *incorrect graph* and\n  the missing edge; it is deemed a correct graph and the missing edge;", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for selecting a graphical model for $p$-vector-valued stationary\nGaussian time series was recently proposed by Matsuda and uses the\nKullback-Leibler divergence measure to define a test statistic. This statistic\nwas used in a backward selection procedure, but the algorithm is prohibitively\nexpensive for large $p.$ A high degree of sparsity is not assumed. We show that\nreformulation in terms of a multiple hypothesis test reduces computation time\nby $O(p^2)$ and simulations support the assertion that power levels are\nattained at least as good as those achieved by Matsuda's much slower approach.\nMoreover, the new scheme is readily parallelizable for even greater speed\ngains.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:01:00 GMT"}, {"version": "v2", "created": "Tue, 10 Feb 2015 18:58:03 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Wolstenholme", "R. J.", ""], ["Walden", "A. T.", ""]]}, {"id": "1502.01997", "submitter": "Julien Stoehr", "authors": "Julien Stoehr (I3M), Nial Friel (UCD)", "title": "Calibration of conditional composite likelihood for Bayesian inference\n  on Gibbs random fields", "comments": "JMLR Workshop and Conference Proceedings, 18th International\n  Conference on Artificial Intelligence and Statistics (AISTATS), San Diego,\n  California, USA, 9-12 May 2015 (Vol. 38, pp. 921-929). arXiv admin note:\n  substantial text overlap with arXiv:1207.5758", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields play an important role in statistics, however, the\nresulting likelihood is typically unavailable due to an intractable normalizing\nconstant. Composite likelihoods offer a principled means to construct useful\napproximations. This paper provides a mean to calibrate the posterior\ndistribution resulting from using a composite likelihood and illustrate its\nperformance in several examples.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 19:22:27 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 11:38:42 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Stoehr", "Julien", "", "I3M"], ["Friel", "Nial", "", "UCD"]]}, {"id": "1502.02008", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Asad Hasan, Marshall Jiang, Mansour T.A. Sharabiani", "title": "Stochastic Newton Sampler: R Package sns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package sns implements Stochastic Newton Sampler (SNS), a\nMetropolis-Hastings Monte Carlo Markov Chain algorithm where the proposal\ndensity function is a multivariate Gaussian based on a local, second-order\nTaylor series expansion of log-density. The mean of the proposal function is\nthe full Newton step in Newton-Raphson optimization algorithm. Taking advantage\nof the local, multivariate geometry captured in log-density Hessian allows SNS\nto be more efficient than univariate samplers, approaching independent sampling\nas the density function increasingly resembles a multivariate Gaussian. SNS\nrequires the log-density Hessian to be negative-definite everywhere in order to\nconstruct a valid proposal function. This property holds, or can be easily\nchecked, for many GLM-like models. When initial point is far from density peak,\nrunning SNS in non-stochastic mode by taking the Newton step, augmented with\nwith line search, allows the MCMC chain to converge to high-density areas\nfaster. For high-dimensional problems, partitioning of state space into\nlower-dimensional subsets, and applying SNS to the subsets within a Gibbs\nsampling framework can significantly improve the mixing of SNS chains. In\naddition to the above strategies for improving convergence and mixing, sns\noffers diagnostics and visualization capabilities, as well as a function for\nsample-based calculation of Bayesian predictive posterior distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 19:59:55 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Hasan", "Asad", ""], ["Jiang", "Marshall", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1502.02130", "submitter": "Carole Bernard", "authors": "Carole Bernard and Don McLeish", "title": "Algorithms for Finding Copulas Minimizing Convex Functions of Sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop improved rearrangement algorithms to find the dependence structure\nthat minimizes a convex function of the sum of dependent variables with given\nmargins. We propose a new multivariate dependence measure, which can assess the\nconvergence of the rearrangement algorithms and can be used as a stopping rule.\nWe show how to apply these algorithms for example to finding the dependence\namong variables for which the marginal distributions and the distribution of\nthe sum or the difference are known. As an example, we can find the dependence\nbetween two uniformly distributed variables that makes the distribution of the\nsum of two uniform variables indistinguishable from a normal distribution.\nUsing MCMC techniques, we design an algorithm that converges to the global\noptimum.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 11:57:52 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2015 17:23:38 GMT"}, {"version": "v3", "created": "Wed, 13 Jul 2016 14:54:47 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Bernard", "Carole", ""], ["McLeish", "Don", ""]]}, {"id": "1502.02436", "submitter": "Didier Henrion", "authors": "Y De Castro (LM-Orsay), F Gamboa (IMT), D Henrion (LAAS), J.-B\n  Lasserre (LAAS)", "title": "Exact solutions to Super Resolution on semi-algebraic domains in higher\n  dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the multi-dimensional Super Resolution problem on closed\nsemi-algebraic domains for various sampling schemes such as Fourier or moments.\nWe present a new semidefinite programming (SDP) formulation of the 1\n-minimization in the space of Radon measures in the multi-dimensional frame on\nsemi-algebraic sets. While standard approaches have focused on SDP relaxations\nof the dual program (a popular approach is based on Gram matrix\nrepresentations), this paper introduces an exact formulation of the primal 1\n-minimization exact recovery problem of Super Resolution that unleashes\nstandard techniques (such as moment-sum-of-squares hier-archies) to overcome\nintrinsic limitations of previous works in the literature. Notably, we show\nthat one can exactly solve the Super Resolution problem in dimension greater\nthan 2 and for a large family of domains described by semi-algebraic sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:09:45 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["De Castro", "Y", "", "LM-Orsay"], ["Gamboa", "F", "", "IMT"], ["Henrion", "D", "", "LAAS"], ["Lasserre", "J. -B", "", "LAAS"]]}, {"id": "1502.02438", "submitter": "Maryam Gharehdaghi", "authors": "Behrouz Fathi Vajargah, Maryam Gharehdaghi", "title": "Ergodicity of Fuzzy Markov Chains Based on Simulation Using Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As shown in [1], we reduce periodicity of fuzzy Markov chains using the\nHalton quasi-random generator. In this paper, we employ two different\nquasi-random sequences namely Faure and Kronecker to generate the membership\nvalues of fuzzy Markov chain. Using simulation it is revealed that the number\nof ergodic fuzzy Markov chain simulated by Kronecker sequences is more than the\none obtained by Faure sequences.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 11:14:24 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Vajargah", "Behrouz Fathi", ""], ["Gharehdaghi", "Maryam", ""]]}, {"id": "1502.02523", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Christophe Guyeux, Bastien Conesa, R\\'egis\n  Delage-Mouroux, Mich\\`ele Jouvenot, Philippe Huetz, and Fran\\c{c}oise\n  Desc\\^otes", "title": "A Bregman Proximal ADMM for NMF with Outliers: Estimating features with\n  missing values and outliers: a Bregman-proximal point algorithm for robust\n  Non-negative Matrix Factorization with application to gene expression\n  analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract the relevant features in a given dataset is a difficult task,\nrecently resolved in the non-negative data case with the Non-negative Matrix\nfactorization (NMF) method. The objective of this research work is to extend\nthis method to the case of missing and/or corrupted data due to outliers. To do\nso, data are denoised, missing values are imputed, and outliers are detected\nwhile performing a low-rank non-negative matrix factorization of the recovered\nmatrix. To achieve this goal, a mixture of Bregman proximal methods and of the\nAugmented Lagrangian scheme are used, in a similar way to the so-called\nAlternating Direction of Multipliers method. An application to the analysis of\ngene expression data of patients with bladder cancer is finally proposed.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 15:34:52 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Guyeux", "Christophe", ""], ["Conesa", "Bastien", ""], ["Delage-Mouroux", "R\u00e9gis", ""], ["Jouvenot", "Mich\u00e8le", ""], ["Huetz", "Philippe", ""], ["Desc\u00f4tes", "Fran\u00e7oise", ""]]}, {"id": "1502.02536", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Nested Sequential Monte Carlo Methods", "comments": "Extended version of paper published in Proceedings of the 32nd\n  International Conference on Machine Learning (ICML), Lille, France, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose nested sequential Monte Carlo (NSMC), a methodology to sample from\nsequences of probability distributions, even where the random variables are\nhigh-dimensional. NSMC generalises the SMC framework by requiring only\napproximate, properly weighted, samples from the SMC proposal distribution,\nwhile still resulting in a correct SMC algorithm. Furthermore, NSMC can in\nitself be used to produce such properly weighted samples. Consequently, one\nNSMC sampler can be used to construct an efficient high-dimensional proposal\ndistribution for another NSMC sampler, and this nesting of the algorithm can be\ndone to an arbitrary degree. This allows us to consider complex and\nhigh-dimensional models using SMC. We show results that motivate the efficacy\nof our approach on several filtering problems with dimensions in the order of\n100 to 1 000.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:15:31 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 07:04:58 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 11:01:25 GMT"}], "update_date": "2015-09-14", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.02613", "submitter": "Renliang Gu", "authors": "Renliang Gu and Aleksandar Dogand\\v{z}i\\'c", "title": "Projected Nesterov's Proximal-Gradient Algorithm for Sparse Signal\n  Reconstruction with a Convex Constraint", "comments": null, "journal-ref": "IEEE Trans. Signal Processing, vol, 65, no. 2, (2017) 3510-3525", "doi": "10.1109/TSP.2017.2691661", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a projected Nesterov's proximal-gradient (PNPG) approach for\nsparse signal reconstruction that combines adaptive step size with Nesterov's\nmomentum acceleration. The objective function that we wish to minimize is the\nsum of a convex differentiable data-fidelity (negative log-likelihood (NLL))\nterm and a convex regularization term. We apply sparse signal regularization\nwhere the signal belongs to a closed convex set within the closure of the\ndomain of the NLL; the convex-set constraint facilitates flexible NLL domains\nand accurate signal recovery. Signal sparsity is imposed using the\n$\\ell_1$-norm penalty on the signal's linear transform coefficients or gradient\nmap, respectively. The PNPG approach employs projected Nesterov's acceleration\nstep with restart and an inner iteration to compute the proximal mapping. We\npropose an adaptive step-size selection scheme to obtain a good local\nmajorizing function of the NLL and reduce the time spent backtracking. Thanks\nto step-size adaptation, PNPG does not require Lipschitz continuity of the\ngradient of the NLL. We present an integrated derivation of the momentum\nacceleration and its $\\mathcal{O}(k^{-2})$ convergence-rate and iterate\nconvergence proofs, which account for adaptive step-size selection, inexactness\nof the iterative proximal mapping, and the convex-set constraint. The tuning of\nPNPG is largely application-independent. Tomographic and compressed-sensing\nreconstruction experiments with Poisson generalized linear and Gaussian linear\nmeasurement models demonstrate the performance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:19:46 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 21:25:37 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2015 16:25:34 GMT"}, {"version": "v4", "created": "Sun, 15 May 2016 19:41:15 GMT"}, {"version": "v5", "created": "Wed, 25 May 2016 21:42:27 GMT"}, {"version": "v6", "created": "Wed, 5 Oct 2016 08:04:30 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Gu", "Renliang", ""], ["Dogand\u017ei\u0107", "Aleksandar", ""]]}, {"id": "1502.03042", "submitter": "Leo Duan", "authors": "Leo L. Duan, Xia Wang and Rhonda D. Szczesniak", "title": "Functional Gaussian Process Model for Bayesian Nonparametric Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process is a theoretically appealing model for nonparametric\nanalysis, but its computational cumbersomeness hinders its use in large scale\nand the existing reduced-rank solutions are usually heuristic. In this work, we\npropose a novel construction of Gaussian process as a projection from fixed\ndiscrete frequencies to any continuous location. This leads to a valid\nstochastic process that has a theoretic support with the reduced rank in the\nspectral density, as well as a high-speed computing algorithm. Our method\nprovides accurate estimates for the covariance parameters and concise form of\npredictive distribution for spatial prediction. For non-stationary data, we\nadopt the mixture framework with a customized spectral dependency structure.\nThis enables clustering based on local stationarity, while maintains the joint\nGaussianness. Our work is directly applicable in solving some of the challenges\nin the spatial data, such as large scale computation, anisotropic covariance,\nspatio-temporal modeling, etc. We illustrate the uses of the model via\nsimulations and an application on a massive dataset.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 18:57:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:15:06 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Duan", "Leo L.", ""], ["Wang", "Xia", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1502.03536", "submitter": "Vamsi Ithapu", "authors": "Chris Hinrichs, Vamsi K Ithapu, Qinyuan Sun, Sterling C Johnson, Vikas\n  Singh", "title": "Speeding up Permutation Testing in Neuroimaging", "comments": "NIPS 13", "journal-ref": "Advances in neural information processing systems (2013), pp.\n  890-898", "doi": null, "report-no": null, "categories": "stat.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a significant problem in nearly all\nneuroimaging studies. In order to correct for this phenomena, we require a\nreliable estimate of the Family-Wise Error Rate (FWER). The well known\nBonferroni correction method, while simple to implement, is quite conservative,\nand can substantially under-power a study because it ignores dependencies\nbetween test statistics. Permutation testing, on the other hand, is an exact,\nnon-parametric method of estimating the FWER for a given $\\alpha$-threshold,\nbut for acceptably low thresholds the computational burden can be prohibitive.\nIn this paper, we show that permutation testing in fact amounts to populating\nthe columns of a very large matrix ${\\bf P}$. By analyzing the spectrum of this\nmatrix, under certain conditions, we see that ${\\bf P}$ has a low-rank plus a\nlow-variance residual decomposition which makes it suitable for highly\nsub--sampled --- on the order of $0.5\\%$ --- matrix completion methods. Based\non this observation, we propose a novel permutation testing methodology which\noffers a large speedup, without sacrificing the fidelity of the estimated FWER.\nOur evaluations on four different neuroimaging datasets show that a\ncomputational speedup factor of roughly $50\\times$ can be achieved while\nrecovering the FWER distribution up to very high accuracy. Further, we show\nthat the estimated $\\alpha$-threshold is also recovered faithfully, and is\nstable.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:30:06 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Hinrichs", "Chris", ""], ["Ithapu", "Vamsi K", ""], ["Sun", "Qinyuan", ""], ["Johnson", "Sterling C", ""], ["Singh", "Vikas", ""]]}, {"id": "1502.03655", "submitter": "Johan Dahlin Mr.", "authors": "Manon Kok, Johan Dahlin, Thomas B. Sch\\\"on, Adrian Wills", "title": "Newton-based maximum likelihood estimation in nonlinear state space\n  models", "comments": "17 pages, 2 figures. Accepted for the 17th IFAC Symposium on System\n  Identification (SYSID), Beijing, China, October 2015", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.160", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood (ML) estimation using Newton's method in nonlinear state\nspace models (SSMs) is a challenging problem due to the analytical\nintractability of the log-likelihood and its gradient and Hessian. We estimate\nthe gradient and Hessian using Fisher's identity in combination with a\nsmoothing algorithm. We explore two approximations of the log-likelihood and of\nthe solution of the smoothing problem. The first is a linearization\napproximation which is computationally cheap, but the accuracy typically varies\nbetween models. The second is a sampling approximation which is asymptotically\nvalid for any SSM but is more computationally costly. We demonstrate our\napproach for ML parameter estimation on simulated data from two different SSMs\nwith encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:47:23 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 12:05:38 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Kok", "Manon", ""], ["Dahlin", "Johan", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Wills", "Adrian", ""]]}, {"id": "1502.03656", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Quasi-Newton particle Metropolis-Hastings", "comments": "23 pages, 5 figures. Accepted for the 17th IFAC Symposium on System\n  Identification (SYSID), Beijing, China, October 2015", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.258", "report-no": null, "categories": "stat.CO q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Metropolis-Hastings enables Bayesian parameter inference in general\nnonlinear state space models (SSMs). However, in many implementations a random\nwalk proposal is used and this can result in poor mixing if not tuned correctly\nusing tedious pilot runs. Therefore, we consider a new proposal inspired by\nquasi-Newton algorithms that may achieve similar (or better) mixing with less\ntuning. An advantage compared to other Hessian based proposals, is that it only\nrequires estimates of the gradient of the log-posterior. A possible application\nis parameter inference in the challenging class of SSMs with intractable\nlikelihoods. We exemplify this application and the benefits of the new proposal\nby modelling log-returns of future contracts on coffee by a stochastic\nvolatility model with $\\alpha$-stable observations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 13:48:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 14:10:59 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1502.03697", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on, Manon Kok", "title": "Nonlinear state space smoothing using the conditional particle filter", "comments": "Accepted for the 17th IFAC Symposium on System Identification\n  (SYSID), Beijing, China, October 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate the smoothing distribution in a nonlinear state space model, we\napply the conditional particle filter with ancestor sampling. This gives an\niterative algorithm in a Markov chain Monte Carlo fashion, with asymptotic\nconvergence results. The computational complexity is analyzed, and our proposed\nalgorithm is successfully applied to the challenging problem of sensor fusion\nbetween ultra-wideband and accelerometer/gyroscope measurements for indoor\npositioning. It appears to be a competitive alternative to existing nonlinear\nsmoothing algorithms, in particular the forward filtering-backward simulation\nsmoother.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 15:18:44 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 16:25:50 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2015 14:28:42 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Kok", "Manon", ""]]}, {"id": "1502.03939", "submitter": "Bruno Sudret", "authors": "R. Schoebi, B. Sudret, J. Wiart", "title": "Polynomial-Chaos-based Kriging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulation has become the standard tool in many engineering fields\nfor designing and optimizing systems, as well as for assessing their\nreliability. To cope with demanding analysis such as optimization and\nreliability, surrogate models (a.k.a meta-models) have been increasingly\ninvestigated in the last decade. Polynomial Chaos Expansions (PCE) and Kriging\nare two popular non-intrusive meta-modelling techniques. PCE surrogates the\ncomputational model with a series of orthonormal polynomials in the input\nvariables where polynomials are chosen in coherency with the probability\ndistributions of those input variables. On the other hand, Kriging assumes that\nthe computer model behaves as a realization of a Gaussian random process whose\nparameters are estimated from the available computer runs, i.e. input vectors\nand response values. These two techniques have been developed more or less in\nparallel so far with little interaction between the researchers in the two\nfields. In this paper, PC-Kriging is derived as a new non-intrusive\nmeta-modeling approach combining PCE and Kriging. A sparse set of orthonormal\npolynomials (PCE) approximates the global behavior of the computational model\nwhereas Kriging manages the local variability of the model output. An adaptive\nalgorithm similar to the least angle regression algorithm determines the\noptimal sparse set of polynomials. PC-Kriging is validated on various benchmark\nanalytical functions which are easy to sample for reference results. From the\nnumerical investigations it is concluded that PC-Kriging performs better than\nor at least as good as the two distinct meta-modeling techniques. A larger gain\nin accuracy is obtained when the experimental design has a limited size, which\nis an asset when dealing with demanding computational models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 10:53:52 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Schoebi", "R.", ""], ["Sudret", "B.", ""], ["Wiart", "J.", ""]]}, {"id": "1502.04202", "submitter": "Martin P. Boer", "authors": "Martin P. Boer", "title": "A fast Mixed Model B-splines algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fast algorithm for B-splines in mixed models is presented. B-splines have\nlocal support and are computational attractive, because the corresponding\nmatrices are sparse. A key element of the new algorithm is that the local\ncharacter of B-splines is preserved, while in other existing methods this local\ncharacter is lost. The computation time for the fast algorithm is linear in the\nnumber of B-splines, while computation time scales cubically for existing\ntransformations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 12:28:30 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Boer", "Martin P.", ""]]}, {"id": "1502.04221", "submitter": "Renato J Cintra", "authors": "A. Madanayake, R. J. Cintra, D. Onen, V. S. Dimitrov, N. T.\n  Rajapaksha, L. T. Bruton, A. Edirisuriya", "title": "A Row-parallel 8$\\times$8 2-D DCT Architecture Using Algebraic Integer\n  Based Exact Computation", "comments": "28 pages, 9 figures, 7 tables, corrected typos", "journal-ref": "IEEE Transactions on Circuits and Systems for Video Technology,\n  vol. 22, no. 6, pp. 915--929, 2012", "doi": "10.1109/TCSVT.2011.2181232", "report-no": null, "categories": "cs.AR cs.DM math.NT stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algebraic integer (AI) based time-multiplexed row-parallel architecture\nand two final-reconstruction step (FRS) algorithms are proposed for the\nimplementation of bivariate AI-encoded 2-D discrete cosine transform (DCT). The\narchitecture directly realizes an error-free 2-D DCT without using FRSs between\nrow-column transforms, leading to an 8$\\times$8 2-D DCT which is entirely free\nof quantization errors in AI basis. As a result, the user-selectable accuracy\nfor each of the coefficients in the FRS facilitates each of the 64 coefficients\nto have its precision set independently of others, avoiding the leakage of\nquantization noise between channels as is the case for published DCT designs.\nThe proposed FRS uses two approaches based on (i) optimized Dempster-Macleod\nmultipliers and (ii) expansion factor scaling. This architecture enables\nlow-noise high-dynamic range applications in digital video processing that\nrequires full control of the finite-precision computation of the 2-D DCT. The\nproposed architectures and FRS techniques are experimentally verified and\nvalidated using hardware implementations that are physically realized and\nverified on FPGA chip. Six designs, for 4- and 8-bit input word sizes, using\nthe two proposed FRS schemes, have been designed, simulated, physically\nimplemented and measured. The maximum clock rate and block-rate achieved among\n8-bit input designs are 307.787 MHz and 38.47 MHz, respectively, implying a\npixel rate of 8$\\times$307.787$\\approx$2.462 GHz if eventually embedded in a\nreal-time video-processing system. The equivalent frame rate is about 1187.35\nHz for the image size of 1920$\\times$1080. All implementations are functional\non a Xilinx Virtex-6 XC6VLX240T FPGA device.\n", "versions": [{"version": "v1", "created": "Sat, 14 Feb 2015 16:14:05 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Onen", "D.", ""], ["Dimitrov", "V. S.", ""], ["Rajapaksha", "N. T.", ""], ["Bruton", "L. T.", ""], ["Edirisuriya", "A.", ""]]}, {"id": "1502.04472", "submitter": "Leopoldo Catania", "authors": "Mauro Bernardi and Leopoldo Catania", "title": "Comparison of Value-at-Risk models: the MCS package", "comments": "25 pages. arXiv admin note: substantial text overlap with\n  arXiv:1410.8504", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper compares the Value--at--Risk (VaR) forecasts delivered by\nalternative model specifications using the Model Confidence Set (MCS) procedure\nrecently developed by Hansen et al. (2011). The direct VaR estimate provided by\nthe Conditional Autoregressive Value--at--Risk (CAViaR) models of Eengle and\nManganelli (2004) are compared to those obtained by the popular Autoregressive\nConditional Heteroskedasticity (ARCH) models of Engle (1982) and to the\nrecently introduced Generalised Autoregressive Score (GAS) models of Creal et\nal. (2013) and Harvey (2013). The Hansen's procedure consists on a sequence of\ntests which permits to construct a set of \"superior\" models, where the null\nhypothesis of Equal Predictive Ability (EPA) is not rejected at a certain\nconfidence level. Our empirical results, suggest that, after the Global\nFinancial Crisis (GFC) of 2007-2008, highly non-linear volatility models\ndeliver better VaR forecasts for the European countries as opposed to other\nregions. The R package MCS is introduced for performing the model comparisons\nwhose main features are discussed throughout the paper.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 09:20:26 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Bernardi", "Mauro", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1502.04622", "submitter": "Balaji Lakshminarayanan", "authors": "Balaji Lakshminarayanan, Daniel M. Roy and Yee Whye Teh", "title": "Particle Gibbs for Bayesian Additive Regression Trees", "comments": null, "journal-ref": "Proceedings of the 18th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2015, San Diego, CA, USA. JMLR: W&CP\n  volume 38", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive regression trees are flexible non-parametric models and popular\noff-the-shelf tools for real-world non-linear regression. In application\ndomains, such as bioinformatics, where there is also demand for probabilistic\npredictions with measures of uncertainty, the Bayesian additive regression\ntrees (BART) model, introduced by Chipman et al. (2010), is increasingly\npopular. As data sets have grown in size, however, the standard\nMetropolis-Hastings algorithms used to perform inference in BART are proving\ninadequate. In particular, these Markov chains make local changes to the trees\nand suffer from slow mixing when the data are high-dimensional or the best\nfitting trees are more than a few layers deep. We present a novel sampler for\nBART based on the Particle Gibbs (PG) algorithm (Andrieu et al., 2010) and a\ntop-down particle filtering algorithm for Bayesian decision trees\n(Lakshminarayanan et al., 2013). Rather than making local changes to individual\ntrees, the PG sampler proposes a complete tree to fit the residual. Experiments\nshow that the PG sampler outperforms existing samplers in many settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:48:30 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Lakshminarayanan", "Balaji", ""], ["Roy", "Daniel M.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1502.04822", "submitter": "Johan Westerborn", "authors": "Jimmy Olsson and Johan Westerborn", "title": "An efficient particle-based online EM algorithm for general state-space\n  models", "comments": "6 pages, changed style and bigger revision of the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the parameters of general state-space models is a topic of\nimportance for many scientific and engineering disciplines. In this paper we\npresent an online parameter estimation algorithm obtained by casting our\nrecently proposed particle-based, rapid incremental smoother (PaRIS) into the\nframework of online expectation-maximization (EM) for state-space models\nproposed by Capp\\'e (2011). Previous such particle-based implementations of\nonline EM suffer typically from either the well-known degeneracy of the\ngenealogical particle paths or a quadratic complexity in the number of\nparticles. However, by using the computationally efficient and numerically\nstable PaRIS algorithm for estimating smoothed expectations of time-averaged\nsufficient statistics of the model we obtain a fast algorithm with very limited\nmemory requirements and a computational complexity that grows only linearly\nwith the number of particles. The efficiency of the algorithm is illustrated in\na simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 08:14:31 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 17:01:30 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Olsson", "Jimmy", ""], ["Westerborn", "Johan", ""]]}, {"id": "1502.05287", "submitter": "Sera Aylin Cakiroglu", "authors": "Sera Aylin Cakiroglu", "title": "Optimal Regular Graph Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical problem in optimal design theory is finding an experimental design\nthat is optimal with respect to some criteria in a class of designs. The most\npopular criteria include the A- and D-criteria. Regular graph designs occur in\nmany optimality results and, if the number of blocks is large enough, they are\nA- and D-optimal. We present the results of an exact computer search for the\nbest regular graph designs in large systems for up to 20 points, k<=r<=10 and\nr(k-1)-(v-1)[r(k-1)/(v-1)]<=9.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 16:08:27 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 11:17:13 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Cakiroglu", "Sera Aylin", ""]]}, {"id": "1502.05503", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann, Jukka Corander, Ritabrata Dutta, and Samuel Kaski", "title": "Classification and Bayesian Optimization for Likelihood-Free Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some statistical models are specified via a data generating process for which\nthe likelihood function cannot be computed in closed form. Standard\nlikelihood-based inference is then not feasible but the model parameters can be\ninferred by finding the values which yield simulated data that resemble the\nobserved data. This approach faces at least two major difficulties: The first\ndifficulty is the choice of the discrepancy measure which is used to judge\nwhether the simulated data resemble the observed data. The second difficulty is\nthe computationally efficient identification of regions in the parameter space\nwhere the discrepancy is low. We give here an introduction to our recent work\nwhere we tackle the two difficulties through classification and Bayesian\noptimization.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 09:09:27 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""], ["Dutta", "Ritabrata", ""], ["Kaski", "Samuel", ""]]}, {"id": "1502.05933", "submitter": "Ghislain Durif", "authors": "G. Durif, L. Modolo, J. Michaelsson, J. E. Mold, S. Lambert-Lacroix\n  and F. Picard", "title": "High Dimensional Classification with combined Adaptive Sparse PLS and\n  Logistic Regression", "comments": "9 pages, 3 figures, 4 tables + Supplementary Materials 8 pages, 3\n  figures, 10 tables", "journal-ref": "Bioinformatics 34, 485-493 (2018)", "doi": "10.1093/bioinformatics/btx571", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The high dimensionality of genomic data calls for the development\nof specific classification methodologies, especially to prevent over-optimistic\npredictions. This challenge can be tackled by compression and variable\nselection, which combined constitute a powerful framework for classification,\nas well as data visualization and interpretation. However, current proposed\ncombinations lead to instable and non convergent methods due to inappropriate\ncomputational frameworks. We hereby propose a stable and convergent approach\nfor classification in high dimensional based on sparse Partial Least Squares\n(sparse PLS). Results: We start by proposing a new solution for the sparse PLS\nproblem that is based on proximal operators for the case of univariate\nresponses. Then we develop an adaptive version of the sparse PLS for\nclassification, which combines iterative optimization of logistic regression\nand sparse PLS to ensure convergence and stability. Our results are confirmed\non synthetic and experimental data. In particular we show how crucial\nconvergence and stability can be when cross-validation is involved for\ncalibration purposes. Using gene expression data we explore the prediction of\nbreast cancer relapse. We also propose a multicategorial version of our method\non the prediction of cell-types based on single-cell expression data.\nAvailability: Our approach is implemented in the plsgenomics R-package.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 16:56:59 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 09:07:41 GMT"}, {"version": "v3", "created": "Wed, 6 May 2015 09:18:08 GMT"}, {"version": "v4", "created": "Fri, 12 May 2017 11:59:22 GMT"}, {"version": "v5", "created": "Wed, 30 Aug 2017 16:06:38 GMT"}], "update_date": "2021-04-10", "authors_parsed": [["Durif", "G.", ""], ["Modolo", "L.", ""], ["Michaelsson", "J.", ""], ["Mold", "J. E.", ""], ["Lambert-Lacroix", "S.", ""], ["Picard", "F.", ""]]}, {"id": "1502.05955", "submitter": "Edith Cohen", "authors": "Edith Cohen", "title": "Stream Sampling for Frequency Cap Statistics", "comments": "21 pages, 4 figures, preliminary version will appear in KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.DB stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unaggregated data, in streamed or distributed form, is prevalent and come\nfrom diverse application domains which include interactions of users with web\nservices and IP traffic. Data elements have {\\em keys} (cookies, users,\nqueries) and elements with different keys interleave. Analytics on such data\ntypically utilizes statistics stated in terms of the frequencies of keys. The\ntwo most common statistics are {\\em distinct}, which is the number of active\nkeys in a specified segment, and {\\em sum}, which is the sum of the frequencies\nof keys in the segment. Both are special cases of {\\em cap} statistics, defined\nas the sum of frequencies {\\em capped} by a parameter $T$, which are popular in\nonline advertising platforms. Aggregation by key, however, is costly, requiring\nstate proportional to the number of distinct keys, and therefore we are\ninterested in estimating these statistics or more generally, sampling the data,\nwithout aggregation. We present a sampling framework for unaggregated data that\nuses a single pass (for streams) or two passes (for distributed data) and state\nproportional to the desired sample size. Our design provides the first\neffective solution for general frequency cap statistics. Our $\\ell$-capped\nsamples provide estimates with tight statistical guarantees for cap statistics\nwith $T=\\Theta(\\ell)$ and nonnegative unbiased estimates of {\\em any} monotone\nnon-decreasing frequency statistics. An added benefit of our unified design is\nfacilitating {\\em multi-objective samples}, which provide estimates with\nstatistical guarantees for a specified set of different statistics, using a\nsingle, smaller sample.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 17:53:45 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 13:49:41 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cohen", "Edith", ""]]}, {"id": "1502.06069", "submitter": "Kody Law", "authors": "H{\\aa}kon Hoel, Kody J. H. Law, and Raul Tempone", "title": "Multilevel ensemble Kalman filtering", "comments": null, "journal-ref": "SIAM J. Numer. Anal., 54(3), 1813-1839 (2016)", "doi": "10.1137/15M100955X", "report-no": null, "categories": "math.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work embeds a multilevel Monte Carlo sampling strategy into the Monte\nCarlo step of the ensemble Kalman filter (EnKF) in the setting of finite\ndimensional signal evolution and noisy discrete-time observations. The signal\ndynamics is assumed to be governed by a stochastic differential equation (SDE),\nand a hierarchy of time grids is introduced for multilevel numerical\nintegration of that SDE. The resulting multilevel EnKF is proved to\nasymptotically outperform EnKF in terms of computational cost versus\napproximation accuracy. The theoretical results are illustrated numerically.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 06:18:37 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 20:38:16 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Hoel", "H\u00e5kon", ""], ["Law", "Kody J. H.", ""], ["Tempone", "Raul", ""]]}, {"id": "1502.06349", "submitter": "Gareth Peters Dr", "authors": "Antonio Dalessandro, Gareth W. Peters", "title": "Tensor Approximation of Generalized Correlated Diffusions and Functional\n  Copula Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.MF stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate aspects of semimartingale decompositions, approximation and\nthe martingale representation for multidimensional correlated Markov processes.\nA new interpretation of the dependence among processes is given using the\nmartingale approach. We show that it is possible to represent, in both\ncontinuous and discrete space, that a multidimensional correlated generalized\ndiffusion is a linear combination of processes that originate from the\ndecomposition of the starting multidimensional semimartingale. This result not\nonly reconciles with the existing theory of diffusion approximations and\ndecompositions, but defines the general representation of infinitesimal\ngenerators for both multidimensional generalized diffusions and as we will\ndemonstrate also for the specification of copula density dependence structures.\nThis new result provides immediate representation of the approximate solution\nfor correlated stochastic differential equations. We demonstrate desirable\nconvergence results for the proposed multidimensional semimartingales\ndecomposition approximations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 08:58:30 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Dalessandro", "Antonio", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1502.06440", "submitter": "Erlis Ruli", "authors": "Erlis Ruli, Nicola Sartori, Laura Ventura", "title": "Improved Laplace Approximation for Marginal Likelihoods", "comments": "24 pages", "journal-ref": "Electronic Journal of Statistics 10(2), 3986-4009, 2016", "doi": "10.1214/16-EJS1218", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical applications often involve the calculation of intractable\nmultidimensional integrals. The Laplace formula is widely used to approximate\nsuch integrals. However, in high-dimensional or small sample size problems, the\nshape of the integrand function may be far from that of the Gaussian density,\nand thus the standard Laplace approximation can be inaccurate. We propose an\nimproved Laplace approximation that reduces the asymptotic error of the\nstandard Laplace formula by one order of magnitude, thus leading to third-order\naccuracy. We also show, by means of practical examples of various complexity,\nthat the proposed method is extremely accurate, even in high dimensions,\nimproving over the standard Laplace formula. Such examples also demonstrate\nthat the accuracy of the proposed method is comparable with that of other\nexisting methods, which are computationally more demanding. An R implementation\nof the improved Laplace approximation is also provided through the R package\niLaplace available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 14:20:45 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 12:56:33 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ruli", "Erlis", ""], ["Sartori", "Nicola", ""], ["Ventura", "Laura", ""]]}, {"id": "1502.06498", "submitter": "Antonio D'Ambrosio Dr.", "authors": "Sonia Amodio, Antonio D'Ambrosio and Roberta Siciliano", "title": "Accurate algorithms for identifying the median ranking when dealing with\n  weak and partial rankings under the Kemeny axiomatic approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.ejor.2015.08.048", "report-no": "STAD_REPORT_01_2014", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference rankings virtually appear in all field of science (political\nsciences, behavioral sciences, machine learning, decision making and so on).\nThe well-know social choice problem consists in trying to find a reasonable\nprocedure to use the aggregate preferences expressed by subjects (usually\ncalled judges) to reach a collective decision. This problem turns out to be\nequivalent to the problem of estimating the consensus (central) ranking from\ndata that is known to be a NP-hard Problem. Emond and Mason in 2002 proposed a\nbranch and bound algorithm to calculate the consensus ranking given $n$\nrankings expressed on $m$ objects. Depending on the complexity of the problem,\nthere can be multiple solutions and then the consensus ranking may be not\nunique. We propose a new algorithm to find the consensus ranking that is\nequivalent to Emond and Mason's algorithm in terms of at least one of the\nsolutions reached, but permits a really remarkable saving in computational\ntime.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 16:53:36 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 10:50:22 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Amodio", "Sonia", ""], ["D'Ambrosio", "Antonio", ""], ["Siciliano", "Roberta", ""]]}, {"id": "1502.06557", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Iteratively reweighted adaptive lasso for conditional heteroscedastic\n  time series with applications to AR-ARCH type processes", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 100 (2016) 773-793", "doi": "10.1016/j.csda.2015.11.016", "report-no": null, "categories": "stat.ME q-fin.CP stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage algorithms are of great importance in almost every area of\nstatistics due to the increasing impact of big data. Especially time series\nanalysis benefits from efficient and rapid estimation techniques such as the\nlasso. However, currently lasso type estimators for autoregressive time series\nmodels still focus on models with homoscedastic residuals. Therefore, an\niteratively reweighted adaptive lasso algorithm for the estimation of time\nseries models under conditional heteroscedasticity is presented in a\nhigh-dimensional setting. The asymptotic behaviour of the resulting estimator\nis analysed. It is found that the proposed estimation procedure performs\nsubstantially better than its homoscedastic counterpart. A special case of the\nalgorithm is suitable to compute the estimated multivariate AR-ARCH type models\nefficiently. Extensions to the model like periodic AR-ARCH, threshold AR-ARCH\nor ARMA-GARCH are discussed. Finally, different simulation results and\napplications to electricity market data and returns of metal prices are shown.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 19:14:39 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 23:05:43 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1502.06626", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail, Christos Boutsidis", "title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is the optimal linear auto-encoder of\ndata, and it is often used to construct features. Enforcing sparsity on the\nprincipal components can promote better generalization, while improving the\ninterpretability of the features. We study the problem of constructing optimal\nsparse linear auto-encoders. Two natural questions in such a setting are: i)\nGiven a level of sparsity, what is the best approximation to PCA that can be\nachieved? ii) Are there low-order polynomial-time algorithms which can\nasymptotically achieve this optimal tradeoff between the sparsity and the\napproximation quality?\n  In this work, we answer both questions by giving efficient low-order\npolynomial-time algorithms for constructing asymptotically \\emph{optimal}\nlinear auto-encoders (in particular, sparse features with near-PCA\nreconstruction error) and demonstrate the performance of our algorithms on real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 21:06:39 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Magdon-Ismail", "Malik", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1502.06777", "submitter": "Jose Henrique De Morais Goulart", "authors": "Jos\\'e Henrique De Morais Goulart, Maxime Boizard (SATIE), R\\'emy\n  Boyer, G\\'erard Favier, Pierre Comon (GIPSA-CICS)", "title": "Statistical efficiency of structured cpd estimation applied to\n  Wiener-Hammerstein modeling", "comments": "Accepted for publication in the Proceedings of the European Signal\n  Processing Conference (EUSIPCO) Aug 2015, Nice, France. 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of a structured canonical polyadic decomposition (CPD) is\nuseful to address several important modeling problems in real-world\napplications. In this paper, we consider the identification of a nonlinear\nsystem by means of a Wiener-Hammerstein model, assuming a high-order Volterra\nkernel of that system has been previously estimated. Such a kernel, viewed as a\ntensor, admits a CPD with banded circulant factors which comprise the model\nparameters. To estimate them, we formulate specialized estimators based on\nrecently proposed algorithms for the computation of structured CPDs. Then,\nconsidering the presence of additive white Gaussian noise, we derive a\nclosed-form expression for the Cramer-Rao bound (CRB) associated with this\nestimation problem. Finally, we assess the statistical performance of the\nproposed estimators via Monte Carlo simulations, by comparing their mean-square\nerror with the CRB.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 12:07:50 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2015 09:34:07 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Goulart", "Jos\u00e9 Henrique De Morais", "", "SATIE"], ["Boizard", "Maxime", "", "SATIE"], ["Boyer", "R\u00e9my", "", "GIPSA-CICS"], ["Favier", "G\u00e9rard", "", "GIPSA-CICS"], ["Comon", "Pierre", "", "GIPSA-CICS"]]}, {"id": "1502.06930", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla and James G. Scott", "title": "Tensor decomposition with generalized lasso penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for penalized tensor decomposition (PTD) that\nestimates smoothly varying latent factors in multi-way data. This generalizes\nexisting work on sparse tensor decomposition and penalized matrix\ndecompositions, in a manner parallel to the generalized lasso for regression\nand smoothing problems. Our approach presents many nontrivial challenges at the\nintersection of modeling and computation, which are studied in detail. An\nefficient coordinate-wise optimization algorithm for (PTD) is presented, and\nits convergence properties are characterized. The method is applied both to\nsimulated data and real data on flu hospitalizations in Texas. These results\nshow that our penalized tensor decomposition can offer major improvements on\nexisting methods for analyzing multi-way data that exhibit smooth spatial or\ntemporal features.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 20:03:05 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 05:19:53 GMT"}, {"version": "v3", "created": "Fri, 13 May 2016 01:40:38 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Scott", "James G.", ""]]}, {"id": "1502.07039", "submitter": "Marcel Scharth", "authors": "Eduardo F. Mendes, Marcel Scharth and Robert Kohn", "title": "Markov Interacting Importance Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new Markov chain Monte Carlo (MCMC) sampler called the Markov\nInteracting Importance Sampler (MIIS). The MIIS sampler uses conditional\nimportance sampling (IS) approximations to jointly sample the current state of\nthe Markov Chain and estimate conditional expectations, possibly by\nincorporating a full range of variance reduction techniques. We compute\nRao-Blackwellized estimates based on the conditional expectations to construct\ncontrol variates for estimating expectations under the target distribution. The\ncontrol variates are particularly efficient when there are substantial\ncorrelations between the variables in the target distribution, a challenging\nsetting for MCMC. An important motivating application of MIIS occurs when the\nexact Gibbs sampler is not available because it is infeasible to directly\nsimulate from the conditional distributions. In this case the MIIS method can\nbe more efficient than a Metropolis-within-Gibbs approach. We also introduce\nthe MIIS random walk algorithm, designed to accelerate convergence and improve\nupon the computational efficiency of standard random walk samplers. Simulated\nand empirical illustrations for Bayesian analysis show that the method\nsignificantly reduces the variance of Monte Carlo estimates compared to\nstandard MCMC approaches, at equivalent implementation and computational\neffort.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 03:48:04 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 03:00:21 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Mendes", "Eduardo F.", ""], ["Scharth", "Marcel", ""], ["Kohn", "Robert", ""]]}, {"id": "1502.07252", "submitter": "Pierre  Barbillon", "authors": "Guillaume Damblin, Pierre Barbillon, Merlin Keller, Alberto Pasanisi,\n  Eric Parent", "title": "Adaptive numerical designs for the calibration of computer codes", "comments": null, "journal-ref": "SIAM ASA Journal of Uncertainty Quantification, 2018 6(1), 151-179", "doi": "10.1137/15M1033162", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making good predictions of a physical system using a computer code requires\nthe inputs to be carefully specified. Some of these inputs called control\nvariables have to reproduce physical conditions whereas other inputs, called\nparameters, are specific to the computer code and most often uncertain. The\ngoal of statistical calibration consists in estimating these parameters with\nthe help of a statistical model which links the code outputs with the field\nmeasurements. In a Bayesian setting, the posterior distribution of these\nparameters is normally sampled using MCMC methods. However, they are\nimpractical when the code runs are high time-consuming. A way to circumvent\nthis issue consists of replacing the computer code with a Gaussian process\nemulator, then sampling a cheap-to-evaluate posterior distribution based on it.\nDoing so, calibration is subject to an error which strongly depends on the\nnumerical design of experiments used to fit the emulator. We aim at reducing\nthis error by building a proper sequential design by means of the Expected\nImprovement criterion. Numerical illustrations in several dimensions assess the\nefficiency of such sequential strategies.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 17:09:21 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 09:21:51 GMT"}, {"version": "v3", "created": "Tue, 3 Apr 2018 12:05:56 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Damblin", "Guillaume", ""], ["Barbillon", "Pierre", ""], ["Keller", "Merlin", ""], ["Pasanisi", "Alberto", ""], ["Parent", "Eric", ""]]}, {"id": "1502.07458", "submitter": "Edouard Ollier", "authors": "Edouard Ollier, Adeline Samson, Xavier Delavenne, Vivian Viallon", "title": "A SAEM Algorithm for Fused Lasso Penalized Non Linear Mixed Effect\n  Models: Application to Group Comparison in Pharmacokinetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non linear mixed effect models are classical tools to analyze non linear\nlongitudinal data in many fields such as population Pharmacokinetic. Groups of\nobservations are usually compared by introducing the group affiliations as\nbinary covariates with a reference group that is stated among the groups. This\napproach is relatively limited as it allows only the comparison of the\nreference group to the others. In this work, we propose to compare the groups\nusing a penalized likelihood approach. Groups are described by the same\nstructural model but with parameters that are group specific. The likelihood is\npenalized with a fused lasso penalty that induces sparsity on the differences\nbetween groups for both fixed effects and variances of random effects. A\npenalized Stochastic Approximation EM algorithm is proposed that is coupled to\nAlternating Direction Method Multipliers to solve the maximization step. An\nextensive simulation study illustrates the performance of this algorithm when\ncomparing more than two groups. Then the approach is applied to real data from\ntwo pharmacokinetic drug-drug interaction trials.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 06:50:40 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 09:39:18 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Ollier", "Edouard", ""], ["Samson", "Adeline", ""], ["Delavenne", "Xavier", ""], ["Viallon", "Vivian", ""]]}, {"id": "1502.07532", "submitter": "F Lau Dr", "authors": "Axel Gandy and F. Din-Houn Lau", "title": "The chopthin algorithm for resampling", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TSP.2016.2558166", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resampling is a standard step in particle filters and more generally\nsequential Monte Carlo methods. We present an algorithm, called chopthin, for\nresampling weighted particles. In contrast to standard resampling methods the\nalgorithm does not produce a set of equally weighted particles; instead it\nmerely enforces an upper bound on the ratio between the weights. Simulation\nstudies show that the chopthin algorithm consistently outperforms standard\nresampling methods. The algorithms chops up particles with large weight and\nthins out particles with low weight, hence its name. It implicitly guarantees a\nlower bound on the effective sample size. The algorithm can be implemented\nefficiently, making it practically useful. We show that the expected\ncomputational effort is linear in the number of particles. Implementations for\nC++, R (on CRAN), Python and Matlab are available.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 12:58:38 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 17:29:46 GMT"}, {"version": "v3", "created": "Mon, 20 Apr 2015 13:37:00 GMT"}, {"version": "v4", "created": "Mon, 14 Dec 2015 17:20:26 GMT"}, {"version": "v5", "created": "Wed, 6 Apr 2016 09:06:12 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Gandy", "Axel", ""], ["Lau", "F. Din-Houn", ""]]}, {"id": "1502.07864", "submitter": "Georg Hahn", "authors": "Georg Hahn", "title": "Optimal allocation of Monte Carlo simulations to multiple hypothesis\n  tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis tests are often carried out in practice using p-value\nestimates obtained with bootstrap or permutation tests since the analytical\np-values underlying all hypotheses are usually unknown. This article considers\nthe allocation of a pre-specified total number of Monte Carlo simulations $K\n\\in \\mathbb{N}$ (i.e., permutations or draws from a bootstrap distribution) to\na given number of $m \\in \\mathbb{N}$ hypotheses in order to approximate their\np-values $p \\in [0,1]^m$ in an optimal way, in the sense that the allocation\nminimises the total expected number of misclassified hypotheses. A\nmisclassification occurs if a decision on a single hypothesis, obtained with an\napproximated p-value, differs from the one obtained if its p-value was known\nanalytically. The contribution of this article is threefold: Under the\nassumption that $p$ is known and $K \\in \\mathbb{R}$, and using a normal\napproximation of the Binomial distribution, the optimal real-valued allocation\nof $K$ simulations to $m$ hypotheses is derived when correcting for\nmultiplicity with the Bonferroni correction, both when computing the p-value\nestimates with or without a pseudo-count. Computational subtleties arising in\nthe former case will be discussed. Second, with the help of an algorithm based\non simulated annealing, empirical evidence is given that the optimal integer\nallocation is likely of the same form as the optimal real-valued allocation,\nand that both seem to coincide asympotically. Third, an empirical study on\nsimulated and real data demonstrates that a recently proposed sampling\nalgorithm based on Thompson sampling asympotically mimics the optimal\n(real-valued) allocation when the p-values are unknown and thus estimated at\nruntime.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 11:19:31 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 18:43:11 GMT"}, {"version": "v3", "created": "Thu, 20 Sep 2018 18:36:50 GMT"}, {"version": "v4", "created": "Mon, 18 Mar 2019 14:33:37 GMT"}, {"version": "v5", "created": "Sat, 5 Oct 2019 19:28:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Hahn", "Georg", ""]]}, {"id": "1502.07873", "submitter": "Quan Long", "authors": "Quan Long, Mohammad Motamed, Raul Tempone", "title": "Fast Bayesian Optimal Experimental Design for Seismic Source Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fast method for optimally designing experiments in the context\nof statistical seismic source inversion. In particular, we efficiently compute\nthe optimal number and locations of the receivers or seismographs. The seismic\nsource is modeled by a point moment tensor multiplied by a time-dependent\nfunction. The parameters include the source location, moment tensor components,\nand start time and frequency in the time function. The forward problem is\nmodeled by elastodynamic wave equations. We show that the Hessian of the cost\nfunctional, which is usually defined as the square of the weighted L2 norm of\nthe difference between the experimental data and the simulated data, is\nproportional to the measurement time and the number of receivers. Consequently,\nthe posterior distribution of the parameters, in a Bayesian setting,\nconcentrates around the \"true\" parameters, and we can employ Laplace\napproximation and speed up the estimation of the expected Kullback-Leibler\ndivergence (expected information gain), the optimality criterion in the\nexperimental design procedure. Since the source parameters span several\nmagnitudes, we use a scaling matrix for efficient control of the condition\nnumber of the original Hessian matrix. We use a second-order accurate finite\ndifference method to compute the Hessian matrix and either sparse quadrature or\nMonte Carlo sampling to carry out numerical integration. We demonstrate the\nefficiency, accuracy, and applicability of our method on a two-dimensional\nseismic source inversion problem.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 12:21:53 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Long", "Quan", ""], ["Motamed", "Mohammad", ""], ["Tempone", "Raul", ""]]}, {"id": "1502.07989", "submitter": "Chun Wang", "authors": "Chun Wang, Ming-Hui Chen, Elizabeth Schifano, Jing Wu, and Jun Yan", "title": "Statistical Methods and Computing for Big Data", "comments": null, "journal-ref": "Statistics and Its Interface 9 (2016) 399-414", "doi": "10.4310/SII.2016.v9.n4.a1", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data are data on a massive scale in terms of volume, intensity, and\ncomplexity that exceed the capacity of standard software tools. They present\nopportunities as well as challenges to statisticians. The role of computational\nstatisticians in scientific discovery from big data analyses has been\nunder-recognized even by peer statisticians. This article reviews recent\nmethodological and software developments in statistics that address the big\ndata challenges. Methodologies are grouped into three classes:\nsubsampling-based, divide and conquer, and sequential updating for stream data.\nSoftware review focuses on the open source R and R packages, covering recent\ntools that help break the barriers of computer memory and computing power. Some\nof the tools are illustrated in a case study with a logistic regression for the\nchance of airline delay.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 17:59:22 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 03:09:07 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wang", "Chun", ""], ["Chen", "Ming-Hui", ""], ["Schifano", "Elizabeth", ""], ["Wu", "Jing", ""], ["Yan", "Jun", ""]]}]