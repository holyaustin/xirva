[{"id": "1811.00097", "submitter": "Paul McNicholas", "authors": "Sharon M. McNicholas, Paul D. McNicholas and Daniel A. Ashlock", "title": "An Evolutionary Algorithm with Crossover and Mutation for Model-Based\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An evolutionary algorithm (EA) is developed as an alternative to the EM\nalgorithm for parameter estimation in model-based clustering. This EA\nfacilitates a different search of the fitness landscape, i.e., the likelihood\nsurface, utilizing both crossover and mutation. Furthermore, this EA represents\nan efficient approach to \"hard\" model-based clustering and so it can be viewed\nas a sort of generalization of the k-means algorithm, which is itself\nequivalent to a restricted Gaussian mixture model. The EA is illustrated on\nseveral datasets, and its performance is compared to other hard clustering\napproaches and model-based clustering via the EM algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 20:14:10 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:28:47 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["McNicholas", "Sharon M.", ""], ["McNicholas", "Paul D.", ""], ["Ashlock", "Daniel A.", ""]]}, {"id": "1811.00153", "submitter": "Pritam Ranjan", "authors": "Ru Zhang, Chunfang Devon Lin, Pritam Ranjan", "title": "A Sequential Design Approach for Calibrating a Dynamic Population Growth\n  Model", "comments": "36 pages", "journal-ref": "SIAM/ASA J. Uncertainty Quantification, 7(4), 1245 -1274, 2019", "doi": "10.1137/18M1224544", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive understanding of the population growth of a variety of pests\nis often crucial for efficient crop management. Our motivating application\ncomes from calibrating a two-delay blowfly (TDB) model which is used to\nsimulate the population growth of Panonychus ulmi (Koch) or European red mites\nthat infest on apple leaves and diminish the yield. We focus on the inverse\nproblem, that is, to estimate the set of parameters/inputs of the TDB model\nthat produces the computer model output matching the field observation as\nclosely as possible. The time series nature of both the field observation and\nthe TDB outputs makes the inverse problem significantly more challenging than\nin the scalar valued simulator case.\n  In spirit, we follow the popular sequential design framework of computer\nexperiments. However, due to the time-series response, a singular value\ndecomposition based Gaussian process model is used for the surrogate model, and\nsubsequently, a new expected improvement criterion is developed for choosing\nthe follow-up points. We also propose a new criterion for extracting the\noptimal inverse solution from the final surrogate. Three simulated examples and\nthe real-life TDB calibration problem have been used to demonstrate higher\naccuracy of the proposed approach as compared to popular existing techniques.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2018 23:22:04 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Zhang", "Ru", ""], ["Lin", "Chunfang Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1811.00314", "submitter": "Shanshan Wang", "authors": "Tingting Huang, Gilbert Saporta, Huiwen Wang, Shanshan Wang", "title": "Spatial Functional Linear Model and its Estimation Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical functional linear regression model (FLM) and its extensions,\nwhich are based on the assumption that all individuals are mutually\nindependent, have been well studied and are used by many researchers. This\nindependence assumption is sometimes violated in practice, especially when data\nwith a network structure are collected in scientific disciplines including\nmarketing, sociology and spatial economics. However, relatively few studies\nhave examined the applications of FLM to data with network structures. We\npropose a novel spatial functional linear model (SFLM), that incorporates a\nspatial autoregressive parameter and a spatial weight matrix into FLM to\naccommodate spatial dependencies among individuals. The proposed model is\nrelatively flexible as it takes advantage of FLM in handling high-dimensional\ncovariates and spatial autoregressive (SAR) model in capturing network\ndependencies. We develop an estimation method based on functional principal\ncomponent analysis (FPCA) and maximum likelihood estimation. Simulation studies\nshow that our method performs as well as the FPCA-based method used with FLM\nwhen no network structure is present, and outperforms the latter when network\nstructure is present. A real weather data is also employed to demonstrate the\nutility of the SFLM.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 11:07:13 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Huang", "Tingting", ""], ["Saporta", "Gilbert", ""], ["Wang", "Huiwen", ""], ["Wang", "Shanshan", ""]]}, {"id": "1811.00450", "submitter": "Thomas Nagler", "authors": "Thomas Nagler", "title": "R friendly multi-threading in C++", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calling multi-threaded C++ code from R has its perils. Since the R\ninterpreter is single-threaded, one must not check for user interruptions or\nprint to the R console from multiple threads. One can, however, synchronize\nwith R from the main thread. The R package RcppThread (current version 1.0.0)\ncontains a header only C++ library for thread safe communication with R that\nexploits this fact. It includes C++ classes for threads, a thread pool, and\nparallel loops that routinely synchronize with R. This article explains the\npackage's functionality and gives examples of its usage. The synchronization\nmechanism may also apply to other threading frameworks. Benchmarks suggest\nthat, although synchronization causes overhead, the parallel abstractions of\nRcppThread are competitive with other popular libraries in typical scenarios\nencountered in statistical computing.\n", "versions": [{"version": "v1", "created": "Thu, 1 Nov 2018 15:44:19 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 23:15:38 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 09:35:58 GMT"}, {"version": "v4", "created": "Mon, 8 Feb 2021 10:05:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Nagler", "Thomas", ""]]}, {"id": "1811.00722", "submitter": "Masahiro Tanaka", "authors": "Masahiro Tanaka", "title": "Adaptive MCMC for Generalized Method of Moments with Many Moment\n  Conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalized method of moments (GMM) estimator is unreliable for a large\nnumber of moment conditions, that is, it is comparable, or larger than the\nsample size. While classical GMM literature proposes several provisions to this\nproblem, its Bayesian counterpart (i.e., Bayesian inference using a GMM\ncriterion as a quasi-likelihood) almost totally ignores it. This study bridges\nthis gap by proposing an adaptive Markov Chain Monte Carlo (MCMC) approach to a\nGMM inference with many moment conditions. Particularly, this study focuses on\nthe adaptive tuning of a weighting matrix on the fly. Our proposal consists of\ntwo elements. The first is the use of the nonparametric eigenvalue-regularized\nprecision matrix estimator, which contributes to numerical stability. The\nsecond is the random update of a weighting matrix, which substantially reduces\ncomputational cost, while maintaining the accuracy of the estimation. We then\npresent a simulation study and real data application to compare the performance\nof the proposed approach with existing approaches.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 03:11:14 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 12:52:10 GMT"}, {"version": "v3", "created": "Sat, 19 Oct 2019 16:08:40 GMT"}, {"version": "v4", "created": "Mon, 10 Feb 2020 03:05:24 GMT"}, {"version": "v5", "created": "Wed, 10 Mar 2021 10:01:27 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tanaka", "Masahiro", ""]]}, {"id": "1811.00782", "submitter": "Sofie P{\\o}denphant Jensen", "authors": "Sofie P{\\o}denphant, Kasper Kristensen, Per B. Brockhoff", "title": "The Multiplicative Mixed Model with the mumm R package as a General and\n  Easy Random Interaction Model Tool", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplicative mixed models can be applied in a wide range of scientific\ndisciplines, since they are relevant in every situation where an interaction\nbetween a fixed effect and a random effect is present. Until now, no R package\nhas been published, which can fit this type of models. The lack of\nuser-friendly open source tools to fit these models, is the main reason that\nthe models are not used as often as they could or should be. In this paper we\nintroduce the user-friendly R package $\\mathsf{mumm}$ for fitting\nmultiplicative mixed models in a time-efficient manner. To illustrate the\ninterpretation of the multiplicative term, we provide four data analysis\nexamples, where the model is fitted to data sets that stem from studies in\nsensometrics, agriculture and medicine. With these examples it is shown that\nthe statistical inference can be improved by using a multiplicative mixed\nmodel, instead of a linear mixed model which is usually employed.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 08:51:25 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["P\u00f8denphant", "Sofie", ""], ["Kristensen", "Kasper", ""], ["Brockhoff", "Per B.", ""]]}, {"id": "1811.00890", "submitter": "Maria I. Gorinova", "authors": "Maria I. Gorinova, Andrew D. Gordon, Charles Sutton", "title": "Probabilistic Programming with Densities in SlicStan: Efficient,\n  Flexible and Deterministic", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 3, POPL, Article 35 (January 2019)", "doi": "10.1145/3290348", "report-no": null, "categories": "cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stan is a probabilistic programming language that has been increasingly used\nfor real-world scalable projects. However, to make practical inference\npossible, the language sacrifices some of its usability by adopting a block\nsyntax, which lacks compositionality and flexible user-defined functions.\nMoreover, the semantics of the language has been mainly given in terms of\nintuition about implementation, and has not been formalised.\n  This paper provides a formal treatment of the Stan language, and introduces\nthe probabilistic programming language SlicStan --- a compositional,\nself-optimising version of Stan. Our main contributions are: (1) the\nformalisation of a core subset of Stan through an operational density-based\nsemantics; (2) the design and semantics of the Stan-like language SlicStan,\nwhich facilities better code reuse and abstraction through its compositional\nsyntax, more flexible functions, and information-flow type system; and (3) a\nformal, semantic-preserving procedure for translating SlicStan to Stan.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 14:34:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Gorinova", "Maria I.", ""], ["Gordon", "Andrew D.", ""], ["Sutton", "Charles", ""]]}, {"id": "1811.01091", "submitter": "Andrew Brown", "authors": "Arvind K. Saibaba, Johnathan Bardsley, D. Andrew Brown, Alen\n  Alexanderian", "title": "Efficient Marginalization-based MCMC Methods for Hierarchical Bayesian\n  Inverse Problems", "comments": "27 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models in Bayesian inverse problems are characterized by an\nassumed prior probability distribution for the unknown state and measurement\nerror precision, and hyper-priors for the prior parameters. Combining these\nprobability models using Bayes' law often yields a posterior distribution that\ncannot be sampled from directly, even for a linear model with Gaussian\nmeasurement error and Gaussian prior. Gibbs sampling can be used to sample from\nthe posterior, but problems arise when the dimension of the state is large.\nThis is because the Gaussian sample required for each iteration can be\nprohibitively expensive to compute, and because the statistical efficiency of\nthe Markov chain degrades as the dimension of the state increases. The latter\nproblem can be mitigated using marginalization-based techniques, but these can\nbe computationally prohibitive as well. In this paper, we combine the low-rank\ntechniques of Brown, Saibaba, and Vallelian (2018) with the marginalization\napproach of Rue and Held (2005). We consider two variants of this approach:\ndelayed acceptance and pseudo-marginalization. We provide a detailed analysis\nof the acceptance rates and computational costs associated with our proposed\nalgorithms, and compare their performances on two numerical test cases---image\ndeblurring and inverse heat equation.\n", "versions": [{"version": "v1", "created": "Fri, 2 Nov 2018 21:15:01 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 19:49:10 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Saibaba", "Arvind K.", ""], ["Bardsley", "Johnathan", ""], ["Brown", "D. Andrew", ""], ["Alexanderian", "Alen", ""]]}, {"id": "1811.02609", "submitter": "Raphael Small", "authors": "Raphael Small and Brent A. Coull", "title": "A Variational Inference Algorithm for BKMR in the Cross-Sectional\n  Setting", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of pollutant effects is an important task in environmental\nhealth. Bayesian kernel machine regression (BKMR) is a standard tool for\ninference of individual-level pollutant health-effects, and we present a mean\nfield Variational Inference (VI) algorithm for quick inference when only a\nsingle response per individual is recorded. Using simulation studies in the\ncase of informative priors, we show that VI, although fast, produces\nanti-conservative credible intervals of covariate effects and conservative\ncredible intervals for pollutant effects. To correct the coverage probabilities\nof covariate effects, we propose a simple Generalized Least Squares (GLS)\napproach that induces conservative credible intervals. We also explore using\nBKMR with flat priors and find that, while slower than the case with\ninformative priors, this approach yields uncorrected credible intervals for\ncovariate effects with coverage probabilities that are much closer to the\nnominal 95% level. We further note that fitting BKMR by VI provides a\nremarkable improvement in speed over existing MCMC methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 19:56:01 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Small", "Raphael", ""], ["Coull", "Brent A.", ""]]}, {"id": "1811.02612", "submitter": "Chao Gao", "authors": "Bumeng Zhuo and Chao Gao", "title": "Mixing Time of Metropolis-Hastings for Bayesian Community Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of a Metropolis-Hastings algorithm for\nBayesian community detection. We first establish a posterior strong consistency\nresult for a natural prior distribution on stochastic block models under the\noptimal signal-to-noise ratio condition in the literature. We then give a set\nof conditions that guarantee rapid mixing of a simple Metropolis-Hastings\nalgorithm. The mixing time analysis is based on a careful study of posterior\nratios and a canonical path argument to control the spectral gap of the Markov\nchain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Nov 2018 20:09:16 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Zhuo", "Bumeng", ""], ["Gao", "Chao", ""]]}, {"id": "1811.02963", "submitter": "Dao Nguyen", "authors": "Duc Anh Doan, Dao Nguyen and Xin Dang", "title": "Simulation-based inference methods for partially observed Markov model\n  via the R package is2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partially observed Markov process (POMP) models are powerful tools for time\nseries modeling and analysis. Inherited the flexible framework of R package\npomp, the is2 package extends some useful Monte Carlo statistical methodologies\nto improve on convergence rates. A variety of efficient statistical methods for\nPOMP models have been developed including fixed lag smoothing, second-order\niterated smoothing, momentum iterated filtering, average iterated filtering,\naccelerate iterated filtering and particle iterated filtering. In this paper,\nwe show the utility of these methodologies based on two toy problems. We also\ndemonstrate the potential of some methods in a more complex model, employing a\nnonlinear epidemiological model with a discrete population, seasonality, and\nextra-demographic stochasticity. We discuss the extension beyond POMP models\nand the development of additional methods within the framework provided by is2.\n", "versions": [{"version": "v1", "created": "Wed, 7 Nov 2018 16:29:22 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Doan", "Duc Anh", ""], ["Nguyen", "Dao", ""], ["Dang", "Xin", ""]]}, {"id": "1811.03204", "submitter": "Brian Axelrod", "authors": "Brian Axelrod, Gregory Valiant", "title": "An Efficient Algorithm for High-Dimensional Log-Concave Maximum\n  Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The log-concave maximum likelihood estimator (MLE) problem answers: for a set\nof points $X_1,...X_n \\in \\mathbb R^d$, which log-concave density maximizes\ntheir likelihood? We present a characterization of the log-concave MLE that\nleads to an algorithm with runtime $poly(n,d, \\frac 1 \\epsilon,r)$ to compute a\nlog-concave distribution whose log-likelihood is at most $\\epsilon$ less than\nthat of the MLE, and $r$ is parameter of the problem that is bounded by the\n$\\ell_2$ norm of the vector of log-likelihoods the MLE evaluated at\n$X_1,...,X_n$.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 01:04:51 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Axelrod", "Brian", ""], ["Valiant", "Gregory", ""]]}, {"id": "1811.03521", "submitter": "Joong-Ho Won", "authors": "Joong-Ho Won, Hua Zhou, Kenneth Lange", "title": "Orthogonal Trace-Sum Maximization: Applications, Local Algorithms, and\n  Global Optimality", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of maximizing the sum of traces of matrix\nquadratic forms on a product of Stiefel manifolds. This orthogonal trace-sum\nmaximization (OTSM) problem generalizes many interesting problems such as\ngeneralized canonical correlation analysis (CCA), Procrustes analysis, and\ncryo-electron microscopy of the Nobel prize fame. For these applications\nfinding global solutions is highly desirable but it has been unclear how to\nfind even a stationary point, let alone testing its global optimality. Through\na close inspection of Ky Fan's classical result (1949) on the variational\nformulation of the sum of largest eigenvalues of a symmetric matrix, and a\nsemidefinite programming (SDP) relaxation of the latter, we first provide a\nsimple method to certify global optimality of a given stationary point of OTSM.\nThis method only requires testing whether a symmetric matrix is positive\nsemidefinite. A by-product of this analysis is an unexpected strong duality\nbetween Shapiro-Botha (1988) and Zhang-Singer (2017). After showing that a\npopular algorithm for generalized CCA and Procrustes analysis may generate\noscillating iterates, we propose a simple fix that provably guarantees\nconvergence to a stationary point. The combination of our algorithm and\ncertificate reveals novel global optima of various instances of OTSM.\n", "versions": [{"version": "v1", "created": "Thu, 8 Nov 2018 16:11:02 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 14:13:16 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Won", "Joong-Ho", ""], ["Zhou", "Hua", ""], ["Lange", "Kenneth", ""]]}, {"id": "1811.03750", "submitter": "Jin Zhu", "authors": "Jin Zhu, Wenliang Pan, Wei Zheng, Xueqin Wang", "title": "Ball: An R package for detecting distribution difference and association\n  in metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of modern technology facilitates the appearance of\nnumerous unprecedented complex data which do not satisfy the axioms of\nEuclidean geometry, while most of the statistical hypothesis tests are\navailable in Euclidean or Hilbert spaces. To properly analyze the data of more\ncomplicated structures, efforts have been made to solve the fundamental test\nproblems in more general spaces. In this paper, a publicly available R package\nBall is provided to implement Ball statistical test procedures for K-sample\ndistribution comparison and test of mutual independence in metric spaces, which\nextend the test procedures for two sample distribution comparison and test of\nindependence. The tailormade algorithms as well as engineering techniques are\nemployed on the Ball package to speed up computation to the best of our\nability. Two real data analyses and several numerical studies have been\nperformed and the results certify the powerfulness of Ball package in analyzing\ncomplex data, e.g., spherical data and symmetric positive matrix data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Nov 2018 02:56:51 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2018 01:11:09 GMT"}, {"version": "v3", "created": "Tue, 3 Sep 2019 14:02:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Zhu", "Jin", ""], ["Pan", "Wenliang", ""], ["Zheng", "Wei", ""], ["Wang", "Xueqin", ""]]}, {"id": "1811.04249", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan and Nial Friel", "title": "Bayesian variational inference for exponential random graph models", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving Bayesian inference for exponential random graph models (ERGMs) is a\nchallenging \"doubly intractable\" problem as the normalizing constants of the\nlikelihood and posterior density are both intractable. Markov chain Monte Carlo\n(MCMC) methods which yield Bayesian inference for ERGMs, such as the exchange\nalgorithm, are asymptotically exact but computationally intensive, as a network\nhas to be drawn from the likelihood at every step using, for instance, a \"tie\nno tie\" sampler. In this article, we develop a variety of variational methods\nfor Gaussian approximation of the posterior density and model selection. These\ninclude nonconjugate variational message passing based on an adjusted\npseudolikelihood and stochastic variational inference. To overcome the\ncomputational hurdle of drawing a network from the likelihood at each\niteration, we propose stochastic gradient ascent with biased but consistent\ngradient estimates computed using adaptive self-normalized importance sampling.\nThese methods provide attractive fast alternatives to MCMC for posterior\napproximation. We illustrate the variational methods using real networks and\ncompare their accuracy with results obtained via MCMC and Laplace\napproximation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Nov 2018 13:02:43 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2018 07:53:55 GMT"}, {"version": "v3", "created": "Sat, 23 Nov 2019 23:11:02 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Friel", "Nial", ""]]}, {"id": "1811.04545", "submitter": "Cheng Wang", "authors": "Cheng Wang and Binyan Jiang", "title": "An efficient ADMM algorithm for high dimensional precision matrix\n  estimation via penalized quadratic loss", "comments": "18 pages, 2 tables and 3 figures", "journal-ref": "Computational Statistics & Data Analysis,2019", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of high dimensional precision matrices has been a central\ntopic in statistical learning. However, as the number of parameters scales\nquadratically with the dimension $p$, many state-of-the-art methods do not\nscale well to solve problems with a very large $p$. In this paper, we propose a\nvery efficient algorithm for precision matrix estimation via penalized\nquadratic loss functions. Under the high dimension low sample size setting, the\ncomputation complexity of our algorithm is linear in both the sample size and\nthe number of parameters. Such a computation complexity is in some sense\noptimal, as it is the same as the complexity needed for computing the sample\ncovariance matrix. Numerical studies show that our algorithm is much more\nefficient than other state-of-the-art methods when the dimension $p$ is very\nlarge.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 03:58:14 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 05:20:28 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Wang", "Cheng", ""], ["Jiang", "Binyan", ""]]}, {"id": "1811.04988", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky, Gianluca Geraci, Mike Eldred, John D. Jakeman", "title": "A Generalized Approximate Control Variate Framework for Multifidelity\n  Uncertainty Quantification", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109257", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and analyze a variance reduction approach for Monte Carlo (MC)\nsampling that accelerates the estimation of statistics of computationally\nexpensive simulation models using an ensemble of models with lower cost. These\nlower cost models --- which are typically lower fidelity with unknown\nstatistics --- are used to reduce the variance in statistical estimators\nrelative to a MC estimator with equivalent cost. We derive the conditions under\nwhich our proposed approximate control variate framework recovers existing\nmulti-model variance reduction schemes as special cases. We demonstrate that\nthese existing strategies use recursive sampling strategies, and as a result,\ntheir maximum possible variance reduction is limited to that of a control\nvariate algorithm that uses only a single low-fidelity model with known mean.\nThis theoretical result holds regardless of the number of low-fidelity models\nand/or samples used to build the estimator. We then derive new sampling\nstrategies within our framework that circumvent this limitation to make\nefficient use of all available information sources. In particular, we\ndemonstrate that a significant gap can exist, of orders of magnitude in some\ncases, between the variance reduction achievable by using a single low-fidelity\nmodel and our non-recursive approach. We also present initial sample allocation\napproaches for exploiting this gap. They yield the greatest benefit when\naugmenting the high-fidelity model evaluations is impractical because, for\ninstance, they arise from a legacy database. Several analytic examples and an\nexample with a hyperbolic PDE describing elastic wave propagation in\nheterogeneous media are used to illustrate the main features of the\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 20:18:09 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 18:47:52 GMT"}, {"version": "v3", "created": "Thu, 6 Jun 2019 22:51:49 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Geraci", "Gianluca", ""], ["Eldred", "Mike", ""], ["Jakeman", "John D.", ""]]}, {"id": "1811.05031", "submitter": "Charles Margossian", "authors": "Charles C. Margossian", "title": "A Review of automatic differentiation and its efficient implementation", "comments": "32 pages, 5 figures, submitted for publication. WIREs Data Mining\n  Knowl Discov, March 2019", "journal-ref": null, "doi": "10.1002/WIDM.1305", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Derivatives play a critical role in computational statistics, examples being\nBayesian inference using Hamiltonian Monte Carlo sampling and the training of\nneural networks. Automatic differentiation is a powerful tool to automate the\ncalculation of derivatives and is preferable to more traditional methods,\nespecially when differentiating complex algorithms and mathematical functions.\nThe implementation of automatic differentiation however requires some care to\ninsure efficiency. Modern differentiation packages deploy a broad range of\ncomputational techniques to improve applicability, run time, and memory\nmanagement. Among these techniques are operation overloading, region based\nmemory, and expression templates. There also exist several mathematical\ntechniques which can yield high performance gains when applied to complex\nalgorithms. For example, semi-analytical derivatives can reduce by orders of\nmagnitude the runtime required to numerically solve and differentiate an\nalgebraic equation. Open problems include the extension of current packages to\nprovide more specialized routines, and efficient methods to perform\nhigher-order differentiation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Nov 2018 22:52:46 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 11:37:12 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Margossian", "Charles C.", ""]]}, {"id": "1811.05061", "submitter": "Sangin Lee", "authors": "Dongshin Kim, Sangin Lee and Sunghoon Kwon", "title": "A unified algorithm for the non-convex penalized estimation: The ncpen\n  package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various R packages have been developed for the non-convex penalized\nestimation but they can only be applied to the smoothly clipped absolute\ndeviation (SCAD) or minimax concave penalty (MCP). We develop an R package,\nentitled ncpen, for the non-convex penalized estimation in order to make data\nanalysts to experience other non-convex penalties. The package ncpen implements\na unified algorithm based on the convex concave procedure and modified local\nquadratic approximation algorithm, which can be applied to a broader range of\nnon-convex penalties, including the SCAD and MCP as special examples. Many\nuser-friendly functionalities such as generalized information criteria,\ncross-validation and L2-stabilization are provided also.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 01:36:12 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kim", "Dongshin", ""], ["Lee", "Sangin", ""], ["Kwon", "Sunghoon", ""]]}, {"id": "1811.05073", "submitter": "Leah F. South", "authors": "Leah F. South and Chris J. Oates and Antonietta Mira and Christopher\n  Drovandi", "title": "Regularised Zero-Variance Control Variates for High-Dimensional Variance\n  Reduction", "comments": "21 pages plus 14 pages of appendices. The revised file has overall\n  efficiency including run-time and also some adjusted discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-variance control variates (ZV-CV) are a post-processing method to reduce\nthe variance of Monte Carlo estimators of expectations using the derivatives of\nthe log target. Once the derivatives are available, the only additional\ncomputational effort lies in solving a linear regression problem. Significant\nvariance reductions have been achieved with this method in low dimensional\nexamples, but the number of covariates in the regression rapidly increases with\nthe dimension of the target. In this paper, we present compelling empirical\nevidence that the use of penalised regression techniques in the selection of\nhigh-dimensional control variates provides performance gains over the classical\nleast squares method. Another type of regularisation based on using subsets of\nderivatives, or a priori regularisation as we refer to it in this paper, is\nalso proposed to reduce computational and storage requirements. Several\nexamples showing the utility and limitations of regularised ZV-CV for Bayesian\ninference are given. The methods proposed in this paper are accessible through\nthe R package ZVCV.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 02:22:07 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 04:03:03 GMT"}, {"version": "v3", "created": "Mon, 11 Nov 2019 18:37:43 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 17:35:14 GMT"}, {"version": "v5", "created": "Sun, 7 Jun 2020 13:27:56 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["South", "Leah F.", ""], ["Oates", "Chris J.", ""], ["Mira", "Antonietta", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1811.05494", "submitter": "Alvin Chua", "authors": "Alvin J. K. Chua", "title": "Sampling from manifold-restricted distributions using tangent bundle\n  projections", "comments": "Published version; Python implementation available at\n  https://github.com/alvincjk/sampling-manifold-restricted-gaussians", "journal-ref": "Stat. Comput. 30, 587 (2020)", "doi": "10.1007/s11222-019-09907-8", "report-no": null, "categories": "stat.CO astro-ph.IM gr-qc stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in Bayesian inference is the sampling of target probability\ndistributions at sufficient resolution and accuracy to estimate the probability\ndensity, and to compute credible regions. Often by construction, many target\ndistributions can be expressed as some higher-dimensional closed-form\ndistribution with parametrically constrained variables, i.e., one that is\nrestricted to a smooth submanifold of Euclidean space. I propose a\nderivative-based importance sampling framework for such distributions. A base\nset of $n$ samples from the target distribution is used to map out the tangent\nbundle of the manifold, and to seed $nm$ additional points that are projected\nonto the tangent bundle and weighted appropriately. The method essentially acts\nas an upsampling complement to any standard algorithm. It is designed for the\nefficient production of approximate high-resolution histograms from\nmanifold-restricted Gaussian distributions, and can provide large computational\nsavings when sampling directly from the target distribution is expensive.\n", "versions": [{"version": "v1", "created": "Tue, 13 Nov 2018 19:00:48 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2019 19:36:18 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 20:27:51 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Chua", "Alvin J. K.", ""]]}, {"id": "1811.05678", "submitter": "Berent {\\AA}nund Str{\\o}mnes Lunde", "authors": "Berent {\\AA}. S. Lunde, Tore S. Kleppe, Hans J. Skaug", "title": "Saddlepoint-adjusted inversion of characteristic functions", "comments": "V2: 20 pages; 5 figures; 1 table; Updated with discussion of\n  convergence acceleration", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For certain types of statistical models, the characteristic function (Fourier\ntransform) is available in closed form, whereas the probability density\nfunction has an intractable form, typically as an infinite sum of probability\nweighted densities. Important such examples include solutions of stochastic\ndifferential equations with jumps, the Tweedie model, and Poisson mixture\nmodels. We propose a novel and general numerical method for retrieving the\nprobability density function from the characteristic function, conditioned on\nthe existence of the moment generating function. Unlike methods based on direct\napplication of quadrature to the inverse Fourier transform, the proposed method\nallows accurate evaluation of the log-probability density function arbitrarily\nfar out in the tail. Moreover, unlike ordinary saddlepoint approximations, the\nproposed methodology is in principle exact modulus discretization and\ntruncation error of quadrature applied to inversion in a high-density region.\nOwing to these properties, the proposed method is computationally stable and\nvery accurate under log-likelihood optimisation. The method is illustrated for\na normal variance-mean mixture, and in an application of maximum likelihood\nestimation to a jump diffusion model for financial data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 08:10:49 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 08:08:01 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Lunde", "Berent \u00c5. S.", ""], ["Kleppe", "Tore S.", ""], ["Skaug", "Hans J.", ""]]}, {"id": "1811.05821", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Martin Leutbecher, Marianna Szab\\'o and Zied Ben\n  Bouall\\`egue", "title": "Statistical post-processing of dual-resolution ensemble forecasts", "comments": "25 pages, 12 figures, 2 tables", "journal-ref": "Quarterly Journal of the Royal Meteorological Society 145 (2019),\n  1705-1720", "doi": "10.1002/qj.3521", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost as well as the probabilistic skill of ensemble\nforecasts depends on the spatial resolution of the numerical weather prediction\nmodel and the ensemble size. Periodically, e.g. when more computational\nresources become available, it is appropriate to reassess the balance between\nresolution and ensemble size. Recently, it has been proposed to investigate\nthis balance in the context of dual-resolution ensembles, which use members\nwith two different resolutions to make probabilistic forecasts. This study\ninvestigates whether statistical post-processing of such dual-resolution\nensemble forecasts changes the conclusions regarding the optimal\ndual-resolution configuration.\n  Medium-range dual-resolution ensemble forecasts of 2-metre temperature have\nbeen calibrated using ensemble model output statistics. The forecasts are\nproduced with ECMWF's Integrated Forecast System and have horizontal\nresolutions between 18 km and 45 km. The ensemble sizes range from 8 to 254\nmembers. The forecasts are verified with SYNOP station data. Results show that\nscore differences between various single and dual-resolution configurations are\nstrongly reduced by statistical post-processing. Therefore, the benefit of some\ndual-resolution configurations over single resolution configurations appears to\nbe less pronounced than for raw forecasts. Moreover, the ranking of the\nensemble configurations can be affected by the statistical post-processing.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 14:55:54 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Leutbecher", "Martin", ""], ["Szab\u00f3", "Marianna", ""], ["Bouall\u00e8gue", "Zied Ben", ""]]}, {"id": "1811.05965", "submitter": "Eli Sennesh", "authors": "Eli Sennesh, Adam \\'Scibior, Hao Wu, Jan-Willem van de Meent", "title": "Composing Modeling and Inference Operations with Probabilistic Program\n  Combinators", "comments": "Published at the NeurIPS workshop \"All of Bayesian Nonparametrics\n  (Especially the Useful Bits)\" 2018\n  (https://sites.google.com/view/nipsbnp2018/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic programs with dynamic computation graphs can define measures\nover sample spaces with unbounded dimensionality, which constitute programmatic\nanalogues to Bayesian nonparametrics. Owing to the generality of this model\nclass, inference relies on `black-box' Monte Carlo methods that are often not\nable to take advantage of conditional independence and exchangeability, which\nhave historically been the cornerstones of efficient inference. We here seek to\ndevelop a `middle ground' between probabilistic models with fully dynamic and\nfully static computation graphs. To this end, we introduce a combinator library\nfor the Probabilistic Torch framework. Combinators are functions that accept\nmodels and return transformed models. We assume that models are dynamic, but\nthat model composition is static, in the sense that combinator application\ntakes place prior to evaluating the model on data. Combinators provide\nprimitives for both model and inference composition. Model combinators take the\nform of classic functional programming constructs such as map and reduce. These\nconstructs define a computation graph at a coarsened level of representation,\nin which nodes correspond to models, rather than individual variables.\nInference combinators implement operations such as importance resampling and\napplication of a transition kernel, which alter the evaluation strategy for a\nmodel whilst preserving proper weighting. Owing to this property, models\ndefined using combinators can be trained using stochastic methods that optimize\neither variational or wake-sleep style objectives. As a validation of this\nprinciple, we use combinators to implement black box inference for hidden\nMarkov models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 18:53:28 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 14:16:04 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 01:05:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Sennesh", "Eli", ""], ["\u015acibior", "Adam", ""], ["Wu", "Hao", ""], ["van de Meent", "Jan-Willem", ""]]}, {"id": "1811.06007", "submitter": "Anahita Nodehi", "authors": "Anahita Nodehi, Mousa Golalizadeh, Mehdi Maadooliat and Claudio\n  Agostinelli", "title": "Estimation of Multivariate Wrapped Models for Data in Torus", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate circular observations, i.e. points on a torus are nowadays very\ncommon. Multivariate wrapped models are often appropriate to describe data\npoints scattered on p-dimensional torus. However, statistical inference based\non this model is quite complicated since each contribution in the log\nlikelihood involve an infinite sum of indices in Z^p where p is the dimension\nof the problem. To overcome this, two estimates procedures based on Expectation\nMaximization and Classification Expectation Maximization algorithms are\nproposed that worked well in moderate dimension size. The performance of the\nintroduced methods are studied by Monte Carlo simulation and illustrated on\nthree real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Nov 2018 19:05:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Nodehi", "Anahita", ""], ["Golalizadeh", "Mousa", ""], ["Maadooliat", "Mehdi", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "1811.06150", "submitter": "David Moore", "authors": "Dave Moore and Maria I. Gorinova", "title": "Effect Handling for Composable Program Transformations in Edward2", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algebraic effects and handlers have emerged in the programming languages\ncommunity as a convenient, modular abstraction for controlling computational\neffects. They have found several applications including concurrent programming,\nmeta programming, and more recently, probabilistic programming, as part of\nPyro's Poutines library. We investigate the use of effect handlers as a\nlightweight abstraction for implementing probabilistic programming languages\n(PPLs). We interpret the existing design of Edward2 as an accidental\nimplementation of an effect-handling mechanism, and extend that design to\nsupport nested, composable transformations. We demonstrate that this enables\nstraightforward implementation of sophisticated model transformations and\ninference algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 02:51:29 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Moore", "Dave", ""], ["Gorinova", "Maria I.", ""]]}, {"id": "1811.06289", "submitter": "Charles-Edouard Br\\'ehier", "authors": "Charles-Edouard Br\\'ehier and Tony Leli\\`evre", "title": "On a new class of score functions to estimate tail probabilities of some\n  stochastic processes with Adaptive Multilevel Splitting", "comments": null, "journal-ref": null, "doi": "10.1063/1.5081440", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the application of the Adaptive Multilevel Splitting algorithm\nfor the estimation of tail probabilities of solutions of Stochastic\nDifferential Equations evaluated at a given time, and of associated temporal\naverages.\n  We introduce a new, very general and effective family of score functions\nwhich is designed for these problems. We illustrate its behavior on a series of\nnumerical experiments. In particular, we demonstrate how it can be used to\nestimate large deviation rate functionals for the longtime limit of temporal\naverages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 10:44:59 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Br\u00e9hier", "Charles-Edouard", ""], ["Leli\u00e8vre", "Tony", ""]]}, {"id": "1811.06601", "submitter": "Shyamalendu Sinha", "authors": "Shyamalendu Sinha and Jeffrey D. Hart", "title": "Estimating the Mean and Variance of a High-dimensional Normal\n  Distribution Using a Mixture Prior", "comments": null, "journal-ref": "Computational Statistics and Data Analysis 138 (2019) 201-221", "doi": "10.1016/j.csda.2019.04.006", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a framework for estimating the mean and variance of a\nhigh-dimensional normal density. The main setting considered is a fixed number\nof vector following a high-dimensional normal distribution with unknown mean\nand diagonal covariance matrix. The diagonal covariance matrix can be known or\nunknown. If the covariance matrix is unknown, the sample size can be as small\nas $2$. The proposed estimator is based on the idea that the unobserved pairs\nof mean and variance for each dimension are drawn from an unknown bivariate\ndistribution, which we model as a mixture of normal-inverse gammas. The mixture\nof normal-inverse gamma distributions provides advantages over more traditional\nempirical Bayes methods, which are based on a normal-normal model. When fitting\na mixture model, we are essentially clustering the unobserved mean and variance\npairs for each dimension into different groups, with each group having a\ndifferent normal-inverse gamma distribution. The proposed estimator of each\nmean is the posterior mean of shrinkage estimates, each of which shrinks a\nsample mean towards a different component of the mixture distribution.\nSimilarly, the proposed estimator of variance has an analogous interpretation\nin terms of sample variances and components of the mixture distribution. If\ndiagonal covariance matrix is known, then the sample size can be as small as\n$1$, and we treat the pairs of known variance and unknown mean for each\ndimension as random observations coming from a flexible mixture of\nnormal-inverse gamma distributions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 21:49:40 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Sinha", "Shyamalendu", ""], ["Hart", "Jeffrey D.", ""]]}, {"id": "1811.07186", "submitter": "Prateek Jaiswal", "authors": "Prateek Jaiswal, Harsha Honnappa, and Raghu Pasupathy", "title": "Optimal Allocations for Sample Average Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single stage stochastic program without recourse with a\nstrictly convex loss function. We assume a compact decision space and grid it\nwith a finite set of points. In addition, we assume that the decision maker can\ngenerate samples of the stochastic variable independently at each grid point\nand form a sample average approximation (SAA) of the stochastic program. Our\nobjective in this paper is to characterize an asymptotically optimal linear\nsample allocation rule, given a fixed sampling budget, which maximizes the\ndecay rate of probability of making false decision.\n", "versions": [{"version": "v1", "created": "Sat, 17 Nov 2018 16:48:24 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Jaiswal", "Prateek", ""], ["Honnappa", "Harsha", ""], ["Pasupathy", "Raghu", ""]]}, {"id": "1811.07546", "submitter": "Takashi Goda", "authors": "Takashi Goda, Tomohiko Hironaka, Takeru Iwamoto", "title": "Multilevel Monte Carlo estimation of expected information gains", "comments": null, "journal-ref": "Stochastic Analysis and Applications, Volume 38, Issue 4, 581-600,\n  2020", "doi": "10.1080/07362994.2019.1705168", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected information gain is an important quality criterion of Bayesian\nexperimental designs, which measures how much the information entropy about\nuncertain quantity of interest $\\theta$ is reduced on average by collecting\nrelevant data $Y$. However, estimating the expected information gain has been\nconsidered computationally challenging since it is defined as a nested\nexpectation with an outer expectation with respect to $Y$ and an inner\nexpectation with respect to $\\theta$. In fact, the standard, nested Monte Carlo\nmethod requires a total computational cost of $O(\\varepsilon^{-3})$ to achieve\na root-mean-square accuracy of $\\varepsilon$. In this paper we develop an\nefficient algorithm to estimate the expected information gain by applying a\nmultilevel Monte Carlo (MLMC) method. To be precise, we introduce an antithetic\nMLMC estimator for the expected information gain and provide a sufficient\ncondition on the data model under which the antithetic property of the MLMC\nestimator is well exploited such that optimal complexity of\n$O(\\varepsilon^{-2})$ is achieved. Furthermore, we discuss how to incorporate\nimportance sampling techniques within the MLMC estimator to avoid arithmetic\nunderflow. Numerical experiments show the considerable computational cost\nsavings compared to the nested Monte Carlo method for a simple test case and a\nmore realistic pharmacokinetic model.\n", "versions": [{"version": "v1", "created": "Mon, 19 Nov 2018 08:11:28 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 05:09:04 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 08:29:38 GMT"}, {"version": "v4", "created": "Mon, 7 Oct 2019 03:34:12 GMT"}, {"version": "v5", "created": "Fri, 6 Dec 2019 03:38:05 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Goda", "Takashi", ""], ["Hironaka", "Tomohiko", ""], ["Iwamoto", "Takeru", ""]]}, {"id": "1811.07715", "submitter": "Ying Wai Li", "authors": "Alfred C. K. Farris, Ying Wai Li, Markus Eisenbach", "title": "Histogram-Free Multicanonical Monte Carlo Sampling to Calculate the\n  Density of States", "comments": "11 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1707.07049", "journal-ref": "Comput. Phys. Comm. 235, 297-304 (2019)", "doi": "10.1016/j.cpc.2018.09.025", "report-no": null, "categories": "physics.comp-ph cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a new multicanonical Monte Carlo algorithm to obtain the density of\nstates for physical systems with continuous state variables in statistical\nmechanics. Our algorithm is able to obtain a closed-form expression for the\ndensity of states expressed in a chosen basis set, instead of a numerical array\nof finite resolution as in previous variants of this class of MC methods such\nas the multicanonical sampling and Wang-Landau sampling. This is enabled by\nstoring the visited states directly and avoiding the explicit collection of a\nhistogram. This practice also has the advantage of avoiding undesirable\nartificial errors caused by the discretization and binning of continuous state\nvariables. Our results show that this scheme is capable of obtaining converged\nresults with a much reduced number of Monte Carlo steps, leading to a\nsignificant speedup over existing algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 15 Nov 2018 20:26:43 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Farris", "Alfred C. K.", ""], ["Li", "Ying Wai", ""], ["Eisenbach", "Markus", ""]]}, {"id": "1811.08566", "submitter": "Bradley Eck", "authors": "Bei Chen and Bradley Eck and Francesco Fusco and Robert Gormally and\n  Mark Purcell and Mathieu Sinn and Seshu Tirupathi", "title": "Castor: Contextual IoT Time Series Data and Model Management at Scale", "comments": "6 pages, 6 figures, ICDM 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate Castor, a cloud-based system for contextual IoT time series\ndata and model management at scale. Castor is designed to assist Data\nScientists in (a) exploring and retrieving all relevant time series and\ncontextual information that is required for their predictive modelling tasks;\n(b) seamlessly storing and deploying their predictive models in a cloud\nproduction environment; (c) monitoring the performance of all predictive models\nin production and (semi-)automatically retraining them in case of performance\ndeterioration. The main features of Castor are: (1) an efficient pipeline for\ningesting IoT time series data in real time; (2) a scalable, hybrid data\nmanagement service for both time series and contextual data; (3) a versatile\nsemantic model for contextual information which can be easily adopted to\ndifferent application domains; (4) an abstract framework for developing and\nstoring predictive models in R or Python; (5) deployment services which\nautomatically train and/or score predictive models upon user-defined\nconditions. We demonstrate Castor for a real-world Smart Grid use case and\ndiscuss how it can be adopted to other application domains such as Smart\nBuildings, Telecommunication, Retail or Manufacturing.\n", "versions": [{"version": "v1", "created": "Tue, 20 Nov 2018 11:26:16 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 14:04:23 GMT"}, {"version": "v3", "created": "Fri, 8 Feb 2019 13:26:19 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Chen", "Bei", ""], ["Eck", "Bradley", ""], ["Fusco", "Francesco", ""], ["Gormally", "Robert", ""], ["Purcell", "Mark", ""], ["Sinn", "Mathieu", ""], ["Tirupathi", "Seshu", ""]]}, {"id": "1811.08595", "submitter": "Vahid Tadayon", "authors": "Vahid Tadayon", "title": "On an Extension of Stochastic Approximation EM Algorithm for Incomplete\n  Data Problems", "comments": "14th Iranian Statistics Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Stochastic Approximation EM (SAEM) algorithm, a variant stochastic\napproximation of EM, is a versatile tool for inference in incomplete data\nmodels. In this paper, we review the fundamental EM algorithm and then focus\nespecially on the stochastic version of EM. In order to construct the SAEM, the\nalgorithm combines EM with a variant of stochastic approximation that uses\nMarkov chain Monte-Carlo to deal with the missing data. The algorithm is\nintroduced in general form and can be used to a wide range of problems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 04:23:45 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 19:30:57 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tadayon", "Vahid", ""]]}, {"id": "1811.08820", "submitter": "\\'Angel F. Garc\\'ia-Fern\\'andez", "authors": "\\'Angel F. Garc\\'ia-Fern\\'andez, Lennart Svensson", "title": "Trajectory PHD and CPHD filters", "comments": "MATLAB implementations are provided here:\n  https://github.com/Agarciafernandez/MTT", "journal-ref": "In IEEE Transactions on Signal Processing, vol. 67, no. 22, pp.\n  5702-5714, Nov. 2019", "doi": "10.1109/TSP.2019.2943234", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the probability hypothesis density filter (PHD) and the\ncardinality PHD (CPHD) filter for sets of trajectories, which are referred to\nas the trajectory PHD (TPHD) and trajectory CPHD (TCPHD) filters. Contrary to\nthe PHD/CPHD filters, the TPHD/TCPHD filters are able to produce trajectory\nestimates from first principles. The TPHD filter is derived by recursively\nobtaining the best Poisson multitrajectory density approximation to the\nposterior density over the alive trajectories by minimising the\nKullback-Leibler divergence. The TCPHD is derived in the same way but\npropagating an independent identically distributed (IID) cluster\nmultitrajectory density approximation. We also propose the Gaussian mixture\nimplementations of the TPHD and TCPHD recursions, the Gaussian mixture TPHD\n(GMTPHD) and the Gaussian mixture TCPHD (GMTCPHD), and the L-scan\ncomputationally efficient implementations, which only update the density of the\ntrajectory states of the last L time steps.\n", "versions": [{"version": "v1", "created": "Wed, 21 Nov 2018 16:48:18 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 09:28:32 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 12:43:38 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""], ["Svensson", "Lennart", ""]]}, {"id": "1811.09436", "submitter": "JInglai Li", "authors": "Tengchao Yu, Linjun Lu, Jinglai Li", "title": "A weight-bounded importance sampling method for variance reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling (IS) is an important technique to reduce the estimation\nvariance in Monte Carlo simulations. In many practical problems, however, the\nuse of IS method may result in unbounded variance, and thus fail to provide\nreliable estimates. To address the issue, we propose a method which can prevent\nthe risk of unbounded variance; the proposed method performs the standard IS\nfor the integral of interest in a region only in which the IS weight is bounded\nand use the result as an approximation to the original integral. It can be\nverified that the resulting estimator has a finite variance. Moreover, we also\nprovide a normality test based method to identify the region with bounded IS\nweight (termed as the safe region) from the samples drawn from the standard IS\ndistribution. With numerical examples, we demonstrate that the proposed method\ncan yield rather reliable estimate when the standard IS fails, and it also\noutperforms the defensive IS, a popular method to prevent unbounded variance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 11:36:57 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 22:48:33 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Yu", "Tengchao", ""], ["Lu", "Linjun", ""], ["Li", "Jinglai", ""]]}, {"id": "1811.09446", "submitter": "Richard Brown", "authors": "Richard D. Brown, Johnathan M. Bardsley, and Tiangang Cui", "title": "Semivariogram methods for modeling Whittle-Mat\\'ern priors in Bayesian\n  inverse problems", "comments": null, "journal-ref": "Inverse Problems 36 055006 (2020)", "doi": "10.1088/1361-6420/ab762e", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new technique, based on semivariogram methodology, for obtaining\npoint estimates for use in prior modeling for solving Bayesian inverse\nproblems. This method requires a connection between Gaussian processes with\ncovariance operators defined by the Mat\\'ern covariance function and Gaussian\nprocesses with precision (inverse-covariance) operators defined by the Green's\nfunctions of a class of elliptic stochastic partial differential equations\n(SPDEs). We present a detailed mathematical description of this connection. We\nwill show that there is an equivalence between these two Gaussian processes\nwhen the domain is infinite -- for us, $\\mathbb{R}^2$ -- which breaks down when\nthe domain is finite due to the effect of boundary conditions on Green's\nfunctions of PDEs. We show how this connection can be re-established using\nextended domains. We then introduce the semivariogram method for estimating the\nMat\\'ern covariance parameters, which specify the Gaussian prior needed for\nstabilizing the inverse problem. Results are extended from the isotropic case\nto the anisotropic case where the correlation length in one direction is larger\nthan another. Finally, we consider the situation where the correlation length\nis spatially dependent rather than constant. We implement each method in\ntwo-dimensional image inpainting test cases to show that it works on practical\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 12:12:46 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 23:59:55 GMT"}, {"version": "v3", "created": "Sat, 9 May 2020 21:32:30 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Brown", "Richard D.", ""], ["Bardsley", "Johnathan M.", ""], ["Cui", "Tiangang", ""]]}, {"id": "1811.09469", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Dan Crisan, Joaqu\\'in M\\'iguez", "title": "Parallel sequential Monte Carlo for stochastic gradient-free nonconvex\n  optimization", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and analyze a parallel sequential Monte Carlo methodology for\nthe numerical solution of optimization problems that involve the minimization\nof a cost function that consists of the sum of many individual components. The\nproposed scheme is a stochastic zeroth order optimization algorithm which\ndemands only the capability to evaluate small subsets of components of the cost\nfunction. It can be depicted as a bank of samplers that generate particle\napproximations of several sequences of probability measures. These measures are\nconstructed in such a way that they have associated probability density\nfunctions whose global maxima coincide with the global minima of the original\ncost function. The algorithm selects the best performing sampler and uses it to\napproximate a global minimum of the cost function. We prove analytically that\nthe resulting estimator converges to a global minimum of the cost function\nalmost surely and provide explicit convergence rates in terms of the number of\ngenerated Monte Carlo samples. We show, by way of numerical examples, that the\nalgorithm can tackle cost functions with multiple minima or with broad \"flat\"\nregions which are hard to minimize using gradient-based techniques.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 13:33:08 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 21:49:27 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 14:25:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Crisan", "Dan", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1811.09550", "submitter": "Thomas Prescott", "authors": "Thomas P Prescott and Ruth E Baker", "title": "Multifidelity Approximate Bayesian Computation", "comments": "25 pages plus Supplementary Material (as appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vital stage in the mathematical modelling of real-world systems is to\ncalibrate a model's parameters to observed data. Likelihood-free parameter\ninference methods, such as Approximate Bayesian Computation, build Monte Carlo\nsamples of the uncertain parameter distribution by comparing the data with\nlarge numbers of model simulations. However, the computational expense of\ngenerating these simulations forms a significant bottleneck in the practical\napplication of such methods. We identify how simulations of cheap, low-fidelity\nmodels have been used separately in two complementary ways to reduce the\ncomputational expense of building these samples, at the cost of introducing\nadditional variance to the resulting parameter estimates. We explore how these\napproaches can be unified so that cost and benefit are optimally balanced, and\nwe characterise the optimal choice of how often to simulate from cheap,\nlow-fidelity models in place of expensive, high-fidelity models in Monte Carlo\nABC algorithms. The resulting early accept/reject multifidelity ABC algorithm\nthat we propose is shown to give improved performance over existing\nmultifidelity and high-fidelity approaches.\n", "versions": [{"version": "v1", "created": "Fri, 23 Nov 2018 16:40:31 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 11:02:41 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Prescott", "Thomas P", ""], ["Baker", "Ruth E", ""]]}, {"id": "1811.09747", "submitter": "Ari Pakman", "authors": "Ari Pakman and Liam Paninski", "title": "Amortized Bayesian inference for clustering models", "comments": "Presented at BNP@NeurIPS 2018 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for efficient amortized approximate Bayesian inference\nover posterior distributions of probabilistic clustering models, such as\nDirichlet process mixture models. The approach is based on mapping distributed,\nsymmetry-invariant representations of cluster arrangements into conditional\nprobabilities. The method parallelizes easily, yields iid samples from the\napproximate posterior of cluster assignments with the same computational cost\nof a single Gibbs sampler sweep, and can easily be applied to both conjugate\nand non-conjugate models, as training only requires samples from the generative\nmodel.\n", "versions": [{"version": "v1", "created": "Sat, 24 Nov 2018 02:17:20 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1811.10192", "submitter": "He Zhou", "authors": "He Zhou, Yi Yang and Wei Qian", "title": "Tweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie's compound Poisson model is a popular method to model insurance\nclaims with probability mass at zero and nonnegative, highly right-skewed\ndistribution. In particular, it is not uncommon to have extremely unbalanced\ndata with excessively large proportion of zero claims, and even traditional\nTweedie model may not be satisfactory for fitting the data. In this paper, we\npropose a boosting-assisted zero-inflated Tweedie model, called EMTboost, that\nallows zero probability mass to exceed a traditional model. We makes a\nnonparametric assumption on its Tweedie model component, that unlike a linear\nmodel, is able to capture nonlinearities, discontinuities, and complex higher\norder interactions among predictors. A specialized Expectation-Maximization\nalgorithm is developed that integrates a blockwise coordinate descent strategy\nand a gradient tree-boosting algorithm to estimate key model parameters. We use\nextensive simulation and data analysis on synthetic zero-inflated\nauto-insurance claim data to illustrate our method's prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 06:11:34 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 00:22:24 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Zhou", "He", ""], ["Yang", "Yi", ""], ["Qian", "Wei", ""]]}, {"id": "1811.10275", "submitter": "Francois-Xavier Briol", "authors": "Francois-Xavier Briol, Chris J. Oates, Mark Girolami, Michael A.\n  Osborne, Dino Sejdinovic", "title": "Rejoinder for \"Probabilistic Integration: A Role in Statistical\n  Computation?\"", "comments": "Accepted to Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is the rejoinder for the paper \"Probabilistic Integration: A\nRole in Statistical Computation?\" to appear in Statistical Science with\ndiscussion. We would first like to thank the reviewers and many of our\ncolleagues who helped shape this paper, the editor for selecting our paper for\ndiscussion, and of course all of the discussants for their thoughtful,\ninsightful and constructive comments. In this rejoinder, we respond to some of\nthe points raised by the discussants and comment further on the fundamental\nquestions underlying the paper: (i) Should Bayesian ideas be used in numerical\nanalysis?, and (ii) If so, what role should such approaches have in statistical\ncomputation?\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 10:30:38 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Briol", "Francois-Xavier", ""], ["Oates", "Chris J.", ""], ["Girolami", "Mark", ""], ["Osborne", "Michael A.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1811.11025", "submitter": "Wenying Deng", "authors": "Wenying Deng, Jeremiah Zhe Liu, Erin Lake, Brent A. Coull", "title": "CVEK: Robust Estimation and Testing for Nonlinear Effects using Kernel\n  Machine Ensemble", "comments": "5 figures. arXiv admin note: text overlap with arXiv:1710.01406", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package CVEK introduces a suite of flexible machine learning models and\nrobust hypothesis tests for learning the joint nonlinear effects of multiple\ncovariates in limited samples. It implements the Cross-validated Ensemble of\nKernels (CVEK)(Liu and Coull 2017), an ensemble-based kernel machine learning\nmethod that adaptively learns the joint nonlinear effect of multiple covariates\nfrom data, and provides powerful hypothesis tests for both main effects of\nfeatures and interactions among features. The R Package CVEK provides a\nflexible, easy-to-use implementation of CVEK, and offers a wide range of\nchoices for the kernel family (for instance, polynomial, radial basis\nfunctions, Mat\\'ern, neural network, and others), model selection criteria,\nensembling method (averaging, exponential weighting, cross-validated stacking),\nand the type of hypothesis test (asymptotic or parametric bootstrap). Through\nextensive simulations we demonstrate the validity and robustness of this\napproach, and provide practical guidelines on how to design an estimation\nstrategy for optimal performance in different data scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Nov 2018 09:20:47 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 23:04:02 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Deng", "Wenying", ""], ["Liu", "Jeremiah Zhe", ""], ["Lake", "Erin", ""], ["Coull", "Brent A.", ""]]}, {"id": "1811.11031", "submitter": "Eliane Pinheiro", "authors": "Eliane C. Pinheiro, Silvia L.P. Ferrari and Francisco M.C. Medeiros", "title": "Higher-order approximate confidence intervals", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard confidence intervals employed in applied statistical analysis are\nusually based on asymptotic approximations. Such approximations can be\nconsiderably inaccurate in small and moderate sized samples. We derive accurate\nconfidence intervals based on higher-order approximate quantiles of the score\nfunction. The coverage approximation error is $O(n^{-3/2})$ while the\napproximation error of confidence intervals based on the asymptotic normality\nof MLEs is $O(n^{-1/2})$. Monte Carlo simulations confirm the theoretical\nfindings. An implementation for regression models and real data applications\nare provided.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 14:46:11 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 14:37:08 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 21:55:13 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Pinheiro", "Eliane C.", ""], ["Ferrari", "Silvia L. P.", ""], ["Medeiros", "Francisco M. C.", ""]]}, {"id": "1811.11302", "submitter": "Immanuel Manohar", "authors": "Immanuel Manohar", "title": "A QR Decomposition Approach to Factor Modelling: A Thesis Report", "comments": "Master's thesis, 2014. This is the complete extended version of the\n  paper with complete information to the corresponding paper: \"A QR\n  Decomposition Approach to Factor Modelling\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An observed $K$-dimensional series $\\left\\{ y_{n}\\right\\} _{n=1}^{N}$ is\nexpressed in terms of a lower $p$-dimensional latent series called factors\n$f_{n}$ and random noise $\\varepsilon_{n}$. The equation,\n$y_{n}=Qf_{n}+\\varepsilon_{n}$ is taken to relate the factors with the\nobservation. The goal is to determine the dimension of the factors, $p$, the\nfactor loading matrix, $Q$, and the factors $f_{n}$. Here, it is assumed that\nthe noise co-variance is positive definite and allowed to be correlated with\nthe factors. An augmented matrix, \\[\n\\tilde{M}\\triangleq\\left[\\begin{array}{cccc} \\tilde{\\Sigma}_{yy}(1) &\n\\tilde{\\Sigma}_{yy}(2) & \\ldots & \\tilde{\\Sigma}_{yy}(m)\\end{array}\\right] \\]\nis formed using the observed sample autocovariances\n$\\tilde{\\Sigma}_{yy}(l)=\\frac{1}{N-l}\\sum_{n=1}^{N-l}\\left(y_{n+l}-\\bar{y}\\right)\\left(y_{n}-\\bar{y}\\right)^{\\top}$,\n$\\bar{y}=\\frac{1}{N}\\sum_{n=1}^{N}y_{n}$. Estimating $p$ is equated to\ndetermining the numerical rank of $\\tilde{M}$. Using Rank Revealing QR (RRQR)\ndecomposition, a model order detection scheme is proposed for determining the\nnumerical rank and for estimating the loading matrix $Q$. The rate of\nconvergence of the estimates, as $K$ and $N$ tends to infinity, is derived and\ncompared with that of the existing Eigen Value Decomposition based approach.\nTwo applications of this algorithm, i) The problem of extracting signals from\ntheir noisy mixtures and ii) modelling of the S&P index are presented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 23:02:19 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Manohar", "Immanuel", ""]]}, {"id": "1811.11394", "submitter": "Shaomeng Qin", "authors": "Shao-Meng Qin", "title": "Spin-glass model for the C-dismantling problem", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevE.98.062309", "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C-dismantling (CD) problem aims at finding the minimum vertex set D of a\ngraph G(V,E) after removing which the remaining graph will break into connected\ncomponents with the size not larger than C. In this paper, we introduce a\nspin-glass model with C+1 integer-value states into the CD problem and then\nstudy the properties of this spin-glass model by the belief-propagation (BP)\nequations under the replica-symmetry ansatz. We give the lower bound $\\rho_c$\nof the relative size of D with finite C on regular random graphs and\nErdos-Renyi random graphs. We find $\\rho_c$ will decrease gradually with\ngrowing C and it converges to $\\rho_\\infty$ as C$\\to\\infty$. The CD problem is\ncalled dismantling problem when C is a small finite fraction of |V|. Therefore,\n$\\rho_\\infty$ is also the lower bound of the dismantling problem when\n|V|$\\to\\infty$. To reduce the computation complexity of the BP equations,\ntaking the knowledge of the probability of a random selected vertex belonging\nto a remaining connected component with the size A, the original BP equations\ncan be simplified to one with only three states when C$\\to\\infty$. The\nsimplified BP equations are very similar to the BP equations of the feedback\nvertex set spin-glass model [H.-J.~Zhou, Eur. Phys. J. B 86, 455 (2013)]. At\nlast, we develop two practical belief-propagation-guide decimation algorithms\nbased on the original BP equations (CD-BPD) and the simplified BP equations\n(SCD-BPD) to solve the CD problem on a certain graph. Our BPD algorithms and\ntwo other state-of-art heuristic algorithms are applied on various random\ngraphs and some real world networks. Computation results show that the CD-BPD\nis the best in all tested algorithms in the case of small C. But considering\nthe performance and computation consumption, we recommend using SCD-BPD for the\nnetwork with small clustering coefficient when C is large.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 05:42:24 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Qin", "Shao-Meng", ""]]}, {"id": "1811.11733", "submitter": "Ruoqing Zhu", "authors": "Ruoqing Zhu, Jiyang Zhang, Ruilin Zhao, Peng Xu, Wenzhuo Zhou and Xin\n  Zhang", "title": "orthoDr: Semiparametric Dimension Reduction via Orthogonality\n  Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  orthoDr is a package in R that solves dimension reduction problems using\northogonality constrained optimization approach. The package serves as a\nunified framework for many regression and survival analysis dimension reduction\nmodels that utilize semiparametric estimating equations. The main computational\nmachinery of orthoDr is a first-order algorithm developed by\n\\cite{wen2013feasible} for optimization within the Stiefel manifold. We\nimplement the algorithm through Rcpp and OpenMP for fast computation. In\naddition, we developed a general-purpose solver for such constrained problems\nwith user-specified objective functions, which works as a drop-in version of\noptim(). The package also serves as a platform for future methodology\ndevelopments along this line of work.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 18:34:56 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 07:21:27 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Zhu", "Ruoqing", ""], ["Zhang", "Jiyang", ""], ["Zhao", "Ruilin", ""], ["Xu", "Peng", ""], ["Zhou", "Wenzhuo", ""], ["Zhang", "Xin", ""]]}, {"id": "1811.11804", "submitter": "Vladimir Minin", "authors": "Mathieu Fourment, Andrew F. Magee, Chris Whidden, Arman Bilge,\n  Frederick A. Matsen IV, Vladimir N. Minin", "title": "19 dubious ways to compute the marginal likelihood of a phylogenetic\n  tree topology", "comments": "37 pages, 5 figures and 1 table in main text, plus supplementary\n  materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood of a model is a key quantity for assessing the\nevidence provided by the data in support of a model. The marginal likelihood is\nthe normalizing constant for the posterior density, obtained by integrating the\nproduct of the likelihood and the prior with respect to model parameters. Thus,\nthe computational burden of computing the marginal likelihood scales with the\ndimension of the parameter space. In phylogenetics, where we work with tree\ntopologies that are high-dimensional models, standard approaches to computing\nmarginal likelihoods are very slow. Here we study methods to quickly compute\nthe marginal likelihood of a single fixed tree topology. We benchmark the speed\nand accuracy of 19 different methods to compute the marginal likelihood of\nphylogenetic topologies on a suite of real datasets. These methods include\nseveral new ones that we develop explicitly to solve this problem, as well as\nexisting algorithms that we apply to phylogenetic models for the first time.\nAltogether, our results show that the accuracy of these methods varies widely,\nand that accuracy does not necessarily correlate with computational burden. Our\nnewly developed methods are orders of magnitude faster than standard\napproaches, and in some cases, their accuracy rivals the best established\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 19:59:03 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Fourment", "Mathieu", ""], ["Magee", "Andrew F.", ""], ["Whidden", "Chris", ""], ["Bilge", "Arman", ""], ["Matsen", "Frederick A.", "IV"], ["Minin", "Vladimir N.", ""]]}, {"id": "1811.11896", "submitter": "Pai Liu", "authors": "Pai Liu, Jingwei Gan, and Rajan K. Chakrabarty", "title": "Variational Autoencoding the Lagrangian Trajectories of Particles in a\n  Combustion System", "comments": "2nd version: typo corrected, corresponding author changed 19 pages, 9\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep learning method to simulate the motion of particles\ntrapped in a chaotic recirculating flame. The Lagrangian trajectories of\nparticles, captured using a high-speed camera and subsequently reconstructed in\n3-dimensional space, were used to train a variational autoencoder (VAE) which\ncomprises multiple layers of convolutional neural networks. We show that the\ntrajectories, which are statistically representative of those determined in\nexperiments, can be generated using the VAE network. The performance of our\nmodel is evaluated with respect to the accuracy and generalization of the\noutputs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 00:44:58 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 03:18:25 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Liu", "Pai", ""], ["Gan", "Jingwei", ""], ["Chakrabarty", "Rajan K.", ""]]}, {"id": "1811.12602", "submitter": "Hossein Keshavarz", "authors": "Hossein Keshavarz, XuanLong Nguyen and Clayton Scott", "title": "Local inversion-free estimation of spatial Gaussian processes", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": "10.13140/RG.2.2.17553.17764", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximizing the likelihood has been widely used for estimating the unknown\ncovariance parameters of spatial Gaussian processes. However, evaluating and\noptimizing the likelihood function can be computationally intractable,\nparticularly for large number of (possibly) irregularly spaced observations,\ndue to the need to handle the inverse of ill-conditioned and large covariance\nmatrices. Extending the \"inversion-free\" method of Anitescu, Chen and Stein\n\\cite{anitescu2017inversion}, we investigate a broad class of covariance\nparameter estimators based on inversion-free surrogate losses and block\ndiagonal approximation schemes of the covariance structure. This class of\nestimators yields a spectrum for negotiating the trade-off between statistical\naccuracy and computational cost. We present fixed-domain asymptotic properties\nof our proposed method, establishing $\\sqrt{n}$-consistency and asymptotic\nnormality results for isotropic Matern Gaussian processes observed on a\nmulti-dimensional and irregular lattice. Simulation studies are also presented\nfor assessing the scalability and statistical efficiency of the proposed\nalgorithm for large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 03:44:51 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 09:00:48 GMT"}, {"version": "v3", "created": "Sun, 7 Jul 2019 19:50:59 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Keshavarz", "Hossein", ""], ["Nguyen", "XuanLong", ""], ["Scott", "Clayton", ""]]}, {"id": "1811.12788", "submitter": "Jerome Stenger", "authors": "Jerome Stenger, Fabrice Gamboa, Merlin Keller, Bertrand Iooss", "title": "Optimal Uncertainty Quantification on moment class using canonical\n  moments", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We gain robustness on the quantification of a risk measurement by accounting\nfor all sources of uncertainties tainting the inputs of a computer code. We\nevaluate the maximum quantile over a class of distributions defined only by\nconstraints on their moments. The methodology is based on the theory of\ncanonical moments that appears to be a well-suited framework for practical\noptimization.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 13:38:04 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Stenger", "Jerome", ""], ["Gamboa", "Fabrice", ""], ["Keller", "Merlin", ""], ["Iooss", "Bertrand", ""]]}]