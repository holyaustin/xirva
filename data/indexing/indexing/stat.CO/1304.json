[{"id": "1304.0150", "submitter": "Antonio Punzo", "authors": "Salvatore Ingrassia and Antonio Punzo", "title": "Fitting Bivariate Mixed-Type Data via the Generalized Linear Exponential\n  Cluster-Weighted Model", "comments": "the paper has been withdrawn by the authors because they are working\n  on a quite extended version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cluster-weighted model (CWM) is a mixture model with random covariates\nwhich allows for flexible clustering and density estimation of a random vector\ncomposed by a response variable and by a set of covariates. In this class of\nmodels, the generalized linear exponential CWM is here introduced especially\nfor modeling bivariate data of mixed-type. Its natural counterpart, in the\nfamily of latent class models, is also defined. Maximum likelihood parameter\nestimates are derived using the EM algorithm and model selection is carried out\nusing the Bayesian information criterion (BIC). Artificial and real data are\nfinally considered to exemplify and appreciate the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 01:41:26 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 08:07:22 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Ingrassia", "Salvatore", ""], ["Punzo", "Antonio", ""]]}, {"id": "1304.0151", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Anthony Lee, Christopher Yau, Xiaole Zhang", "title": "The Alive Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we develop a particle filter for approximating\nFeynman-Kac models with indicator potentials. Examples of such models include\napproximate Bayesian computation (ABC) posteriors associated with hidden Markov\nmodels (HMMs) or rare-event problems. Such models require the use of advanced\nparticle filter or Markov chain Monte Carlo (MCMC) algorithms e.g. Jasra et al.\n(2012), to perform estimation. One of the drawbacks of existing particle\nfilters, is that they may 'collapse', in that the algorithm may terminate\nearly, due to the indicator potentials. In this article, using a special case\nof the locally adaptive particle filter in Lee et al. (2013), which is closely\nrelated to Le Gland & Oudjane (2004), we use an algorithm which can deal with\nthis latter problem, whilst introducing a random cost per-time step. This\nalgorithm is investigated from a theoretical perspective and several results\nare given which help to validate the algorithms and to provide guidelines for\ntheir implementation. In addition, we show how this algorithm can be used\nwithin MCMC, using particle MCMC (Andrieu et al. 2010). Numerical examples are\npresented for ABC approximations of HMMs.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 02:51:44 GMT"}], "update_date": "2013-04-02", "authors_parsed": [["Jasra", "Ajay", ""], ["Lee", "Anthony", ""], ["Yau", "Christopher", ""], ["Zhang", "Xiaole", ""]]}, {"id": "1304.0462", "submitter": "Priscilla Canizares", "authors": "Priscilla Canizares (1), Scott E. Field (2), Jonathan R. Gair (1),\n  Manuel Tiglio (2,3) ((1) Institute of Astronomy, Cambridge (UK), (2)\n  University of Maryland, College Park (USA), (3) California Institute of\n  Technology, Pasadena (USA))", "title": "Gravitational wave parameter estimation with compressed likelihood\n  evaluations", "comments": "18 pages, 12 figures (2 updated) and 2 tables. Minor edits based on\n  referee report", "journal-ref": "Phys. Rev. D 87, 124005 (2013)", "doi": "10.1103/PhysRevD.87.124005", "report-no": null, "categories": "gr-qc astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main bottlenecks in gravitational wave (GW) astronomy is the high\ncost of performing parameter estimation and GW searches on the fly. We propose\na novel technique based on Reduced Order Quadratures (ROQs), an application and\ndata-specific quadrature rule, to perform fast and accurate likelihood\nevaluations. These are the dominant cost in Markov chain Monte Carlo (MCMC)\nalgorithms, which are widely employed in parameter estimation studies, and so\nROQs offer a new way to accelerate GW parameter estimation. We illustrate our\napproach using a four dimensional GW burst model embedded in noise. We build an\nROQ for this model, and perform four dimensional MCMC searches with both the\nstandard and ROQs quadrature rules, showing that, for this model, the ROQ\napproach is around 25 times faster than the standard approach with essentially\nno loss of accuracy. The speed-up from using ROQs is expected to increase for\nmore complex GW signal models and therefore has significant potential to\naccelerate parameter estimation of GW sources such as compact binary\ncoalescences.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 20:02:21 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2013 11:35:49 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Canizares", "Priscilla", ""], ["Field", "Scott E.", ""], ["Gair", "Jonathan R.", ""], ["Tiglio", "Manuel", ""]]}, {"id": "1304.0499", "submitter": "Eric Chi", "authors": "Eric C. Chi, Kenneth Lange", "title": "Splitting Methods for Convex Clustering", "comments": "37 pages, 6 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 24(4):994-1013,\n  2015", "doi": "10.1080/10618600.2014.948181", "report-no": null, "categories": "stat.ML math.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is a fundamental problem in many scientific applications. Standard\nmethods such as $k$-means, Gaussian mixture models, and hierarchical\nclustering, however, are beset by local minima, which are sometimes drastically\nsuboptimal. Recently introduced convex relaxations of $k$-means and\nhierarchical clustering shrink cluster centroids toward one another and ensure\na unique global minimizer. In this work we present two splitting methods for\nsolving the convex clustering problem. The first is an instance of the\nalternating direction method of multipliers (ADMM); the second is an instance\nof the alternating minimization algorithm (AMA). In contrast to previously\nconsidered algorithms, our ADMM and AMA formulations provide simple and unified\nframeworks for solving the convex clustering problem under the previously\nstudied norms and open the door to potentially novel norms. We demonstrate the\nperformance of our algorithm on both simulated and real data examples. While\nthe differences between the two algorithms appear to be minor on the surface,\ncomplexity analysis and numerical experiments show AMA to be significantly more\nefficient.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 23:37:20 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 16:23:54 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Chi", "Eric C.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1304.0503", "submitter": "Niels Richard Hansen", "authors": "Niels Richard Hansen", "title": "Nonparametric likelihood based estimation of linear filters for point\n  processes", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider models for multivariate point processes where the intensity is\ngiven nonparametrically in terms of functions in a reproducing kernel Hilbert\nspace. The likelihood function involves a time integral and is consequently not\ngiven in terms of a finite number of kernel evaluations. The main result is a\nrepresentation of the gradient of the log-likelihood, which we use to derive\ncomputable approximations of the log-likelihood and the gradient by time\ndiscretization. These approximations are then used to minimize the approximate\npenalized log-likelihood. For time and memory efficiency the implementation\nrelies crucially on the use of sparse matrices. As an illustration we consider\nneuron network modeling, and we use this example to investigate how the\ncomputational costs of the approximations depend on the resolution of the time\ndiscretization. The implementation is available in the R package ppstat.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 23:53:54 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2013 12:40:53 GMT"}, {"version": "v3", "created": "Wed, 12 Feb 2014 22:24:24 GMT"}], "update_date": "2014-02-14", "authors_parsed": [["Hansen", "Niels Richard", ""]]}, {"id": "1304.1250", "submitter": "Chunhua Shen", "authors": "Fumin Shen, Chunhua Shen, Rhys Hill, Anton van den Hengel, Zhenmin\n  Tang", "title": "Fast Approximate L_infty Minimization: Speeding Up Robust Regression", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimization of the $L_\\infty$ norm, which can be viewed as approximately\nsolving the non-convex least median estimation problem, is a powerful method\nfor outlier removal and hence robust regression. However, current techniques\nfor solving the problem at the heart of $L_\\infty$ norm minimization are slow,\nand therefore cannot scale to large problems. A new method for the minimization\nof the $L_\\infty$ norm is presented here, which provides a speedup of multiple\norders of magnitude for data with high dimension. This method, termed Fast\n$L_\\infty$ Minimization, allows robust regression to be applied to a class of\nproblems which were previously inaccessible. It is shown how the $L_\\infty$\nnorm minimization problem can be broken up into smaller sub-problems, which can\nthen be solved extremely efficiently. Experimental results demonstrate the\nradical reduction in computation time, along with robustness against large\nnumbers of outliers in a few model-fitting problems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 05:57:56 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Shen", "Fumin", ""], ["Shen", "Chunhua", ""], ["Hill", "Rhys", ""], ["Hengel", "Anton van den", ""], ["Tang", "Zhenmin", ""]]}, {"id": "1304.1350", "submitter": "Alex Lenkoski", "authors": "Alex Lenkoski", "title": "A Direct Sampler for G-Wishart Variates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The G-Wishart distribution is the conjugate prior for precision matrices that\nencode the conditional independencies of a Gaussian graphical model. While the\ndistribution has received considerable attention, posterior inference has\nproven computationally challenging, in part due to the lack of a direct\nsampler. In this note, we rectify this situation. The existence of a direct\nsampler offers a host of new possibilities for the use of G-Wishart variates.\nWe discuss one such development by outlining a new transdimensional model\nsearch algorithm--which we term double reversible jump--that leverages this\nsampler to avoid normalizing constant calculation when comparing graphical\nmodels. We conclude with two short studies meant to investigate our algorithm's\nvalidity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 12:36:16 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Lenkoski", "Alex", ""]]}, {"id": "1304.1778", "submitter": "Silvia Liverani", "authors": "David I. Hastie, Silvia Liverani and Sylvia Richardson", "title": "Sampling from Dirichlet process mixture models with unknown\n  concentration parameter: Mixing issues in large data implementations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of Markov chain Monte Carlo sampling from a general\nstick-breaking Dirichlet process mixture model, with concentration parameter\nalpha. This paper introduces a Gibbs sampling algorithm that combines the slice\nsampling approach of Walker (2007) and the retrospective sampling approach of\nPapaspiliopoulos and Roberts (2008). Our general algorithm is implemented as\nefficient open source C++ software, available as an R package, and is based on\na blocking strategy similar to that suggested by Papaspiliopoulos (2008) and\nimplemented by Yau et al (2011).\n  We discuss the difficulties of achieving good mixing in MCMC samplers of this\nnature and investigate sensitivity to initialisation. We additionally consider\nthe challenges when an additional layer of hierarchy is added such that joint\ninference is to be made on alpha. We introduce a new label switching move and\ncompute the marginal model posterior to help to surmount these difficulties.\nOur work is illustrated using a profile regression (Molitor et al, 2010)\napplication, where we demonstrate good mixing behaviour for both synthetic and\nreal examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 18:09:03 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2014 16:10:04 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Hastie", "David I.", ""], ["Liverani", "Silvia", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1304.1887", "submitter": "Nicolas Chopin", "authors": "Nicolas Chopin, Sumeetpal S. Singh", "title": "On particle Gibbs sampling", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ629 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 3, 1855-1883", "doi": "10.3150/14-BEJ629", "report-no": "IMS-BEJ-BEJ629", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The particle Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm to\nsample from the full posterior distribution of a state-space model. It does so\nby executing Gibbs sampling steps on an extended target distribution defined on\nthe space of the auxiliary variables generated by an interacting particle\nsystem. This paper makes the following contributions to the theoretical study\nof this algorithm. Firstly, we present a coupling construction between two\nparticle Gibbs updates from different starting points and we show that the\ncoupling probability may be made arbitrarily close to one by increasing the\nnumber of particles. We obtain as a direct corollary that the particle Gibbs\nkernel is uniformly ergodic. Secondly, we show how the inclusion of an\nadditional Gibbs sampling step that reselects the ancestors of the particle\nGibbs' extended target distribution, which is a popular approach in practice to\nimprove mixing, does indeed yield a theoretically more efficient algorithm as\nmeasured by the asymptotic variance. Thirdly, we extend particle Gibbs to work\nwith lower variance resampling schemes. A detailed numerical study is provided\nto demonstrate the efficiency of particle Gibbs and the proposed variants.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 12:46:56 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 10:36:09 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Chopin", "Nicolas", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1304.2048", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine)", "title": "Bayesian Computational Tools", "comments": "26 pages, 10 figures, revision of a paper written as a chapter for\n  the Annual Review of Statistics and Its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter surveys advances in the field of Bayesian computation over the\npast twenty years, with missing data. It also contains some novel computational\nentries on the double-exponential model that may be of interest per se.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 19:27:56 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 05:34:51 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine"]]}, {"id": "1304.2069", "submitter": "Peter Ruckdeschel", "authors": "Christina Erlwein and Peter Ruckdeschel", "title": "Robustification of Elliott's on-line EM algorithm for HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a robustification of an on-line algorithm for\nmodelling asset prices within a hidden Markov model (HMM). In this HMM\nframework, parameters of the model are guided by a Markov chain in discrete\ntime, parameters of the asset returns are therefore able to switch between\ndifferent regimes. The parameters are estimated through an on-line algorithm,\nwhich utilizes incoming information from the market and leads to adaptive\noptimal estimates. We robustify this algorithm step by step against additive\noutliers appearing in the observed asset prices with the rationale to better\nhandle possible peaks or missings in asset returns.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 22:10:26 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Erlwein", "Christina", ""], ["Ruckdeschel", "Peter", ""]]}, {"id": "1304.2129", "submitter": "Mikkel Meyer Andersen", "authors": "Mikkel Meyer Andersen, Poul Svante Eriksen, Niels Morling", "title": "A gentle introduction to the discrete Laplace method for estimating\n  Y-STR haplotype frequencies", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Y-STR data simulated under a Fisher-Wright model of evolution with a\nsingle-step mutation model turns out to be well predicted by a method using\ndiscrete Laplace distributions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 08:07:19 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 13:29:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 17:00:37 GMT"}, {"version": "v4", "created": "Wed, 16 Oct 2013 16:47:02 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Andersen", "Mikkel Meyer", ""], ["Eriksen", "Poul Svante", ""], ["Morling", "Niels", ""]]}, {"id": "1304.3673", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Bayesian analysis of matrix data with rstiefel", "comments": "This is a vignette for the R-package \"rstiefel\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate the use of the R-package \"rstiefel\" for matrix-variate data\nanalysis in the context of two examples. The first example considers estimation\nof a reduced-rank mean matrix in the presence of normally distributed noise.\nThe second example considers the modeling of a social network of friendships\namong teenagers. Bayesian estimation for these models requires the ability to\nsimulate from the matrix-variate von Mises-Fisher distributions and the\nmatrix-variate Bingham distributions on the Stiefel manifold.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 16:28:41 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1304.3800", "submitter": "Luca Martino", "authors": "Luca Martino and David Luengo", "title": "Extremely efficient generation of Gamma random variables for \\alpha >= 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gamma distribution is well-known and widely used in many signal\nprocessing and communications applications. In this letter, a simple and\nextremely efficient accept/reject algorithm is introduced for the generation of\nindependent random variables from a Gamma distribution with any shape parameter\n\\alpha >= 1. The proposed method uses another Gamma distribution with integer\n\\alpha_p <= \\alpha, from which samples can be easily drawn, as proposal\nfunction. For this reason, the new technique attains a higher acceptance rate\n(AR) for \\alpha >= 3 than all the methods currently available in the\nliterature, with AR tends to 1 as \\alpha\\ diverges.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 11:31:37 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 15:01:39 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2013 23:14:41 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Martino", "Luca", ""], ["Luengo", "David", ""]]}, {"id": "1304.4333", "submitter": "John Geweke", "authors": "John Geweke, Garland Durham, Huaxin Xu", "title": "Bayesian Inference for Logistic Regression Models Using Sequential\n  Posterior Simulation", "comments": "28 pages, 8 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logistic specification has been used extensively in non-Bayesian\nstatistics to model the dependence of discrete outcomes on the values of\nspecified covariates. Because the likelihood function is globally weakly\nconcave estimation by maximum likelihood is generally straightforward even in\ncommonly arising applications with scores or hundreds of parameters. In\ncontrast Bayesian inference has proven awkward, requiring normal approximations\nto the likelihood or specialized adaptations of existing Markov chain Monte\nCarlo and data augmentation methods. This paper approaches Bayesian inference\nin logistic models using recently developed generic sequential posterior\nsimulaton (SPS) methods that require little more than the ability to evaluate\nthe likelihood function. Compared with existing alternatives SPS is much\nsimpler, and provides numerical standard errors and accurate approximations of\nmarginal likelihoods as by-products. The SPS algorithm for Bayesian inference\nis amenable to massively parallel implementation, and when implemented using\ngraphical processing units it is more efficient than existing alternatives. The\npaper demonstrates these points by means of several examples.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 05:24:57 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Geweke", "John", ""], ["Durham", "Garland", ""], ["Xu", "Huaxin", ""]]}, {"id": "1304.4334", "submitter": "John Geweke", "authors": "Garland Durham, John Geweke", "title": "Adaptive Sequential Posterior Simulators for Massively Parallel\n  Computing Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massively parallel desktop computing capabilities now well within the reach\nof individual academics modify the environment for posterior simulation in\nfundamental and potentially quite advantageous ways. But to fully exploit these\nbenefits algorithms that conform to parallel computing environments are needed.\nSequential Monte Carlo comes very close to this ideal whereas other approaches\nlike Markov chain Monte Carlo do not. This paper presents a sequential\nposterior simulator well suited to this computing environment. The simulator\nmakes fewer analytical and programming demands on investigators, and is faster,\nmore reliable and more complete than conventional posterior simulators. The\npaper extends existing sequential Monte Carlo methods and theory to provide a\nthorough and practical foundation for sequential posterior simulation that is\nwell suited to massively parallel computing environments. It provides detailed\nrecommendations on implementation, yielding an algorithm that requires only\ncode for simulation from the prior and evaluation of prior and data densities\nand works well in a variety of applications representative of serious empirical\nwork in economics and finance. The algorithm is robust to pathological\nposterior distributions, generates accurate marginal likelihood approximations,\nand provides estimates of numerical standard error and relative numerical\nefficiency intrinsically. The paper concludes with an application that\nillustrates the potential of these simulators for applied Bayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 05:57:08 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Durham", "Garland", ""], ["Geweke", "John", ""]]}, {"id": "1304.4564", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "A high-dimensional two-sample test for the mean using random subspaces", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 74, 26-38 (2014)", "doi": "10.1016/j.csda.2013.12.003", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in genetics is that of testing whether a set of highly\ndependent gene expressions differ between two populations, typically in a\nhigh-dimensional setting where the data dimension is larger than the sample\nsize. Most high-dimensional tests for the equality of two mean vectors rely on\nnaive diagonal or trace estimators of the covariance matrix, ignoring\ndependencies between variables. A test recently proposed by Lopes et al. (2012)\nimplicitly incorporates dependencies by using random pseudo-projections to a\nlower-dimensional space. Their test offers higher power when the variables are\ndependent, but lacks desirable invariance properties and relies on asymptotic\np-values that are too conservative. We illustrate how a permutation approach\ncan be used to obtain p-values for the Lopes et al. test and how modifying the\ntest using random subspaces leads to a test statistic that is invariant under\nlinear transformations of the marginal distributions. The resulting test does\nnot rely on assumptions about normality or the structure of the covariance\nmatrix. We show by simulation that the new test has higher power than competing\ntests in realistic settings motivated by microarray gene expression data. We\nalso discuss the computational aspects of high-dimensional permutation tests\nand provide an efficient R implementation of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 19:18:43 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1304.4786", "submitter": "Pedro Galeano", "authors": "Esdras Joseph, Pedro Galeano and Rosa E. Lillo", "title": "The Mahalanobis distance for functional data with applications to\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general notion of Mahalanobis distance for functional\ndata that extends the classical multivariate concept to situations where the\nobserved data are points belonging to curves generated by a stochastic process.\nMore precisely, a new semi-distance for functional observations that generalize\nthe usual Mahalanobis distance for multivariate datasets is introduced. For\nthat, the development uses a regularized square root inverse operator in\nHilbert spaces. Some of the main characteristics of the functional Mahalanobis\nsemi-distance are shown. Afterwards, new versions of several well known\nfunctional classification procedures are developed using the Mahalanobis\ndistance for functional data as a measure of proximity between functional\nobservations. The performance of several well known functional classification\nprocedures are compared with those methods used in conjunction with the\nMahalanobis distance for functional data, with positive results, through a\nMonte Carlo study and the analysis of two real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 12:13:58 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Joseph", "Esdras", ""], ["Galeano", "Pedro", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1304.4890", "submitter": "Dabao Zhang", "authors": "Yanzhu Lin, Min Zhang, Dabao Zhang", "title": "Generalized Orthogonal Components Regression for High Dimensional\n  Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Here we propose an algorithm, named generalized orthogonal components\nregression (GOCRE), to explore the relationship between a categorical outcome\nand a set of massive variables. A set of orthogonal components are sequentially\nconstructed to account for the variation of the categorical outcome, and\ntogether build up a generalized linear model (GLM). This algorithm can be\nconsidered as an extension of the partial least squares (PLS) for GLMs, but\novercomes several issues of existing extensions based on iteratively reweighted\nleast squares (IRLS). First, existing extensions construct a different set of\ncomponents at each iteration and thus cannot provide a convergent set of\ncomponents. Second, existing extensions are computationally intensive because\nof repetitively constructing a full set of components. Third, although they\npursue the convergence of regression coefficients, the resultant regression\ncoefficients may still diverge especially when building logistic regression\nmodels. GOCRE instead sequentially builds up each orthogonal component upon\nconvergent construction, and simultaneously regresses against these orthogonal\ncomponents to fit the GLM. The performance of the new method is demonstrated by\nboth simulation studies and a real data example.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 17:04:10 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Lin", "Yanzhu", ""], ["Zhang", "Min", ""], ["Zhang", "Dabao", ""]]}, {"id": "1304.5688", "submitter": "Luciano Andrade LA", "authors": "Joao Ricardo Nickenig Vissoci, Clarissa G. Rodrigues, Luciano de\n  Andrade, Jose Eduardo Santana, Amrapali Zaveri, Ricardo Pietrobon", "title": "A Framework for Reproducible, Interactive Research: Application to\n  health and social sciences", "comments": "09 pages, 01 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this article is to introduce a reporting framework for\nreproducible, interactive research applied to Big Clinical Data, based on open\nsource technologies. The framework is constituted by the following three axes:\n(i) data, (ii) analytical codes and (iii) dissemination. In this paper,\ndifferent documentation formats and online repositories are introduced. To\nintegrate and manage the reproducible contents, we propose the R Language as\nthe tool of choice. All the information is then published and gathered in a\nwebsite for different projects. This framework is free and user friendly and is\nproposed to enhance reproducibility of health-science reports.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 04:55:14 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Vissoci", "Joao Ricardo Nickenig", ""], ["Rodrigues", "Clarissa G.", ""], ["de Andrade", "Luciano", ""], ["Santana", "Jose Eduardo", ""], ["Zaveri", "Amrapali", ""], ["Pietrobon", "Ricardo", ""]]}, {"id": "1304.5768", "submitter": "Pierre E. Jacob", "authors": "Arnaud Doucet (University of Oxford), Pierre E. Jacob (Harvard\n  University) and Sylvain Rubenthaler (Universit\\'e de Nice-Sophia Antipolis)", "title": "Derivative-Free Estimation of the Score Vector and Observed Information\n  Matrix with Application to State-Space Models", "comments": "Technical report, 43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ionides, King et al. (see e.g. Inference for nonlinear dynamical systems,\nPNAS 103) have recently introduced an original approach to perform maximum\nlikelihood parameter estimation in state-space models which only requires being\nable to simulate the latent Markov model according to its prior distribution.\nTheir methodology relies on an approximation of the score vector for general\nstatistical models based upon an artificial posterior distribution and bypasses\nthe calculation of any derivative. We show here that this score estimator can\nbe derived from a simple application of Stein's lemma and how an additional\napplication of this lemma provides an original derivative-free estimator of the\nobserved information matrix. We establish that these estimators exhibit\nrobustness properties compared to finite difference estimators while their bias\nand variance scale as well as finite difference type estimators, including\nsimultaneous perturbations (see e.g. Spall, IEEE Trans. on Automatic Control\n37), with respect to the dimension of the parameter. For state-space models\nwhere sequential Monte Carlo computation is required, these estimators can be\nfurther improved. In this specific context, we derive original derivative-free\nestimators of the score vector and observed information matrix which are\ncomputed using sequential Monte Carlo approximations of smoothed additive\nfunctionals associated with a modified version of the original state-space\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 17:25:25 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 09:19:59 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2015 17:22:13 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Doucet", "Arnaud", "", "University of Oxford"], ["Jacob", "Pierre E.", "", "Harvard\n  University"], ["Rubenthaler", "Sylvain", "", "Universit\u00e9 de Nice-Sophia Antipolis"]]}, {"id": "1304.7808", "submitter": "Frederik Beaujean", "authors": "Frederik Beaujean and Allen Caldwell", "title": "Initializing adaptive importance sampling with Markov chains", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "MPP-2013-121", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive importance sampling is a powerful tool to sample from complicated\ntarget densities, but its success depends sensitively on the initial proposal\ndensity. An algorithm is presented to automatically perform the initialization\nusing Markov chains and hierarchical clustering. The performance is checked on\nchallenging multimodal examples in up to 20 dimensions and compared to results\nfrom nested sampling. Our approach yields a proposal that leads to rapid\nconvergence and accurate estimation of overall normalization and marginal\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 21:15:07 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Beaujean", "Frederik", ""], ["Caldwell", "Allen", ""]]}, {"id": "1304.7973", "submitter": "Tomonari Sei", "authors": "Tomonari Sei and Alfred Kume", "title": "Calculating the normalising constant of the Bingham distribution on the\n  sphere using the holonomic gradient method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we implement the holonomic gradient method to exactly compute\nthe normalising constant of Bingham distributions. This idea is originally\napplied for general Fisher-Bingham distributions in Nakayama et al. (2011). In\nthis paper we explicitly apply this algorithm to show the exact calculation of\nthe normalising constant; derive explicitly the Pfaffian system for this\nparametric case; implement the general approach for the maximum likelihood\nsolution search and finally adjust the method for degenerate cases, namely when\nthe parameter values have multiplicities.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 12:22:17 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Sei", "Tomonari", ""], ["Kume", "Alfred", ""]]}, {"id": "1304.8108", "submitter": "Mohit Singh", "authors": "Mohit Singh, Nisheeth K. Vishnoi", "title": "Entropy, Optimization and Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of computing max-entropy distributions\nover a discrete set of objects subject to observed marginals. Interest in such\ndistributions arises due to their applicability in areas such as statistical\nphysics, economics, biology, information theory, machine learning,\ncombinatorics and, more recently, approximation algorithms. A key difficulty in\ncomputing max-entropy distributions has been to show that they have\npolynomially-sized descriptions. We show that such descriptions exist under\ngeneral conditions. Subsequently, we show how algorithms for (approximately)\ncounting the underlying discrete set can be translated into efficient\nalgorithms to (approximately) compute max-entropy distributions. In the reverse\ndirection, we show how access to algorithms that compute max-entropy\ndistributions can be used to count, which establishes an equivalence between\ncounting and computing max-entropy distributions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 18:39:26 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Singh", "Mohit", ""], ["Vishnoi", "Nisheeth K.", ""]]}]