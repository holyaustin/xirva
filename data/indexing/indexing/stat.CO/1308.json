[{"id": "1308.0049", "submitter": "Won Chang", "authors": "Won Chang, Murali Haran, Roman Olson and Klaus Keller", "title": "A composite likelihood approach to computer model calibration using\n  high-dimensional spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer models are used to model complex processes in various disciplines.\nOften, a key source of uncertainty in the behavior of complex computer models\nis uncertainty due to unknown model input parameters. Statistical computer\nmodel calibration is the process of inferring model parameter values, along\nwith associated uncertainties, from observations of the physical process and\nfrom model outputs at various parameter settings. Observations and model\noutputs are often in the form of high-dimensional spatial fields, especially in\nthe environmental sciences. Sound statistical inference may be computationally\nchallenging in such situations. Here we introduce a composite likelihood-based\napproach to perform computer model calibration with high-dimensional spatial\ndata. While composite likelihood has been studied extensively in the context of\nspatial statistics, computer model calibration using composite likelihood poses\nseveral new challenges. We propose a computationally efficient approach for\nBayesian computer model calibration using composite likelihood. We also develop\na methodology based on asymptotic theory for adjusting the composite likelihood\nposterior distribution so that it accurately represents posterior\nuncertainties. We study the application of our new approach in the context of\ncalibration for a climate model.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2013 22:16:39 GMT"}], "update_date": "2013-08-02", "authors_parsed": [["Chang", "Won", ""], ["Haran", "Murali", ""], ["Olson", "Roman", ""], ["Keller", "Klaus", ""]]}, {"id": "1308.0399", "submitter": "Dirk Kroese P", "authors": "Dirk P. Kroese and Zdravko I. Botev", "title": "Spatial Process Generation", "comments": "41 pages, 31 figures, 13 matlab programs, 6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generation of random spatial data on a computer is an important tool for\nunderstanding the behavior of spatial processes. In this paper we describe how\nto generate realizations from the main types of spatial processes, including\nGaussian and Markov random fields, point processes, spatial Wiener processes,\nand Levy fields. Concrete MATLAB code is provided.\n", "versions": [{"version": "v1", "created": "Fri, 2 Aug 2013 03:46:33 GMT"}], "update_date": "2013-08-05", "authors_parsed": [["Kroese", "Dirk P.", ""], ["Botev", "Zdravko I.", ""]]}, {"id": "1308.0660", "submitter": "Hideyuki Suzuki", "authors": "Hideyuki Suzuki", "title": "Monte Carlo simulation of classical spin models with chaotic billiards", "comments": "9 pages, 7 figures", "journal-ref": "Physical Review E 88 (2013), 052144", "doi": "10.1103/PhysRevE.88.052144", "report-no": null, "categories": "cond-mat.stat-mech nlin.CD stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has recently been shown that the computing abilities of Boltzmann\nmachines, or Ising spin-glass models, can be implemented by chaotic billiard\ndynamics without any use of random numbers. In this paper, we further\nnumerically investigate the capabilities of the chaotic billiard dynamics as a\ndeterministic alternative to random Monte Carlo methods by applying it to\nclassical spin models in statistical physics. First, we verify that the\nbilliard dynamics can yield samples that converge to the true distribution of\nthe Ising model on a small lattice, and we show that it appears to have the\nsame convergence rate as random Monte Carlo sampling. Second, we apply the\nbilliard dynamics to finite-size scaling analysis of the critical behavior of\nthe Ising model and show that the phase transition point and the critical\nexponents are correctly obtained. Third, we extend the billiard dynamics to\nspins that take more than two states and show that it can be applied\nsuccessfully to the Potts model. We also discuss the possibility of extensions\nto continuous-valued models such as the XY model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Aug 2013 05:35:07 GMT"}, {"version": "v2", "created": "Sun, 15 Dec 2013 09:18:22 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Suzuki", "Hideyuki", ""]]}, {"id": "1308.0764", "submitter": "Rajarshi Mukherjee", "authors": "Rajarshi Mukherjee, Natesh S. Pillai, Xihong Lin", "title": "Hypothesis testing for high-dimensional sparse binary regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1279 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 352-381", "doi": "10.1214/14-AOS1279", "report-no": "IMS-AOS-AOS1279", "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the detection boundary for minimax hypothesis testing\nin the context of high-dimensional, sparse binary regression models. Motivated\nby genetic sequencing association studies for rare variant effects, we\ninvestigate the complexity of the hypothesis testing problem when the design\nmatrix is sparse. We observe a new phenomenon in the behavior of detection\nboundary which does not occur in the case of Gaussian linear regression. We\nderive the detection boundary as a function of two components: a design matrix\nsparsity index and signal strength, each of which is a function of the sparsity\nof the alternative. For any alternative, if the design matrix sparsity index is\ntoo high, any test is asymptotically powerless irrespective of the magnitude of\nsignal strength. For binary design matrices with the sparsity index that is not\ntoo high, our results are parallel to those in the Gaussian case. In this\ncontext, we derive detection boundaries for both dense and sparse regimes. For\nthe dense regime, we show that the generalized likelihood ratio is rate\noptimal; for the sparse regime, we propose an extended Higher Criticism Test\nand show it is rate optimal and sharp. We illustrate the finite sample\nproperties of the theoretical results using simulation studies.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 01:06:15 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 02:42:31 GMT"}, {"version": "v3", "created": "Thu, 5 Mar 2015 10:30:23 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Pillai", "Natesh S.", ""], ["Lin", "Xihong", ""]]}, {"id": "1308.0774", "submitter": "Jesse Windle", "authors": "Jesse Windle, Carlos M. Carvalho, James G. Scott, Liang Sun", "title": "Efficient Data Augmentation in Dynamic Models for Binary and Count Data", "comments": "22 Pages, 1 figure, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic linear models with Gaussian observations and Gaussian states lead to\nclosed-form formulas for posterior simulation. However, these closed-form\nformulas break down when the response or state evolution ceases to be Gaussian.\nDynamic, generalized linear models exemplify a class of models for which this\nis the case, and include, amongst other models, dynamic binomial logistic\nregression and dynamic negative binomial regression. Finding and appraising\nposterior simulation techniques for these models is important since modeling\ntemporally correlated categories or counts is useful in a variety of\ndisciplines, including ecology, economics, epidemiology, medicine, and\nneuroscience. In this paper, we present one such technique, P\\'olya-Gamma data\naugmentation, and compare it against two competing methods. We find that the\nP\\'olya-Gamma approach works well for dynamic logistic regression and for\ndynamic negative binomial regression when the count sizes are small.\nSupplementary files are provided for replicating the benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 4 Aug 2013 03:59:37 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2013 18:57:45 GMT"}], "update_date": "2013-09-20", "authors_parsed": [["Windle", "Jesse", ""], ["Carvalho", "Carlos M.", ""], ["Scott", "James G.", ""], ["Sun", "Liang", ""]]}, {"id": "1308.1313", "submitter": "Georg  Stadler Omar Ghattas", "authors": "Tan Bui-Thanh, Omar Ghattas, James Martin and Georg Stadler", "title": "A computational framework for infinite-dimensional Bayesian inverse\n  problems. Part I: The linearized case, with application to global seismic\n  inversion", "comments": "30 pages; to appear in SIAM Journal on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational framework for estimating the uncertainty in the\nnumerical solution of linearized infinite-dimensional statistical inverse\nproblems. We adopt the Bayesian inference formulation: given observational data\nand their uncertainty, the governing forward problem and its uncertainty, and a\nprior probability distribution describing uncertainty in the parameter field,\nfind the posterior probability distribution over the parameter field. The prior\nmust be chosen appropriately in order to guarantee well-posedness of the\ninfinite-dimensional inverse problem and facilitate computation of the\nposterior. Furthermore, straightforward discretizations may not lead to\nconvergent approximations of the infinite-dimensional problem. And finally,\nsolution of the discretized inverse problem via explicit construction of the\ncovariance matrix is prohibitive due to the need to solve the forward problem\nas many times as there are parameters. Our computational framework builds on\nthe infinite-dimensional formulation proposed by Stuart (A. M. Stuart, Inverse\nproblems: A Bayesian perspective, Acta Numerica, 19 (2010), pp. 451-559), and\nincorporates a number of components aimed at ensuring a convergent\ndiscretization of the underlying infinite-dimensional inverse problem. The\nframework additionally incorporates algorithms for manipulating the prior,\nconstructing a low rank approximation of the data-informed component of the\nposterior covariance operator, and exploring the posterior that together ensure\nscalability of the entire framework to very high parameter dimensions. We\ndemonstrate this computational framework on the Bayesian solution of an inverse\nproblem in 3D global seismic wave propagation with hundreds of thousands of\nparameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Aug 2013 15:39:11 GMT"}], "update_date": "2013-08-07", "authors_parsed": [["Bui-Thanh", "Tan", ""], ["Ghattas", "Omar", ""], ["Martin", "James", ""], ["Stadler", "Georg", ""]]}, {"id": "1308.1883", "submitter": "Joaqu\\'in M\\'iguez", "authors": "Dan Crisan, Joaquin Miguez", "title": "Nested particle filters for online parameter estimation in discrete-time\n  state-space Markov models", "comments": "Just a format update compared to the previous version. This is the\n  final manuscript accepted for publication in the Bernoulli journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of approximating the posterior probability\ndistribution of the fixed parameters of a state-space dynamical system using a\nsequential Monte Carlo method. The proposed approach relies on a nested\nstructure that employs two layers of particle filters to approximate the\nposterior probability measure of the static parameters and the dynamic state\nvariables of the system of interest, in a vein similar to the recent\n\"sequential Monte Carlo square\" (SMC$^2$) algorithm. However, unlike the\nSMC$^2$ scheme, the proposed technique operates in a purely recursive manner.\nIn particular, the computational complexity of the recursive steps of the\nmethod introduced herein is constant over time. We analyse the approximation of\nintegrals of real bounded functions with respect to the posterior distribution\nof the system parameters computed via the proposed scheme. As a result, we\nprove, under regularity assumptions, that the approximation errors vanish\nasymptotically in $L_p$ ($p \\ge 1$) with convergence rate proportional to\n$\\frac{1}{\\sqrt{N}} + \\frac{1}{\\sqrt{M}}$, where $N$ is the number of Monte\nCarlo samples in the parameter space and $N\\times M$ is the number of samples\nin the state space. This result also holds for the approximation of the joint\nposterior distribution of the parameters and the state variables. We discuss\nthe relationship between the SMC$^2$ algorithm and the new recursive method and\npresent a simple example in order to illustrate some of the theoretical\nfindings with computer simulations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Aug 2013 15:43:35 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 19:04:40 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 23:21:58 GMT"}, {"version": "v4", "created": "Wed, 1 Jun 2016 21:41:27 GMT"}, {"version": "v5", "created": "Wed, 10 May 2017 21:21:42 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Crisan", "Dan", ""], ["Miguez", "Joaquin", ""]]}, {"id": "1308.2045", "submitter": "Jim Griffin", "authors": "Jim E. Griffin", "title": "An adaptive truncation method for inference in Bayesian nonparametric\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many exact Markov chain Monte Carlo algorithms have been developed for\nposterior inference in Bayesian nonparametric models which involve\ninfinite-dimensional priors. However, these methods are not generic and special\nmethodology must be developed for different classes of prior or different\nmodels. Alternatively, the infinite-dimensional prior can be truncated and\nstandard Markov chain Monte Carlo methods used for inference. However, the\nerror in approximating the infinite-dimensional posterior can be hard to\ncontrol for many models. This paper describes an adaptive truncation method\nwhich allows the level of the truncation to be decided by the algorithm and so\ncan avoid large errors in approximating the posterior. A sequence of truncated\npriors is constructed which are sampled using Markov chain Monte Carlo methods\nembedded in a sequential Monte Carlo algorithm. Implementational details for\ninfinite mixture models with stick-breaking priors and normalized random\nmeasures with independent increments priors are discussed. The methodology is\nillustrated on infinite mixture models, a semiparametric linear mixed model and\na nonparametric time series model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 07:55:22 GMT"}, {"version": "v2", "created": "Wed, 21 May 2014 10:57:41 GMT"}], "update_date": "2014-05-22", "authors_parsed": [["Griffin", "Jim E.", ""]]}, {"id": "1308.2218", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of random projections has become very popular for large-scale\napplications in statistical learning, information retrieval, bio-informatics\nand other applications. Using a well-designed coding scheme for the projected\ndata, which determines the number of bits needed for each projected value and\nhow to allocate these bits, can significantly improve the effectiveness of the\nalgorithm, in storage cost as well as computational speed. In this paper, we\nstudy a number of simple coding schemes, focusing on the task of similarity\nestimation and on an application to training linear classifiers. We demonstrate\nthat uniform quantization outperforms the standard existing influential method\n(Datar et. al. 2004). Indeed, we argue that in many cases coding with just a\nsmall number of bits suffices. Furthermore, we also develop a non-uniform 2-bit\ncoding scheme that generally performs well in practice, as confirmed by our\nexperiments on training linear support vector machines (SVM).\n", "versions": [{"version": "v1", "created": "Fri, 9 Aug 2013 19:50:24 GMT"}], "update_date": "2013-08-12", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "1308.2443", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Shudong Sun, Tariq P. Sattar and Juan M. Corchado", "title": "Fighting Sample Degeneracy and Impoverishment in Particle Filters: A\n  Review of Intelligent Approaches", "comments": "Expert Systems with Applications, 2014", "journal-ref": "Expert Systems with Applications, Volume 41, Issue 8, Pages\n  3944-3954 (15 June 2014)", "doi": "10.1016/j.eswa.2013.12.031", "report-no": null, "categories": "cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last two decades there has been a growing interest in Particle\nFiltering (PF). However, PF suffers from two long-standing problems that are\nreferred to as sample degeneracy and impoverishment. We are investigating\nmethods that are particularly efficient at Particle Distribution Optimization\n(PDO) to fight sample degeneracy and impoverishment, with an emphasis on\nintelligence choices. These methods benefit from such methods as Markov Chain\nMonte Carlo methods, Mean-shift algorithms, artificial intelligence algorithms\n(e.g., Particle Swarm Optimization, Genetic Algorithm and Ant Colony\nOptimization), machine learning approaches (e.g., clustering, splitting and\nmerging) and their hybrids, forming a coherent standpoint to enhance the\nparticle filter. The working mechanism, interrelationship, pros and cons of\nthese approaches are provided. In addition, Approaches that are effective for\ndealing with high-dimensionality are reviewed. While improving the filter\nperformance in terms of accuracy, robustness and convergence, it is noted that\nadvanced techniques employed in PF often causes additional computational\nrequirement that will in turn sacrifice improvement obtained in real life\nfiltering. This fact, hidden in pure simulations, deserves the attention of the\nusers and designers of new filters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Aug 2013 01:38:17 GMT"}, {"version": "v2", "created": "Thu, 9 Jan 2014 02:32:06 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Li", "Tiancheng", ""], ["Sun", "Shudong", ""], ["Sattar", "Tariq P.", ""], ["Corchado", "Juan M.", ""]]}, {"id": "1308.3015", "submitter": "Nisar Ahmed", "authors": "Nisar Ahmed, Tsung-Lin Yang, Mark Campbell", "title": "On Generalized Bayesian Data Fusion with Complex Models in Large Scale\n  Networks", "comments": "Revised version of paper submitted to 2013 Workshop on Wireless\n  Intelligent Sensor Networks (WISeNET 2013) at Duke University, June 5, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in communications, mobile computing, and artificial\nintelligence have greatly expanded the application space of intelligent\ndistributed sensor networks. This in turn motivates the development of\ngeneralized Bayesian decentralized data fusion (DDF) algorithms for robust and\nefficient information sharing among autonomous agents using probabilistic\nbelief models. However, DDF is significantly challenging to implement for\ngeneral real-world applications requiring the use of dynamic/ad hoc network\ntopologies and complex belief models, such as Gaussian mixtures or hybrid\nBayesian networks. To tackle these issues, we first discuss some new key\nmathematical insights about exact DDF and conservative approximations to DDF.\nThese insights are then used to develop novel generalized DDF algorithms for\ncomplex beliefs based on mixture pdfs and conditional factors. Numerical\nexamples motivated by multi-robot target search demonstrate that our methods\nlead to significantly better fusion results, and thus have great potential to\nenhance distributed intelligent reasoning in sensor networks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Aug 2013 02:30:40 GMT"}], "update_date": "2013-08-15", "authors_parsed": [["Ahmed", "Nisar", ""], ["Yang", "Tsung-Lin", ""], ["Campbell", "Mark", ""]]}, {"id": "1308.3416", "submitter": "Yixin Fang", "authors": "Yixin Fang, Binhuan Wang, and Yang Feng", "title": "Tuning Parameter Selection in Regularized Estimations of Large\n  Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently many regularized estimators of large covariance matrices have been\nproposed, and the tuning parameters in these estimators are usually selected\nvia cross-validation. However, there is no guideline on the number of folds for\nconducting cross-validation and there is no comparison between cross-validation\nand the methods based on bootstrap. Through extensive simulations, we suggest\n10-fold cross-validation (nine-tenths for training and one-tenth for\nvalidation) be appropriate when the estimation accuracy is measured in the\nFrobenius norm, while 2-fold cross-validation (half for training and half for\nvalidation) or reverse 3-fold cross-validation (one-third for training and\ntwo-thirds for validation) be appropriate in the operator norm. We also suggest\nthe \"optimal\" cross-validation be more appropriate than the methods based on\nbootstrap for both types of norm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Aug 2013 14:38:12 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Fang", "Yixin", ""], ["Wang", "Binhuan", ""], ["Feng", "Yang", ""]]}, {"id": "1308.3779", "submitter": "Luca Martino", "authors": "L. Martino, R. Casarin, F. Leisen, D. Luengo", "title": "Adaptive Independent Sticky MCMC algorithms", "comments": "A preliminary Matlab code is provided at\n  https://www.mathworks.com/matlabcentral/fileexchange/54701-adaptive-independent-sticky-metropolis--aism--algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a novel class of adaptive Monte Carlo methods,\ncalled adaptive independent sticky MCMC algorithms, for efficient sampling from\na generic target probability density function (pdf). The new class of\nalgorithms employs adaptive non-parametric proposal densities which become\ncloser and closer to the target as the number of iterations increases. The\nproposal pdf is built using interpolation procedures based on a set of support\npoints which is constructed iteratively based on previously drawn samples. The\nalgorithm's efficiency is ensured by a test that controls the evolution of the\nset of support points. This extra stage controls the computational cost and the\nconvergence of the proposal density to the target. Each part of the novel\nfamily of algorithms is discussed and several examples are provided. Although\nthe novel algorithms are presented for univariate target densities, we show\nthat they can be easily extended to the multivariate context within a\nGibbs-type sampler. The ergodicity is ensured and discussed. Exhaustive\nnumerical examples illustrate the efficiency of sticky schemes, both as a\nstand-alone methods to sample from complicated one-dimensional pdfs and within\nGibbs in order to draw from multi-dimensional target distributions.\n", "versions": [{"version": "v1", "created": "Sat, 17 Aug 2013 12:35:08 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2013 12:59:49 GMT"}, {"version": "v3", "created": "Tue, 5 Aug 2014 17:42:02 GMT"}, {"version": "v4", "created": "Sat, 2 Jan 2016 14:49:27 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Martino", "L.", ""], ["Casarin", "R.", ""], ["Leisen", "F.", ""], ["Luengo", "D.", ""]]}, {"id": "1308.4084", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Noemi Petra, Georg Stadler, Omar Ghattas", "title": "A-optimal design of experiments for infinite-dimensional Bayesian linear\n  inverse problems with regularized $\\ell_0$-sparsification", "comments": "27 pages, accepted for publication in SIAM Journal on Scientific\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient method for computing A-optimal experimental designs\nfor infinite-dimensional Bayesian linear inverse problems governed by partial\ndifferential equations (PDEs). Specifically, we address the problem of\noptimizing the location of sensors (at which observational data are collected)\nto minimize the uncertainty in the parameters estimated by solving the inverse\nproblem, where the uncertainty is expressed by the trace of the posterior\ncovariance. Computing optimal experimental designs (OEDs) is particularly\nchallenging for inverse problems governed by computationally expensive PDE\nmodels with infinite-dimensional (or, after discretization, high-dimensional)\nparameters. To alleviate the computational cost, we exploit the problem\nstructure and build a low-rank approximation of the parameter-to-observable\nmap, preconditioned with the square root of the prior covariance operator. This\nrelieves our method from expensive PDE solves when evaluating the optimal\nexperimental design objective function and its derivatives. Moreover, we employ\na randomized trace estimator for efficient evaluation of the OED objective\nfunction. We control the sparsity of the sensor configuration by employing a\nsequence of penalty functions that successively approximate the\n$\\ell_0$-\"norm\"; this results in binary designs that characterize optimal\nsensor locations. We present numerical results for inference of the initial\ncondition from spatio-temporal observations in a time-dependent\nadvection-diffusion problem in two and three space dimensions. We find that an\noptimal design can be computed at a cost, measured in number of forward PDE\nsolves, that is independent of the parameter and sensor dimensions. We\ndemonstrate numerically that $\\ell_0$-sparsified experimental designs obtained\nvia a continuation method outperform $\\ell_1$-sparsified designs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Aug 2013 17:59:23 GMT"}, {"version": "v2", "created": "Tue, 27 May 2014 22:52:32 GMT"}], "update_date": "2014-05-29", "authors_parsed": [["Alexanderian", "Alen", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1308.4601", "submitter": "Johan Dahlin Mr.", "authors": "Johan Dahlin and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "Inference in Gaussian models with missing data using Equalisation\n  Maximisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equalisation Maximisation (EqM) is an algorithm for estimating parameters in\nauto-regressive (AR) models where some fraction of the data is missing. It has\npreviously been shown that the EqM algorithm is a competitive alternative to\nexpectation maximisation, estimating models with equal predictive capability at\na lower computational cost.\n  The EqM algorithm has previously been motivated as a heuristic. In this\npaper, we instead show that EqM can be viewed as an approximation of a proximal\npoint algorithm. We also derive the method for the entire class of Gaussian\nmodels and exemplify its use for estimation of ARMA models with missing data.\nThe resulting method is evaluated in numerical simulations, resulting in\nsimilar results as for the AR processes.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 14:49:03 GMT"}], "update_date": "2013-08-22", "authors_parsed": [["Dahlin", "Johan", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1308.4690", "submitter": "Longhai Li", "authors": "Longhai Li and Weixin Yao", "title": "High-dimensional Feature Selection Using Hierarchical Bayesian Logistic\n  Regression with Heavy-tailed Priors", "comments": "This is an earlier version of the paper arXiv:1405.3319. We do not\n  want to cause confusion to readers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of selecting the most useful features from a great many (eg,\nthousands) of candidates arises in many areas of modern sciences. An\ninteresting problem from genomic research is that, from thousands of genes that\nare active (expressed) in certain tissue cells, we want to find the genes that\ncan be used to separate tissues of different classes (eg. cancer and normal).\nIn this paper, we report our empirical experiences of using Bayesian logistic\nregression based on heavy-tailed priors with moderately small degree freedom\n(such as 1) and very small scale, and using Hamiltonian Monte Carlo to do\ncomputation. We discuss the advantages and limitations of this method, and\nillustrate the difficulties that remain unsolved. The method is applied to a\nreal microarray data set related to prostate cancer. The method identifies only\n3 non-redundant genes out of 6033 candidates but achieves better leave-one-out\ncross-validated prediction accuracy than many other methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Aug 2013 20:04:34 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 03:12:27 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Li", "Longhai", ""], ["Yao", "Weixin", ""]]}, {"id": "1308.4871", "submitter": "Caitr\\'iona Ryan", "authors": "Nial Friel and Caitriona Ryan and Jason Wyse", "title": "Bayesian model selection for the latent position cluster model for\n  Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latent position cluster model is a popular model for the statistical\nanalysis of network data. This approach assumes that there is an underlying\nlatent space in which the actors follow a finite mixture distribution.\nMoreover, actors which are close in this latent space tend to be tied by an\nedge. This is an appealing approach since it allows the model to cluster actors\nwhich consequently provides the practitioner with useful qualitative\ninformation. However, exploring the uncertainty in the number of underlying\nlatent components in the mixture distribution is a very complex task. The\ncurrent state-of-the-art is to use an approximate form of BIC for this purpose,\nwhere an approximation of the log-likelihood is used instead of the true\nlog-likelihood which is unavailable. The main contribution of this paper is to\nshow that through the use of conjugate prior distributions it is possible to\nanalytically integrate out almost all of the model parameters, leaving a\nposterior distribution which depends on the allocation vector of the mixture\nmodel. A consequence of this is that it is possible to carry out posterior\ninference over the number of components in the latent mixture distribution\nwithout using trans-dimensional MCMC algorithms such as reversible jump MCMC.\nMoreover, our algorithm allows for more reasonable computation times for larger\nnetworks than the standard methods using the latentnet package (Krivitsky and\nHandcock 2008; Krivitsky and Handcock 2013).\n", "versions": [{"version": "v1", "created": "Thu, 22 Aug 2013 14:10:49 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Friel", "Nial", ""], ["Ryan", "Caitriona", ""], ["Wyse", "Jason", ""]]}, {"id": "1308.5697", "submitter": "Emmanuel Candes", "authors": "Rafi Witten and Emmanuel Candes", "title": "Randomized algorithms for low-rank matrix factorizations: sharp\n  performance bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of randomized algorithms for numerical linear algebra, e.g.\nfor computing approximate QR and SVD factorizations, has recently become an\nintense area of research. This paper studies one of the most frequently\ndiscussed algorithms in the literature for dimensionality\nreduction---specifically for approximating an input matrix with a low-rank\nelement. We introduce a novel and rather intuitive analysis of the algorithm in\nMartinsson et al. (2008), which allows us to derive sharp estimates and give\nnew insights about its performance. This analysis yields theoretical guarantees\nabout the approximation error and at the same time, ultimate limits of\nperformance (lower bounds) showing that our upper bounds are tight. Numerical\nexperiments complement our study and show the tightness of our predictions\ncompared with empirical observations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 21:00:35 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Witten", "Rafi", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1308.5717", "submitter": "Alicia Johnson", "authors": "Alicia A. Johnson and James M. Flegal", "title": "A Modified Gibbs Sampler on General State Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a modified Gibbs sampler for general state spaces. We establish\nthat this modification can lead to substantial gains in statistical efficiency\nwhile maintaining the overall quality of convergence. We illustrate our results\nin two examples including a toy Normal-Normal model and a Bayesian version of\nthe random effects model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Aug 2013 22:53:02 GMT"}], "update_date": "2013-08-28", "authors_parsed": [["Johnson", "Alicia A.", ""], ["Flegal", "James M.", ""]]}, {"id": "1308.5953", "submitter": "James Hensman", "authors": "James Hensman, Peter Glaus, Antti Honkela, Magnus Rattray", "title": "Fast Approximate Inference of Transcript Expression Levels from RNA-seq\n  Data", "comments": "This paper has been withdrawn by the authors. Please see much revised\n  edition arXiv:1412.5995", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: The mapping of RNA-seq reads to their transcripts of origin is a\nfundamental task in transcript expression estimation and differential\nexpression scoring. Where ambiguities in mapping exist due to transcripts\nsharing sequence, e.g. alternative isoforms or alleles, the problem becomes an\ninstance of non-trivial probabilistic inference. Bayesian inference in such a\nproblem is intractable and approximate methods must be used such as Markov\nchain Monte Carlo (MCMC) and Variational Bayes. Standard implementations of\nthese methods can be prohibitively slow for large datasets and complex gene\nmodels.\n  Results: We propose an approximate inference scheme based on Variational\nBayes applied to an existing model of transcript expression inference from\nRNA-seq data. We apply recent advances in Variational Bayes algorithmics to\nimprove the convergence of the algorithm beyond the standard variational\nexpectation-maximisation approach. We apply our algorithm to simulated and\nbiological datasets, demonstrating that the increase in speed requires only a\nsmall trade-off in accuracy of expression level estimation.\n  Availability: The methods were implemented in R and C++, and are available as\npart of the BitSeq project at https://code.google.com/p/bitseq/. The methods\nwill be made available through the BitSeq Bioconductor package at the next\nstable release.\n", "versions": [{"version": "v1", "created": "Tue, 27 Aug 2013 19:16:40 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 10:35:01 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Hensman", "James", ""], ["Glaus", "Peter", ""], ["Honkela", "Antti", ""], ["Rattray", "Magnus", ""]]}, {"id": "1308.6221", "submitter": "Noemi Petra", "authors": "Noemi Petra, James Martin, Georg Stadler, Omar Ghattas", "title": "A computational framework for infinite-dimensional Bayesian inverse\n  problems: Part II. Stochastic Newton MCMC with application to ice sheet flow\n  inverse problems", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the numerical solution of infinite-dimensional inverse problems in\nthe framework of Bayesian inference. In the Part I companion to this paper\n(arXiv.org:1308.1313), we considered the linearized infinite-dimensional\ninverse problem. Here in Part II, we relax the linearization assumption and\nconsider the fully nonlinear infinite-dimensional inverse problem using a\nMarkov chain Monte Carlo (MCMC) sampling method. To address the challenges of\nsampling high-dimensional pdfs arising from Bayesian inverse problems governed\nby PDEs, we build on the stochastic Newton MCMC method. This method exploits\nproblem structure by taking as a proposal density a local Gaussian\napproximation of the posterior pdf, whose construction is made tractable by\ninvoking a low-rank approximation of its data misfit component of the Hessian.\nHere we introduce an approximation of the stochastic Newton proposal in which\nwe compute the low-rank-based Hessian at just the MAP point, and then reuse\nthis Hessian at each MCMC step. We compare the performance of the proposed\nmethod to the original stochastic Newton MCMC method and to an independence\nsampler. The comparison of the three methods is conducted on a synthetic ice\nsheet inverse problem. For this problem, the stochastic Newton MCMC method with\na MAP-based Hessian converges at least as rapidly as the original stochastic\nNewton MCMC method, but is far cheaper since it avoids recomputing the Hessian\nat each step. On the other hand, it is more expensive per sample than the\nindependence sampler; however, its convergence is significantly more rapid, and\nthus overall it is much cheaper. Finally, we present extensive analysis and\ninterpretation of the posterior distribution, and classify directions in\nparameter space based on the extent to which they are informed by the prior or\nthe observations.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 17:14:29 GMT"}, {"version": "v2", "created": "Fri, 11 Apr 2014 15:40:14 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Petra", "Noemi", ""], ["Martin", "James", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1308.6315", "submitter": "Paul McNicholas", "authors": "Katherine Morris and Paul D. McNicholas", "title": "Clustering, Classification, Discriminant Analysis, and Dimension\n  Reduction via Generalized Hyperbolic Mixtures", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.10.008", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for dimension reduction with clustering, classification, or\ndiscriminant analysis is introduced. This mixture model-based approach is based\non fitting generalized hyperbolic mixtures on a reduced subspace within the\nparadigm of model-based clustering, classification, or discriminant analysis. A\nreduced subspace of the data is derived by considering the extent to which\ngroup means and group covariances vary. The members of the subspace arise\nthrough linear combinations of the original data, and are ordered by importance\nvia the associated eigenvalues. The observations can be projected onto the\nsubspace, resulting in a set of variables that captures most of the clustering\ninformation available. The use of generalized hyperbolic mixtures gives a\nrobust framework capable of dealing with skewed clusters. Although dimension\nreduction is increasingly in demand across many application areas, the authors\nare most familiar with biological applications and so two of the five real data\nexamples are within that sphere. Simulated data are also used for illustration.\nThe approach introduced herein can be considered the most general such approach\navailable, and so we compare results to three special and limiting cases.\nComparisons with several well established techniques illustrate its promising\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Aug 2013 21:24:50 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 22:35:18 GMT"}, {"version": "v3", "created": "Tue, 23 Jun 2015 22:20:50 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Morris", "Katherine", ""], ["McNicholas", "Paul D.", ""]]}]