[{"id": "1603.00069", "submitter": "Pavlo Mozharovskyi", "authors": "Pavlo Mozharovskyi", "title": "Tukey depth: linear programming and applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Determining the representativeness of a point within a data cloud has\nrecently become a desirable task in multivariate analysis. The concept of\nstatistical depth function, which reflects centrality of an arbitrary point,\nappears to be useful and has been studied intensively during the last decades.\nHere the issue of exact computation of the classical Tukey data depth is\naddressed. The paper suggests an algorithm that exploits connection between the\nTukey depth and linear separability and is based on iterative application of\nlinear programming. The algorithm further develops the idea of the cone\nsegmentation of the Euclidean space and allows for efficient implementation due\nto the special search structure. The presentation is complemented by\nrelationship to similar concepts and examples of application.\n", "versions": [{"version": "v1", "created": "Mon, 29 Feb 2016 21:58:17 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Mozharovskyi", "Pavlo", ""]]}, {"id": "1603.00293", "submitter": "Ulrich Matter", "authors": "Ulrich Matter", "title": "RWebData: A High-Level Interface to the Programmable Web", "comments": "Working Paper. Keywords: R, programmable web, big public data, web\n  api, rest", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of the programmable web offers new opportunities for the empirically\ndriven social sciences. The access, compilation and preparation of data from\nthe programmable web for statistical analysis can, however, involve substantial\nup-front costs for the practical researcher. The R-package RWebData provides a\nhigh-level framework that allows data to be easily collected from the\nprogrammable web in a format that can directly be used for statistical analysis\nin R (R Core Team 2013) without bothering about the data's initial format and\nnesting structure. It was developed specifically for users who have no\nexperience with web technologies and merely use R as a statistical software.\nThe core idea and methodological contribution of the package are the\ndisentangling of parsing web data and mapping them with a generic algorithm\n(independent of the initial data structure) to a flat table-like\nrepresentation. This paper provides an overview of the high-level functions for\nR-users, explains the basic architecture of the package, and illustrates the\nimplemented data mapping algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 14:44:47 GMT"}, {"version": "v2", "created": "Tue, 19 Jul 2016 15:48:04 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Matter", "Ulrich", ""]]}, {"id": "1603.00297", "submitter": "Ahmed Alhamzawi", "authors": "Rahim Alhamzawi", "title": "Bayesian Quantile Regression for Ordinal Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the pioneering work by Koenker and Bassett (1978), quantile regression\nmodels and its applications have become increasingly popular and important for\nresearch in many areas. In this paper, a random effects ordinal quantile\nregression model is proposed for analysis of longitudinal data with ordinal\noutcome of interest. An efficient Gibbs sampling algorithm was derived for\nfitting the model to the data based on a location scale mixture representation\nof the skewed double exponential distribution. The proposed approach is\nillustrated using simulated data and a real data example. This is the first\nwork to discuss quantile regression for analysis of longitudinal data with\nordinal outcome.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2016 09:11:24 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Alhamzawi", "Rahim", ""]]}, {"id": "1603.00351", "submitter": "Yolanda Hagar", "authors": "Yolanda Hagar and Vanja Dukic", "title": "Analyzing Non-proportional Hazards: Use of the MRH Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we demonstrate the analysis of right-censored survival\noutcomes using the MRH package in R. The MRH package implements the\nmulti-resolution hazard (MRH) model, which is a Polya-tree based, Bayesian\nsemi-parametric method for flexible estimation of the hazard rate and covariate\neffects. The package allows for covariates to be included under the\nproportional and non-proportional hazards assumption, and for robust estimation\nof the hazard rate in periods of sparsely observed failures via a \"pruning\"\ntool.\n", "versions": [{"version": "v1", "created": "Tue, 1 Mar 2016 16:48:56 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Hagar", "Yolanda", ""], ["Dukic", "Vanja", ""]]}, {"id": "1603.00788", "submitter": "Alp Kucukelbir", "authors": "Alp Kucukelbir, Dustin Tran, Rajesh Ranganath, Andrew Gelman, David M.\n  Blei", "title": "Automatic Differentiation Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling is iterative. A scientist posits a simple model, fits\nit to her data, refines it according to her analysis, and repeats. However,\nfitting complex models to large data is a bottleneck in this process. Deriving\nalgorithms for new models can be both mathematically and computationally\nchallenging, which makes it difficult to efficiently cycle through the steps.\nTo this end, we develop automatic differentiation variational inference (ADVI).\nUsing our method, the scientist only provides a probabilistic model and a\ndataset, nothing else. ADVI automatically derives an efficient variational\ninference algorithm, freeing the scientist to refine and explore many models.\nADVI supports a broad class of models-no conjugacy assumptions are required. We\nstudy ADVI across ten different models and apply it to a dataset with millions\nof observations. ADVI is integrated into Stan, a probabilistic programming\nsystem; it is available for immediate use.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 16:43:15 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Kucukelbir", "Alp", ""], ["Tran", "Dustin", ""], ["Ranganath", "Rajesh", ""], ["Gelman", "Andrew", ""], ["Blei", "David M.", ""]]}, {"id": "1603.01136", "submitter": "Kody Law", "authors": "Pierre Del Moral, Ajay Jasra, Kody Law, and Yan Zhou", "title": "Multilevel Sequential Monte Carlo Samplers for Normalizing Constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the sequential Monte Carlo (SMC) approximation of\nratios of normalizing constants associated to posterior distributions which in\nprinciple rely on continuum models. Therefore, the Monte Carlo estimation error\nand the discrete approximation error must be balanced. A multilevel strategy is\nutilized to substantially reduce the cost to obtain a given error level in the\napproximation as compared to standard estimators. Two estimators are considered\nand relative variance bounds are given. The theoretical results are numerically\nillustrated for the example of identifying a parametrized permeability in an\nelliptic equation given point-wise observations of the pressure.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 15:36:49 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Del Moral", "Pierre", ""], ["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Zhou", "Yan", ""]]}, {"id": "1603.01372", "submitter": "Petr  Tichavsky", "authors": "Petr Tichavsky, Anh Huy Phan, Andrzej Cichocki", "title": "Numerical CP Decomposition of Some Difficult Tensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a numerical method is proposed for canonical polyadic (CP)\ndecomposition of small size tensors. The focus is primarily on decomposition of\ntensors that correspond to small matrix multiplications. Here, rank of the\ntensors is equal to the smallest number of scalar multiplications that are\nnecessary to accomplish the matrix multiplication. The proposed method is based\non a constrained Levenberg-Marquardt optimization. Numerical results indicate\nthe rank and border ranks of tensors that correspond to multiplication of\nmatrices of the size 2x3 and 3x2, 3x3 and 3x2, 3x3 and 3x3, and 3x4 and 4x3.\nThe ranks are 11, 15, 23 and 29, respectively. In particular, a novel algorithm\nfor multiplying the matrices of the sizes 3x3 and 3x2 with 15 multiplications\nis presented.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 08:18:11 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Tichavsky", "Petr", ""], ["Phan", "Anh Huy", ""], ["Cichocki", "Andrzej", ""]]}, {"id": "1603.01551", "submitter": "Jem Corcoran", "authors": "Jem Corcoran and Dale Jennings and Paul VaughanMiller", "title": "Perfect and $\\varepsilon$-Perfect Simulation Methods for the One\n  Dimensional Kac Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review the derivation of the Kac master equation model for random\ncollisions of particles, its relationship to the Poisson process, and existing\nalgorithms for simulating values from the marginal distribution of velocity for\na single particle at any given time. We describe and implement a new algorithm\nthat efficiently and more fully leverages properties of the Poisson process,\nshow that it performs at least as well as existing methods, and give empirical\nevidence that it may perform better at capturing the tails of the single\nparticle velocity distribution. Finally, we derive and implement a novel\n\"$\\varepsilon$-perfect sampling\" algorithm for the limiting marginal\ndistribution as time goes to infinity. In this case the importance is a proof\nof concept that has the potential to be expanded to more interesting (DSMC)\ndirect simulation Monte Carlo applications.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 17:56:45 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Corcoran", "Jem", ""], ["Jennings", "Dale", ""], ["VaughanMiller", "Paul", ""]]}, {"id": "1603.01765", "submitter": "Mark Tygert", "authors": "Arthur Szlam, Andrew Tulloch, and Mark Tygert", "title": "Accurate principal component analysis via a few iterations of\n  alternating least squares", "comments": "9 pages, 3 tables", "journal-ref": "SIAM Journal on Matrix Analysis and Applications, 38 (2): 425-433,\n  2017", "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few iterations of alternating least squares with a random starting point\nprovably suffice to produce nearly optimal spectral- and Frobenius-norm\naccuracies of low-rank approximations to a matrix; iterating to convergence is\nunnecessary. Thus, software implementing alternating least squares can be\nretrofitted via appropriate setting of parameters to calculate nearly optimally\naccurate low-rank approximations highly efficiently, with no need for\nconvergence.\n", "versions": [{"version": "v1", "created": "Sat, 5 Mar 2016 22:30:38 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Szlam", "Arthur", ""], ["Tulloch", "Andrew", ""], ["Tygert", "Mark", ""]]}, {"id": "1603.01882", "submitter": "Robert Zinkov", "authors": "Robert Zinkov, Chung-chieh Shan", "title": "Composing inference algorithms as program transformations", "comments": "10 pages, 5 figures. To appear in Proceedings of the 33rd Conference\n  on Uncertainty in Artificial Intelligence (UAI2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic inference procedures are usually coded painstakingly from\nscratch, for each target model and each inference algorithm. We reduce this\neffort by generating inference procedures from models automatically. We make\nthis code generation modular by decomposing inference algorithms into reusable\nprogram-to-program transformations. These transformations perform exact\ninference as well as generate probabilistic programs that compute expectations,\ndensities, and MCMC samples. The resulting inference procedures are about as\naccurate and fast as other probabilistic programming systems on real-world\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 21:30:10 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 16:01:42 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Zinkov", "Robert", ""], ["Shan", "Chung-chieh", ""]]}, {"id": "1603.01897", "submitter": "Gael Martin Prof", "authors": "Don S. Poskitt, Gael M. Martin and Simone D. Grose", "title": "Bias Correction of Semiparametric Long Memory Parameter Estimators via\n  the Pre-filtered Sieve Bootstrap", "comments": "This is an extended version (with additional numerical results) of\n  the paper with the same name forthcoming in Econometric Theory, 2016. arXiv\n  admin note: substantial text overlap with arXiv:1402.6781", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates bootstrap-based bias correction of semiparametric\nestimators of the long memory parameter, $d$, in fractionally integrated\nprocesses. The re-sampling method involves the application of the sieve\nbootstrap to data pre-filtered by a preliminary semiparametric estimate of the\nlong memory parameter. Theoretical justification for using the bootstrap\ntechnique to bias adjust log periodogram and semiparametric local Whittle\nestimators of the memory parameter is provided in the case where the true value\nof $d$ lies in the range $0\\leq d<0.5$. That the bootstrap method provides\nconfidence intervals with the correct asymptotic coverage is also proven, with\nthe intervals shown to adjust explicitly for bias, as estimated via the\nbootstrap. Simulation evidence comparing the performance of the bootstrap bias\ncorrection with analytical bias-correction techniques is presented. The\nbootstrap method is shown to produce notable bias reductions, in particular\nwhen applied to an estimator for which some degree of bias reduction has\nalready been accomplished by analytical means.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2016 23:29:05 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Poskitt", "Don S.", ""], ["Martin", "Gael M.", ""], ["Grose", "Simone D.", ""]]}, {"id": "1603.02532", "submitter": "Antti Honkela", "authors": "Otte Hein\\\"avaara, Janne Lepp\\\"a-aho, Jukka Corander and Antti Honkela", "title": "On the inconsistency of $\\ell_1$-penalised sparse precision matrix\n  estimation", "comments": "9 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various $\\ell_1$-penalised estimation methods such as graphical lasso and\nCLIME are widely used for sparse precision matrix estimation. Many of these\nmethods have been shown to be consistent under various quantitative assumptions\nabout the underlying true covariance matrix. Intuitively, these conditions are\nrelated to situations where the penalty term will dominate the optimisation. In\nthis paper, we explore the consistency of $\\ell_1$-based methods for a class of\nsparse latent variable -like models, which are strongly motivated by several\ntypes of applications. We show that all $\\ell_1$-based methods fail\ndramatically for models with nearly linear dependencies between the variables.\nWe also study the consistency on models derived from real gene expression data\nand note that the assumptions needed for consistency never hold even for modest\nsized gene networks and $\\ell_1$-based methods also become unreliable in\npractice for larger networks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 14:24:11 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Hein\u00e4vaara", "Otte", ""], ["Lepp\u00e4-aho", "Janne", ""], ["Corander", "Jukka", ""], ["Honkela", "Antti", ""]]}, {"id": "1603.02834", "submitter": "Jere Koskela", "authors": "Jere Koskela, Dario Spano and Paul A. Jenkins", "title": "Inference and rare event simulation for stopped Markov processes via\n  reverse-time sequential Monte Carlo", "comments": "21 pages, 6 figures", "journal-ref": "Statistics and Computing 28(1):131-144, 2018", "doi": "10.1007/s11222-017-9722-1", "report-no": null, "categories": "stat.CO math.PR q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a sequential Monte Carlo algorithm for Markov chain trajectories\nwith proposals constructed in reverse time, which is advantageous when paths\nare conditioned to end in a rare set. The reverse time proposal distribution is\nconstructed by approximating the ratio of Green's functions in Nagasawa's\nformula. Conditioning arguments can be used to interpret these ratios as\nlow-dimensional conditional sampling distributions of some coordinates of the\nprocess given the others. Hence the difficulty in designing SMC proposals in\nhigh dimension is greatly reduced. We illustrate our method on estimating an\noverflow probability in a queueing model, the probability that a diffusion\nfollows a narrowing corridor, and the initial location of an infection in an\nepidemic model on a network.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 10:26:24 GMT"}, {"version": "v2", "created": "Wed, 13 Jul 2016 10:02:39 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2017 13:37:02 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""], ["Spano", "Dario", ""], ["Jenkins", "Paul A.", ""]]}, {"id": "1603.03141", "submitter": "Ricardo Oliveros-Ramos", "authors": "Ricardo Oliveros-Ramos and Yunne-Jai Shin", "title": "Calibrar: an R package for fitting complex ecological models", "comments": "15 pages, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fitting or parameter estimation of complex ecological models is a\nchallenging optimization task, with a notable lack of tools for fitting complex\nstochastic models. calibrar is an R package that has been developed for fitting\ncomplex ecological models to data, including complex Individual Based Models.\nIt is a generic tool that can be used for any type of model, especially those\nwith non-differentiable objective functions. calibrar supports multiple phases\nand constrained optimization. It implements maximum likelihood estimation\nmethods and automated construction of the objective function from simulated\nmodel outputs. User-level expertise in R is necessary to handle calibration\nexperiments with calibrar, but there is no need to modify the model's code,\nwhich can be programmed in any language. For more experienced users, calibrar\nallows the implementation of user-defined objective functions. The package\nsource code is fully accessible from github\n(htpps://roliveros-ramos.github.com/calibrar) and can be installed directly\nfrom CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 04:00:10 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Oliveros-Ramos", "Ricardo", ""], ["Shin", "Yunne-Jai", ""]]}, {"id": "1603.03316", "submitter": "Marius Thomas", "authors": "Marius Thomas and Bj\\\"orn Bornkamp", "title": "Comparing Approaches to Treatment Effect Estimation for Subgroups in\n  Clinical Trials", "comments": "Version 2 is a minor revision of the original manuscript based on\n  reviewers' comments. The title of the manuscript has been changed and several\n  small remarks have been added throughout the document, that should improve\n  the presentation of the methods and the discussion of results. Moreover\n  supplementary material has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying subgroups, which respond differently to a treatment, both in\nterms of efficacy and safety, is an important part of drug development. A\nwell-known challenge in exploratory subgroup analyses is the small sample size\nin the considered subgroups, which is usually too low to allow for definite\ncomparisons. In early phase trials this problem is further exaggerated, because\nlimited or no clinical prior information on the drug and plausible subgroups is\navailable. We evaluate novel strategies for treatment effect estimation in\nthese settings in a simulation study motivated by real clinical trial\nsituations. We compare several approaches to estimate treatment effects for\nselected subgroups, employing model averaging, resampling and Lasso regression\nmethods. Two subgroup identification approaches are employed, one based on\ncategorization of covariates and the other based on splines. Our results show\nthat naive estimation of the treatment effect, which ignores that a selection\nhas taken place, leads to bias and overoptimistic conclusions. For the\nconsidered simulation scenarios virtually all evaluated novel methods provide\nmore adequate estimates of the treatment effect for selected subgroups, in\nterms of bias, MSE and confidence interval coverage.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 16:19:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 09:47:11 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Thomas", "Marius", ""], ["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1603.03491", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried and Qingyun Sun", "title": "Bayesian Opponent Exploitation in Imperfect-Information Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two fundamental problems in computational game theory are computing a Nash\nequilibrium and learning to exploit opponents given observations of their play\n(opponent exploitation). The latter is perhaps even more important than the\nformer: Nash equilibrium does not have a compelling theoretical justification\nin game classes other than two-player zero-sum, and for all games one can\npotentially do better by exploiting perceived weaknesses of the opponent than\nby following a static equilibrium strategy throughout the match. The natural\nsetting for opponent exploitation is the Bayesian setting where we have a prior\nmodel that is integrated with observations to create a posterior opponent model\nthat we respond to. The most natural, and a well-studied prior distribution is\nthe Dirichlet distribution. An exact polynomial-time algorithm is known for\nbest-responding to the posterior distribution for an opponent assuming a\nDirichlet prior with multinomial sampling in normal-form games; however, for\nimperfect-information games the best known algorithm is based on approximating\nan infinite integral without theoretical guarantees. We present the first exact\nalgorithm for a natural class of imperfect-information games. We demonstrate\nthat our algorithm runs quickly in practice and outperforms the best prior\napproaches. We also present an algorithm for the uniform prior setting.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2016 23:50:51 GMT"}, {"version": "v2", "created": "Sat, 17 Sep 2016 19:35:22 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 06:23:30 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2017 22:04:34 GMT"}, {"version": "v5", "created": "Wed, 27 Jun 2018 02:35:11 GMT"}, {"version": "v6", "created": "Thu, 28 Jun 2018 00:55:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Ganzfried", "Sam", ""], ["Sun", "Qingyun", ""]]}, {"id": "1603.03510", "submitter": "Radu V. Craiu", "authors": "Jinyoung Yang, Evgeny Levi, Radu V. Craiu and Jeffrey S. Rosenthal", "title": "Adaptive Component-wise Multiple-Try Metropolis Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most widely used samplers in practice is the component-wise\nMetropolis-Hastings (CMH) sampler that updates in turn the components of a\nvector valued Markov chain using accept-reject moves generated from a proposal\ndistribution. When the target distribution of a Markov chain is irregularly\nshaped, a `good' proposal distribution for one part of the state space might be\na `poor' one for another part of the state space. We consider a component-wise\nmultiple-try Metropolis (CMTM) algorithm that can automatically choose from a\nset of candidate moves sampled from different distributions. The computational\nefficiency is increased using an adaptation rule for the CMTM algorithm that\ndynamically builds a better set of proposal distributions as the Markov chain\nruns. The ergodicity of the adaptive chain is demonstrated theoretically. The\nperformance is studied via simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 03:22:09 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 14:15:52 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Yang", "Jinyoung", ""], ["Levi", "Evgeny", ""], ["Craiu", "Radu V.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1603.03819", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho, Jason Xu, Forrest W. Crawford, Vladimir N. Minin, Marc\n  A. Suchard", "title": "Birth/birth-death processes and their computable transition\n  probabilities with biological applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birth-death processes track the size of a univariate population, but many\nbiological systems involve interaction between populations, necessitating\nmodels for two or more populations simultaneously. A lack of efficient methods\nfor evaluating finite-time transition probabilities of bivariate processes,\nhowever, has restricted statistical inference in these models. Researchers rely\non computationally expensive methods such as matrix exponentiation or Monte\nCarlo approximation, restricting likelihood-based inference to small systems,\nor indirect methods such as approximate Bayesian computation. In this paper, we\nintroduce the birth(death)/birth-death process, a tractable bivariate extension\nof the birth-death process. We develop an efficient and robust algorithm to\ncalculate the transition probabilities of birth(death)/birth-death processes\nusing a continued fraction representation of their Laplace transforms. Next, we\nidentify several exemplary models arising in molecular epidemiology,\nmacro-parasite evolution, and infectious disease modeling that fall within this\nclass, and demonstrate advantages of our proposed method over existing\napproaches to inference in these models. Notably, the ubiquitous stochastic\nsusceptible-infectious-removed (SIR) model falls within this class, and we\nemphasize that computable transition probabilities newly enable direct\ninference of parameters in the SIR model. We also propose a very fast method\nfor approximating the transition probabilities under the SIR model via a novel\nbranching process simplification, and compare it to the continued fraction\nrepresentation method with application to the 17th century plague in Eyam.\nAlthough the two methods produce similar maximum a posteriori estimates, the\nbranching process approximation fails to capture the correlation structure in\nthe joint posterior distribution.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2016 23:17:12 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 16:35:35 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Xu", "Jason", ""], ["Crawford", "Forrest W.", ""], ["Minin", "Vladimir N.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1603.04166", "submitter": "Zdravko Botev", "authors": "Z. I. Botev", "title": "The Normal Law Under Linear Restrictions: Simulation and Estimation via\n  Minimax Tilting", "comments": "27 pages; 4 figures, Journal of the Royal Statistical Society: Series\n  B (Statistical Methodology) (2016)", "journal-ref": null, "doi": "10.1111/rssb.12162", "report-no": "Australian Research Council grant DE140100993", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation from the truncated multivariate normal distribution in high\ndimensions is a recurrent problem in statistical computing, and is typically\nonly feasible using approximate MCMC sampling. In this article we propose a\nminimax tilting method for exact iid simulation from the truncated multivariate\nnormal distribution. The new methodology provides both a method for simulation\nand an efficient estimator to hitherto intractable Gaussian integrals. We prove\nthat the estimator possesses a rare vanishing relative error asymptotic\nproperty. Numerical experiments suggest that the proposed scheme is accurate in\na wide range of setups for which competing estimation schemes fail. We give an\napplication to exact iid simulation from the Bayesian posterior of the probit\nregression model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 09:02:13 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Botev", "Z. I.", ""]]}, {"id": "1603.04229", "submitter": "Thomas Nagler", "authors": "Thomas Nagler", "title": "kdecopula: An R Package for the Kernel Estimation of Bivariate Copula\n  Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the R package kdecopula (current version 0.9.0), which provides\nfast implementations of various kernel estimators for the copula density. Due\nto a variety of available plotting options it is particularly useful for the\nexploratory analysis of dependence structures. It can be further used for\naccurate nonparametric estimation of copula densities and resampling. The\nimplementation features spline interpolation of the estimates to allow for fast\nevaluation of density estimates and integrals thereof. We utilize this for a\nfast renormalization scheme that ensures that estimates are bona fide copula\ndensities and additionally improves the estimators' accuracy. The performance\nof the methods is illustrated by simulations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 12:12:53 GMT"}, {"version": "v2", "created": "Sat, 14 May 2016 16:26:39 GMT"}, {"version": "v3", "created": "Fri, 23 Dec 2016 13:52:46 GMT"}, {"version": "v4", "created": "Mon, 15 May 2017 22:59:52 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Nagler", "Thomas", ""]]}, {"id": "1603.04360", "submitter": "Feng Liang", "authors": "Jin Wang, Feng Liang, Yuan Ji", "title": "An Ensemble EM Algorithm for Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian approach to variable selection in the context of linear\nregression. Motivated by a recent work by Rockova and George (2014), we propose\nan EM algorithm that returns the MAP estimate of the set of relevant variables.\nDue to its particular updating scheme, our algorithm can be implemented\nefficiently without inverting a large matrix in each iteration and therefore\ncan scale up with big data. We also show that the MAP estimate returned by our\nEM algorithm achieves variable selection consistency even when $p$ diverges\nwith $n$. In practice, our algorithm could get stuck with local modes, a common\nproblem with EM algorithms. To address this issue, we propose an ensemble EM\nalgorithm, in which we repeatedly apply the EM algorithm on a subset of the\nsamples with a subset of the covariates, and then aggregate the variable\nselection results across those bootstrap replicates. Empirical studies have\ndemonstrated the superior performance of the ensemble EM algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2016 17:51:05 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Wang", "Jin", ""], ["Liang", "Feng", ""], ["Ji", "Yuan", ""]]}, {"id": "1603.04577", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo, Tuhin Bhakta, Morten Jakobsen, and Geir N{\\ae}vdal", "title": "An Ensemble 4D Seismic History Matching Framework with Sparse\n  Representation Based on Wavelet Multiresolution Analysis", "comments": "SPE-180025-MS, SPE Bergen One Day Seminar", "journal-ref": "SPE Journal, 2017, paper number SPE-180025-PA", "doi": "10.2118/180025-PA", "report-no": null, "categories": "physics.data-an math.NA math.OC physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose an ensemble 4D seismic history matching framework for\nreservoir characterization. Compared to similar existing frameworks in\nreservoir engineering community, the proposed one consists of some relatively\nnew ingredients, in terms of the type of seismic data in choice, wavelet\nmultiresolution analysis for the chosen seismic data and related data noise\nestimation, and the use of recently developed iterative ensemble history\nmatching algorithms.\n  Typical seismic data used for history matching, such as acoustic impedance,\nare inverted quantities, whereas extra uncertainties may arise during the\ninversion processes. In the proposed framework we avoid such intermediate\ninversion processes. In addition, we also adopt wavelet-based sparse\nrepresentation to reduce data size. Concretely, we use intercept and gradient\nattributes derived from amplitude versus angle (AVA) data, apply multilevel\ndiscrete wavelet transforms (DWT) to attribute data, and estimate noise level\nof resulting wavelet coefficients. We then select the wavelet coefficients\nabove a certain threshold value, and history-match these leading wavelet\ncoefficients using an iterative ensemble smoother.\n  (The rest of the abstract is omitted for exceeding the limit of length)\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 07:27:10 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Luo", "Xiaodong", ""], ["Bhakta", "Tuhin", ""], ["Jakobsen", "Morten", ""], ["N\u00e6vdal", "Geir", ""]]}, {"id": "1603.04613", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Luke R. Lloyd-Jones, Geoffrey J. McLachlan", "title": "A Block Minorization--Maximization Algorithm for Heteroscedastic\n  Regression", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2016.2586180", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computation of the maximum likelihood (ML) estimator for heteroscedastic\nregression models is considered. The traditional Newton algorithms for the\nproblem require matrix multiplications and inversions, which are bottlenecks in\nmodern Big Data contexts. A new Big Data-appropriate minorization--maximization\n(MM) algorithm is considered for the computation of the ML estimator. The MM\nalgorithm is proved to generate monotonically increasing sequences of\nlikelihood values and to be convergent to a stationary point of the\nlog-likelihood function. A distributed and parallel implementation of the MM\nalgorithm is presented and the MM algorithm is shown to have differing time\ncomplexity to the Newton algorithm. Simulation studies demonstrate that the MM\nalgorithm improves upon the computation time of the Newton algorithm in some\npractical scenarios where the number of observations is large.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2016 09:40:29 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 08:42:09 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Lloyd-Jones", "Luke R.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1603.05038", "submitter": "Reik Donner", "authors": "Jonathan F. Siegmund, Nicole Siegmund, Reik V. Donner", "title": "CoinCalc -- A new R package for quantifying simultaneities of event\n  series", "comments": null, "journal-ref": null, "doi": "10.1016/j.cageo.2016.10.004", "report-no": null, "categories": "stat.CO physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the new R package CoinCalc for performing event coincidence\nanalysis (ECA), a novel statistical method to quantify the simultaneity of\nevents contained in two series of observations, either as simultaneous or\nlagged coincidences within a user-specific temporal tolerance window. The\npackage also provides different analytical as well as surrogate-based\nsignificance tests (valid under different assumptions about the nature of the\nobserved event series) as well as an intuitive visualization of the identified\ncoincidences. We demonstrate the usage of CoinCalc based on two typical\ngeoscientific example problems addressing the relationship between\nmeteorological extremes and plant phenology as well as that between soil\nproperties and land cover.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2016 11:23:22 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Siegmund", "Jonathan F.", ""], ["Siegmund", "Nicole", ""], ["Donner", "Reik V.", ""]]}, {"id": "1603.05486", "submitter": "Andreas Svensson", "authors": "Andreas Svensson and Thomas B. Sch\\\"on", "title": "A flexible state space model for learning nonlinear dynamical systems", "comments": null, "journal-ref": "Automatica 80(2017), page 189-199", "doi": null, "report-no": null, "categories": "stat.CO cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a nonlinear state-space model with the state transition and\nobservation functions expressed as basis function expansions. The coefficients\nin the basis function expansions are learned from data. Using a connection to\nGaussian processes we also develop priors on the coefficients, for tuning the\nmodel flexibility and to prevent overfitting to data, akin to a Gaussian\nprocess state-space model. The priors can alternatively be seen as a\nregularization, and helps the model in generalizing the data without\nsacrificing the richness offered by the basis function expansion. To learn the\ncoefficients and other unknown parameters efficiently, we tailor an algorithm\nusing state-of-the-art sequential Monte Carlo methods, which comes with\ntheoretical guarantees on the learning. Our approach indicates promising\nresults when evaluated on a classical benchmark as well as real data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 13:51:17 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 09:41:05 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1603.05522", "submitter": "Sumeetpal S Singh", "authors": "Lan Jiang, Sumeetpal S. Singh", "title": "Tracking multiple moving objects in images using Markov Chain Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Bayesian state and parameter learning algorithm for multiple target\ntracking (MTT) models with image observations is proposed. Specifically, a\nMarkov chain Monte Carlo algorithm is designed to sample from the posterior\ndistribution of the unknown number of targets, their birth and death times,\nstates and model parameters, which constitutes the complete solution to the\ntracking problem. The conventional approach is to pre-process the images to\nextract point observations and then perform tracking. We model the image\ngeneration process directly to avoid potential loss of information when\nextracting point observations. Numerical examples show that our algorithm has\nimproved tracking performance over commonly used techniques, for both synthetic\nexamples and real florescent microscopy data, especially in the case of dim\ntargets with overlapping illuminated regions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 15:03:10 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1603.06119", "submitter": "Zheng Zhang", "authors": "Zheng Zhang and Tsui-Wei Weng and Luca Daniel", "title": "A Big-Data Approach to Handle Process Variations: Uncertainty\n  Quantification by Tensor Recovery", "comments": "2016 IEEE 20th Workshop on Signal and Power Integrity (SPI), 8-11 May\n  2016, Turin, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic spectral methods have become a popular technique to quantify the\nuncertainties of nano-scale devices and circuits. They are much more efficient\nthan Monte Carlo for certain design cases with a small number of random\nparameters. However, their computational cost significantly increases as the\nnumber of random parameters increases. This paper presents a big-data approach\nto solve high-dimensional uncertainty quantification problems. Specifically, we\nsimulate integrated circuits and MEMS at only a small number of quadrature\nsamples, then, a huge number of (e.g., $1.5 \\times 10^{27}$) solution samples\nare estimated from the available small-size (e.g., $500$) solution samples via\na low-rank and tensor-recovery method. Numerical results show that our\nalgorithm can easily extend the applicability of tensor-product stochastic\ncollocation to IC and MEMS problems with over 50 random parameters, whereas the\ntraditional algorithm can only handle several random parameters.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2016 17:38:52 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Zhang", "Zheng", ""], ["Weng", "Tsui-Wei", ""], ["Daniel", "Luca", ""]]}, {"id": "1603.06216", "submitter": "Henri Nurminen M.Sc.", "authors": "Henri Nurminen, Tohid Ardeshiri, Robert Piche, and Fredrik Gustafsson", "title": "Skew-t inference with improved covariance matrix approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering and smoothing algorithms for linear discrete-time state-space\nmodels with skew-t distributed measurement noise are presented. The proposed\nalgorithms improve upon our earlier proposed filter and smoother using the mean\nfield variational Bayes approximation of the posterior distribution to a skew-t\nlikelihood and normal prior. Our simulations show that the proposed variational\nBayes approximation gives a more accurate approximation of the posterior\ncovariance matrix than our earlier proposed method. Furthermore, the novel\nfilter and smoother outperform our earlier proposed methods and conventional\nlow complexity alternatives in accuracy and speed.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 14:05:47 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Nurminen", "Henri", ""], ["Ardeshiri", "Tohid", ""], ["Piche", "Robert", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1603.06381", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Kody Law, Yan Zhou", "title": "Forward and Inverse Uncertainty Quantification using Multilevel Monte\n  Carlo Algorithms for an Elliptic Nonlocal Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers uncertainty quantification for an elliptic nonlocal\nequation. In particular, it is assumed that the parameters which define the\nkernel in the nonlocal operator are uncertain and a priori distributed\naccording to a probability measure. It is shown that the induced probability\nmeasure on some quantities of interest arising from functionals of the solution\nto the equation with random inputs is well-defined; as is the posterior\ndistribution on parameters given observations. As the elliptic nonlocal\nequation cannot be solved approximate posteriors are constructed. The\nmultilevel Monte Carlo (MLMC) and multilevel sequential Monte Carlo (MLSMC)\nsampling algorithms are used for a priori and a posteriori estimation,\nrespectively, of quantities of interest. These algorithms reduce the amount of\nwork to estimate posterior expectations, for a given level of error, relative\nto Monte Carlo and i.i.d. sampling from the posterior at a given level of\napproximation of the solution of the elliptic nonlocal equation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2016 11:00:29 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Zhou", "Yan", ""]]}, {"id": "1603.06687", "submitter": "Gordon Smyth", "authors": "G\\\"oknur Giner and Gordon K. Smyth", "title": "statmod: Probability Calculations for the Inverse Gaussian Distribution", "comments": "18 pages, 2 figures. Accepted for publication in The R Journal,\n  Volume 8 (2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse Gaussian distribution (IGD) is a well known and often used\nprobability distribution for which fully reliable numerical algorithms have not\nbeen available. Our aim in this article is to develop software for this\ndistribution for the R programming environment. We develop fast, reliable basic\nprobability functions (dinvgauss, pinvgauss, qinvgauss and rinvgauss) that work\nfor all possible parameter values and which achieve close to full machine\naccuracy. The most challenging task is to compute quantiles for given\ncumulative probabilities and we develop a simple but elegant mathematical\nsolution to this problem. We show that Newton's method for finding the\nquantiles of a IGD always converges monotonically when started from the mode of\nthe distribution. Simple Taylor series expansions are used to improve accuracy\non the log-scale. The IGD probability functions provide the same options and\nobey the same conventions as do probability functions provided in the standard\nR stats package. The IGD functions are part of the statmod package available\nfrom the CRAN repository.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 07:01:59 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 01:01:33 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Giner", "G\u00f6knur", ""], ["Smyth", "Gordon K.", ""]]}, {"id": "1603.06907", "submitter": "Nuno Fachada", "authors": "Nuno Fachada, Jo\\~ao Rodrigues, Vitor V. Lopes, Rui C. Martins,\n  Agostinho C. Rosa", "title": "micompr: An R Package for Multivariate Independent Comparison of\n  Observations", "comments": "The peer-reviewed version of this paper is published in The R Journal\n  at\n  https://journal.r-project.org/archive/2016-2/fachada-rodrigues-lopes-etal.pdf\n  . This version is typeset by the authors and differs only in pagination and\n  typographical detail", "journal-ref": "The R Journal, 8(2): 405-420 (2016)", "doi": "10.32614/RJ-2016-055", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package micompr implements a procedure for assessing if two or more\nmultivariate samples are drawn from the same distribution. The procedure uses\nprincipal component analysis to convert multivariate observations into a set of\nlinearly uncorrelated statistical measures, which are then compared using a\nnumber of statistical methods. This technique is independent of the\ndistributional properties of samples and automatically selects features that\nbest explain their differences. The procedure is appropriate for comparing\nsamples of time series, images, spectrometric measures or similar\nhigh-dimension multivariate observations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Mar 2016 18:57:41 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 19:43:40 GMT"}, {"version": "v3", "created": "Sun, 16 Oct 2016 16:35:57 GMT"}, {"version": "v4", "created": "Fri, 17 Feb 2017 11:12:09 GMT"}, {"version": "v5", "created": "Tue, 21 Feb 2017 19:06:05 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Fachada", "Nuno", ""], ["Rodrigues", "Jo\u00e3o", ""], ["Lopes", "Vitor V.", ""], ["Martins", "Rui C.", ""], ["Rosa", "Agostinho C.", ""]]}, {"id": "1603.07117", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Michel Broniatowski", "title": "A Proximal Point Algorithm for Minimum Divergence Estimators with\n  Application to Mixture Models", "comments": "19 pages. Article submitted to Journal Entropy, special issue :\n  Diffierential Geometrical Theory of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators derived from a divergence criterion such as $\\varphi-$divergences\nare generally more robust than the maximum likelihood ones. We are interested\nin particular in the so-called MD$\\varphi$DE, an estimator built using a dual\nrepresentation of $\\varphi$--divergences. We present in this paper an iterative\nproximal point algorithm which permits to calculate such estimator. This\nalgorithm contains by its construction the well-known EM algorithm. Our work is\nbased on the paper of \\citep{Tseng} on the likelihood function. We provide\nseveral convergence properties of the sequence generated by the algorithm, and\nimprove the existing results by relaxing the identifiability condition on the\nproximal term, a condition which is not verified for most mixture models and\nhard to be verified for non mixture ones. Since convergence analysis uses\nregularity conditions (continuity and differentiability) of the objective\nfunction, which has a supremal form, we find it useful to present some\nanalytical approaches for studying such functions. Convergence of the EM\nalgorithm is discussed here again in a Gaussian and Weibull mixtures in the\nspirit of our approach. Simulations are provided to confirm the validity of our\nwork and the robustness of the resulting estimators against outliers.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 10:19:16 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 16:50:39 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Broniatowski", "Michel", ""]]}, {"id": "1603.07181", "submitter": "Paolo Perrone", "authors": "Paolo Perrone and Nihat Ay", "title": "Iterative Scaling Algorithm for Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we define a procedure for evaluating KL-projections (I- and\nrI-projections) of channels. These can be useful in the decomposition of mutual\ninformation between input and outputs, e.g. to quantify synergies and\ninteractions of different orders, as well as information integration and other\nrelated measures of complexity.\n  The algorithm is a generalization of the standard iterative scaling\nalgorithm, which we here extend from probability distributions to channels\n(also known as transition kernels).\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 13:48:04 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 15:07:17 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Perrone", "Paolo", ""], ["Ay", "Nihat", ""]]}, {"id": "1603.07237", "submitter": "Coralie Merle", "authors": "Coralie Merle (IMAG, CBGP, IBC), Rapha\\\"el Leblois (CBGP, IBC),\n  Fran\\c{c}ois Rousset (ISEM, IBC), Pierre Pudlo (I2M, IBC)", "title": "Resampling: an improvement of Importance Sampling in varying population\n  size models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential importance sampling algorithms have been defined to estimate\nlikelihoods in models of ancestral population processes. However, these\nalgorithms are based on features of the models with constant population size,\nand become inefficient when the population size varies in time, making\nlikelihood-based inferences difficult in many demographic situations. In this\nwork, we modify a previous sequential importance sampling algorithm to improve\nthe efficiency of the likelihood estimation. Our procedure is still based on\nfeatures of the model with constant size, but uses a resampling technique with\na new resampling probability distribution depending on the pairwise composite\nlikelihood. We tested our algorithm, called sequential importance sampling with\nresampling (SISR) on simulated data sets under different demographic cases. In\nmost cases, we divided the computational cost by two for the same accuracy of\ninference, in some cases even by one hundred. This study provides the first\nassessment of the impact of such resampling techniques on parameter inference\nusing sequential importance sampling, and extends the range of situations where\nlikelihood inferences can be easily performed.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2016 15:31:05 GMT"}], "update_date": "2016-03-24", "authors_parsed": [["Merle", "Coralie", "", "IMAG, CBGP, IBC"], ["Leblois", "Rapha\u00ebl", "", "CBGP, IBC"], ["Rousset", "Fran\u00e7ois", "", "ISEM, IBC"], ["Pudlo", "Pierre", "", "I2M, IBC"]]}, {"id": "1603.07888", "submitter": "Xiaoyu Liu", "authors": "Xiaoyu Liu and Serge Guillas", "title": "Dimension reduction for Gaussian process emulation: an application to\n  the influence of bathymetry on tsunami heights", "comments": "26 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High accuracy complex computer models, or simulators, require large resources\nin time and memory to produce realistic results. Statistical emulators are\ncomputationally cheap approximations of such simulators. They can be built to\nreplace simulators for various purposes, such as the propagation of\nuncertainties from inputs to outputs or the calibration of some internal\nparameters against observations. However, when the input space is of high\ndimension, the construction of an emulator can become prohibitively expensive.\nIn this paper, we introduce a joint framework merging emulation with dimension\nreduction in order to overcome this hurdle. The gradient-based kernel dimension\nreduction technique is chosen due to its ability to drastically decrease\ndimensionality with little loss in information. The Gaussian process emulation\ntechnique is combined with this dimension reduction approach. Our proposed\napproach provides an answer to the dimension reduction issue in emulation for a\nwide range of simulation problems that cannot be tackled using existing\nmethods. The efficiency and accuracy of the proposed framework is demonstrated\ntheoretically, and compared with other methods on an elliptic partial\ndifferential equation (PDE) problem. We finally present a realistic application\nto tsunami modeling. The uncertainties in the bathymetry (seafloor elevation)\nare modeled as high-dimensional realizations of a spatial process using a\ngeostatistical approach. Our dimension-reduced emulation enables us to compute\nthe impact of these uncertainties on resulting possible tsunami wave heights\nnear-shore and on-shore. We observe a significant increase in the spread of\nuncertainties in the tsunami heights due to the contribution of the bathymetry\nuncertainties. These results highlight the need to include the effect of\nuncertainties in the bathymetry in tsunami early warnings and risk assessments.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 11:59:53 GMT"}, {"version": "v2", "created": "Wed, 24 Aug 2016 12:17:55 GMT"}], "update_date": "2016-08-25", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Guillas", "Serge", ""]]}, {"id": "1603.08088", "submitter": "Charles-Edouard Br\\'ehier", "authors": "Michel Bena\\\"im, Charles-Edouard Br\\'ehier", "title": "Convergence of Adaptive Biasing Potential methods for diffusions", "comments": null, "journal-ref": null, "doi": "10.1016/j.crma.2016.05.011", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the consistency of an adaptive importance sampling strategy based on\nbiasing the potential energy function $V$ of a diffusion process\n$dX_t^0=-\\nabla V(X_t^0)dt+dW_t$; for the sake of simplicity, periodic boundary\nconditions are assumed, so that $X_t^0$ lives on the flat $d$-dimensional\ntorus. The goal is to sample its invariant distribution\n$\\mu=Z^{-1}\\exp\\bigl(-V(x)\\bigr)\\,dx$. The bias $V_t-V$, where $V_t$ is the new\n(random and time-dependent) potential function, acts only on some coordinates\nof the system, and is designed to flatten the corresponding empirical\noccupation measure of the diffusion $X$ in the large time regime.\n  The diffusion process writes $dX_t=-\\nabla V_t(X_t)dt+dW_t$, where the bias\n$V_t-V$ is function of the key quantity $\\overline{\\mu}_t$: a probability\noccupation measure which depends on the past of the process, {\\it i.e.} on\n$(X_s)_{s\\in [0,t]}$. We are thus dealing with a self-interacting diffusion.\n  In this note, we prove that when $t$ goes to infinity, $\\overline{\\mu}_t$\nalmost surely converges to $\\mu$. Moreover, the approach is justified by the\nconvergence of the bias to a limit which has an intepretation in terms of a\nfree energy.\n  The main argument is a change of variables, which formally validates the\nconsistency of the approach. The convergence is then rigorously proven adapting\nthe ODE method from stochastic approximation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Mar 2016 08:47:23 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Bena\u00efm", "Michel", ""], ["Br\u00e9hier", "Charles-Edouard", ""]]}, {"id": "1603.08163", "submitter": "Farouk Nathoo", "authors": "Farouk S. Nathoo, Keelin Greenlaw, Mary Lesperance", "title": "Regularization Parameter Selection for a Bayesian Multi-Level Group\n  Lasso Regression Model with Application to Imaging Genomics", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the choice of tuning parameters for a Bayesian multi-level\ngroup lasso model developed for the joint analysis of neuroimaging and genetic\ndata. The regression model we consider relates multivariate phenotypes\nconsisting of brain summary measures (volumetric and cortical thickness values)\nto single nucleotide polymorphism (SNPs) data and imposes penalization at two\nnested levels, the first corresponding to genes and the second corresponding to\nSNPs. Associated with each level in the penalty is a tuning parameter which\ncorresponds to a hyperparameter in the hierarchical Bayesian formulation.\nFollowing previous work on Bayesian lassos we consider the estimation of tuning\nparameters through either hierarchical Bayes based on hyperpriors and Gibbs\nsampling or through empirical Bayes based on maximizing the marginal likelihood\nusing a Monte Carlo EM algorithm. For the specific model under consideration we\nfind that these approaches can lead to severe overshrinkage of the regression\nparameter estimates in the high-dimensional setting or when the genetic effects\nare weak. We demonstrate these problems through simulation examples and study\nan approximation to the marginal likelihood which sheds light on the cause of\nthis problem. We then suggest an alternative approach based on the widely\napplicable information criterion (WAIC), an asymptotic approximation to\nleave-one-out cross-validation that can be computed conveniently within an MCMC\nframework.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 02:34:02 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Nathoo", "Farouk S.", ""], ["Greenlaw", "Keelin", ""], ["Lesperance", "Mary", ""]]}, {"id": "1603.08232", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Minh-Ngoc Tran, Mattias Villani, Robert Kohn and\n  Khue-Dung Dang", "title": "The block-Poisson estimator for optimally tuned exact subsampling MCMC", "comments": "The main paper is 28 pages. The supplementary material is 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speeding up Markov Chain Monte Carlo (MCMC) for datasets with many\nobservations by data subsampling has recently received considerable attention.\nA pseudo-marginal MCMC method is proposed that estimates the likelihood by data\nsubsampling using a block-Poisson estimator. The estimator is a product of\nPoisson estimators, allowing us to update a single block of subsample\nindicators in each MCMC iteration so that a desired correlation is achieved\nbetween the logs of successive likelihood estimates. This is important since\npseudo-marginal MCMC with positively correlated likelihood estimates can use\nsubstantially smaller subsamples without adversely affecting the sampling\nefficiency. The block-Poisson estimator is unbiased but not necessarily\npositive, so the algorithm runs the MCMC on the absolute value of the\nlikelihood estimator and uses an importance sampling correction to obtain\nconsistent estimates of the posterior mean of any function of the parameters.\nOur article derives guidelines to select the optimal tuning parameters for our\nmethod and shows that it compares very favourably to regular MCMC without\nsubsampling, and to two other recently proposed exact subsampling approaches in\nthe literature.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2016 16:25:34 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 02:38:59 GMT"}, {"version": "v3", "created": "Fri, 20 Jan 2017 03:57:16 GMT"}, {"version": "v4", "created": "Mon, 26 Mar 2018 07:05:31 GMT"}, {"version": "v5", "created": "Tue, 10 Apr 2018 07:06:36 GMT"}, {"version": "v6", "created": "Tue, 7 Apr 2020 03:42:46 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Quiroz", "Matias", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""], ["Dang", "Khue-Dung", ""]]}, {"id": "1603.08815", "submitter": "Dustin Tran", "authors": "Dustin Tran, Minjae Kim, Finale Doshi-Velez", "title": "Spectral M-estimation with Applications to Hidden Markov Models", "comments": "Appears in Artificial Intelligence and Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method of moment estimators exhibit appealing statistical properties, such as\nasymptotic unbiasedness, for nonconvex problems. However, they typically\nrequire a large number of samples and are extremely sensitive to model\nmisspecification. In this paper, we apply the framework of M-estimation to\ndevelop both a generalized method of moments procedure and a principled method\nfor regularization. Our proposed M-estimator obtains optimal sample efficiency\nrates (in the class of moment-based estimators) and the same well-known rates\non prediction accuracy as other spectral estimators. It also makes it\nstraightforward to incorporate regularization into the sample moment\nconditions. We demonstrate empirically the gains in sample efficiency from our\napproach on hidden Markov models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2016 15:34:29 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Tran", "Dustin", ""], ["Kim", "Minjae", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1603.09005", "submitter": "Joaquin Miguez", "authors": "Dan Crisan, Joaquin Miguez", "title": "Uniform convergence over time of a nested particle filtering scheme for\n  recursive parameter estimation in state--space Markov models", "comments": "arXiv admin note: text overlap with arXiv:1308.1883", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the performance of a recursive Monte Carlo method for the Bayesian\nestimation of the static parameters of a discrete--time state--space Markov\nmodel. The algorithm employs two layers of particle filters to approximate the\nposterior probability distribution of the model parameters. In particular, the\nfirst layer yields an empirical distribution of samples on the parameter space,\nwhile the filters in the second layer are auxiliary devices to approximate the\n(analytically intractable) likelihood of the parameters. This approach relates\nthe this algorithm to the recent sequential Monte Carlo square (SMC$^2$)\nmethod, which provides a {\\em non-recursive} solution to the same problem. In\nthis paper, we investigate the approximation, via the proposed scheme, of\nintegrals of real bounded functions with respect to the posterior distribution\nof the system parameters. Under assumptions related to the compactness of the\nparameter support and the stability and continuity of the sequence of posterior\ndistributions for the state--space model, we prove that the $L_p$ norms of the\napproximation errors vanish asymptotically (as the number of Monte Carlo\nsamples generated by the algorithm increases) and uniformly over time. We also\nprove that, under the same assumptions, the proposed scheme can asymptotically\nidentify the parameter values for a class of models. We conclude the paper with\na numerical example that illustrates the uniform convergence results by\nexploring the accuracy and stability of the proposed algorithm operating with\nlong sequences of observations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 00:11:38 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Crisan", "Dan", ""], ["Miguez", "Joaquin", ""]]}, {"id": "1603.09088", "submitter": "Christian P. Robert", "authors": "Christian P. Robert and Judith Rousseau (Universit\\'e Paris-Dauphine,\n  PSL)", "title": "Some comments about James Watson's and Chris Holmes' \"Approximate Models\n  and Robust Decisions\": Nonparametric Bayesian clay for robust decision bricks", "comments": "7 pages, discussion of Watson and Holmes (2016) to appear in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note discusses Watson and Holmes (2016) and their pro- posals towards\nmore robust Bayesian decisions. While we acknowledge and commend the authors\nfor setting new and all-encompassing prin- ciples of Bayesian robustness, and\nwe appreciate the strong anchoring of those within a decision-theoretic\nreferential, we remain uncertain as to which extent such principles can be\napplied outside binary de- cisions. We also wonder at the ultimate relevance of\nKullback-Leibler neighbourhoods to characterise robustness and favour\nextensions along non-parametric axes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 09:09:32 GMT"}, {"version": "v2", "created": "Sat, 9 Apr 2016 10:19:06 GMT"}], "update_date": "2016-04-12", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine,\n  PSL"], ["Rousseau", "Judith", "", "Universit\u00e9 Paris-Dauphine,\n  PSL"]]}, {"id": "1603.09157", "submitter": "Johan W{\\aa}gberg", "authors": "Jack Umenberger, Johan W\\r{a}gberg, Ian R. Manchester, Thomas B.\n  Sch\\\"on", "title": "Linear System Identification via EM with Latent Disturbances and\n  Lagrangian Relaxation", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the application of the Expectation Maximization algorithm to\nidentification of dynamical systems, internal states are typically chosen as\nlatent variables, for simplicity. In this work, we propose a different choice\nof latent variables, namely, system disturbances. Such a formulation elegantly\nhandles the problematic case of singular state space models, and is shown,\nunder certain circumstances, to improve the fidelity of bounds on the\nlikelihood, leading to convergence in fewer iterations. To access these\nbenefits we develop a Lagrangian relaxation of the nonconvex optimization\nproblems that arise in the latent disturbances formulation, and proceed via\nsemidefinite programming.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 12:37:27 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Umenberger", "Jack", ""], ["W\u00e5gberg", "Johan", ""], ["Manchester", "Ian R.", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1603.09272", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Paul Blomstedt and Samuel Kaski", "title": "Bayesian inference in hierarchical models by combining independent\n  posteriors", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models are versatile tools for joint modeling of data sets\narising from different, but related, sources. Fully Bayesian inference may,\nhowever, become computationally prohibitive if the source-specific data models\nare complex, or if the number of sources is very large. To facilitate\ncomputation, we propose an approach, where inference is first made\nindependently for the parameters of each data set, whereupon the obtained\nposterior samples are used as observed data in a substitute hierarchical model,\nbased on a scaled likelihood function. Compared to direct inference in a full\nhierarchical model, the approach has the advantage of being able to speed up\nconvergence by breaking down the initial large inference problem into smaller\nindividual subproblems with better convergence properties. Moreover it enables\nparallel processing of the possibly complex inferences of the source-specific\nparameters, which may otherwise create a computational bottleneck if processed\njointly as part of a hierarchical model. The approach is illustrated with both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2016 16:42:35 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 09:33:22 GMT"}], "update_date": "2016-05-06", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Blomstedt", "Paul", ""], ["Kaski", "Samuel", ""]]}]