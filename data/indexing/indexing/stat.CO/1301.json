[{"id": "1301.0413", "submitter": "Ernie Esser", "authors": "Ernie Esser, Yifei Lou, Jack Xin", "title": "A Method for Finding Structured Sparse Solutions to Non-negative Least\n  Squares Problems with Applications", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demixing problems in many areas such as hyperspectral imaging and\ndifferential optical absorption spectroscopy (DOAS) often require finding\nsparse nonnegative linear combinations of dictionary elements that match\nobserved data. We show how aspects of these problems, such as misalignment of\nDOAS references and uncertainty in hyperspectral endmembers, can be modeled by\nexpanding the dictionary with grouped elements and imposing a structured\nsparsity assumption that the combinations within each group should be sparse or\neven 1-sparse. If the dictionary is highly coherent, it is difficult to obtain\ngood solutions using convex or greedy methods, such as non-negative least\nsquares (NNLS) or orthogonal matching pursuit. We use penalties related to the\nHoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsity\npenalties to be added to the objective in NNLS-type models. For solving the\nresulting nonconvex models, we propose a scaled gradient projection algorithm\nthat requires solving a sequence of strongly convex quadratic programs. We\ndiscuss its close connections to convex splitting methods and difference of\nconvex programming. We also present promising numerical results for example\nDOAS analysis and hyperspectral demixing problems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 10:09:41 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Esser", "Ernie", ""], ["Lou", "Yifei", ""], ["Xin", "Jack", ""]]}, {"id": "1301.0877", "submitter": "Wei Gao", "authors": "Wei Gao, Ping Shing Chan, Hon Keung Tony Ng and Xiaolei Lu", "title": "Efficient Computational Algorithm for Optimal Allocation in Regression\n  Models", "comments": "17 pages and 2 tables, accepted by Journal of Computational and\n  Applied Mathematics in 2013 Journal of Computational and Applied Mathematics\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss the optimal allocation problem in an experiment\nwhen a regression model is used for statistical analysis. Monotonic convergence\nfor a general class of multiplicative algorithms for $D$-optimality has been\ndiscussed in the literature. Here, we provide an alternate proof of the\nmonotonic convergence for $D$-criterion with a simple computational algorithm\nand furthermore show it converges to the $D$-optimality. We also discuss an\nalgorithm as well as a conjecture of the monotonic convergence for\n$A$-criterion. Monte Carlo simulations are used to demonstrate the reliability,\nefficiency and usefulness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 06:39:29 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 15:18:13 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Gao", "Wei", ""], ["Chan", "Ping Shing", ""], ["Ng", "Hon Keung Tony", ""], ["Lu", "Xiaolei", ""]]}, {"id": "1301.1650", "submitter": "Julien Bect", "authors": "Alireza Roodaki (LTCI), Julien Bect (E3S), Gilles Fleury (E3S)", "title": "Relabeling and Summarizing Posterior Distributions in Signal\n  Decomposition Problems when the Number of Components is Unknown", "comments": "arXiv admin note: text overlap with arXiv:1111.6298", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problems of relabeling and summarizing posterior\ndistributions that typically arise, in a Bayesian framework, when dealing with\nsignal decomposition problems with an unknown number of components. Such\nposterior distributions are defined over union of subspaces of differing\ndimensionality and can be sampled from using modern Monte Carlo techniques, for\ninstance the increasingly popular RJ-MCMC method. No generic approach is\navailable, however, to summarize the resulting variable-dimensional samples and\nextract from them component-specific parameters. We propose a novel approach,\nnamed Variable-dimensional Approximate Posterior for Relabeling and Summarizing\n(VAPoRS), to this problem, which consists in approximating the posterior\ndistribution of interest by a \"simple\"---but still\nvariable-dimensional---parametric distribution. The distance between the two\ndistributions is measured using the Kullback-Leibler divergence, and a\nStochastic EM-type algorithm, driven by the RJ-MCMC sampler, is proposed to\nestimate the parameters. Two signal decomposition problems are considered, to\nshow the capability of VAPoRS both for relabeling and for summarizing variable\ndimensional posterior distributions: the classical problem of detecting and\nestimating sinusoids in white Gaussian noise on the one hand, and a particle\ncounting problem motivated by the Pierre Auger project in astrophysics on the\nother hand.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 19:47:19 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Roodaki", "Alireza", "", "LTCI"], ["Bect", "Julien", "", "E3S"], ["Fleury", "Gilles", "", "E3S"]]}, {"id": "1301.2053", "submitter": "Vakili Kaveh", "authors": "Kaveh Vakili and Eric Schmitt", "title": "Finding Multivariate Outliers With FastPCS", "comments": "21 pages 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Projection Congruent Subset (PCS) Outlyingness is a new index of\nmultivariate outlyingness obtained by considering univariate projections of the\ndata. Like many other outlier detection procedures, PCS searches for a subset\nwhich minimizes a criterion. The difference is that the new criterion was\ndesigned to be insensitive to the outliers. PCS is supported by FastPCS, a fast\nand affine equivariant algorithm which we also detail. Both an extensive\nsimulation study and a real data application from the field of engineering show\nthat FastPCS performs better than its competitors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 08:47:03 GMT"}, {"version": "v10", "created": "Wed, 17 Jul 2013 19:40:24 GMT"}, {"version": "v11", "created": "Thu, 18 Jul 2013 06:25:41 GMT"}, {"version": "v12", "created": "Wed, 31 Jul 2013 12:53:44 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2013 22:49:56 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2013 00:15:04 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2013 15:57:17 GMT"}, {"version": "v5", "created": "Mon, 15 Apr 2013 12:54:45 GMT"}, {"version": "v6", "created": "Tue, 16 Apr 2013 21:07:37 GMT"}, {"version": "v7", "created": "Fri, 19 Apr 2013 09:22:03 GMT"}, {"version": "v8", "created": "Wed, 19 Jun 2013 15:55:57 GMT"}, {"version": "v9", "created": "Tue, 25 Jun 2013 11:40:00 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Vakili", "Kaveh", ""], ["Schmitt", "Eric", ""]]}, {"id": "1301.2167", "submitter": "Isabella Gollini", "authors": "Isabella Gollini and Thomas Brendan Murphy", "title": "Mixture of Latent Trait Analyzers for Model-Based Clustering of\n  Categorical Data", "comments": "Accepted to appear in Statistics and Computing; Main paper and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering methods for continuous data are well established and\ncommonly used in a wide range of applications. However, model-based clustering\nmethods for categorical data are less standard. Latent class analysis is a\ncommonly used method for model-based clustering of binary data and/or\ncategorical data, but due to an assumed local independence structure there may\nnot be a correspondence between the estimated latent classes and groups in the\npopulation of interest. The mixture of latent trait analyzers model extends\nlatent class analysis by assuming a model for the categorical response\nvariables that depends on both a categorical latent class and a continuous\nlatent trait variable; the discrete latent class accommodates group structure\nand the continuous latent trait accommodates dependence within these groups.\nFitting the mixture of latent trait analyzers model is potentially difficult\nbecause the likelihood function involves an integral that cannot be evaluated\nanalytically. We develop a variational approach for fitting the mixture of\nlatent trait models and this provides an efficient model fitting strategy. The\nmixture of latent trait analyzers model is demonstrated on the analysis of data\nfrom the National Long Term Care Survey (NLTCS) and voting in the U.S.\nCongress. The model is shown to yield intuitive clustering results and it gives\na much better fit than either latent class analysis or latent trait analysis\nalone.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 15:45:18 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 18:00:46 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Gollini", "Isabella", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1301.2266", "submitter": "Nando de Freitas", "authors": "Nando de Freitas, Pedro Hojen-Sorensen, Michael I. Jordan, Stuart\n  Russell", "title": "Variational MCMC", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-120-127", "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of learning algorithms that combines variational\napproximation and Markov chain Monte Carlo (MCMC) simulation. Naive algorithms\nthat use the variational approximation as proposal distribution can perform\npoorly because this approximation tends to underestimate the true variance and\nother features of the data. We solve this problem by introducing more\nsophisticated MCMC algorithms. One of these algorithms is a mixture of two MCMC\nkernels: a random walk Metropolis kernel and a blockMetropolis-Hastings (MH)\nkernel with a variational approximation as proposaldistribution. The MH kernel\nallows one to locate regions of high probability efficiently. The Metropolis\nkernel allows us to explore the vicinity of these regions. This algorithm\noutperforms variationalapproximations because it yields slightly better\nestimates of the mean and considerably better estimates of higher moments, such\nas covariances. It also outperforms standard MCMC algorithms because it locates\ntheregions of high probability quickly, thus speeding up convergence. We\ndemonstrate this algorithm on the problem of Bayesian parameter estimation for\nlogistic (sigmoid) belief networks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:23:18 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["de Freitas", "Nando", ""], ["Hojen-Sorensen", "Pedro", ""], ["Jordan", "Michael I.", ""], ["Russell", "Stuart", ""]]}, {"id": "1301.2677", "submitter": "Satoshi Kuriki", "authors": "Xiaoling Dou, Satoshi Kuriki, Gwo Dong Lin, Donald Richards", "title": "EM algorithms for estimating the Bernstein copula", "comments": "34 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method that uses order statistics to construct multivariate distributions\nwith fixed marginals and which utilizes a representation of the Bernstein\ncopula in terms of a finite mixture distribution is proposed.\nExpectation-maximization (EM) algorithms to estimate the Bernstein copula are\nproposed, and a local convergence property is proved. Moreover, asymptotic\nproperties of the proposed semiparametric estimators are provided. Illustrative\nexamples are presented using three real data sets and a 3-dimensional simulated\ndata set. These studies show that the Bernstein copula is able to represent\nvarious distributions flexibly and that the proposed EM algorithms work well\nfor such data.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 11:44:14 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2013 12:50:14 GMT"}, {"version": "v3", "created": "Sun, 22 Dec 2013 04:51:15 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2014 12:43:59 GMT"}], "update_date": "2014-01-16", "authors_parsed": [["Dou", "Xiaoling", ""], ["Kuriki", "Satoshi", ""], ["Lin", "Gwo Dong", ""], ["Richards", "Donald", ""]]}, {"id": "1301.2897", "submitter": "Ajay Jasra", "authors": "David Nott, Xiaole Zhang, Chris Yau, Ajay Jasra", "title": "A sequential algorithm for fast fitting of Dirichlet process mixture\n  models", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2013.870906", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose an improvement on the sequential updating and\ngreedy search (SUGS) algorithm Wang and Dunson for fast fitting of Dirichlet\nprocess mixture models. The SUGS algorithm provides a means for very fast\napproximate Bayesian inference for mixture data which is particularly of use\nwhen data sets are so large that many standard Markov chain Monte Carlo (MCMC)\nalgorithms cannot be applied efficiently, or take a prohibitively long time to\nconverge. In particular, these ideas are used to initially interrogate the\ndata, and to refine models such that one can potentially apply exact data\nanalysis later on. SUGS relies upon sequentially allocating data to clusters\nand proceeding with an update of the posterior on the subsequent allocations\nand parameters which assumes this allocation is correct. Our modification\nsoftens this approach, by providing a probability distribution over\nallocations, with a similar computational cost; this approach has an\ninterpretation as a variational Bayes procedure and hence we term it\nvariational SUGS (VSUGS). It is shown in simulated examples that VSUGS can\nout-perform, in terms of density estimation and classification, the original\nSUGS algorithm in many scenarios. In addition, we present a data analysis for\nflow cytometry data, and SNP data via a three-class dirichlet process mixture\nmodel illustrating the apparent improvement over SUGS.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 09:40:10 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Nott", "David", ""], ["Zhang", "Xiaole", ""], ["Yau", "Chris", ""], ["Jasra", "Ajay", ""]]}, {"id": "1301.2917", "submitter": "Nial Friel", "authors": "Nial Friel", "title": "Evidence and Bayes factor estimation for Gibbs random fields", "comments": "21 pages (Appeared in Journal of Computational and Graphical\n  Statistics) (Minor typos corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields play an important role in statistics. However they are\ncomplicated to work with due to an intractability of the likelihood function\nand there has been much work devoted to finding computational algorithms to\nallow Bayesian inference to be conducted for such so-called doubly intractable\ndistributions. This paper extends this work and addresses the issue of\nestimating the evidence and Bayes factor for such models. The approach which we\ndevelop is shown to yield good performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 10:41:12 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2013 15:27:41 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 21:14:26 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Friel", "Nial", ""]]}, {"id": "1301.2975", "submitter": "Theodore  Kypraios", "authors": "Simon R. White, Theodore Kypraios, Simon P. Preston", "title": "Fast Approximate Bayesian Computation for discretely observed Markov\n  models using a factorised posterior distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications involve inference for complicated\nstochastic models for which the likelihood function is difficult or even\nimpossible to calculate, and hence conventional likelihood-based inferential\nechniques cannot be used. In such settings, Bayesian inference can be performed\nusing Approximate Bayesian Computation (ABC). However, in spite of many recent\ndevelopments to ABC methodology, in many applications the computational cost of\nABC necessitates the choice of summary statistics and tolerances that can\npotentially severely bias the estimate of the posterior.\n  We propose a new \"piecewise\" ABC approach suitable for discretely observed\nMarkov models that involves writing the posterior density of the parameters as\na product of factors, each a function of only a subset of the data, and then\nusing ABC within each factor. The approach has the advantage of side-stepping\nthe need to choose a summary statistic and it enables a stringent tolerance to\nbe set, making the posterior \"less approximate\". We investigate two methods for\nestimating the posterior density based on ABC samples for each of the factors:\nthe first is to use a Gaussian approximation for each factor, and the second is\nto use a kernel density estimate. Both methods have their merits. The Gaussian\napproximation is simple, fast, and probably adequate for many applications. On\nthe other hand, using instead a kernel density estimate has the benefit of\nconsistently estimating the true ABC posterior as the number of ABC samples\ntends to infinity. We illustrate the piecewise ABC approach for three examples;\nin each case, the approach enables \"exact matching\" between simulations and\ndata and offers fast and accurate inference.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 13:53:07 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 16:04:42 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["White", "Simon R.", ""], ["Kypraios", "Theodore", ""], ["Preston", "Simon P.", ""]]}, {"id": "1301.2983", "submitter": "Julian Faraway", "authors": "Julian J. Faraway", "title": "Does Data Splitting Improve Prediction?", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-014-9522-9", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data splitting divides data into two parts. One part is reserved for model\nselection. In some applications, the second part is used for model validation\nbut we use this part for estimating the parameters of the chosen model. We\nfocus on the problem of constructing reliable predictive distributions for\nfuture observed values. We judge the predictive performance using log scoring.\nWe compare the full data strategy with the data splitting strategy for\nprediction. We show how the full data score can be decomposed into model\nselection, parameter estimation and data reuse costs. Data splitting is\npreferred when data reuse costs are high. We investigate the relative\nperformance of the strategies in four simulation scenarios. We introduce a\nhybrid estimator called SAFE that uses one part for model selection but both\nparts for estimation. We discuss the choice to use a split data analysis versus\na full data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 14:09:49 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 15:06:02 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Faraway", "Julian J.", ""]]}, {"id": "1301.3166", "submitter": "Scott Sisson", "authors": "D. Prangle and M. G. B. Blum and G. Popovic and S. A. Sisson", "title": "Diagnostic tools of approximate Bayesian computation using the coverage\n  property", "comments": "Figures 8-13 are Supplementary Information Figures S1-S6", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is an approach for sampling from an\napproximate posterior distribution in the presence of a computationally\nintractable likelihood function. A common implementation is based on simulating\nmodel, parameter and dataset triples, (m,\\theta,y), from the prior, and then\naccepting as samples from the approximate posterior, those pairs (m,\\theta) for\nwhich y, or a summary of y, is \"close\" to the observed data. Closeness is\ntypically determined though a distance measure and a kernel scale parameter,\n\\epsilon. Appropriate choice of \\epsilon is important to producing a good\nquality approximation. This paper proposes diagnostic tools for the choice of\n\\epsilon based on assessing the coverage property, which asserts that credible\nintervals have the correct coverage levels. We provide theoretical results on\ncoverage for both model and parameter inference, and adapt these into\ndiagnostics for the ABC context. We re-analyse a study on human demographic\nhistory to determine whether the adopted posterior approximation was\nappropriate. R code implementing the proposed methodology is freely available\nin the package \"abc.\"\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 21:47:58 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Prangle", "D.", ""], ["Blum", "M. G. B.", ""], ["Popovic", "G.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1301.3617", "submitter": "Mike Ludkovski", "authors": "Junjing Lin and Michael Ludkovski", "title": "Sequential Bayesian Inference in Hidden Markov Stochastic Kinetic Models\n  with Application to Detection and Response to Seasonal Epidemics", "comments": "26 pages, 7 figures", "journal-ref": "Statistics and Computing, Volume 24, Issue 6 , pp 1047-1062 (2014)", "doi": "10.1007/s11222-013-9419-z", "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequential Bayesian inference in stochastic kinetic models with\nlatent factors. Assuming continuous observation of all the reactions, our focus\nis on joint inference of the unknown reaction rates and the dynamic latent\nstates, modeled as a hidden Markov factor. Using insights from nonlinear\nfiltering of continuous-time jump Markov processes we develop a novel\nsequential Monte Carlo algorithm for this purpose. Our approach applies the\nideas of particle learning to minimize particle degeneracy and exploit the\nanalytical jump Markov structure. A motivating application of our methods is\nmodeling of seasonal infectious disease outbreaks represented through a\ncompartmental epidemic model. We demonstrate inference in such models with\nseveral numerical illustrations and also discuss predictive analysis of\nepidemic countermeasures using sequential Bayes estimates.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 08:05:04 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Lin", "Junjing", ""], ["Ludkovski", "Michael", ""]]}, {"id": "1301.3853", "submitter": "Arnaud Doucet", "authors": "Arnaud Doucet, Nando de Freitas, Kevin Murphy, Stuart Russell", "title": "Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-176-183", "categories": "cs.LG cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters (PFs) are powerful sampling-based inference/learning\nalgorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a\nprincipled way, any type of probability distribution, nonlinearity and\nnon-stationarity. They have appeared in several fields under such names as\n\"condensation\", \"sequential Monte Carlo\" and \"survival of the fittest\". In this\npaper, we show how we can exploit the structure of the DBN to increase the\nefficiency of particle filtering, using a technique known as\nRao-Blackwellisation. Essentially, this samples some of the variables, and\nmarginalizes out the rest exactly, using the Kalman filter, HMM filter,\njunction tree algorithm, or any other finite dimensional optimal filter. We\nshow that Rao-Blackwellised particle filters (RBPFs) lead to more accurate\nestimates than standard PFs. We demonstrate RBPFs on two problems, namely\nnon-stationary online regression with radial basis function networks and robot\nlocalization and map building. We also discuss other potential application\nareas and provide references to some finite dimensional optimal filters.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:50:01 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Doucet", "Arnaud", ""], ["de Freitas", "Nando", ""], ["Murphy", "Kevin", ""], ["Russell", "Stuart", ""]]}, {"id": "1301.3890", "submitter": "Dale Schuurmans", "authors": "Dale Schuurmans, Finnegan Southey", "title": "Monte Carlo Inference via Greedy Importance Sampling", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-523-532", "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for conducting Monte Carlo inference in graphical\nmodels which combines explicit search with generalized importance sampling. The\nidea is to reduce the variance of importance sampling by searching for\nsignificant points in the target distribution. We prove that it is possible to\nintroduce search and still maintain unbiasedness. We then demonstrate our\nprocedure on a few simple inference tasks and show that it can improve the\ninference quality of standard MCMC methods, including Gibbs sampling,\nMetropolis sampling, and Hybrid Monte Carlo. This paper extends previous work\nwhich showed how greedy importance sampling could be correctly realized in the\none-dimensional case.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:52:30 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Schuurmans", "Dale", ""], ["Southey", "Finnegan", ""]]}, {"id": "1301.3928", "submitter": "Matthew Harrison", "authors": "Matthew T. Harrison, Jeffrey W. Miller", "title": "Importance sampling for weighted binary random matrices with specified\n  margins", "comments": "39 pages (13 pages main text, 26 pages supplementary material);\n  supersedes arXiv:0906.1004", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sequential importance sampling algorithm is developed for the distribution\nthat results when a matrix of independent, but not identically distributed,\nBernoulli random variables is conditioned on a given sequence of row and column\nsums. This conditional distribution arises in a variety of applications and\nincludes as a special case the uniform distribution over zero-one tables with\nspecified margins. The algorithm uses dynamic programming to combine hard\nmargin constraints, combinatorial approximations, and additional non-uniform\nweighting in a principled way to give state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 21:34:22 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Harrison", "Matthew T.", ""], ["Miller", "Jeffrey W.", ""]]}, {"id": "1301.3946", "submitter": "Hoyt Koepke", "authors": "Hoyt Koepke and Elizabeth Thompson", "title": "Efficient Identification of Equivalences in Dynamic Graphs and Pedigree\n  Structures", "comments": "Code for paper available at\n  http://www.stat.washington.edu/~hoytak/code/hashreduce", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for designing test and query functions for complex\nstructures that vary across a given parameter such as genetic marker position.\nThe operations we are interested in include equality testing, set operations,\nisolating unique states, duplication counting, or finding equivalence classes\nunder identifiability constraints. A motivating application is locating\nequivalence classes in identity-by-descent (IBD) graphs, graph structures in\npedigree analysis that change over genetic marker location. The nodes of these\ngraphs are unlabeled and identified only by their connecting edges, a\nconstraint easily handled by our approach. The general framework introduced is\npowerful enough to build a range of testing functions for IBD graphs, dynamic\npopulations, and other structures using a minimal set of operations. The\ntheoretical and algorithmic properties of our approach are analyzed and proved.\nComputational results on several simulations demonstrate the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:11:14 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 21:41:35 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2013 21:15:43 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Koepke", "Hoyt", ""], ["Thompson", "Elizabeth", ""]]}, {"id": "1301.3947", "submitter": "Hilary Parker", "authors": "Hilary S. Parker, H\\'ector Corrada Bravo and Jeffrey T. Leek", "title": "Removing batch effects for prediction problems with frozen surrogate\n  variable analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch effects are responsible for the failure of promising genomic prognos-\ntic signatures, major ambiguities in published genomic results, and retractions\nof widely-publicized findings. Batch effect corrections have been developed to\nre- move these artifacts, but they are designed to be used in population\nstudies. But genomic technologies are beginning to be used in clinical\napplications where sam- ples are analyzed one at a time for diagnostic,\nprognostic, and predictive applica- tions. There are currently no batch\ncorrection methods that have been developed specifically for prediction. In\nthis paper, we propose an new method called frozen surrogate variable analysis\n(fSVA) that borrows strength from a training set for individual sample batch\ncorrection. We show that fSVA improves prediction ac- curacy in simulations and\nin public genomic studies. fSVA is available as part of the sva Bioconductor\npackage.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:16:02 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Parker", "Hilary S.", ""], ["Bravo", "H\u00e9ctor Corrada", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1301.4019", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Anthony Lee and Pierre E. Jacob", "title": "Parallel resampling in the particle filter", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern parallel computing devices, such as the graphics processing unit\n(GPU), have gained significant traction in scientific and statistical\ncomputing. They are particularly well-suited to data-parallel algorithms such\nas the particle filter, or more generally Sequential Monte Carlo (SMC), which\nare increasingly used in statistical inference. SMC methods carry a set of\nweighted particles through repeated propagation, weighting and resampling\nsteps. The propagation and weighting steps are straightforward to parallelise,\nas they require only independent operations on each particle. The resampling\nstep is more difficult, as standard schemes require a collective operation,\nsuch as a sum, across particle weights. Focusing on this resampling step, we\nanalyse two alternative schemes that do not involve a collective operation\n(Metropolis and rejection resamplers), and compare them to standard schemes\n(multinomial, stratified and systematic resamplers). We find that, in certain\ncircumstances, the alternative resamplers can perform significantly faster on a\nGPU, and to a lesser extent on a CPU, than the standard approaches. Moreover,\nin single precision, the standard approaches are numerically biased for upwards\nof hundreds of thousands of particles, while the alternatives are not. This is\nparticularly important given greater single- than double-precision throughput\non modern devices, and the consequent temptation to use single precision with a\ngreater number of particles. Finally, we provide auxiliary functions useful for\nimplementation, such as for the permutation of ancestry vectors to enable\nin-place propagation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 09:14:43 GMT"}, {"version": "v2", "created": "Wed, 7 May 2014 06:58:30 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2015 11:32:37 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Lee", "Anthony", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1301.4155", "submitter": "Zhiyuan  Weng", "authors": "Zhiyuan Weng and Petar Djuric", "title": "A Search-free DOA Estimation Algorithm for Coprime Arrays", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, coprime arrays have been in the focus of research because of their\npotential in exploiting redundancy in spanning large apertures with fewer\nelements than suggested by theory. A coprime array consists of two uniform\nlinear subarrays with inter-element spacings $M\\lambda/2$ and $N\\lambda/2$,\nwhere $M$ and $N$ are coprime integers and $\\lambda$ is the wavelength of the\nsignal. In this paper, we propose a fast search-free method for\ndirection-of-arrival (DOA) estimation with coprime arrays. It is based on the\nuse of methods that operate on the uniform linear subarrays of the coprime\narray and that enjoy many processing advantages. We first estimate the DOAs for\neach uniform linear subarray separately and then combine the estimates from the\nsubarrays. For combining the estimates, we propose a method that projects the\nestimated point in the two-dimensional plane onto one-dimensional line segments\nthat correspond to the entire angular domain. By doing so, we avoid the search\nstep and consequently, we greatly reduce the computational complexity of the\nmethod. We demonstrate the performance of the method with computer simulations\nand compare it with that of the FD-root MUSIC method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 16:41:13 GMT"}, {"version": "v2", "created": "Thu, 13 Mar 2014 01:38:10 GMT"}, {"version": "v3", "created": "Sat, 13 Dec 2014 13:22:20 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Weng", "Zhiyuan", ""], ["Djuric", "Petar", ""]]}, {"id": "1301.4168", "submitter": "Mareija Eskelinen", "authors": "Luke Bornn, Yutian Chen, Nando de Freitas, Mareija Eskelin, Jing Fang,\n  Max Welling", "title": "Herded Gibbs Sampling", "comments": "19 pages, including the appendix. Submission for ICLR 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gibbs sampler is one of the most popular algorithms for inference in\nstatistical models. In this paper, we introduce a herding variant of this\nalgorithm, called herded Gibbs, that is entirely deterministic. We prove that\nherded Gibbs has an $O(1/T)$ convergence rate for models with independent\nvariables and for fully connected probabilistic graphical models. Herded Gibbs\nis shown to outperform Gibbs in the tasks of image denoising with MRFs and\nnamed entity recognition with CRFs. However, the convergence for herded Gibbs\nfor sparsely connected probabilistic graphical models is still an open problem.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 17:37:56 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 01:55:06 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Bornn", "Luke", ""], ["Chen", "Yutian", ""], ["de Freitas", "Nando", ""], ["Eskelin", "Mareija", ""], ["Fang", "Jing", ""], ["Welling", "Max", ""]]}, {"id": "1301.4499", "submitter": "Marco Selig", "authors": "Marco Selig, Michael R. Bell, Henrik Junklewitz, Niels Oppermann,\n  Martin Reinecke, Maksim Greiner, Carlos Pachajoa, Torsten A. En{\\ss}lin", "title": "NIFTY - Numerical Information Field Theory - a versatile Python library\n  for signal inference", "comments": "9 pages, 3 tables, 4 figures, accepted by Astronomy & Astrophysics;\n  refereed version, 1 figure added, results unchanged", "journal-ref": null, "doi": "10.1051/0004-6361/201321236", "report-no": null, "categories": "astro-ph.IM cs.IT cs.MS math-ph math.IT math.MP physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NIFTY, \"Numerical Information Field Theory\", is a software package designed\nto enable the development of signal inference algorithms that operate\nregardless of the underlying spatial grid and its resolution. Its\nobject-oriented framework is written in Python, although it accesses libraries\nwritten in Cython, C++, and C for efficiency. NIFTY offers a toolkit that\nabstracts discretized representations of continuous spaces, fields in these\nspaces, and operators acting on fields into classes. Thereby, the correct\nnormalization of operations on fields is taken care of automatically without\nconcerning the user. This allows for an abstract formulation and programming of\ninference algorithms, including those derived within information field theory.\nThus, NIFTY permits its user to rapidly prototype algorithms in 1D, and then\napply the developed code in higher-dimensional settings of real world problems.\nThe set of spaces on which NIFTY operates comprises point sets, n-dimensional\nregular grids, spherical spaces, their harmonic counterparts, and product\nspaces constructed as combinations of those. The functionality and diversity of\nthe package is demonstrated by a Wiener filter code example that successfully\nruns without modification regardless of the space on which the inference\nproblem is defined.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 21:00:01 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2013 17:24:59 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Selig", "Marco", ""], ["Bell", "Michael R.", ""], ["Junklewitz", "Henrik", ""], ["Oppermann", "Niels", ""], ["Reinecke", "Martin", ""], ["Greiner", "Maksim", ""], ["Pachajoa", "Carlos", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1301.4674", "submitter": "Elvan Ceyhan", "authors": "E. Ceyhan, T. Nishino, J. Alexopolous, R. D. Todd, K. N. Botteron, M.\n  I. Miller, J. T. Ratnanather", "title": "Censoring Distances Based on Labeled Cortical Distance Maps in Cortical\n  Morphometry", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-12-1", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape differences are manifested in cortical structures due to\nneuropsychiatric disorders. Such differences can be measured by labeled\ncortical distance mapping (LCDM) which characterizes the morphometry of the\nlaminar cortical mantle of cortical structures. LCDM data consist of signed\ndistances of gray matter (GM) voxels with respect to GM/white matter (WM)\nsurface. Volumes and descriptive measures (such as means and variances) for\neach subject and the pooled distances provide the morphometric differences\nbetween diagnostic groups, but they do not reveal all the morphometric\ninformation contained in LCDM distances. To extract more information from LCDM\ndata, censoring of the distances is introduced. For censoring of LCDM\ndistances, the range of LCDM distances is partitioned at a fixed increment\nsize; and at each censoring step, and distances not exceeding the censoring\ndistance are kept. Censored LCDM distances inherit the advantages of the pooled\ndistances. Furthermore, the analysis of censored distances provides information\nabout the location of morphometric differences which cannot be obtained from\nthe pooled distances. However, at each step, the censored distances aggregate,\nwhich might confound the results. The influence of data aggregation is\ninvestigated with an extensive Monte Carlo simulation analysis and it is\ndemonstrated that this influence is negligible. As an illustrative example, GM\nof ventral medial prefrontal cortices (VMPFCs) of subjects with major\ndepressive disorder (MDD), subjects at high risk (HR) of MDD, and healthy\ncontrol (Ctrl) subjects are used. A significant reduction in laminar thickness\nof the VMPFC and perhaps shrinkage in MDD and HR subjects is observed when\ncompared to Ctrl subjects. The methodology is also applicable to LCDM-based\nmorphometric measures of other cortical structures affected by disease.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2013 19:28:06 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Ceyhan", "E.", ""], ["Nishino", "T.", ""], ["Alexopolous", "J.", ""], ["Todd", "R. D.", ""], ["Botteron", "K. N.", ""], ["Miller", "M. I.", ""], ["Ratnanather", "J. T.", ""]]}, {"id": "1301.4976", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova, James G. Booth and Martin T. Wells", "title": "Supervised Classification Using Sparse Fisher's LDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that in a supervised classification setting when the number\nof features is smaller than the number of observations, Fisher's linear\ndiscriminant rule is asymptotically Bayes. However, there are numerous modern\napplications where classification is needed in the high-dimensional setting.\nNaive implementation of Fisher's rule in this case fails to provide good\nresults because the sample covariance matrix is singular. Moreover, by\nconstructing a classifier that relies on all features the interpretation of the\nresults is challenging. Our goal is to provide robust classification that\nrelies only on a small subset of important features and accounts for the\nunderlying correlation structure. We apply a lasso-type penalty to the\ndiscriminant vector to ensure sparsity of the solution and use a shrinkage type\nestimator for the covariance matrix. The resulting optimization problem is\nsolved using an iterative coordinate ascent algorithm. Furthermore, we analyze\nthe effect of nonconvexity on the sparsity level of the solution and highlight\nthe difference between the penalized and the constrained versions of the\nproblem. The simulation results show that the proposed method performs\nfavorably in comparison to alternatives. The method is used to classify\nleukemia patients based on DNA methylation features.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 20:35:43 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 19:40:26 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Gaynanova", "Irina", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1301.5035", "submitter": "Kjell Konis", "authors": "Kjell Konis", "title": "Computing Robust Leverage Diagnostics when the Design Matrix Contains\n  Coded Categorical Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a robust leverage diagnostic in linear regression, Rousseeuw and van\nZomeren [1990] proposed using robust distance (Mahalanobis distance computed\nusing robust estimates of location and covariance). However, a design matrix X\nthat contains coded categorical predictor variables is often sufficiently\nsparse that robust estimates of location and covariance cannot be computed.\nSpecifically, matrices formed by taking subsets of the rows of X are likely to\nbe singular, causing algorithms that rely on subsampling to fail. Following the\nspirit of Maronna and Yohai [2000], we observe that extreme leverage points are\nextreme in the continuous predictor variables. We therefore propose a robust\nleverage diagnostic that combines a robust analysis of the continuous predictor\nvariables and the classical definition of leverage.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 23:14:39 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Konis", "Kjell", ""]]}, {"id": "1301.5054", "submitter": "Alexandre Bouchard-C\\^ot\\'e", "authors": "Alexandre Bouchard-C\\^ot\\'e", "title": "A Note on Probabilistic Models over Strings: the Linear Algebra Approach", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.FL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models over strings have played a key role in developing\nmethods allowing indels to be treated as phylogenetically informative events.\nThere is an extensive literature on using automata and transducers on\nphylogenies to do inference on these probabilistic models, in which an\nimportant theoretical question in the field is the complexity of computing the\nnormalization of a class of string-valued graphical models. This question has\nbeen investigated using tools from combinatorics, dynamic programming, and\ngraph theory, and has practical applications in Bayesian phylogenetics. In this\nwork, we revisit this theoretical question from a different point of view,\nbased on linear algebra. The main contribution is a new proof of a known result\non the complexity of inference on TKF91, a well-known probabilistic model over\nstrings. Our proof uses a different approach based on classical linear algebra\nresults, and is in some cases easier to extend to other models. The proving\nmethod also has consequences on the implementation and complexity of inference\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 01:46:25 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2013 23:54:48 GMT"}], "update_date": "2013-07-15", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1301.6064", "submitter": "Simon Byrne", "authors": "Simon Byrne and Mark Girolami", "title": "Geodesic Monte Carlo on Embedded Manifolds", "comments": null, "journal-ref": "Scandinavian Journal of Statistics (2013), 40 (4), pages 825-845", "doi": "10.1111/sjos.12036", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods explicitly defined on the manifold of\nprobability distributions have recently been established. These methods are\nconstructed from diffusions across the manifold and the solution of the\nequations describing geodesic flows in the Hamilton--Jacobi representation.\nThis paper takes the differential geometric basis of Markov chain Monte Carlo\nfurther by considering methods to simulate from probability distributions that\nthemselves are defined on a manifold, with common examples being classes of\ndistributions describing directional statistics. Proposal mechanisms are\ndeveloped based on the geodesic flows over the manifolds of support for the\ndistributions and illustrative examples are provided for the hypersphere and\nStiefel manifold of orthonormal matrices.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2013 15:21:39 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 16:27:56 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}, {"id": "1301.6282", "submitter": "Erkan Buzbas", "authors": "Erkan O. Buzbas and Noah A. Rosenberg", "title": "AABC: approximate approximate Bayesian computation when simulating a\n  large number of data sets is computationally infeasible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods perform inference on\nmodel-specific parameters of mechanistically motivated parametric statistical\nmodels when evaluating likelihoods is difficult. Central to the success of ABC\nmethods is computationally inexpensive simulation of data sets from the\nparametric model of interest. However, when simulating data sets from a model\nis so computationally expensive that the posterior distribution of parameters\ncannot be adequately sampled by ABC, inference is not straightforward. We\npresent approximate approximate Bayesian computation\" (AABC), a class of\nmethods that extends simulation-based inference by ABC to models in which\nsimulating data is expensive. In AABC, we first simulate a limited number of\ndata sets that is computationally feasible to simulate from the parametric\nmodel. We use these data sets as fixed background information to inform a\nnon-mechanistic statistical model that approximates the correct parametric\nmodel and enables efficient simulation of a large number of data sets by\nBayesian resampling methods. We show that under mild assumptions, the posterior\ndistribution obtained by AABC converges to the posterior distribution obtained\nby ABC, as the number of data sets simulated from the parametric model and the\nsample size of the observed data set increase simultaneously. We illustrate the\nperformance of AABC on a population-genetic model of natural selection, as well\nas on a model of the admixture history of hybrid populations.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 19:31:26 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Buzbas", "Erkan O.", ""], ["Rosenberg", "Noah A.", ""]]}, {"id": "1301.6365", "submitter": "Florian Rohart", "authors": "Florian Rohart, Magali San-Cristobal and B\\'eatrice Laurent", "title": "Fixed effects Selection in high dimensional Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider linear mixed models in which the observations are grouped. A\nL1-penalization on the fixed effects coefficients of the log-likelihood\nobtained by considering the random effects as missing values is proposed. A\nmulticycle ECM algorithm is used to solve the optimization problem; it can be\ncombined with any variable selection method developed for linear models. The\nalgorithm allows the number of parameters p to be larger than the total number\nof observations n; it is faster than the lmmLasso (Schelldorfer,2011) since no\nn*n matrix has to be inverted. We show that the theoretical results of\nSchelldorfer (2011) apply for our method when the variances of both the random\neffects and the residuals are known. The combination of the algorithm with a\nvariable selection method (Rohart 2011) shows good results in estimating the\nset of relevant fixed effects coefficients as well as estimating the variances;\nit outperforms the lmmLasso both in the common case (p< n) and in the\nhigh-dimensional case (p > n).\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2013 15:39:44 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Rohart", "Florian", ""], ["San-Cristobal", "Magali", ""], ["Laurent", "B\u00e9atrice", ""]]}, {"id": "1301.6559", "submitter": "Giovanna Menardi", "authors": "Adelchi Azzalini and Giovanna Menardi", "title": "Clustering Via Nonparametric Density Estimation: the R Package\n  pdfCluster", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package pdfCluster performs cluster analysis based on a nonparametric\nestimate of the density of the observed variables. After summarizing the main\naspects of the methodology, we describe the features and the usage of the\npackage, and finally illustrate its working with the aid of two datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 14:40:52 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Azzalini", "Adelchi", ""], ["Menardi", "Giovanna", ""]]}, {"id": "1301.6635", "submitter": "Jeffrey W. Miller", "authors": "Jeffrey W. Miller, Matthew T. Harrison", "title": "Exact sampling and counting for fixed-margin matrices", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1131 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: text overlap with\n  arXiv:1104.0323", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 3, 1569-1592", "doi": "10.1214/13-AOS1131", "report-no": "IMS-AOS-AOS1131", "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The uniform distribution on matrices with specified row and column sums is\noften a natural choice of null model when testing for structure in two-way\ntables (binary or nonnegative integer). Due to the difficulty of sampling from\nthis distribution, many approximate methods have been developed. We will show\nthat by exploiting certain symmetries, exact sampling and counting is in fact\npossible in many nontrivial real-world cases. We illustrate with real datasets\nincluding ecological co-occurrence matrices and contingency tables.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 18:33:47 GMT"}, {"version": "v2", "created": "Tue, 13 Aug 2013 11:08:27 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Harrison", "Matthew T.", ""]]}, {"id": "1301.7026", "submitter": "Nicola Lunardon", "authors": "Nicola Lunardon", "title": "Prepivoting composite score statistics by weighted bootstrap iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role played by the composite analogue of the log likelihood ratio in\nhypothesis testing and in setting confidence regions is not as prominent as it\nis in the canonical likelihood setting, since its asymptotic distribution\ndepends on the unknown parameter. Approximate pivots based on the composite log\nlikelihood ratio can be derived by using asymptotic arguments. However, the\nactual distribution of such pivots may differ considerably from the asymptotic\nreference, leading to tests and confidence regions whose levels are distant\nfrom the nominal ones. The use of bootstrap rather than asymptotic\ndistributions in the composite likelihood framework is explored. Prepivoted\ntests and confidence sets based on a suitable statistic turn out to be accurate\nand computationally appealing inferential tools.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 19:16:38 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Lunardon", "Nicola", ""]]}, {"id": "1301.7271", "submitter": "Antonio Forcina", "authors": "Antonio Forcina", "title": "An efficient Fisher-scoring algorithm for fitting latent class models\n  with individual covariates", "comments": "this is a revision of the version uploaded 2 years ago", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For latent class models where the class weights depend on individual\ncovariates, we derive a simple expression for computing the score vector and a\nconvenient hybrid between the observed and the expected information matrices\nwhich is always positive defnite. These ingredients, combined with a\nmaximization algorithm based on line search, provides an efficient tool for\nmaximum likelihood estimation. In particular, the proposed algorithm is such\nthat the log-likelihood never decreases from one step to the next and the\nchoice of starting values is not crucial for reaching a local maximum. We show\nhow the same algorithm may be used for numerical investigation of the effect of\nmodel mispecifications. An application to education transmission is used as an\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 16:19:07 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 08:11:23 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Forcina", "Antonio", ""]]}]