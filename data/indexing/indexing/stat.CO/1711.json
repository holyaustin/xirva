[{"id": "1711.00101", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and Kehui Chen", "title": "Nonparametric covariance estimation for mixed longitudinal studies, with\n  applications in midlife women's health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In mixed longitudinal studies, a group of subjects enter the study at\ndifferent ages (cross-sectional) and are followed for successive years\n(longitudinal). In the context of such studies, we consider nonparametric\ncovariance estimation with samples of noisy and partially observed functional\ntrajectories. The proposed algorithm is based on a noniterative\nsequential-aggregation scheme with only basic matrix operations and closed-form\nsolutions in each step. The good performance of the proposed method is\nsupported by both theory and numerical experiments. We also apply the proposed\nprocedure to a study on the working memory of midlife women, based on data from\nthe Study of Women's Health Across the Nation (SWAN).\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:42:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:30:28 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 22:17:09 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:07:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Chen", "Kehui", ""]]}, {"id": "1711.00177", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Xianzheng Huang", "title": "Bandwidth selection for nonparametric modal regression", "comments": "To appear in Communications in Statistics - Simulation and\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of estimating local modes of a conditional density based on\nkernel density estimators, we show that existing bandwidth selection methods\ndeveloped for kernel density estimation are unsuitable for mode estimation. We\npropose two methods to select bandwidths tailored for mode estimation in the\nregression setting. Numerical studies using synthetic data and a real-life data\nset are carried out to demonstrate the performance of the proposed methods in\ncomparison with several well received bandwidth selection methods for density\nestimation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:52:56 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Zhou", "Haiming", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1711.00484", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang, Amy Braverman, and Hai Nguyen", "title": "Spatial Statistical Downscaling for Constructing High-Resolution Nature\n  Runs in Global Observing System Simulation Experiments", "comments": "Accepted version in Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system simulation experiments (OSSEs) have been widely used as a\nrigorous and cost-effective way to guide development of new observing systems,\nand to evaluate the performance of new data assimilation algorithms. Nature\nruns (NRs), which are outputs from deterministic models, play an essential role\nin building OSSE systems for global atmospheric processes because they are used\nboth to create synthetic observations at high spatial resolution, and to\nrepresent the \"true\" atmosphere against which the forecasts are verified.\nHowever, most NRs are generated at resolutions coarser than actual\nobservations. Here, we propose a principled statistical downscaling framework\nto construct high-resolution NRs via conditional simulation from\ncoarse-resolution numerical model output. We use nonstationary spatial\ncovariance function models that have basis function representations. This\napproach not only explicitly addresses the change-of-support problem, but also\nallows fast computation with large volumes of numerical model output. We also\npropose a data-driven algorithm to select the required basis functions\nadaptively, in order to increase the flexibility of our nonstationary\ncovariance function models. In this article we demonstrate these techniques by\ndownscaling a coarse-resolution physical NR at a native resolution of\n$1^{\\circ} \\text{ latitude} \\times 1.25^{\\circ} \\text{ longitude}$ of global\nsurface $\\text{CO}_2$ concentrations to 655,362 equal-area hexagons.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 18:01:26 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 18:43:29 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""], ["Braverman", "Amy", ""], ["Nguyen", "Hai", ""]]}, {"id": "1711.00564", "submitter": "Gregor Kastner", "authors": "Martin Feldkircher, Florian Huber, Gregor Kastner", "title": "Sophisticated and small versus simple and sizeable: When does it pay off\n  to introduce drifting coefficients in Bayesian VARs?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the relationship between model size and complexity in the\ntime-varying parameter VAR framework via thorough predictive exercises for the\nEuro Area, the United Kingdom and the United States. It turns out that\nsophisticated dynamics through drifting coefficients are important in small\ndata sets while simpler models tend to perform better in sizeable data sets. To\ncombine best of both worlds, novel shrinkage priors help to mitigate the curse\nof dimensionality, resulting in competitive forecasts for all scenarios\nconsidered. Furthermore, we discuss dynamic model selection to improve upon the\nbest performing individual model for each point in time.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 23:34:11 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 16:45:40 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Kastner", "Gregor", ""]]}, {"id": "1711.00789", "submitter": "Li Ma", "authors": "Meng Li and Li Ma", "title": "Learning Asymmetric and Local Features in Multi-Dimensional Data through\n  Wavelets with Recursive Partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective learning of asymmetric and local features in images and other data\nobserved on multi-dimensional grids is a challenging objective critical for a\nwide range of image processing applications involving biomedical and natural\nimages. It requires methods that are sensitive to local details while fast\nenough to handle massive numbers of images of ever increasing sizes. We\nintroduce a probabilistic model-based framework that achieves these objectives\nby incorporating adaptivity into discrete wavelet transforms (DWT) through\nBayesian hierarchical modeling, thereby allowing wavelet bases to adapt to the\ngeometric structure of the data while maintaining the high computational\nscalability of wavelet methods---linear in the sample size (e.g., the\nresolution of an image). We derive a recursive representation of the Bayesian\nposterior model which leads to an exact message passing algorithm to complete\nlearning and inference. While our framework is applicable to a range of\nproblems including multi-dimensional signal processing, compression, and\nstructural learning, we illustrate its work and evaluate its performance in the\ncontext of image reconstruction using real images from the ImageNet database,\ntwo widely used benchmark datasets, and a dataset from retinal optical\ncoherence tomography and compare its performance to state-of-the-art methods\nbased on basis transforms and deep learning.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 15:51:16 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 12:22:56 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 02:05:25 GMT"}, {"version": "v4", "created": "Mon, 29 Apr 2019 22:53:35 GMT"}, {"version": "v5", "created": "Fri, 6 Nov 2020 19:23:52 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Li", "Meng", ""], ["Ma", "Li", ""]]}, {"id": "1711.00922", "submitter": "Ari Pakman", "authors": "Ari Pakman", "title": "Binary Bouncy Particle Sampler", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bouncy Particle Sampler is a novel rejection-free non-reversible sampler\nfor differentiable probability distributions over continuous variables. We\ngeneralize the algorithm to piecewise differentiable distributions and apply it\nto generic binary distributions using a piecewise differentiable augmentation.\nWe illustrate the new algorithm in a binary Markov Random Field example, and\ncompare it to binary Hamiltonian Monte Carlo. Our results suggest that binary\nBPS samplers are better for easy to mix distributions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 20:36:43 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Pakman", "Ari", ""]]}, {"id": "1711.01206", "submitter": "Yuling Jiao", "authors": "Jian Huang, Yuling Jiao, Xiliang Lu, Liping Zhu", "title": "Robust Decoding from 1-Bit Compressive Sampling with Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1-bit compressive sensing (1-bit CS) where target signal is coded into a\nbinary measurement, one goal is to recover the signal from noisy and quantized\nsamples. Mathematically, the 1-bit CS model reads: $y = \\eta \\odot\\textrm{sign}\n(\\Psi x^* + \\epsilon)$, where $x^{*}\\in \\mathcal{R}^{n}, y\\in \\mathcal{R}^{m}$,\n$\\Psi \\in \\mathcal{R}^{m\\times n}$, and $\\epsilon$ is the random error before\nquantization and $\\eta\\in \\mathcal{R}^{n}$ is a random vector modeling the sign\nflips. Due to the presence of nonlinearity, noise and sign flips, it is quite\nchallenging to decode from the 1-bit CS. In this paper, we consider least\nsquares approach under the over-determined and under-determined settings. For\n$m>n$, we show that, up to a constant $c$, with high probability, the least\nsquares solution $x_{\\textrm{ls}}$ approximates $ x^*$ with precision $\\delta$\nas long as $m \\geq\\widetilde{\\mathcal{O}}(\\frac{n}{\\delta^2})$. For $m< n$, we\nprove that, up to a constant $c$, with high probability, the\n$\\ell_1$-regularized least-squares solution $x_{\\ell_1}$ lies in the ball with\ncenter $x^*$ and radius $\\delta$ provided that $m \\geq \\mathcal{O}( \\frac{s\\log\nn}{\\delta^2})$ and $\\|x^*\\|_0 := s < m$. We introduce a Newton type method, the\nso-called primal and dual active set (PDAS) algorithm, to solve the nonsmooth\noptimization problem. The PDAS possesses the property of one-step convergence.\nIt only requires to solve a small least squares problem on the active set.\nTherefore, the PDAS is extremely efficient for recovering sparse signals\nthrough continuation. We propose a novel regularization parameter selection\nrule which does not introduce any extra computational overhead. Extensive\nnumerical experiments are presented to illustrate the robustness of our\nproposed model and the efficiency of our algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 15:22:23 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Lu", "Xiliang", ""], ["Zhu", "Liping", ""]]}, {"id": "1711.01410", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys and Mira Kattwinkel", "title": "SPUX: Scalable Particle Markov Chain Monte Carlo for uncertainty\n  quantification in stochastic ecological models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of individual based models (IBMs), successful in modeling complex\necological dynamical systems, is often performed only ad-hoc. Bayesian\ninference can be used for both parameter estimation and uncertainty\nquantification, but its successful application to realistic scenarios has been\nhindered by the complex stochastic nature of IBMs. Computationally expensive\ntechniques such as Particle Filter (PF) provide marginal likelihood estimates,\nwhere multiple model simulations (particles) are required to get a sample from\nthe state distribution conditional on the observed data. Particle ensembles are\nre-sampled at each data observation time, requiring particle destruction and\nreplication, which lead to an increase in algorithmic complexity. We present\nSPUX, a Python implementation of parallel Particle Markov Chain Monte Carlo\n(PMCMC) algorithm, which mitigates high computational costs by distributing\nparticles over multiple computational units. Adaptive load re-balancing\ntechniques are used to mitigate computational work imbalances introduced by\nre-sampling. Framework performance is investigated and significant speed-ups\nare observed for a simple predator-prey IBM model.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 07:34:31 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Kattwinkel", "Mira", ""]]}, {"id": "1711.01504", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Ryan P. Browne and Paul D. McNicholas", "title": "Mixtures of Hidden Truncation Hyperbolic Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixture of factor analyzers model was first introduced over 20 years ago\nand, in the meantime, has been extended to several non-Gaussian analogues. In\ngeneral, these analogues account for situations with heavy tailed and/or skewed\nclusters. An approach is introduced that unifies many of these approaches into\none very general model: the mixture of hidden truncation hyperbolic factor\nanalyzers (MHTHFA) model. In the process of doing this, a hidden truncation\nhyperbolic factor analysis model is also introduced. The MHTHFA model is\nillustrated for clustering as well as semi-supervised classification using two\nreal datasets.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:41:41 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 19:24:42 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1711.02621", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Nisheeth K. Vishnoi", "title": "Convex Optimization with Unbounded Nonconvex Oracles using Simulated\n  Annealing", "comments": "To appear in COLT 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a convex objective function $F$ when\none can only evaluate its noisy approximation $\\hat{F}$. Unless one assumes\nsome structure on the noise, $\\hat{F}$ may be an arbitrary nonconvex function,\nmaking the task of minimizing $F$ intractable. To overcome this, prior work has\noften focused on the case when $F(x)-\\hat{F}(x)$ is uniformly-bounded. In this\npaper we study the more general case when the noise has magnitude $\\alpha F(x)\n+ \\beta$ for some $\\alpha, \\beta > 0$, and present a polynomial time algorithm\nthat finds an approximate minimizer of $F$ for this noise model. Previously,\nMarkov chains, such as the stochastic gradient Langevin dynamics, have been\nused to arrive at approximate solutions to these optimization problems.\nHowever, for the noise model considered in this paper, no single temperature\nallows such a Markov chain to both mix quickly and concentrate near the global\nminimizer. We bypass this by combining \"simulated annealing\" with the\nstochastic gradient Langevin dynamics, and gradually decreasing the temperature\nof the chain in order to approach the global minimizer. As a corollary one can\napproximately minimize a nonconvex function that is close to a convex function;\nhowever, the closeness can deteriorate as one moves away from the optimum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 17:36:47 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 09:00:43 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1711.02691", "submitter": "Radu Herbei", "authors": "Jeffrey J. Gory, Radu Herbei and Laura S. Kubatko", "title": "Bayesian Inference of Selection in the Wright-Fisher Diffusion Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of population-level allele frequency data across\none or more related populations necessitates the development of methods that\ncan efficiently estimate population genetics parameters, such as the strength\nof selection acting on the population(s), from such data. Existing methods for\nthis problem in the setting of the Wright-Fisher diffusion model are primarily\nlikelihood-based, and rely on numerical approximation for likelihood\ncomputation and on bootstrapping for assessment of variability in the resulting\nestimates, requiring extensive computation. Recent work (Jenkins and Spano,\n2015) has provided a method for obtaining exact samples from general\nWright-Fisher diffusion processes, enabling the development of methods for\nBayesian estimation in this setting. We develop and implement a Bayesian method\nfor estimating the strength of selection based on the Wright-Fisher diffusion\nfor data sampled at a single time point. The method utilizes the work of\nJenkins and Spano (2015) to develop a Markov chain Monte Carlo algorithm to\ndraw samples from the joint posterior distribution of the selection coefficient\nand the allele frequencies. We demonstrate that when assumptions about the\ninitial allele frequencies are accurate the method performs well for both\nsimulated data and for an empirical data set on hypoxia in flies (Zhou et al.\n2011), where we find evidence for strong positive selection in a region of\nchromosome 2L previously identified by Ronen et al. (2013). We discuss possible\nextensions of our method to the more general settings commonly encountered in\npractice, highlighting the advantages of Bayesian approaches to inference in\nthis setting.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 19:14:47 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Gory", "Jeffrey J.", ""], ["Herbei", "Radu", ""], ["Kubatko", "Laura S.", ""]]}, {"id": "1711.03259", "submitter": "Yinqiu He", "authors": "Yinqiu He, Gongjun Xu", "title": "Estimating Tail Probabilities of the Ratio of the Largest Eigenvalue to\n  the Trace of a Wishart Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an efficient Monte Carlo method to estimate the tail\nprobabilities of the ratio of the largest eigenvalue to the trace of the\nWishart matrix, which plays an important role in multivariate data analysis.\nThe estimator is constructed based on a change-of-measure technique and it is\nproved to be asymptotically efficient for both the real and complex Wishart\nmatrices. Simulation studies further show the outperformance of the proposed\nmethod over existing approaches based on asymptotic approximations, especially\nwhen estimating probabilities of rare events.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 05:35:48 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 11:36:17 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["He", "Yinqiu", ""], ["Xu", "Gongjun", ""]]}, {"id": "1711.03799", "submitter": "Alexander Scheel", "authors": "Alexander Scheel and Klaus Dietmayer", "title": "Tracking Multiple Vehicles Using a Variational Radar Model", "comments": "This is a preprint (i.e. the accepted version) of: A. Scheel and K.\n  Dietmayer, \"Tracking Multiple Vehicles Using a Variational Radar Model,\" in\n  IEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 10, pp.\n  3721-3736, 2019. Digital Object Identifier 10.1109/TITS.2018.2879041", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.RO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-resolution radar sensors are able to resolve multiple detections per\nobject and therefore provide valuable information for vehicle environment\nperception. For instance, multiple detections allow to infer the size of an\nobject or to more precisely measure the object's motion. Yet, the increased\namount of data raises the demands on tracking modules: measurement models that\nare able to process multiple detections for an object are necessary and\nmeasurement-to-object associations become more complex. This paper presents a\nnew variational radar model for tracking vehicles using radar detections and\ndemonstrates how this model can be incorporated into a Random-Finite-Set-based\nmulti-object filter. The measurement model is learned from actual data using\nvariational Gaussian mixtures and avoids excessive manual engineering. In\ncombination with the multiobject tracker, the entire process chain from the raw\nmeasurements to the resulting tracks is formulated probabilistically. The\npresented approach is evaluated on experimental data and it is demonstrated\nthat the data-driven measurement model outperforms a manually designed model.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 13:02:28 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 10:41:29 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 09:25:46 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Scheel", "Alexander", ""], ["Dietmayer", "Klaus", ""]]}, {"id": "1711.04181", "submitter": "Diego Marcondes", "authors": "Diego Marcondes, Adilson Simonis and Junior Barrera", "title": "Feature Selection based on the Local Lift Dependence Scale", "comments": null, "journal-ref": null, "doi": "10.3390/e20020097", "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a classical approach to feature selection: minimization of a\ncost function applied on estimated joint distributions. However, the search\nspace in which such minimization is performed is extended. In the original\nformulation, the search space is the Boolean lattice of features sets (BLFS),\nwhile, in the present formulation, it is a collection of Boolean lattices of\nordered pairs (features, associated value) (CBLOP), indexed by the elements of\nthe BLFS. In this approach, we may not only select the features that are most\nrelated to a variable Y, but also select the values of the features that most\ninfluence the variable or that are most prone to have a specific value of Y. A\nlocal formulation of Shanon's mutual information is applied on a CBLOP to\nselect features, namely, the Local Lift Dependence Scale, an scale for\nmeasuring variable dependence in multiple resolutions. The main contribution of\nthis paper is to define and apply this local measure, which permits to analyse\nlocal properties of joint distributions that are neglected by the classical\nShanon's global measure. The proposed approach is applied to a dataset\nconsisting of student performances on a university entrance exam, as well as on\nundergraduate courses. The approach is also applied to two datasets of the UCI\nMachine Learning Repository.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 18:51:13 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 10:34:57 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 20:03:31 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Marcondes", "Diego", ""], ["Simonis", "Adilson", ""], ["Barrera", "Junior", ""]]}, {"id": "1711.04280", "submitter": "Nadhir Ben Rached", "authors": "Nadhir Ben Rached and Zdravko Botev and Abla Kammoun and Mohamed-Slim\n  Alouini and Raul Tempone", "title": "On the Sum of Order Statistics and Applications to Wireless\n  Communication Systems Performances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating the cumulative distribution function\n(CDF) of the sum of order statistics, which serves to compute outage\nprobability (OP) values at the output of generalized selection combining\nreceivers. Generally, closed-form expressions of the CDF of the sum of order\nstatistics are unavailable for many practical distributions. Moreover, the\nnaive Monte Carlo (MC) method requires a substantial computational effort when\nthe probability of interest is sufficiently small. In the region of small OP\nvalues, we propose instead two effective variance reduction techniques that\nyield a reliable estimate of the CDF with small computing cost. The first\nestimator, which can be viewed as an importance sampling estimator, has bounded\nrelative error under a certain assumption that is shown to hold for most of the\nchallenging distributions. An improvement of this estimator is then proposed\nfor the Pareto and the Weibull cases. The second is a conditional MC estimator\nthat achieves the bounded relative error property for the Generalized Gamma\ncase and the logarithmic efficiency in the Log-normal case. Finally, the\nefficiency of these estimators is compared via various numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 12 Nov 2017 12:12:47 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Rached", "Nadhir Ben", ""], ["Botev", "Zdravko", ""], ["Kammoun", "Abla", ""], ["Alouini", "Mohamed-Slim", ""], ["Tempone", "Raul", ""]]}, {"id": "1711.04399", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "Circularly-Coupled Markov Chain Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show how to run an N-time-step Markov chain simulation in a circular\nfashion, so that the state at time 0 follows the state at time N-1 in the same\nway as states at times t follow those at times t-1 for 0<t<N. This wrap-around\nof the chain is achieved using a coupling procedure, and produces states that\nall have close to the equilibrium distribution of the Markov chain, under the\nassumption that coupled chains are likely to coalesce in less than N/2\niterations. This procedure therefore automatically eliminates the initial\nportion of the chain that would otherwise need to be discarded to get good\nestimates of equilibrium averages. The assumption of rapid coalescence can be\ntested using auxiliary chains started at times spaced between 0 and N. When\nmultiple processors are available, such auxiliary chains can be simulated in\nparallel, and pieced together to give the circularly-coupled chain, in less\ntime than a sequential simulation would have taken, provided that coalescence\nis indeed rapid.\n  The practical utility of these procedures is dependent on the development of\ngood coupling schemes. I show how a specialized random-grid Metropolis\nalgorithm can be used to produce the required exact coalescence. On its own,\nthis method is not efficient in high dimensions, but it can be used to produce\nexact coalescence once other methods have brought the coupled chains close\ntogether. I investigate how well this combined scheme works with standard\nMetropolis, Langevin, and Gibbs sampling updates. Using such strategies, I show\nthat circular coupling can work effectively in a Bayesian logistic regression\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 02:59:16 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Neal", "Radford M.", ""]]}, {"id": "1711.04632", "submitter": "Daniel W. Meyer", "authors": "Daniel W. Meyer", "title": "(Un)Conditional Sample Generation Based on Distribution Element Trees", "comments": "published online in the Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": "10.1080/10618600.2018.1482768", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, distribution element trees (DETs) were introduced as an accurate\nand computationally efficient method for density estimation. In this work, we\ndemonstrate that the DET formulation promotes an easy and inexpensive way to\ngenerate random samples similar to a smooth bootstrap. These samples can be\ngenerated unconditionally, but also, without further complications,\nconditionally utilizing available information about certain probability-space\ncomponents.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 15:22:45 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 06:35:42 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Meyer", "Daniel W.", ""]]}, {"id": "1711.04694", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Marcel Schoengens and Lorenzo Pacchiardi and\n  Avinash Ummadisingu and Nicole Widmer and Jukka-Pekka Onnela and Antonietta\n  Mira", "title": "ABCpy: A High-Performance Computing Perspective to Approximate Bayesian\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABCpy is a highly modular scientific library for Approximate Bayesian\nComputation (ABC) written in Python. The main contribution of this paper is to\ndocument a software engineering effort that enables domain scientists to easily\napply ABC to their research without being ABC experts; using ABCpy they can\neasily run large parallel simulations without much knowledge about\nparallelization. Further, ABCpy enables ABC experts to easily develop new\ninference schemes and evaluate them in a standardized environment and to extend\nthe library with new algorithms. These benefits come mainly from the modularity\nof ABCpy. We give an overview of the design of ABCpy and provide a performance\nevaluation concentrating on parallelization. This points us towards the\ninherent imbalance in some of the ABC algorithms. We develop a dynamic\nscheduling MPI implementation to mitigate this issue and evaluate the various\nABC algorithms according to their adaptability towards high-performance\ncomputing.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:40:40 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 11:03:39 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 20:42:22 GMT"}, {"version": "v4", "created": "Thu, 25 Feb 2021 11:51:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Schoengens", "Marcel", ""], ["Pacchiardi", "Lorenzo", ""], ["Ummadisingu", "Avinash", ""], ["Widmer", "Nicole", ""], ["Onnela", "Jukka-Pekka", ""], ["Mira", "Antonietta", ""]]}, {"id": "1711.04702", "submitter": "Deisy Morselli Gysi", "authors": "Deisy Morselli Gysi, Andre Voigt, Tiago de Miranda Fragoso, Eivind\n  Almaas and Katja Nowick", "title": "wTO: an R package for computing weighted topological overlap and\n  consensus networks with an integrated visualization tool", "comments": null, "journal-ref": null, "doi": "10.1186/s12859-018-2351-7", "report-no": null, "categories": "q-bio.MN stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analyses, such as of gene co-expression networks, metabolic networks\nand ecological networks have become a central approach for the systems-level\nstudy of biological data. Several software packages exist for generating and\nanalyzing such networks, either from correlation scores or the absolute value\nof a transformed score called weighted topological overlap (wTO). However,\nsince gene regulatory processes can up- or down-regulate genes, it is of great\ninterest to explicitly consider both positive and negative correlations when\nconstructing a gene co-expression network. Here, we present an R package for\ncalculating the wTO, that, in contrast to existing packages, explicitly\naddresses the sign of the wTO values, and is thus especially valuable for the\nanalysis of gene regulatory networks. The package includes the calculation of\np-values (raw and adjusted) for each pairwise gene score. Our package also\nallows the calculation of networks from time series (without replicates). Since\nnetworks from independent datasets (biological repeats or related studies) are\nnot the same due to technical and biological noise in the data, we\nadditionally, incorporated a novel method for calculating a consensus network\n(CN) from two or more networks into our R package. We compare our new wTO\npackage to state of art packages and demonstrate the application of the wTO and\nCN functions using 3 independently derived datasets from healthy human\npre-frontal cortex samples. To showcase an example for the time series\napplication we utilized a metagenomics data set. In this work, we developed a\nsoftware package that allows the computation of wTO networks, CNs and a\nvisualization tool in the R statistical environment. It is publicly available\non CRAN repositories under the GPL-2 Open Source License\n(https://cran.r-project.org/web/packages/wTO/).\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 16:54:57 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:44:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gysi", "Deisy Morselli", ""], ["Voigt", "Andre", ""], ["Fragoso", "Tiago de Miranda", ""], ["Almaas", "Eivind", ""], ["Nowick", "Katja", ""]]}, {"id": "1711.04812", "submitter": "Liuyi Hu", "authors": "Liuyi Hu, Wenbin Lu, Jin Zhou and Hua Zhou", "title": "MM Algorithms for Variance Component Estimation and Selection in\n  Logistic Linear Mixed Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Logistic linear mixed model is widely used in experimental designs and\ngenetic analysis with binary traits. Motivated by modern applications, we\nconsider the case with many groups of random effects and each group corresponds\nto a variance component. When the number of variance components is large,\nfitting the logistic linear mixed model is challenging. We develop two\nefficient and stable minorization-maximization (MM) algorithms for the\nestimation of variance components based on the Laplace approximation of the\nlogistic model. One of them leads to a simple iterative soft-thresholding\nalgorithm for variance component selection using maximum penalized approximated\nlikelihood. We demonstrate the variance component estimation and selection\nperformance of our algorithms by simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 19:39:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Hu", "Liuyi", ""], ["Lu", "Wenbin", ""], ["Zhou", "Jin", ""], ["Zhou", "Hua", ""]]}, {"id": "1711.04827", "submitter": "Biljana Jonoska Stojkova", "authors": "Biljana Jonoska Stojkova and David A. Campbell", "title": "Incremental Mixture Importance Sampling with Shotgun optimization", "comments": "27 pages, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general optimization strategy, which combines results\nfrom different optimization or parameter estimation methods to overcome\nshortcomings of a single method. Shotgun optimization is developed as a\nframework which employs different optimization strategies, criteria, or\nconditional targets to enable wider likelihood exploration. The introduced\nShotgun optimization approach is embedded into an incremental mixture\nimportance sampling algorithm to produce improved posterior samples for\nmultimodal densities and creates robustness in cases where the likelihood and\nprior are in disagreement. Despite using different optimization approaches, the\nsamples are combined into samples from a single target posterior. The diversity\nof the framework is demonstrated on parameter estimation from differential\nequation models employing diverse strategies including numerical solutions and\napproximations thereof. Additionally the approach is demonstrated on mixtures\nof discrete and continuous parameters and is shown to ease estimation from\nsynthetic likelihood models. R code of the implemented examples is stored in a\nzipped archive (codeSubmit.zip).\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 20:09:24 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Stojkova", "Biljana Jonoska", ""], ["Campbell", "David A.", ""]]}, {"id": "1711.05174", "submitter": "Yining Wang", "authors": "Zeyuan Allen-Zhu and Yuanzhi Li and Aarti Singh and Yining Wang", "title": "Near-Optimal Discrete Optimization for Experimental Design: A Regret\n  Minimization Approach", "comments": "33 pages, 4 tables. A preliminary version of this paper titled\n  \"Near-Optimal Experimental Design via Regret Minimization\" with weaker\n  results appeared in the Proceedings of the 34th International Conference on\n  Machine Learning (ICML 2017), Sydney", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The experimental design problem concerns the selection of k points from a\npotentially large design pool of p-dimensional vectors, so as to maximize the\nstatistical efficiency regressed on the selected k design points. Statistical\nefficiency is measured by optimality criteria, including A(verage),\nD(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the\nT-optimality, exact optimization is NP-hard.\n  We propose a polynomial-time regret minimization framework to achieve a\n$(1+\\varepsilon)$ approximation with only $O(p/\\varepsilon^2)$ design points,\nfor all the optimality criteria above.\n  In contrast, to the best of our knowledge, before our work, no\npolynomial-time algorithm achieves $(1+\\varepsilon)$ approximations for\nD/E/G-optimality, and the best poly-time algorithm achieving\n$(1+\\varepsilon)$-approximation for A/V-optimality requires $k =\n\\Omega(p^2/\\varepsilon)$ design points.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 16:21:57 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Allen-Zhu", "Zeyuan", ""], ["Li", "Yuanzhi", ""], ["Singh", "Aarti", ""], ["Wang", "Yining", ""]]}, {"id": "1711.05307", "submitter": "Lingge Li", "authors": "Lingge Li, Andrew Holbrook, Babak Shahbaba, Pierre Baldi", "title": "Neural Network Gradient Hamiltonian Monte Carlo", "comments": null, "journal-ref": "Comput Stat (2019) 34: 281", "doi": "10.1007/s00180-018-00861-z", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo is a widely used algorithm for sampling from\nposterior distributions of complex Bayesian models. It can efficiently explore\nhigh-dimensional parameter spaces guided by simulated Hamiltonian flows.\nHowever, the algorithm requires repeated gradient calculations, and these\ncomputations become increasingly burdensome as data sets scale. We present a\nmethod to substantially reduce the computation burden by using a neural network\nto approximate the gradient. First, we prove that the proposed method still\nmaintains convergence to the true distribution though the approximated gradient\nno longer comes from a Hamiltonian system. Second, we conduct experiments on\nsynthetic examples and real data sets to validate the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 20:23:50 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 03:49:10 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Li", "Lingge", ""], ["Holbrook", "Andrew", ""], ["Shahbaba", "Babak", ""], ["Baldi", "Pierre", ""]]}, {"id": "1711.05337", "submitter": "Nawaf Bou-Rabee", "authors": "Nawaf Bou-Rabee and Jes\\'us Mar\\'ia Sanz-Serna", "title": "Geometric integrators and the Hamiltonian Monte Carlo method", "comments": "Final version will appear in Acta Numerica 2018", "journal-ref": "Acta Numerica, Vol. 27, pp. 113-206, 2018", "doi": "10.1017/S0962492917000101", "report-no": null, "categories": "math.PR math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper surveys in detail the relations between numerical integration and\nthe Hamiltonian (or hybrid) Monte Carlo method (HMC). Since the computational\ncost of HMC mainly lies in the numerical integrations, these should be\nperformed as efficiently as possible. However, HMC requires methods that have\nthe geometric properties of being volume-preserving and reversible, and this\nlimits the number of integrators that may be used. On the other hand, these\ngeometric properties have important quantitative implications on the\nintegration error, which in turn have an impact on the acceptance rate of the\nproposal. While at present the velocity Verlet algorithm is the method of\nchoice for good reasons, we argue that Verlet can be improved upon. We also\ndiscuss in detail the behavior of HMC as the dimensionality of the target\ndistribution increases.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 22:38:46 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Bou-Rabee", "Nawaf", ""], ["Sanz-Serna", "Jes\u00fas Mar\u00eda", ""]]}, {"id": "1711.05825", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bootstrapped synthetic likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and synthetic likelihood (SL)\ntechniques have enabled the use of Bayesian inference for models that may be\nsimulated, but for which the likelihood cannot be evaluated pointwise at values\nof an unknown parameter $\\theta$. The main idea in ABC and SL is to, for\ndifferent values of $\\theta$ (usually chosen using a Monte Carlo algorithm),\nbuild estimates of the likelihood based on simulations from the model\nconditional on $\\theta$. The quality of these estimates determines the\nefficiency of an ABC/SL algorithm. In standard ABC/SL, the only means to\nimprove an estimated likelihood at $\\theta$ is to simulate more times from the\nmodel conditional on $\\theta$, which is infeasible in cases where the simulator\nis computationally expensive. In this paper we describe how to use\nbootstrapping as a means for improving SL estimates whilst using fewer\nsimulations from the model, and also investigate its use in ABC. Further, we\ninvestigate the use of the bag of little bootstraps as a means for applying\nthis approach to large datasets, yielding Monte Carlo algorithms that\naccurately approximate posterior distributions whilst only simulating\nsubsamples of the full data. Examples of the approach applied to i.i.d.,\ntemporal and spatial data are given.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:13:48 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 23:16:04 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1711.06695", "submitter": "David Kepplinger", "authors": "David Kepplinger, Peter Filzmoser, Kurt Varmuza", "title": "Variable selection with genetic algorithms using repeated\n  cross-validation of PLS regression models as fitness measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic algorithms are a widely used method in chemometrics for extracting\nvariable subsets with high prediction power. Most fitness measures used by\nthese genetic algorithms are based on the ordinary least-squares fit of the\nresulting model to the entire data or a subset thereof. Due to\nmulticollinearity, partial least squares regression is often more appropriate,\nbut rarely considered in genetic algorithms due to the additional cost for\nestimating the optimal number of components. We introduce two novel fitness\nmeasures for genetic algorithms, explicitly designed to estimate the internal\nprediction performance of partial least squares regression models built from\nthe variable subsets. Both measures estimate the optimal number of components\nusing cross-validation and subsequently estimate the prediction performance by\npredicting the response of observations not included in model-fitting. This is\nrepeated multiple times to estimate the measures' variations due to different\nrandom splits. Moreover, one measure was optimized for speed and more accurate\nestimation of the prediction performance for observations not included during\nvariable selection. This leads to variable subsets with high internal and\nexternal prediction power. Results on high-dimensional chemical-analytical data\nshow that the variable subsets acquired by this approach have competitive\ninternal prediction power and superior external prediction power compared to\nvariable subsets extracted with other fitness measures.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 19:05:25 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Kepplinger", "David", ""], ["Filzmoser", "Peter", ""], ["Varmuza", "Kurt", ""]]}, {"id": "1711.06719", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and Eric P. Xing", "title": "Techniques for proving Asynchronous Convergence results for Markov Chain\n  Monte Carlo methods", "comments": "Workshop on Advances in Approximate Bayesian Inference, 31st\n  Conference on Neural Information Processing Systems, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods such as Gibbs sampling are finding\nwidespread use in applied statistics and machine learning. These often lead to\ndifficult computational problems, which are increasingly being solved on\nparallel and distributed systems such as compute clusters. Recent work has\nproposed running iterative algorithms such as gradient descent and MCMC in\nparallel asynchronously for increased performance, with good empirical results\nin certain problems. Unfortunately, for MCMC this parallelization technique\nrequires new convergence theory, as it has been explicitly demonstrated to lead\nto divergence on some examples. Recent theory on Asynchronous Gibbs sampling\ndescribes why these algorithms can fail, and provides a way to alter them to\nmake them converge. In this article, we describe how to apply this theory in a\ngeneric setting, to understand the asynchronous behavior of any MCMC algorithm,\nincluding those implemented using parameter servers, and those not based on\nGibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 20:46:38 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 21:38:29 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:01:13 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 15:54:34 GMT"}, {"version": "v5", "created": "Sun, 3 Jun 2018 23:42:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Terenin", "Alexander", ""], ["Xing", "Eric P.", ""]]}, {"id": "1711.06771", "submitter": "Zachary Charles", "authors": "Zachary Charles, Dimitris Papailiopoulos, Jordan Ellenberg", "title": "Approximate Gradient Coding via Sparse Random Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed algorithms are often beset by the straggler effect, where the\nslowest compute nodes in the system dictate the overall running time.\nCoding-theoretic techniques have been recently proposed to mitigate stragglers\nvia algorithmic redundancy. Prior work in coded computation and gradient coding\nhas mainly focused on exact recovery of the desired output. However, slightly\ninexact solutions can be acceptable in applications that are robust to noise,\nsuch as model training via gradient-based algorithms. In this work, we present\ncomputationally simple gradient codes based on sparse graphs that guarantee\nfast and approximately accurate distributed computation. We demonstrate that\nsacrificing a small amount of accuracy can significantly increase algorithmic\nrobustness to stragglers.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 23:19:30 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Charles", "Zachary", ""], ["Papailiopoulos", "Dimitris", ""], ["Ellenberg", "Jordan", ""]]}, {"id": "1711.06999", "submitter": "Daniele Durante", "authors": "Daniele Durante and Tommaso Rigon", "title": "Conditionally conjugate mean-field variational Bayes for logistic models", "comments": null, "journal-ref": "Statistical Science (2019). 34, 472-485", "doi": "10.1214/19-STS712", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a common strategy for approximate Bayesian\ninference, but simple methods are only available for specific classes of models\nincluding, in particular, representations having conditionally conjugate\nconstructions within an exponential family. Models with logit components are an\napparently notable exception to this class, due to the absence of conjugacy\nbetween the logistic likelihood and the Gaussian priors for the coefficients in\nthe linear predictor. To facilitate approximate inference within this widely\nused class of models, Jaakkola and Jordan (2000) proposed a simple variational\napproach which relies on a family of tangent quadratic lower bounds of logistic\nlog-likelihoods, thus restoring conjugacy between these approximate bounds and\nthe Gaussian priors. This strategy is still implemented successfully, but less\nattempts have been made to formally understand the reasons underlying its\nexcellent performance. To cover this key gap, we provide a formal connection\nbetween the above bound and a recent P\\'olya-gamma data augmentation for\nlogistic regression. Such a result places the computational methods associated\nwith the aforementioned bounds within the framework of variational inference\nfor conditionally conjugate exponential family models, thereby allowing recent\nadvances for this class to be inherited also by the methods relying on Jaakkola\nand Jordan (2000).\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 10:50:24 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 06:59:24 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""], ["Rigon", "Tommaso", ""]]}, {"id": "1711.07177", "submitter": "Jelena Markovic", "authors": "Amir Sepehri and Jelena Markovic", "title": "Non-reversible, tuning- and rejection-free Markov chain Monte Carlo via\n  iterated random functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a non-reversible, tuning- and rejection-free Markov\nchain Monte Carlo which naturally fits in the framework of hit-and-run. The\nsampler only requires access to the gradient of the log-density function, hence\nthe normalizing constant is not needed. We prove the proposed Markov chain is\ninvariant for the target distribution and illustrate its applicability through\na wide range of examples. We show that the sampler introduced in the present\npaper is intimately related to the continuous sampler of Peters and de With\n(2012), Bouchard-Cote et al. (2017). In particular, the computation is quite\nsimilar in the sense that both are centered around simulating an inhomogenuous\nPoisson process. The computation can be simplified when the gradient of the\nlog-density admits a computationally efficient directional decomposition into a\nsum of two monotone functions. We apply our sampler in selective inference,\ngaining significant improvement over the formerly used sampler (Tian et al.\n2016).\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 07:39:32 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 21:38:03 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Sepehri", "Amir", ""], ["Markovic", "Jelena", ""]]}, {"id": "1711.07424", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella", "title": "Informed proposals for local MCMC in discrete spaces", "comments": "20 pages + 14 pages of supplementary, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lack of methodological results to design efficient Markov chain\nMonte Carlo (MCMC) algorithms for statistical models with discrete-valued\nhigh-dimensional parameters. Motivated by this consideration, we propose a\nsimple framework for the design of informed MCMC proposals (i.e.\nMetropolis-Hastings proposal distributions that appropriately incorporate local\ninformation about the target) which is naturally applicable to both discrete\nand continuous spaces. We explicitly characterize the class of optimal proposal\ndistributions under this framework, which we refer to as locally-balanced\nproposals, and prove their Peskun-optimality in high-dimensional regimes. The\nresulting algorithms are straightforward to implement in discrete spaces and\nprovide orders of magnitude improvements in efficiency compared to alternative\nMCMC schemes, including discrete versions of Hamiltonian Monte Carlo.\nSimulations are performed with both simulated and real datasets, including a\ndetailed application to Bayesian record linkage. A direct connection with\ngradient-based MCMC suggests that locally-balanced proposals may be seen as a\nnatural way to extend the latter to discrete spaces.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 17:31:00 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Zanella", "Giacomo", ""]]}, {"id": "1711.07564", "submitter": "Chaoxu Zhou", "authors": "Jose Blanchet, Donald Goldfarb, Garud Iyengar, Fengpei Li, Chaoxu Zhou", "title": "Unbiased Simulation for Optimizing Stochastic Function Compositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce an unbiased gradient simulation algorithms for\nsolving convex optimization problem with stochastic function compositions. We\nshow that the unbiased gradient generated from the algorithm has finite\nvariance and finite expected computation cost. We then combined the unbiased\ngradient simulation with two variance reduced algorithms (namely SVRG and SCSG)\nand showed that the proposed optimization algorithms based on unbiased gradient\nsimulations exhibit satisfactory convergence properties. Specifically, in the\nSVRG case, the algorithm with simulated gradient can be shown to converge\nlinearly to optima in expectation and almost surely under strong convexity.\nFinally, for the numerical experiment,we applied the algorithms to two\nimportant cases of stochastic function compositions optimization: maximizing\nthe Cox's partial likelihood model and training conditional random fields.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 21:58:01 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Blanchet", "Jose", ""], ["Goldfarb", "Donald", ""], ["Iyengar", "Garud", ""], ["Li", "Fengpei", ""], ["Zhou", "Chaoxu", ""]]}, {"id": "1711.07582", "submitter": "Anqi Fu", "authors": "Anqi Fu, Balasubramanian Narasimhan, and Stephen Boyd", "title": "CVXR: An R Package for Disciplined Convex Optimization", "comments": "34 pages, 9 figures", "journal-ref": "Journal of Statistical Software, 94(14), 1-34, 2020", "doi": "10.18637/jss.v094.i14", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CVXR is an R package that provides an object-oriented modeling language for\nconvex optimization, similar to CVX, CVXPY, YALMIP, and Convex.jl. It allows\nthe user to formulate convex optimization problems in a natural mathematical\nsyntax rather than the restrictive form required by most solvers. The user\nspecifies an objective and set of constraints by combining constants,\nvariables, and parameters using a library of functions with known mathematical\nproperties. CVXR then applies signed disciplined convex programming (DCP) to\nverify the problem's convexity. Once verified, the problem is converted into\nstandard conic form using graph implementations and passed to a cone solver\nsuch as ECOS or SCS. We demonstrate CVXR's modeling framework with several\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 00:01:17 GMT"}, {"version": "v2", "created": "Fri, 1 Dec 2017 01:43:05 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 22:59:58 GMT"}, {"version": "v4", "created": "Mon, 29 Jun 2020 18:26:09 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fu", "Anqi", ""], ["Narasimhan", "Balasubramanian", ""], ["Boyd", "Stephen", ""]]}, {"id": "1711.07748", "submitter": "Michael Fop", "authors": "Michael Fop, Thomas Brendan Murphy, Luca Scrucca", "title": "Model-based Clustering with Sparse Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite Gaussian mixture models are widely used for model-based clustering of\ncontinuous data. Nevertheless, since the number of model parameters scales\nquadratically with the number of variables, these models can be easily\nover-parameterized. For this reason, parsimonious models have been developed\nvia covariance matrix decompositions or assuming local independence. However,\nthese remedies do not allow for direct estimation of sparse covariance matrices\nnor do they take into account that the structure of association among the\nvariables can vary from one cluster to the other. To this end, we introduce\nmixtures of Gaussian covariance graph models for model-based clustering with\nsparse covariance matrices. A penalized likelihood approach is employed for\nestimation and a general penalty term on the graph configurations can be used\nto induce different levels of sparsity and incorporate prior knowledge. Model\nestimation is carried out using a structural-EM algorithm for parameters and\ngraph structure estimation, where two alternative strategies based on a genetic\nalgorithm and an efficient stepwise search are proposed for inference. With\nthis approach, sparse component covariance matrices are directly obtained. The\nframework results in a parsimonious model-based clustering of the data via a\nflexible model for the within-group joint distribution of the variables.\nExtensive simulated data experiments and application to illustrative datasets\nshow that the method attains good classification performance and model quality.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 12:36:08 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 12:21:14 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fop", "Michael", ""], ["Murphy", "Thomas Brendan", ""], ["Scrucca", "Luca", ""]]}, {"id": "1711.08030", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Pierre A. Gremaud, and Ralph C. Smith", "title": "Variance-based sensitivity analysis for time-dependent processes", "comments": "28 Pages; revised version; accepted for publication in Reliability\n  Engineering & System Safety", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global sensitivity analysis of time-dependent processes requires\nhistory-aware approaches. We develop for that purpose a variance-based method\nthat leverages the correlation structure of the problems under study and\nemploys surrogate models to accelerate the computations. The errors resulting\nfrom fixing unimportant uncertain parameters to their nominal values are\nanalyzed through a priori estimates. We illustrate our approach on a harmonic\noscillator example and on a nonlinear dynamic cholera model.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 20:40:45 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 14:27:55 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 17:18:25 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Alexanderian", "Alen", ""], ["Gremaud", "Pierre A.", ""], ["Smith", "Ralph C.", ""]]}, {"id": "1711.08683", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over", "title": "Bayesian random-effects meta-analysis using the bayesmeta R package", "comments": "51 pages, 8 figures", "journal-ref": "Journal of Statistical Software, 93(6):1-51, 2020", "doi": "10.18637/jss.v093.i06", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random-effects or normal-normal hierarchical model is commonly utilized\nin a wide range of meta-analysis applications. A Bayesian approach to inference\nis very attractive in this context, especially when a meta-analysis is based\nonly on few studies. The bayesmeta R package provides readily accessible tools\nto perform Bayesian meta-analyses and generate plots and summaries, without\nhaving to worry about computational details. It allows for flexible prior\nspecification and instant access to the resulting posterior distributions,\nincluding prediction and shrinkage estimation, and facilitating for example\nquick sensitivity checks. The present paper introduces the underlying theory\nand showcases its usage.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 13:11:43 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 09:24:35 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["R\u00f6ver", "Christian", ""]]}, {"id": "1711.08822", "submitter": "Kin Wai Chan", "authors": "Kin Wai Chan and Xiao-Li Meng", "title": "Multiple Improvements of Multiple Imputation Likelihood Ratio Tests", "comments": "45 pages, 9 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) inference handles missing data by first properly\nimputing the missing values $m$ times, and then combining the $m$ analysis\nresults from applying a complete-data procedure to each of the completed\ndatasets. However, the existing method for combining likelihood ratio tests has\nmultiple defects: (i) the combined test statistic can be negative in practice\nwhen the reference null distribution is a standard $F$ distribution; (ii) it is\nnot invariant to re-parametrization; (iii) it fails to ensure monotonic power\ndue to its use of an inconsistent estimator of the fraction of missing\ninformation (FMI) under the alternative hypothesis; and (iv) it requires\nnon-trivial access to the likelihood ratio test statistic as a function of\nestimated parameters instead of datasets. This paper shows, via both\ntheoretical derivations and empirical investigations, that essentially all of\nthese problems can be straightforwardly addressed if we are willing to perform\nan additional likelihood ratio test by stacking the $m$ completed datasets as\none big completed dataset. A particularly intriguing finding is that the FMI\nitself can be estimated consistently by a likelihood ratio statistic for\ntesting whether the $m$ completed datasets produced by MI can be regarded\neffectively as samples coming from a common model. Practical guidelines are\nprovided based on an extensive comparison of existing MI tests.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:18:45 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 03:48:21 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Chan", "Kin Wai", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1711.09131", "submitter": "Salar Fattahi", "authors": "Salar Fattahi, Richard Y. Zhang, Somayeh Sojoudi", "title": "Sparse Inverse Covariance Estimation for Chordal Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Graphical Lasso (GL), a popular optimization\nproblem for learning the sparse representations of high-dimensional datasets,\nwhich is well-known to be computationally expensive for large-scale problems.\nRecently, we have shown that the sparsity pattern of the optimal solution of GL\nis equivalent to the one obtained from simply thresholding the sample\ncovariance matrix, for sparse graphs under different conditions. We have also\nderived a closed-form solution that is optimal when the thresholded sample\ncovariance matrix has an acyclic structure. As a major generalization of the\nprevious result, in this paper we derive a closed-form solution for the GL for\ngraphs with chordal structures. We show that the GL and thresholding\nequivalence conditions can significantly be simplified and are expected to hold\nfor high-dimensional problems if the thresholded sample covariance matrix has a\nchordal structure. We then show that the GL and thresholding equivalence is\nenough to reduce the GL to a maximum determinant matrix completion problem and\ndrive a recursive closed-form solution for the GL when the thresholded sample\ncovariance matrix has a chordal structure. For large-scale problems with up to\n450 million variables, the proposed method can solve the GL problem in less\nthan 2 minutes, while the state-of-the-art methods converge in more than 2\nhours.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 20:45:26 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Fattahi", "Salar", ""], ["Zhang", "Richard Y.", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1711.09365", "submitter": "Zaid Sawlan", "authors": "Marco Iglesias, Zaid Sawlan, Marco Scavino, Raul Tempone, Christopher\n  Wood", "title": "Ensemble-marginalized Kalman filter for linear time-dependent PDEs with\n  noisy boundary conditions: Application to heat transfer in building walls", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aac224", "report-no": null, "categories": "stat.CO math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present the ensemble-marginalized Kalman filter (EnMKF), a\nsequential algorithm analogous to our previously proposed approach [1,2], for\nestimating the state and parameters of linear parabolic partial differential\nequations in initial-boundary value problems when the boundary data are noisy.\nWe apply EnMKF to infer the thermal properties of building walls and to\nestimate the corresponding heat flux from real and synthetic data. Compared\nwith a modified Ensemble Kalman Filter (EnKF) that is not marginalized, EnMKF\nreduces the bias error, avoids the collapse of the ensemble without needing to\nadd inflation, and converges to the mean field posterior using $50\\%$ or less\nof the ensemble size required by EnKF. According to our results, the\nmarginalization technique in EnMKF is key to performance improvement with\nsmaller ensembles at any fixed time.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 10:34:58 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 12:16:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Iglesias", "Marco", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Tempone", "Raul", ""], ["Wood", "Christopher", ""]]}, {"id": "1711.09545", "submitter": "Matthew Dixon", "authors": "Matthew Dixon, Diego Klabjan and Lan Wei", "title": "OSTSC: Over Sampling for Time Series Classification in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OSTSC package is a powerful oversampling approach for classifying\nunivariant, but multinomial time series data in R. This article provides a\nbrief overview of the oversampling methodology implemented by the package. A\ntutorial of the OSTSC package is provided. We begin by providing three test\ncases for the user to quickly validate the functionality in the package. To\ndemonstrate the performance impact of OSTSC, we then provide two medium size\nimbalanced time series datasets. Each example applies a TensorFlow\nimplementation of a Long Short-Term Memory (LSTM) classifier - a type of a\nRecurrent Neural Network (RNN) classifier - to imbalanced time series. The\nclassifier performance is compared with and without oversampling. Finally,\nlarger versions of these two datasets are evaluated to demonstrate the\nscalability of the package. The examples demonstrate that the OSTSC package\nimproves the performance of RNN classifiers applied to highly imbalanced time\nseries data. In particular, OSTSC is observed to increase the AUC of LSTM from\n0.543 to 0.784 on a high frequency trading dataset consisting of 30,000 time\nseries observations.\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 05:43:48 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Dixon", "Matthew", ""], ["Klabjan", "Diego", ""], ["Wei", "Lan", ""]]}, {"id": "1711.10186", "submitter": "Michael Grayling", "authors": "Michael Grayling, Adrian Mander", "title": "Calculations involving the multivariate normal and multivariate t\n  distributions with and without truncation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a set of Stata commands and Mata functions to evaluate\ndifferent distributional quantities of the multivariate normal distribution,\nand a particular type of non-central multivariate t distribution. Specifically,\ntheir densities, distribution functions, equicoordinate quantiles, and\npseudo-random vectors can be computed efficiently, either in the absence or\npresence of variable truncation.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 09:02:15 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Grayling", "Michael", ""], ["Mander", "Adrian", ""]]}, {"id": "1711.10262", "submitter": "Peter Green", "authors": "Peter J. Diggle, Peter J. Green and Bernard W. Silverman", "title": "Julian Ernst Besag, 26 March 1945 -- 6 August 2010, a biographical\n  memoir", "comments": "26 pages, 14 figures; minor revisions, omission of full bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Julian Besag was an outstanding statistical scientist, distinguished for his\npioneering work on the statistical theory and analysis of spatial processes,\nespecially conditional lattice systems. His work has been seminal in\nstatistical developments over the last several decades ranging from image\nanalysis to Markov chain Monte Carlo methods. He clarified the role of\nauto-logistic and auto-normal models as instances of Markov random fields and\npaved the way for their use in diverse applications. Later work included\ninvestigations into the efficacy of nearest neighbour models to accommodate\nspatial dependence in the analysis of data from agricultural field trials,\nimage restoration from noisy data, and texture generation using lattice models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 12:54:53 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 16:54:37 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Diggle", "Peter J.", ""], ["Green", "Peter J.", ""], ["Silverman", "Bernard W.", ""]]}, {"id": "1711.10411", "submitter": "Yang Feng", "authors": "Yang Feng, Yichao Wu, Leonard Stefanski", "title": "Nonparametric Independence Screening via Favored Smoothing Bandwidth", "comments": "22 pages", "journal-ref": "Journal of Statistical Planning and Inference Volume 197, December\n  2018, Pages 1-14", "doi": "10.1016/j.jspi.2017.11.006", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible nonparametric regression method for\nultrahigh-dimensional data. As a first step, we propose a fast screening method\nbased on the favored smoothing bandwidth of the marginal local constant\nregression. Then, an iterative procedure is developed to recover both the\nimportant covariates and the regression function. Theoretically, we prove that\nthe favored smoothing bandwidth based screening possesses the model selection\nconsistency property. Simulation studies as well as real data analysis show the\ncompetitive performance of the new procedure.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 17:11:20 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Feng", "Yang", ""], ["Wu", "Yichao", ""], ["Stefanski", "Leonard", ""]]}, {"id": "1711.10635", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Jacob Bien", "title": "Valid Inference Corrected for Outlier Removal", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least square (OLS) estimation of a linear regression model is\nwell-known to be highly sensitive to outliers. It is common practice to (1)\nidentify and remove outliers by looking at the data and (2) to fit OLS and form\nconfidence intervals and p-values on the remaining data as if this were the\noriginal data collected. This standard \"detect-and-forget\" approach has been\nshown to be problematic, and in this paper we highlight the fact that it can\nlead to invalid inference and show how recently developed tools in selective\ninference can be used to properly account for outlier detection and removal.\nOur inferential procedures apply to a general class of outlier removal\nprocedures that includes several of the most commonly used approaches. We\nconduct simulations to corroborate the theoretical results, and we apply our\nmethod to three real data sets to illustrate how our inferential results can\ndiffer from the traditional detect-and-forget strategy. A companion R package,\noutference, implements these new procedures with an interface that matches the\nfunctions commonly used for inference with lm in R.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:18:56 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 01:13:13 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 04:41:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chen", "Shuxiao", ""], ["Bien", "Jacob", ""]]}, {"id": "1711.10765", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Learning nonlinear state-space models using smooth particle-filter-based\n  likelihood approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When classical particle filtering algorithms are used for maximum likelihood\nparameter estimation in nonlinear state-space models, a key challenge is that\nestimates of the likelihood function and its derivatives are inherently noisy.\nThe key idea in this paper is to run a particle filter based on a current\nparameter estimate, but then use the output from this particle filter to\nre-evaluate the likelihood function approximation also for other parameter\nvalues. This results in a (local) deterministic approximation of the likelihood\nand any standard optimization routine can be applied to find the maximum of\nthis local approximation. By iterating this procedure we eventually arrive at a\nfinal parameter estimate.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 10:44:24 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Svensson", "Andreas", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1711.11190", "submitter": "Paul McNicholas", "authors": "Anjali Silva, Steven J. Rothstein, Paul D. McNicholas and Sanjeena\n  Subedi", "title": "A Multivariate Poisson-Log Normal Mixture Model for Clustering\n  Transcriptome Sequencing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data of discrete and skewed nature is commonly encountered\nin high-throughput sequencing studies. Analyzing the network itself or the\ninterplay between genes in this type of data continues to present many\nchallenges. As data visualization techniques become cumbersome for higher\ndimensions and unconvincing when there is no clear separation between\nhomogeneous subgroups within the data, cluster analysis provides an intuitive\nalternative. The aim of applying mixture model-based clustering in this context\nis to discover groups of co-expressed genes, which can shed light on biological\nfunctions and pathways of gene products. A mixture of multivariate Poisson-Log\nNormal (MPLN) model is proposed for clustering of high-throughput transcriptome\nsequencing data. The MPLN model is able to fit a wide range of correlation and\noverdispersion situations, and is ideal for modeling multivariate count data\nfrom RNA sequencing studies. Parameter estimation is carried out via a Markov\nchain Monte Carlo expectation-maximization algorithm (MCMC-EM), and information\ncriteria are used for model selection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 02:04:15 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Silva", "Anjali", ""], ["Rothstein", "Steven J.", ""], ["McNicholas", "Paul D.", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "1711.11220", "submitter": "Jue Wang", "authors": "Ery Arias-Castro and Jue Wang", "title": "RANSAC Algorithms for Subspace Recovery and Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the RANSAC algorithm in the context of subspace recovery and\nsubspace clustering. We derive some theory and perform some numerical\nexperiments. We also draw some correspondences with the methods of Hardt and\nMoitra (2013) and Chen and Lerman (2009b).\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:29:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Wang", "Jue", ""]]}]