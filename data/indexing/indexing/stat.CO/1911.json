[{"id": "1911.00192", "submitter": "Xun Shen", "authors": "Xun Shen, Jiancang Zhuang and Xingguo Zhang", "title": "Parallel Randomized Algorithm for Chance Constrained Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chance constrained program is computationally intractable due to the\nexistence of chance constraints, which are randomly disturbed and should be\nsatisfied with a probability. This paper proposes a two-layer randomized\nalgorithm to address chance constrained program. Randomized optimization is\napplied to search the optimizer which satisfies chance constraints in a\nframework of parallel algorithm. Firstly, multiple decision samples are\nextracted uniformly in the decision domain without considering the chance\nconstraints. Then, in the second sampling layer, violation probabilities of all\nthe extracted decision samples are checked by extracting the disturbance\nsamples and calculating the corresponding violation probabilities. The decision\nsamples with violation probabilities higher than the required level are\ndiscarded. The minimizer of the cost function among the remained feasible\ndecision samples are used to update optimizer iteratively. Numerical\nsimulations are implemented to validate the proposed method for non-convex\nproblems comparing with scenario approach. The proposed method exhibits better\nrobustness in finding probabilistic feasible optimizer.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 03:24:29 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 02:02:20 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 00:56:03 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Shen", "Xun", ""], ["Zhuang", "Jiancang", ""], ["Zhang", "Xingguo", ""]]}, {"id": "1911.00198", "submitter": "Longhai Li", "authors": "Tingxuan Wu, Cindy Feng, Longhai Li", "title": "Residual Analysis for Censored Regression via Randomized Survival\n  Probabilities", "comments": "33 pages. Revised with reviewers' comments", "journal-ref": "Statistics in Medicine, 2021, Volume40, Issue6, Pages 1482-1497", "doi": "10.1002/sim.8852", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Residual analysis is extremely important in regression modelling. Residuals\nare used to graphically and numerically check the overall goodness-of-fit of a\nmodel, to discover the direction for improving the model, and to identify\noutlier observations. Cox-Snell residuals, which are transformed from survival\nprobabilities (SPs), are typically used for checking survival regression models\nfor failure times. Survival probabilities are uniformly distributed under the\ntrue model when there is no censored failure time. However, the SPs for\ncensored failure times are no longer uniformly distributed. Several non-random\nmethods have been proposed to modify CS residuals or SPs in the literature.\nHowever, their sampling distributions under the true model are not\ncharacterized, resulting in a lack of reference distributions for analysis with\nthese modified residuals. In this paper, we propose to use randomized survival\nprobabilities (RSP) to define residuals for censored data. We will show that\nRSPs always have the uniform distribution under the true model even with\ncensored times. Therefore, they can be transformed into residuals with the\nnormal quantile function. We call such residuals by normally-transformed RSP\n(NRSP) residuals. We conduct extensive simulation studies to demonstrate that\nNRSP residuals are normally distributed when the fitted model is correctly\nspecified. Consequently, the GOF test method by applying Shapiro-Wilk normality\ntest to NRSP residuals (NRSP-SW) is well-calibrated. Our simulation studies\nalso show the great power of the NRSP-SW method in detecting three kinds of\nmodel discrepancies. We also demonstrate the effectiveness of NRSP residuals in\nassessing three AFT models for a breast-cancer recurrent-free failure times\ndataset.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 04:27:59 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 15:50:35 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2020 16:24:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Wu", "Tingxuan", ""], ["Feng", "Cindy", ""], ["Li", "Longhai", ""]]}, {"id": "1911.00223", "submitter": "Huanbiao Zhu", "authors": "Huanbiao Zhu and Werner Stuetzle", "title": "A Simple and Efficient Method to Compute a Single Linkage Dendrogram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of computing a single linkage dendrogram. A possible\napproach is to: (i) Form an edge weighted graph $G$ over the data, with edge\nweights reflecting dissimilarities. (ii) Calculate the MST $T$ of $G$. (iii)\nBreak the longest edge of $T$ thereby splitting it into subtrees $T_L$, $T_R$.\n(iv) Apply the splitting process recursively to the subtrees. This approach has\nthe attractive feature that Prim's algorithm for MST construction calculates\ndistances as needed, and hence there is no need to ever store the inter-point\ndistance matrix. The recursive partitioning algorithm requires us to determine\nthe vertices (and edges) of $T_L$ and $T_R$. We show how this can be done\neasily and efficiently using information generated by Prim's algorithm without\nany additional computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 06:36:51 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Zhu", "Huanbiao", ""], ["Stuetzle", "Werner", ""]]}, {"id": "1911.00294", "submitter": "Adam Foster", "authors": "Adam Foster, Martin Jankowiak, Matthew O'Meara, Yee Whye Teh, Tom\n  Rainforth", "title": "A Unified Stochastic Gradient Approach to Designing Bayesian-Optimal\n  Experiments", "comments": "Published as a conference paper at AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fully stochastic gradient based approach to Bayesian optimal\nexperimental design (BOED). Our approach utilizes variational lower bounds on\nthe expected information gain (EIG) of an experiment that can be simultaneously\noptimized with respect to both the variational and design parameters. This\nallows the design process to be carried out through a single unified stochastic\ngradient ascent procedure, in contrast to existing approaches that typically\nconstruct a pointwise EIG estimator, before passing this estimator to a\nseparate optimizer. We provide a number of different variational objectives\nincluding the novel adaptive contrastive estimation (ACE) bound. Finally, we\nshow that our gradient-based approaches are able to provide effective design\noptimization in substantially higher dimensional settings than existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 10:45:12 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 15:16:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Foster", "Adam", ""], ["Jankowiak", "Martin", ""], ["O'Meara", "Matthew", ""], ["Teh", "Yee Whye", ""], ["Rainforth", "Tom", ""]]}, {"id": "1911.00469", "submitter": "Stefan B\\\"ohringer", "authors": "Stefan B\\\"ohringer and Dietmar Lohmann", "title": "Exact model comparisons in the plausibility framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plausibility is a formalization of exact tests for parametric models and\ngeneralizes procedures such as Fisher's exact test. The resulting tests are\nbased on cumulative probabilities of the probability density function and\nevaluate consistency with a parametric family while providing exact control of\nthe $\\alpha$ level for finite sample size. Model comparisons are inefficient in\nthis approach. We generalize plausibility by incorporating weighing which\nallows to perform model comparisons. We show that one weighing scheme is\nasymptotically equivalent to the likelihood ratio test (LRT) and has finite\nsample guarantees for the test size under the null hypothesis unlike the LRT.\nWe confirm theoretical properties in simulations that mimic the data set of our\ndata application. We apply the method to a retinoblastoma data set and\ndemonstrate a parent-of-origin effect. Weighted plausibility also has\napplications in high-dimensional data analysis and P-values for penalized\nregression models can be derived. We demonstrate superior performance as\ncompared to a data-splitting procedure in a simulation study. We apply weighted\nplausibility to a high-dimensional gene expression, case-control prostate\ncancer data set. We discuss the flexibility of the approach by relating\nweighted plausibility to targeted learning, the bootstrap, and sparsity\nselection.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2019 17:19:33 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 17:03:07 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["B\u00f6hringer", "Stefan", ""], ["Lohmann", "Dietmar", ""]]}, {"id": "1911.00619", "submitter": "Siddhant Wahal", "authors": "Siddhant Wahal and George Biros", "title": "BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented\n  uncertainty quantification. Part I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating rare event probabilities, focusing on\nsystems whose evolution is governed by differential equations with uncertain\ninput parameters. If the system dynamics is expensive to compute, standard\nsampling algorithms such as the Monte Carlo method may require infeasible\nrunning times to accurately evaluate these probabilities. We propose an\nimportance sampling scheme (which we call BIMC) that relies on solving an\nauxiliary, fictitious Bayesian inverse problem. The solution of the inverse\nproblem yields a posterior PDF, a local Gaussian approximation to which serves\nas the importance sampling density. We apply BIMC to several problems and\ndemonstrate that it can lead to computational savings of several orders of\nmagnitude over the Monte Carlo method. We delineate conditions under which BIMC\nis optimal, as well as conditions when it can fail to yield an effective IS\ndensity.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 00:27:13 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wahal", "Siddhant", ""], ["Biros", "George", ""]]}, {"id": "1911.00648", "submitter": "Dennis Sun", "authors": "Alex Boyd and Dennis L. Sun", "title": "salmon: A Symbolic Linear Regression Package for Python", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most attractive features of R is its linear modeling capabilities.\nWe describe a Python package, salmon, that brings the best of R's linear\nmodeling functionality to Python in a Pythonic way---by providing composable\nobjects for specifying and fitting linear models. This object-oriented design\nalso enables other features that enhance ease-of-use, such as automatic\nvisualizations and intelligent model building.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 04:42:28 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Boyd", "Alex", ""], ["Sun", "Dennis L.", ""]]}, {"id": "1911.00685", "submitter": "Shengxin Zhu", "authors": "Shengxin Zhu and Andrew J Wathen", "title": "Sparse inversion for derivative of log determinant", "comments": "15", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for Gaussian process, marginal likelihood methods or restricted\nmaximum likelihood methods often require derivatives of log determinant terms.\nThese log determinants are usually parametric with variance parameters of the\nunderlying statistical models. This paper demonstrates that, when the\nunderlying matrix is sparse, how to take the advantage of sparse\ninversion---selected inversion which share the same sparsity as the original\nmatrix---to accelerate evaluating the derivative of log determinant.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 09:22:38 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Zhu", "Shengxin", ""], ["Wathen", "Andrew J", ""]]}, {"id": "1911.00757", "submitter": "Komlan Atitey", "authors": "Komlan Atitey, Pavel Loskot and Lyudmila Mihaylova", "title": "Variational Bayesian inference of hidden stochastic processes with\n  unknown parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating hidden processes from non-linear noisy observations is\nparticularly difficult when the parameters of these processes are not known.\nThis paper adopts a machine learning approach to devise variational Bayesian\ninference for such scenarios. In particular, a random process generated by the\nautoregressive moving average (ARMA) linear model is inferred from\nnon-linearity noise observations. The posterior distribution of hidden states\nare approximated by a set of weighted particles generated by the sequential\nMonte carlo (SMC) algorithm involving sampling with importance sampling\nresampling (SISR). Numerical efficiency and estimation accuracy of the proposed\ninference method are evaluated by computer simulations. Furthermore, the\nproposed inference method is demonstrated on a practical problem of estimating\nthe missing values in the gene expression time series assuming vector\nautoregressive (VAR) data model.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 17:27:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Atitey", "Komlan", ""], ["Loskot", "Pavel", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1911.00782", "submitter": "Bao Wang", "authors": "Bao Wang, Difan Zou, Quanquan Gu, Stanley Osher", "title": "Laplacian Smoothing Stochastic Gradient Markov Chain Monte Carlo", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an important Markov Chain Monte Carlo (MCMC) method, stochastic gradient\nLangevin dynamics (SGLD) algorithm has achieved great success in Bayesian\nlearning and posterior sampling. However, SGLD typically suffers from slow\nconvergence rate due to its large variance caused by the stochastic gradient.\nIn order to alleviate these drawbacks, we leverage the recently developed\nLaplacian Smoothing (LS) technique and propose a Laplacian smoothing stochastic\ngradient Langevin dynamics (LS-SGLD) algorithm. We prove that for sampling from\nboth log-concave and non-log-concave densities, LS-SGLD achieves strictly\nsmaller discretization error in $2$-Wasserstein distance, although its mixing\nrate can be slightly slower. Experiments on both synthetic and real datasets\nverify our theoretical results, and demonstrate the superior performance of\nLS-SGLD on different machine learning tasks including posterior sampling,\nBayesian logistic regression and training Bayesian convolutional neural\nnetworks. The code is available at\n\\url{https://github.com/BaoWangMath/LS-MCMC}.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 20:32:11 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Wang", "Bao", ""], ["Zou", "Difan", ""], ["Gu", "Quanquan", ""], ["Osher", "Stanley", ""]]}, {"id": "1911.00797", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio, Roger S. Bivand, H{\\aa}vard Rue", "title": "Bayesian model averaging with the integrated nested Laplace\n  approximation", "comments": "Submitted to Econometrics (MDPI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated nested Laplace approximation (INLA) for Bayesian inference is\nan efficient approach to estimate the posterior marginal distributions of the\nparameters and latent effects of Bayesian hierarchical models that can be\nexpressed as latent Gaussian Markov random fields (GMRF). The representation as\na GMRF allows the associated software R-INLA to estimate the posterior\nmarginals in a fraction of the time as typical Markov chain Monte Carlo\nalgorithms. INLA can be extended by means of Bayesian model averaging (BMA) to\nincrease the number of models that it can fit to conditional latent GMRF. In\nthis paper we review the use of BMA with INLA and propose a new example on\nspatial econometrics models.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 23:46:07 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Bivand", "Roger S.", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1911.00878", "submitter": "Jagath Senarathne", "authors": "S. G. Jagath Senarathne, Antony M. Overstall, James M. McGree", "title": "Bayesian adaptive N-of-1 trials for estimating population and individual\n  treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a novel adaptive design algorithm that can be used to\nfind optimal treatment allocations in N-of-1 clinical trials. This new\nmethodology uses two Laplace approximations to provide a computationally\nefficient estimate of population and individual random effects within a\nrepeated measures, adaptive design framework. Given the efficiency of this\napproach, it is also adopted for treatment selection to target the collection\nof data for the precise estimation of treatment effects. To evaluate this\napproach, we consider both a simulated and motivating N-of-1 clinical trial\nfrom the literature. For each trial, our methods were compared to the\nmulti-armed bandit approach and a randomised N-of-1 trial design in terms of\nidentifying the best treatment for each patient and the information gained\nabout the model parameters. The results show that our new approach selects\ndesigns that are highly efficient in achieving each of these objectives. As\nsuch, we propose our Laplace-based algorithm as an efficient approach for\ndesigning adaptive N-of-1 trials.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 12:53:41 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 02:27:53 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2020 03:59:33 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Senarathne", "S. G. Jagath", ""], ["Overstall", "Antony M.", ""], ["McGree", "James M.", ""]]}, {"id": "1911.00915", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty, Suman K. Bhattacharya and Kshitij Khare", "title": "Estimating accuracy of the MCMC variance estimator: a central limit\n  theorem for batch means estimators", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The batch means estimator of the MCMC variance is a simple and effective\nmeasure of accuracy for MCMC based ergodic averages. Under various regularity\nconditions, the estimator has been shown to be consistent for the true\nvariance. However, the estimator can be unstable in practice as it depends\ndirectly on the raw MCMC output. A measure of accuracy of the batch means\nestimator itself, ideally in the form of a confidence interval, is therefore\ndesirable. The asymptotic variance of the batch means estimator is known;\nhowever, without any knowledge of asymptotic distribution, asymptotic variances\nare in general insufficient to describe variability. In this article we prove a\ncentral limit theorem for the batch means estimator that allows for the\nconstruction of asymptotically accurate confidence intervals for the batch\nmeans estimator. Additionally, our results provide a Markov chain analogue of\nthe classical CLT for the sample variance parameter for i.i.d. observations.\nOur result assumes standard regularity conditions similar to the ones assumed\nin the literature for proving consistency. Simulated and real data examples are\nincluded as illustrations and applications of the CLT.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2019 15:51:44 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Bhattacharya", "Suman K.", ""], ["Khare", "Kshitij", ""]]}, {"id": "1911.00995", "submitter": "Nicholas James", "authors": "Nick James, Max Menzies, Lamiae Azizi, Jennifer Chan", "title": "Novel semi-metrics for multivariate change point analysis and anomaly\n  detection", "comments": "Accepted manuscript. Minor edits since v2. Equal contribution from\n  first two authors", "journal-ref": "Physica D: Nonlinear Phenomena 412 (2020) 132636", "doi": "10.1016/j.physd.2020.132636", "report-no": null, "categories": "cs.LG math.DS stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for determining similarity and anomalies\nbetween time series, most practically effective in large collections of (likely\nrelated) time series, by measuring distances between structural breaks within\nsuch a collection. We introduce a class of \\emph{semi-metric} distance\nmeasures, which we term \\emph{MJ distances}. These semi-metrics provide an\nadvantage over existing options such as the Hausdorff and Wasserstein metrics.\nWe prove they have desirable properties, including better sensitivity to\noutliers, while experiments on simulated data demonstrate that they uncover\nsimilarity within collections of time series more effectively. Semi-metrics\ncarry a potential disadvantage: without the triangle inequality, they may not\nsatisfy a \"transitivity property of closeness.\" We analyse this failure with\nproof and introduce an computational method to investigate, in which we\ndemonstrate that our semi-metrics violate transitivity infrequently and mildly.\nFinally, we apply our methods to cryptocurrency and measles data, introducing a\njudicious application of eigenvalue analysis.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 00:04:30 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 14:58:46 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 10:44:42 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""], ["Azizi", "Lamiae", ""], ["Chan", "Jennifer", ""]]}, {"id": "1911.01018", "submitter": "Chao Gao", "authors": "Chao Gao and Anderson Y. Zhang", "title": "Iterative Algorithm for Discrete Structure Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general modeling and algorithmic framework for discrete\nstructure recovery that can be applied to a wide range of problems. Under this\nframework, we are able to study the recovery of clustering labels, ranks of\nplayers, signs of regression coefficients, cyclic shifts, and even group\nelements from a unified perspective. A simple iterative algorithm is proposed\nfor discrete structure recovery, which generalizes methods including Lloyd's\nalgorithm and the power method. A linear convergence result for the proposed\nalgorithm is established in this paper under appropriate abstract conditions on\nstochastic errors and initialization. We illustrate our general theory by\napplying it on several representative problems: (1) clustering in Gaussian\nmixture model, (2) approximate ranking, (3) sign recovery in compressed\nsensing, (4) multireference alignment, and (5) group synchronization, and show\nthat minimax rate is achieved in each case.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 03:11:10 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 05:49:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "1911.01208", "submitter": "Alon Kipnis", "authors": "Alon Kipnis", "title": "Higher Criticism for Discriminating Word-Frequency Tables and Testing\n  Authorship", "comments": "under review (AOAS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the Higher Criticism (HC) goodness-of-fit test to measure closeness\nbetween word-frequency tables. We apply this measure to authorship attribution\nchallenges, where the goal is to identify the author of a document using other\ndocuments whose authorship is known. The method is simple yet performs well\nwithout handcrafting and tuning; reporting accuracy at the state of the art\nlevel in various current challenges. As an inherent side effect, the HC\ncalculation identifies a subset of discriminating words. In practice, the\nidentified words have low variance across documents belonging to a corpus of\nhomogeneous authorship. We conclude that in comparing the similarity of a new\ndocument and a corpus of a single author, HC is mostly affected by words\ncharacteristic of the author and is relatively unaffected by topic structure.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2019 23:47:25 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 19:41:44 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2020 20:04:32 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Kipnis", "Alon", ""]]}, {"id": "1911.01268", "submitter": "Siddhant Wahal", "authors": "Siddhant Wahal and George Biros", "title": "BIMC: The Bayesian Inverse Monte Carlo method for goal-oriented\n  uncertainty quantification. Part II", "comments": "Added link to Part I in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Part I (arXiv:1911.00619) of this article, we proposed an importance\nsampling algorithm to compute rare-event probabilities in forward uncertainty\nquantification problems. The algorithm, which we termed the \"Bayesian Inverse\nMonte Carlo (BIMC) method\", was shown to be optimal for problems in which the\ninput-output operator is nearly linear. But applying the original BIMC to\nhighly nonlinear systems can lead to several different failure modes. In this\npaper, we modify the BIMC method to extend its applicability to a wider class\nof systems. The modified algorithm, which we call \"Adaptive-BIMC (A-BIMC)\", has\ntwo stages. In the first stage, we solve a sequence of optimization problems to\nroughly identify those regions of parameter space which trigger the rare-event.\nIn the second stage, we use the stage one results to construct a mixture of\nGaussians that can be then used in an importance sampling algorithm to estimate\nrare event probability. We propose using a local surrogate that minimizes\ncostly forward solves. The effectiveness of A-BIMC is demonstrated via several\nsynthetic examples. Yet again, the modified algorithm is prone to failure. We\nsystematically identify conditions under which it fails to lead to an effective\nimportance sampling distribution.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 15:03:10 GMT"}, {"version": "v2", "created": "Tue, 5 Nov 2019 14:16:58 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Wahal", "Siddhant", ""], ["Biros", "George", ""]]}, {"id": "1911.01340", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon and Arnaud Doucet", "title": "Non-reversible jump algorithms for Bayesian nested model selection", "comments": "To appear in Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-reversible Markov chain Monte Carlo methods often outperform their\nreversible counterparts in terms of asymptotic variance of ergodic averages and\nmixing properties. Lifting the state-space (Chen et al., 1999; Diaconis et al.,\n2000) is a generic technique for constructing such samplers. The idea is to\nthink of the random variables we want to generate as position variables and to\nassociate to them direction variables so as to design Markov chains which do\nnot have the diffusive behaviour often exhibited by reversible schemes. In this\npaper, we explore the benefits of using such ideas in the context of Bayesian\nmodel choice for nested models, a class of models for which the model indicator\nvariable is an ordinal random variable. By lifting this model indicator\nvariable, we obtain non-reversible jump algorithms, a non-reversible version of\nthe popular reversible jump algorithms introduced by Green (1995). This simple\nalgorithmic modification provides samplers which can empirically outperform\ntheir reversible counterparts at no extra computational cost. The code to\nreproduce all experiments is available online.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 17:05:18 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 16:44:23 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 01:26:00 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Gagnon", "Philippe", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1911.01373", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias, Petros Dellaportas", "title": "Gradient-based Adaptive Markov Chain Monte Carlo", "comments": "17 pages, 7 Figures, NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a gradient-based learning method to automatically adapt Markov\nchain Monte Carlo (MCMC) proposal distributions to intractable targets. We\ndefine a maximum entropy regularised objective function, referred to as\ngeneralised speed measure, which can be robustly optimised over the parameters\nof the proposal distribution by applying stochastic gradient optimisation. An\nadvantage of our method compared to traditional adaptive MCMC methods is that\nthe adaptation occurs even when candidate state values are rejected. This is a\nhighly desirable property of any adaptation strategy because the adaptation\nstarts in early iterations even if the initial proposal distribution is far\nfrom optimum. We apply the framework for learning multivariate random walk\nMetropolis and Metropolis-adjusted Langevin proposals with full covariance\nmatrices, and provide empirical evidence that our method can outperform other\nMCMC algorithms, including Hamiltonian Monte Carlo schemes.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:03:06 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 15:00:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Dellaportas", "Petros", ""]]}, {"id": "1911.01383", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Joaqu\\'in M\\'iguez, and Petar M. Djuri\\'c", "title": "On the performance of particle filters with adaptive number of particles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of a class of particle filters (PFs) that can\nautomatically tune their computational complexity by evaluating online certain\npredictive statistics which are invariant for a broad class of state-space\nmodels. To be specific, we propose a family of block-adaptive PFs based on the\nmethodology of Elvira et al (2017). In this class of algorithms, the number of\nMonte Carlo samples (known as particles) is adjusted periodically, and we prove\nthat the theoretical error bounds of the PF actually adapt to the updates in\nthe number of particles. The evaluation of the predictive statistics that lies\nat the core of the methodology is done by generating fictitious observations,\ni.e., particles in the observation space. We study, both analytically and\nnumerically, the impact of the number $K$ of these particles on the performance\nof the algorithm. In particular, we prove that if the predictive statistics\nwith $K$ fictitious observations converged exactly, then the particle\napproximation of the filtering distribution would match the first $K$ elements\nin a series of moments of the true filter. This result can be understood as a\nconverse to some convergence theorems for PFs. From this analysis, we deduce an\nalternative predictive statistic that can be computed (for some models) without\nsampling any fictitious observations at all. Finally, we conduct an extensive\nsimulation study that illustrates the theoretical results and provides further\ninsights into the complexity, performance and behavior of the new class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:10:55 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:34:23 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["M\u00edguez", "Joaqu\u00edn", ""], ["Djuri\u0107", "Petar M.", ""]]}, {"id": "1911.01414", "submitter": "Chaim Even-Zohar", "authors": "Chaim Even-Zohar, Calvin Leng", "title": "Counting Small Permutation Patterns", "comments": null, "journal-ref": "SODA 2021", "doi": null, "report-no": null, "categories": "cs.DS math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sample of n generic points in the xy-plane defines a permutation that\nrelates their ranks along the two axes. Every subset of k points similarly\ndefines a pattern, which occurs in that permutation. The number of occurrences\nof small patterns in a large permutation arises in many areas, including\nnonparametric statistics. It is therefore desirable to count them more\nefficiently than the straightforward ~O(n^k) time algorithm.\n  This work proposes new algorithms for counting patterns. We show that all\npatterns of order 2 and 3, as well as eight patterns of order 4, can be counted\nin nearly linear time. To that end, we develop an algebraic framework that we\ncall corner tree formulas. Our approach generalizes the existing methods and\nallows a systematic study of their scope.\n  Using the machinery of corner trees, we find twenty-three independent linear\ncombinations of order-4 patterns, that can be computed in time ~O(n). We also\ndescribe an algorithm that counts one of the remaining 4-patterns, and hence\nall 4-patterns, in time ~O(n^(3/2)).\n  As a practical application, we provide a nearly linear time computation of a\nstatistic by Yanagimoto (1970), Bergsma and Dassios (2010). This statistic\nyields a natural and strongly consistent variant of Hoeffding's test for\nindependence of X and Y, given a random sample as above. This improves upon the\nso far most efficient ~O(n^2) algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 18:57:04 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 18:55:43 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 18:38:43 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Even-Zohar", "Chaim", ""], ["Leng", "Calvin", ""]]}, {"id": "1911.01511", "submitter": "Mar\\'ia Magdalena  Lucini", "authors": "Mar\\'ia Magdalena Lucini and Peter Jan van Leeuwen and Manuel Pulido", "title": "Model uncertainty estimation using the expectation maximization\n  algorithm and a particle flow filter", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model error covariances play a central role in the performance of data\nassimilation methods applied to nonlinear state-space models. However, these\ncovariances are largely unknown in most of the applications. A misspecification\nof the model error covariance has a strong impact on the computation of the\nposterior probability density function, leading to unreliable estimations and\neven to a total failure of the assimilation procedure. In this work, we propose\nthe combination of the Expectation-Maximization algorithm (EM) with an\nefficient particle filter to estimate the model error covariance, using a batch\nof observations. Based on the EM algorithm principles, the proposed method\nencompasses two stages: the expectation stage, in which a particle filter is\nused with the present estimate of the model error covariance as given to find\nthe probability density function that maximizes the likelihood, followed by a\nmaximization stage in which the expectation under the probability density\nfunction found in the expectation step is maximized as a function of the\nelements of the model error covariance. This novel algorithm here presented\ncombines the EM with a fixed point algorithm and does not require a particle\nsmoother to approximate the posterior densities. We demonstrate that the new\nmethod accurately and efficiently solves the linear model problem. Furthermore,\nfor the chaotic nonlinear Lorenz-96 model the method is stable even for\nobservation error covariance 10 times larger than the estimated model error\ncovariance matrix, and also that it is successful in high-dimensional\nsituations where the dimension of the estimated matrix is 1600.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 22:16:01 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Lucini", "Mar\u00eda Magdalena", ""], ["van Leeuwen", "Peter Jan", ""], ["Pulido", "Manuel", ""]]}, {"id": "1911.01525", "submitter": "Wei Han", "authors": "Wei Han, Yun Yang", "title": "Statistical Inference in Mean-Field Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct non-asymptotic analysis on the mean-field variational inference\nfor approximating posterior distributions in complex Bayesian models that may\ninvolve latent variables. We show that the mean-field approximation to the\nposterior can be well-approximated relative to the Kullback-Leibler divergence\ndiscrepancy measure by a normal distribution whose center is the maximum\nlikelihood estimator (MLE). In particular, our results imply that the center of\nthe mean-field approximation matches the MLE up to higher-order terms and there\nis essentially no loss of efficiency in using it as a point estimator for the\nparameter in any regular parametric model with latent variables. We also\npropose a new class of variational weighted likelihood bootstrap (VWLB) methods\nfor quantifying the uncertainty in the mean-field variational inference. The\nproposed VWLB can be viewed as a new sampling scheme that produces independent\nsamples for approximating the posterior. Comparing with traditional sampling\nalgorithms such Markov Chain Monte Carlo, VWLB can be implemented in parallel\nand is free of tuning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 23:08:49 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Han", "Wei", ""], ["Yang", "Yun", ""]]}, {"id": "1911.01815", "submitter": "Leonardo Egidi PhD", "authors": "Leonardo Egidi and Ioannis Ntzoufras", "title": "A Bayesian Quest for Finding a Unified Model for Predicting Volleyball\n  Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volleyball is a team sport with unique and specific characteristics. We\nintroduce a new two level-hierarchical Bayesian model which accounts for theses\nvolleyball specific characteristics. In the first level, we model the set\noutcome with a simple logistic regression model. Conditionally on the winner of\nthe set, in the second level, we use a truncated negative binomial distribution\nfor the points earned by the loosing team. An additional Poisson distributed\ninflation component is introduced to model the extra points played in the case\nthat the two teams have point difference less than two points. The number of\npoints of the winner within each set is deterministically specified by the\nwinner of the set and the points of the inflation component. The team specific\nabilities and the home effect are used as covariates on all layers of the model\n(set, point, and extra inflated points). The implementation of the proposed\nmodel on the Italian Superlega 2017/2018 data shows an exceptional\nreproducibility of the final league table and a satisfactory predictive\nability.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 14:39:54 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 09:20:20 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Egidi", "Leonardo", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1911.02089", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon", "title": "Informed reversible jump algorithms", "comments": "Major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating information about the target distribution in proposal\nmechanisms generally increases the efficiency of Markov chain Monte Carlo\nalgorithms, when compared with those based on naive random walks. Hamiltonian\nMonte Carlo represents a successful example of fixed-dimensional algorithms\nincorporating gradient information. In trans-dimensional algorithms, Green\n(2003) recommended to generate the parameter proposals during model switches\nfrom normal distributions with informative means and covariance matrices. These\nproposal distributions can be viewed as asymptotic approximations to the\nparameter distributions, where the limit is with regard to the sample size.\nModels are typically proposed using uninformed uniform distributions. In this\npaper, we build on the approach of Zanella (2020) for discrete spaces to\nincorporate information about neighbouring models. We rely on approximations to\nposterior model probabilities that are asymptotically exact. We prove that, in\nsome scenarios, the samplers combining this approach with that of Green (2003)\nbehave like those that use the exact model probabilities and sample from the\nparameter distributions, in the large sample regime. We show that the\nimplementation of the proposed samplers is straightforward in some cases. The\nmethodology is applied to a real-data example. The code is available online.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 21:16:11 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 02:51:19 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Gagnon", "Philippe", ""]]}, {"id": "1911.02261", "submitter": "Damiano Rossello", "authors": "Christos E. Kountzakis, Damiano Rossello", "title": "Acceptability Indices of Performance for Bounded C\\`adl\\`ag Processes", "comments": "22 pages, 6 Figures, 2 Tables, 2 Appendixes", "journal-ref": null, "doi": "10.1080/17442508.2019.1687705", "report-no": null, "categories": "q-fin.MF q-fin.RM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indices of acceptability are well suited to frame the axiomatic features of\nmany performance measures, associated to terminal random cash flows.We extend\nthis notion to classes of c\\`adl\\`ag processes modelling cash flows over a\nfixed investment horizon.We provide a representation result for bounded paths.\nWe suggest an acceptability index based both on the static Average\nValue-at-Risk functional and the running minimum of the paths, which eventually\nrepresents a RAROC-type model. Some numerical comparisons clarify the magnitude\nof performance evaluation for processes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 09:14:11 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kountzakis", "Christos E.", ""], ["Rossello", "Damiano", ""]]}, {"id": "1911.02397", "submitter": "Sevag Kevork", "authors": "Sevag Kevork and G\\\"oran Kauermann", "title": "Iterative Estimation of Mixed Exponential Random Graph Models with Nodal\n  Random Effects", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of unobserved node specific heterogeneity in Exponential Random\nGraph Models (ERGM) is a general concern, both with respect to model validity\nas well as estimation instability. We therefore extend the ERGM by including\nnode specific random effects that account for unobserved heterogeneity in the\nnetwork. This leads to a mixed model with parametric as well as random\ncoefficients, labelled as mixed ERGM. Estimation is carried out by combining\napproximate penalized pseudolikelihood estimation for the random effects with\nmaximum likelihood estimation for the remaining parameters in the model. This\napproach provides a stable algorithm, which allows to fit nodal heterogeneity\neffects even for large scale networks. We also propose model selection based on\nthe AIC to check for node specific heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 14:00:43 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Kevork", "Sevag", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1911.02551", "submitter": "Tal Galili", "authors": "Tal Galili, Alan OCallaghan, Jonathan Sidi, Carson Sievert", "title": "heatmaply: an R package for creating interactive cluster heatmaps for\n  online publishing", "comments": "3 pages", "journal-ref": "Bioinformatics 34.9 (2017): 1600-1602", "doi": "10.1093/bioinformatics/btx657", "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Summary: heatmaply is an R package for easily creating interactive cluster\nheatmaps that can be shared online as a stand-alone HTML file. Interactivity\nincludes a tooltip display of values when hovering over cells, as well as the\nability to zoom in to specific sections of the figure from the data matrix, the\nside dendrograms, or annotated labels. Thanks to the synergistic relationship\nbetween heatmaply and other R packages, the user is empowered by a refined\ncontrol over the statistical and visual aspects of the heatmap layout.\n  Availability and implementation: The heatmaply package is available under the\nGPL-2 Open Source license. It comes with a detailed vignette, and is freely\navailable from: http://cran.r-project.org/package=heatmaply.\n  Supplementary information: Supplementary data are available at Bioinformatics\nonline.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2019 13:33:44 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Galili", "Tal", ""], ["OCallaghan", "Alan", ""], ["Sidi", "Jonathan", ""], ["Sievert", "Carson", ""]]}, {"id": "1911.02629", "submitter": "Peter Marcy", "authors": "Peter W. Marcy, Scott A. Vander Wiel, Curtis B. Storlie, Veronica\n  Livescu, Curt A. Bronkhorst", "title": "Modeling Material Stress Using Integrated Gaussian Markov Random Fields", "comments": null, "journal-ref": null, "doi": "10.1080/02664763.2019.1686131", "report-no": "LA-UR-16-28920", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The equations of a physical constitutive model for material stress within\ntantalum grains were solved numerically using a tetrahedrally meshed volume.\nThe resulting output included a scalar vonMises stress for each of the more\nthan 94,000 tetrahedra within the finite element discretization. In this paper,\nwe define an intricate statistical model for the spatial field of vonMises\nstress which uses the given grain geometry in a fundamental way. Our model\nrelates the three-dimensional field to integrals of latent stochastic processes\ndefined on the vertices of the one- and two-dimensional grain boundaries. An\nintuitive neighborhood structure of said boundary nodes suggested the use of a\nlatent Gaussian Markov random field (GMRF). However, despite the potential for\ncomputational gains afforded by GMRFs, the integral nature of our model and the\nsheer number of data points pose substantial challenges for a full Bayesian\nanalysis. To overcome these problems and encourage efficient exploration of the\nposterior distribution, a number of techniques are now combined: parallel\ncomputing, sparse matrix methods, and a modification of a block update strategy\nwithin the sampling routine. In addition, we use an auxiliary variables\napproach to accommodate the presence of outliers in the data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 21:07:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Marcy", "Peter W.", ""], ["Wiel", "Scott A. Vander", ""], ["Storlie", "Curtis B.", ""], ["Livescu", "Veronica", ""], ["Bronkhorst", "Curt A.", ""]]}, {"id": "1911.02656", "submitter": "Karthik Bharath", "authors": "Rachel Carrington, Karthik Bharath and Simon Preston", "title": "Invariance and identifiability issues for word embeddings", "comments": "NIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are commonly obtained as optimizers of a criterion function\n$f$ of a text corpus, but assessed on word-task performance using a different\nevaluation function $g$ of the test data. We contend that a possible source of\ndisparity in performance on tasks is the incompatibility between classes of\ntransformations that leave $f$ and $g$ invariant. In particular, word\nembeddings defined by $f$ are not unique; they are defined only up to a class\nof transformations to which $f$ is invariant, and this class is larger than the\nclass to which $g$ is invariant. One implication of this is that the apparent\nsuperiority of one word embedding over another, as measured by word task\nperformance, may largely be a consequence of the arbitrary elements selected\nfrom the respective solution sets. We provide a formal treatment of the above\nidentifiability issue, present some numerical examples, and discuss possible\nresolutions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2019 22:41:04 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Carrington", "Rachel", ""], ["Bharath", "Karthik", ""], ["Preston", "Simon", ""]]}, {"id": "1911.02720", "submitter": "Eric Kawaguchi", "authors": "Eric S. Kawaguchi, Jenny I. Shen, Marc A. Suchard, and Gang Li", "title": "Scalable Algorithms for Large Competing Risks Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops two orthogonal contributions to scalable sparse\nregression for competing risks time-to-event data. First, we study and\naccelerate the broken adaptive ridge method (BAR), an $\\ell_0$-based\niteratively reweighted $\\ell_2$-penalization algorithm that achieves sparsity\nin its limit, in the context of the Fine-Gray (1999) proportional\nsubdistributional hazards (PSH) model. In particular, we derive a new algorithm\nfor BAR regression, named cycBAR, that performs cyclic update of each\ncoordinate using an explicit thresholding formula. The new cycBAR algorithm\neffectively avoids fitting multiple reweighted $\\ell_2$-penalizations and thus\nyields impressive speedups over the original BAR algorithm. Second, we address\na pivotal computational issue related to fitting the PSH model. Specifically,\nthe computation costs of the log-pseudo likelihood and its derivatives for PSH\nmodel grow at the rate of $O(n^2)$ with the sample size $n$ in current\nimplementations. We propose a novel forward-backward scan algorithm that\nreduces the computation costs to $O(n)$. The proposed method applies to both\nunpenalized and penalized estimation for the PSH model and has exhibited\ndrastic speedups over current implementations. Finally, combining the two\nalgorithms can yields $>1,000$ fold speedups over the original BAR algorithm.\nIllustrations of the impressive scalability of our proposed algorithm for large\ncompeting risks data are given using both simulations and a United States Renal\nData System data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 02:24:25 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Kawaguchi", "Eric S.", ""], ["Shen", "Jenny I.", ""], ["Suchard", "Marc A.", ""], ["Li", "Gang", ""]]}, {"id": "1911.02748", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak, Kisung You, Sujit K. Ghosh, Bingyue Su, Joseph Kelly", "title": "Data transforming augmentation for heteroscedastic models", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2019.1704295", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation (DA) turns seemingly intractable computational problems\ninto simple ones by augmenting latent missing data. In addition to\ncomputational simplicity, it is now well-established that DA equipped with a\ndeterministic transformation can improve the convergence speed of iterative\nalgorithms such as an EM algorithm or Gibbs sampler. In this article, we\noutline a framework for the transformation-based DA, which we call data\ntransforming augmentation (DTA), allowing augmented data to be a deterministic\nfunction of latent and observed data, and unknown parameters. Under this\nframework, we investigate a novel DTA scheme that turns heteroscedastic models\ninto homoscedastic ones to take advantage of simpler computations typically\navailable in homoscedastic cases. Applying this DTA scheme to fitting linear\nmixed models, we demonstrate simpler computations and faster convergence rates\nof resulting iterative algorithms, compared with those under a\nnon-transformation-based DA scheme. We also fit a Beta-Binomial model using the\nproposed DTA scheme, which enables sampling approximate marginal posterior\ndistributions that are available only under homoscedasticity. An R package,\nRdta, is publicly available at CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:14:01 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 02:53:11 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tak", "Hyungsuk", ""], ["You", "Kisung", ""], ["Ghosh", "Sujit K.", ""], ["Su", "Bingyue", ""], ["Kelly", "Joseph", ""]]}, {"id": "1911.02753", "submitter": "Chuanping Yu", "authors": "Chuanping Yu, Xiaoming Huo", "title": "Optimal Projections in the Distance-Based Statistical Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new way to calculate distance-based statistics,\nparticularly when the data are multivariate. The main idea is to pre-calculate\nthe optimal projection directions given the variable dimension, and to project\nmultidimensional variables onto these pre-specified projection directions; by\nsubsequently utilizing the fast algorithm that is developed in Huo and\nSz\\'ekely [2016] for the univariate variables, the computational complexity can\nbe improved from $O(m^2)$ to $O(n m \\cdot \\mbox{log}(m))$, where $n$ is the\nnumber of projection directions and $m$ is the sample size. When $n \\ll\nm/\\log(m)$, computational savings can be achieved. The key challenge is how to\nfind the optimal pre-specified projection directions. This can be obtained by\nminimizing the worse-case difference between the true distance and the\napproximated distance, which can be formulated as a nonconvex optimization\nproblem in a general setting. In this paper, we show that the exact solution of\nthe nonconvex optimization problem can be derived in two special cases: the\ndimension of the data is equal to either $2$ or the number of projection\ndirections. In the generic settings, we propose an algorithm to find some\napproximate solutions. Simulations confirm the advantage of our method, in\ncomparison with the pure Monte Carlo approach, in which the directions are\nrandomly selected rather than pre-calculated.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 04:33:27 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Yu", "Chuanping", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1911.02764", "submitter": "Jonathan Scarlett", "authors": "Jonathan Scarlett", "title": "An Efficient Algorithm for Capacity-Approaching Noisy Adaptive Group\n  Testing", "comments": "ISIT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the group testing problem with adaptive test\ndesigns and noisy outcomes. We propose a computationally efficient four-stage\nprocedure with components including random binning, identification of bins\ncontaining defective items, 1-sparse recovery via channel codes, and a\n\"clean-up\" step to correct any errors from the earlier stages. We prove that\nthe asymptotic required number of tests comes very close to the best known\ninformation-theoretic achievability bound (which is based on computationally\nintractable decoding), and approaches a capacity-based converse bound in the\nlow-sparsity regime.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2019 05:50:25 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Scarlett", "Jonathan", ""]]}, {"id": "1911.03017", "submitter": "Marius Hofert", "authors": "Erik Hintz, Marius Hofert, Christiane Lemieux", "title": "Normal variance mixtures: Distribution, density and parameter estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normal variance mixtures are a class of multivariate distributions that\ngeneralize the multivariate normal by randomizing (or mixing) the covariance\nmatrix via multiplication by a non-negative random variable W. The multivariate\nt distribution is an example of such mixture, where W has an inverse-gamma\ndistribution. Algorithms to compute the joint distribution function and perform\nparameter estimation for the multivariate normal and t (with integer degrees of\nfreedom) can be found in the literature and are implemented in, e.g., the R\npackage mvtnorm. In this paper, efficient algorithms to perform these tasks in\nthe general case of a normal variance mixture are proposed. In addition to the\nabove two tasks, the evaluation of the joint (logarithmic) density function of\na general normal variance mixture is tackled as well, as it is needed for\nparameter estimation and does not always exist in closed form in this more\ngeneral setup. For the evaluation of the joint distribution function, the\nproposed algorithms apply randomized quasi-Monte Carlo (RQMC) methods in a way\nthat improves upon existing methods proposed for the multivariate normal and t\ndistributions. An adaptive RQMC algorithm that similarly exploits the superior\nconvergence properties of RQMC methods is presented for the task of evaluating\nthe joint log-density function. This allows the parameter estimation task to be\naccomplished via an EM-like algorithm where all weights and log-densities are\nnumerically estimated. It is demonstrated through numerical examples that the\nsuggested algorithms are quite fast; even for high dimensions around 1000 the\ndistribution function can be estimated with moderate accuracy using only a few\nseconds of run time. Even log-densities around -100 can be estimated accurately\nand quickly. An implementation of all algorithms presented in this work is\navailable in the R package nvmix (version >= 0.0.4).\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2019 03:34:21 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 03:25:00 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 16:48:33 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Hintz", "Erik", ""], ["Hofert", "Marius", ""], ["Lemieux", "Christiane", ""]]}, {"id": "1911.04109", "submitter": "Yiping Hong", "authors": "Yiping Hong, Sameh Abdulah, Marc G. Genton, Ying Sun", "title": "Efficiency Assessment of Approximated Spatial Predictions for Large\n  Datasets", "comments": "43 pages + 8 pages of Supplementary Material, 8 figures, 8 tables + 8\n  tables in Supplementary Material. The Abstract is slightly abridged compared\n  to the article. Corrected the affiliation of Sameh Abdulah", "journal-ref": "Spatial Statistics, 43, 100517 (2021)", "doi": "10.1016/j.spasta.2021.100517", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the well-known computational showstopper of the exact Maximum\nLikelihood Estimation (MLE) for large geospatial observations, a variety of\napproximation methods have been proposed in the literature, which usually\nrequire tuning certain inputs. For example, the recently developed Tile\nLow-Rank approximation (TLR) method involves many tuning parameters, including\nnumerical accuracy. To properly choose the tuning parameters, it is crucial to\nadopt a meaningful criterion for the assessment of the prediction efficiency\nwith different inputs, which the most commonly-used Mean Square Prediction\nError (MSPE) criterion and the Kullback-Leibler Divergence criterion cannot\nfully describe. In this paper, we present three other criteria, the Mean Loss\nof Efficiency (MLOE), Mean Misspecification of the Mean Square Error (MMOM),\nand Root mean square MOM (RMOM), and show numerically that, in comparison with\nthe common MSPE criterion and the Kullback-Leibler Divergence criterion, our\ncriteria are more informative, and thus more adequate to assess the loss of the\nprediction efficiency by using the approximated or misspecified covariance\nmodels. Hence, our suggested criteria are more useful for the determination of\ntuning parameters for sophisticated approximation methods of spatial model\nfitting. To illustrate this, we investigate the trade-off between the execution\ntime, estimation accuracy, and prediction efficiency for the TLR method with\nextensive simulation studies and suggest proper settings of the TLR tuning\nparameters. We then apply the TLR method to a large spatial dataset of soil\nmoisture in the area of the Mississippi River basin, and compare the TLR with\nthe Gaussian predictive process and the composite likelihood method, showing\nthat our suggested criteria can successfully be used to choose the tuning\nparameters that can keep the estimation or the prediction accuracy in\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 06:39:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 15:38:13 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 13:33:03 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hong", "Yiping", ""], ["Abdulah", "Sameh", ""], ["Genton", "Marc G.", ""], ["Sun", "Ying", ""]]}, {"id": "1911.04212", "submitter": "Yasin Asar", "authors": "Yasin Asar and R. Arabi Belaghi", "title": "Estimation in Weibull Distribution Under Progressively Type-I Hybrid\n  Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the estimation of unknown parameters of Weibull\ndistribution when the lifetime data are observed in the presence of\nprogressively type-I hybrid censoring scheme. The Newton-Raphson algorithm,\nExpectation-Maximization (EM) algorithm and Stochastic EM (SEM) algorithm are\nutilized to derive the maximum likelihood estimates (MLEs) for the unknown\nparameters. Moreover, Bayesian estimators using Tierney-Kadane Method and\nMarkov Chain Monte Carlo (MCMC) method are obtained under three different loss\nfunctions, namely, squared error loss (SEL), linear-exponential (LINEX) and\ngeneralized entropy loss (GEL) functions. Also, the shrinkage pre-test\nestimators are derived. An extensive Monte Carlo simulation experiment is\nconducted under different schemes so that the performances of the listed\nestimators are compared using mean squared error, confidence interval length\nand coverage probabilities. Asymptotic normality and MCMC samples are used to\nobtain the confidence intervals and highest posterior density (HPD) intervals\nrespectively. Further, a real data example is presented to illustrate the\nmethods. Finally, some conclusive remarks are presented.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 12:32:38 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Asar", "Yasin", ""], ["Belaghi", "R. Arabi", ""]]}, {"id": "1911.04224", "submitter": "Xun Shen", "authors": "Xun Shen, Jiancang Zhuang, and Xingguo Zhang", "title": "Approximate Uncertain Program", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.00192", "journal-ref": null, "doi": "10.1109/ACCESS.2019.2958621", "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chance constrained program where one seeks to minimize an objective over\ndecisions which satisfy randomly disturbed constraints with a given probability\nis computationally intractable. This paper proposes an approximate approach to\naddress chance constrained program. Firstly, a single layer neural-network is\nused to approximate the function from decision domain to violation probability\ndomain. The algorithm for updating parameters in single layer neural-network\nadopts sequential extreme learning machine. Based on the neural violation\nprobability approximate model, a randomized algorithm is then proposed to\napproach the optimizer in the probabilistic feasible domain of decision. In the\nrandomized algorithm, samples are extracted from decision domain uniformly at\nfirst. Then, violation probabilities of all samples are calculated according to\nneural violation probability approximate model. The ones with violation\nprobability higher than the required level are discarded. The minimizer in the\nremained feasible decision samples is used to update sampling policy. The\npolicy converges to the optimal feasible decision. Numerical simulations are\nimplemented to validate the proposed method for non-convex problems comparing\nwith scenario approach and parallel randomized algorithm. The result shows that\nproposed method have improved performance.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2019 09:26:12 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 02:40:49 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Shen", "Xun", ""], ["Zhuang", "Jiancang", ""], ["Zhang", "Xingguo", ""]]}, {"id": "1911.04616", "submitter": "Ziheng Chen", "authors": "Ziheng Chen and Hongshik Ahn", "title": "Item Response Theory based Ensemble in Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel probabilistic framework to improve the\naccuracy of a weighted majority voting algorithm. In order to assign higher\nweights to the classifiers which can correctly classify hard-to-classify\ninstances, we introduce the Item Response Theory (IRT) framework to evaluate\nthe samples' difficulty and classifiers' ability simultaneously. Three models\nare created with different assumptions suitable for different cases. When\nmaking an inference, we keep a balance between the accuracy and complexity. In\nour experiment, all the base models are constructed by single trees via\nbootstrap. To explain the models, we illustrate how the IRT ensemble model\nconstructs the classifying boundary. We also compare their performance with\nother widely used methods and show that our model performs well on 19 datasets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2019 23:48:18 GMT"}], "update_date": "2019-11-13", "authors_parsed": [["Chen", "Ziheng", ""], ["Ahn", "Hongshik", ""]]}, {"id": "1911.05423", "submitter": "Roel Ceballos", "authors": "Analaine May A. Tatoy, Roel F. Ceballos", "title": "Human Immunodeficiency Virus(HIV) Cases in the Philippines: Analysis and\n  Forecasting", "comments": null, "journal-ref": "JP Journal of Biostatistics, 16(2), 67-77, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reports from the Health Department in the Philippines show that cases of\nHuman Immunodeficiency Virus (HIV) are increasing despite management and\ncontrol efforts by the government. Worldwide, the Philippines has one of the\nfastest growing number of HIV cases. The aim of the study is to analyze HIV\ncases by determining the best model in forecasting its future number of cases.\nThe data set was retrieved from National HIV/AIDS and STI Surveillance and\nStrategic Information Unit (NHSSS) of the Department of Health containing 132\nobservations. This data set was divided into two parts, one for model building\nand another for forecast evaluation. The original series has an increasing\ntrend and is nonstationary with indication of non-constant variance. Box-Cox\ntransformation and ordinary differencing were performed on the series. The\ndifferenced series is stationary and tentative models were identified through\nACF and PACF plots. SARIMA has the smallest chosen AIC value. The chosen model\nundergoes the diagnostic checking. The residuals of the model behave like a\nwhite noise while the forecast errors behave like a Gaussian white noise.\nConsidering all diagnostics, the model may be used for forecasting the monthly\ncases of HIV in the Philippines. Forecasted values show that HIV cases will\nmaintain their current trend.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2019 12:29:51 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Tatoy", "Analaine May A.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1911.05497", "submitter": "Hongtao Liu", "authors": "Hongtao Liu", "title": "Going Negative Online? -- A Study of Negative Advertising on Social\n  Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A growing number of empirical studies suggest that negative advertising is\neffective in campaigning, while the mechanisms are rarely mentioned. With the\nscandal of Cambridge Analytica and Russian intervention behind the Brexit and\nthe 2016 presidential election, people have become aware of the political ads\non social media and have pressured congress to restrict political advertising\non social media. Following the related legislation, social media companies\nbegan disclosing their political ads archive for transparency during the summer\nof 2018 when the midterm election campaign was just beginning. This research\ncollects the data of the related political ads in the context of the U.S.\nmidterm elections since August to study the overall pattern of political ads on\nsocial media and uses sets of machine learning methods to conduct sentiment\nanalysis on these ads to classify the negative ads. A novel approach is applied\nthat uses AI image recognition to study the image data. Through data\nvisualization, this research shows that negative advertising is still the\nminority, Republican advertisers and third party organizations are more likely\nto engage in negative advertising than their counterparts. Based on ordinal\nregressions, this study finds that anger evoked information-seeking is one of\nthe main mechanisms causing negative ads to be more engaging and effective\nrather than the negative bias theory. Overall, this study provides a unique\nunderstanding of political advertising on social media by applying innovative\ndata science methods. Further studies can extend the findings, methods, and\ndatasets in this study, and several suggestions are given for future research.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2019 15:43:23 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Liu", "Hongtao", ""]]}, {"id": "1911.05754", "submitter": "Arya Pourzanjani", "authors": "Arya A. Pourzanjani and Linda R. Petzold", "title": "Implicit Hamiltonian Monte Carlo for Sampling Multiscale Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) has been widely adopted in the statistics\ncommunity because of its ability to sample high-dimensional distributions much\nmore efficiently than other Metropolis-based methods. Despite this, HMC often\nperforms sub-optimally on distributions with high correlations or marginal\nvariances on multiple scales because the resulting stiffness forces the\nleapfrog integrator in HMC to take an unreasonably small stepsize. We provide\nintuition as well as a formal analysis showing how these multiscale\ndistributions limit the stepsize of leapfrog and we show how the implicit\nmidpoint method can be used, together with Newton-Krylov iteration, to\ncircumvent this limitation and achieve major efficiency gains. Furthermore, we\noffer practical guidelines for when to choose between implicit midpoint and\nleapfrog and what stepsize to use for each method, depending on the\ndistribution being sampled. Unlike previous modifications to HMC, our method is\ngenerally applicable to highly non-Gaussian distributions exhibiting multiple\nscales. We illustrate how our method can provide a dramatic speedup over\nleapfrog in the context of the No-U-Turn sampler (NUTS) applied to several\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2019 21:28:59 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2019 00:01:08 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Pourzanjani", "Arya A.", ""], ["Petzold", "Linda R.", ""]]}, {"id": "1911.06286", "submitter": "Chiheb Ben Hammouda", "authors": "Chiheb Ben Hammouda, Nadhir Ben Rached, Raul Tempone", "title": "Importance sampling for a robust and efficient multilevel Monte Carlo\n  estimator for stochastic reaction networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multilevel Monte Carlo (MLMC) method for continuous-time Markov chains,\nfirst introduced by Anderson and Higham (SIAM Multiscal Model. Simul. 10(1),\n2012), is a highly efficient simulation technique that can be used to estimate\nvarious statistical quantities for stochastic reaction networks (SRNs), in\nparticular for stochastic biological systems. Unfortunately, the robustness and\nperformance of the multilevel method can be affected by the high kurtosis, a\nphenomenon observed at the deep levels of MLMC, which leads to inaccurate\nestimates of the sample variance. In this work, we address cases where the\nhigh-kurtosis phenomenon is due to \\textit{catastrophic coupling\n(characteristic of pure jump processes where coupled consecutive paths are\nidentical in most of the simulations, while differences only appear in a tiny\nproportion) and introduce a pathwise-dependent importance sampling (IS)\ntechnique that improves the robustness and efficiency of the multilevel method.\nOur theoretical results, along with the conducted numerical experiments,\ndemonstrate that our proposed method significantly reduces the kurtosis of the\ndeep levels of MLMC, and also improves the strong convergence rate from\n$\\beta=1$ for the standard case (without IS), to $\\beta=1+\\delta$, where\n$0<\\delta<1$ is a user-selected parameter in our IS algorithm. Due to the\ncomplexity theorem of MLMC, and given a pre-selected tolerance, $\\text{TOL}$,\nthis results in an improvement of the complexity from\n$\\mathcal{O}\\left(\\text{TOL}^{-2} \\log(\\text{TOL})^2\\right)$ in the standard\ncase to $\\mathcal{O}\\left(\\text{TOL}^{-2}\\right)$, which is the optimal\ncomplexity of the MLMC estimator. We achieve all these improvements with a\nnegligible additional cost since our IS algorithm is only applied a few times\nacross each simulated path.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 18:18:55 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 16:48:10 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Hammouda", "Chiheb Ben", ""], ["Rached", "Nadhir Ben", ""], ["Tempone", "Raul", ""]]}, {"id": "1911.06416", "submitter": "Dirk Eddelbuettel", "authors": "Dirk Eddelbuettel", "title": "Thirteen Simple Steps for Creating An R Package with an External C++\n  Library", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We desribe how we extend R with an external C++ code library by using the\nRcpp package. Our working example uses the recent machine learning library and\napplication 'Corels' providing optimal yet easily interpretable rule lists\n<arXiv:1704.01701> which we bring to R in the form of the 'RcppCorels' package.\nWe discuss each step in the process, and derive a set of simple rules and\nrecommendations which are illustrated with the concrete example.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2019 23:42:35 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Eddelbuettel", "Dirk", ""]]}, {"id": "1911.06583", "submitter": "Mari Myllym\\\"aki", "authors": "Mari Myllym\\\"aki and Tom\\'a\\v{s} Mrkvi\\v{c}ka", "title": "GET: Global envelopes in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work describes the R package GET that implements global envelopes for a\ngeneral set of $d$-dimensional vectors $T$ in various applications. A\n$100(1-\\alpha)$% global envelope is a band bounded by two vectors such that the\nprobability that $T$ falls outside this envelope in any of the $d$ points is\nequal to $\\alpha$. Global means that the probability is controlled\nsimultaneously for all the $d$ elements of the vectors. The global envelopes\ncan be employed for central regions of functional or multivariate data, for\ngraphical Monte Carlo and permutation tests where the test statistic is\nmultivariate or functional, and for global confidence and prediction bands.\nIntrinsic graphical interpretation property is introduced for global envelopes,\nand the global envelopes included in the GET package that have the property are\ndescribed. Examples of different uses of global envelopes and their\nimplementation in the GET package are presented, including global envelopes for\nsingle and several one- or two-dimensional functions, Monte Carlo\ngoodness-of-fit tests for simple and composite hypotheses, comparison of\ndistributions, graphical functional analysis of variance (ANOVA), and general\nlinear model (GLM), and confidence bands in polynomial regression.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 12:18:14 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 16:11:40 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Myllym\u00e4ki", "Mari", ""], ["Mrkvi\u010dka", "Tom\u00e1\u0161", ""]]}, {"id": "1911.06743", "submitter": "Augusto Fasano", "authors": "Augusto Fasano, Daniele Durante and Giacomo Zanella", "title": "Scalable and Accurate Variational Bayes for High-Dimensional Binary\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern methods for Bayesian regression beyond the Gaussian response setting\nare computationally impractical or inaccurate in high dimensions. As discussed\nin recent literature, bypassing this trade-off is still an open problem even in\nbasic binary regression models, and there is limited theory on the quality of\nvariational approximations in high-dimensional settings. To address this gap,\nwe study the approximation accuracy of routine-use mean-field variational Bayes\nin high-dimensional probit regression with Gaussian priors, obtaining new and\npractically relevant results on the pathological behavior of this strategy in\nuncertainty quantification, estimation and prediction, that also suggest\ncaution against maximum a posteriori estimates when p>n. Motivated by these\nresults, we develop a new partially-factorized variational approximation for\nthe posterior distribution of the probit coefficients that leverages a\nrepresentation with global and local variables but, unlike for classical\nmean-field assumptions, it avoids a fully factorized approximation, and instead\nassumes a factorization only for local variables. We prove that the resulting\napproximation belongs to a tractable class of unified skew-normal densities\nthat incorporates skewness and, unlike for state-of-the-art mean-field\nsolutions, converges to the exact posterior density as p goes to infinity. To\nsolve the variational optimization problem, we derive a tractable CAVI\nalgorithm that easily scales to p in tens of thousands, and provably requires a\nnumber of iterations converging to 1 as p goes to infinity. Such findings are\nalso illustrated in extensive empirical studies where our new solution is shown\nto improve the accuracy of mean-field variational Bayes for any n and p, with\nthe magnitude of these gains being remarkable in those high-dimensional p>n\nsettings where state-of-the-art methods are computationally impractical.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2019 16:51:29 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 14:33:54 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 13:56:59 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 17:04:49 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Fasano", "Augusto", ""], ["Durante", "Daniele", ""], ["Zanella", "Giacomo", ""]]}, {"id": "1911.06944", "submitter": "Donghui Yan", "authors": "Ke Alexander Wang, Xinran Bian, Pan Liu, Donghui Yan", "title": "$DC^2$: A Divide-and-conquer Algorithm for Large-scale Kernel Learning\n  with Application to Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Divide-and-conquer is a general strategy to deal with large scale problems.\nIt is typically applied to generate ensemble instances, which potentially\nlimits the problem size it can handle. Additionally, the data are often divided\nby random sampling which may be suboptimal. To address these concerns, we\npropose the $DC^2$ algorithm. Instead of ensemble instances, we produce\nstructure-preserving signature pieces to be assembled and conquered. $DC^2$\nachieves the efficiency of sampling-based large scale kernel methods while\nenabling parallel multicore or clustered computation. The data partition and\nsubsequent compression are unified by recursive random projections. Empirically\ndividing the data by random projections induces smaller mean squared\napproximation errors than conventional random sampling. The power of $DC^2$ is\ndemonstrated by our clustering algorithm $rpfCluster^+$, which is as accurate\nas some fastest approximate spectral clustering algorithms while maintaining a\nrunning time close to that of K-means clustering. Analysis on $DC^2$ when\napplied to spectral clustering shows that the loss in clustering accuracy due\nto data division and reduction is upper bounded by the data approximation error\nwhich would vanish with recursive random projections. Due to its easy\nimplementation and flexibility, we expect $DC^2$ to be applicable to general\nlarge scale learning problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2019 03:10:36 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Wang", "Ke Alexander", ""], ["Bian", "Xinran", ""], ["Liu", "Pan", ""], ["Yan", "Donghui", ""]]}, {"id": "1911.07142", "submitter": "Ick Hoon Jin", "authors": "Jaewoo Park and Ick Hoon Jin and Michael Schweinberger", "title": "Bayesian Model Selection for High-Dimensional Ising Models, With\n  Applications to Educational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly-intractable posterior distributions arise in many applications of\nstatistics concerned with discrete and dependent data, including physics,\nspatial statistics, machine learning, the social sciences, and other fields. A\nspecific example is psychometrics, which has adapted high-dimensional Ising\nmodels from machine learning, with a view to studying the interactions among\nbinary item responses in educational assessments. To estimate high-dimensional\nIsing models from educational assessment data, $\\ell_1$-penalized nodewise\nlogistic regressions have been used. Theoretical results in high-dimensional\nstatistics show that $\\ell_1$-penalized nodewise logistic regressions can\nrecover the true interaction structure with high probability, provided that\ncertain assumptions are satisfied. Those assumptions are hard to verify in\npractice and may be violated, and quantifying the uncertainty about the\nestimated interaction structure and parameter estimators is challenging. We\npropose a Bayesian approach that helps quantify the uncertainty about the\ninteraction structure and parameters without requiring strong assumptions, and\ncan be applied to Ising models with thousands of parameters. We demonstrate the\nadvantages of the proposed Bayesian approach compared with $\\ell_1$-penalized\nnodewise logistic regressions by simulation studies and applications to small\nand large educational data sets with up to 2,485 parameters. Among other\nthings, the simulation studies suggest that the Bayesian approach is more\nrobust against model misspecification due to omitted covariates than\n$\\ell_1$-penalized nodewise logistic regressions.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 03:19:10 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 06:49:00 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Park", "Jaewoo", ""], ["Jin", "Ick Hoon", ""], ["Schweinberger", "Michael", ""]]}, {"id": "1911.07172", "submitter": "Chencheng Cai", "authors": "Chencheng Cai and Rong Chen", "title": "State Space Emulation and Annealed Sequential Monte Carlo for High\n  Dimensional Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional optimization problems can be reformulated into a\nproblem of finding theoptimal state path under an equivalent state space model\nsetting. In this article, we present a general emulation strategy for\ndeveloping a state space model whose likelihood function (or posterior\ndistribution) shares the same general landscape as the objective function of\nthe original optimization problem. Then the solution of the optimization\nproblem is the same as the optimal state path that maximizes the likelihood\nfunction or the posterior distribution under the emulated system. To find such\nan optimal path, we adapt a simulated annealing approach by inserting a\ntemperature control into the emulated dynamic system and propose a novel\nannealed Sequential Monte Carlo (SMC) method that effectively generating Monte\nCarlo sample paths utilizing samples obtained on the high temperature scale.\nCompared to the vanilla simulated annealing implementation, annealed SMC is an\niterative algorithm for state space model optimization that directly generates\nstate paths from the equilibrium distributions with a decreasing sequence of\ntemperatures through sequential importance sampling which does not require\nburn-in or mixing iterations to ensure quasi-equilibrium condition. Several\napplications of state space model emulation and the corresponding annealed SMC\nresults are demonstrated.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2019 07:26:16 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Cai", "Chencheng", ""], ["Chen", "Rong", ""]]}, {"id": "1911.07688", "submitter": "Daniel Foreman-Mackey", "authors": "Daniel Foreman-Mackey, Will M. Farr, Manodeep Sinha, Anne M.\n  Archibald, David W. Hogg, Jeremy S. Sanders, Joe Zuntz, Peter K. G. Williams,\n  Andrew R. J. Nelson, Miguel de Val-Borro, Tobias Erhardt, Ilya Pashchenko,\n  Oriol Abril Pla", "title": "emcee v3: A Python ensemble sampling toolkit for affine-invariant MCMC", "comments": "Published in the Journal for Open Source Software", "journal-ref": "Journal of Open Source Software, 2 4(43), 1864 (2019)", "doi": "10.21105/joss.01864", "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  emcee is a Python library implementing a class of affine-invariant ensemble\nsamplers for Markov chain Monte Carlo (MCMC). This package has been widely\napplied to probabilistic modeling problems in astrophysics where it was\noriginally published, with some applications in other fields. When it was first\nreleased in 2012, the interface implemented in emcee was fundamentally\ndifferent from the MCMC libraries that were popular at the time, such as PyMC,\nbecause it was specifically designed to work with \"black box\" models instead of\nstructured graphical models. This has been a popular interface for applications\nin astrophysics because it is often non-trivial to implement realistic physics\nwithin the modeling frameworks required by other libraries. Since emcee's\nrelease, other libraries have been developed with similar interfaces, such as\ndynesty (Speagle 2019). The version 3.0 release of emcee is the first major\nrelease of the library in about 6 years and it includes a full re-write of the\ncomputational backend, several commonly requested features, and a set of new\n\"move\" implementations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 15:10:42 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Foreman-Mackey", "Daniel", ""], ["Farr", "Will M.", ""], ["Sinha", "Manodeep", ""], ["Archibald", "Anne M.", ""], ["Hogg", "David W.", ""], ["Sanders", "Jeremy S.", ""], ["Zuntz", "Joe", ""], ["Williams", "Peter K. G.", ""], ["Nelson", "Andrew R. J.", ""], ["de Val-Borro", "Miguel", ""], ["Erhardt", "Tobias", ""], ["Pashchenko", "Ilya", ""], ["Pla", "Oriol Abril", ""]]}, {"id": "1911.07728", "submitter": "Joris Mulder", "authors": "Joris Mulder, Xin Gu, Anton Olsson-Collentine, Andrew Tomarken,\n  Florian B\\\"oing-Messing, Herbert Hoijtink, Marlyne Meijerink, Donald R.\n  Williams, Janosch Menke, Jean-Paul Fox, Yves Rosseel, Eric-Jan Wagenmakers,\n  and Caspar van Lissa", "title": "BFpack: Flexible Bayes Factor Testing of Scientific Theories in R", "comments": "52 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a tremendous methodological development of Bayes factors for\nhypothesis testing in the social and behavioral sciences, and related fields.\nThis development is due to the flexibility of the Bayes factor for testing\nmultiple hypotheses simultaneously, the ability to test complex hypotheses\ninvolving equality as well as order constraints on the parameters of interest,\nand the interpretability of the outcome as the weight of evidence provided by\nthe data in support of competing scientific theories. The available software\ntools for Bayesian hypothesis testing are still limited however. In this paper\nwe present a new R-package called BFpack that contains functions for Bayes\nfactor hypothesis testing for the many common testing problems. The software\nincludes novel tools (i) for Bayesian exploratory testing (null vs positive vs\nnegative effects), (ii) for Bayesian confirmatory testing (competing hypotheses\nwith equality and/or order constraints), (iii) for common statistical analyses,\nsuch as linear regression, generalized linear models, (multivariate) analysis\nof (co)variance, correlation analysis, and random intercept models, (iv) using\ndefault priors, and (v) while allowing data to contain missing observations\nthat are missing at random.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 16:00:25 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Mulder", "Joris", ""], ["Gu", "Xin", ""], ["Olsson-Collentine", "Anton", ""], ["Tomarken", "Andrew", ""], ["B\u00f6ing-Messing", "Florian", ""], ["Hoijtink", "Herbert", ""], ["Meijerink", "Marlyne", ""], ["Williams", "Donald R.", ""], ["Menke", "Janosch", ""], ["Fox", "Jean-Paul", ""], ["Rosseel", "Yves", ""], ["Wagenmakers", "Eric-Jan", ""], ["van Lissa", "Caspar", ""]]}, {"id": "1911.07827", "submitter": "\\'Alvaro Briz-Red\\'on", "authors": "\\'Alvaro Briz-Red\\'on and Francisco Mart\\'inez-Ruiz and Francisco\n  Montes", "title": "DRHotNet: An R package for detecting differential risk hotspots on a\n  linear network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common applications of spatial data analysis is detecting\nzones, at a certain investigation level, where a point-referenced event under\nstudy is especially concentrated. The detection of this kind of zones, which\nare usually referred to as hotspots, is essential in certain fields such as\ncriminology, epidemiology or traffic safety. Traditionally, hotspot detection\nprocedures have been developed over areal units of analysis. Although working\nat this spatial scale can be suitable enough for many research or practical\npurposes, detecting hotspots at a more accurate level (for instance, at the\nroad segment level) may be more convenient sometimes. Furthermore, it is\ntypical that hotspot detection procedures are entirely focused on the\ndetermination of zones where an event is (overall) highly concentrated. It is\nless common, by far, that such procedures prioritize the location of zones\nwhere a specific type of event is overrepresented in relation to the other\ntypes observed, which have been denoted as differential risk hotspots. The R\npackage DRHotNet provides several functionalities to facilitate the detection\nof differential risk hotspots along a linear network. In this paper, DRHotNet\nis depicted and its usage in the R console is shown through a detailed analysis\nof a crime dataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 18:43:04 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Briz-Red\u00f3n", "\u00c1lvaro", ""], ["Mart\u00ednez-Ruiz", "Francisco", ""], ["Montes", "Francisco", ""]]}, {"id": "1911.07947", "submitter": "Sanvesh Srivastava", "authors": "Nariankadu D. Shyamalkumar and Sanvesh Srivastava", "title": "An Algorithm for Distributed Bayesian Inference in Generalized Linear\n  Models", "comments": "24 pages, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Monte Carlo algorithms, such as Markov chain Monte Carlo (MCMC) and\nHamiltonian Monte Carlo (HMC), are routinely used for Bayesian inference in\ngeneralized linear models; however, these algorithms are prohibitively slow in\nmassive data settings because they require multiple passes through the full\ndata in every iteration. Addressing this problem, we develop a scalable\nextension of these algorithms using the divide-and-conquer (D&C) technique that\ndivides the data into a sufficiently large number of subsets, draws parameters\nin parallel on the subsets using a \\textit{powered} likelihood, and produces\nMonte Carlo draws of the parameter by combining parameter draws obtained from\neach subset. These combined parameter draws play the role of draws from the\noriginal sampling algorithm. Our main contributions are two-fold. First, we\ndemonstrate through diverse simulated and real data analyses that our\ndistributed algorithm is comparable to the current state-of-the-art D&C\nalgorithm in terms of statistical accuracy and computational efficiency.\nSecond, providing theoretical support for our empirical observations, we\nidentify regularity assumptions under which the proposed algorithm leads to\nasymptotically optimal inference. We illustrate our methodology through normal\nlinear and logistic regressions, where parts of our D&C algorithm are\nanalytically tractable.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2019 20:50:17 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 19:31:39 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Shyamalkumar", "Nariankadu D.", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "1911.08048", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu, Jing Lei, and Kathryn Roeder", "title": "Gradient-based Sparse Principal Component Analysis with Extensions to\n  Online Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) is an important technique for\ndimensionality reduction of high-dimensional data. However, most existing\nsparse PCA algorithms are based on non-convex optimization, which provide\nlittle guarantee on the global convergence. Sparse PCA algorithms based on a\nconvex formulation, for example the Fantope projection and selection (FPS),\novercome this difficulty, but are computationally expensive. In this work we\nstudy sparse PCA based on the convex FPS formulation, and propose a new\nalgorithm that is computationally efficient and applicable to large and\nhigh-dimensional data sets. Nonasymptotic and explicit bounds are derived for\nboth the optimization error and the statistical accuracy, which can be used for\ntesting and inference problems. We also extend our algorithm to online learning\nproblems, where data are obtained in a streaming fashion. The proposed\nalgorithm is applied to high-dimensional gene expression data for the detection\nof functional gene groups.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2019 02:17:09 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Qiu", "Yixuan", ""], ["Lei", "Jing", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1911.08725", "submitter": "David  Nott", "authors": "Xuejun Yu, David J. Nott, Minh-Ngoc Tran and Nadja Klein", "title": "Assessment and adjustment of approximate inference algorithms using the\n  law of total variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common method for assessing validity of Bayesian sampling or approximate\ninference methods makes use of simulated data replicates for parameters drawn\nfrom the prior. Under continuity assumptions, quantiles of functions of the\nsimulated parameter values within corresponding posterior distributions are\nuniformly distributed. Checking for uniformity when a posterior density is\napproximated numerically provides a diagnostic for algorithm validity.\nFurthermore, adjustments to achieve uniformity can improve the quality of\napproximate inference methods. A weakness of this general approach is that it\nseems difficult to extend beyond scalar functions of interest. The present\narticle develops an alternative to quantile-based checking and adjustment\nmethods which is inherently multivariate. The new approach is based on use of\nthe tower property of conditional expectation and the law of total variance for\nrelating prior and posterior expectations and covariances. For adjustment,\napproximate inferences are modified so that the correct prior to posterior\nrelationships hold. We illustrate the method in three examples. The first uses\nan auxiliary model in a likelihood-free inference problem. The second considers\ncorrections for variational Bayes approximations in a deep neural network\ngeneralized linear mixed model. Our final application considers a deep neural\nnetwork surrogate for approximating Gaussian process regression predictive\ninference.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 06:16:20 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Yu", "Xuejun", ""], ["Nott", "David J.", ""], ["Tran", "Minh-Ngoc", ""], ["Klein", "Nadja", ""]]}, {"id": "1911.09012", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Parsimonious Mixtures of Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, data have become increasingly higher dimensional, which has\nprompted an increased need for dimension reduction techniques. This is perhaps\nespecially true for clustering (unsupervised classification) as well as\nsemi-supervised and supervised classification. Many methods have been proposed\nin the literature for two-way (multivariate) data and quite recently methods\nhave been presented for three-way (matrix variate) data. One such such method\nis the mixtures of matrix variate bilinear factor analyzers (MMVBFA) model.\nHerein, we propose of total of 64 parsimonious MMVBFA models. Simulated and\nreal data are used for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 16:30:31 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1911.09067", "submitter": "Bruno Sudret", "authors": "X. Zhu and B. Sudret", "title": "Replication-based emulation of the response distribution of stochastic\n  simulators using generalized lambda distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2019-008", "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited computational power, performing uncertainty quantification\nanalyses with complex computational models can be a challenging task. This is\nexacerbated in the context of stochastic simulators, the response of which to a\ngiven set of input parameters, rather than being a deterministic value, is a\nrandom variable with unknown probability density function (PDF). Of interest in\nthis paper is the construction of a surrogate that can accurately predict this\nresponse PDF for any input parameters. We suggest using a flexible distribution\nfamily -- the generalized lambda distribution -- to approximate the response\nPDF. The associated distribution parameters are cast as functions of input\nparameters and represented by sparse polynomial chaos expansions. To build such\na surrogate model, we propose an approach based on a local inference of the\nresponse PDF at each point of the experimental design based on replicated model\nevaluations. Two versions of this framework are proposed and compared on\nanalytical examples and case studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2019 18:03:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Zhu", "X.", ""], ["Sudret", "B.", ""]]}, {"id": "1911.09274", "submitter": "Pulong Ma", "authors": "Pulong Ma and Anirban Mondal and Bledar Konomi and Jonathan Hobbs and\n  Joon Song and Emily Kang", "title": "Computer Model Emulation with High-Dimensional Functional Output in\n  Large-Scale Observing System Uncertainty Experiments", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing system uncertainty experiments (OSUEs) have been recently proposed\nas a cost-effective way to perform probabilistic assessment of retrievals for\nNASA's Orbiting Carbon Observatory-2 (OCO-2) mission. One important component\nin the OCO-2 retrieval algorithm is a full-physics forward model that describes\nthe mathematical relationship between atmospheric variables such as carbon\ndioxide and radiances measured by the remote sensing instrument. This forward\nmodel is complicated and computationally expensive but large-scale OSUEs\nrequire evaluation of this model numerous times, which makes it infeasible for\ncomprehensive experiments. To tackle this issue, we develop a statistical\nemulator to facilitate large-scale OSUEs in the OCO-2 mission with independent\nemulation. Within each distinct spectral band, the emulator represents\nradiances output at irregular wavelengths via a linear combination of basis\nfunctions and random coefficients. These random coefficients are then modeled\nwith nearest-neighbor Gaussian processes with built-in input dimension\nreduction via active subspace. The proposed emulator reduces dimensionality in\nboth input space and output space, so that fast computation is achieved within\na fully Bayesian inference framework. Validation experiments demonstrate that\nthis emulator outperforms other competing statistical methods and a reduced\norder model that approximates the full-physics forward model.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 03:58:27 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 17:33:13 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Ma", "Pulong", ""], ["Mondal", "Anirban", ""], ["Konomi", "Bledar", ""], ["Hobbs", "Jonathan", ""], ["Song", "Joon", ""], ["Kang", "Emily", ""]]}, {"id": "1911.09294", "submitter": "Guiwen Tan", "authors": "Lipo Wang and Guiwen Tan, Hui Cao", "title": "Multi-level scalar structure in complex system analyses", "comments": "This paper will be revised and new results will be added", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn physics.data-an stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The geometrical structure is among the most fundamental ingredients in\nunderstanding complex systems. Is there any systematic approach in defining\nstructures quantitatively, rather than illustratively? If yes, what are the\nbasic principles to follow? By introducing the concept of extremal points at\ndifferent scale levels, a multi-level dissipation element approach has been\ndeveloped to define structures at different scale levels, in accordance with\nthe concept of structure hierarchy. Each dissipation element can be\ncharacterized by the length scale and the scalar variance inside. Using the\ntwo-dimensional fractal Brownian motion as a benchmark case, the conditional\nmean of the scalar difference with respect to the length scale shows clearly a\npower law and the scaling exponent is in agreement with the Hurst number. For\nthe 3D turbulence velocity component, the 1/3 scaling law can be represented.\nThese results indicate the important linkage between the turbulence physics and\now structure, if well posed and defined. In principle, the multi-level\ndissipation element idea is generally applicable in analyzing other multiscale\ncomplex systems as well.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 05:24:37 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 12:15:04 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Wang", "Lipo", ""], ["Tan", "Guiwen", ""], ["Cao", "Hui", ""]]}, {"id": "1911.09511", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Nicolas Idrobo, Rocio Titiunik", "title": "A Practical Introduction to Regression Discontinuity Designs:\n  Foundations", "comments": null, "journal-ref": null, "doi": "10.1017/9781108684606", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Element and its accompanying Element, Matias D. Cattaneo, Nicolas\nIdrobo, and Rocio Titiunik provide an accessible and practical guide for the\nanalysis and interpretation of Regression Discontinuity (RD) designs that\nencourages the use of a common set of practices and facilitates the\naccumulation of RD-based empirical evidence. In this Element, the authors\ndiscuss the foundations of the canonical Sharp RD design, which has the\nfollowing features: (i) the score is continuously distributed and has only one\ndimension, (ii) there is only one cutoff, and (iii) compliance with the\ntreatment assignment is perfect. In the accompanying Element, the authors\ndiscuss practical and conceptual extensions to the basic RD setup.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 14:58:18 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Idrobo", "Nicolas", ""], ["Titiunik", "Rocio", ""]]}, {"id": "1911.09656", "submitter": "Mike West", "authors": "Mike West", "title": "Bayesian forecasting of multivariate time series: Scalability, structure\n  uncertainty and decisions", "comments": "2018 Akaike Memorial Lecture Award paper", "journal-ref": null, "doi": "10.1007/s10463-019-00741-3", "report-no": null, "categories": "stat.ME eess.SP math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I overview recent research advances in Bayesian state-space modeling of\nmultivariate time series. A main focus is on the decouple/recouple concept that\nenables application of state-space models to increasingly large-scale data,\napplying to continuous or discrete time series outcomes. The scope includes\nlarge-scale dynamic graphical models for forecasting and multivariate\nvolatility analysis in areas such as economics and finance, multi-scale\napproaches for forecasting discrete/count time series in areas such as\ncommercial sales and demand forecasting, and dynamic network flow models for\nareas including internet traffic monitoring. In applications, explicit\nforecasting, monitoring and decision goals are paramount and should factor into\nmodel assessment and comparison, a perspective that is highlighted.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 18:35:12 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 15:57:39 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["West", "Mike", ""]]}, {"id": "1911.09698", "submitter": "Christian P. Robert", "authors": "Wu Changye and Christian P. Robert", "title": "Parallelising MCMC via Random Forests", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  For Bayesian computation in big data contexts, the divide-and-conquer MCMC\nconcept splits the whole data set into batches, runs MCMC algorithms separately\nover each batch to produce samples of parameters, and combines them to produce\nan approximation of the target distribution. In this article, we embed random\nforests into this framework and use each subposterior/partial-posterior as a\nproposal distribution to implement importance sampling. Unlike the existing\ndivide-and-conquer MCMC, our methods are based on scaled subposteriors, whose\nscale factors are not necessarily restricted to being equal to one or to the\nnumber of subsets. Through several experiments, we show that our methods work\nwell with models ranging from Gaussian cases to strongly non-Gaussian cases,\nand include model misspecification.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2019 19:02:13 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Changye", "Wu", ""], ["Robert", "Christian P.", ""]]}, {"id": "1911.09839", "submitter": "Wonyeol Lee", "authors": "Hyoungjin Lim, Gwonsoo Che, Wonyeol Lee, Hongseok Yang", "title": "Differentiable Algorithm for Marginalising Changepoints", "comments": "To appear at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm for marginalising changepoints in time-series models\nthat assume a fixed number of unknown changepoints. Our algorithm is\ndifferentiable with respect to its inputs, which are the values of latent\nrandom variables other than changepoints. Also, it runs in time O(mn) where n\nis the number of time steps and m the number of changepoints, an improvement\nover a naive marginalisation method with O(n^m) time complexity. We derive the\nalgorithm by identifying quantities related to this marginalisation problem,\nshowing that these quantities satisfy recursive relationships, and transforming\nthe relationships to an algorithm via dynamic programming. Since our algorithm\nis differentiable, it can be applied to convert a model non-differentiable due\nto changepoints to a differentiable one, so that the resulting models can be\nanalysed using gradient-based inference or learning techniques. We empirically\nshow the effectiveness of our algorithm in this application by tackling the\nposterior inference problem on synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 03:54:25 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Lim", "Hyoungjin", ""], ["Che", "Gwonsoo", ""], ["Lee", "Wonyeol", ""], ["Yang", "Hongseok", ""]]}, {"id": "1911.09880", "submitter": "Paul Brown", "authors": "Paul T. Brown, Chaitanya Joshi, Stephen Joe, Haavard Rue", "title": "A Novel Method of Marginalisation using Low Discrepancy Sequences for\n  Integrated Nested Laplace Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, it has been shown that approximations to marginal posterior\ndistributions obtained using a low discrepancy sequence (LDS) can outperform\nstandard grid-based methods with respect to both accuracy and computational\nefficiency. This recent method, which we will refer to as LDS-StM, can also\nproduce good approximations to multimodal posteriors. However, implementation\nof LDS-StM into integrated nested Laplace approximations (INLA), a methodology\nin which grid-based methods are used, is challenging. Motivated by this\nproblem, we propose modifications to LDS-StM that improves the approximations\nand make it compatible with INLA, without sacrificing computational speed. We\nalso present two examples to demonstrate that LDS-StM with modifications can\noutperform INLA's own grid approximation with respect to speed and accuracy. We\nalso demonstrate the flexibility of the new approach for the approximation of\nmultimodal marginals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 06:40:37 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 00:10:18 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Brown", "Paul T.", ""], ["Joshi", "Chaitanya", ""], ["Joe", "Stephen", ""], ["Rue", "Haavard", ""]]}, {"id": "1911.09985", "submitter": "Neelesh Upadhye Dr", "authors": "Aastha M. Sathe and N. S. Upadhye", "title": "Estimation of the Parameters of Symmetric Stable ARMA and ARMA-GARCH\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we first propose the modified Hannan-Rissanen Method for\nestimating the parameters of the autoregressive moving average (ARMA) process\nwith symmetric stable noise and symmetric stable generalized autoregressive\nconditional heteroskedastic (GARCH) noise. Next, we propose the modified\nempirical characteristic function method for the estimation of GARCH parameters\nwith symmetric stable noise. Further, we show the efficiency, accuracy, and\nsimplicity of our methods through Monte-Carlo simulation. Finally, we apply our\nproposed methods to model financial data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 11:58:12 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Sathe", "Aastha M.", ""], ["Upadhye", "N. S.", ""]]}, {"id": "1911.10232", "submitter": "Kai Ni", "authors": "Amit Pande, Kai Ni and Venkataramani Kini", "title": "SWAG: Item Recommendations using Convolutions on Weighted Graphs", "comments": "10 pages, 8 figures, 2019 IEEE BigData special session", "journal-ref": "2019 IEEE International Conference on Big Data", "doi": "10.1109/BigData47090.2019.9005633", "report-no": null, "categories": "cs.IR cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent advancements in deep neural networks for graph-structured data have\nled to state-of-the-art performance on recommender system benchmarks. In this\nwork, we present a Graph Convolutional Network (GCN) algorithm SWAG (Sample\nWeight and AGgregate), which combines efficient random walks and graph\nconvolutions on weighted graphs to generate embeddings for nodes (items) that\nincorporate both graph structure as well as node feature information such as\nitem-descriptions and item-images. The three important SWAG operations that\nenable us to efficiently generate node embeddings based on graph structures are\n(a) Sampling of graph to homogeneous structure, (b) Weighting the sampling,\nwalks and convolution operations, and (c) using AGgregation functions for\ngenerating convolutions. The work is an adaptation of graphSAGE over weighted\ngraphs. We deploy SWAG at Target and train it on a graph of more than 500K\nproducts sold online with over 50M edges. Offline and online evaluations reveal\nthe benefit of using a graph-based approach and the benefits of weighing to\nproduce high quality embeddings and product recommendations.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2019 19:51:58 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Pande", "Amit", ""], ["Ni", "Kai", ""], ["Kini", "Venkataramani", ""]]}, {"id": "1911.10434", "submitter": "Yuedong Wang", "authors": "Danqing Xu and Yuedong Wang", "title": "Low Rank Approximation for Smoothing Spline via Eigensystem Truncation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing splines provide a powerful and flexible means for nonparametric\nestimation and inference. With a cubic time complexity, fitting smoothing\nspline models to large data is computationally prohibitive. In this paper, we\nuse the theoretical optimal eigenspace to derive a low rank approximation of\nthe smoothing spline estimates. We develop a method to approximate the\neigensystem when it is unknown and derive error bounds for the approximate\nestimates. The proposed methods are easy to implement with existing software.\nExtensive simulations show that the new methods are accurate, fast, and\ncompares favorably against existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2019 23:50:29 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 17:04:32 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Xu", "Danqing", ""], ["Wang", "Yuedong", ""]]}, {"id": "1911.11002", "submitter": "Jeffrey Doser", "authors": "Mahdi Teimouri, Jeffrey W. Doser and Andrew O. Finley", "title": "ForestFit : An R package for modeling tree diameter distributions", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the diameter distribution of trees in forest stands is a common\nforestry task that supports key biologically and economically relevant\nmanagement decisions. The choice of model used to represent the diameter\ndistribution and how to estimate its parameters has received much attention in\nthe forestry literature; however, accessible software that facilitates\ncomprehensive comparison of the myriad modeling approaches is not available. To\nthis end, we developed an R package called ForestFit that simplifies estimation\nof common probability distributions used to model tree diameter distributions,\nincluding the two- and three-parameter Weibull distributions, Johnson's SB\ndistribution, Birnbaum-Saunders distribution, and finite mixture distributions.\nFrequentist and Bayesian techniques are provided for individual tree diameter\ndata, as well as grouped data. Additional functionality facilitates fitting\ngrowth curves to height-diameter data. The package also provides a set of\nfunctions for computing probability distributions and simulating random\nrealizations from common finite mixture models.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2019 15:49:57 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Teimouri", "Mahdi", ""], ["Doser", "Jeffrey W.", ""], ["Finley", "Andrew O.", ""]]}, {"id": "1911.11650", "submitter": "Ben Mansour Dia", "authors": "Ben Mansour Dia", "title": "A continuation method in Bayesian inference", "comments": "27 pages,9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a continuation method that entails generating a sequence of\ntransition probability density functions from the prior to the posterior in the\ncontext of Bayesian inference for parameter estimation problems. The\ncharacterization of transition distributions, by tempering the likelihood\nfunction, results in a homogeneous nonlinear partial integro-differential\nequation whose existence and uniqueness of solutions are addressed. The\nposterior probability distribution comes as the interpretation of the final\nstate of the path of transition distributions. A computationally stable scaling\ndomain for the likelihood is explored for the approximation of the expected\ndeviance, where we manage to hold back all the evaluations of the forward\npredictive model at the prior stage. It follows the computational tractability\nof the posterior distribution and opens access to the posterior distribution\nfor direct samplings. To get a solution formulation of the expected deviance,\nwe derive a partial differential equation governing the moments generating\nfunction of the log-likelihood. We show also that a spectral formulation of the\nexpected deviance can be obtained for low-dimensional problems under certain\nconditions. The computational efficiency of the proposed method is demonstrated\nthrough three differents numerical examples that focus on analyzing the\ncomputational bias generated by the method, assessing the continuation method\nin the Bayesian inference with non-Gaussian noise, and evaluating its ability\nto invert a multimodal parameter of interest.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 15:46:33 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Dia", "Ben Mansour", ""]]}, {"id": "1911.11709", "submitter": "Ana Fernandez Vidal", "authors": "Ana F. Vidal, Valentin De Bortoli, Marcelo Pereyra and Alain Durmus", "title": "Maximum likelihood estimation of regularisation parameters in\n  high-dimensional inverse problems: an empirical Bayesian approach. Part I:\n  Methodology and Experiments", "comments": "37 pages - SIIMS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many imaging problems require solving an inverse problem that is\nill-conditioned or ill-posed. Imaging methods typically address this difficulty\nby regularising the estimation problem to make it well-posed. This often\nrequires setting the value of the so-called regularisation parameters that\ncontrol the amount of regularisation enforced. These parameters are notoriously\ndifficult to set a priori, and can have a dramatic impact on the recovered\nestimates. In this work, we propose a general empirical Bayesian method for\nsetting regularisation parameters in imaging problems that are convex w.r.t.\nthe unknown image. Our method calibrates regularisation parameters directly\nfrom the observed data by maximum marginal likelihood estimation, and can\nsimultaneously estimate multiple regularisation parameters. Furthermore, the\nproposed algorithm uses the same basic operators as proximal optimisation\nalgorithms, namely gradient and proximal operators, and it is therefore\nstraightforward to apply to problems that are currently solved by using\nproximal optimisation techniques. Our methodology is demonstrated with a range\nof experiments and comparisons with alternative approaches from the literature.\nThe considered experiments include image denoising, non-blind image\ndeconvolution, and hyperspectral unmixing, using synthesis and analysis priors\ninvolving the L1, total-variation, total-variation and L1, and\ntotal-generalised-variation pseudo-norms. A detailed theoretical analysis of\nthe proposed method is presented in the companion paper arXiv:2008.05793.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2019 17:31:00 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 09:17:14 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 10:44:18 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Vidal", "Ana F.", ""], ["De Bortoli", "Valentin", ""], ["Pereyra", "Marcelo", ""], ["Durmus", "Alain", ""]]}, {"id": "1911.12003", "submitter": "Shu-Chuan Chen", "authors": "Justie Su-Tzu Juan, Yi-Ching Chen, Chen-Hui Lin, Shu-Chuan (Grace)\n  Chen", "title": "Measuring similarity between two mixture trees using mixture distance\n  metric and algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ancestral mixture model, proposed by Chen and Lindsay (2006), is an important\nmodel to build a hierarchical tree from high dimensional binary sequences.\nMixture trees created from ancestral mixture models involve in the inferred\nevolutionary relationships among various biological species. Moreover, it\ncontains the information of time when the species mutates. Tree comparison\nmetric, an essential issue in bioinformatics, is to measure the similarity\nbetween trees. However, to our knowledge, the approach to the comparison\nbetween two mixture trees is still under development. In this paper, we propose\na new metric, named mixture distance metric, to measure the similarity of two\nmixture trees. It uniquely considers the factor of evolutionary times between\ntrees. In addition, we also further develop two algorithms to compute the\nmixture distance between two mixture trees. One requires O(n^2) and the other\nrequires O(nh) computation time with O(n) preprocessing time, where n denotes\nthe number of leaves in the two mixture trees, and h denotes the minimum height\nof these two trees.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2019 07:58:01 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Juan", "Justie Su-Tzu", "", "Grace"], ["Chen", "Yi-Ching", "", "Grace"], ["Lin", "Chen-Hui", "", "Grace"], ["Shu-Chuan", "", "", "Grace"], ["Chen", "", ""]]}, {"id": "1911.13142", "submitter": "Mohammad Ghorbani Dr.", "authors": "Ottmar Cronie, Mohammad Ghorbani, Jorge Mateu and Jun Yu", "title": "Functional marked point processes -- A natural structure to unify\n  spatio-temporal frameworks and to analyse dependent functional data", "comments": "44 pages, 3 figures with 9 plots", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper treats functional marked point processes (FMPPs), which are\ndefined as marked point processes where the marks are random elements in some\n(Polish) function space. Such marks may represent e.g. spatial paths or\nfunctions of time. To be able to consider e.g. multivariate FMPPs, we also\nattach an additional, Euclidean, mark to each point. We indicate how FMPPs\nquite naturally connect the point process framework with both the functional\ndata analysis framework and the geostatistical framework. We further show that\nvarious existing models fit well into the FMPP framework. In addition, we\nintroduce a new family of summary statistics, weighted marked reduced moment\nmeasures, together with their non-parametric estimators, in order to study\nfeatures of the functional marks. We further show how they generalise other\nsummary statistics and we finally apply these tools to analyse population\nstructures, such as demographic evolution and sex ratio over time, in Spanish\nprovinces.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2019 15:10:50 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Cronie", "Ottmar", ""], ["Ghorbani", "Mohammad", ""], ["Mateu", "Jorge", ""], ["Yu", "Jun", ""]]}]