[{"id": "1002.0567", "submitter": "Paul Voutier", "authors": "Paul M. Voutier", "title": "A New Approximation to the Normal Distribution Quantile Function", "comments": "added contact details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.ST q-fin.CP stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a new approximation to the normal distribution quantile function.\nIt has a similar form to the approximation of Beasley and Springer [3],\nproviding a maximum absolute error of less than $2.5 \\cdot 10^{-5}$. This is\nless accurate than [3], but still sufficient for many applications. However it\nis faster than [3]. This is its primary benefit, which can be crucial to many\napplications, including in financial markets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 20:31:52 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 07:56:25 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Voutier", "Paul M.", ""]]}, {"id": "1002.1940", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis, Alkiviadis Tsimpiris", "title": "Measures of Analysis of Time Series (MATS): A MATLAB Toolkit for\n  Computation of Multiple Measures on Time Series Data Bases", "comments": "25 pages, 9 figures, two tables, the software can be downloaded at\n  http://eeganalysis.web.auth.gr/indexen.htm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as physiology and finance, large time series data\nbases are to be analyzed requiring the computation of linear, nonlinear and\nother measures. Such measures have been developed and implemented in commercial\nand freeware softwares rather selectively and independently. The Measures of\nAnalysis of Time Series ({\\tt MATS}) {\\tt MATLAB} toolkit is designed to handle\nan arbitrary large set of scalar time series and compute a large variety of\nmeasures on them, allowing for the specification of varying measure parameters\nas well. The variety of options with added facilities for visualization of the\nresults support different settings of time series analysis, such as the\ndetection of dynamics changes in long data records, resampling (surrogate or\nbootstrap) tests for independence and linearity with various test statistics,\nand discrimination power of different measures and for different combinations\nof their parameters. The basic features of {\\tt MATS} are presented and the\nimplemented measures are briefly described. The usefulness of {\\tt MATS} is\nillustrated on some empirical examples along with screenshots.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2010 18:44:54 GMT"}], "update_date": "2010-02-10", "authors_parsed": [["Kugiumtzis", "Dimitris", ""], ["Tsimpiris", "Alkiviadis", ""]]}, {"id": "1002.2017", "submitter": "Robert Kohn", "authors": "Weijun Xu, Li Yang and Robert Kohn", "title": "Computationally Efficient Estimation of Factor Multivariate Stochastic\n  Volatility Models", "comments": "32 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An MCMC simulation method based on a two stage delayed rejection\nMetropolis-Hastings algorithm is proposed to estimate a factor multivariate\nstochastic volatility model. The first stage uses kstep iteration towards the\nmode, with k small, and the second stage uses an adaptive random walk proposal\ndensity. The marginal likelihood approach of Chib (1995) is used to choose the\nnumber of factors, with the posterior density ordinates approximated by\nGaussian copula. Simulation and real data applications suggest that the\nproposed simulation method is computationally much more efficient than the\napproach of Chib. Nardari and Shephard (2006}. This increase in computational\nefficiency is particularly important in calculating marginal likelihoods\nbecause it is necessary to carry out the simulation a number of times to\nestimate the posterior ordinates for a given marginal likelihood. In addition\nto the MCMC method, the paper also proposes a fast approximate EM method to\nestimate the factor multivariate stochastic volatility model. The estimates\nfrom the approximate EM method are of interest in their own right, but are\nespecially useful as initial inputs to MCMC methods, making them more efficient\ncomputationally. The methodology is illustrated using simulated and real\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 02:55:05 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Xu", "Weijun", ""], ["Yang", "Li", ""], ["Kohn", "Robert", ""]]}, {"id": "1002.2243", "submitter": "Sam George", "authors": "Sam O. George, H. Bola George, Scott V. Nguyen", "title": "Effect of Wind Intermittency on the Electric Grid: Mitigating the Risk\n  of Energy Deficits", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an physics.soc-ph stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful implementation of California's Renewable Portfolio Standard (RPS)\nmandating 33 percent renewable energy generation by 2020 requires inclusion of\na robust strategy to mitigate increased risk of energy deficits (blackouts) due\nto short time-scale (sub 1 hour) intermittencies in renewable energy sources.\nOf these RPS sources, wind energy has the fastest growth rate--over 25%\nyear-over-year. If these growth trends continue, wind energy could make up 15\npercent of California's energy portfolio by 2016 (wRPS15). However, the\nhour-to-hour variations in wind energy (speed) will create large hourly energy\ndeficits that require installation of other, more predictable, compensation\ngeneration capacity and infrastructure. Compensating for the energy deficits of\nwRPS15 could potentially cost tens of billions in additional dollar-expenditure\nfor fossil and / or nuclear generation capacity. There is a real possibility\nthat carbon dioxide and other greenhouse gas (GHG) emission reductions will\nmiss the California Assembly Bill 32 (CA AB 32) target by a wide margin once\nthe wRPS15 compensation system is in place. This work presents a set of\nanalytics tools that show the impact of short-term intermittencies to help\npolicy makers understand and plan for wRPS15 integration. What are the right\npolicy choices for RPS that include wind energy?\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 23:42:09 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["George", "Sam O.", ""], ["George", "H. Bola", ""], ["Nguyen", "Scott V.", ""]]}, {"id": "1002.2684", "submitter": "Christian P. Robert", "authors": "Christian P. Robert and Jean-Michel Marin", "title": "On computational tools for Bayesian data analysis", "comments": "This is a chapter for the book \"Bayesian Methods and Expert\n  Elicitation\" edited by Klaus Bocker, 23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Robert and Rousseau (2010) addressed the foundational aspects of\nBayesian analysis, the current chapter details its practical aspects through a\nreview of the computational methods available for approximating Bayesian\nprocedures. Recent innovations like Monte Carlo Markov chain, sequential Monte\nCarlo methods and more recently Approximate Bayesian Computation techniques\nhave considerably increased the potential for Bayesian applications and they\nhave also opened new avenues for Bayesian inference, first and foremost\nBayesian model choice.\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 15:50:53 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2010 07:12:50 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Robert", "Christian P.", ""], ["Marin", "Jean-Michel", ""]]}, {"id": "1002.2702", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "Bayesian computational methods", "comments": "This is a revised version of a chapter written for the Handbook of\n  Computational Statistics, edited by J. Gentle, W. Hardle and Y. Mori in 2003,\n  in preparation for the second edition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we will first present the most standard computational\nchallenges met in Bayesian Statistics, focussing primarily on mixture\nestimation and on model choice issues, and then relate these problems with\ncomputational solutions. Of course, this chapter is only a terse introduction\nto the problems and solutions related to Bayesian computations. For more\ncomplete references, see Robert and Casella (2004, 2009), or Marin and Robert\n(2007), among others. We also restrain from providing an introduction to\nBayesian Statistics per se and for comprehensive coverage, address the reader\nto Robert (2007), (again) among others.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2010 15:36:25 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "1002.2706", "submitter": "Leonardo Bottolo", "authors": "Leonardo Bottolo, Sylvia Richardson", "title": "Evolutionary Stochastic Search for Bayesian model exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing Bayesian variable selection for linear Gaussian regression\nmodels for analysing high dimensional data sets is of current interest in many\nfields. In order to make such analysis operational, we propose a new sampling\nalgorithm based upon Evolutionary Monte Carlo and designed to work under the\n\"large p, small n\" paradigm, thus making fully Bayesian multivariate analysis\nfeasible, for example, in genetics/genomics experiments. Two real data examples\nin genomics are presented, demonstrating the performance of the algorithm in a\nspace of up to 10,000 covariates. Finally the methodology is compared with a\nrecently proposed search algorithms in an extensive simulation study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Feb 2010 13:57:26 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Bottolo", "Leonardo", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1002.3640", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "Improved EM for Mixture Proportions with Applications to Nonparametric\n  ML Estimation for Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improved EM strategies, based on the idea of efficient data augmentation\n(Meng and van Dyk 1997, 1998), are presented for ML estimation of mixture\nproportions. The resulting algorithms inherit the simplicity, ease of\nimplementation, and monotonic convergence properties of EM, but have\nconsiderably improved speed. Because conventional EM tends to be slow when\nthere exists a large overlap between the mixture components, we can improve the\nspeed without sacrificing the simplicity or stability, if we can reformulate\nthe problem so as to reduce the amount of overlap. We propose simple\n\"squeezing\" strategies for that purpose. Moreover, for high-dimensional\nproblems, such as computing the nonparametric MLE of the distribution function\nwith censored data, a natural and effective remedy for conventional EM is to\nadd exchange steps (based on improved EM) between adjacent mixture components,\nwhere the overlap is most severe. Theoretical considerations show that the\nresulting EM-type algorithms, when carefully implemented, are globally\nconvergent. Simulated and real data examples show dramatic improvement in speed\nin realistic situations.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2010 23:23:27 GMT"}], "update_date": "2010-02-22", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "1002.3784", "submitter": "Juerg Schelldorfer js", "authors": "J\\\"urg Schelldorfer, Peter B\\\"uhlmann and Sara van de Geer", "title": "Estimation for High-Dimensional Linear Mixed-Effects Models Using\n  $\\ell_1$-Penalization", "comments": null, "journal-ref": "Scandinavian Journal of Statistics 2011, 38: 197-214", "doi": "10.1111/j.1467-9469.2011.00740.x", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an $\\ell_1$-penalized estimation procedure for high-dimensional\nlinear mixed-effects models. The models are useful whenever there is a grouping\nstructure among high-dimensional observations, i.e. for clustered data. We\nprove a consistency and an oracle optimality result and we develop an algorithm\nwith provable numerical convergence. Furthermore, we demonstrate the\nperformance of the method on simulated and a real high-dimensional data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2010 17:08:03 GMT"}, {"version": "v2", "created": "Thu, 25 Nov 2010 14:05:02 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Schelldorfer", "J\u00fcrg", ""], ["B\u00fchlmann", "Peter", ""], ["van de Geer", "Sara", ""]]}, {"id": "1002.3946", "submitter": "Rosemary Braun", "authors": "Rosemary Braun, Gregory Leibon, Scott Pauls, and Daniel Rockmore", "title": "Partition Decoupling for Multi-gene Analysis of Gene Expression\n  Profiling Data", "comments": "Revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the extention and application of a new unsupervised statistical\nlearning technique--the Partition Decoupling Method--to gene expression data.\nBecause it has the ability to reveal non-linear and non-convex geometries\npresent in the data, the PDM is an improvement over typical gene expression\nanalysis algorithms, permitting a multi-gene analysis that can reveal\nphenotypic differences even when the individual genes do not exhibit\ndifferential expression. Here, we apply the PDM to publicly-available gene\nexpression data sets, and demonstrate that we are able to identify cell types\nand treatments with higher accuracy than is obtained through other approaches.\nBy applying it in a pathway-by-pathway fashion, we demonstrate how the PDM may\nbe used to find sets of mechanistically-related genes that discriminate\nphenotypes.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 02:38:20 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2011 21:44:26 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Braun", "Rosemary", ""], ["Leibon", "Gregory", ""], ["Pauls", "Scott", ""], ["Rockmore", "Daniel", ""]]}, {"id": "1002.4682", "submitter": "Yohei Tutiya", "authors": "Kazutaka Kurihara, Yohei Tutiya", "title": "Non-Central Limit Theorem Statistical Analysis for the \"Long-tailed\"\n  Internet Society", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a statistical analysis method and introduces the\ncorresponding software package \"tailstat,\" which is believed to be widely\napplicable to today's internet society. The proposed method facilitates\nstatistical analyses with small sample sets from given populations, which\nrender the central limit theorem inapplicable. A large-scale case study\ndemonstrates the effectiveness of the method and provides implications for\napplying similar analyses to other cases.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 02:52:35 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Kurihara", "Kazutaka", ""], ["Tutiya", "Yohei", ""]]}, {"id": "1002.4775", "submitter": "Robert Kohn", "authors": "Ralph Silva, Robert Kohn, Paolo Giordani, Xiuyan Mun", "title": "A copula based approach to adaptive sampling", "comments": "33 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article is concerned with adaptive sampling schemes for Bayesian\ninference that update the proposal densities using previous iterates. We\nintroduce a copula based proposal density which is made more efficient by\ncombining it with antithetic variable sampling. We compare the copula based\nproposal to an adaptive proposal density based on a multivariate mixture of\nnormals and an adaptive random walk Metropolis proposal. We also introduce a\nrefinement of the random walk proposal which performs better for multimodal\ntarget distributions. We compare the sampling schemes using challenging but\nrealistic models and priors applied to real data examples. The results show\nthat for the examples studied, the adaptive independent \\MH{} proposals are\nmuch more efficient than the adaptive random walk proposals and that in general\nthe copula based proposal has the best acceptance rates and lowest\ninefficiencies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 12:47:50 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Silva", "Ralph", ""], ["Kohn", "Robert", ""], ["Giordani", "Paolo", ""], ["Mun", "Xiuyan", ""]]}]