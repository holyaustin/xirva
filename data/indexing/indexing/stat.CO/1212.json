[{"id": "1212.0122", "submitter": "Luca Martino", "authors": "David Luengo and Luca Martino", "title": "Fully Adaptive Gaussian Mixture Metropolis-Hastings Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2013.6638846", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods are widely used in signal processing and\ncommunications for statistical inference and stochastic optimization. In this\nwork, we introduce an efficient adaptive Metropolis-Hastings algorithm to draw\nsamples from generic multi-modal and multi-dimensional target distributions.\nThe proposal density is a mixture of Gaussian densities with all parameters\n(weights, mean vectors and covariance matrices) updated using all the\npreviously generated samples applying simple recursive rules. Numerical results\nfor the one and two-dimensional cases are provided.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2012 15:05:59 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2013 11:33:56 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 13:28:20 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Luengo", "David", ""], ["Martino", "Luca", ""]]}, {"id": "1212.0534", "submitter": "Changgee Chang", "authors": "John R. Birge, Changgee Chang, and Nicholas G. Polson", "title": "Split Sampling: Expectations, Normalisation and Rare Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a methodology that we call split sampling methods to\nestimate high dimensional expectations and rare event probabilities. Split\nsampling uses an auxiliary variable MCMC simulation and expresses the\nexpectation of interest as an integrated set of rare event probabilities. We\nderive our estimator from a Rao-Blackwellised estimate of a marginal auxiliary\nvariable distribution. We illustrate our method with two applications. First,\nwe compute a shortest network path rare event probability and compare our\nmethod to estimation to a cross entropy approach. Then, we compute a\nnormalisation constant of a high dimensional mixture of Gaussians and compare\nour estimate to one based on nested sampling. We discuss the relationship\nbetween our method and other alternatives such as the product of conditional\nprobability estimator and importance sampling. The methods developed here are\navailable in the R package: SplitSampling.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2012 20:55:18 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2013 21:22:50 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Birge", "John R.", ""], ["Chang", "Changgee", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1212.0609", "submitter": "Biao Wu Dr", "authors": "Biao Wu, Michael A. Kouritzin and Fraser Newton", "title": "Properties of Quick Simulation Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herein, we introduce and study a new class of discrete random fields designed\nfor quick simulation and covariance inference under inhomogeneous condition.\n  Simulation of these correlated fields can be done in a single pass instead of\nrelying on multi-pass convergent methods like the Gibbs Sampler or other Markov\nChain Monte Carlo methods. The fields are constructed directly from specified\nmarginal probability mass functions and covariances between nearby sites.\n  The proposition on which the construction is based establishes when and how\nit is possible to simplify the conditional probabilities of each site given the\nother sites in a manner that makes simulation quite feasible yet maintains\ndesired marginal probabilities and covariances between sites.\n  Special cases of these correlated fields have been deployed successfully in\ndata authentication, object detection and image generation. The limitations\nthat must be imposed on the covariances and marginal probabilities in order for\nthe algorithm to work are studied. What's more, a necessary and sufficient\ncondition that guarantees the permutation property of correlated random fields\nare investigated. In particular, Markov random fields as a subclass of\ncorrelated random fields are derived by a general and natural condition.\nConsequently, a direct and flexible single pass algorithm for simulating Markov\nrandom fields follows.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 04:04:47 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Wu", "Biao", ""], ["Kouritzin", "Michael A.", ""], ["Newton", "Fraser", ""]]}, {"id": "1212.0634", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Better subset regression", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To find efficient screening methods for high dimensional linear regression\nmodels, this paper studies the relationship between model fitting and screening\nperformance. Under a sparsity assumption, we show that a subset that includes\nthe true submodel always yields smaller residual sum of squares (i.e., has\nbetter model fitting) than all that do not in a general asymptotic setting.\nThis indicates that, for screening important variables, we could follow a\n\"better fitting, better screening\" rule, i.e., pick a \"better\" subset that has\nbetter model fitting. To seek such a better subset, we consider the\noptimization problem associated with best subset regression. An EM algorithm,\ncalled orthogonalizing subset screening, and its accelerating version are\nproposed for searching for the best subset. Although the two algorithms cannot\nguarantee that a subset they yield is the best, their monotonicity property\nmakes the subset have better model fitting than initial subsets generated by\npopular screening methods, and thus the subset can have better screening\nperformance asymptotically. Simulation results show that our methods are very\ncompetitive in high dimensional variable screening even for finite sample\nsizes.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 07:49:48 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2013 02:58:03 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1212.0849", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim, Lan Jiang, Sumeetpal S. Singh, Tom Dean", "title": "Estimating the Static Parameters in Linear Gaussian Multiple Target\n  Tracking Models", "comments": "17 double column pages, 9 figures", "journal-ref": null, "doi": null, "report-no": "CUED/F-INFENG/TR.681", "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present both offline and online maximum likelihood estimation (MLE)\ntechniques for inferring the static parameters of a multiple target tracking\n(MTT) model with linear Gaussian dynamics. We present the batch and online\nversions of the expectation-maximisation (EM) algorithm for short and long data\nsets respectively, and we show how Monte Carlo approximations of these methods\ncan be implemented. Performance is assessed in numerical examples using\nsimulated data for various scenarios and a comparison with a Bayesian\nestimation procedure is also provided.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2012 20:51:07 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Yildirim", "Sinan", ""], ["Jiang", "Lan", ""], ["Singh", "Sumeetpal S.", ""], ["Dean", "Tom", ""]]}, {"id": "1212.1038", "submitter": "Erlis Ruli", "authors": "Erlis Ruli, Nicola Sartori and Laura Ventura", "title": "A note on marginal posterior simulation via higher-order tail area\n  approximations", "comments": null, "journal-ref": "Bayesian Analysis 09 2014", "doi": "10.1214/13-BA851", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of higher-order tail area approximations for Bayesian\nsimulation. These approximations give rise to an alternative simulation scheme\nto MCMC for Bayesian computation of marginal posterior distributions for a\nscalar parameter of interest, in the presence of nuisance parameters. Its\nadvantage over MCMC methods is that samples are drawn independently with lower\ncomputational time and the implementation requires only standard maximum\nlikelihood routines. The method is illustrated by a genetic linkage model, a\nnormal regression with censored data and a logistic regression model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2012 14:29:56 GMT"}], "update_date": "2014-05-23", "authors_parsed": [["Ruli", "Erlis", ""], ["Sartori", "Nicola", ""], ["Ventura", "Laura", ""]]}, {"id": "1212.1479", "submitter": "Scott Sisson", "authors": "Y. Fan and D. J. Nott and S. A. Sisson", "title": "Approximate Bayesian Computation via Regression Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods, which are applicable when the\nlikelihood is difficult or impossible to calculate, are an active topic of\ncurrent research. Most current ABC algorithms directly approximate the\nposterior distribution, but an alternative, less common strategy is to\napproximate the likelihood function. This has several advantages. First, in\nsome problems, it is easier to approximate the likelihood than to approximate\nthe posterior. Second, an approximation to the likelihood allows reference\nanalyses to be constructed based solely on the likelihood. Third, it is\nstraightforward to perform sensitivity analyses for several different choices\nof prior once an approximation to the likelihood is constructed, which needs to\nbe done only once. The contribution of the present paper is to consider\nregression density estimation techniques to approximate the likelihood in the\nABC setting. Our likelihood approximations build on recently developed marginal\nadaptation density estimators by extending them for conditional density\nestimation. Our approach facilitates reference Bayesian inference, as well as\nfrequentist inference. The method is demonstrated via a challenging problem of\ninference for stereological extremes, where we perform both frequentist and\nBayesian inference.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2012 21:31:56 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Fan", "Y.", ""], ["Nott", "D. J.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1212.1639", "submitter": "Kenichiro McAlinn", "authors": "Kenichiro McAlinn, Teruo Nakatsuma", "title": "Fully Parallel Particle Learning for GPGPUs and Other Parallel Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel parallel resampling algorithm for fully parallelized\nparticle filters, which is designed with GPUs (graphics processing units) or\nsimilar parallel computing devices in mind. With our new algorithm, a full\ncycle of particle filtering (computing the value of the likelihood for each\nparticle, constructing the cumulative distribution function (CDF) for\nresampling, resampling the particles with the CDF, and propagating new\nparticles for the next cycle) can be executed in a massively and completely\nparallel manner. One of the advantages of our algorithm is that every single\nnumerical computation or memory access related to the particle filtering is\nexecuted solely inside the GPU in parallel, and no data transfer between the\nGPU's device memory and the CPU's host memory occurs unless for further\nprocessing, so that it can circumvent the limited memory bandwidth between the\nGPU and the CPU. To demonstrate the advantage of our parallel algorithm, we\nconducted a Monte Carlo experiment in which we apply the parallel algorithm as\nwell as conventional sequential algorithms for estimation of a simple state\nspace model via particle learning, and compare them in terms of execution time.\nThe results show that the parallel algorithm is far superior to the sequential\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2012 16:06:35 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 19:06:11 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["McAlinn", "Kenichiro", ""], ["Nakatsuma", "Teruo", ""]]}, {"id": "1212.1778", "submitter": "Vittorio Perduca", "authors": "The Minh Luong, Vittorio Perduca, Gregory Nuel", "title": "Hidden Markov Model Applications in Change-Point Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of change-points in heterogeneous sequences is a statistical\nchallenge with many applications in fields such as finance, signal analysis and\nbiology. A wide variety of literature exists for finding an ideal set of\nchange-points for characterizing the data. In this tutorial we elaborate on the\nHidden Markov Model (HMM) and present two different frameworks for applying HMM\nto change-point models. Then we provide a summary of two procedures for\ninference in change-point analysis, which are particular cases of the\nforward-backward algorithm for HMMs, and discuss common implementation\nproblems. Lastly, we provide two examples of the HMM methods on available data\nsets and we shortly discuss about the applications to current genomics studies.\nThe R code used in the examples is provided in the appendix.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 11:07:03 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Luong", "The Minh", ""], ["Perduca", "Vittorio", ""], ["Nuel", "Gregory", ""]]}, {"id": "1212.1788", "submitter": "Juan Carlos Jimenez", "authors": "J.C. Jimenez", "title": "Approximate discrete-time schemes for the estimation of diffusion\n  processes from complete observations", "comments": "The new version corrects some typos and provides extra information\n  that might contribute to clarify some aspects. A new assertion in Theorem 1\n  was added, and a new convergence result was included (current Theorem 2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a modification of the conventional approximations to the\nquasi-maximum likelihood method is introduced for the parameter estimation of\ndiffusion processes from discrete observations. This is based on a convergent\napproximation to the first two conditional moments of the diffusion process\nthrough discrete-time schemes. It is shown that, for finite samples, the\nresulting approximate estimators converge to the quasi-maximum likelihood one\nwhen the error between the discrete-time approximation and the diffusion\nprocess decreases. For an increasing number of observations, the approximate\nestimators are asymptotically normal distributed and their bias decreases when\nthe mentioned error does it. A simulation study is provided to illustrate the\nperformance of the new estimators. The results show that, with respect to the\nconventional approximate estimators, the new ones significantly enhance the\nparameter estimation of the test equations. The proposed estimators are\nintended for the recurrent practical situation where a nonlinear stochastic\nsystem should be identified from a reduced number of complete observations\ndistant in time.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 13:21:42 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 01:12:45 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Jimenez", "J. C.", ""]]}, {"id": "1212.1791", "submitter": "James Tucker", "authors": "J. Derek Tucker, Wei Wu, and Anuj Srivastava", "title": "Generative Models for Functional Data using Phase and Amplitude\n  Separation", "comments": "19 Pages, accepted for publication to Computational Statistics and\n  Data Analysis (Dec 2012)", "journal-ref": null, "doi": "10.1016/j.csda.2012.12.001", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Constructing generative models for functional observations is an important\ntask in statistical functional analysis. In general, functional data contains\nboth phase (or x or horizontal) and amplitude (or y or vertical) variability.\nTradi- tional methods often ignore the phase variability and focus solely on\nthe amplitude variation, using cross-sectional techniques such as fPCA for\ndimensional reduction and data modeling. Ignoring phase variability leads to a\nloss of structure in the data and inefficiency in data models. This paper\npresents an approach that relies on separating the phase (x-axis) and amplitude\n(y-axis), then modeling these components using joint distributions. This\nseparation, in turn, is performed using a technique called elastic shape\nanalysis of curves that involves a new mathematical representation of\nfunctional data. Then, using individual fPCAs, one each for phase and amplitude\ncomponents, while respecting the nonlinear geometry of the phase representation\nspace; impose joint probability models on principal coefficients of these\ncomponents. These ideas are demonstrated using random sampling, for models\nestimated from simulated and real datasets, and show their superiority over\nmodels that ignore phase-amplitude separation. Furthermore, the generative\nmodels are applied to classification of functional data and achieve high\nperformance in applications involv- ing SONAR signals of underwater objects,\nhandwritten signatures, and periodic body movements recorded by smart phones.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2012 13:41:50 GMT"}, {"version": "v2", "created": "Tue, 18 Dec 2012 15:27:57 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tucker", "J. Derek", ""], ["Wu", "Wei", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1212.2135", "submitter": "John Birge", "authors": "John R. Birge and Nicholas G. Polson", "title": "Optimisation via Slice Sampling", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a simulation-based approach to optimisation with\nmulti-modal functions using slice sampling. Our method specifies the objective\nfunction as an energy potential in a Boltzmann distribution and then we use\nauxiliary exponential slice variables to provide samples for a variety of\nenergy levels. Our slice sampler draws uniformly over the augmented slice\nregion. We identify the global modes by projecting the path of the chain back\nto the underlying space. Four standard test functions are used to illustrate\nthe methodology: Rosenbrock, Himmelblau, Rastrigin, and Shubert. These\nfunctions demonstrate the flexibility of our approach as they include functions\nwith long ridges (Rosenbrock), multi-modality (Himmelblau, Shubert) and many\nlocal modes dominated by one global (Rastrigin). The methods described here are\nimplemented in the {\\tt R} package {\\tt McmcOpt}.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 17:07:25 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Birge", "John R.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1212.2228", "submitter": "Xun Huan", "authors": "Xun Huan, Youssef M. Marzouk", "title": "Gradient-based stochastic optimization methods in Bayesian experimental\n  design", "comments": "Preprint 40 pages, 10 figures (121 small figures). v1 submitted to\n  the International Journal for Uncertainty Quantification on December 10,\n  2012; v2 submitted on September 10, 2013. v2 changes: (a) clarified algorithm\n  stopping criteria and other parameters; (b) emphasized paper contributions,\n  plus other minor edits; v3 submitted on December 26, 2014. v3 changes: minor\n  edits", "journal-ref": "International Journal for Uncertainty Quantification 4 (2014)\n  479-510", "doi": "10.1615/Int.J.UncertaintyQuantification.2014006730", "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal experimental design (OED) seeks experiments expected to yield the\nmost useful data for some purpose. In practical circumstances where experiments\nare time-consuming or resource-intensive, OED can yield enormous savings. We\npursue OED for nonlinear systems from a Bayesian perspective, with the goal of\nchoosing experiments that are optimal for parameter inference. Our objective in\nthis context is the expected information gain in model parameters, which in\ngeneral can only be estimated using Monte Carlo methods. Maximizing this\nobjective thus becomes a stochastic optimization problem.\n  This paper develops gradient-based stochastic optimization methods for the\ndesign of experiments on a continuous parameter space. Given a Monte Carlo\nestimator of expected information gain, we use infinitesimal perturbation\nanalysis to derive gradients of this estimator. We are then able to formulate\ntwo gradient-based stochastic optimization approaches: (i) Robbins-Monro\nstochastic approximation, and (ii) sample average approximation combined with a\ndeterministic quasi-Newton method. A polynomial chaos approximation of the\nforward model accelerates objective and gradient evaluations in both cases. We\ndiscuss the implementation of these optimization methods, then conduct an\nempirical comparison of their performance. To demonstrate design in a nonlinear\nsetting with partial differential equation forward models, we use the problem\nof sensor placement for source inversion. Numerical results yield useful\nguidelines on the choice of algorithm and sample sizes, assess the impact of\nestimator bias, and quantify tradeoffs of computational cost versus solution\nquality and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2012 21:47:11 GMT"}, {"version": "v2", "created": "Tue, 10 Sep 2013 21:36:57 GMT"}, {"version": "v3", "created": "Fri, 26 Dec 2014 22:54:32 GMT"}], "update_date": "2014-12-30", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1212.2393", "submitter": "Halis Sak", "authors": "Halis Sak and Wolfgang H\\\"ormann", "title": "Simulating the Continuation of a Time Series in R", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of the continuation of a given time series is useful for many\npractical applications. But no standard procedure for this task is suggested in\nthe literature. It is therefore demonstrated how to use the seasonal ARIMA\nprocess to simulate the continuation of an observed time series. The R-code\npresented uses well-known modeling procedures for ARIMA models and conditional\nsimulation of a SARIMA model with known parameters. A small example\ndemonstrates the correctness and practical relevance of the new idea.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2012 11:56:34 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Sak", "Halis", ""], ["H\u00f6rmann", "Wolfgang", ""]]}, {"id": "1212.2458", "submitter": "Jose Carlos Ferreira da Rocha", "authors": "Jose Carlos Ferreira da Rocha, Fabio Gagliardi Cozman, Cassio Polpo de\n  Campos", "title": "Inference in Polytrees with Sets of Probabilities", "comments": "Appears in Proceedings of the Nineteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2003)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2003-PG-217-224", "categories": "cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferences in directed acyclic graphs associated with probability sets and\nprobability intervals are NP-hard, even for polytrees. In this paper we focus\non such inferences, and propose: 1) a substantial improvement on Tessems A / R\nalgorithm FOR polytrees WITH probability intervals; 2) a new algorithm FOR\ndirection - based local search(IN sets OF probability) that improves ON\nexisting methods; 3) a collection OF branch - AND - bound algorithms that\ncombine the previous techniques.The first two techniques lead TO approximate\nsolutions, WHILE branch - AND - bound procedures can produce either exact OR\napproximate solutions.We report ON dramatic improvements ON existing techniques\nFOR inference WITH probability sets AND intervals, IN SOME cases reducing the\ncomputational effort BY many orders OF magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 19 Oct 2012 15:05:11 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["da Rocha", "Jose Carlos Ferreira", ""], ["Cozman", "Fabio Gagliardi", ""], ["de Campos", "Cassio Polpo", ""]]}, {"id": "1212.3721", "submitter": "Juan Carlos Jimenez", "authors": "J.C. Jimenez", "title": "Approximate continuous-discrete filters for the estimation of diffusion\n  processes from partial and noisy observations", "comments": "The new version corrects some typos and provides extra information\n  that might contribute to clarify some aspects. A new assertion in Theorem 1\n  was added, and a new convergence result was included (current Theorem 2).\n  arXiv admin note: substantial text overlap with arXiv:1212.1788", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an alternative approximation to the innovation method is\nintroduced for the parameter estimation of diffusion processes from partial and\nnoisy observations. This is based on a convergent approximation to the first\ntwo conditional moments of the innovation process through approximate\ncontinuous-discrete filters of minimum variance. It is shown that, for finite\nsamples, the resulting approximate estimators converge to the exact one when\nthe error of the approximate filters decreases. For an increasing number of\nobservations, the estimators are asymptotically normal distributed and their\nbias decreases when the above mentioned error does it. A simulation study is\nprovided to illustrate the performance of the new estimators. The results show\nthat, with respect to the conventional approximate estimators, the new ones\nsignificantly enhance the parameter estimation of the test equations. The\nproposed estimators are intended for the recurrent practical situation where a\nnonlinear stochastic system should be identified from a reduced number of\npartial and noisy observations distant in time.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2012 20:34:08 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2013 01:51:36 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Jimenez", "J. C.", ""]]}, {"id": "1212.3845", "submitter": "Giulio Cottone", "authors": "Giulio Cottone and Mario Di Paola", "title": "Fractional Spectral Moments for Digital Simulation of Multivariate Wind\n  Velocity Fields", "comments": "12 pages, 2 figures", "journal-ref": "Journal of Wind Engineering and Industrial Aerodynamics, 2011,\n  99(6-7):741 - 747", "doi": "10.1016/j.jweia.2011.03.006", "report-no": null, "categories": "cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method for the digital simulation of wind velocity fields by\nFractional Spectral Moment function is proposed. It is shown that by\nconstructing a digital filter whose coefficients are the fractional spectral\nmoments, it is possible to simulate samples of the target process as\nsuperposition of Riesz fractional derivatives of a Gaussian white noise\nprocesses. The key of this simulation technique is the generalized Taylor\nexpansion proposed by the authors. The method is extended to multivariate\nprocesses and practical issues on the implementation of the method are\nreported.\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2012 22:33:22 GMT"}], "update_date": "2012-12-18", "authors_parsed": [["Cottone", "Giulio", ""], ["Di Paola", "Mario", ""]]}, {"id": "1212.5203", "submitter": "Michael Larsen", "authors": "Michael D. Larsen", "title": "An Experiment with Hierarchical Bayesian Record Linkage", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In record linkage (RL), or exact file matching, the goal is to identify the\nlinks between entities with information on two or more files. RL is an\nimportant activity in areas including counting the population, enhancing survey\nframes and data, and conducting epidemiological and follow-up studies. RL is\nchallenging when files are very large, no accurate personal identification (ID)\nnumber is present on all files for all units, and some information is recorded\nwith error. Without an unique ID number one must rely on comparisons of names,\naddresses, dates, and other information to find the links. Latent class models\ncan be used to automatically score the value of information for determining\nmatch status. Data for fitting models come from comparisons made within groups\nof units that pass initial file blocking requirements. Data distributions can\nvary across blocks. This article examines the use of prior information and\nhierarchical latent class models in the context of RL.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2012 19:37:25 GMT"}], "update_date": "2012-12-21", "authors_parsed": [["Larsen", "Michael D.", ""]]}, {"id": "1212.5669", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsk\\'y", "title": "Estimation, Testing, and Prediction Regions of the Fixed and Random\n  Effects by Solving the Henderson's Mixed Model Equations", "comments": null, "journal-ref": "Measurement Science Review, Vol. 12, No. 6, 2012, 234-248", "doi": "10.2478/v10048-012-0033-6", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present a brief overview of the methods for making statistical inference\n(testing statistical hypotheses, construction of confidence and/or prediction\nintervals and regions) about linear functions of the fixed effects and/or about\nthe fixed and random effects simultaneously, in conventional simple linear\nmixed model. The presented approach is based on solutions from the Henderson's\nmixed model equations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2012 09:07:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 08:59:48 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Witkovsk\u00fd", "Viktor", ""]]}, {"id": "1212.5760", "submitter": "Paul McNicholas", "authors": "Yuhong Wei and Paul D. McNicholas", "title": "Mixture Model Averaging for Clustering", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-014-0182-6", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixture model-based clustering applications, it is common to fit several\nmodels from a family and report clustering results from only the `best' one. In\nsuch circumstances, selection of this best model is achieved using a model\nselection criterion, most often the Bayesian information criterion. Rather than\nthrow away all but the best model, we average multiple models that are in some\nsense close to the best one, thereby producing a weighted average of clustering\nresults. Two (weighted) averaging approaches are considered: averaging the\ncomponent membership probabilities and averaging models. In both cases, Occam's\nwindow is used to determine closeness to the best model and weights are\ncomputed within a Bayesian model averaging paradigm. In some cases, we need to\nmerge components before averaging; we introduce a method for merging mixture\ncomponents based on the adjusted Rand index. The effectiveness of our\nmodel-based clustering averaging approaches is illustrated using a family of\nGaussian mixture models on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2012 04:29:13 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2013 14:26:16 GMT"}, {"version": "v3", "created": "Sat, 26 Jul 2014 20:36:39 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Wei", "Yuhong", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1212.6020", "submitter": "Gordon J Ross", "authors": "Gordon J. Ross, Dimitris K. Tasoulis, Niall M. Adams", "title": "Sequential Monitoring of a Bernoulli Sequence when the Pre-change\n  Parameter is Unknown", "comments": "Computational Statistics, March 2012", "journal-ref": "Computational Statistics (2012), 28:463-479", "doi": "10.1007/s00180-012-0311-7", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of monitoring for a change in the mean of a sequence of Bernoulli\nrandom variables has been widely studied. However most existing approaches make\nat least one of the following assumptions, which may be violated in many\nreal-world situations: 1) the pre-change value of the Bernoulli parameter is\nknown in advance, 2) computational efficiency is not paramount, and 3) enough\nobservations occur between change points to allow asymptotic approximations to\nbe used. We develop a novel change detection method based on Fisher's Exact\nTest which does not make any of these assumptions. We show that our method can\nbe implemented in a computationally efficient manner, and is hence suited to\nsequential monitoring where new observations are constantly being received over\ntime. We assess our method's performance empirically via using simulated data,\nand find that it is comparable to the optimal CUSUM scheme which assumes both\npre- and post-change values of the parameter to be known.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2012 11:12:21 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ross", "Gordon J.", ""], ["Tasoulis", "Dimitris K.", ""], ["Adams", "Niall M.", ""]]}, {"id": "1212.6546", "submitter": "Richard Warr", "authors": "Richard L. Warr", "title": "Numerical Approximation of Probability Mass Functions Via the Inverse\n  Discrete Fourier Transform", "comments": null, "journal-ref": null, "doi": "10.1007/s11009-013-9366-3", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  First passage distributions of semi-Markov processes are of interest in\nfields such as reliability, survival analysis, and many others. The problem of\nfinding or computing first passage distributions is, in general, quite\nchallenging. We take the approach of using characteristic functions (or Fourier\ntransforms) and inverting them, to numerically calculate the first passage\ndistribution. Numerical inversion of characteristic functions can be\nnumerically unstable for a general probability measure, however, we show for\nlattice distributions they can be quickly calculated using the inverse discrete\nFourier transform. Using the fast Fourier transform algorithm these\ncomputations can be extremely fast. In addition to the speed of this approach,\nwe are able to prove a few useful bounds for the numerical inversion error of\nthe characteristic functions. These error bounds rely on the existence of a\nfirst or second moment of the distribution, or on an eventual monotonicity\ncondition. We demonstrate these techniques in an example and include R-code.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2012 18:01:26 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Warr", "Richard L.", ""]]}, {"id": "1212.6899", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "Simulation of Populations in a Time-, Age- and Duration Dependent\n  Illness-Death Model", "comments": "10 pages, 4 figures, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevant events in a three state illness-death model (IDM) of a chronic\ndisease are the diagnosis of the disease and death with or without the disease.\nIn this article a simulation framework for populations moving in the IDM is\npresented. The simulation is closely related to the concept of Lexis diagrams\nin event history analysis. Details of the implementation and an example of a\nhypothetical disease are described.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2012 14:38:31 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Brinks", "Ralph", ""]]}]