[{"id": "1812.00080", "submitter": "Ionut-Gabriel Farcas", "authors": "Ionut-Gabriel Farcas, Tobias G\\\"orler, Hans-Joachim Bungartz, Frank\n  Jenko and Tobias Neckel", "title": "Sensitivity-driven adaptive sparse stochastic approximations in plasma\n  microinstability analysis", "comments": "34 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.CE cs.NA math.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying uncertainty in predictive simulations for real-world problems is\nof paramount importance - and far from trivial, mainly due to the large number\nof stochastic parameters and significant computational requirements. Adaptive\nsparse grid approximations are an established approach to overcome these\nchallenges. However, standard adaptivity is based on global information, thus\nproperties such as lower intrinsic stochastic dimensionality or anisotropic\ncoupling of the input directions, which are common in practical applications,\nare not fully exploited. We propose a novel structure-exploiting\ndimension-adaptive sparse grid approximation methodology using Sobol'\ndecompositions in each subspace to introduce a sensitivity scoring system to\ndrive the adaptive process. By employing local sensitivity information, we\nexplore and exploit the anisotropic coupling of the stochastic inputs as well\nas the lower intrinsic stochastic dimensionality. The proposed approach is\ngeneric, i.e., it can be formulated in terms of arbitrary approximation\noperators and point sets. In particular, we consider sparse grid interpolation\nand pseudo-spectral projection constructed on (L)-Leja sequences. The power and\nusefulness of the proposed method is demonstrated by applying it to the\nanalysis of gyrokinetic microinstabilities in fusion plasmas, one of the key\nscientific problems in plasma physics and fusion research. In this context, it\nis shown that a 12D parameter space can be scanned very efficiently, gaining\nmore than an order of magnitude in computational cost over the standard\nadaptive approach. Moreover, it allows for the uncertainty propagation and\nsensitivity analysis in higher-dimensional plasma microturbulence problems,\nwhich would be almost impossible to tackle with standard screening approaches.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 22:41:34 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 17:28:09 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Farcas", "Ionut-Gabriel", ""], ["G\u00f6rler", "Tobias", ""], ["Bungartz", "Hans-Joachim", ""], ["Jenko", "Frank", ""], ["Neckel", "Tobias", ""]]}, {"id": "1812.00173", "submitter": "Michael McCourt", "authors": "Michael McCourt and Gregory Fasshauer and David Kozak", "title": "A Nonstationary Designer Space-Time Kernel", "comments": "5 pages, 2 figures, NIPS 2018 spacetime workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, kriging models are often designed using a stationary\ncovariance structure; this translation-invariance produces models which have\nnumerous favorable properties. This assumption can be limiting, though, in\ncircumstances where the dynamics of the model have a fundamental asymmetry,\nsuch as in modeling phenomena that evolve over time from a fixed initial\nprofile. We propose a new nonstationary kernel which is only defined over the\nhalf-line to incorporate time more naturally in the modeling process.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 08:11:33 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["McCourt", "Michael", ""], ["Fasshauer", "Gregory", ""], ["Kozak", "David", ""]]}, {"id": "1812.00375", "submitter": "Lijian Jiang", "authors": "Yuming Ba and Lijian Jiang", "title": "Ensemble-based implicit sampling for Bayesian inverse problems with\n  non-Gaussian priors", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, we develop an ensemble-based implicit sampling method for\nBayesian inverse problems. For Bayesian inference, the iterative ensemble\nsmoother (IES) and implicit sampling are integrated to obtain importance\nensemble samples, which build an importance density. The proposed method shares\na similar idea to importance sampling. IES is used to approximate mean and\ncovariance of a posterior distribution. This provides the MAP point and the\ninverse of Hessian matrix, which are necessary to construct the implicit map in\nimplicit sampling. The importance samples are generated by the implicit map and\nthe corresponding weights are the ratio between the importance density and\nposterior density. In the proposed method, we use the ensemble samples of IES\nto find the optimization solution of likelihood function and the inverse of\nHessian matrix. This approach avoids the explicit computation for Jacobian\nmatrix and Hessian matrix, which are very computationally expensive in high\ndimension spaces. To treat non-Gaussian models, discrete cosine transform and\nGaussian mixture model are used to characterize the non-Gaussian priors. The\nensemble-based implicit sampling method is extended to the non-Gaussian priors\nfor exploring the posterior of unknowns in inverse problems. The proposed\nmethod is used for each individual Gaussian model in the Gaussian mixture\nmodel. The proposed approach substantially improves the applicability of\nimplicit sampling method. A few numerical examples are presented to demonstrate\nthe efficacy of the proposed method with applications of inverse problems for\nsubsurface flow problems and anomalous diffusion models in porous media.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 12:00:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Ba", "Yuming", ""], ["Jiang", "Lijian", ""]]}, {"id": "1812.00538", "submitter": "Cai Li", "authors": "Cai Li, Luo Xiao, Sheng Luo", "title": "Fast Covariance Estimation for Multivariate Sparse Functional Data", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation is essential yet underdeveloped for analyzing\nmultivariate functional data. We propose a fast covariance estimation method\nfor multivariate sparse functional data using bivariate penalized splines. The\ntensor-product B-spline formulation of the proposed method enables a simple\nspectral decomposition of the associated covariance operator and explicit\nexpressions of the resulting eigenfunctions as linear combinations of B-spline\nbases, thereby dramatically facilitating subsequent principal component\nanalysis. We derive a fast algorithm for selecting the smoothing parameters in\ncovariance smoothing using leave-one-subject-out cross-validation. The method\nis evaluated with extensive numerical studies and applied to an Alzheimer's\ndisease study with multiple longitudinal outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:20:44 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 21:25:34 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Li", "Cai", ""], ["Xiao", "Luo", ""], ["Luo", "Sheng", ""]]}, {"id": "1812.01162", "submitter": "Zekun Xu", "authors": "Zekun Xu, Eric B. Laber, Ana-Maria Staicu", "title": "Hierarchical Continuous Time Hidden Markov Model, with Application in\n  Zero-Inflated Accelerometer Data", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": "10.1007/978-3-030-33416-1_7", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable devices including accelerometers are increasingly being used to\ncollect high-frequency human activity data in situ. There is tremendous\npotential to use such data to inform medical decision making and public health\npolicies. However, modeling such data is challenging as they are\nhigh-dimensional, heterogeneous, and subject to informative missingness, e.g.,\nzero readings when the device is removed by the participant. We propose a\nflexible and extensible continuous-time hidden Markov model to extract\nmeaningful activity patterns from human accelerometer data. To facilitate\nestimation with massive data we derive an efficient learning algorithm that\nexploits the hierarchical structure of the parameters indexing the proposed\nmodel. We also propose a bootstrap procedure for interval estimation. The\nproposed methods are illustrated using data from the 2003 - 2004 and 2005 -\n2006 National Health and Nutrition Examination Survey.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 01:53:38 GMT"}, {"version": "v2", "created": "Tue, 25 Dec 2018 12:22:04 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Zekun", ""], ["Laber", "Eric B.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1812.01345", "submitter": "Kari Heine", "authors": "Kari Heine and Alex Beskos and Ajay Jasra and David Balding and Maria\n  De Iorio", "title": "Bridging trees for posterior inference on Ancestral Recombination Graphs", "comments": "23 pages, 9 figures, accepted for publication in Proceedings of the\n  Royal Society A", "journal-ref": null, "doi": "10.1098/rspa.2018.0568", "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Markov chain Monte Carlo algorithm, implemented in software\nArbores, for inferring the history of a sample of DNA sequences. Our principal\ninnovation is a bridging procedure, previously applied only for simple\nstochastic processes, in which the local computations within a bridge can\nproceed independently of the rest of the DNA sequence, facilitating large-scale\nparallelisation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 11:36:19 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Heine", "Kari", ""], ["Beskos", "Alex", ""], ["Jasra", "Ajay", ""], ["Balding", "David", ""], ["De Iorio", "Maria", ""]]}, {"id": "1812.01380", "submitter": "Piet Groeneboom", "authors": "Piet Groeneboom", "title": "The Lagrange approach in the monotone single index model", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The finite-dimensional parameters of the monotone single index model are\noften estimated by minimization of a least squares criterion and\nreparametrization to deal with the non-unicity. We avoid the reparametrization\nby using a Lagrange-type method and replace the minimization over the\nfinite-dimensional parameter alpha by a `crossing of zero' criterion at the\nderivative level. In particular, we consider a simple score estimator (SSE), an\nefficient score estimator (ESE), and a penalized least squares estimator (PLSE)\nfor which we can apply this method. The SSE and ESE were discussed in\nBalabdaoui, Groeneboom and Hendrickx (2018}, but the proofs still used\nreparametrization. Another version of the PLSE was discussed in Kuchibhotla and\nPatra (2017), where also reparametrization was used. The estimators are\ncompared with the profile least squares estimator (LSE), Han's maximum rank\nestimator (MRE), the effective dimension reduction estimator (EDR) and a linear\nleast squares estimator, which can be used if the covariates have an\nelliptically symmetric distribution. We also investigate the effects of random\nstarting values in the search algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:41:14 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 14:41:12 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Groeneboom", "Piet", ""]]}, {"id": "1812.01502", "submitter": "Kari Heine", "authors": "Kari Heine and Nick Whiteley and A. Taylan Cemgil", "title": "Parallelising Particle Filters with Butterfly Interactions", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap particle filter (BPF) is the corner stone of many popular\nalgorithms used for solving inference problems involving time series that are\nobserved through noisy measurements in a non-linear and non-Gaussian context.\nThe long term stability of BPF arises from particle interactions which in the\ncontext of modern parallel computing systems typically means that particle\ninformation needs to be communicated between processing elements, which makes\nparallel implementation of BPF nontrivial.\n  In this paper we show that it is possible to constrain the interactions in a\nway which, under some assumptions, enables the reduction of the cost of\ncommunicating the particle information while still preserving the consistency\nand the long term stability of the BPF. Numerical experiments demonstrate that\nalthough the imposed constraints introduce additional error, the proposed\nmethod shows potential to be the method of choice in certain settings.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 16:08:38 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Heine", "Kari", ""], ["Whiteley", "Nick", ""], ["Cemgil", "A. Taylan", ""]]}, {"id": "1812.01553", "submitter": "Edward Wagstaff", "authors": "Ed Wagstaff and Saad Hamid and Michael Osborne", "title": "Batch Selection for Parallelisation of Bayesian Quadrature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration over non-negative integrands is a central problem in machine\nlearning (e.g. for model averaging, (hyper-)parameter marginalisation, and\ncomputing posterior predictive distributions). Bayesian Quadrature is a\nprobabilistic numerical integration technique that performs promisingly when\ncompared to traditional Markov Chain Monte Carlo methods. However, in contrast\nto easily-parallelised MCMC methods, Bayesian Quadrature methods have, thus\nfar, been essentially serial in nature, selecting a single point to sample at\neach step of the algorithm. We deliver methods to select batches of points at\neach step, based upon those recently presented in the Batch Bayesian\nOptimisation literature. Such parallelisation significantly reduces computation\ntime, especially when the integrand is expensive to sample.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 17:47:30 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Wagstaff", "Ed", ""], ["Hamid", "Saad", ""], ["Osborne", "Michael", ""]]}, {"id": "1812.01655", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, \\'Emilie Chouzenoux, V\\'ictor Elvira, Joaqu\\'in\n  M\\'iguez", "title": "A probabilistic incremental proximal gradient method", "comments": "5 pages, includes an extra numerical experiment", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a probabilistic optimization method, named\nprobabilistic incremental proximal gradient (PIPG) method, by developing a\nprobabilistic interpretation of the incremental proximal gradient algorithm. We\nexplicitly model the update rules of the incremental proximal gradient method\nand develop a systematic approach to propagate the uncertainty of the solution\nestimate over iterations. The PIPG algorithm takes the form of Bayesian\nfiltering updates for a state-space model constructed by using the cost\nfunction. Our framework makes it possible to utilize well-known exact or\napproximate Bayesian filters, such as Kalman or extended Kalman filters, to\nsolve large-scale regularized optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 19:49:46 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 12:43:28 GMT"}, {"version": "v3", "created": "Sat, 5 Jan 2019 12:03:33 GMT"}, {"version": "v4", "created": "Tue, 23 Apr 2019 13:01:22 GMT"}, {"version": "v5", "created": "Wed, 19 Jun 2019 13:42:34 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Chouzenoux", "\u00c9milie", ""], ["Elvira", "V\u00edctor", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1812.01871", "submitter": "Philipp Otto", "authors": "Philipp Otto", "title": "spGARCH: An R-Package for Spatial and Spatiotemporal ARCH models", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2019-053", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a general overview on spatial and spatiotemporal ARCH models\nis provided. In particular, we distinguish between three different spatial\nARCH-type models. In addition to the original definition of Otto et al. (2016),\nwe introduce an exponential spatial ARCH model in this paper. For this new\nmodel, maximum-likelihood estimators for the parameters are proposed. In\naddition, we consider a new complex-valued definition of the spatial ARCH\nprocess. From a practical point of view, the use of the R-package spGARCH is\ndemonstrated. To be precise, we show how the proposed spatial ARCH models can\nbe simulated and summarize the variety of spatial models, which can be\nestimated by the estimation functions provided in the package. Eventually, we\napply all procedures to a real-data example.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 09:32:17 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Otto", "Philipp", ""]]}, {"id": "1812.02609", "submitter": "Emilia Pompe", "authors": "Emilia Pompe, Chris Holmes, Krzysztof {\\L}atuszy\\'nski", "title": "A Framework for Adaptive MCMC Targeting Multimodal Distributions", "comments": "65 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Monte Carlo method for sampling from multimodal\ndistributions. The idea of this technique is based on splitting the task into\ntwo: finding the modes of a target distribution $\\pi$ and sampling, given the\nknowledge of the locations of the modes. The sampling algorithm relies on steps\nof two types: local ones, preserving the mode; and jumps to regions associated\nwith different modes. Besides, the method learns the optimal parameters of the\nalgorithm while it runs, without requiring user intervention. Our technique\nshould be considered as a flexible framework, in which the design of moves can\nfollow various strategies known from the broad MCMC literature.\n  In order to design an adaptive scheme that facilitates both local and jump\nmoves, we introduce an auxiliary variable representing each mode and we define\na new target distribution $\\tilde{\\pi}$ on an augmented state space\n$\\mathcal{X}~\\times~\\mathcal{I}$, where $\\mathcal{X}$ is the original state\nspace of $\\pi$ and $\\mathcal{I}$ is the set of the modes. As the algorithm runs\nand updates its parameters, the target distribution $\\tilde{\\pi}$ also keeps\nbeing modified. This motivates a new class of algorithms, Auxiliary Variable\nAdaptive MCMC. We prove general ergodic results for the whole class before\nspecialising to the case of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 15:36:54 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 14:34:36 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Pompe", "Emilia", ""], ["Holmes", "Chris", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""]]}, {"id": "1812.03092", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "A tutorial on generalizing the default Bayesian t-test via posterior\n  sampling and encompassing priors", "comments": "in press at Communications for Statistical Methods and Applications", "journal-ref": null, "doi": "10.29220/CSAM.2019.26.2.217", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of so-called default Bayesian hypothesis tests, scientists in\napplied fields have gained access to a powerful and principled method for\ntesting hypotheses. However, such default tests usually come with a compromise,\nrequiring the analyst to accept a one-size-fits-all approach to hypothesis\ntesting. Further, such tests may not have the flexibility to test problems the\nscientist really cares about. In this tutorial, I demonstrate a flexible\napproach to generalizing one specific default test (the JZS t-test; Rouder et\nal., 2009) that is becoming increasingly popular in the social and behavioral\nsciences. The approach uses two theoretical results, the Savage-Dickey density\nratio (Dickey and Lientz, 1980) and the technique of encompassing priors\n(Klugkist et al., 2005) in combination with MCMC sampling via an easy-to-use\nprobabilistic modeling package for R called Greta. Through a comprehensive\nmathematical description of the techniques as well as illustrative examples,\nthe reader is presented with a general, flexible workflow that can be extended\nto solve problems relevant to his or her own work.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 16:29:15 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 19:47:01 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1812.03214", "submitter": "Yu Zhang", "authors": "Yu Zhang, Xiangzhong Fang", "title": "On the lengths of $t$-based confidence intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n=mk$ $iid$ samples from $N(\\theta,\\sigma^2)$ with $\\theta$ and\n$\\sigma^2$ unknown, we have two ways to construct $t$-based confidence\nintervals for $\\theta$. The traditional method is to treat these $n$ samples as\n$n$ groups and calculate the intervals. The second, and less frequently used,\nmethod is to divide them into $m$ groups with each group containing $k$\nelements. For this method, we calculate the mean of each group, and these $k$\nmean values can be treated as $iid$ samples from $N(\\theta,\\sigma^2/k)$. We can\nuse these $k$ values to construct $t$-based confidence intervals. Intuition\ntells us that, at the same confidence level $1-\\alpha$, the first method should\nbe better than the second one. Yet if we define \"better\" in terms of the\nexpected length of the confidence interval, then the second method is better\nbecause the expected length of the confidence interval obtained from the first\nmethod is shorter than the one obtained from the second method. Our work proves\nthis intuition theoretically. We also specify that when the elements in each\ngroup are correlated, the first method becomes an invalid method, while the\nsecond method can give us correct results. We illustrate this with analytical\nexpressions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 20:53:18 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zhang", "Yu", ""], ["Fang", "Xiangzhong", ""]]}, {"id": "1812.04324", "submitter": "Soghra Bohlourihajjar", "authors": "Soghra Bohlourihajjar, Soleiman Khazaei", "title": "Bayesian Nonparametric Model for Weighted Data Using Mixture of Burr XII\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture model (DPMM) is a popular Bayesian nonparametric\nmodel. In this paper, we apply this model to weighted data and then estimate\nthe un-weighted distribution from the corresponding weighted distribution using\nthe metropolis-Hastings algorithm. We then apply the DPMM with different\nkernels to simulated and real data sets. In particular, we work with lifetime\ndata in the presence of censored data and then calculate estimated density and\nsurvival values.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 10:45:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Bohlourihajjar", "Soghra", ""], ["Khazaei", "Soleiman", ""]]}, {"id": "1812.04403", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller and Torsten A. En{\\ss}lin", "title": "Encoding prior knowledge in the structure of the likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inference of deep hierarchical models is problematic due to strong\ndependencies between the hierarchies. We investigate a specific transformation\nof the model parameters based on the multivariate distributional transform.\nThis transformation is a special form of the reparametrization trick, flattens\nthe hierarchy and leads to a standard Gaussian prior on all resulting\nparameters. The transformation also transfers all the prior information into\nthe structure of the likelihood, hereby decoupling the transformed parameters a\npriori from each other. A variational Gaussian approximation in this\nstandardized space will be excellent in situations of relatively uninformative\ndata. Additionally, the curvature of the log-posterior is well-conditioned in\ndirections that are weakly constrained by the data, allowing for fast inference\nin such a scenario. In an example we perform the transformation explicitly for\nGaussian process regression with a priori unknown correlation structure. Deep\nmodels are inferred rapidly in highly and slowly in poorly informed situations.\nThe flat model show exactly the opposite performance pattern. A synthesis of\nboth, the deep and the flat perspective, provides their combined advantages and\novercomes the individual limitations, leading to a faster inference.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 14:03:55 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1812.05575", "submitter": "Lucilio Cordero-Grande", "authors": "Lucilio Cordero-Grande", "title": "MIXANDMIX: numerical techniques for the computation of empirical\n  spectral distributions of population mixtures", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The MIXANDMIX (mixtures by Anderson mixing) tool for the computation of the\nempirical spectral distribution of random matrices generated by mixtures of\npopulations is described. Within the population mixture model the mapping\nbetween the population distributions and the limiting spectral distribution can\nbe obtained by solving a set of systems of non-linear equations, for which an\nefficient implementation is provided. The contributions include a method for\naccelerated fixed point convergence, a homotopy continuation strategy to\nprevent convergence to non-admissible solutions, a blind non-uniform grid\nconstruction for effective distribution support detection and approximation,\nand a parallel computing architecture. Comparisons are performed with available\npackages for the single population case and with results obtained by simulation\nfor the more general model implemented here. Results show competitive\nperformance and improved flexibility.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:49:09 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 16:44:33 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Cordero-Grande", "Lucilio", ""]]}, {"id": "1812.05928", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa, Vaibhav Rajan", "title": "Automatic Differentiation in Mixture Models", "comments": "19 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1301.1505, arXiv:1502.05767, arXiv:1503.06302 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss two specific classes of models - Gaussian Mixture\nCopula models and Mixture of Factor Analyzers - and the advantages of doing\ninference with gradient descent using automatic differentiation. Gaussian\nmixture models are a popular class of clustering methods, that offers a\nprincipled statistical approach to clustering. However, the underlying\nassumption, that every mixing component is normally distributed, can often be\ntoo rigid for several real life datasets. In order to to relax the assumption\nabout the normality of mixing components, a new class of parametric mixture\nmodels that are based on Copula functions - Gaussian Mixuture Copula Models\nwere introduced. Estimating the parameters of the proposed Gaussian Mixture\nCopula Model (GMCM) through maximum likelihood has been intractable due to the\npositive semi-positive-definite constraints on the variance-covariance\nmatrices. Previous attempts were limited to maximizing a proxy-likelihood which\ncan be maximized using EM algorithm. These existing methods, even though easier\nto implement, does not guarantee any convergence nor monotonic increase of the\nGMCM Likelihood. In this paper, we use automatic differentiation tools to\nmaximize the exact likelihood of GMCM, at the same time avoiding any constraint\nequations or Lagrange multipliers. We show how our method leads a monotonic\nincrease in likelihood and converges to a (local) optimum value of likelihood.\n  In this paper, we also show how Automatic Differentiation can be used for\ninference with Mixture of Factor Analyzers and advantages of doing so. We also\ndiscuss how this method also has all the properties such as monotonic increase\nin likelihood and convergence to a local optimum.\n  Note that our work is also applicable to special cases of these two models -\nfor e.g. Simple Copula models, Factor Analyzer model, etc.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 16:33:35 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "1812.06309", "submitter": "Bruno Sudret", "authors": "C. Lataniotis, S. Marelli and B. Sudret", "title": "Extending classical surrogate modelling to high-dimensions through\n  supervised dimensionality reduction: a data-driven approach", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": "RSUQ-2018-008B", "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to their versatility, ease of deployment and high-performance,\nsurrogate models have become staple tools in the arsenal of uncertainty\nquantification (UQ). From local interpolants to global spectral decompositions,\nsurrogates are characterised by their ability to efficiently emulate complex\ncomputational models based on a small set of model runs used for training. An\ninherent limitation of many surrogate models is their susceptibility to the\ncurse of dimensionality, which traditionally limits their applicability to a\nmaximum of $\\mathcal{O}(10^2)$ input dimensions. We present a novel approach at\nhigh-dimensional surrogate modelling that is model-, dimensionality reduction-\nand surrogate model- agnostic (black box), and can enable the solution of high\ndimensional (i.e. up to $\\mathcal{O}(10^4)$) problems. After introducing the\ngeneral algorithm, we demonstrate its performance by combining Kriging and\npolynomial chaos expansions surrogates and kernel principal component analysis.\nIn particular, we compare the generalisation performance that the resulting\nsurrogates achieve to the classical sequential application of dimensionality\nreduction followed by surrogate modelling on several benchmark applications,\ncomprising an analytical function and two engineering applications of\nincreasing dimensionality and complexity.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 15:39:41 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 09:54:05 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 11:05:08 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Lataniotis", "C.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1812.06696", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Yixian Wang, Shih-Gu Huang, Ilwoo Lyu", "title": "Rapid Acceleration of the Permutation Test via Slow Random Walks in the\n  Permutation Group", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The permutation test is an often used test procedure in brain imaging.\nUnfortunately, generating every possible permutation for large-scale brain\nimage datasets such as HCP and ADNI with hundreds images is not practical. Many\nprevious attempts at speeding up the permutation test rely on various\napproximation strategies such as estimating the tail distribution with known\nparametric distributions. In this study, we show how to rapidly accelerate the\npermutation test without any type of approximate strategies by exploiting the\nunderlying algebraic structure of the permutation group. The method is applied\nto large number of MRIs in two applications: (1) localizing the male and female\ndifferences and (2) localizing the regions of high genetic heritability in the\nsulcal and gyral pattern of the human cortical brain.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 11:06:34 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 17:58:47 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Chung", "Moo K.", ""], ["Wang", "Yixian", ""], ["Huang", "Shih-Gu", ""], ["Lyu", "Ilwoo", ""]]}, {"id": "1812.07042", "submitter": "Joseph Hart", "authors": "Joseph Hart and Pierre Gremaud", "title": "Robustness of the Sobol' indices to marginal distribution uncertainty", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis (GSA) quantifies the influence of uncertain\nvariables in a mathematical model. The Sobol' indices, a commonly used tool in\nGSA, seek to do this by attributing to each variable its relative contribution\nto the variance of the model output. In order to compute Sobol' indices, the\nuser must specify a probability distribution for the uncertain variables. This\ndistribution is typically unknown and must be chosen using limited data and/or\nknowledge. The usefulness of the Sobol' indices depends on their robustness to\nthis distributional uncertainty. This article presents a novel method which\nuses \"optimal perturbations\" of the marginal probability density functions to\nanalyze the robustness of the Sobol' indices. The method is illustrated through\nsynthetic examples and a model for contaminant transport.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 20:33:49 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Hart", "Joseph", ""], ["Gremaud", "Pierre", ""]]}, {"id": "1812.07240", "submitter": "Gertraud Malsiner-Walli", "authors": "Gilles Celeux, Kaniav Kamary, Gertraud Malsiner-Walli, Jean-Michel\n  Marin, Christian P. Robert", "title": "Computational Solutions for Bayesian Inference in Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter surveys the most standard Monte Carlo methods available for\nsimulating from a posterior distribution associated with a mixture and conducts\nsome experiments about the robustness of the Gibbs sampler in high dimensional\nGaussian settings. This is a chapter prepared for the forthcoming 'Handbook of\nMixture Analysis'.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 08:50:35 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Celeux", "Gilles", ""], ["Kamary", "Kaniav", ""], ["Malsiner-Walli", "Gertraud", ""], ["Marin", "Jean-Michel", ""], ["Robert", "Christian P.", ""]]}, {"id": "1812.07673", "submitter": "Thomas Trikalinos", "authors": "Alexandra G. Ellis, Rowan Iskandar, Christopher H. Schmid, John B.\n  Wong, Thomas A. Trikalinos", "title": "Active learning for efficiently training emulators of computationally\n  expensive mathematical models", "comments": "Counting appendix materials: 31 pages, 7 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An emulator is a fast-to-evaluate statistical approximation of a detailed\nmathematical model (simulator). When used in lieu of simulators, emulators can\nexpedite tasks that require many repeated evaluations, such as sensitivity\nanalyses, policy optimization, model calibration, and value-of-information\nanalyses. Emulators are developed using the output of simulators at specific\ninput values (design points). Developing an emulator that closely approximates\nthe simulator can require many design points, which becomes computationally\nexpensive. We describe a self-terminating active learning algorithm to\nefficiently develop emulators tailored to a specific emulation task, and\ncompare it with algorithms that optimize geometric criteria (random latin\nhypercube sampling and maximum projection designs) and other active learning\nalgorithms (treed Gaussian Processes that optimize typical active learning\ncriteria). We compared the algorithms' root mean square error (RMSE) and\nmaximum absolute deviation from the simulator (MAX) for seven benchmark\nfunctions and in a prostate cancer screening model. In the empirical analyses,\nin simulators with greatly-varying smoothness over the input domain, active\nlearning algorithms resulted in emulators with smaller RMSE and MAX for the\nsame number of design points. In all other cases, all algorithms performed\ncomparably. The proposed algorithm attained satisfactory performance in all\nanalyses, had smaller variability than the treed Gaussian Processes (it is\ndeterministic), and, on average, had similar or better performance as the treed\nGaussian Processes in 6 out of 7 benchmark functions and in the prostate cancer\nmodel.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 22:31:31 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 18:56:38 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Ellis", "Alexandra G.", ""], ["Iskandar", "Rowan", ""], ["Schmid", "Christopher H.", ""], ["Wong", "John B.", ""], ["Trikalinos", "Thomas A.", ""]]}, {"id": "1812.07801", "submitter": "Thomas Wutzler", "authors": "Thomas Wutzler", "title": "Efficient treatment of model discrepancy by Gaussian Processes -\n  Importance for imbalanced multiple constraint inversions", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic simulation models are inverted against observations in order to\ngain inference on modeled processes. However, with the increasing ability to\ncollect high resolution observations, these observations represent more\npatterns of detailed processes that are not part of a modeling purpose. This\nmismatch results in model discrepancies, i.e. systematic differences between\nobservations and model predictions. When discrepancies are not accounted for\nproperly, posterior uncertainty is underestimated. Furthermore parameters are\ninferred so that model discrepancies appear with observation data stream with\nfew records instead of data streams corresponding to the weak model parts. This\nimpedes the identification of weak process formulations that need to be\nimproved. Therefore, we developed an efficient formulation to account for model\ndiscrepancy by the statistical model of Gaussian processes (GP). This paper\npresents a new Bayesian sampling scheme for model parameters and discrepancies,\nexplains the effects of its application on inference by a basic example, and\ndemonstrates applicability to a real world model-data integration study.\n  The GP approach correctly identified model discrepancy in rich data streams.\nInnovations in sampling allowed successful application to observation data\nstreams of several thousand records. Moreover, the proposed new formulation\ncould be combined with gradient-based optimization. As a consequence, model\ninversion studies should acknowledge model discrepancies, especially when using\nmultiple imbalanced data streams. To this end, studies can use the proposed GP\napproach to improve inference on model parameters and modeled processes.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:21:50 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Wutzler", "Thomas", ""]]}, {"id": "1812.07929", "submitter": "Kjartan Kloster Osmundsen", "authors": "Kjartan Kloster Osmundsen, Tore Selland Kleppe, Roman Liesenfeld", "title": "Importance Sampling-based Transport Map Hamiltonian Monte Carlo for\n  Bayesian Hierarchical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an importance sampling (IS)-based transport map Hamiltonian Monte\nCarlo procedure for performing full Bayesian analysis in general nonlinear\nhigh-dimensional hierarchical models. Using IS techniques to construct a\ntransport map, the proposed method transforms the typically highly challenging\ntarget distribution of a hierarchical model into a target which is easily\nsampled using standard Hamiltonian Monte Carlo. Conventional applications of\nhigh-dimensional IS, where infinite variance of IS weights can be a serious\nproblem, require computationally costly high-fidelity IS distributions. An\nappealing property of our method is that the IS distributions employed can be\nof rather low fidelity, making it computationally cheap. We illustrate our\nalgorithm in applications to challenging dynamic state-space models, where it\nexhibits very high simulation efficiency compared to relevant benchmarks, even\nfor variants of the proposed method implemented using a few dozen lines of code\nin the Stan statistical software.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 13:15:43 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 14:28:26 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Osmundsen", "Kjartan Kloster", ""], ["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""]]}, {"id": "1812.07978", "submitter": "Remi Daviet", "authors": "Remi Daviet", "title": "Inference with Hamiltonian Sequential Monte Carlo Simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper proposes a new Monte-Carlo simulator combining the advantages of\nSequential Monte Carlo simulators and Hamiltonian Monte Carlo simulators. The\nresult is a method that is robust to multimodality and complex shapes to use\nfor inference in presence of difficult likelihoods or target functions. Several\nexamples are provided.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:49:38 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Daviet", "Remi", ""]]}, {"id": "1812.08413", "submitter": "Michele Piana", "authors": "Anna Maria Massone, Federica Sciacchitano, Michele Piana, Alberto\n  Sorrentino", "title": "Compressed sensing and Sequential Monte Carlo for solar hard X-ray\n  imaging", "comments": "submitted to 'Nuovo Cimento' as proceeding SOHE3", "journal-ref": null, "doi": "10.1393/ncc/i2019-19031-0", "report-no": null, "categories": "astro-ph.SR math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two inversion methods for the reconstruction of hard X-ray solar\nimages. The methods are tested against experimental visibilities recorded by\nthe Reuven Ramaty High Energy Solar Spectroscopic Imager (RHESSI) and synthetic\nvisibilities based on the design of the Spectrometer/Telescope for Imaging\nX-rays (STIX).\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 08:30:43 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Massone", "Anna Maria", ""], ["Sciacchitano", "Federica", ""], ["Piana", "Michele", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1812.09063", "submitter": "Thorsten Dickhaus", "authors": "Jonathan von Schroeder and Thorsten Dickhaus", "title": "Efficient Calculation of the Joint Distribution of Order Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the joint distribution of order\nstatistics of stochastically independent random variables in one- and two-group\nmodels. While recursive formulas for evaluating the joint cumulative\ndistribution function of such order statistics exist in the literature for a\nlonger time, their numerical implementation remains a challenging task. We\ntackle this task by presenting novel generalizations of known recursions which\nwe utilize to obtain exact results (calculated in rational arithmetic) as well\nas faithfully rounded results. Finally, some applications in stepwise multiple\nhypothesis testing are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:43:07 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["von Schroeder", "Jonathan", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "1812.09064", "submitter": "Christopher Nemeth", "authors": "Jamie Fairbrother, Christopher Nemeth, Maxime Rischard, Johanni Brea,\n  Thomas Pinder", "title": "GaussianProcesses.jl: A Nonparametric Bayes package for the Julia\n  Language", "comments": "32 pages, 10 figures. Updated version includes sparse GPs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are a class of flexible nonparametric Bayesian tools that\nare widely used across the sciences, and in industry, to model complex data\nsources. Key to applying Gaussian process models is the availability of\nwell-developed open source software, which is available in many programming\nlanguages. In this paper, we present a tutorial of the GaussianProcesses.jl\npackage that has been developed for the Julia programming language.\nGaussianProcesses.jl utilises the inherent computational benefits of the Julia\nlanguage, including multiple dispatch and just-in-time compilation, to produce\na fast, flexible and user-friendly Gaussian processes package. The package\nprovides many mean and kernel functions with supporting inference tools to fit\nexact Gaussian process models, as well as a range of alternative likelihood\nfunctions to handle non-Gaussian data (e.g. binary classification models) and\nsparse approximations for scalable Gaussian processes. The package makes\nefficient use of existing Julia packages to provide users with a range of\noptimization and plotting tools.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:45:16 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 20:40:58 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Fairbrother", "Jamie", ""], ["Nemeth", "Christopher", ""], ["Rischard", "Maxime", ""], ["Brea", "Johanni", ""], ["Pinder", "Thomas", ""]]}, {"id": "1812.09384", "submitter": "Dootika Vats", "authors": "Dootika Vats and Christina Knudson", "title": "Revisiting the Gelman-Rubin Diagnostic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gelman and Rubin's (1992) convergence diagnostic is one of the most popular\nmethods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the\nseminal paper, researchers have developed sophisticated methods for estimating\nvariance of Monte Carlo averages. We show that these estimators find immediate\nuse in the Gelman-Rubin statistic, a connection not previously established in\nthe literature. We incorporate these estimators to upgrade both the univariate\nand multivariate Gelman-Rubin statistics, leading to improved stability in MCMC\ntermination time. An immediate advantage is that our new Gelman-Rubin statistic\ncan be calculated for a single chain. In addition, we establish a one-to-one\nrelationship between the Gelman-Rubin statistic and effective sample size.\nLeveraging this relationship, we develop a principled termination criterion for\nthe Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved\ndiagnostic via examples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 21:48:15 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 16:08:09 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 11:55:23 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Vats", "Dootika", ""], ["Knudson", "Christina", ""]]}, {"id": "1812.09424", "submitter": "Yuan-chin Ivan Chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Distributed sequential method for analyzing massive data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyse a very large data set containing lengthy variables, we adopt a\nsequential estimation idea and propose a parallel divide-and-conquer method. We\nconduct several conventional sequential estimation procedures separately, and\nproperly integrate their results while maintaining the desired statistical\nproperties. Additionally, using a criterion from the statistical experiment\ndesign, we adopt an adaptive sample selection, together with an adaptive\nshrinkage estimation method, to simultaneously accelerate the estimation\nprocedure and identify the effective variables. We confirm the cogency of our\nmethods through theoretical justifications and numerical results derived from\nsynthesized data sets. We then apply the proposed method to three real data\nsets, including those pertaining to appliance energy use and particulate matter\nconcentration.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 00:54:31 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1812.09583", "submitter": "Zunjing Wang", "authors": "Xiao-Feng Xie and Zunjing Jenipher Wang", "title": "Uncovering Urban Mobility and City Dynamics from Large-Scale Taxi\n  Origin-Destination (O-D) Trips: Case Study in Washington DC Area", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": "WIO-TR-18-003", "categories": "stat.CO cs.DS physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We perform a systematic analysis on the large-scale taxi trip data to uncover\nurban mobility and city dynamics in multimodal urban transportation\nenvironments. As a case study, we use the taxi origin-destination trip data and\nsome additional data sources in Washington DC area. We first study basic\ncharacteristics of taxi trips, then focus on five important aspects. Three of\nthem concern urban mobility, which are respectively mobility and cost including\neffect of traffic congestion, trip safety, and multimodal connectivity; the\nother two pertain to city dynamics, which are respectively transportation\nresilience and the relation between trip patterns and land use. For these\naspects, we use appropriate statistical methods and geographic techniques to\nmine patterns and characteristics from taxi trip data for better understanding\nqualitative and quantitative impacts of the inputs from key stakeholders on\navailable measures of effectiveness on urban mobility and city dynamics, where\nkey stakeholders include road users, system operators, and city. Finally, we\nbriefly summarize our findings and discuss some critical roles and implications\nof the uncovered patterns and characteristics from the relation between taxi\nsystem and key stakeholders. The results can support road users by providing\nevidence-based information of trip cost, mobility, safety, multimodal\nconnectivity and transportation resilience, can assist taxi drivers and\noperators to deliver transportation services in a higher quality of mobility,\nsafety and operational efficiency, and can also help city planners and policy\nmakers to transform multimodal transportation and to manage urban resources in\na more effective and better way.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 19:13:59 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Xie", "Xiao-Feng", ""], ["Wang", "Zunjing Jenipher", ""]]}, {"id": "1812.09587", "submitter": "Yury Maximov", "authors": "Valerii Likhosherstov, Yury Maximov, Michael Chertkov", "title": "Inference and Sampling of $K_{33}$-free Ising Models", "comments": "20 pages", "journal-ref": "36-th International Conference on Machine Learning, PMLR 97, 2019", "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call an Ising model tractable when it is possible to compute its partition\nfunction value (statistical inference) in polynomial time. The tractability\nalso implies an ability to sample configurations of this model in polynomial\ntime. The notion of tractability extends the basic case of planar zero-field\nIsing models. Our starting point is to describe algorithms for the basic case\ncomputing partition function and sampling efficiently. To derive the\nalgorithms, we use an equivalent linear transition to perfect matching counting\nand sampling on an expanded dual graph. Then, we extend our tractable inference\nand sampling algorithms to models, whose triconnected components are either\nplanar or graphs of $O(1)$ size. In particular, it results in a polynomial-time\ninference and sampling algorithms for $K_{33}$ (minor) free topologies of\nzero-field Ising models - a generalization of planar graphs with a potentially\nunbounded genus.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 19:32:44 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 19:43:47 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Likhosherstov", "Valerii", ""], ["Maximov", "Yury", ""], ["Chertkov", "Michael", ""]]}, {"id": "1812.09738", "submitter": "Whei Yeap Suen", "authors": "Whei Yeap Suen, Thomas J. Elliott, Jayne Thompson, Andrew J. P.\n  Garner, John R. Mahoney, Vlatko Vedral, Mile Gu", "title": "Surveying structural complexity in quantum many-body systems", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum many-body systems exhibit a rich and diverse range of exotic\nbehaviours, owing to their underlying non-classical structure. These systems\npresent a deep structure beyond those that can be captured by measures of\ncorrelation and entanglement alone. Using tools from complexity science, we\ncharacterise such structure. We investigate the structural complexities that\ncan be found within the patterns that manifest from the observational data of\nthese systems. In particular, using two prototypical quantum many-body systems\nas test cases - the one-dimensional quantum Ising and Bose-Hubbard models - we\nexplore how different information-theoretic measures of complexity are able to\nidentify different features of such patterns. This work furthers the\nunderstanding of fully-quantum notions of structure and complexity in quantum\nsystems and dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 16:23:55 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 13:30:58 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Suen", "Whei Yeap", ""], ["Elliott", "Thomas J.", ""], ["Thompson", "Jayne", ""], ["Garner", "Andrew J. P.", ""], ["Mahoney", "John R.", ""], ["Vedral", "Vlatko", ""], ["Gu", "Mile", ""]]}, {"id": "1812.09786", "submitter": "Ben Moews", "authors": "Ben Moews, Rafael S. de Souza, Emille E. O. Ishida, Alex I. Malz,\n  Caroline Heneka, Ricardo Vilalta, Joe Zuntz (for the COIN Collaboration)", "title": "Stress testing the dark energy equation of state imprint on supernova\n  data", "comments": "14 pages, 9 figures", "journal-ref": "Phys. Rev. D 99, 123529 (2019)", "doi": "10.1103/PhysRevD.99.123529", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work determines the degree to which a standard Lambda-CDM analysis based\non type Ia supernovae can identify deviations from a cosmological constant in\nthe form of a redshift-dependent dark energy equation of state w(z). We\nintroduce and apply a novel random curve generator to simulate instances of\nw(z) from constraint families with increasing distinction from a cosmological\nconstant. After producing a series of mock catalogs of binned type Ia\nsupernovae corresponding to each w(z) curve, we perform a standard Lambda-CDM\nanalysis to estimate the corresponding posterior densities of the absolute\nmagnitude of type Ia supernovae, the present-day matter density, and the\nequation of state parameter. Using the Kullback-Leibler divergence between\nposterior densities as a difference measure, we demonstrate that a standard\ntype Ia supernova cosmology analysis has limited sensitivity to extensive\nredshift dependencies of the dark energy equation of state. In addition, we\nreport that larger redshift-dependent departures from a cosmological constant\ndo not necessarily manifest easier-detectable incompatibilities with the\nLambda-CDM model. Our results suggest that physics beyond the standard model\nmay simply be hidden in plain sight.\n", "versions": [{"version": "v1", "created": "Sun, 23 Dec 2018 22:26:18 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 01:06:18 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Moews", "Ben", "", "for the COIN Collaboration"], ["de Souza", "Rafael S.", "", "for the COIN Collaboration"], ["Ishida", "Emille E. O.", "", "for the COIN Collaboration"], ["Malz", "Alex I.", "", "for the COIN Collaboration"], ["Heneka", "Caroline", "", "for the COIN Collaboration"], ["Vilalta", "Ricardo", "", "for the COIN Collaboration"], ["Zuntz", "Joe", "", "for the COIN Collaboration"]]}, {"id": "1812.09799", "submitter": "Merijn Mestdagh", "authors": "Merijn Mestdagh, Stijn Verdonck, Kristof Meers, Tim Loossens, Francis\n  Tuerlinckx", "title": "Prepaid parameter estimation without likelihoods", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1007181", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various fields, statistical models of interest are analytically\nintractable. As a result, statistical inference is greatly hampered by\ncomputational constraints. However, given a model, different users with\ndifferent data are likely to perform similar computations. Computations done by\none user are potentially useful for other users with different data sets. We\npropose a pooling of resources across researchers to capitalize on this. More\nspecifically, we preemptively chart out the entire space of possible model\noutcomes in a prepaid database. Using advanced interpolation techniques, any\nindividual estimation problem can now be solved on the spot. The prepaid method\ncan easily accommodate different priors as well as constraints on the\nparameters. We created prepaid databases for three challenging models and\ndemonstrate how they can be distributed through an online parameter estimation\nservice. Our method outperforms state-of-the-art estimation techniques in both\nspeed (with a 23,000 to 100,000-fold speed up) and accuracy, and is able to\nhandle previously quasi inestimable models.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 00:09:23 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mestdagh", "Merijn", ""], ["Verdonck", "Stijn", ""], ["Meers", "Kristof", ""], ["Loossens", "Tim", ""], ["Tuerlinckx", "Francis", ""]]}, {"id": "1812.09885", "submitter": "Christian P. Robert", "authors": "Gilles Celeux (INRIA), Sylvia Fruewirth-Schnatter (WU), and Christian\n  P. Robert (PSL)", "title": "Model Selection for Mixture Models - Perspectives and Strategies", "comments": "This is a preprint of a chapter in Handbook of Mixture Analysis\n  (2018), edited by Gilles Celeux, Sylvia Fruewirth-Schnatter, and Christian P.\n  Robert", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number G of components in a finite mixture distribution is an\nimportant and difficult inference issue. This is a most important question,\nbecause statistical inference about the resulting model is highly sensitive to\nthe value of G. Selecting an erroneous value of G may produce a poor density\nestimate. This is also a most difficult question from a theoretical perspective\nas it relates to unidentifiability issues of the mixture model. This is further\na most relevant question from a practical viewpoint since the meaning of the\nnumber of components G is strongly related to the modelling purpose of a\nmixture distribution. We distinguish in this chapter between selecting G as a\ndensity estimation problem in Section 2 and selecting G in a model-based\nclustering framework in Section 3. Both sections discuss frequentist as well as\nBayesian approaches. We present here some of the Bayesian solutions to the\ndifferent interpretations of picking the \"right\" number of components in a\nmixture, before concluding on the ill-posed nature of the question.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 10:45:09 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Celeux", "Gilles", "", "INRIA"], ["Fruewirth-Schnatter", "Sylvia", "", "WU"], ["Robert", "Christian P.", "", "PSL"]]}, {"id": "1812.10150", "submitter": "Somayeh Zarezadeh", "authors": "M. Siavashi and S. Zarezadeh", "title": "An Algorithm for computing the t-signature of two-state networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the importance of signature vector in studying the reliability of\nnetworks, some methods have been proposed by researchers to obtain the\nsignature. The notion of signature is used when at most one link may fail at\neach time instant. It is more realistic to consider the case where non of the\ncomponents, one component or more than one component of the network may be\ndestroyed at each time. Motivated by this, the concept of t-signature has been\nrecently defined to get the reliability of such a network. The t-signature is a\nprobability vector and depends only on the network structure. In this paper, we\npropose an algorithm to compute the t-signature. The performance of the\nproposed algorithm is evaluated for some networks.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 18:51:15 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Siavashi", "M.", ""], ["Zarezadeh", "S.", ""]]}, {"id": "1812.10612", "submitter": "Richard Arnold", "authors": "Richard Arnold", "title": "Sampling on the sphere from $f(x) \\propto x^TAx$", "comments": "6 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for drawing random samples of unit vectors $x$ in $R^p$ with density\nproportional to $x^TAx$ where $A$ is a symmetric, positive definite matrix.\nIncludes an R function which implements the method.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 03:40:12 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Arnold", "Richard", ""]]}, {"id": "1812.10939", "submitter": "Johan Westerborn Alenl\\\"ov", "authors": "Johan Alenl\\\"ov and Jimmy Olsson", "title": "Particle-based adaptive-lag online marginal smoothing in general\n  state-space models", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2019.2941066", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm, an adaptive-lag smoother, approximating\nefficiently, in an online fashion, sequences of expectations under the marginal\nsmoothing distributions in general state-space models. The algorithm evolves\nrecursively a bank of estimators, one for each marginal, in resemblance with\nthe so-called particle-based, rapid incremental smoother (PaRIS). Each\nestimator is propagated until a stopping criterion, measuring the fluctuations\nof the estimates, is met. The presented algorithm is furnished with theoretical\nresults describing its asymptotic limit and memory usage.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 09:53:29 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 19:32:56 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Alenl\u00f6v", "Johan", ""], ["Olsson", "Jimmy", ""]]}, {"id": "1812.11592", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "A Geometric Theory of Higher-Order Automatic Differentiation", "comments": "55 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order automatic differentiation is a ubiquitous tool across statistics,\nmachine learning, and computer science. Higher-order implementations of\nautomatic differentiation, however, have yet to realize the same utility. In\nthis paper I derive a comprehensive, differential geometric treatment of\nautomatic differentiation that naturally identifies the higher-order\ndifferential operators amenable to automatic differentiation as well as\nexplicit procedures that provide a scaffolding for high-performance\nimplementations.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 19:06:30 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1812.11689", "submitter": "Donghui Yan", "authors": "Donghui Yan, Yingjie Wang, Jin Wang, Honggang Wang and Zhenpeng Li", "title": "K-nearest Neighbor Search by Random Projection Forests", "comments": "15 pages, 4 figures, 2018 IEEE Big Data Conference", "journal-ref": "IEEE International Conference on Big Data, 2018", "doi": "10.1109/BigData.2018.8622307", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  K-nearest neighbor (kNN) search has wide applications in many areas,\nincluding data mining, machine learning, statistics and many applied domains.\nInspired by the success of ensemble methods and the flexibility of tree-based\nmethodology, we propose random projection forests (rpForests), for kNN search.\nrpForests finds kNNs by aggregating results from an ensemble of random\nprojection trees with each constructed recursively through a series of\ncarefully chosen random projections. rpForests achieves a remarkable accuracy\nin terms of fast decay in the missing rate of kNNs and that of discrepancy in\nthe kNN distances. rpForests has a very low computational complexity. The\nensemble nature of rpForests makes it easily run in parallel on multicore or\nclustered computers; the running time is expected to be nearly inversely\nproportional to the number of cores or machines. We give theoretical insights\nby showing the exponential decay of the probability that neighboring points\nwould be separated by ensemble random projection trees when the ensemble size\nincreases. Our theory can be used to refine the choice of random projections in\nthe growth of trees, and experiments show that the effect is remarkable.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 03:54:27 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Yan", "Donghui", ""], ["Wang", "Yingjie", ""], ["Wang", "Jin", ""], ["Wang", "Honggang", ""], ["Li", "Zhenpeng", ""]]}]