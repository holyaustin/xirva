[{"id": "2010.00408", "submitter": "Pierre Alquier", "authors": "Pierre Alquier, Badr-Eddine Ch\\'erief-Abdellatif, Alexis Derumigny,\n  Jean-David Fermanian", "title": "Estimation of copulas via Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with robust inference for parametric copula models.\nEstimation using Canonical Maximum Likelihood might be unstable, especially in\nthe presence of outliers. We propose to use a procedure based on the Maximum\nMean Discrepancy (MMD) principle. We derive non-asymptotic oracle inequalities,\nconsistency and asymptotic normality of this new estimator. In particular, the\noracle inequality holds without any assumption on the copula family, and can be\napplied in the presence of outliers or under misspecification. Moreover, in our\nMMD framework, the statistical inference of copula models for which there\nexists no density with respect to the Lebesgue measure on $[0,1]^d$, as the\nMarshall-Olkin copula, becomes feasible. A simulation study shows the\nrobustness of our new procedures, especially compared to pseudo-maximum\nlikelihood estimation. An R package implementing the MMD estimator for copula\nmodels is available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 13:50:17 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Alquier", "Pierre", ""], ["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "2010.00718", "submitter": "Byron Jaeger", "authors": "Byron C. Jaeger, Nicholas J. Tierney, Noah R. Simon", "title": "When to Impute? Imputation before and during cross-validation", "comments": "11 pages (main text, not including references), 6 tables, and 4\n  figures. Code to replicate manuscript available at\n  https://github.com/bcjaeger/Imputation-and-CV", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation (CV) is a technique used to estimate generalization error\nfor prediction models. For pipeline modeling algorithms (i.e. modeling\nprocedures with multiple steps), it has been recommended the entire sequence of\nsteps be carried out during each replicate of CV to mimic the application of\nthe entire pipeline to an external testing set. While theoretically sound,\nfollowing this recommendation can lead to high computational costs when a\npipeline modeling algorithm includes computationally expensive operations, e.g.\nimputation of missing values. There is a general belief that unsupervised\nvariable selection (i.e. ignoring the outcome) can be applied before conducting\nCV without incurring bias, but there is less consensus for unsupervised\nimputation of missing values. We empirically assessed whether conducting\nunsupervised imputation prior to CV would result in biased estimates of\ngeneralization error or result in poorly selected tuning parameters and thus\ndegrade the external performance of downstream models. Results show that\ndespite optimistic bias, the reduced variance of imputation before CV compared\nto imputation during each replicate of CV leads to a lower overall root mean\nsquared error for estimation of the true external R-squared and the performance\nof models tuned using CV with imputation before versus during each replication\nis minimally different. In conclusion, unsupervised imputation before CV\nappears valid in certain settings and may be a helpful strategy that enables\nanalysts to use more flexible imputation techniques without incurring high\ncomputational costs.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 23:04:16 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Jaeger", "Byron C.", ""], ["Tierney", "Nicholas J.", ""], ["Simon", "Noah R.", ""]]}, {"id": "2010.00794", "submitter": "Sayani Gupta", "authors": "Sayani Gupta, Rob J Hyndman, Dianne Cook and Antony Unwin", "title": "Visualizing probability distributions across bivariate cyclic temporal\n  granularities", "comments": "32 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconstructing a time index into time granularities can assist in exploration\nand automated analysis of large temporal data sets. This paper describes\nclasses of time deconstructions using linear and cyclic time granularities.\nLinear granularities respect the linear progression of time such as hours,\ndays, weeks and months. Cyclic granularities can be circular such as\nhour-of-the-day, quasi-circular such as day-of-the-month, and aperiodic such as\npublic holidays. The hierarchical structure of granularities creates a nested\nordering: hour-of-the-day and second-of-the-minute are single-order-up.\nHour-of-the-week is multiple-order-up, because it passes over day-of-the-week.\nMethods are provided for creating all possible granularities for a time index.\nA recommendation algorithm provides an indication whether a pair of\ngranularities can be meaningfully examined together (a \"harmony\"), or when they\ncannot (a \"clash\").\n  Time granularities can be used to create data visualizations to explore for\nperiodicities, associations and anomalies. The granularities form categorical\nvariables (ordered or unordered) which induce groupings of the observations.\nAssuming a numeric response variable, the resulting graphics are then displays\nof distributions compared across combinations of categorical variables.\n  The methods implemented in the open source R package `gravitas` are\nconsistent with a tidy workflow, with probability distributions examined using\nthe range of graphics available in `ggplot2`.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 05:47:43 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Gupta", "Sayani", ""], ["Hyndman", "Rob J", ""], ["Cook", "Dianne", ""], ["Unwin", "Antony", ""]]}, {"id": "2010.00896", "submitter": "Sebastien Coube", "authors": "S\\'ebastien Coube and Beno\\^it Liquet", "title": "Improving performances of MCMC for Nearest Neighbor Gaussian Process\n  models with full data augmentation", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Even though Nearest Neighbor Gaussian Processes (NNGP) alleviate considerably\nMCMC implementation of Bayesian space-time models, they do not solve the\nconvergence problems caused by high model dimension. Frugal alternatives such\nas response or collapsed algorithms are an answer.gree Our approach is to keep\nfull data augmentation but to try and make it more efficient. We present two\nstrategies to do so. The first scheme is to pay a particular attention to the\nseemingly trivial fixed effects of the model. We show empirically that\nre-centering the latent field on the intercept critically improves chain\nbehavior. We extend this approach to other fixed effects that may interfere\nwith a coherent spatial field. We propose a simple method that requires no\ntuning while remaining affordable thanks to NNGP's sparsity. The second scheme\naccelerates the sampling of the random field using Chromatic samplers. This\nmethod makes long sequential simulation boil down to group-parallelized or\ngroup-vectorized sampling. The attractive possibility to parallelize NNGP\nlikelihood can therefore be carried over to field sampling. We present a R\nimplementation of our methods for Gaussian fields in the public repository\nhttps://github.com/SebastienCoube/Improving_NNGP_full_augmentation . An\nextensive vignette is provided. We run our implementation on two synthetic toy\nexamples along with the state of the art package spNNGP. Finally, we apply our\nmethod on a real data set of lead contamination in the United States of America\nmainland.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 09:51:08 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 10:42:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Coube", "S\u00e9bastien", ""], ["Liquet", "Beno\u00eet", ""]]}, {"id": "2010.01084", "submitter": "Wei Deng", "authors": "Wei Deng and Qi Feng and Georgios Karagiannis and Guang Lin and Faming\n  Liang", "title": "Accelerating Convergence of Replica Exchange Stochastic Gradient MCMC\n  via Variance Reduction", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replica exchange stochastic gradient Langevin dynamics (reSGLD) has shown\npromise in accelerating the convergence in non-convex learning; however, an\nexcessively large correction for avoiding biases from noisy energy estimators\nhas limited the potential of the acceleration. To address this issue, we study\nthe variance reduction for noisy energy estimators, which promotes much more\neffective swaps. Theoretically, we provide a non-asymptotic analysis on the\nexponential acceleration for the underlying continuous-time Markov jump\nprocess; moreover, we consider a generalized Girsanov theorem which includes\nthe change of Poisson measure to overcome the crude discretization based on the\nGr\\\"{o}wall's inequality and yields a much tighter error in the 2-Wasserstein\n($\\mathcal{W}_2$) distance. Numerically, we conduct extensive experiments and\nobtain the state-of-the-art results in optimization and uncertainty estimates\nfor synthetic experiments and image data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 16:23:35 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:24:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Deng", "Wei", ""], ["Feng", "Qi", ""], ["Karagiannis", "Georgios", ""], ["Lin", "Guang", ""], ["Liang", "Faming", ""]]}, {"id": "2010.01297", "submitter": "Kim Phuc Tran", "authors": "K.D. Tran, Q.U.A Khaliq, A. A. Nadi, H Tran, and K.P. Tran", "title": "One-sided Shewhart control charts for monitoring the ratio of two normal\n  variables in Short Production Runs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring the ratio of two normal random variables plays an important role\nin several manufacturing environments. For short production runs, however, the\ncontrol charts assumed infinite processes cannot function effectively to detect\nanomalies. In this paper, we tackle this problem by proposing two one-sided\nShewhart-type charts to monitor the ratio of two normal random variables for an\ninfinite horizon production. The statistical performance of the proposed charts\nis investigated using the truncated average run length as a performance measure\nin short production runs. In order to help the quality practitioner to\nimplement these control charts, we have provided ready-to-use tables of the\ncontrol limit parameters. An illustrative example from the food industry is\ngiven for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 07:38:21 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tran", "K. D.", ""], ["Khaliq", "Q. U. A", ""], ["Nadi", "A. A.", ""], ["Tran", "H", ""], ["Tran", "K. P.", ""]]}, {"id": "2010.01482", "submitter": "HaiYing Wang", "authors": "Haim Bar and HaiYing Wang", "title": "Reproducible Science with LaTeX", "comments": null, "journal-ref": "J. data sci. 19(2021), no. 1, (2021) 111-125", "doi": "10.6339/21-JDS998", "report-no": null, "categories": "cs.SE cs.CL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a procedure to execute external source codes from a LaTeX\ndocument and include the calculation outputs in the resulting Portable Document\nFormat (pdf) file automatically. It integrates programming tools into the LaTeX\nwriting tool to facilitate the production of reproducible research. In our\nproposed approach to a LaTeX-based scientific notebook the user can easily\ninvoke any programming language or a command-line program when compiling the\nLaTeX document, while using their favorite LaTeX editor in the writing process.\nThe required LaTeX setup, a new Python package, and the defined preamble are\ndiscussed in detail, and working examples using R, Julia, and MatLab to\nreproduce existing research are provided to illustrate the proposed procedure.\nWe also demonstrate how to include system setting information in a paper by\ninvoking shell scripts when compiling the document.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 04:04:07 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 01:54:58 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim", ""], ["Wang", "HaiYing", ""]]}, {"id": "2010.01510", "submitter": "Maxime Vono", "authors": "Maxime Vono, Nicolas Dobigeon and Pierre Chainais", "title": "High-dimensional Gaussian sampling: a review and a unifying approach\n  based on a stochastic proximal point algorithm", "comments": "53 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient sampling from a high-dimensional Gaussian distribution is an old\nbut high-stake issue. Vanilla Cholesky samplers imply a computational cost and\nmemory requirements which can rapidly become prohibitive in high dimension. To\ntackle these issues, multiple methods have been proposed from different\ncommunities ranging from iterative numerical linear algebra to Markov chain\nMonte Carlo (MCMC) approaches. Surprisingly, no complete review and comparison\nof these methods have been conducted. This paper aims at reviewing all these\napproaches by pointing out their differences, close relations, benefits and\nlimitations. In addition to this state of the art, this paper proposes a\nunifying Gaussian simulation framework by deriving a stochastic counterpart of\nthe celebrated proximal point algorithm in optimization. This framework offers\na novel and unifying revisit of most of the existing MCMC approaches while\nextending them. Guidelines to choose the appropriate Gaussian simulation method\nfor a given sampling problem in high dimension are proposed and illustrated\nwith numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:29:32 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:04:43 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Vono", "Maxime", ""], ["Dobigeon", "Nicolas", ""], ["Chainais", "Pierre", ""]]}, {"id": "2010.01760", "submitter": "Daichi Mochihashi", "authors": "Daichi Mochihashi", "title": "Unbounded Slice Sampling", "comments": "Research Memorandum No.1209, The Institute of Statistical Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slice sampling is an efficient Markov Chain Monte Carlo algorithm to sample\nfrom an unnormalized density with acceptance ratio always $1$. However, when\nthe variable to sample is unbounded, its \"stepping-out\" heuristic works only\nlocally, making it difficult to uniformly explore possible candidates. This\npaper proposes a simple change-of-variable method to slice sample an unbounded\nvariable equivalently from [0,1).\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 03:38:45 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Mochihashi", "Daichi", ""]]}, {"id": "2010.01844", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein, Michael Stanley Smith, David J. Nott", "title": "Deep Distributional Time Series Models and the Probabilistic Forecasting\n  of Intraday Electricity Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) with rich feature vectors of past values can\nprovide accurate point forecasts for series that exhibit complex serial\ndependence. We propose two approaches to constructing deep time series\nprobabilistic models based on a variant of RNN called an echo state network\n(ESN). The first is where the output layer of the ESN has stochastic\ndisturbances and a shrinkage prior for additional regularization. The second\napproach employs the implicit copula of an ESN with Gaussian disturbances,\nwhich is a deep copula process on the feature space. Combining this copula with\na non-parametrically estimated marginal distribution produces a deep\ndistributional time series model. The resulting probabilistic forecasts are\ndeep functions of the feature vector and also marginally calibrated. In both\napproaches, Bayesian Markov chain Monte Carlo methods are used to estimate the\nmodels and compute forecasts. The proposed models are suitable for the complex\ntask of forecasting intraday electricity prices. Using data from the Australian\nNational Electricity Market, we show that our deep time series models provide\naccurate short term probabilistic price forecasts, with the copula model\ndominating. Moreover, the models provide a flexible framework for incorporating\nprobabilistic forecasts of electricity demand as additional features, which\nincreases upper tail forecast accuracy from the copula model significantly.\n", "versions": [{"version": "v1", "created": "Mon, 5 Oct 2020 08:02:29 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 11:17:24 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Klein", "Nadja", ""], ["Smith", "Michael Stanley", ""], ["Nott", "David J.", ""]]}, {"id": "2010.02411", "submitter": "Abd AlRahman AlMomani", "authors": "Abd AlRahman AlMomani and Erik Bollt", "title": "ERFit: Entropic Regression Fit Matlab Package, for Data-Driven System\n  Identification of Underlying Dynamic Equations", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.CL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven sparse system identification becomes the general framework for a\nwide range of problems in science and engineering. It is a problem of growing\nimportance in applied machine learning and artificial intelligence algorithms.\nIn this work, we developed the Entropic Regression Software Package (ERFit), a\nMATLAB package for sparse system identification using the entropic regression\nmethod. The code requires minimal supervision, with a wide range of options\nthat make it adapt easily to different problems in science and engineering. The\nERFit is available at https://github.com/almomaa/ERFit-Package\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 01:07:15 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["AlMomani", "Abd AlRahman", ""], ["Bollt", "Erik", ""]]}, {"id": "2010.02469", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Francis K.C. Hui, David I. Warton, and Trevor\n  Hastie", "title": "Generalized Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured or latent variables are often the cause of correlations between\nmultivariate measurements and are studied in a variety of fields such as\npsychology, ecology, and medicine. For Gaussian measurements, there are\nclassical tools such as factor analysis or principal component analysis with a\nwell-established theory and fast algorithms. Generalized Linear Latent Variable\nmodels (GLLVM) generalize such factor models to non-Gaussian responses.\nHowever, current algorithms for estimating model parameters in GLLVMs require\nintensive computation and do not scale to large datasets with thousands of\nobservational units or responses. In this article, we propose a new approach\nfor fitting GLLVMs to such high-volume, high-dimensional datasets. We\napproximate the likelihood using penalized quasi-likelihood and use a Newton\nmethod and Fisher scoring to learn the model parameters. Our method greatly\nreduces the computation time and can be easily parallelized, enabling\nfactorization at unprecedented scale using commodity hardware. We illustrate\napplication of our method on a dataset of 48,000 observational units with over\n2,000 observed species in each unit, finding that most of the variability can\nbe explained with a handful of factors.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 04:28:19 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hui", "Francis K. C.", ""], ["Warton", "David I.", ""], ["Hastie", "Trevor", ""]]}, {"id": "2010.02482", "submitter": "Anru R. Zhang", "authors": "Yuchen Zhou and Anru R. Zhang and Lili Zheng and Yazhen Wang", "title": "Optimal High-order Tensor SVD via Tensor-Train Orthogonal Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies a general framework for high-order tensor SVD. We propose\na new computationally efficient algorithm, tensor-train orthogonal iteration\n(TTOI), that aims to estimate the low tensor-train rank structure from the\nnoisy high-order tensor observation. The proposed TTOI consists of\ninitialization via TT-SVD (Oseledets, 2011) and new iterative backward/forward\nupdates. We develop the general upper bound on estimation error for TTOI with\nthe support of several new representation lemmas on tensor matricizations. By\ndeveloping a matching information-theoretic lower bound, we also prove that\nTTOI achieves the minimax optimality under the spiked tensor model. The merits\nof the proposed TTOI are illustrated through applications to estimation and\ndimension reduction of high-order Markov processes, numerical studies, and a\nreal data example on New York City taxi travel records. The software of the\nproposed algorithm is available online.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 05:18:24 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Zhou", "Yuchen", ""], ["Zhang", "Anru R.", ""], ["Zheng", "Lili", ""], ["Wang", "Yazhen", ""]]}, {"id": "2010.02848", "submitter": "Zhu Wang", "authors": "Zhu Wang", "title": "Unified Robust Estimation via the COCO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation is concerned with how to provide reliable parameter\nestimates in the presence of outliers. Numerous robust loss functions have been\nproposed in regression and classification, along with various computing\nalgorithms. This article proposes a unified framework for loss function\nconstruction and parameter estimation. The CC-family contains composite of\nconcave and convex functions. The properties of the CC-family are investigated,\nand CC-estimation is innovatively conducted via composite optimization by\nconjugation operator (COCO). The weighted estimators are simple to implement,\ndemonstrate robust quality in penalized generalized linear models and support\nvector machines, and can be conveniently extended to even more broad\napplications with existing software.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 16:10:51 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Wang", "Zhu", ""]]}, {"id": "2010.03053", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias, Jakub Sygnowski, Yutian Chen", "title": "Sequential Changepoint Detection in Neural Networks with Checkpoints", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for online changepoint detection and simultaneous\nmodel learning which is applicable to highly parametrized models, such as deep\nneural networks. It is based on detecting changepoints across time by\nsequentially performing generalized likelihood ratio tests that require only\nevaluations of simple prediction score functions. This procedure makes use of\ncheckpoints, consisting of early versions of the actual model parameters, that\nallow to detect distributional changes by performing predictions on future\ndata. We define an algorithm that bounds the Type I error in the sequential\ntesting procedure. We demonstrate the efficiency of our method in challenging\ncontinual learning applications with unknown task changepoints, and show\nimproved performance compared to online Bayesian changepoint detection.\n", "versions": [{"version": "v1", "created": "Tue, 6 Oct 2020 21:49:54 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Sygnowski", "Jakub", ""], ["Chen", "Yutian", ""]]}, {"id": "2010.03106", "submitter": "Kevin Tian", "authors": "Yin Tat Lee, Ruoqi Shen, Kevin Tian", "title": "Structured Logconcave Sampling with a Restricted Gaussian Oracle", "comments": "56 pages. The results of Section 5 of this paper, as well as an\n  empirical evaluation, appeared earlier as arXiv:2006.05976. Updated version\n  polishes exposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give algorithms for sampling several structured logconcave families to\nhigh accuracy. We further develop a reduction framework, inspired by proximal\npoint methods in convex optimization, which bootstraps samplers for regularized\ndensities to improve dependences on problem conditioning. A key ingredient in\nour framework is the notion of a \"restricted Gaussian oracle\" (RGO) for $g:\n\\mathbb{R}^d \\rightarrow \\mathbb{R}$, which is a sampler for distributions\nwhose negative log-likelihood sums a quadratic and $g$. By combining our\nreduction framework with our new samplers, we obtain the following bounds for\nsampling structured distributions to total variation distance $\\epsilon$. For\ncomposite densities $\\exp(-f(x) - g(x))$, where $f$ has condition number\n$\\kappa$ and convex (but possibly non-smooth) $g$ admits an RGO, we obtain a\nmixing time of $O(\\kappa d \\log^3\\frac{\\kappa d}{\\epsilon})$, matching the\nstate-of-the-art non-composite bound; no composite samplers with better mixing\nthan general-purpose logconcave samplers were previously known. For logconcave\nfinite sums $\\exp(-F(x))$, where $F(x) = \\frac{1}{n}\\sum_{i \\in [n]} f_i(x)$\nhas condition number $\\kappa$, we give a sampler querying $\\widetilde{O}(n +\n\\kappa\\max(d, \\sqrt{nd}))$ gradient oracles to $\\{f_i\\}_{i \\in [n]}$; no\nhigh-accuracy samplers with nontrivial gradient query complexity were\npreviously known. For densities with condition number $\\kappa$, we give an\nalgorithm obtaining mixing time $O(\\kappa d \\log^2\\frac{\\kappa d}{\\epsilon})$,\nimproving the prior state-of-the-art by a logarithmic factor with a\nsignificantly simpler analysis; we also show a zeroth-order algorithm attains\nthe same query complexity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 01:43:07 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 20:17:48 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 02:19:53 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Lee", "Yin Tat", ""], ["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""]]}, {"id": "2010.03485", "submitter": "Feras Saad", "authors": "Feras A. Saad, Martin C. Rinard, Vikash K. Mansinghka", "title": "SPPL: Probabilistic Programming with Fast Exact Symbolic Inference", "comments": null, "journal-ref": "Proceedings of the 42nd ACM SIGPLAN International Conference on\n  Programming Language Design and Implementation (PLDI '21), June 20-25, 2021,\n  Virtual, Canada. ACM, New York, NY, USA", "doi": "10.1145/3453483.3454078", "report-no": null, "categories": "cs.PL cs.LG cs.SC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Sum-Product Probabilistic Language (SPPL), a new probabilistic\nprogramming language that automatically delivers exact solutions to a broad\nrange of probabilistic inference queries. SPPL translates probabilistic\nprograms into sum-product expressions, a new symbolic representation and\nassociated semantic domain that extends standard sum-product networks to\nsupport mixed-type distributions, numeric transformations, logical formulas,\nand pointwise and set-valued constraints. We formalize SPPL via a novel\ntranslation strategy from probabilistic programs to sum-product expressions and\ngive sound exact algorithms for conditioning on and computing probabilities of\nevents. SPPL imposes a collection of restrictions on probabilistic programs to\nensure they can be translated into sum-product expressions, which allow the\nsystem to leverage new techniques for improving the scalability of translation\nand inference by automatically exploiting probabilistic structure. We implement\na prototype of SPPL with a modular architecture and evaluate it on benchmarks\nthe system targets, showing that it obtains up to 3500x speedups over\nstate-of-the-art symbolic systems on tasks such as verifying the fairness of\ndecision tree classifiers, smoothing hidden Markov models, conditioning\ntransformed random variables, and computing rare event probabilities.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 15:42:37 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 07:29:46 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 12:21:13 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Saad", "Feras A.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2010.03509", "submitter": "Moritz Schauer", "authors": "Frank van der Meulen and Moritz Schauer", "title": "Automatic Backward Filtering Forward Guiding for Markov processes and\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We incorporate discrete and continuous time Markov processes as building\nblocks into probabilistic graphical models with latent and observed variables.\nWe introduce the automatic Backward Filtering Forward Guiding (BFFG) paradigm\n(Mider et al., 2020) for programmable inference on latent states and model\nparameters. Our starting point is a generative model, a forward description of\nthe probabilistic process dynamics. We backpropagate the information provided\nby observations through the model to transform the generative (forward) model\ninto a pre-conditional model guided by the data. It approximates the actual\nconditional model with known likelihood-ratio between the two. The backward\nfilter and the forward change of measure are suitable to be incorporated into a\nprobabilistic programming context because they can be formulated as a set of\ntransformation rules.\n  The guided generative model can be incorporated in different approaches to\nefficiently sample latent states and parameters conditional on observations. We\nshow applicability in a variety of settings, including Markov chains with\ndiscrete state space, interacting particle systems, state space models,\nbranching diffusions and Gamma processes.\n", "versions": [{"version": "v1", "created": "Wed, 7 Oct 2020 16:30:13 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 16:50:29 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "2010.03828", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez and Mar\\'ia Durb\\'an and Paul\n  H.C. Eilers and Dae-Jin Lee and Francisco Gonzalez", "title": "Multidimensional Adaptive Penalised Splines with Application to Neurons'\n  Activity Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-spline models have achieved great popularity both in statistical and in\napplied research. A possible drawback of P-spline is that they assume a smooth\ntransition of the covariate effect across its whole domain. In some practical\napplications, however, it is desirable and needed to adapt smoothness locally\nto the data, and adaptive P-splines have been suggested. Yet, the extra\nflexibility afforded by adaptive P-spline models is obtained at the cost of a\nhigh computational burden, especially in a multidimensional setting.\nFurthermore, to the best of our knowledge, the literature lacks proposals for\nadaptive P-splines in more than two dimensions. Motivated by the need for\nanalysing data derived from experiments conducted to study neurons' activity in\nthe visual cortex, this work presents a novel locally adaptive anisotropic\nP-spline model in two (e.g., space) and three (space and time) dimensions.\nEstimation is based on the recently proposed SOP (Separation of Overlapping\nPrecision matrices) method, which provides the speed we look for. The practical\nperformance of the proposal is evaluated through simulations, and comparisons\nwith alternative methods are reported. In addition to the spatio-temporal\nanalysis of the data that motivated this work, we also discuss an application\nin two dimensions on the absenteeism of workers.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 08:05:23 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:03:46 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Durb\u00e1n", "Mar\u00eda", ""], ["Eilers", "Paul H. C.", ""], ["Lee", "Dae-Jin", ""], ["Gonzalez", "Francisco", ""]]}, {"id": "2010.04058", "submitter": "Stephane Robin", "authors": "St\\'ephane Robin and Luca Scrucca", "title": "Mixture-based estimation of entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entropy is a measure of uncertainty that plays a central role in\ninformation theory. When the distribution of the data is unknown, an estimate\nof the entropy needs be obtained from the data sample itself. We propose a\nsemi-parametric estimate, based on a mixture model approximation of the\ndistribution of interest. The estimate can rely on any type of mixture, but we\nfocus on Gaussian mixture model to demonstrate its accuracy and versatility.\nPerformance of the proposed approach is assessed through a series of simulation\nstudies. We also illustrate its use on two real-life data examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 15:23:27 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Robin", "St\u00e9phane", ""], ["Scrucca", "Luca", ""]]}, {"id": "2010.04133", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi and Eric C. Chi", "title": "A User-Friendly Computational Framework for Robust Structured Regression\n  Using the L$_2$ Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a user-friendly computational framework for implementing robust\nversions of a wide variety of structured regression methods using the L$_{2}$\ncriterion. In addition to introducing a scalable algorithm for performing\nL$_{2}$E regression, our framework also enables robust regression using the\nL$_{2}$ criterion for additional structural constraints, works without\nrequiring complex tuning procedures, can be used to automatically identify\nheterogeneous subpopulations, and can incorporate readily available non-robust\nstructured regression solvers. We provide convergence guarantees for the\nframework and demonstrate its flexibility with some examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Oct 2020 17:24:40 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Chi", "Eric C.", ""]]}, {"id": "2010.04680", "submitter": "Megan Murray", "authors": "Megan Hollister Murray and Jeffrey D. Blume", "title": "False Discovery Rate Computation: Illustrations and Modifications", "comments": "This article includes 18 pages and 9 figures. It is in process of\n  being submitted to \"The R Journal\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  False discovery rates (FDR) are an essential component of statistical\ninference, representing the propensity for an observed result to be mistaken.\nFDR estimates should accompany observed results to help the user contextualize\nthe relevance and potential impact of findings. This paper introduces a new\nuser-friendly R package for computing FDRs and adjusting p-values for FDR\ncontrol. These tools respect the critical difference between the adjusted\np-value and the estimated FDR for a particular finding, which are sometimes\nnumerically identical but are often confused in practice. Newly augmented\nmethods for estimating the null proportion of findings - an important part of\nthe FDR estimation procedure - are proposed and evaluated. The package is\nbroad, encompassing a variety of methods for FDR estimation and FDR control,\nand includes plotting functions for easy display of results. Through extensive\nillustrations, we strongly encourage wider reporting of false discovery rates\nfor observed findings.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 17:03:50 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Murray", "Megan Hollister", ""], ["Blume", "Jeffrey D.", ""]]}, {"id": "2010.05463", "submitter": "Francisco Leonardo Bezerra Martins", "authors": "Francisco Leonardo Bezerra Martins, Jos\\'e Cl\\'audio do Nascimento", "title": "Power law dynamics in genealogical graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NE physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several populational networks present complex topologies when implemented in\nevolutionary algorithms. A common feature of these topologies is the emergence\nof a power law. In genealogical networks, the power law can be observed by\nmeasuring the impact of individuals in the population, which can be calculated\nthrough the Event Takeover Value (ETV) algorithm. In this paper, we show\nevidence that the different power-law deviations, resulting from the ETV\ndistributions of genealogical graphs, are static images of a dynamic evolution\nthat can be well described by $q$-exponential distribution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 05:35:31 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 05:09:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Martins", "Francisco Leonardo Bezerra", ""], ["Nascimento", "Jos\u00e9 Cl\u00e1udio do", ""]]}, {"id": "2010.05619", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Anders Ellern Bilgrau, Wessel N. van Wieringen", "title": "rags2ridges: A One-Stop-Shop for Graphical Modeling of High-Dimensional\n  Precision Matrices", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A graphical model is an undirected network representing the conditional\nindependence properties between random variables. Graphical modeling has become\npart and parcel of systems or network approaches to multivariate data, in\nparticular when the variable dimension exceeds the observation dimension.\nrags2ridges is an R package for graphical modeling of high-dimensional\nprecision matrices. It provides a modular framework for the extraction,\nvisualization, and analysis of Gaussian graphical models from high-dimensional\ndata. Moreover, it can handle the incorporation of prior information as well as\nmultiple heterogeneous data classes. As such, it provides a one-stop-shop for\ngraphical modeling of high-dimensional precision matrices. The functionality of\nthe package is illustrated with an example dataset pertaining to blood-based\nmetabolite measurements in persons suffering from Alzheimer's Disease.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:43:54 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["Bilgrau", "Anders Ellern", ""], ["van Wieringen", "Wessel N.", ""]]}, {"id": "2010.05898", "submitter": "Maarten Bieshaar", "authors": "Maarten Bieshaar, Jens Schreiber, Stephan Vogt, Andr\\'e Gensler,\n  Bernhard Sick", "title": "Quantile Surfaces -- Generalizing Quantile Regression to Multivariate\n  Targets", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI), currently under review, 15 page, 23 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a novel approach to multivariate probabilistic\nforecasting. Our approach is based on an extension of single-output quantile\nregression (QR) to multivariate-targets, called quantile surfaces (QS). QS uses\na simple yet compelling idea of indexing observations of a probabilistic\nforecast through direction and vector length to estimate a central tendency. We\nextend the single-output QR technique to multivariate probabilistic targets. QS\nefficiently models dependencies in multivariate target variables and represents\nprobability distributions through discrete quantile levels. Therefore, we\npresent a novel two-stage process. In the first stage, we perform a\ndeterministic point forecast (i.e., central tendency estimation). Subsequently,\nwe model the prediction uncertainty using QS involving neural networks called\nquantile surface regression neural networks (QSNN). Additionally, we introduce\nnew methods for efficient and straightforward evaluation of the reliability and\nsharpness of the issued probabilistic QS predictions. We complement this by the\ndirectional extension of the Continuous Ranked Probability Score (CRPS) score.\nFinally, we evaluate our novel approach on synthetic data and two currently\nresearched real-world challenges in two different domains: First, probabilistic\nforecasting for renewable energy power generation, second, short-term cyclists\ntrajectory forecasting for autonomously driving vehicles. Especially for the\nlatter, our empirical results show that even a simple one-layer QSNN\noutperforms traditional parametric multivariate forecasting techniques, thus\nimproving the state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 16:35:37 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Bieshaar", "Maarten", ""], ["Schreiber", "Jens", ""], ["Vogt", "Stephan", ""], ["Gensler", "Andr\u00e9", ""], ["Sick", "Bernhard", ""]]}, {"id": "2010.06346", "submitter": "Benjamin Harroue", "authors": "Benjamin Harrou\\'e, Jean-Fran\\c{c}ois Giovannelli and Marcelo Pereyra", "title": "Bayesian model selection for unsupervised image deconvolution with\n  structured Gaussian priors", "comments": "5 pages,5 figures, conference paper, journal paper in processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the objective comparison of stochastic models to solve\ninverse problems, more specifically image restoration. Most often, model\ncomparison is addressed in a supervised manner, that can be time-consuming and\npartly arbitrary. Here we adopt an unsupervised Bayesian approach and\nobjectively compare the models based on their posterior probabilities, directly\nfrom the data without ground truth available. The probabilities depend on the\nmarginal likelihood or \"evidence\" of the models and we resort to the Chib\napproach including a Gibbs sampler. We focus on the family of Gaussian models\nwith circulant covariances and unknown hyperparameters, and compare different\ntypes of covariance matrices for the image and noise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 13:02:27 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Harrou\u00e9", "Benjamin", ""], ["Giovannelli", "Jean-Fran\u00e7ois", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2010.06465", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Karim Zouaoui-Boudjeltia, Christos Kotsalos,\n  Alexandre Rousseau, Daniel Ribeiro de Sousa, Jean-Marc Desmet, Alain Van\n  Meerhaeghe, Antonietta Mira, Bastien Chopard", "title": "Interpretable pathological test for Cardio-vascular disease: Approximate\n  Bayesian computation with distance learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardio/cerebrovascular diseases (CVD) have become one of the major health\nissue in our societies. But recent studies show that the present clinical tests\nto detect CVD are ineffectual as they do not consider different stages of\nplatelet activation or the molecular dynamics involved in platelet interactions\nand are incapable to consider inter-individual variability. Here we propose a\nstochastic platelet deposition model and an inferential scheme for uncertainty\nquantification of these parameters using Approximate Bayesian Computation and\ndistance learning. Finally we show that our methodology can learn biologically\nmeaningful parameters, which are the specific dysfunctioning parameters in each\ntype of patients, from data collected from healthy volunteers and patients.\nThis work opens up an unprecedented opportunity of personalized pathological\ntest for CVD detection and medical treatment. Also our proposed methodology can\nbe used to other fields of science where we would need machine learning tools\nto be interpretable.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 15:20:21 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Zouaoui-Boudjeltia", "Karim", ""], ["Kotsalos", "Christos", ""], ["Rousseau", "Alexandre", ""], ["de Sousa", "Daniel Ribeiro", ""], ["Desmet", "Jean-Marc", ""], ["Van Meerhaeghe", "Alain", ""], ["Mira", "Antonietta", ""], ["Chopard", "Bastien", ""]]}, {"id": "2010.06583", "submitter": "Philipp Frank", "authors": "Philipp Frank and Torsten A. En{\\ss}lin", "title": "Probabilistic simulation of partial differential equations", "comments": "13 pages, 4 figures, submitted to Physical Review E", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA astro-ph.IM cs.NA physics.comp-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulations of differential equations require a time discretization,\nwhich inhibits to identify the exact solution with certainty. Probabilistic\nsimulations take this into account via uncertainty quantification. The\nconstruction of a probabilistic simulation scheme can be regarded as Bayesian\nfiltering by means of probabilistic numerics. Gaussian prior based filters,\nspecifically Gauss-Markov priors, have successfully been applied to simulation\nof ordinary differential equations (ODEs) and give rise to filtering problems\nthat can be solved efficiently. This work extends this approach to partial\ndifferential equations (PDEs) subject to periodic boundary conditions and\nutilizes continuous Gaussian processes in space and time to arrive at a\nBayesian filtering problem structurally similar to the ODE setting. The usage\nof a process that is Markov in time and statistically homogeneous in space\nleads to a probabilistic spectral simulation method that allows for an\nefficient realization. Furthermore, the Bayesian perspective allows the\nincorporation of methods developed within the context of information field\ntheory such as the estimation of the power spectrum associated with the prior\ndistribution, to be jointly estimated along with the solution of the PDE.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 10:07:51 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Frank", "Philipp", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "2010.06738", "submitter": "David Gunawan", "authors": "David Gunawan and Robert Kohn and David Nott", "title": "Variational Approximation of Factor Stochastic Volatility Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation and prediction in high dimensional multivariate factor stochastic\nvolatility models is an important and active research area because such models\nallow a parsimonious representation of multivariate stochastic volatility.\nBayesian inference for factor stochastic volatility models is usually done by\nMarkov chain Monte Carlo methods, often by particle Markov chain Monte Carlo,\nwhich are usually slow for high dimensional or long time series because of the\nlarge number of parameters and latent states involved. Our article makes two\ncontributions. The first is to propose fast and accurate variational Bayes\nmethods to approximate the posterior distribution of the states and parameters\nin factor stochastic volatility models. The second contribution is to extend\nthis batch methodology to develop fast sequential variational updates for\nprediction as new observations arrive. The methods are applied to simulated and\nreal datasets and shown to produce good approximate inference and prediction\ncompared to the latest particle Markov chain Monte Carlo approaches, but are\nmuch faster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 23:37:58 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 11:41:18 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Nott", "David", ""]]}, {"id": "2010.06840", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Ian Hunter", "title": "Rapid Generation of Stochastic Signals with Specified Statistics", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate a novel algorithm for generating stationary stochastic signals\nwith a specified power spectral density (or equivalently, via the\nWiener-Khinchin relation, a specified autocorrelation function) while\nsatisfying constraints on the signal's probability density function. A tightly\nrelated problem has already been essentially solved by methods involving\nnonlinear filtering, however we use a fundamentally different approach\ninvolving optimization and stochastic interchange which immediately generalizes\nto generating signals with a broader range of statistics. This combination of\noptimization and stochastic interchange eliminates drawbacks associated with\neither method in isolation, improving the best-case scaling in runtime to\ngenerate a signal of length $n$ from $\\mathcal{O}(n^2)$ for stochastic\ninterchange on its own to $\\mathcal{O}(n \\: \\text{log} \\: n)$ without\nparallelization or $\\mathcal{O}(n)$ with full parallelization. We demonstrate\nthis speedup experimentally, and furthermore show that the signals we generate\nmatch the desired autocorrelation more accurately than those generated by\nstochastic interchange on its own. We observe that the signals we produce,\nunlike those generated by optimization on its own, are stationary.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 06:58:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Spanbauer", "Span", ""], ["Hunter", "Ian", ""]]}, {"id": "2010.06889", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Florian Pfisterer and Bernd Bischl", "title": "Neural Mixture Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present neural mixture distributional regression (NMDR), a holistic\nframework to estimate complex finite mixtures of distributional regressions\ndefined by flexible additive predictors. Our framework is able to handle a\nlarge number of mixtures of potentially different distributions in\nhigh-dimensional settings, allows for efficient and scalable optimization and\ncan be applied to recent concepts that combine structured regression models\nwith deep neural networks. While many existing approaches for mixture models\naddress challenges in optimization of such and provide results for convergence\nunder specific model assumptions, our approach is assumption-free and instead\nmakes use of optimizers well-established in deep learning. Through extensive\nnumerical experiments and a high-dimensional deep learning application we\nprovide evidence that the proposed approach is competitive to existing\napproaches and works well in more complex scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 09:00:16 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Pfisterer", "Florian", ""], ["Bischl", "Bernd", ""]]}, {"id": "2010.06937", "submitter": "Martin Tveten", "authors": "Martin Tveten, Idris A. Eckley, Paul Fearnhead", "title": "Scalable changepoint and anomaly detection in cross-correlated data with\n  an application to condition monitoring", "comments": "48 pages, 25 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a condition monitoring application arising from subsea\nengineering we derive a novel, scalable approach to detecting anomalous mean\nstructure in a subset of correlated multivariate time series. Given the need to\nanalyse such series efficiently we explore a computationally efficient\napproximation of the maximum likelihood solution to the resulting modelling\nframework, and develop a new dynamic programming algorithm for solving the\nresulting Binary Quadratic Programme when the precision matrix of the time\nseries at any given time-point is banded. Through a comprehensive simulation\nstudy, we show that the resulting methods perform favourably compared to\ncompeting methods both in the anomaly and change detection settings, even when\nthe sparsity structure of the precision matrix estimate is misspecified. We\nalso demonstrate its ability to correctly detect faulty time-periods of a pump\nwithin the motivating application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 10:37:03 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 18:49:09 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Tveten", "Martin", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2010.06994", "submitter": "Alejandro Catalina Feli\\'u", "authors": "Alejandro Catalina, Paul-Christian B\\\"urkner, Aki Vehtari", "title": "Projection Predictive Inference for Generalized Linear and Additive\n  Multilevel Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection predictive inference is a decision theoretic Bayesian approach\nthat decouples model estimation from decision making. Given a reference model\npreviously built including all variables present in the data, projection\npredictive inference projects its posterior onto a constrained space of a\nsubset of variables. Variable selection is then performed by sequentially\nadding relevant variables until predictive performance is satisfactory.\nPreviously, projection predictive inference has been demonstrated only for\ngeneralized linear models (GLMs) and Gaussian processes (GPs) where it showed\nsuperior performance to competing variable selection procedures. In this work,\nwe extend projection predictive inference to support variable and structure\nselection for generalized linear multilevel models (GLMMs) and generalized\nadditive multilevel models (GAMMs). Our simulative and real-word experiments\ndemonstrate that our method can drastically reduce the model complexity\nrequired to reach reference predictive performance and achieve good frequency\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:14:38 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Catalina", "Alejandro", ""], ["B\u00fcrkner", "Paul-Christian", ""], ["Vehtari", "Aki", ""]]}, {"id": "2010.07064", "submitter": "Onur Teymur", "authors": "Onur Teymur, Jackson Gorham, Marina Riabiz and Chris. J. Oates", "title": "Optimal quantisation of probability measures using maximum mean\n  discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have proposed minimisation of maximum mean discrepancy\n(MMD) as a method to quantise probability measures, i.e., to approximate a\ntarget distribution by a representative point set. We consider sequential\nalgorithms that greedily minimise MMD over a discrete candidate set. We propose\na novel non-myopic algorithm and, in order to both improve statistical\nefficiency and reduce computational cost, we investigate a variant that applies\nthis technique to a mini-batch of the candidate set at each iteration. When the\ncandidate points are sampled from the target, the consistency of these new\nalgorithm - and their mini-batch variants - is established. We demonstrate the\nalgorithms on a range of important computational problems, including\noptimisation of nodes in Bayesian cubature and the thinning of Markov chain\noutput.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 13:09:48 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 16:03:37 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 09:43:56 GMT"}, {"version": "v4", "created": "Fri, 12 Feb 2021 11:40:18 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Teymur", "Onur", ""], ["Gorham", "Jackson", ""], ["Riabiz", "Marina", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2010.07991", "submitter": "Pavel Loskot", "authors": "Pavel Loskot", "title": "Polynomial Representations of High-Dimensional Observations of Random\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper investigates the problem of performing correlation analysis when\nthe number of observations is very large. In such a case, it is often necessary\nto combine the random observations to achieve dimensionality reduction of the\nproblem. A novel class of statistical measures is obtained by approximating the\nTaylor expansion of a general multivariate scalar function by a univariate\npolynomial in the variable given as a simple sum of the original random\nvariables. The mean value of the polynomial is then a weighted sum of\nstatistical central sum-moments with the weights being application dependent.\nComputing the sum-moments is computationally efficient and amenable to\nmathematical analysis, provided that the distribution of the sum of random\nvariables can be obtained. Among several auxiliary results also obtained, the\nfirst order sum-moments corresponding to sample means are used to reduce the\nnumerical complexity of linear regression by partitioning the data into\ndisjoint subsets. Illustrative examples are provided assuming the first and the\nsecond order Markov processes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 19:21:57 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Loskot", "Pavel", ""]]}, {"id": "2010.08047", "submitter": "Kirill Neklyudov", "authors": "Kirill Neklyudov, Max Welling", "title": "Orbital MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex\ndeterministic transformations to generate proposal points that are then\nfiltered by the Metropolis-Hastings-Green (MHG) test. However, the condition of\nthe target measure invariance puts restrictions on the design of these\ntransformations. In this paper, we first derive the acceptance test for the\nstochastic Markov kernel considering arbitrary deterministic maps as proposal\ngenerators. When applied to the transformations with orbits of period two\n(involutions), the test reduces to the MHG test. Based on the derived test we\npropose two practical algorithms: one operates by constructing periodic orbits\nfrom any diffeomorphism, another on contractions of the state space (such as\noptimization trajectories). Finally, we perform an empirical study\ndemonstrating the practical advantages of both kernels.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 22:25:52 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:12:30 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Neklyudov", "Kirill", ""], ["Welling", "Max", ""]]}, {"id": "2010.08134", "submitter": "Aditya Mishra", "authors": "Aditya Mishra, Dipak K. Dey, Yong Chen, Kun Chen", "title": "Generalized Co-sparse Factor Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate regression techniques are commonly applied to explore the\nassociations between large numbers of outcomes and predictors. In real-world\napplications, the outcomes are often of mixed types, including continuous\nmeasurements, binary indicators, and counts, and the observations may also be\nincomplete. Building upon the recent advances in mixed-outcome modeling and\nsparse matrix factorization, generalized co-sparse factor regression (GOFAR) is\nproposed, which utilizes the flexible vector generalized linear model framework\nand encodes the outcome dependency through a sparse singular value\ndecomposition (SSVD) of the integrated natural parameter matrix. To avoid the\nestimation of the notoriously difficult joint SSVD, GOFAR proposes both\nsequential and parallel unit-rank estimation procedures. By combining the ideas\nof alternating convex search and majorization-minimization, an efficient\nalgorithm with guaranteed convergence is developed to solve the sparse\nunit-rank problem and implemented in the R package gofar. Extensive simulation\nstudies and two real-world applications demonstrate the effectiveness of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 03:39:16 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Mishra", "Aditya", ""], ["Dey", "Dipak K.", ""], ["Chen", "Yong", ""], ["Chen", "Kun", ""]]}, {"id": "2010.08317", "submitter": "Matheus Saldanha", "authors": "Matheus Henrique Junqueira Saldanha and Adriano Kamimura Suzuki", "title": "Methods to Deal with Unknown Populational Minima during Parameter\n  Inference", "comments": "Submitted to Springer's Statistics and Computing. 14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a myriad of phenomena that are better modelled with semi-infinite\ndistribution families, many of which are studied in survival analysis. When\nperforming inference, lack of knowledge of the populational minimum becomes a\nproblem, which can be dealt with by making a good guess thereof, or by\nhandcrafting a grid of initial parameters that will be useful for that\nparticular problem. These solutions are fine when analyzing a single set of\nsamples, but it becomes unfeasible when there are multiple datasets and a\ncase-by-case analysis would be too time consuming. In this paper we propose\nmethods to deal with the populational minimum in algorithmic, efficient and/or\nsimple ways. Six methods are presented and analyzed, two of which have full\ntheoretical support, but lack simplicity. The other four are simple and have\nsome theoretical grounds in non-parametric results such as the law of iterated\nlogarithm, and they exhibited very good results when it comes to maximizing\nlikelihood and being able to recycle the grid of initial parameters among the\ndatasets. With our results, we hope to ease the inference process for\npractitioners, and expect that these methods will eventually be included in\nsoftware packages themselves.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 11:15:18 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Saldanha", "Matheus Henrique Junqueira", ""], ["Suzuki", "Adriano Kamimura", ""]]}, {"id": "2010.08495", "submitter": "Fan Yin", "authors": "Fan Yin, Guanyu Hu, Weining Shen", "title": "Analysis of professional basketball field goal attempts via a Bayesian\n  matrix clustering approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric matrix clustering approach to analyze the\nlatent heterogeneity structure in the shot selection data collected from\nprofessional basketball players in the National Basketball Association (NBA).\nThe proposed method adopts a mixture of finite mixtures framework and fully\nutilizes the spatial information via a mixture of matrix normal distribution\nrepresentation. We propose an efficient Markov chain Monte Carlo algorithm for\nposterior sampling that allows simultaneous inference on both the number of\nclusters and the cluster configurations. We also establish large sample\nconvergence properties for the posterior distribution. The excellent empirical\nperformance of the proposed method is demonstrated via simulation studies and\nan application to shot chart data from selected players in the 2017 18 NBA\nregular season.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 16:53:09 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yin", "Fan", ""], ["Hu", "Guanyu", ""], ["Shen", "Weining", ""]]}, {"id": "2010.08509", "submitter": "Stephen Walker", "authors": "Yanxin Li, Stephen G. Walker", "title": "A Latent Slice Sampling Algorithm", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new sampling algorithm which has the potential\nto be adopted as a universal replacement to the Metropolis--Hastings algorithm.\nIt is related to the slice sampler, and motivated by an algorithm which is\napplicable to discrete probability distributions %which can be viewed as an\nalternative to the Metropolis--Hastings algorithm in this setting, which\nobviates the need for a proposal distribution, in that is has no accept/reject\ncomponent. This paper looks at the continuous counterpart. A latent variable\ncombined with a slice sampler and a shrinkage procedure applied to uniform\ndensity functions creates a highly efficient sampler which can generate random\nvariables from very high dimensional distributions as a single block.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 17:08:15 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Li", "Yanxin", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2010.08573", "submitter": "John O'Leary", "authors": "John O'Leary, Guanyang Wang, Pierre E. Jacob", "title": "Maximal couplings of the Metropolis-Hastings algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Couplings play a central role in the analysis of Markov chain Monte Carlo\nalgorithms and appear increasingly often in the algorithms themselves, e.g. in\nconvergence diagnostics, parallelization, and variance reduction techniques.\nExisting couplings of the Metropolis-Hastings algorithm handle the proposal and\nacceptance steps separately and fall short of the upper bound on one-step\nmeeting probabilities given by the coupling inequality. This paper introduces\nmaximal couplings which achieve this bound while retaining the practical\nadvantages of current methods. We consider the properties of these couplings\nand examine their behavior on a selection of numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 18:12:05 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["O'Leary", "John", ""], ["Wang", "Guanyang", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "2010.08627", "submitter": "Qiuyun Zhu", "authors": "Qiuyun Zhu, Yves Atchade", "title": "Minimax Quasi-Bayesian estimation in sparse canonical correlation\n  analysis via a Rayleigh quotient function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a popular statistical technique for\nexploring the relationship between datasets. The estimation of sparse canonical\ncorrelation vectors has emerged in recent years as an important but challenging\nvariation of the CCA problem, with widespread applications. Currently available\nrate-optimal estimators for sparse canonical correlation vectors are expensive\nto compute. We propose a quasi-Bayesian estimation procedure that achieves the\nminimax estimation rate, and yet is easy to compute by Markov Chain Monte Carlo\n(MCMC). The method builds on ([37]) and uses a re-scaled Rayleigh quotient\nfunction as a quasi-log-likelihood. However unlike these authors, we adopt a\nBayesian framework that combines this quasi-log-likelihood with a\nspike-and-slab prior that serves to regularize the inference and promote\nsparsity. We investigated the empirical behavior of the proposed method on both\ncontinuous and truncated data, and we noted that it outperforms several\nstate-of-the-art methods. As an application, we use the methodology to\nmaximally correlate clinical variables and proteomic data for a better\nunderstanding of covid-19 disease.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 21:00:57 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 16:52:56 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Zhu", "Qiuyun", ""], ["Atchade", "Yves", ""]]}, {"id": "2010.08729", "submitter": "Tsuyoshi Ishizone", "authors": "Tsuyoshi Ishizone, Tomoyuki Higuchi, Kazuyuki Nakamura", "title": "Ensemble Kalman Variational Objectives: Nonlinear Latent Trajectory\n  Inference with A Hybrid of Variational Inference and Ensemble Kalman Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Inference (VI) combined with Bayesian nonlinear filtering\nproduces the state-of-the-art results for latent trajectory inference. A body\nof recent works focused on Sequential Monte Carlo (SMC) and its expansion,\ne.g., Forward Filtering Backward Simulation (FFBSi). These studies achieved a\ngreat success, however, remain a serious problem for particle degeneracy. In\nthis paper, we propose Ensemble Kalman Objectives (EnKOs), the hybrid method of\nVI and Ensemble Kalman Filter (EnKF), to infer the State Space Models (SSMs).\nUnlike the SMC based methods, the our proposed method can identify the latent\ndynamics given fewer particles because of its rich particle diversity. We\ndemonstrate that EnKOs outperform the SMC based methods in terms of predictive\nability for three benchmark nonlinear dynamics systems tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 07:01:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Ishizone", "Tsuyoshi", ""], ["Higuchi", "Tomoyuki", ""], ["Nakamura", "Kazuyuki", ""]]}, {"id": "2010.08893", "submitter": "Tianhui Zhou", "authors": "Tianhui Zhou, Guangyu Tong, Fan Li, Laine E. Thomas, Fan Li", "title": "PSweight: An R Package for Propensity Score Weighting Analysis", "comments": "18 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Propensity score weighting is an important tool for comparative effectiveness\nresearch.Besides the inverse probability of treatment weights (IPW), recent\ndevelopment has introduced a general class of balancing weights, corresponding\nto alternative target populations and estimands. In particular, the overlap\nweights (OW) lead to optimal covariate balance and estimation efficiency, and a\ntarget population of scientific and policy interest. We develop the R package\nPSweight to provide a comprehensive design and analysis platform for causal\ninference based on propensity score weighting. PSweight supports (i) a variety\nof balancing weights, (ii) binary and multiple treatments,(iii) simple and\naugmented weighting estimators, (iv) nuisance-adjusted sandwich variances,\nand(v) ratio estimands. PSweight also provides diagnostic tables and graphs for\ncovariate balance assessment. We demonstrate the functionality of the package\nusing a data example from the NationalChild Development Survey (NCDS), where we\nevaluate the causal effect of educational attainment on income.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 00:28:18 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 01:06:54 GMT"}, {"version": "v3", "created": "Fri, 12 Mar 2021 02:21:07 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 02:50:43 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Zhou", "Tianhui", ""], ["Tong", "Guangyu", ""], ["Li", "Fan", ""], ["Thomas", "Laine E.", ""], ["Li", "Fan", ""]]}, {"id": "2010.08908", "submitter": "Michael Minyi Zhang", "authors": "Lizhen Lin, Bayan Saparbayeva, Michael Minyi Zhang, David B. Dunson", "title": "Accelerated Algorithms for Convex and Non-Convex Optimization on\n  Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general scheme for solving convex and non-convex optimization\nproblems on manifolds. The central idea is that, by adding a multiple of the\nsquared retraction distance to the objective function in question, we\n\"convexify\" the objective function and solve a series of convex sub-problems in\nthe optimization procedure. One of the key challenges for optimization on\nmanifolds is the difficulty of verifying the complexity of the objective\nfunction, e.g., whether the objective function is convex or non-convex, and the\ndegree of non-convexity. Our proposed algorithm adapts to the level of\ncomplexity in the objective function. We show that when the objective function\nis convex, the algorithm provably converges to the optimum and leads to\naccelerated convergence. When the objective function is non-convex, the\nalgorithm will converge to a stationary point. Our proposed method unifies\ninsights from Nesterov's original idea for accelerating gradient descent\nalgorithms with recent developments in optimization algorithms in Euclidean\nspace. We demonstrate the utility of our algorithms on several manifold\noptimization tasks such as estimating intrinsic and extrinsic Fr\\'echet means\non spheres and low-rank matrix factorization with Grassmann manifolds applied\nto the Netflix rating data set.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 02:48:22 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Lin", "Lizhen", ""], ["Saparbayeva", "Bayan", ""], ["Zhang", "Michael Minyi", ""], ["Dunson", "David B.", ""]]}, {"id": "2010.08941", "submitter": "Joseph Resch", "authors": "Joseph Resch, Abhyuday Mandal, Pritam Ranjan", "title": "Inverse Problem for Dynamic Computer Simulators via Multiple\n  Scalar-valued Contour Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider a dynamic computer simulator that produces a\ntime-series response $y_t(x)$ over $L$ time points, for every given input\nparameter $x$. We propose a method for solving inverse problems, which refer to\nthe finding of a set of inputs that generates a pre-specified simulator output.\nInspired by the sequential approach of contour estimation via expected\nimprovement criterion developed by Ranjan et al. (2008, DOI:\n10.1198/004017008000000541), our proposed method discretizes the target\nresponse series on $k \\; (\\ll L)$ time points, and then iteratively solves $k$\nscalar-valued inverse problems with respect to the discretized targets. We also\npropose to use spline smoothing of the target response series to identify the\noptimal number of knots, $k$, and the actual location of the knots for\ndiscretization. The performance of the proposed methods is compared for several\ntest-function based computer simulators and the motivating real application\nthat uses a rainfall-runoff measurement model named Matlab-Simulink model.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 08:28:12 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 04:50:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Resch", "Joseph", ""], ["Mandal", "Abhyuday", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2010.09154", "submitter": "Hongzhi Wang", "authors": "Hongzhi Wang, Qian Xiao and Abhyuday Mandal", "title": "Musings about Constructions of Efficient Latin Hypercube Designs with\n  Flexible Run-sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient Latin hypercube designs (LHDs), including maximin distance LHDs,\nmaximum projection LHDs and orthogonal LHDs, are widely used in computer\nexperiments. It is challenging to construct such designs with flexible sizes,\nespecially for large ones. In the current literature, various algebraic methods\nand search algorithms have been proposed for identifying efficient LHDs, each\nhaving its own pros and cons. In this paper, we review, summarize and compare\nsome currently popular methods aiming to provide guidance for experimenters on\nwhat method should be used in practice. Using the R package we developed which\nintegrates and improves various algebraic and searching methods, many of the\ndesigns found in this paper are better than the existing ones. They are easy to\nuse for practitioners and can serve as benchmarks for the future developments\non LHDs.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 00:40:10 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 21:39:56 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Wang", "Hongzhi", ""], ["Xiao", "Qian", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "2010.09188", "submitter": "Xiu Yang", "authors": "Muqing Zheng, Ang Li, Tam\\'as Terlaky, Xiu Yang", "title": "A Bayesian Approach for Characterizing and Mitigating Gate and\n  Measurement Errors", "comments": "Updated the introduction and the description of methodology in the\n  new version", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various noise models have been developed in quantum computing study to\ndescribe the propagation and effect of the noise which is caused by imperfect\nimplementation of hardware. Identifying parameters such as gate and readout\nerror rates are critical to these models. We use a Bayesian inference approach\nto identity posterior distributions of these parameters, such that they can be\ncharacterized more elaborately. By characterizing the device errors in this\nway, we can further improve the accuracy of quantum error mitigation.\nExperiments conducted on IBM's quantum computing devices suggest that our\napproach provides better error mitigation performance than existing techniques\nused by the vendor. Also, our approach outperforms the standard Bayesian\ninference method in such experiments.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 03:27:28 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 13:10:28 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zheng", "Muqing", ""], ["Li", "Ang", ""], ["Terlaky", "Tam\u00e1s", ""], ["Yang", "Xiu", ""]]}, {"id": "2010.09267", "submitter": "Touboul Adrien", "authors": "Julien Reygner (CERMICS, GdR MASCOT-NUM), Adrien Touboul (CERMICS, IRT\n  SystemX)", "title": "Reweighting samples under covariate shift using a Wasserstein distance\n  criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering two random variables with different laws to which we only have\naccess through finite size iid samples, we address how to reweight the first\nsample so that its empirical distribution converges towards the true law of the\nsecond sample as the size of both samples goes to infinity. We study an optimal\nreweighting that minimizes the Wasserstein distance between the empirical\nmeasures of the two samples, and leads to an expression of the weights in terms\nof Nearest Neighbors. The consistency and some asymptotic convergence rates in\nterms of expected Wasserstein distance are derived, and do not need the\nassumption of absolute continuity of one random variable with respect to the\nother. These results have some application in Uncertainty Quantification for\ndecoupled estimation and in the bound of the generalization error for the\nNearest Neighbor Regression under covariate shift.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 07:23:55 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Reygner", "Julien", "", "CERMICS, GdR MASCOT-NUM"], ["Touboul", "Adrien", "", "CERMICS, IRT\n  SystemX"]]}, {"id": "2010.09335", "submitter": "Damjan Vukcevic", "authors": "Jeffrey Pullin, Lyle Gurrin, Damjan Vukcevic", "title": "Statistical Models of Repeated Categorical Ratings: The R package rater", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common occurrence in many disciplines is the need to assign a set of items\ninto categories or classes with known labels. This is often done by one or more\nexpert raters, or sometimes by an automated process. If these assignments, or\n'ratings', are difficult to do, a common tactic is to repeat them by different\nraters, or even by the same rater multiple times on different occasions.\n  We present an R package, rater, available on CRAN, that implements Bayesian\nversions of several statistical models that allow analysis of repeated\ncategorical rating data. Inference is possible for the true underlying (latent)\nclass of each item, as well as the accuracy of each rater.\n  The models are based on, and include, the Dawid-Skene model. We use the Stan\nprobabilistic programming language as the main computational engine.\n  We illustrate usage of rater through a few examples. We also discuss in\ndetail the techniques of marginalisation and conditioning, which are necessary\nfor these models but also apply more generally to other models implemented in\nStan.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 09:17:40 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 09:21:06 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 12:54:53 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Pullin", "Jeffrey", ""], ["Gurrin", "Lyle", ""], ["Vukcevic", "Damjan", ""]]}, {"id": "2010.09712", "submitter": "Chaim Even-Zohar", "authors": "Chaim Even-Zohar", "title": "independence: Fast Rank Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1948 Hoeffding devised a nonparametric test that detects dependence\nbetween two continuous random variables X and Y, based on the ranking of n\npaired samples (Xi,Yi). The computation of this commonly-used test statistic\ntakes O(n log n) time. Hoeffding's test is consistent against any dependent\nprobability density f(x,y), but can be fooled by other bivariate distributions\nwith continuous margins. Variants of this test with full consistency have been\nconsidered by Blum, Kiefer, and Rosenblatt (1961), Yanagimoto (1970), Bergsma\nand Dassios (2010). The so far best known algorithms to compute these stronger\nindependence tests have required quadratic time. Here we improve their run time\nto O(n log n), by elaborating on new methods for counting ranking patterns,\nfrom a recent paper by the author and Leng (SODA'21). Therefore, in all\ncircumstances under which the classical Hoeffding independence test is\napplicable, we provide novel competitive algorithms for consistent testing\nagainst all alternatives. Our R package, independence, offers a highly\noptimized implementation of these rank-based tests. We demonstrate its\ncapabilities on large-scale datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 17:59:24 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 21:32:47 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Even-Zohar", "Chaim", ""]]}, {"id": "2010.09800", "submitter": "Wei Deng", "authors": "Wei Deng and Guang Lin and Faming Liang", "title": "A Contour Stochastic Gradient Langevin Dynamics Algorithm for\n  Simulations of Multi-modal Distributions", "comments": "Accepted by NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adaptively weighted stochastic gradient Langevin dynamics\nalgorithm (SGLD), so-called contour stochastic gradient Langevin dynamics\n(CSGLD), for Bayesian learning in big data statistics. The proposed algorithm\nis essentially a \\emph{scalable dynamic importance sampler}, which\nautomatically \\emph{flattens} the target distribution such that the simulation\nfor a multi-modal distribution can be greatly facilitated. Theoretically, we\nprove a stability condition and establish the asymptotic convergence of the\nself-adapting parameter to a {\\it unique fixed-point}, regardless of the\nnon-convexity of the original energy function; we also present an error\nanalysis for the weighted averaging estimators. Empirically, the CSGLD\nalgorithm is tested on multiple benchmark datasets including CIFAR10 and\nCIFAR100. The numerical results indicate its superiority over the existing\nstate-of-the-art algorithms in training deep neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:20:47 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Deng", "Wei", ""], ["Lin", "Guang", ""], ["Liang", "Faming", ""]]}, {"id": "2010.09879", "submitter": "Changbao Wu", "authors": "Hon Yiu So, Urun Erbas Oz, Lauren Griffith, Susan Kirkland, Jinhua Ma,\n  Parminder Raina, Nazmul Sohel, Mary E. Thompson, Christina Wolfson and\n  Changbao Wu", "title": "Modelling Complex Survey Data Using R, SAS, SPSS and Stata: A Comparison\n  Using CLSA Datasets", "comments": "There is a data usage issue with the paper and it is requested by the\n  data owner (CLSA) to withdraw the paper at this time", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R software has become popular among researchers due to its flexibility\nand open-source nature. However, researchers in the fields of public health and\nepidemiological studies are more customary to commercial statistical softwares\nsuch as SAS, SPSS and Stata. This paper provides a comprehensive comparison on\nanalysis of health survey data using the R survey package, SAS, SPSS and Stata.\nWe describe detailed R codes and procedures for other software packages on\ncommonly encountered statistical analyses, such as estimation of population\nmeans and regression analysis, using datasets from the Canadian Longitudinal\nStudy on Aging (CLSA). It is hoped that the paper stimulates interest among\nhealth science researchers to carry data analysis using R and also serves as a\ncookbook for statistical analysis using different software packages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 21:28:23 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 16:10:52 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["So", "Hon Yiu", ""], ["Oz", "Urun Erbas", ""], ["Griffith", "Lauren", ""], ["Kirkland", "Susan", ""], ["Ma", "Jinhua", ""], ["Raina", "Parminder", ""], ["Sohel", "Nazmul", ""], ["Thompson", "Mary E.", ""], ["Wolfson", "Christina", ""], ["Wu", "Changbao", ""]]}, {"id": "2010.10194", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Lorenz Haubner, Axel Munk, Peter B\\\"uhlmann", "title": "Optimistic search strategy: Change point detection for large-scale data\n  via adaptive logarithmic queries", "comments": "extended Table 1; added Model II and Lemma 5.3; added further minor\n  explanations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a classical and ever reviving topic, change point detection is often\nformulated as a search for the maximum of a gain function describing improved\nfits when segmenting the data. Searching through all candidate split points on\nthe grid for finding the best one requires $O(T)$ evaluations of the gain\nfunction for an interval with $T$ observations. If each evaluation is\ncomputationally demanding (e.g. in high-dimensional models), this can become\ninfeasible. Instead, we propose optimistic search strategies with $O(\\log T)$\nevaluations exploiting specific structure of the gain function.\n  Towards solid understanding of our strategies, we investigate in detail the\nclassical univariate Gaussian change in mean setup. For some of our proposals\nwe prove asymptotic minimax optimality for single and multiple change point\nscenarios. Our search strategies generalize far beyond the theoretically\nanalyzed univariate setup. We illustrate, as an example, massive computational\nspeedup in change point detection for high-dimensional Gaussian graphical\nmodels. More generally, we demonstrate empirically that our optimistic search\nmethods lead to competitive estimation performance while heavily reducing\nrun-time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 11:09:52 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 23:50:55 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["Haubner", "Lorenz", ""], ["Munk", "Axel", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2010.10275", "submitter": "Ryan Martin", "authors": "Vaidehi Dixit and Ryan Martin", "title": "Estimating a mixing distribution on the sphere using predictive\n  recursion", "comments": "26 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are commonly used when data show signs of heterogeneity and,\noften, it is important to estimate the distribution of the latent variable\nresponsible for that heterogeneity. This is a common problem for data taking\nvalues in a Euclidean space, but the work on mixing distribution estimation\nbased on directional data taking values on the unit sphere is limited. In this\npaper, we propose using the predictive recursion (PR) algorithm to solve for a\nmixture on a sphere. One key feature of PR is its computational efficiency.\nMoreover, compared to likelihood-based methods that only support finite mixing\ndistribution estimates, PR is able to estimate a smooth mixing density. PR's\nasymptotic consistency in spherical mixture models is established, and\nsimulation results showcase its benefits compared to existing likelihood-based\nmethods. We also show two real-data examples to illustrate how PR can be used\nfor goodness-of-fit testing and clustering.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 13:46:06 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Dixit", "Vaidehi", ""], ["Martin", "Ryan", ""]]}, {"id": "2010.10346", "submitter": "Fernando Llorente Fern\\'andez", "authors": "F. Llorente, L. Martino, D. Delgado, G. Camps-Valls", "title": "Deep Importance Sampling based on Regression for Model Inversion and\n  Emulation", "comments": null, "journal-ref": null, "doi": "10.1016/j.dsp.2021.103104", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding systems by forward and inverse modeling is a recurrent topic of\nresearch in many domains of science and engineering. In this context, Monte\nCarlo methods have been widely used as powerful tools for numerical inference\nand optimization. They require the choice of a suitable proposal density that\nis crucial for their performance. For this reason, several adaptive importance\nsampling (AIS) schemes have been proposed in the literature. We here present an\nAIS framework called Regression-based Adaptive Deep Importance Sampling\n(RADIS). In RADIS, the key idea is the adaptive construction via regression of\na non-parametric proposal density (i.e., an emulator), which mimics the\nposterior distribution and hence minimizes the mismatch between proposal and\ntarget densities. RADIS is based on a deep architecture of two (or more) nested\nIS schemes, in order to draw samples from the constructed emulator. The\nalgorithm is highly efficient since employs the posterior approximation as\nproposal density, which can be improved adding more support points. As a\nconsequence, RADIS asymptotically converges to an exact sampler under mild\nconditions. Additionally, the emulator produced by RADIS can be in turn used as\na cheap surrogate model for further studies. We introduce two specific RADIS\nimplementations that use Gaussian Processes (GPs) and Nearest Neighbors (NN)\nfor constructing the emulator. Several numerical experiments and comparisons\nshow the benefits of the proposed schemes. A real-world application in remote\nsensing model inversion and emulation confirms the validity of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 15:12:30 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 18:46:18 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Llorente", "F.", ""], ["Martino", "L.", ""], ["Delgado", "D.", ""], ["Camps-Valls", "G.", ""]]}, {"id": "2010.10660", "submitter": "Housen Li", "authors": "Miguel del Alamo and Housen Li and Axel Munk and Frank Werner", "title": "Variational Multiscale Nonparametric Regression: Algorithms and\n  Implementation", "comments": "Codes are available at https://github.com/housenli/MIND", "journal-ref": "Algorithms 2020, 13(11), 296", "doi": "10.3390/a13110296", "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistically efficient methods come with tremendous\ncomputational challenges, often leading to large-scale optimisation problems.\nIn this work, we examine such computational issues for recently developed\nestimation methods in nonparametric regression with a specific view on image\ndenoising. We consider in particular certain variational multiscale estimators\nwhich are statistically optimal in minimax sense, yet computationally\nintensive. Such an estimator is computed as the minimiser of a smoothness\nfunctional (e.g., TV norm) over the class of all estimators such that none of\nits coefficients with respect to a given multiscale dictionary is statistically\nsignificant. The so obtained multiscale Nemirowski-Dantzig estimator (MIND) can\nincorporate any convex smoothness functional and combine it with a proper\ndictionary including wavelets, curvelets and shearlets. The computation of MIND\nin general requires to solve a high-dimensional constrained convex optimisation\nproblem with a specific structure of the constraints induced by the statistical\nmultiscale testing criterion. To solve this explicitly, we discuss three\ndifferent algorithmic approaches: the Chambolle-Pock, ADMM and semismooth\nNewton algorithms. Algorithmic details and an explicit implementation is\npresented and the solutions are then compared numerically in a simulation study\nand on various test images. We thereby recommend the Chambolle-Pock algorithm\nin most cases for its fast convergence. We stress that our analysis can also be\ntransferred to signal recovery and other denoising problems to recover more\ngeneral objects whenever it is possible to borrow statistical strength from\ndata patches of similar object structure.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 22:52:05 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 15:30:48 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["del Alamo", "Miguel", ""], ["Li", "Housen", ""], ["Munk", "Axel", ""], ["Werner", "Frank", ""]]}, {"id": "2010.10698", "submitter": "Jianhui Ning", "authors": "Jianhui Ning and Yao Xiao and Zikang Xiong", "title": "Batch Sequential Adaptive Designs for Global Optimization", "comments": "20Pages, 4 Figures, 8 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared with the fixed-run designs, the sequential adaptive designs (SAD)\nare thought to be more efficient and effective. Efficient global optimization\n(EGO) is one of the most popular SAD methods for expensive black-box\noptimization problems. A well-recognized weakness of the original EGO in\ncomplex computer experiments is that it is serial, and hence the modern\nparallel computing techniques cannot be utilized to speed up the running of\nsimulator experiments. For those multiple points EGO methods, the heavy\ncomputation and points clustering are the obstacles. In this work, a novel\nbatch SAD method, named \"accelerated EGO\", is forwarded by using a refined\nsampling/importance resampling (SIR) method to search the points with large\nexpected improvement (EI) values. The computation burden of the new method is\nmuch lighter, and the points clustering is also avoided. The efficiency of the\nproposed SAD is validated by nine classic test functions with dimension from 2\nto 12. The empirical results show that the proposed algorithm indeed can\nparallelize original EGO, and gain much improvement compared against the other\nparallel EGO algorithm especially under high-dimensional case. Additionally, we\nalso apply the new method to the hyper-parameter tuning of Support Vector\nMachine (SVM). Accelerated EGO obtains comparable cross validation accuracy\nwith other methods and the CPU time can be reduced a lot due to the parallel\ncomputation and sampling method.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 01:11:35 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Ning", "Jianhui", ""], ["Xiao", "Yao", ""], ["Xiong", "Zikang", ""]]}, {"id": "2010.10992", "submitter": "Anay Mehrotra", "authors": "L. Elisa Celis, Chris Hays, Anay Mehrotra, Nisheeth K. Vishnoi", "title": "The Effect of the Rooney Rule on Implicit Bias in the Long Term", "comments": "Abstract shortened to satisfy the 1920 character limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust body of evidence demonstrates the adverse effects of implicit bias\nin various contexts--from hiring to health care. The Rooney Rule is an\nintervention developed to counter implicit bias and has been implemented in the\nprivate and public sectors. The Rooney Rule requires that a selection panel\ninclude at least one candidate from an underrepresented group in their\nshortlist of candidates. Recently, Kleinberg and Raghavan proposed a model of\nimplicit bias and studied the effectiveness of the Rooney Rule when applied to\na single selection decision. However, selection decisions often occur\nrepeatedly over time. Further, it has been observed that, given consistent\ncounterstereotypical feedback, implicit biases against underrepresented\ncandidates can change.\n  We consider a model of how a selection panel's implicit bias changes over\ntime given their hiring decisions either with or without the Rooney Rule in\nplace. Our main result is that, when the panel is constrained by the Rooney\nRule, their implicit bias roughly reduces at a rate that is the inverse of the\nsize of the shortlist--independent of the number of candidates, whereas without\nthe Rooney Rule, the rate is inversely proportional to the number of\ncandidates. Thus, when the number of candidates is much larger than the size of\nthe shortlist, the Rooney Rule enables a faster reduction in implicit bias,\nproviding an additional reason in favor of using it as a strategy to mitigate\nimplicit bias. Towards empirically evaluating the long-term effect of the\nRooney Rule in repeated selection decisions, we conduct an iterative candidate\nselection experiment on Amazon MTurk. We observe that, indeed, decision-makers\nsubject to the Rooney Rule select more minority candidates in addition to those\nrequired by the rule itself than they would if no rule is in effect, and do so\nwithout considerably decreasing the utility of candidates selected.\n", "versions": [{"version": "v1", "created": "Wed, 21 Oct 2020 13:33:00 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Celis", "L. Elisa", ""], ["Hays", "Chris", ""], ["Mehrotra", "Anay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "2010.11771", "submitter": "Paul Fearnhead", "authors": "Augustin Chevallier, Paul Fearnhead, Matthew Sutton", "title": "Reversible Jump PDMP Samplers for Variable Selection", "comments": "Code available from https://github.com/matt-sutton/rjpdmp", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of Markov chain Monte Carlo (MCMC) algorithms, based on\nsimulating piecewise deterministic Markov processes (PDMPs), have recently\nshown great promise: they are non-reversible, can mix better than standard MCMC\nalgorithms, and can use subsampling ideas to speed up computation in big data\nscenarios. However, current PDMP samplers can only sample from posterior\ndensities that are differentiable almost everywhere, which precludes their use\nfor model choice. Motivated by variable selection problems, we show how to\ndevelop reversible jump PDMP samplers that can jointly explore the discrete\nspace of models and the continuous space of parameters. Our framework is\ngeneral: it takes any existing PDMP sampler, and adds two types of\ntrans-dimensional moves that allow for the addition or removal of a variable\nfrom the model. We show how the rates of these trans-dimensional moves can be\ncalculated so that the sampler has the correct invariant distribution.\nSimulations show that the new samplers can mix better than standard MCMC\nalgorithms. Our empirical results show they are also more efficient than\ngradient-based samplers that avoid model choice through use of continuous\nspike-and-slab priors which replace a point mass at zero for each parameter\nwith a density concentrated around zero.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:46:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chevallier", "Augustin", ""], ["Fearnhead", "Paul", ""], ["Sutton", "Matthew", ""]]}, {"id": "2010.11779", "submitter": "Matthew Fisher Mr", "authors": "Matthew A. Fisher, Tui Nolan, Matthew M. Graham, Dennis Prangle, Chris\n  J. Oates", "title": "Measure Transport with Kernel Stein Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measure transport underpins several recent algorithms for posterior\napproximation in the Bayesian context, wherein a transport map is sought to\nminimise the Kullback--Leibler divergence (KLD) from the posterior to the\napproximation. The KLD is a strong mode of convergence, requiring absolute\ncontinuity of measures and placing restrictions on which transport maps can be\npermitted. Here we propose to minimise a kernel Stein discrepancy (KSD)\ninstead, requiring only that the set of transport maps is dense in an $L^2$\nsense and demonstrating how this condition can be validated. The consistency of\nthe associated posterior approximation is established and empirical results\nsuggest that KSD is competitive and more flexible alternative to KLD for\nmeasure transport.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 14:55:47 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 12:08:15 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fisher", "Matthew A.", ""], ["Nolan", "Tui", ""], ["Graham", "Matthew M.", ""], ["Prangle", "Dennis", ""], ["Oates", "Chris J.", ""]]}, {"id": "2010.12089", "submitter": "Jordan Purdy", "authors": "Jordan Purdy and Brian Glass", "title": "The Pursuit of Algorithmic Fairness: On \"Correcting\" Algorithmic\n  Unfairness in a Child Welfare Reunification Success Classifier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithmic fairness of predictive analytic tools in the public sector\nhas increasingly become a topic of rigorous exploration. While instruments\npertaining to criminal recidivism and academic admissions, for example, have\ngarnered much attention, the predictive instruments of Child Welfare\njurisdictions have received considerably less attention. This is in part\nbecause comparatively few such instruments exist and because even fewer have\nbeen scrutinized through the lens of algorithmic fairness. In this work, we\nseek to address both of these gaps. To this end, a novel classification\nalgorithm for predicting reunification success within Oregon Child Welfare is\npresented, including all of the relevant details associated with building such\nan instrument. The purpose of this tool is to maximize the number of stable\nreunifications and identify potentially unstable reunifications which may\nrequire additional resources and scrutiny. Additionally, because the\nalgorithmic fairness of the resulting tool, if left unaltered, is\nunquestionably lacking, the utilized procedure for mitigating such unfairness\nis presented, along with the rationale behind each difficult and unavoidable\nchoice. This procedure, though similar to other post-processing group-specific\nthresholding methods, is novel in its use of a penalized optimizer and\ncontextually requisite subsampling. These novel methodological components yield\na rich and informative empirical understanding of the trade-off continuum\nbetween fairness and accuracy. As the developed procedure is generalizable\nacross a variety of group-level definitions of algorithmic fairness, as well as\nacross an arbitrary number of protected attribute levels and risk thresholds,\nthe approach is broadly applicable both within and beyond Child Welfare.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 22:07:43 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Purdy", "Jordan", ""], ["Glass", "Brian", ""]]}, {"id": "2010.12334", "submitter": "Anthony Coolen", "authors": "ACC Coolen and T Nikoletopoulos", "title": "Dynamical replica analysis of quantum annealing", "comments": "21 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum annealing aims to provide a faster method for finding the minima of\ncomplicated functions, compared to classical computing, so there is an\nincreasing interest in the relaxation dynamics of quantum spin systems.\nMoreover, it is known that problems in quantum annealing caused by first order\nphase transitions can be reduced via appropriate temporal adjustment of control\nparameters, aimed at steering the system away from local minima. To do this\noptimally, it would be helpful to predict the evolution of the system at the\nlevel of macroscopic observables. Solving the dynamics of a quantum ensemble is\nnontrivial, as it requires modelling not just the quantum spin system itself\nbut also its interaction with the environment, with which it exchanges energy.\nAn interesting alternative approach to the dynamics of quantum spin systems was\nproposed about a decade ago. It involves creating a stochastic proxy dynamics\nvia the Suzuki-Trotter mapping of the quantum ensemble to a classical one (the\nquantum Monte Carlo method), and deriving from this new dynamics closed\nmacroscopic equations for macroscopic observables, using the dynamical replica\nmethod. In this chapter we give an introduction to this approach, focusing on\nthe ideas and assumptions behind the derivations, and on its potential and\nlimitations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 12:17:38 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Coolen", "ACC", ""], ["Nikoletopoulos", "T", ""]]}, {"id": "2010.12514", "submitter": "James Johndrow", "authors": "James E. Johndrow, Natesh S. Pillai, Aaron Smith", "title": "No Free Lunch for Approximate MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely known that the performance of Markov chain Monte Carlo (MCMC)\ncan degrade quickly when targeting computationally expensive posterior\ndistributions, such as when the sample size is large. This has motivated the\nsearch for MCMC variants that scale well to large datasets. One general\napproach has been to look at only a subsample of the data at every step. In\nthis note, we point out that well-known MCMC convergence results often imply\nthat these \"subsampling\" MCMC algorithms cannot greatly improve performance. We\napply these generic results to realistic statistical problems and proposed\nalgorithms, and also discuss some design principles suggested by the results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:32:28 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Johndrow", "James E.", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "2010.12522", "submitter": "Christophe Ley", "authors": "Fatemeh Ghaderinezhad, Christophe Ley and Ben Serrien", "title": "The Wasserstein Impact Measure (WIM): a generally applicable, practical\n  tool for quantifying prior impact in Bayesian statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prior distribution is a crucial building block in Bayesian analysis, and\nits choice will impact the subsequent inference. It is therefore important to\nhave a convenient way to quantify this impact, as such a measure of prior\nimpact will help us to choose between two or more priors in a given situation.\nA recently proposed approach consists in determining the Wasserstein distance\nbetween posteriors resulting from two distinct priors, revealing how close or\ndistant they are. In particular, if one prior is the uniform/flat prior, this\ndistance leads to a genuine measure of prior impact for the other prior. While\nhighly appealing and successful from a theoretical viewpoint, this proposal\nsuffers from practical limitations: it requires prior distributions to be\nnested, posterior distributions should not be of a too complex form, in most\nconsidered settings the exact distance was not computed but sharp upper and\nlower bounds were proposed, and the proposal so far is restricted to scalar\nparameter settings. In this paper, we overcome all these limitations by\nintroducing a practical version of this theoretical approach, namely the\nWasserstein Impact Measure (WIM). In three simulated scenarios, we will compare\nthe WIM to the theoretical Wasserstein approach, as well as to two competitor\nprior impact measures from the literature. We finally illustrate the\nversatility of the WIM by applying it on two datasets.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:42:35 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Ghaderinezhad", "Fatemeh", ""], ["Ley", "Christophe", ""], ["Serrien", "Ben", ""]]}, {"id": "2010.12568", "submitter": "Devin Johnson", "authors": "Devin S. Johnson, Brian M. Brost and Mevin B. Hooten", "title": "Greater Than the Sum of its Parts: Computationally Flexible Bayesian\n  Hierarchical Modeling", "comments": "32 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We propose a multistage method for making inference at all levels of a\nBayesian hierarchical model (BHM) using natural data partitions to increase\nefficiency by allowing computations to take place in parallel form using\nsoftware that is most appropriate for each data partition. The full\nhierarchical model is then approximated by the product of independent normal\ndistributions for the data component of the model. In the second stage, the\nBayesian maximum {\\it a posteriori} (MAP) estimator is found by maximizing the\napproximated posterior density with respect to the parameters. If the\nparameters of the model can be represented as normally distributed random\neffects then the second stage optimization is equivalent to fitting a\nmultivariate normal linear mixed model. This method can be extended to account\nfor common fixed parameters shared between data partitions, as well as\nparameters that are distinct between partitions. In the case of distinct\nparameter estimation, we consider a third stage that re-estimates the distinct\nparameters for each data partition based on the results of the second stage.\nThis allows more information from the entire data set to properly inform the\nposterior distributions of the distinct parameters. The method is demonstrated\nwith two ecological data sets and models, a random effects GLM and an\nIntegrated Population Model (IPM). The multistage results were compared to\nestimates from models fit in single stages to the entire data set. Both\nexamples demonstrate that multistage point and posterior standard deviation\nestimates closely approximate those obtained from fitting the models with all\ndata simultaneously and can therefore be considered for fitting hierarchical\nBayesian models when it is computationally prohibitive to do so in one step.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 17:53:56 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Johnson", "Devin S.", ""], ["Brost", "Brian M.", ""], ["Hooten", "Mevin B.", ""]]}, {"id": "2010.12833", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis, Simon Michael Papalexiou,\n  Andreas Langousis, Sina Khatami, Elena Volpi, Salvatore Grimaldi", "title": "Global-scale massive feature extraction from monthly hydroclimatic time\n  series: Statistical characterizations, spatial patterns and hydrological\n  similarity", "comments": null, "journal-ref": "Science of the Total Environment 767 (2021) 144612", "doi": "10.1016/j.scitotenv.2020.144612", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hydroclimatic time series analysis focuses on a few feature types (e.g.,\nautocorrelations, trends, extremes), which describe a small portion of the\nentire information content of the observations. Aiming to exploit a larger part\nof the available information and, thus, to deliver more reliable results (e.g.,\nin hydroclimatic time series clustering contexts), here we approach\nhydroclimatic time series analysis differently, i.e., by performing massive\nfeature extraction. In this respect, we develop a big data framework for\nhydroclimatic variable behaviour characterization. This framework relies on\napproximately 60 diverse features and is completely automatic (in the sense\nthat it does not depend on the hydroclimatic process at hand). We apply the new\nframework to characterize mean monthly temperature, total monthly precipitation\nand mean monthly river flow. The applications are conducted at the global scale\nby exploiting 40-year-long time series originating from over 13 000 stations.\nWe extract interpretable knowledge on seasonality, trends, autocorrelation,\nlong-range dependence and entropy, and on feature types that are met less\nfrequently. We further compare the examined hydroclimatic variable types in\nterms of this knowledge and, identify patterns related to the spatial\nvariability of the features. For this latter purpose, we also propose and\nexploit a hydroclimatic time series clustering methodology. This new\nmethodology is based on Breiman's random forests. The descriptive and\nexploratory insights gained by the global-scale applications prove the\nusefulness of the adopted feature compilation in hydroclimatic contexts.\nMoreover, the spatially coherent patterns characterizing the clusters delivered\nby the new methodology build confidence in its future exploitation...\n", "versions": [{"version": "v1", "created": "Sat, 24 Oct 2020 08:27:17 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 23:31:22 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""], ["Papalexiou", "Simon Michael", ""], ["Langousis", "Andreas", ""], ["Khatami", "Sina", ""], ["Volpi", "Elena", ""], ["Grimaldi", "Salvatore", ""]]}, {"id": "2010.13039", "submitter": "Debdeep Pati", "authors": "Indrajit Ghosh, Anirban Bhattacharya and Debdeep Pati", "title": "Statistical optimality and stability of tangent transform algorithms in\n  logit models", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic approach to finding variational approximation in an otherwise\nintractable non-conjugate model is to exploit the general principle of convex\nduality by minorizing the marginal likelihood that renders the problem\ntractable. While such approaches are popular in the context of variational\ninference in non-conjugate Bayesian models, theoretical guarantees on\nstatistical optimality and algorithmic convergence are lacking. Focusing on\nlogistic regression models, we provide mild conditions on the data generating\nprocess to derive non-asymptotic upper bounds to the risk incurred by the\nvariational optima. We demonstrate that these assumptions can be completely\nrelaxed if one considers a slight variation of the algorithm by raising the\nlikelihood to a fractional power. Next, we utilize the theory of dynamical\nsystems to provide convergence guarantees for such algorithms in logistic and\nmultinomial logit regression. In particular, we establish local asymptotic\nstability of the algorithm without any assumptions on the data-generating\nprocess. We explore a special case involving a semi-orthogonal design under\nwhich a global convergence is obtained. The theory is further illustrated using\nseveral numerical studies.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 05:15:13 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Ghosh", "Indrajit", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "2010.13252", "submitter": "Nick Barrowman", "authors": "Nick Barrowman and Richard J. Webster", "title": "Exploring data subsets with vtree", "comments": "30 pages, 26 figures. Submitted to Journal of Statistical Software.\n  The editors of the Journal requested a number of changes from the initial\n  version before the manuscript could be sent for peer review. All results were\n  produced using R version 4.0.3 and version 5.1.9 of the vtree package,\n  available on CRAN", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable trees are a new method for the exploration of discrete multivariate\ndata. They display nested subsets and corresponding frequencies and\npercentages. Manual calculation of these quantities can be laborious,\nespecially when there are many multi-level factors and missing data. Here we\nintroduce variable trees and their implementation in the vtree R package, draw\ncomparisons with existing methods (contingency tables, mosaic plots, Venn/Euler\ndiagrams, and UpSet), and illustrate their utility using two case studies.\nVariable trees can be used to (1) reveal patterns in nested subsets, (2)\nexplore missing data, and (3) generate study flow diagrams (e.g., CONSORT\ndiagrams) directly from data frames, to support reproducible research and open\nscience.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 23:51:08 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 00:22:26 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Barrowman", "Nick", ""], ["Webster", "Richard J.", ""]]}, {"id": "2010.13452", "submitter": "Hawre Jalal", "authors": "Hawre Jalal and Fernando Alarid-Escudero", "title": "BayCANN: Streamlining Bayesian Calibration with Artificial Neural\n  Network Metamodeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: Bayesian calibration is theoretically superior to standard\ndirect-search algorithm because it can reveal the full joint posterior\ndistribution of the calibrated parameters. However, to date, Bayesian\ncalibration has not been used often in health decision sciences due to\npractical and computational burdens. In this paper we propose to use artificial\nneural networks (ANN) as one solution to these limitations.\n  Methods: Bayesian Calibration using Artificial Neural Networks (BayCANN)\ninvolves (1) training an ANN metamodel on a sample of model inputs and outputs,\nand (2) then calibrating the trained ANN metamodel instead of the full model in\na probabilistic programming language to obtain the posterior joint distribution\nof the calibrated parameters. We demonstrate BayCANN by calibrating a natural\nhistory model of colorectal cancer to adenoma prevalence and cancer incidence\ndata. In addition, we compare the efficiency and accuracy of BayCANN against\nperforming a Bayesian calibration directly on the simulation model using an\nincremental mixture importance sampling (IMIS) algorithm.\n  Results: BayCANN was generally more accurate than IMIS in recovering the\n\"true\" parameter values. The ratio of the absolute ANN deviation from the truth\ncompared to IMIS for eight out of the nine calibrated parameters were less than\none indicating that BayCANN was more accurate than IMIS. In addition, BayCANN\ntook about 15 minutes total compared to the IMIS method which took 80 minutes.\n  Conclusions: In our case study, BayCANN was more accurate than IMIS and was\nfive-folds faster. Because BayCANN does not depend on the structure of the\nsimulation model, it can be adapted to models of various levels of complexity\nwith minor changes to its structure. We provide BayCANN's open-source\nimplementation in R.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 09:47:39 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Jalal", "Hawre", ""], ["Alarid-Escudero", "Fernando", ""]]}, {"id": "2010.13687", "submitter": "Samuel Orso", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser, Yuming Zhang", "title": "A General Approach for Simulation-based Bias Correction in High\n  Dimensional Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in statistical analysis lies in controlling the bias\nof estimators due to the ever-increasing data size and model complexity.\nApproximate numerical methods and data features like censoring and\nmisclassification often result in analytical and/or computational challenges\nwhen implementing standard estimators. As a consequence, consistent estimators\nmay be difficult to obtain, especially in complex and/or high dimensional\nsettings. In this paper, we study the properties of a general simulation-based\nestimation framework that allows to construct bias corrected consistent\nestimators. We show that the considered approach leads, under more general\nconditions, to stronger bias correction properties compared to alternative\nmethods. Besides its bias correction advantages, the considered method can be\nused as a simple strategy to construct consistent estimators in settings where\nalternative methods may be challenging to apply. Moreover, the considered\nframework can be easily implemented and is computationally efficient. These\ntheoretical results are highlighted with simulation studies of various commonly\nused models, including the negative binomial regression (with and without\ncensoring) and the logistic regression (with and without misclassification\nerrors). Additional numerical illustrations are provided in the supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 16:07:01 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 20:37:49 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""], ["Zhang", "Yuming", ""]]}, {"id": "2010.13884", "submitter": "Andrew Fowlie Assoc. Prof.", "authors": "Andrew Fowlie, Will Handley, Liangliang Su", "title": "Nested sampling with plateaus", "comments": "7 pages, 6 figures. minor changes and clarifications. closely matches\n  published version", "journal-ref": null, "doi": "10.1093/mnras/stab590", "report-no": null, "categories": "stat.CO astro-ph.IM hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently emphasised by Riley (2019); Schittenhelm & Wacker (2020) that\nthat in the presence of plateaus in the likelihood function nested sampling\n(NS) produces faulty estimates of the evidence and posterior densities. After\ninformally explaining the cause of the problem, we present a modified version\nof NS that handles plateaus and can be applied retrospectively to NS runs from\npopular NS software using anesthetic. In the modified NS, live points in a\nplateau are evicted one by one without replacement, with ordinary NS\ncompression of the prior volume after each eviction but taking into account the\ndynamic number of live points. The live points are replenished once all points\nin the plateau are removed. We demonstrate it on a number of examples. Since\nthe modification is simple, we propose that it becomes the canonical version of\nSkilling's NS algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 20:21:04 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 05:30:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Fowlie", "Andrew", ""], ["Handley", "Will", ""], ["Su", "Liangliang", ""]]}, {"id": "2010.13921", "submitter": "Jem Corcoran", "authors": "Caleb Miller, Michael D. Schneider, Jem N. Corcoran, Jason Bernstein", "title": "Bayesian Fusion of Data Partitioned Particle Estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian data fusion method to approximate a posterior\ndistribution from an ensemble of particle estimates that only have access to\nsubsets of the data. Our approach relies on approximate probabilistic inference\nof model parameters through Monte Carlo methods, followed by an update and\nresample scheme related to multiple importance sampling to combine information\nfrom the initial estimates. We show the method is convergent in the particle\nlimit and directly suited to application on multi-sensor data fusion problems\nby demonstrating efficacy on a multi-sensor Keplerian orbit determination\nproblem and a bearings-only tracking problem.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 21:58:59 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Miller", "Caleb", ""], ["Schneider", "Michael D.", ""], ["Corcoran", "Jem N.", ""], ["Bernstein", "Jason", ""]]}, {"id": "2010.13934", "submitter": "Yujie Zhao", "authors": "Yujie Zhao, Xiaoming Huo", "title": "A Homotopic Method to Solve the Lasso Problems with an Improved Upper\n  Bound of Convergence Rate", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In optimization, it is known that when the objective functions are strictly\nconvex and well-conditioned, gradient based approaches can be extremely\neffective, e.g., achieving the exponential rate in convergence. On the other\nhand, the existing Lasso-type of estimator in general cannot achieve the\noptimal rate due to the undesirable behavior of the absolute function at the\norigin. A homotopic method is to use a sequence of surrogate functions to\napproximate the $\\ell_1$ penalty that is used in the Lasso-type of estimators.\nThe surrogate functions will converge to the $\\ell_1$ penalty in the Lasso\nestimator. At the same time, each surrogate function is strictly convex, which\nenables provable faster numerical rate of convergence. In this paper, we\ndemonstrate that by meticulously defining the surrogate functions, one can\nprove faster numerical convergence rate than any existing methods in computing\nfor the Lasso-type of estimators. Namely, the state-of-the-art algorithms can\nonly guarantee $O(1/\\epsilon)$ or $O(1/\\sqrt{\\epsilon})$ convergence rates,\nwhile we can prove an $O([\\log(1/\\epsilon)]^2)$ for the newly proposed\nalgorithm. Our numerical simulations show that the new algorithm also performs\nbetter empirically.\n", "versions": [{"version": "v1", "created": "Mon, 26 Oct 2020 22:37:49 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zhao", "Yujie", ""], ["Huo", "Xiaoming", ""]]}, {"id": "2010.14128", "submitter": "Rowland Seymour", "authors": "R. G. Seymour, D. Sirl, S. Preston, I. L. Dryden, M. J. A. Ellis, B.\n  Perrat, J. Goulding", "title": "The Bayesian Spatial Bradley--Terry Model: Urban Deprivation Modeling in\n  Tanzania", "comments": "23 pages, 7 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the most deprived regions of any country or city is key if policy\nmakers are to design successful interventions. However, locating areas with the\ngreatest need is often surprisingly challenging in developing countries. Due to\nthe logistical challenges of traditional household surveying, official\nstatistics can be slow to be updated; estimates that exist can be coarse, a\nconsequence of prohibitive costs and poor infrastructures; and mass\nurbanisation can render manually surveyed figures rapidly out-of-date.\nComparative judgement models, such as the Bradley--Terry model, offer a\npromising solution. Leveraging local knowledge, elicited via comparisons of\ndifferent areas' affluence, such models can both simplify logistics and\ncircumvent biases inherent to house-hold surveys. Yet widespread adoption\nremains limited, due to the large amount of data existing approaches still\nrequire. We address this via development of a novel Bayesian Spatial\nBradley--Terry model, which substantially decreases the amount of data\ncomparisons required for effective inference. This model integrates a network\nrepresentation of the city or country, along with assumptions of spatial\nsmoothness that allow deprivation in one area to be informed by neighbouring\nareas. We demonstrate the practical effectiveness of this method, through a\nnovel comparative judgement data set collected in Dar es Salaam, Tanzania.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 08:40:26 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 09:06:45 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 13:15:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Seymour", "R. G.", ""], ["Sirl", "D.", ""], ["Preston", "S.", ""], ["Dryden", "I. L.", ""], ["Ellis", "M. J. A.", ""], ["Perrat", "B.", ""], ["Goulding", "J.", ""]]}, {"id": "2010.14340", "submitter": "Paula Saavedra-Nieves", "authors": "Paula Saavedra-Nieves", "title": "Nonparametric estimation of highest density regions for COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Highest density regions refer to level sets containing points of relatively\nhigh density. Their estimation from a random sample, generated from the\nunderlying density, allows to determine the clusters of the corresponding\ndistribution. This task can be accomplished considering different nonparametric\nperspectives. From a practical point of view, reconstructing highest density\nregions can be interpreted as a way of determining hot-spots, a crucial task\nfor understanding COVID-19 space-time evolution. In this work, we compare the\nbehavior of classical plug-in methods and a recently proposed hybrid algorithm\nfor highest density regions estimation through an extensive simulation study.\nBoth methodologies are applied to analyze a real data set about COVID-19 cases\nin the United States.\n", "versions": [{"version": "v1", "created": "Tue, 27 Oct 2020 14:56:23 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 12:41:17 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2020 09:59:31 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Saavedra-Nieves", "Paula", ""]]}, {"id": "2010.14734", "submitter": "Derek Beaton", "authors": "Derek Beaton (1) ((1) Rotman Research Institute, Baycrest Health\n  Sciences)", "title": "Generalized eigen, singular value, and partial least squares\n  decompositions: The GSVD package", "comments": "38 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The generalized singular value decomposition (GSVD, a.k.a. \"SVD triplet\",\n\"duality diagram\" approach) provides a unified strategy and basis to perform\nnearly all of the most common multivariate analyses (e.g., principal\ncomponents, correspondence analysis, multidimensional scaling, canonical\ncorrelation, partial least squares). Though the GSVD is ubiquitous, powerful,\nand flexible, it has very few implementations. Here I introduce the GSVD\npackage for R. The general goal of GSVD is to provide a small set of accessible\nfunctions to perform the GSVD and two other related decompositions (generalized\neigenvalue decomposition, generalized partial least squares-singular value\ndecomposition). Furthermore, GSVD helps provide a more unified conceptual\napproach and nomenclature to many techniques. I first introduce the concept of\nthe GSVD, followed by a formal definition of the generalized decompositions.\nNext I provide some key decisions made during development, and then a number of\nexamples of how to use GSVD to implement various statistical techniques. These\nexamples also illustrate one of the goals of GSVD: how others can (or should)\nbuild analysis packages that depend on GSVD. Finally, I discuss the possible\nfuture of GSVD.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 03:57:27 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 13:24:14 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2020 23:59:48 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Beaton", "Derek", ""]]}, {"id": "2010.14901", "submitter": "Luis Mendo", "authors": "Luis Mendo", "title": "Simulating a coin with irrational bias using rational arithmetic", "comments": "This version adds a comparison with the \"interval algorithm\" by Han\n  and Hoshi. 20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm is presented that, taking a sequence of unbiased coins as inputs\nand using only rational arithmetic, simulates a Bernoulli random variable with\npossibly irrational parameter $\\tau$. It requires a series representation of\n$\\tau$ with positive, rational terms, and a rational bound on its truncation\nerror that converges to $0$. The number of required inputs has an exponentially\nbounded tail, and its mean is at most $3$. The number of operations has a tail\nthat can be bounded in terms of the sequence of truncation error bounds.\n  The algorithm is applied to two specific values of $\\tau$, including Euler's\nconstant, for which obtaining a simple simulation algorithm was an open\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 11:39:14 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 12:00:58 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 23:47:39 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Mendo", "Luis", ""]]}, {"id": "2010.15181", "submitter": "Jeremie Coullon", "authors": "Jeremie Coullon and Robert J Webber", "title": "Ensemble sampler for infinite-dimensional inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new Markov chain Monte Carlo (MCMC) sampler for\ninfinite-dimensional inverse problems. Our new sampler is based on the affine\ninvariant ensemble sampler, which uses interacting walkers to adapt to the\ncovariance structure of the target distribution. We extend this ensemble\nsampler for the first time to infinite-dimensional function spaces, yielding a\nhighly efficient gradient-free MCMC algorithm. Because our new ensemble sampler\ndoes not require gradients or posterior covariance estimates, it is simple to\nimplement and broadly applicable.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 18:44:32 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 12:15:33 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Coullon", "Jeremie", ""], ["Webber", "Robert J", ""]]}, {"id": "2010.15403", "submitter": "Marcin W\\k{a}torek", "authors": "Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z, Jaros{\\l}aw Kwapie\\'n,\n  Ludovico Minati, Pawe{\\l} O\\'swi\\k{e}cimka, Marek Stanuszek", "title": "Multiscale characteristics of the emerging global cryptocurrency market", "comments": null, "journal-ref": "Physics Reports 901 1-82 (2021)", "doi": "10.1016/j.physrep.2020.10.005", "report-no": null, "categories": "q-fin.ST cs.CE econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The review introduces the history of cryptocurrencies, offering a description\nof the blockchain technology behind them. Differences between cryptocurrencies\nand the exchanges on which they are traded have been shown. The central part\nsurveys the analysis of cryptocurrency price changes on various platforms. The\nstatistical properties of the fluctuations in the cryptocurrency market have\nbeen compared to the traditional markets. With the help of the latest\nstatistical physics methods the non-linear correlations and multiscale\ncharacteristics of the cryptocurrency market are analyzed. In the last part the\nco-evolution of the correlation structure among the 100 cryptocurrencies having\nthe largest capitalization is retraced. The detailed topology of cryptocurrency\nnetwork on the Binance platform from bitcoin perspective is also considered.\nFinally, an interesting observation on the Covid-19 pandemic impact on the\ncryptocurrency market is presented and discussed: recently we have witnessed a\n\"phase transition\" of the cryptocurrencies from being a hedge opportunity for\nthe investors fleeing the traditional markets to become a part of the global\nmarket that is substantially coupled to the traditional financial instruments\nlike the currencies, stocks, and commodities.\n  The main contribution is an extensive demonstration that structural\nself-organization in the cryptocurrency markets has caused the same to attain\ncomplexity characteristics that are nearly indistinguishable from the Forex\nmarket at the level of individual time-series. However, the cross-correlations\nbetween the exchange rates on cryptocurrency platforms differ from it. The\ncryptocurrency market is less synchronized and the information flows more\nslowly, which results in more frequent arbitrage opportunities. The methodology\nused in the review allows the latter to be detected, and lead-lag relationships\nto be discovered.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 07:56:01 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:32:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["W\u0105torek", "Marcin", ""], ["Dro\u017cd\u017c", "Stanis\u0142aw", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["Minati", "Ludovico", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["Stanuszek", "Marek", ""]]}, {"id": "2010.15511", "submitter": "Shunichi Nomura", "authors": "Shunichi Nomura", "title": "An Exact Solution Path Algorithm for SLOPE and Quasi-Spherical OSCAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted $L_1$ penalization estimator (SLOPE) is a regularization technique for\nsorted absolute coefficients in high-dimensional regression. By arbitrarily\nsetting its regularization weights $\\lambda$ under the monotonicity constraint,\nSLOPE can have various feature selection and clustering properties. On weight\ntuning, the selected features and their clusters are very sensitive to the\ntuning parameters. Moreover, the exhaustive tracking of their changes is\ndifficult using grid search methods. This study presents a solution path\nalgorithm that provides the complete and exact path of solutions for SLOPE in\nfine-tuning regularization weights. A simple optimality condition for SLOPE is\nderived and used to specify the next splitting point of the solution path. This\nstudy also proposes a new design of a regularization sequence $\\lambda$ for\nfeature clustering, which is called the quasi-spherical and octagonal shrinkage\nand clustering algorithm for regression (QS-OSCAR). QS-OSCAR is designed with a\ncontour surface of the regularization terms most similar to a sphere. Among\nseveral regularization sequence designs, sparsity and clustering performance\nare compared through simulation studies. The numerical observations show that\nQS-OSCAR performs feature clustering more efficiently than other designs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 12:03:22 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Nomura", "Shunichi", ""]]}, {"id": "2010.15805", "submitter": "Hong Zhou", "authors": "Lap Chi Lau and Hong Zhou", "title": "A Local Search Framework for Experimental Design", "comments": "Improved probability bound in Theorem 1.4. A preliminary version\n  accepted by SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a local search framework to design and analyze both combinatorial\nalgorithms and rounding algorithms for experimental design problems. This\nframework provides a unifying approach to match and improve all known results\nin D/A/E-design and to obtain new results in previously unknown settings.\n  For combinatorial algorithms, we provide a new analysis of the classical\nFedorov's exchange method. We prove that this simple local search algorithm\nworks well as long as there exists an almost optimal solution with good\ncondition number. Moreover, we design a new combinatorial local search\nalgorithm for E-design using the regret minimization framework.\n  For rounding algorithms, we provide a unified randomized exchange algorithm\nto match and improve previous results for D/A/E-design. Furthermore, the\nalgorithm works in the more general setting to approximately satisfy multiple\nknapsack constraints, which can be used for weighted experimental design and\nfor incorporating fairness constraints into experimental design.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 17:43:06 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 05:00:04 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Lau", "Lap Chi", ""], ["Zhou", "Hong", ""]]}, {"id": "2010.15951", "submitter": "Zhenwei Dai", "authors": "Zhenwei Dai, Aditya Desai, Reinhard Heckel, Anshumali Shrivastava", "title": "Active Sampling Count Sketch (ASCS) for Online Sparse Estimation of a\n  Trillion Scale Covariance Matrix", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating and storing the covariance (or correlation) matrix of\nhigh-dimensional data is computationally challenging because both memory and\ncomputational requirements scale quadratically with the dimension. Fortunately,\nhigh-dimensional covariance matrices as observed in text, click-through,\nmeta-genomics datasets, etc are often sparse. In this paper, we consider the\nproblem of efficient sparse estimation of covariance matrices with possibly\ntrillions of entries. The size of the datasets we target requires the algorithm\nto be online, as more than one pass over the data is prohibitive. In this\npaper, we propose Active Sampling Count Sketch (ASCS), an online and one-pass\nsketching algorithm, that recovers the large entries of the covariance matrix\naccurately. Count Sketch (CS), and other sub-linear compressed sensing\nalgorithms, offer a natural solution to the problem in theory. However, vanilla\nCS does not work well in practice due to a low signal-to-noise ratio (SNR). At\nthe heart of our approach is a novel active sampling strategy that increases\nthe SNR of classical CS. We demonstrate the practicality of our algorithm with\nsynthetic data and real-world high dimensional datasets. ASCS significantly\nimproves over vanilla CS, demonstrating the merit of our active sampling\nstrategy.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 21:20:15 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 02:15:14 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Dai", "Zhenwei", ""], ["Desai", "Aditya", ""], ["Heckel", "Reinhard", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2010.16114", "submitter": "Seyoon Ko", "authors": "Seyoon Ko, Hua Zhou, Jin Zhou, and Joong-Ho Won", "title": "DistStat.jl: Towards Unified Programming for High-Performance\n  Statistical Computing Environments in Julia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for high-performance computing (HPC) is ever-increasing for\neveryday statistical computing purposes. The downside is that we need to write\nspecialized code for each HPC environment. CPU-level parallelization needs to\nbe explicitly coded for effective use of multiple nodes in cluster\nsupercomputing environments. Acceleration via graphics processing units (GPUs)\nrequires to write kernel code. The Julia software package DistStat.jl\nimplements a data structure for distributed arrays that work on both multi-node\nCPU clusters and multi-GPU environments transparently. This package paves a way\nto developing high-performance statistical software in various HPC environments\nsimultaneously. As a demonstration of the transparency and scalability of the\npackage, we provide applications to large-scale nonnegative matrix\nfactorization, multidimensional scaling, and $\\ell_1$-regularized Cox\nproportional hazards model on an 8-GPU workstation and a 720-CPU-core virtual\ncluster in Amazon Web Services (AWS) cloud. As a case in point, we analyze the\non-set of type-2 diabetes from the UK Biobank with 400,000 subjects and 500,000\nsingle nucleotide polymorphisms using the $\\ell_1$-regularized Cox proportional\nhazards model. Fitting a half-million-variate regression model took less than\n50 minutes on AWS.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2020 08:16:47 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Ko", "Seyoon", ""], ["Zhou", "Hua", ""], ["Zhou", "Jin", ""], ["Won", "Joong-Ho", ""]]}]