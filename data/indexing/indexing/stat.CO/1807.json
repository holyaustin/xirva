[{"id": "1807.00420", "submitter": "Alexander Terenin", "authors": "Alexander Terenin and Daniel Thorngren", "title": "A Piecewise Deterministic Markov Process via $(r,\\theta)$ swaps in\n  hyperspherical coordinates", "comments": null, "journal-ref": "Workshop on Bayesian Inference and Maximum Entropy Methods in\n  Science and Engineering, 2018", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a class of stochastic processes known as piecewise deterministic\nMarkov processes has been used to define continuous-time Markov chain Monte\nCarlo algorithms with a number of attractive properties, including\ncompatibility with stochastic gradients like those typically found in\noptimization and variational inference, and high efficiency on certain big data\nproblems. Not many processes in this class that are capable of targeting\narbitrary invariant distributions are currently known, and within one subclass\nall previously known processes utilize linear transition functions. In this\nwork, we derive a process whose transition function is nonlinear through\nsolving its Fokker-Planck equation in hyperspherical coordinates. We explore\nits behavior on Gaussian targets, as well as a Bayesian logistic regression\nmodel with synthetic data. We discuss implications to both the theory of\npiecewise deterministic Markov processes, and to Bayesian statisticians as well\nas physicists seeking to use them for simulation-based computation.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 00:13:49 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Terenin", "Alexander", ""], ["Thorngren", "Daniel", ""]]}, {"id": "1807.01057", "submitter": "Axel Finke", "authors": "Axel Finke, Arnaud Doucet, Adam M. Johansen", "title": "Limit theorems for sequential MCMC methods", "comments": null, "journal-ref": "Adv. Appl. Probab. 52 (2020) 377-403", "doi": "10.1017/apr.2020.9", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods, also known as particle filters,\nconstitute a class of algorithms used to approximate expectations with respect\nto a sequence of probability distributions as well as the normalising constants\nof those distributions. Sequential MCMC methods are an alternative class of\ntechniques addressing similar problems in which particles are sampled according\nto an MCMC kernel rather than conditionally independently at each time step.\nThese methods were introduced over twenty years ago by Berzuini et al. (1997).\nRecently, there has been a renewed interest in such algorithms as they\ndemonstrate an empirical performance superior to that of SMC methods in some\napplications. We establish a strong law of large numbers and a central limit\ntheorem for sequential MCMC methods and provide conditions under which errors\ncan be controlled uniformly in time. In the context of state-space models, we\nprovide conditions under which sequential MCMC methods can indeed outperform\nstandard SMC methods in terms of asymptotic variance of the corresponding Monte\nCarlo estimators.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 09:54:34 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2018 06:33:27 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Finke", "Axel", ""], ["Doucet", "Arnaud", ""], ["Johansen", "Adam M.", ""]]}, {"id": "1807.01239", "submitter": "Reihaneh Entezari", "authors": "Reihaneh Entezari, Patrick E. Brown, and Jeffrey S. Rosenthal", "title": "Bayesian Spatial Analysis of Hardwood Tree Counts in Forests via MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform Bayesian Inference to analyze spatial tree count\ndata from the Timiskaming and Abitibi River forests in Ontario, Canada. We\nconsider a Bayesian Generalized Linear Geostatistical Model and implement a\nMarkov Chain Monte Carlo algorithm to sample from its posterior distribution.\nHow spatial predictions for new sites in the forests change as the amount of\ntraining data is reduced is studied and compared with a Logistic Regression\nmodel without a spatial effect. Finally, we discuss a stratified sampling\napproach for selecting subsets of data that allows for potential better\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:35:52 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Entezari", "Reihaneh", ""], ["Brown", "Patrick E.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1807.01334", "submitter": "Reihaneh Entezari", "authors": "Reihaneh Entezari", "title": "Breast Cancer Diagnosis via Classification Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the Wisconsin Diagnostic Breast Cancer Data using\nMachine Learning classification techniques, such as the SVM, Bayesian Logistic\nRegression (Variational Approximation), and K-Nearest-Neighbors. We describe\neach model, and compare their performance through different measures. We\nconclude that SVM has the best performance among all other classifiers, while\nit competes closely with the Bayesian Logistic Regression that is ranked second\nbest method for this dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 18:13:55 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Entezari", "Reihaneh", ""]]}, {"id": "1807.01346", "submitter": "Joseph Marion", "authors": "Joseph Marion and Scott C. Schmidler", "title": "Finite Sample $L_2$ Bounds for Sequential Monte Carlo and Adaptive Path\n  Selection", "comments": "Correcting errors in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a bound on the finite sample error of sequential Monte Carlo (SMC)\non static spaces using the $L_2$ distance between interpolating distributions\nand the mixing times of Markov kernels. This result is unique in that it is the\nfirst finite sample convergence result for SMC that does not require an upper\nbound on the importance weights. Using this bound we show that careful\nselection of the interpolating distributions can lead to substantial\nimprovements in the computational complexity of the algorithm. This result also\njustifies the adaptive selection of SMC distributions using the relative\neffective sample size commonly used in the literature and we establish\nconditions guaranteeing the approximation accuracy of the adaptive SMC\napproach. We then demonstrate empirically that this procedure provides\nnearly-optimal sequences of distributions in an automatic fashion for realistic\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 19:02:32 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 12:35:23 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Marion", "Joseph", ""], ["Schmidler", "Scott C.", ""]]}, {"id": "1807.01914", "submitter": "H{\\aa}kon Tjelmeland", "authors": "Xin Luo and H{\\aa}kon Tjelmeland", "title": "A multiple-try Metropolis-Hastings algorithm with tailored proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new multiple-try Metropolis-Hastings algorithm designed to be\nespecially beneficial when a tailored proposal distribution is available. The\nalgorithm is based on a given acyclic graph $G$, where one of the nodes in $G$,\n$k$ say, contains the current state of the Markov chain and the remaining nodes\ncontain proposed states generated by applying the tailored proposal\ndistribution. The Metropolis-Hastings algorithm alternates between two types of\nupdates. The first update type is using the tailored proposal distribution to\ngenerate new states in all nodes in $G$ except in node $k$. The second update\ntype is generating a new value for $k$, thereby changing the value of the\ncurrent state. We evaluate the effectiveness of the proposed scheme in an\nexample with previously defined target and proposal distributions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 09:31:18 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Luo", "Xin", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "1807.02614", "submitter": "Florian Maire", "authors": "Marie Vialaret and Florian Maire", "title": "On the convergence time of some non-reversible Markov chain Monte Carlo\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is commonly admitted that non-reversible Markov chain Monte Carlo (MCMC)\nalgorithms usually yield more accurate MCMC estimators than their reversible\ncounterparts. In this note, we show that in addition to their variance\nreduction effect, some non-reversible MCMC algorithms have also the undesirable\nproperty to slow down the convergence of the Markov chain. This point, which\nhas been overlooked by the literature, has obvious practical implications. We\nillustrate this phenomenon for different non-reversible versions of the\nMetropolis-Hastings algorithm on several discrete state space examples and\ndiscuss ways to mitigate the risk of a small asymptotic variance/slow\nconvergence scenario.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 05:21:55 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 21:15:16 GMT"}, {"version": "v3", "created": "Fri, 23 Aug 2019 20:00:23 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Vialaret", "Marie", ""], ["Maire", "Florian", ""]]}, {"id": "1807.02885", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Zhan Luo, Alex D. Leow, Andrew L. Alexander, Richard J.\n  Davidson, H. Hill Goldsmith", "title": "Exact Combinatorial Inference for Brain Images", "comments": "Accepted for publication in MICCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The permutation test is known as the exact test procedure in statistics.\nHowever, often it is not exact in practice and only an approximate method since\nonly a small fraction of every possible permutation is generated. Even for a\nsmall sample size, it often requires to generate tens of thousands\npermutations, which can be a serious computational bottleneck. In this paper,\nwe propose a novel combinatorial inference procedure that enumerates all\npossible permutations combinatorially without any resampling. The proposed\nmethod is validated against the standard permutation test in simulation studies\nwith the ground truth. The method is further applied in twin DTI study in\ndetermining the genetic contribution of the minimum spanning tree of the\nstructural brain connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 21:07:27 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chung", "Moo K.", ""], ["Luo", "Zhan", ""], ["Leow", "Alex D.", ""], ["Alexander", "Andrew L.", ""], ["Davidson", "Richard J.", ""], ["Goldsmith", "H. Hill", ""]]}, {"id": "1807.03419", "submitter": "Yu-hsuan Wang", "authors": "Wenyu Chen, Mathias Drton, and Y. Samuel Wang", "title": "On Causal Discovery with Equal Variance Assumption", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asz049", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has shown that causal structure can be uniquely identified from\nobservational data when these follow a structural equation model whose error\nterms have equal variances. We show that this fact is implied by an ordering\namong (conditional) variances. We demonstrate that ordering estimates of these\nvariances yields a simple yet state-of-the-art method for causal structure\nlearning that is readily extendable to high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 23:11:17 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 15:47:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Wenyu", ""], ["Drton", "Mathias", ""], ["Wang", "Y. Samuel", ""]]}, {"id": "1807.04145", "submitter": "Francisco Cuevas-Pacheco Mr.", "authors": "Francisco Cuevas and Emilio Porcu and Denis Allard", "title": "Fast and exact simulation of isotropic Gaussian random fields on\n  $\\mathbb{S}^{2}$ and $\\mathbb{S}^{2}\\times \\mathbb{R}$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method for fast and exact simulation of Gaussian random fields\non spheres having isotropic covariance functions. The method proposed is then\nextended to Gaussian random fields defined over spheres cross time and having\ncovariance functions that depend on geodesic distance in space and on temporal\nseparation. The crux of the method is in the use of block circulant matrices\nobtained working on regular grids defined over longitude $\\times$ latitude.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:05:49 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Cuevas", "Francisco", ""], ["Porcu", "Emilio", ""], ["Allard", "Denis", ""]]}, {"id": "1807.04202", "submitter": "Itai Dattner", "authors": "Rami Yaari and Itai Dattner", "title": "simode: R Package for statistical inference of ordinary differential\n  equations using separable integral-matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe simode: Separable Integral Matching for Ordinary\nDifferential Equations. The statistical methodologies applied in the package\nfocus on several minimization procedures of an integral-matching criterion\nfunction, taking advantage of the mathematical structure of the differential\nequations like separability of parameters from equations. Application of\nintegral based methods to parameter estimation of ordinary differential\nequations was shown to yield more accurate and stable results comparing to\nderivative based ones. Linear features such as separability were shown to ease\noptimization and inference. We demonstrate the functionalities of the package\nusing various systems of ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 15:38:54 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 14:16:37 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Yaari", "Rami", ""], ["Dattner", "Itai", ""]]}, {"id": "1807.04489", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan and Didrik Nielsen", "title": "Fast yet Simple Natural-Gradient Descent for Variational Inference in\n  Complex Models", "comments": "Camera-ready version", "journal-ref": "International Symposium on Information Theory and Its Applications\n  (ISITA), 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference plays an important role in advancing machine learning, but\nfaces computational challenges when applied to complex models such as deep\nneural networks. Variational inference circumvents these challenges by\nformulating Bayesian inference as an optimization problem and solving it using\ngradient-based optimization. In this paper, we argue in favor of\nnatural-gradient approaches which, unlike their gradient-based counterparts,\ncan improve convergence by exploiting the information geometry of the\nsolutions. We show how to derive fast yet simple natural-gradient updates by\nusing a duality associated with exponential-family distributions. An attractive\nfeature of these methods is that, by using natural-gradients, they are able to\nextract accurate local approximations for individual model components. We\nsummarize recent results for Bayesian deep learning showing the superiority of\nnatural-gradient approaches over their gradient counterparts.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 09:15:46 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 07:59:27 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Nielsen", "Didrik", ""]]}, {"id": "1807.04594", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Victor Elvira, Joaquin Miguez", "title": "The Incremental Proximal Method: A Probabilistic Perspective", "comments": "Presented at ICASSP, 15-20 April 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we highlight a connection between the incremental proximal\nmethod and stochastic filters. We begin by showing that the proximal operators\ncoincide, and hence can be realized with, Bayes updates. We give the explicit\nform of the updates for the linear regression problem and show that there is a\none-to-one correspondence between the proximal operator of the least-squares\nregression and the Bayes update when the prior and the likelihood are Gaussian.\nWe then carry out this observation to a general sequential setting: We consider\nthe incremental proximal method, which is an algorithm for large-scale\noptimization, and show that, for a linear-quadratic cost function, it can\nnaturally be realized by the Kalman filter. We then discuss the implications of\nthis idea for nonlinear optimization problems where proximal operators are in\ngeneral not realizable. In such settings, we argue that the extended Kalman\nfilter can provide a systematic way for the derivation of practical procedures.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 13:16:42 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Elvira", "Victor", ""], ["Miguez", "Joaquin", ""]]}, {"id": "1807.05048", "submitter": "Rand Wilcox", "authors": "Rand Wilcox, Guillaume Rousselet, Cyril Pernet", "title": "Improved Methods for Making Inferences About Multiple Skipped\n  Correlations", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A skipped correlation has the advantage of dealing with outliers in a manner\nthat takes into account the overall structure of the data cloud. For p-variate\ndata, $p \\ge 2$, there is an extant method for testing the hypothesis of a zero\ncorrelation for each pair of variables that is designed to control the\nprobability of one or more Type I errors. And there are methods for the related\nsituation where the focus is on the association between a dependent variable\nand $p$ explanatory variables. However, there are limitations and several\nconcerns with extant techniques. The paper describes alternative approaches\nthat deal with these issues.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 13:09:28 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Wilcox", "Rand", ""], ["Rousselet", "Guillaume", ""], ["Pernet", "Cyril", ""]]}, {"id": "1807.05187", "submitter": "Jiangjiang Zhang", "authors": "Jiangjiang Zhang and Qiang Zheng and Dingjiang Chen and Laosheng Wu\n  and Lingzao Zeng", "title": "Surrogate-Based Bayesian Inverse Modeling of the Hydrological System: An\n  Adaptive Approach Considering Surrogate Approximation Error", "comments": "60 pages, 14 figures", "journal-ref": null, "doi": "10.1029/2019WR025721", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inverse modeling is important for a better understanding of\nhydrological processes. However, this approach can be computationally\ndemanding, as it usually requires a large number of model evaluations. To\naddress this issue, one can take advantage of surrogate modeling techniques.\nNevertheless, when approximation error of the surrogate model is neglected, the\ninversion result will be biased. In this paper, we develop a surrogate-based\nBayesian inversion framework that explicitly quantifies and gradually reduces\nthe approximation error of the surrogate. Specifically, two strategies are\nproposed to quantify the surrogate error. The first strategy works by\nquantifying the surrogate prediction uncertainty with a Bayesian method, while\nthe second strategy uses another surrogate to simulate and correct the\napproximation error of the primary surrogate. By adaptively refining the\nsurrogate over the posterior distribution, we can gradually reduce the\nsurrogate approximation error to a small level. Demonstrated with three case\nstudies involving high dimensionality, multimodality, and a real-world\napplication, it is found that both strategies can reduce the bias introduced by\nsurrogate approximation error, while the second strategy that integrates two\nmethods (i.e., polynomial chaos expansion and Gaussian process in this work)\nthat complement each other shows the best performance.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:07:39 GMT"}, {"version": "v2", "created": "Sat, 21 Jul 2018 09:02:05 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 02:56:51 GMT"}, {"version": "v4", "created": "Fri, 21 Feb 2020 02:31:12 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Zhang", "Jiangjiang", ""], ["Zheng", "Qiang", ""], ["Chen", "Dingjiang", ""], ["Wu", "Laosheng", ""], ["Zeng", "Lingzao", ""]]}, {"id": "1807.05421", "submitter": "Alain Durmus", "authors": "Alain Durmus, Arnaud Guillin, Pierre Monmarch\\'e", "title": "Piecewise Deterministic Markov Processes and their invariant measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise Deterministic Markov Processes (PDMPs) are studied in a general\nframework. First, different constructions are proven to be equivalent. Second,\nwe introduce a coupling between two PDMPs following the same differential flow\nwhich implies quantitative bounds on the total variation between the marginal\ndistributions of the two processes. Finally two results are established\nregarding the invariant measures of PDMPs. A practical condition to show that a\nprobability measure is invariant for the associated PDMP semi-group is\npresented. In a second time, a bound on the invariant probability measures in\n$V$-norm of two PDMPs following the same differential flow is established. This\nlast result is then applied to study the asymptotic bias of some non-exact PDMP\nMCMC methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 16:38:28 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 17:46:56 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Durmus", "Alain", ""], ["Guillin", "Arnaud", ""], ["Monmarch\u00e9", "Pierre", ""]]}, {"id": "1807.05507", "submitter": "Shiwei Lan", "authors": "Shiwei Lan", "title": "Adaptive Dimension Reduction to Accelerate Infinite-Dimensional\n  Geometric Markov Chain Monte Carlo", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2019.04.043", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inverse problems highly rely on efficient and effective inference\nmethods for uncertainty quantification (UQ). Infinite-dimensional MCMC\nalgorithms, directly defined on function spaces, are robust under refinement of\nphysical models. Recent development of this class of algorithms has started to\nincorporate the geometry of the posterior informed by data so that they are\ncapable of exploring complex probability structures. However, the required\ngeometric quantities are usually expensive to obtain in high dimensions. On the\nother hand, most geometric information of the unknown parameter space in this\nsetting is concentrated in an \\emph{intrinsic} finite-dimensional subspace. To\nmitigate the computational intensity and scale up the applications of\ninfinite-dimensional geometric MCMC ($\\infty$-GMC), we apply geometry-informed\nalgorithms to the intrinsic subspace to probe its complex structure, and\nsimpler methods like preconditioned Crank-Nicolson (pCN) to its geometry-flat\ncomplementary subspace. In this work, we take advantage of dimension reduction\ntechniques to accelerate the original $\\infty$-GMC algorithms. More\nspecifically, partial spectral decomposition of the (prior or posterior)\ncovariance operator is used to identify certain number of principal\neigen-directions as a basis for the intrinsic subspace. The combination of\ndimension-independent algorithms, geometric information, and dimension\nreduction yields more efficient implementation, \\emph{(adaptive)\ndimension-reduced infinite-dimensional geometric MCMC}. With a small amount of\ncomputational overhead, we can achieve over 70 times speed-up compared to pCN\nusing a simulated elliptic inverse problem and an inverse problem involving\nturbulent combustion. A number of error bounds comparing various MCMC proposals\nare presented to predict the asymptotic behavior of the proposed\ndimension-reduced algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 08:01:34 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 20:53:38 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Lan", "Shiwei", ""]]}, {"id": "1807.06456", "submitter": "Yassine Hamoudi", "authors": "Yassine Hamoudi and Fr\\'ed\\'eric Magniez", "title": "Quantum Chebyshev's Inequality and Applications", "comments": "27 pages; v3: better presentation, lower bound in Theorem 4.3 is new", "journal-ref": "Proceedings of the 46th International Colloquium on Automata,\n  Languages, and Programming (ICALP), volume 132 of LIPIcs, pages 69:1-99:16,\n  2019", "doi": "10.4230/LIPIcs.ICALP.2019.69", "report-no": null, "categories": "quant-ph cs.CC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide new quantum algorithms with polynomial speed-up for\na range of problems for which no such results were known, or we improve\nprevious algorithms. First, we consider the approximation of the frequency\nmoments $F_k$ of order $k \\geq 3$ in the multi-pass streaming model with\nupdates (turnstile model). We design a $P$-pass quantum streaming algorithm\nwith memory $M$ satisfying a tradeoff of $P^2 M = \\tilde{O}(n^{1-2/k})$,\nwhereas the best classical algorithm requires $P M = \\Theta(n^{1-2/k})$. Then,\nwe study the problem of estimating the number $m$ of edges and the number $t$\nof triangles given query access to an $n$-vertex graph. We describe optimal\nquantum algorithms that perform $\\tilde{O}(\\sqrt{n}/m^{1/4})$ and\n$\\tilde{O}(\\sqrt{n}/t^{1/6} + m^{3/4}/\\sqrt{t})$ queries respectively. This is\na quadratic speed-up compared to the classical complexity of these problems.\n  For this purpose we develop a new quantum paradigm that we call Quantum\nChebyshev's inequality. Namely we demonstrate that, in a certain model of\nquantum sampling, one can approximate with relative error the mean of any\nrandom variable with a number of quantum samples that is linear in the ratio of\nthe square root of the variance to the mean. Classically the dependency is\nquadratic. Our algorithm subsumes a previous result of Montanaro [Mon15]. This\nnew paradigm is based on a refinement of the Amplitude Estimation algorithm of\nBrassard et al. [BHMT02] and of previous quantum algorithms for the mean\nestimation problem. We show that this speed-up is optimal, and we identify\nanother common model of quantum sampling where it cannot be obtained. For our\napplications, we also adapt the variable-time amplitude amplification technique\nof Ambainis [Amb10] into a variable-time amplitude estimation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:09:38 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 09:59:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jan 2019 18:09:46 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Hamoudi", "Yassine", ""], ["Magniez", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1807.07133", "submitter": "Philipp Hunziker", "authors": "Philipp Hunziker (Northeastern University), Julian Wucherpfennig\n  (Hertie School of Governance), Aya Kachi (University of Basel) and\n  Nils-Christian Bormann (University of Exeter)", "title": "A Scalable MCEM Estimator for Spatio-Temporal Autoregressive Models", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large spatio-temporal lattice data are becoming increasingly common\nacross a variety of disciplines. However, estimating interdependence across\nspace and time in large areal datasets remains challenging, as existing\napproaches are often (i) not scalable, (ii) designed for conditionally Gaussian\noutcome data, or (iii) are limited to cross-sectional and univariate outcomes.\nThis paper proposes an MCEM estimation strategy for a family of latent-Gaussian\nmultivariate spatio-temporal models that addresses these issues. The proposed\nestimator is applicable to a wide range of non-Gaussian outcomes, and\nimplementations for binary and count outcomes are discussed explicitly. The\nmethodology is illustrated on simulated data, as well as on weekly data of\nIS-related events in Syrian districts.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 20:20:05 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Hunziker", "Philipp", "", "Northeastern University"], ["Wucherpfennig", "Julian", "", "Hertie School of Governance"], ["Kachi", "Aya", "", "University of Basel"], ["Bormann", "Nils-Christian", "", "University of Exeter"]]}, {"id": "1807.07621", "submitter": "Christopher Aicher", "authors": "Christopher Aicher and Emily B. Fox", "title": "Approximate Collapsed Gibbs Clustering with Expectation Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for approximating collapsed Gibbs sampling in\ngenerative latent variable cluster models. Collapsed Gibbs is a popular MCMC\nmethod, which integrates out variables in the posterior to improve mixing.\nUnfortunately for many complex models, integrating out these variables is\neither analytically or computationally intractable. We efficiently approximate\nthe necessary collapsed Gibbs integrals by borrowing ideas from expectation\npropagation. We present two case studies where exact collapsed Gibbs sampling\nis intractable: mixtures of Student-t's and time series clustering. Our\nexperiments on real and synthetic data show that our approximate sampler\nenables a runtime-accuracy tradeoff in sampling these types of models,\nproviding results with competitive accuracy much more rapidly than the naive\nGibbs samplers one would otherwise rely on in these scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2018 19:40:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Aicher", "Christopher", ""], ["Fox", "Emily B.", ""]]}, {"id": "1807.08084", "submitter": "Renato J Cintra", "authors": "D. F. G. Coelho, R. J. Cintra, A. C. Frery, V. S. Dimitrov", "title": "Fast Matrix Inversion and Determinant Computation for Polarimetric\n  Synthetic Aperture Radar", "comments": "7 pages, 1 figure", "journal-ref": "Computers and Geosciences, no. 119 (2018), pages 109-114", "doi": "10.1016/j.cageo.2018.07.002", "report-no": null, "categories": "cs.NA cs.DS eess.SP math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a fast algorithm for simultaneous inversion and\ndeterminant computation of small sized matrices in the context of fully\nPolarimetric Synthetic Aperture Radar (PolSAR) image processing and analysis.\nThe proposed fast algorithm is based on the computation of the adjoint matrix\nand the symmetry of the input matrix. The algorithm is implemented in a general\npurpose graphical processing unit (GPGPU) and compared to the usual approach\nbased on Cholesky factorization. The assessment with simulated observations and\ndata from an actual PolSAR sensor show a speedup factor of about two when\ncompared to the usual Cholesky factorization. Moreover, the expressions\nprovided here can be implemented in any platform.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 04:52:06 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Coelho", "D. F. G.", ""], ["Cintra", "R. J.", ""], ["Frery", "A. C.", ""], ["Dimitrov", "V. S.", ""]]}, {"id": "1807.08213", "submitter": "Yuguang Yue", "authors": "Yuguang Yue, Lieven Vandenberghe, Weng Kee Wong", "title": "T-optimal designs for multi-factor polynomial regression models via a\n  semidefinite relaxation method", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider T-optimal experiment design problems for discriminating\nmulti-factor polynomial regression models where the design space is defined by\npolynomial inequalities and the regression parameters are constrained to given\nconvex sets. Our proposed optimality criterion is formulated as a convex\noptimization problem with a moment cone constraint. When the regression models\nhave one factor, an exact semidefinite representation of the moment cone\nconstraint can be applied to obtain an equivalent semidefinite program. When\nthere are two or more factors in the models, we apply a moment relaxation\ntechnique and approximate the moment cone constraint by a hierarchy of\nsemidefinite-representable outer approximations. When the relaxation hierarchy\nconverges, an optimal discrimination design can be recovered from the optimal\nmoment matrix, and its optimality is confirmed by an equivalence theorem. The\nmethodology is illustrated with several examples.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 23:51:07 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 16:08:29 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yue", "Yuguang", ""], ["Vandenberghe", "Lieven", ""], ["Wong", "Weng Kee", ""]]}, {"id": "1807.08409", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Mattias Villani, Robert Kohn, Minh-Ngoc Tran, Khue-Dung\n  Dang", "title": "Subsampling MCMC - An introduction for the survey statistician", "comments": "Accepted for publication in Sankhya A. Previous uploaded version\n  contained a bug in generating the figures and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of computing power and efficient Markov Chain Monte\nCarlo (MCMC) simulation algorithms have revolutionized Bayesian statistics,\nmaking it a highly practical inference method in applied work. However, MCMC\nalgorithms tend to be computationally demanding, and are particularly slow for\nlarge datasets. Data subsampling has recently been suggested as a way to make\nMCMC methods scalable on massively large data, utilizing efficient sampling\nschemes and estimators from the survey sampling literature. These developments\ntend to be unknown by many survey statisticians who traditionally work with\nnon-Bayesian methods, and rarely use MCMC. Our article explains the idea of\ndata subsampling in MCMC by reviewing one strand of work, Subsampling MCMC, a\nso called pseudo-marginal MCMC approach to speeding up MCMC through data\nsubsampling. The review is written for a survey statistician without previous\nknowledge of MCMC methods since our aim is to motivate survey sampling experts\nto contribute to the growing Subsampling MCMC literature.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 02:49:41 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 01:59:17 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 09:50:24 GMT"}, {"version": "v4", "created": "Thu, 20 Sep 2018 16:39:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Quiroz", "Matias", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Dang", "Khue-Dung", ""]]}, {"id": "1807.08673", "submitter": "Iker Perez", "authors": "Iker Perez, Giuliano Casale", "title": "Variational inequalities and mean-field approximations for partially\n  observed systems of queueing networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queueing networks are systems of theoretical interest that find widespread\nuse in the performance evaluation of interconnected resources. In comparison to\ncounterpart models in genetics or mathematical biology, the stochastic (jump)\nprocesses induced by queueing networks have distinctive coupling and\nsynchronization properties. This has prevented the derivation of variational\napproximations for conditional representations of transient dynamics, which\nrely on simplifying independence assumptions. Here, we present a model\naugmentation to a multivariate counting process for interactions across service\nstations, and we enable the variational evaluation of mean-field measures for\npartially-observed multi-class networks. We also show that our framework offers\nan efficient and improved alternative for inference tasks, where existing\nvariational or numerically intensive solutions do not work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:22:21 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 17:08:22 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Perez", "Iker", ""], ["Casale", "Giuliano", ""]]}, {"id": "1807.08691", "submitter": "Lawrence Middleton", "authors": "Lawrence Middleton, George Deligiannidis, Arnaud Doucet and Pierre E.\n  Jacob", "title": "Unbiased Markov chain Monte Carlo for intractable target distributions", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing numerical integration when the integrand itself cannot be\nevaluated point-wise is a challenging task that arises in statistical analysis,\nnotably in Bayesian inference for models with intractable likelihood functions.\nMarkov chain Monte Carlo (MCMC) algorithms have been proposed for this setting,\nsuch as the pseudo-marginal method for latent variable models and the exchange\nalgorithm for a class of undirected graphical models. As with any MCMC\nalgorithm, the resulting estimators are justified asymptotically in the limit\nof the number of iterations, but exhibit a bias for any fixed number of\niterations due to the Markov chains starting outside of stationarity. This\n\"burn-in\" bias is known to complicate the use of parallel processors for MCMC\ncomputations. We show how to use coupling techniques to generate unbiased\nestimators in finite time, building on recent advances for generic MCMC\nalgorithms. We establish the theoretical validity of some of these procedures\nby extending existing results to cover the case of polynomially ergodic Markov\nchains. The efficiency of the proposed estimators is compared with that of\nstandard MCMC estimators, with theoretical arguments and numerical experiments\nincluding state space models and Ising models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:03:09 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 13:43:40 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2020 19:03:35 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Middleton", "Lawrence", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1807.08713", "submitter": "Jonas Latz", "authors": "Matthieu Bult\\'e, Jonas Latz, Elisabeth Ullmann", "title": "A practical example for the non-linear Bayesian filtering of model\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this tutorial we consider the non-linear Bayesian filtering of static\nparameters in a time-dependent model. We outline the theoretical background and\ndiscuss appropriate solvers. We focus on particle-based filters and present\nSequential Importance Sampling (SIS) and Sequential Monte Carlo (SMC).\nThroughout the paper we illustrate the concepts and techniques with a practical\nexample using real-world data. The task is to estimate the gravitational\nacceleration of the Earth $g$ by using observations collected from a simple\npendulum. Importantly, the particle filters enable the adaptive updating of the\nestimate for $g$ as new observations become available. For tutorial purposes we\nprovide the data set and a Python implementation of the particle filters.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:43:49 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 21:22:35 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Bult\u00e9", "Matthieu", ""], ["Latz", "Jonas", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "1807.08834", "submitter": "Zack Almquist", "authors": "Abhirup Mallik and Zack W. Almquist", "title": "Stable Multiple Time Step Simulation/Prediction from Lagged Dynamic\n  Network Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in computers and automated data collection strategies\nhave greatly increased the interest in statistical modeling of dynamic\nnetworks. Many of the statistical models employed for inference on large-scale\ndynamic networks suffer from limited forward simulation/prediction ability. A\nmajor problem with many of the forward simulation procedures is the tendency\nfor the model to become degenerate in only a few time steps, i.e., the\nsimulation/prediction procedure results in either null graphs or complete\ngraphs. Here, we describe an algorithm for simulating a sequence of networks\ngenerated from lagged dynamic network regression models DNR(V), a sub-family of\nTERGMs. We introduce a smoothed estimator for forward prediction based on\nsmoothing of the change statistics obtained for a dynamic network regression\nmodel. We focus on the implementation of the algorithm, providing a series of\nmotivating examples with comparisons to dynamic network models from the\nliterature. We find that our algorithm significantly improves multi-step\nprediction/simulation over standard DNR(V) forecasting. Furthermore, we show\nthat our method performs comparably to existing more complex dynamic network\nanalysis frameworks (SAOM and STERGMs) for small networks over short time\nperiods, and significantly outperforms these approaches over long time time\nintervals and/or large networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 21:20:06 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Mallik", "Abhirup", ""], ["Almquist", "Zack W.", ""]]}, {"id": "1807.09155", "submitter": "Allyson Souris", "authors": "Allyson Souris, Anirban Bhattacharya, Debdeep Pati", "title": "The Soft Multivariate Truncated Normal Distribution with Applications to\n  Bayesian Constrained Estimation", "comments": "23 Pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new distribution, called the soft tMVN distribution, which\nprovides a smooth approximation to the truncated multivariate normal (tMVN)\ndistribution with linear constraints. An efficient blocked Gibbs sampler is\ndeveloped to sample from the soft tMVN distribution in high dimensions. We\nprovide theoretical support to the approximation capability of the soft tMVN\nand provide further empirical evidence thereof. The soft tMVN distribution can\nbe used to approximate simulations from a multivariate truncated normal\ndistribution with linear constraints, or itself as a prior in shape-constrained\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:40:22 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 16:14:39 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Souris", "Allyson", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1807.09288", "submitter": "Lewis Rendell", "authors": "Lewis J. Rendell, Adam M. Johansen, Anthony Lee and Nick Whiteley", "title": "Global consensus Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To conduct Bayesian inference with large data sets, it is often convenient or\nnecessary to distribute the data across multiple machines. We consider a\nlikelihood function expressed as a product of terms, each associated with a\nsubset of the data. Inspired by global variable consensus optimisation, we\nintroduce an instrumental hierarchical model associating auxiliary statistical\nparameters with each term, which are conditionally independent given the\ntop-level parameters. One of these top-level parameters controls the\nunconditional strength of association between the auxiliary parameters. This\nmodel leads to a distributed MCMC algorithm on an extended state space yielding\napproximations of posterior expectations. A trade-off between computational\ntractability and fidelity to the original model can be controlled by changing\nthe association strength in the instrumental model. We further propose the use\nof a SMC sampler with a sequence of association strengths, allowing both the\nautomatic determination of appropriate strengths and for a bias correction\ntechnique to be applied. In contrast to similar distributed Monte Carlo\nalgorithms, this approach requires few distributional assumptions. The\nperformance of the algorithms is illustrated with a number of simulated\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:07:15 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 14:45:18 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 18:07:20 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Rendell", "Lewis J.", ""], ["Johansen", "Adam M.", ""], ["Lee", "Anthony", ""], ["Whiteley", "Nick", ""]]}, {"id": "1807.09299", "submitter": "Daniel Sussman", "authors": "Fei Fang, Daniel L. Sussman, Vince Lyzinski", "title": "Tractable Graph Matching via Soft Seeding", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graph matching problem aims to discover a latent correspondence between\nthe vertex sets of two observed graphs. This problem has proven to be quite\nchallenging, with few satisfying methods that are computationally tractable and\nwidely applicable. The FAQ algorithm has proven to have good performance on\nbenchmark problems and works with a indefinite relaxation of the problem. Due\nto the indefinite relaxation, FAQ is not guaranteed to find the global maximum.\nHowever, if prior information is known about the true correspondence, this can\nbe leveraged to initialize the algorithm near the truth. We show that given\ncertain properties of this initial matrix, with high probability the FAQ\nalgorithm will converge in two steps to the truth under a flexible model for\npairs of random graphs. Importantly, this result implies that there will be no\nlocal optima near the global optima, providing a method to assess performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 18:31:13 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Fang", "Fei", ""], ["Sussman", "Daniel L.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1807.09655", "submitter": "Pablo de Oliveira Castro", "authors": "Devan Sohier, Pablo de Oliveira Castro, Fran\\c{c}ois F\\'evotte, Bruno\n  Lathuili\\`ere, Eric Petit, Olivier Jamond", "title": "Confidence Intervals for Stochastic Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying errors and losses due to the use of Floating-Point (FP)\ncalculations in industrial scientific computing codes is an important part of\nthe Verification, Validation and Uncertainty Quantification (VVUQ) process.\nStochastic Arithmetic is one way to model and estimate FP losses of accuracy,\nwhich scales well to large, industrial codes. It exists in different flavors,\nsuch as CESTAC or MCA, implemented in various tools such as CADNA, Verificarlo\nor Verrou. These methodologies and tools are based on the idea that FP losses\nof accuracy can be modeled via randomness. Therefore, they share the same need\nto perform a statistical analysis of programs results in order to estimate the\nsignificance of the results. In this paper, we propose a framework to perform a\nsolid statistical analysis of Stochastic Arithmetic. This framework unifies all\nexisting definitions of the number of significant digits (CESTAC and MCA), and\nalso proposes a new quantity of interest: the number of digits contributing to\nthe accuracy of the results. Sound confidence intervals are provided for all\nestimators, both in the case of normally distributed results, and in the\ngeneral case. The use of this framework is demonstrated by two case studies of\nlarge, industrial codes: Europlexus and code_aster.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 09:18:06 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 09:54:28 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 07:39:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sohier", "Devan", ""], ["Castro", "Pablo de Oliveira", ""], ["F\u00e9votte", "Fran\u00e7ois", ""], ["Lathuili\u00e8re", "Bruno", ""], ["Petit", "Eric", ""], ["Jamond", "Olivier", ""]]}, {"id": "1807.09657", "submitter": "Maria L. Daza-Torres", "authors": "Maria L. Daza-Torres (1), Juan Antonio Infante del R\\'io (2), Marcos\n  A. Capistr\\'an (1) and J. Andr\\'es Christen (1) ((1) Centro de\n  Investigaci\\'on en Matem\\'aticas (CIMAT) and (2) Instituto de Matem\\'atica\n  Interdisciplinar y Departamento de An\\'alisis Matem\\'atico y Matem\\'atica\n  Aplicada, Facultad de CC. Matem\\'aticas, Universidad Complutense de Madrid)", "title": "A computational geometry method for the inverse scattering problem", "comments": "20 pages, figures 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we demonstrate a computational method to solve the inverse\nscattering problem for a star-shaped, smooth, penetrable obstacle in 2D. Our\nmethod is based on classical ideas from computational geometry. First, we\napproximate the support of a scatterer by a point cloud. Secondly, we use the\nBayesian paradigm to model the joint conditional probability distribution of\nthe non-convex hull of the point cloud and the constant refractive index of the\nscatterer given near field data. Of note, we use the non-convex hull of the\npoint cloud as spline control points to evaluate, on a finer mesh, the volume\npotential arising in the integral equation formulation of the direct problem.\nFinally, in order to sample the arising posterior distribution, we propose a\nprobability transition kernel that commutes with affine transformations of\nspace. Our findings indicate that our method is reliable to retrieve the\nsupport and constant refractive index of the scatterer simultaneously. Indeed,\nour sampling method is robust to estimate a quantity of interest such as the\narea of the scatterer. We conclude pointing out a series of generalizations of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 20:37:32 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Daza-Torres", "Maria L.", ""], ["del R\u00edo", "Juan Antonio Infante", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "1807.09737", "submitter": "Hans Kersting", "authors": "Hans Kersting, T. J. Sullivan, Philipp Hennig", "title": "Convergence Rates of Gaussian ODE Filters", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recently-introduced class of probabilistic (uncertainty-aware) solvers for\nordinary differential equations (ODEs) applies Gaussian (Kalman) filtering to\ninitial value problems. These methods model the true solution $x$ and its first\n$q$ derivatives \\emph{a priori} as a Gauss--Markov process $\\boldsymbol{X}$,\nwhich is then iteratively conditioned on information about $\\dot{x}$. This\narticle establishes worst-case local convergence rates of order $q+1$ for a\nwide range of versions of this Gaussian ODE filter, as well as global\nconvergence rates of order $q$ in the case of $q=1$ and an integrated Brownian\nmotion prior, and analyses how inaccurate information on $\\dot{x}$ coming from\napproximate evaluations of $f$ affects these rates. Moreover, we show that, in\nthe globally convergent case, the posterior credible intervals are well\ncalibrated in the sense that they globally contract at the same rate as the\ntruncation error. We illustrate these theoretical results by numerical\nexperiments which might indicate their generalizability to $q \\in\n\\{2,3,\\dots\\}$.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jul 2018 17:33:55 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 18:11:45 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 16:54:44 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Kersting", "Hans", ""], ["Sullivan", "T. J.", ""], ["Hennig", "Philipp", ""]]}, {"id": "1807.10046", "submitter": "Jean-Marie Droz", "authors": "Jean-Marie Droz", "title": "Fast computation of p-values for the permutation test based on Pearson's\n  correlation coefficient and other statistical tests", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation tests are among the simplest and most widely used statistical\ntools. Their p-values can be computed by a straightforward sampling of\npermutations. However, this way of computing p-values is often so slow that it\nis replaced by an approximation, which is accurate only for part of the\ninteresting range of parameters. Moreover, the accuracy of the approximation\ncan usually not be improved by increasing the computation time.\n  We introduce a new sampling-based algorithm which uses the fast Fourier\ntransform to compute p-values for the permutation test based on Pearson's\ncorrelation coefficient. The algorithm is practically and asymptotically faster\nthan straightforward sampling. Typically, its complexity is logarithmic in the\ninput size, while the complexity of straightforward sampling is linear. The\nidea behind the algorithm can also be used to accelerate the computation of\np-values for many other common statistical tests. The algorithm is easy to\nimplement, but its analysis involves results from the representation theory of\nthe symmetric group.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 10:03:14 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Droz", "Jean-Marie", ""]]}, {"id": "1807.10259", "submitter": "Jordan Franks", "authors": "Neil K. Chada, Jordan Franks, Ajay Jasra, Kody J. H. Law and Matti\n  Vihola", "title": "Unbiased inference for discretely observed hidden Markov model\n  diffusions", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian inference method for diffusions observed discretely and\nwith noise, which is free of discretisation bias. Unlike existing unbiased\ninference methods, our method does not rely on exact simulation techniques.\nInstead, our method uses standard time-discretised approximations of\ndiffusions, such as the Euler--Maruyama scheme. Our approach is based on\nparticle marginal Metropolis--Hastings, a particle filter, randomised\nmultilevel Monte Carlo, and importance sampling type correction of approximate\nMarkov chain Monte Carlo. The resulting estimator leads to inference without a\nbias from the time-discretisation as the number of Markov chain iterations\nincreases. We give convergence results and recommend allocations for algorithm\ninputs. Our method admits a straightforward parallelisation, and can be\ncomputationally efficient. The user-friendly approach is illustrated on three\nexamples, where the underlying diffusion is an Ornstein--Uhlenbeck process, a\ngeometric Brownian motion, and a 2d non-reversible Langevin equation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:40:18 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 13:47:58 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 12:12:50 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 10:11:52 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2019 15:11:25 GMT"}, {"version": "v6", "created": "Thu, 21 Jan 2021 17:35:11 GMT"}, {"version": "v7", "created": "Sun, 7 Mar 2021 11:47:28 GMT"}, {"version": "v8", "created": "Tue, 9 Mar 2021 12:13:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chada", "Neil K.", ""], ["Franks", "Jordan", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Vihola", "Matti", ""]]}, {"id": "1807.10858", "submitter": "Guillermo Scheffler", "authors": "Guillermo Scheffler, Juan Ruiz, Manuel Pulido", "title": "Inference of stochastic parameterizations for model error treatment\n  using nested ensemble Kalman filters", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3542", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic parameterizations are increasingly being used to represent the\nuncertainty associated with model errors in ensemble forecasting and data\nassimilation. One of the challenges associated with the use of these\nparameterizations is the optimization of the properties of the stochastic\nforcings within their formulation. In this work a hierarchical data\nassimilation approach based on two nested ensemble Kalman filters is proposed\nfor inferring parameters associated with a stochastic parameterization. The\nproposed technique is based on the Rao-Blackwellization of the parameter\nestimation problem. The technique consists in using an ensemble of ensemble\nKalman filters, each of them using a different set of stochastic parameter\nvalues. We show the ability of the technique to infer parameters related to the\ncovariance structure of stochastic representations of model error in the\nLorenz-96 dynamical system. The evaluation is conducted with stochastic twin\nexperiments and imperfect model experiments with unresolved physics in the\nforecast model. The proposed technique performs successfully under different\nmodel error covariance structures. The technique is proposed to be applied\noffline as part of an a priori optimization of the data assimilation system and\ncould in principle be extended to the estimation of other hyperparameters of a\ndata assimilation system.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 23:34:28 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Scheffler", "Guillermo", ""], ["Ruiz", "Juan", ""], ["Pulido", "Manuel", ""]]}, {"id": "1807.11143", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin, Mingyuan Zhou", "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To backpropagate the gradients through stochastic binary layers, we propose\nthe augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low\nvariance, and has low computational complexity. Exploiting variable\naugmentation, REINFORCE, and reparameterization, the ARM estimator achieves\nadaptive variance reduction for Monte Carlo integration by merging two\nexpectations via common random numbers. The variance-reduction mechanism of the\nARM estimator can also be attributed to either antithetic sampling in an\naugmented space, or the use of an optimal anti-symmetric \"self-control\"\nbaseline function together with the REINFORCE estimator in that augmented\nspace. Experimental results show the ARM estimator provides state-of-the-art\nperformance in auto-encoding variational inference and maximum likelihood\nestimation, for discrete latent variable models with one or multiple stochastic\nbinary layers. Python code for reproducible research is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:21:07 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 22:34:47 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yin", "Mingzhang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1807.11537", "submitter": "Maruti Mudunuru", "authors": "M. K. Mudunuru, N. Panda, S. Karra, G. Srinivasan, V. T. Chau, E.\n  Rougier, A. Hunter, and H. S. Viswanathan", "title": "Estimating Failure in Brittle Materials using Graph Theory", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.LG physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In brittle fracture applications, failure paths, regions where the failure\noccurs and damage statistics, are some of the key quantities of interest (QoI).\nHigh-fidelity models for brittle failure that accurately predict these QoI\nexist but are highly computationally intensive, making them infeasible to\nincorporate in upscaling and uncertainty quantification frameworks. The goal of\nthis paper is to provide a fast heuristic to reasonably estimate quantities\nsuch as failure path and damage in the process of brittle failure. Towards this\ngoal, we first present a method to predict failure paths under tensile loading\nconditions and low-strain rates. The method uses a $k$-nearest neighbors\nalgorithm built on fracture process zone theory, and identifies the set of all\npossible pre-existing cracks that are likely to join early to form a large\ncrack. The method then identifies zone of failure and failure paths using\nweighted graphs algorithms. We compare these failure paths to those computed\nwith a high-fidelity model called the Hybrid Optimization Software Simulation\nSuite (HOSS). A probabilistic evolution model for average damage in a system is\nalso developed that is trained using 150 HOSS simulations and tested on 40\nsimulations. A non-parametric approach based on confidence intervals is used to\ndetermine the damage evolution over time along the dominant failure path. For\nupscaling, damage is the key QoI needed as an input by the continuum models.\nThis needs to be informed accurately by the surrogate models for calculating\neffective modulii at continuum-scale. We show that for the proposed average\ndamage evolution model, the prediction accuracy on the test data is more than\n90\\%. In terms of the computational time, the proposed models are $\\approx\n\\mathcal{O}(10^6)$ times faster compared to high-fidelity HOSS.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 19:21:57 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mudunuru", "M. K.", ""], ["Panda", "N.", ""], ["Karra", "S.", ""], ["Srinivasan", "G.", ""], ["Chau", "V. T.", ""], ["Rougier", "E.", ""], ["Hunter", "A.", ""], ["Viswanathan", "H. S.", ""]]}]