[{"id": "1310.0226", "submitter": "Yohan Petetin", "authors": "Yohan Petetin, Fran\\c{c}ois Desbouvries", "title": "A class of fast exact Bayesian filters in dynamical models with jumps", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2329265", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the statistical filtering problem in dynamical\nmodels with jumps. When a particular application relies on physical properties\nwhich are modeled by linear and Gaussian probability density functions with\njumps, an usualmethod consists in approximating the optimal Bayesian estimate\n(in the sense of the Minimum Mean Square Error (MMSE)) in a linear and Gaussian\nJump Markov State Space System (JMSS). Practical solutions include algorithms\nbased on numerical approximations or based on Sequential Monte Carlo (SMC)\nmethods. In this paper, we propose a class of alternative methods which\nconsists in building statistical models which share the same physical\nproperties of interest but in which the computation of the optimal MMSE\nestimate can be done at a computational cost which is linear in the number of\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 1 Oct 2013 10:25:11 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Petetin", "Yohan", ""], ["Desbouvries", "Fran\u00e7ois", ""]]}, {"id": "1310.0655", "submitter": "Tomasz Badowski", "authors": "Tomasz Badowski", "title": "Variance-based sensitivity analysis for stochastic chemical kinetics", "comments": "61 pages, Master's Thesis in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensitivity analysis is a process of computing sensitivity indices, which are\ncertain measures of importance of parameters in influencing the outputs of\nmathematical models. Sensitivity indices computed in variance-based sensitivity\nanalysis yield quantitative answers to questions like how much on average the\nvariance of model output, measuring its uncertainty, decreases, if exact values\nof certain unknown parameters are determined, e. g. in an experiment. We\npropose new schemes for estimation of variance-based sensitivity indices of\noutputs of stochastic models, their conditional expectations, and histograms\ngiven the parameters. Unbiased estimators obtained in these schemes can be used\nin a Monte Carlo (MC) procedure approximating sensitivity indices. We derive\nrelations between variances of final estimators of MC procedures making the\nsame number of evaluations of given function, but using different schemes, both\nfor the newly introduced schemes and for some used before in the literature.\nNumerical experiment for a discrete state stochastic Markov model of a chemical\nreaction network (DM) shows that our method can lead to much lower error than\nmethod analogous to the one offered by Degasperi et al. Further numerical\nexperiments demonstrate that the application of random time change (RTC)\nalgorithm due to Rathinam et al. for simulation of DM can lead to over 30 times\nlower variance of estimators of certain sensitivity indices than when\nGillespie's direct (GD) method is used, and that this variance may\nsignificantly depend on the order of reactions in GD method. We provide some\nintuitions explaining these effects. We generalize measures used for comparing\ndispersion of different distributions, such as coefficient of variation and\nFano factor to the random parameters case, in a way that they can be computed\nalong with variance-based sensitivity indices.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 10:54:12 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2013 09:04:59 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Badowski", "Tomasz", ""]]}, {"id": "1310.0657", "submitter": "Tomasz Badowski", "authors": "Tomasz Badowski", "title": "Variance-based sensitivity analysis and orthogonal approximations for\n  stochastic models", "comments": "99 pages, Master's Thesis in Physics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new unbiased estimators of a number of quantities defined for\nfunctions of conditional moments, like conditional expectations and variances,\nof functions of two independent random variables given the first variable,\nincluding certain outputs of stochastic models given the models parameters.\nThese quantities include variance-based sensitivity indices, mean squared error\nof approximation with functions of the first variable, orthogonal projection\ncoefficients, and newly defined nonlinearity coefficients. We define the above\nestimators and analyze their performance in Monte Carlo procedures using\ngeneralized concept of an estimation scheme and its inefficiency constant. In\nnumerical simulations of chemical reaction networks, using the Gillespie's\ndirect and random time change methods, the new schemes for sensitivity indices\nof conditional expectations in some cases outperformed the ones proposed\npreviously, and variances of some estimators significantly depended on the\nsimulation method being applied.\n", "versions": [{"version": "v1", "created": "Wed, 2 Oct 2013 10:57:17 GMT"}], "update_date": "2013-10-03", "authors_parsed": [["Badowski", "Tomasz", ""]]}, {"id": "1310.0973", "submitter": "Umberto Picchini", "authors": "Umberto Picchini, Julie Lyng Forman", "title": "Accelerating inference for diffusions observed with measurement error\n  and large sample sizes using Approximate Bayesian Computation", "comments": "22 pages, forthcoming in Journal of Statistical Computation and\n  Simulation", "journal-ref": null, "doi": "10.1080/00949655.2014.1002101", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years dynamical modelling has been provided with a range of\nbreakthrough methods to perform exact Bayesian inference. However it is often\ncomputationally unfeasible to apply exact statistical methodologies in the\ncontext of large datasets and complex models. This paper considers a nonlinear\nstochastic differential equation model observed with correlated measurement\nerrors and an application to protein folding modelling. An Approximate Bayesian\nComputation (ABC) MCMC algorithm is suggested to allow inference for model\nparameters within reasonable time constraints. The ABC algorithm uses\nsimulations of \"subsamples\" from the assumed data generating model as well as a\nso-called \"early rejection\" strategy to speed up computations in the ABC-MCMC\nsampler. Using a considerate amount of subsamples does not seem to degrade the\nquality of the inferential results for the considered applications. A\nsimulation study is conducted to compare our strategy with exact Bayesian\ninference, the latter resulting two orders of magnitude slower than ABC-MCMC\nfor the considered setup. Finally the ABC algorithm is applied to a large size\nprotein data. The suggested methodology is fairly general and not limited to\nthe exemplified model and data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 13:23:57 GMT"}, {"version": "v2", "created": "Tue, 5 Aug 2014 10:29:56 GMT"}, {"version": "v3", "created": "Mon, 22 Dec 2014 22:01:46 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Picchini", "Umberto", ""], ["Forman", "Julie Lyng", ""]]}, {"id": "1310.1022", "submitter": "Peter K\\\"oves\\'arki", "authors": "Peter Kovesarki, Ian C. Brock", "title": "Multivariate regression and fit function uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a multivariate polynomial regression method where the\nuncertainty of the input parameters are approximated with Gaussian\ndistributions, derived from the central limit theorem for large weighted sums,\ndirectly from the training sample. The estimated uncertainties can be\npropagated into the optimal fit function, as an alternative to the statistical\nbootstrap method. This uncertainty can be propagated further into a loss\nfunction like quantity, with which it is possible to calculate the expected\nloss function, and allows to select the optimal polynomial degree with\nstatistical significance. Combined with simple phase space splitting methods,\nit is possible to model most features of the training data even with low degree\npolynomials or constants.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 16:16:35 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Kovesarki", "Peter", ""], ["Brock", "Ian C.", ""]]}, {"id": "1310.1034", "submitter": "Jukka Kohonen", "authors": "Jukka Kohonen and Jukka Corander", "title": "Computing Exact Clustering Posteriors with Subset Convolution", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exponential-time exact algorithm is provided for the task of clustering n\nitems of data into k clusters. Instead of seeking one partition, posterior\nprobabilities are computed for summary statistics: the number of clusters, and\npairwise co-occurrence. The method is based on subset convolution, and yields\nthe posterior distribution for the number of clusters in O(n * 3^n) operations,\nor O(n^3 * 2^n) using fast subset convolution. Pairwise co-occurrence\nprobabilities are then obtained in O(n^3 * 2^n) operations. This is\nconsiderably faster than exhaustive enumeration of all partitions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Oct 2013 17:01:34 GMT"}], "update_date": "2013-10-04", "authors_parsed": [["Kohonen", "Jukka", ""], ["Corander", "Jukka", ""]]}, {"id": "1310.1297", "submitter": "Vincent Lyzinski", "authors": "Vince Lyzinski, Daniel L. Sussman, Donniell E. Fishkind, Henry Pao, Li\n  Chen, Joshua T. Vogelstein, Youngser Park, Carey E. Priebe", "title": "Spectral Clustering for Divide-and-Conquer Graph Matching", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallelized bijective graph matching algorithm that leverages\nseeds and is designed to match very large graphs. Our algorithm combines\nspectral graph embedding with existing state-of-the-art seeded graph matching\nprocedures. We justify our approach by proving that modestly correlated, large\nstochastic block model random graphs are correctly matched utilizing very few\nseeds through our divide-and-conquer procedure. We also demonstrate the\neffectiveness of our approach in matching very large graphs in simulated and\nreal data examples, showing up to a factor of 8 improvement in runtime with\nminimal sacrifice in accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Oct 2013 14:40:30 GMT"}, {"version": "v2", "created": "Sun, 12 Jan 2014 00:23:34 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 02:40:51 GMT"}, {"version": "v4", "created": "Wed, 22 Oct 2014 02:42:45 GMT"}, {"version": "v5", "created": "Thu, 12 Mar 2015 19:12:03 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Lyzinski", "Vince", ""], ["Sussman", "Daniel L.", ""], ["Fishkind", "Donniell E.", ""], ["Pao", "Henry", ""], ["Chen", "Li", ""], ["Vogelstein", "Joshua T.", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1310.1537", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "SIMD Parallel MCMC Sampling with Applications for Big-Data Bayesian\n  Analytics", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.02.010", "report-no": null, "categories": "stat.CO cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational intensity and sequential nature of estimation techniques for\nBayesian methods in statistics and machine learning, combined with their\nincreasing applications for big data analytics, necessitate both the\nidentification of potential opportunities to parallelize techniques such as\nMCMC sampling, and the development of general strategies for mapping such\nparallel algorithms to modern CPUs in order to elicit the performance up the\ncompute-based and/or memory-based hardware limits. Two opportunities for\nSingle-Instruction Multiple-Data (SIMD) parallelization of MCMC sampling for\nprobabilistic graphical models are presented. In exchangeable models with many\nobservations such as Bayesian Generalized Linear Models, child-node\ncontributions to the conditional posterior of each node can be calculated\nconcurrently. In undirected graphs with discrete nodes, concurrent sampling of\nconditionally-independent nodes can be transformed into a SIMD form.\nHigh-performance libraries with multi-threading and vectorization capabilities\ncan be readily applied to such SIMD opportunities to gain decent speedup, while\na series of high-level source-code and runtime modifications provide further\nperformance boost by reducing parallelization overhead and increasing data\nlocality for NUMA architectures. For big-data Bayesian GLM graphs, the\nend-result is a routine for evaluating the conditional posterior and its\ngradient vector that is 5 times faster than a naive implementation using\n(built-in) multi-threaded Intel MKL BLAS, and reaches within the striking\ndistance of the memory-bandwidth-induced hardware limit. The proposed\noptimization strategies improve the scaling of performance with number of cores\nand width of vector units (applicable to many-core SIMD processors such as\nIntel Xeon Phi and GPUs), resulting in cost-effectiveness, energy efficiency,\nand higher speed on multi-core x86 processors.\n", "versions": [{"version": "v1", "created": "Sun, 6 Oct 2013 04:02:35 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 22:40:39 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1310.1878", "submitter": "Dimitrios V. Vougas Dr", "authors": "Dimitrios V. Vougas", "title": "Estimation for Unit Root Testing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit estimation and computation of the Dickey Fuller (DF) and DF-type\ntests. Firstly, we show that the usual one step approach, based on the \"DF\nautoregression\", is likely to be subject to misspecification. Secondly, we\nclarify a neglected two step approach for estimation of the DF test. (In fact,\nwe introduce a new two step DF autoregression.) This method is always correctly\nspecified and efficient under the circumstances. However, it is either\nneglected or misused in unit root testing literature. The commonly employed\nhybrid of the (correct) two step method is shown to be inefficient, even\nasymptotically. Finally, we further improve/robustify the proposed two step\nmethod by employing the missing initial observations. Our finally proposed\nmethod is to be used in unit root testing, since it is a new DF autoregression\nthat retains the missing observations.\n", "versions": [{"version": "v1", "created": "Mon, 7 Oct 2013 18:20:42 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2013 17:42:31 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Vougas", "Dimitrios V.", ""]]}, {"id": "1310.2559", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on and Tarn Duong", "title": "Efficient recursive algorithms for functionals based on higher order\n  derivatives of the multivariate Gaussian density", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many developments in Mathematics involve the computation of higher order\nderivatives of Gaussian density functions. The analysis of univariate Gaussian\nrandom variables is a well-established field whereas the analysis of their\nmultivariate counterparts consists of a body of results which are more\ndispersed. These latter results generally fall into two main categories:\ntheoretical expressions which reveal the deep structure of the problem, or\ncomputational algorithms which can mask the connections with closely related\nproblems. In this paper, we unify existing results and develop new results in a\nframework which is both conceptually cogent and computationally efficient. We\nfocus on the underlying connections between higher order derivatives of\nGaussian density functions, the expected value of products of quadratic forms\nin Gaussian random variables, and V-statistics of degree two based on Gaussian\ndensity functions. These three sets of results are combined into an analysis of\nnon-parametric data smoothers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Oct 2013 17:53:16 GMT"}, {"version": "v2", "created": "Sun, 23 Mar 2014 12:55:07 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""], ["Duong", "Tarn", ""]]}, {"id": "1310.2816", "submitter": "Jun Zhu", "authors": "Jun Zhu, Ning Chen, Hugh Perkins, Bo Zhang", "title": "Gibbs Max-margin Topic Models with Data Augmentation", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-margin learning is a powerful approach to building classifiers and\nstructured output predictors. Recent work on max-margin supervised topic models\nhas successfully integrated it with Bayesian topic models to discover\ndiscriminative latent semantic structures and make accurate predictions for\nunseen testing data. However, the resulting learning problems are usually hard\nto solve because of the non-smoothness of the margin loss. Existing approaches\nto building max-margin supervised topic models rely on an iterative procedure\nto solve multiple latent SVM subproblems with additional mean-field assumptions\non the desired posterior distributions. This paper presents an alternative\napproach by defining a new max-margin loss. Namely, we present Gibbs max-margin\nsupervised topic models, a latent variable Gibbs classifier to discover hidden\ntopic representations for various tasks, including classification, regression\nand multi-task learning. Gibbs max-margin supervised topic models minimize an\nexpected margin loss, which is an upper bound of the existing margin loss\nderived from an expected prediction rule. By introducing augmented variables\nand integrating out the Dirichlet variables analytically by conjugacy, we\ndevelop simple Gibbs sampling algorithms with no restricting assumptions and no\nneed to solve SVM subproblems. Furthermore, each step of the\n\"augment-and-collapse\" Gibbs sampling algorithms has an analytical conditional\ndistribution, from which samples can be easily drawn. Experimental results\ndemonstrate significant improvements on time efficiency. The classification\nperformance is also significantly improved over competitors on binary,\nmulti-class and multi-label classification tasks.\n", "versions": [{"version": "v1", "created": "Thu, 10 Oct 2013 13:47:40 GMT"}], "update_date": "2013-10-11", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Ning", ""], ["Perkins", "Hugh", ""], ["Zhang", "Bo", ""]]}, {"id": "1310.3574", "submitter": "Pritam Ranjan", "authors": "Neil A. Spencer, Pritam Ranjan, Franklin Mendivil", "title": "Isomorphism Check for $2^n$ Factorial Designs with Randomization\n  Restrictions", "comments": "25 pages (accepted in Journal of Statistical Theory and Practice)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs with randomization restrictions are often used in\nindustrial experiments when a complete randomization of trials is impractical.\nIn the statistics literature, the analysis, construction and isomorphism of\nfactorial designs has been extensively investigated. Much of the work has been\non a case-by-case basis -- addressing completely randomized designs, randomized\nblock designs, split-plot designs, etc. separately. In this paper we take a\nmore unified approach, developing theoretical results and an efficient\nrelabeling strategy to both construct and check the isomorphism of multi-stage\nfactorial designs with randomization restrictions. The examples presented in\nthis paper particularly focus on split-lot designs.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 06:41:37 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 02:45:24 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 01:18:08 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Spencer", "Neil A.", ""], ["Ranjan", "Pritam", ""], ["Mendivil", "Franklin", ""]]}, {"id": "1310.3596", "submitter": "Zdravko Botev", "authors": "Z. I. Botev and A. Ridder and L. Rojas-Nandayapa", "title": "Semiparametric Cross Entropy for rare-event simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cross Entropy method is a well-known adaptive importance sampling method\nfor rare-event probability estimation, which requires estimating an optimal\nimportance sampling density within a parametric class. In this article we\nestimate an optimal importance sampling density within a wider semiparametric\nclass of distributions. We show that this semiparametric version of the Cross\nEntropy method frequently yields efficient estimators. We illustrate the\nexcellent practical performance of the method with numerical experiments and\nshow that for the problems we consider it typically outperforms alternative\nschemes by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 14 Oct 2013 08:53:15 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Botev", "Z. I.", ""], ["Ridder", "A.", ""], ["Rojas-Nandayapa", "L.", ""]]}, {"id": "1310.3892", "submitter": "Bradley Price", "authors": "Bradley S. Price, Charles J. Geyer, and Adam J. Rothman", "title": "Ridge Fusion in Statistical Learning", "comments": "24 pages and 9 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to jointly estimate multiple\nprecision matrices for use in quadratic discriminant analysis and model based\nclustering. A ridge penalty and a ridge fusion penalty are used to introduce\nshrinkage and promote similarity between precision matrix estimates. Block-wise\ncoordinate descent is used for optimization, and validation likelihood is used\nfor tuning parameter selection. Our method is applied in quadratic discriminant\nanalysis and semi-supervised model based clustering.\n", "versions": [{"version": "v1", "created": "Tue, 15 Oct 2013 01:27:14 GMT"}, {"version": "v2", "created": "Thu, 17 Apr 2014 21:36:52 GMT"}, {"version": "v3", "created": "Mon, 5 May 2014 13:10:03 GMT"}], "update_date": "2014-05-06", "authors_parsed": [["Price", "Bradley S.", ""], ["Geyer", "Charles J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1310.4411", "submitter": "Sean Gorman", "authors": "Sean P. Gorman", "title": "The Danger of a Big Data Episteme and the Need to Evolve GIS", "comments": "AAG Conference 2013 - Los angeles, CA - More data, more problems?\n  Geography and the future of 'big data'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of \"Big Data\" as a dominant technology meme challenges\nGeography's technical underpinnings, found in GIS, while engaging the\ndiscipline in a conversation about the meme's impact on society. This allows\nscholars to engage collaboratively from both a computationally quantitative and\ncritically qualitative perspective. For Geography there is an opportunity to\npoint out these shortcomings through critical appraisals of \"Big Data\" and its\nreflection of society. Complimentarily this opens the door to developing\nmethodologies that will allow for a more realistic interpretation of \"Big Data\"\nanalysis in the context of an unfiltered societal view.\n", "versions": [{"version": "v1", "created": "Wed, 16 Oct 2013 15:11:22 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Gorman", "Sean P.", ""]]}, {"id": "1310.4624", "submitter": "\\\"Omer Demirel", "authors": "\\\"Omer Demirel, Ihor Smal, Wiro Niessen, Erik Meijering, Ivo F.\n  Sbalzarini", "title": "Adaptive Distributed Resampling Algorithm with Non-Proportional\n  Allocation", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributed resampling algorithm with proportional allocation (RNA) is\nkey to implementing particle filtering applications on parallel computer\nsystems. We extend the original work by Bolic et al. by introducing an adaptive\nRNA (ARNA) algorithm, improving RNA by dynamically adjusting the\nparticle-exchange ratio and randomizing the process ring topology. This\nimproves the runtime performance of ARNA by about 9% over RNA with 10% particle\nexchange. ARNA also significantly improves the speed at which information is\nshared between processing elements, leading to about 20-fold faster\nconvergence. The ARNA algorithm requires only a few modifications to the\noriginal RNA, and is hence easy to implement.\n", "versions": [{"version": "v1", "created": "Thu, 17 Oct 2013 09:08:40 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 07:18:04 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2013 18:47:03 GMT"}], "update_date": "2013-10-29", "authors_parsed": [["Demirel", "\u00d6mer", ""], ["Smal", "Ihor", ""], ["Niessen", "Wiro", ""], ["Meijering", "Erik", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1310.5045", "submitter": "\\\"Omer Demirel", "authors": "\\\"Omer Demirel, Ihor Smal, Wiro Niessen, Erik Meijering, Ivo F.\n  Sbalzarini", "title": "PPF - A Parallel Particle Filtering Library", "comments": "8 pages, 8 figures; will appear in the proceedings of the IET Data\n  Fusion & Target Tracking Conference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the parallel particle filtering (PPF) software library, which\nenables hybrid shared-memory/distributed-memory parallelization of particle\nfiltering (PF) algorithms combining the Message Passing Interface (MPI) with\nmultithreading for multi-level parallelism. The library is implemented in Java\nand relies on OpenMPI's Java bindings for inter-process communication. It\nincludes dynamic load balancing, multi-thread balancing, and several\nalgorithmic improvements for PF, such as input-space domain decomposition. The\nPPF library hides the difficulties of efficient parallel programming of PF\nalgorithms and provides application developers with the necessary tools for\nparallel implementation of PF methods. We demonstrate the capabilities of the\nPPF library using two distributed PF algorithms in two scenarios with different\nnumbers of particles. The PPF library runs a 38 million particle problem,\ncorresponding to more than 1.86 GB of particle data, on 192 cores with 67%\nparallel efficiency. To the best of our knowledge, the PPF library is the first\nopen-source software that offers a parallel framework for PF applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 14:55:38 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 13:07:47 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Demirel", "\u00d6mer", ""], ["Smal", "Ihor", ""], ["Niessen", "Wiro", ""], ["Meijering", "Erik", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1310.5182", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Jarad Niemi, Robin M. Weiss", "title": "Massively parallel approximate Gaussian process regression", "comments": "24 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how the big-three computing paradigms -- symmetric multi-processor\n(SMC), graphical processing units (GPUs), and cluster computing -- can together\nbe brought to bare on large-data Gaussian processes (GP) regression problems\nvia a careful implementation of a newly developed local approximation scheme.\nOur methodological contribution focuses primarily on GPU computation, as this\nrequires the most care and also provides the largest performance boost.\nHowever, in our empirical work we study the relative merits of all three\nparadigms to determine how best to combine them. The paper concludes with two\ncase studies. One is a real data fluid-dynamics computer experiment which\nbenefits from the local nature of our approximation; the second is a synthetic\ndata example designed to find the largest design for which (accurate) GP\nemulation can performed on a commensurate predictive set under an hour.\n", "versions": [{"version": "v1", "created": "Fri, 18 Oct 2013 23:13:59 GMT"}, {"version": "v2", "created": "Wed, 4 Jun 2014 15:01:38 GMT"}], "update_date": "2014-06-05", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Niemi", "Jarad", ""], ["Weiss", "Robin M.", ""]]}, {"id": "1310.5541", "submitter": "\\\"Omer Demirel", "authors": "\\\"Omer Demirel, Ihor Smal, Wiro J. Niessen, Erik Meijering, Ivo F.\n  Sbalzarini", "title": "Piecewise Constant Sequential Importance Sampling for Fast Particle\n  Filtering", "comments": "8 pages; will appear in the proceedings of the IET Data Fusion &\n  Target Tracking Conference 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are key algorithms for object tracking under non-linear,\nnon-Gaussian dynamics. The high computational cost of particle filters,\nhowever, hampers their applicability in cases where the likelihood model is\ncostly to evaluate, or where large numbers of particles are required to\nrepresent the posterior. We introduce the approximate sequential importance\nsampling/resampling (ASIR) algorithm, which aims at reducing the cost of\ntraditional particle filters by approximating the likelihood with a mixture of\nuniform distributions over pre-defined cells or bins. The particles in each bin\nare represented by a dummy particle at the center of mass of the original\nparticle distribution and with a state vector that is the average of the states\nof all particles in the same bin. The likelihood is only evaluated for the\ndummy particles, and the resulting weight is identically assigned to all\nparticles in the bin. We derive upper bounds on the approximation error of the\nso-obtained piecewise constant function representation, and analyze how bin\nsize affects tracking accuracy and runtime. Further, we show numerically that\nthe ASIR approximation error converges to that of sequential importance\nsampling/resampling (SIR) as the bin size is decreased. We present a set of\nnumerical experiments from the field of biological image processing and\ntracking that demonstrate ASIR's capabilities. Overall, we consider ASIR a\npromising candidate for simple, fast particle filtering in generic\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Oct 2013 13:42:47 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 09:53:30 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2014 11:28:24 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Demirel", "\u00d6mer", ""], ["Smal", "Ihor", ""], ["Niessen", "Wiro J.", ""], ["Meijering", "Erik", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "1310.6224", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Ryan P. Browne and Paul D. McNicholas", "title": "A Mixture of SDB Skew-t Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1016/j.ecosta.2017.05.001", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of skew-t distributions offer a flexible choice for model-based\nclustering. A mixture model of this sort can be implemented using a variety of\nformulations of the skew-t distribution. Herein we develop a mixture of skew-t\nfactor analyzers model for clustering of high-dimensional data using a flexible\nformulation of the skew-t distribution. Methodological details of our approach,\nwhich represents an extension of the mixture of factor analyzers model to a\nflexible skew-t distribution, are outlined and details of parameter estimation\nare provided. Clustering results are illustrated and compared to an alternative\nformulation of the mixture of skew-t factor analyzers model as well as the\nmixture of factor analyzers model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Oct 2013 13:46:50 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 17:23:33 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1310.7177", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe and Hans Julius Skaug", "title": "Bandwidth Selection In Pre-Smoothed Particle Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the purpose of maximum likelihood estimation of static parameters, we\napply a kernel smoother to the particles in the standard SIR filter for\nnon-linear state space models with additive Gaussian observation noise. This\nreduces the Monte Carlo error in the estimates of both the posterior density of\nthe states and the marginal density of the observation at each time point. We\ncorrect for variance inflation in the smoother, which together with the use of\nGaussian kernels, results in a Gaussian (Kalman) update when the amount of\nsmoothing turns to infinity. We propose and study of a criterion for choosing\nthe optimal bandwidth $h$ in the kernel smoother. Finally, we illustrate our\napproach using examples from econometrics. Our filter is shown to be highly\nsuited for dynamic models with high signal-to-noise ratio, for which the SIR\nfilter has problems.\n", "versions": [{"version": "v1", "created": "Sun, 27 Oct 2013 09:47:14 GMT"}, {"version": "v2", "created": "Wed, 6 May 2015 07:45:24 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Kleppe", "Tore Selland", ""], ["Skaug", "Hans Julius", ""]]}, {"id": "1310.8192", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Sudipto Banerjee, Alan E.Gelfand", "title": "spBayes for large univariate and multivariate point-referenced\n  spatio-temporal data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we detail the reformulation and rewrite of core functions in\nthe spBayes R package. These efforts have focused on improving computational\nefficiency, flexibility, and usability for point-referenced data models.\nAttention is given to algorithm and computing developments that result in\nimproved sampler convergence rate and efficiency by reducing parameter space;\ndecreased sampler run-time by avoiding expensive matrix computations, and;\nincreased scalability to large datasets by implementing a class of predictive\nprocess models that attempt to overcome computational hurdles by representing\nspatial processes in terms of lower-dimensional realizations. Beyond these\ngeneral computational improvements for existing model functions, we detail new\nfunctions for modeling data indexed in both space and time. These new functions\nimplement a class of dynamic spatio-temporal models for settings where space is\nviewed as continuous and time is taken as discrete.\n", "versions": [{"version": "v1", "created": "Wed, 30 Oct 2013 15:16:32 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Finley", "Andrew O.", ""], ["Banerjee", "Sudipto", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1310.8625", "submitter": "Dimitris Bertsimas", "authors": "Dimitris Bertsimas, Rahul Mazumder", "title": "Least quantile regression via modern optimization", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1223 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 6, 2494-2525", "doi": "10.1214/14-AOS1223", "report-no": "IMS-AOS-AOS1223", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the Least Quantile of Squares (LQS) (and in particular the Least\nMedian of Squares) regression problem using modern optimization methods. We\npropose a Mixed Integer Optimization (MIO) formulation of the LQS problem which\nallows us to find a provably global optimal solution for the LQS problem. Our\nMIO framework has the appealing characteristic that if we terminate the\nalgorithm early, we obtain a solution with a guarantee on its sub-optimality.\nWe also propose continuous optimization methods based on first-order\nsubdifferential methods, sequential linear optimization and hybrid combinations\nof them to obtain near optimal solutions to the LQS problem. The MIO algorithm\nis found to benefit significantly from high quality solutions delivered by our\ncontinuous optimization based methods. We further show that the MIO approach\nleads to (a) an optimal solution for any dataset, where the data-points\n$(y_i,\\mathbf{x}_i)$'s are not necessarily in general position, (b) a simple\nproof of the breakdown point of the LQS objective value that holds for any\ndataset and (c) an extension to situations where there are polyhedral\nconstraints on the regression coefficient vector. We report computational\nresults with both synthetic and real-world datasets showing that the MIO\nalgorithm with warm starts from the continuous optimization methods solve small\n($n=100$) and medium ($n=500$) size problems to provable optimality in under\ntwo hours, and outperform all publicly available methods for large-scale\n($n={}$10,000) LQS problems.\n", "versions": [{"version": "v1", "created": "Thu, 31 Oct 2013 18:24:14 GMT"}, {"version": "v2", "created": "Wed, 19 Nov 2014 08:54:26 GMT"}], "update_date": "2014-11-20", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Mazumder", "Rahul", ""]]}]