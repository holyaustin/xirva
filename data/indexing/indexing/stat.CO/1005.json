[{"id": "1005.0312", "submitter": "Yizao Wang", "authors": "Yizao Wang and Stilian A. Stoev", "title": "Conditional Sampling for Spectrally Discrete Max-Stable Random Fields", "comments": "31 pages. 4 figures. Data analysis removed from the Technical Report\n  (previous version). To appear in Advances in Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable random fields play a central role in modeling extreme value\nphenomena. We obtain an explicit formula for the conditional probability in\ngeneral max-linear models, which include a large class of max-stable random\nfields. As a consequence, we develop an algorithm for efficient and exact\nsampling from the conditional distributions. Our method provides a\ncomputational solution to the prediction problem for spectrally discrete\nmax-stable random fields. This work offers new tools and a new perspective to\nmany statistical inference problems for spatial extremes, arising, for example,\nin meteorology, geology, and environmental applications.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2010 14:55:35 GMT"}, {"version": "v2", "created": "Thu, 25 Nov 2010 14:57:36 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Wang", "Yizao", ""], ["Stoev", "Stilian A.", ""]]}, {"id": "1005.0909", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "George Forsythe's last paper", "comments": "10 pages. Text of an invited talk presented at the Stanford 50\n  Conference celebrating the 50th anniversary of George Forsythe's arrival at\n  Stanford and the 75th birthday of Gene Golub. For further details see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub238.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe von Neumann's elegant idea for sampling from the exponential\ndistribution, Forsythe's generalization for sampling from a probability\ndistribution whose density has the form exp(-G(x)), where G(x) is easy to\ncompute (e.g. a polynomial), and my refinement of these ideas to give an\nefficient algorithm for generating pseudo-random numbers with a normal\ndistribution. Later developments are also mentioned.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2010 07:27:48 GMT"}, {"version": "v2", "created": "Fri, 7 May 2010 03:01:58 GMT"}], "update_date": "2010-05-10", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1005.1193", "submitter": "Benjamin Taylor", "authors": "Paul Fearnhead and Benjamin M. Taylor", "title": "An Adaptive Sequential Monte Carlo Sampler", "comments": "41 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods are not only a popular tool in the\nanalysis of state space models, but offer an alternative to MCMC in situations\nwhere Bayesian inference must proceed via simulation. This paper introduces a\nnew SMC method that uses adaptive MCMC kernels for particle dynamics. The\nproposed algorithm features an online stochastic optimization procedure to\nselect the best MCMC kernel and simultaneously learn optimal tuning parameters.\nTheoretical results are presented that justify the approach and give guidance\non how it should be implemented. Empirical results, based on analysing data\nfrom mixture models, show that the new adaptive SMC algorithm (ASMC) can both\nchoose the best MCMC kernel, and learn an appropriate scaling for it. ASMC with\na choice between kernels outperformed the adaptive MCMC algorithm of Haario et\nal. (1998) in 5 out of the 6 cases considered.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2010 12:28:55 GMT"}, {"version": "v2", "created": "Mon, 10 May 2010 05:40:22 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Fearnhead", "Paul", ""], ["Taylor", "Benjamin M.", ""]]}, {"id": "1005.1307", "submitter": "Karim Filali", "authors": "Fadoua Balabdaoui and Karim Filali", "title": "Efficient computation of the cdf of the maximal difference between\n  Brownian bridge and its concave majorant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe two computational methods for calculating the\ncumulative distribution function and the upper quantiles of the maximal\ndifference between a Brownian bridge and its concave majorant. The first method\nhas two different variants that are both based on a Monte Carlo approach,\nwhereas the second uses the Gaver-Stehfest (GS) algorithm for numerical\ninversion of Laplace transform. If the former method is straightforward to\nimplement, it is very much outperformed by the GS algorithm, which provides a\nvery accurate approximation of the cumulative distribution as well as its upper\nquantiles. Our numerical work has a direct application in statistics: the\nmaximal difference between a Brownian bridge and its concave majorant arises in\nconnection with a nonparametric test for monotonicity of a density or\nregression curve on [0, 1]. Our results can be used to construct very accurate\nrejection region for this test at a given asymptotic level.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2010 22:57:27 GMT"}], "update_date": "2010-05-11", "authors_parsed": [["Balabdaoui", "Fadoua", ""], ["Filali", "Karim", ""]]}, {"id": "1005.1971", "submitter": "Ryan J. Tibshirani", "authors": "Ryan J. Tibshirani, Jonathan Taylor", "title": "The solution path of the generalized lasso", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS878 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 3, 1335-1371", "doi": "10.1214/11-AOS878", "report-no": "IMS-AOS-AOS878", "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a path algorithm for the generalized lasso problem. This problem\npenalizes the $\\ell_1$ norm of a matrix D times the coefficient vector, and has\na wide range of applications, dictated by the choice of D. Our algorithm is\nbased on solving the dual of the generalized lasso, which greatly facilitates\ncomputation of the path. For $D=I$ (the usual lasso), we draw a connection\nbetween our approach and the well-known LARS algorithm. For an arbitrary D, we\nderive an unbiased estimate of the degrees of freedom of the generalized lasso\nfit. This estimate turns out to be quite intuitive in many applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2010 23:28:18 GMT"}, {"version": "v2", "created": "Thu, 21 Oct 2010 18:27:15 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2011 17:14:39 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2012 08:28:28 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Tibshirani", "Ryan J.", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1005.2228", "submitter": "Don McLeish", "authors": "Don McLeish", "title": "A general method for debiasing a Monte Carlo estimator", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a process, stochastic or deterministic, obtained by using a\nnumerical integration scheme, or from Monte-Carlo methods involving an\napproximation to an integral, or a Newton-Raphson iteration to approximate the\nroot of an equation. We will assume that we can sample from the distribution of\nthe process from time 0 to finite time n. We propose a scheme for unbiased\nestimation of the limiting value of the process, together with estimates of\nstandard error and apply this to examples including numerical integrals,\nroot-finding and option pricing in a Heston Stochastic Volatility model. This\nresults in unbiased estimators in place of biased ones i nmany potential\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2010 23:33:47 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2010 12:41:56 GMT"}], "update_date": "2010-06-17", "authors_parsed": [["McLeish", "Don", ""]]}, {"id": "1005.2238", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Geoff R. Hosack, Keith R. Hayes", "title": "Ecological non-linear state space model selection via adaptive particle\n  Markov chain Monte Carlo (AdPMCMC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel advanced Particle Markov chain Monte Carlo algorithm that\nis capable of sampling from the posterior distribution of non-linear state\nspace models for both the unobserved latent states and the unknown model\nparameters. We apply this novel methodology to five population growth models,\nincluding models with strong and weak Allee effects, and test if it can\nefficiently sample from the complex likelihood surface that is often associated\nwith these models. Utilising real and also synthetically generated data sets we\nexamine the extent to which observation noise and process error may frustrate\nefforts to choose between these models. Our novel algorithm involves an\nAdaptive Metropolis proposal combined with an SIR Particle MCMC algorithm\n(AdPMCMC). We show that the AdPMCMC algorithm samples complex, high-dimensional\nspaces efficiently, and is therefore superior to standard Gibbs or Metropolis\nHastings algorithms that are known to converge very slowly when applied to the\nnon-linear state space ecological models considered in this paper.\nAdditionally, we show how the AdPMCMC algorithm can be used to recursively\nestimate the Bayesian Cram\\'er-Rao Lower Bound of Tichavsk\\'y (1998). We derive\nexpressions for these Cram\\'er-Rao Bounds and estimate them for the models\nconsidered. Our results demonstrate a number of important features of common\npopulation growth models, most notably their multi-modal posterior surfaces and\ndependence between the static and dynamic parameters. We conclude by sampling\nfrom the posterior distribution of each of the models, and use Bayes factors to\nhighlight how observation noise significantly diminishes our ability to select\namong some of the models, particularly those that are designed to reproduce an\nAllee effect.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 01:28:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Peters", "Gareth W.", ""], ["Hosack", "Geoff R.", ""], ["Hayes", "Keith R.", ""]]}, {"id": "1005.2314", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Some comments on C. S. Wallace's random number generators", "comments": "13 pages. For further information, see\n  http://wwwmaths.anu.edu.au/~brent/pub/pub213.html", "journal-ref": "The Computer Journal 51, 5 (Sept. 2008), 579-584", "doi": "10.1093/comjnl/bxm122", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline some of Chris Wallace's contributions to pseudo-random number\ngeneration. In particular, we consider his idea for generating normally\ndistributed variates without relying on a source of uniform random numbers, and\ncompare it with more conventional methods for generating normal random numbers.\nImplementations of Wallace's idea can be very fast (approximately as fast as\ngood uniform generators). We discuss the statistical quality of the output, and\nmention how certain pitfalls can be avoided.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 13:22:04 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1005.2355", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "On a Multiplicative Algorithm for Computing Bayesian D-optimal Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the minorization-maximization principle (Lange, Hunter and Yang 2000)\nto establish the monotonicity of a multiplicative algorithm for computing\nBayesian D-optimal designs. This proves a conjecture of Dette, Pepelyshev and\nZhigljavsky (2008).\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2010 15:53:10 GMT"}], "update_date": "2010-05-14", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "1005.2908", "submitter": "Xin-She Yang", "authors": "Xin-She Yang and Suash Deb", "title": "Engineering Optimisation by Cuckoo Search", "comments": "17 pages; Yang, X.-S., and Deb, S. (2010), Engineering Optimisation\n  by Cuckoo Search. This revised version included a demo version of the cuckoo\n  search as a supplement", "journal-ref": "Int. J. Mathematical Modelling and Numerical Optimisation, Vol. 1,\n  No.4, 330-343 (2010)", "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new metaheuristic optimisation algorithm, called Cuckoo Search (CS), was\ndeveloped recently by Yang and Deb (2009). This paper presents a more extensive\ncomparison study using some standard test functions and newly designed\nstochastic test functions. We then apply the CS algorithm to solve engineering\ndesign optimisation problems, including the design of springs and welded beam\nstructures. The optimal solutions obtained by CS are far better than the best\nsolutions obtained by an efficient particle swarm optimiser. We will discuss\nthe unique search features used in CS and the implications for further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2010 12:43:51 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2010 09:48:19 GMT"}, {"version": "v3", "created": "Thu, 23 Dec 2010 11:57:01 GMT"}], "update_date": "2010-12-24", "authors_parsed": [["Yang", "Xin-She", ""], ["Deb", "Suash", ""]]}, {"id": "1005.3430", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Nicholas G. Polson", "title": "Simulation-based Regularized Logistic Regression", "comments": "22 pages, 6 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a simulation-based framework for regularized\nlogistic regression, exploiting two novel results for scale mixtures of\nnormals. By carefully choosing a hierarchical model for the likelihood by one\ntype of mixture, and implementing regularization with another, we obtain new\nMCMC schemes with varying efficiency depending on the data type (binary v.\nbinomial, say) and the desired estimator (maximum likelihood, maximum a\nposteriori, posterior mean). Advantages of our omnibus approach include\nflexibility, computational efficiency, applicability in p >> n settings,\nuncertainty estimates, variable selection, and assessing the optimal degree of\nregularization. We compare our methodology to modern alternatives on both\nsynthetic and real data. An R package called reglogit is available on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2010 13:07:12 GMT"}, {"version": "v2", "created": "Thu, 7 Oct 2010 22:00:54 GMT"}, {"version": "v3", "created": "Sun, 7 Aug 2011 22:54:20 GMT"}, {"version": "v4", "created": "Sat, 7 Jan 2012 15:54:39 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "1005.4717", "submitter": "Xi Chen", "authors": "Xi Chen, Qihang Lin, Seyoung Kim, Jaime G. Carbonell, Eric P. Xing", "title": "Smoothing proximal gradient method for general structured sparse\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS514 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 719-752", "doi": "10.1214/11-AOAS514", "report-no": "IMS-AOAS-AOAS514", "categories": "stat.ML cs.LG math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating high-dimensional regression models\nregularized by a structured sparsity-inducing penalty that encodes prior\nstructural information on either the input or output variables. We consider two\nwidely adopted types of penalties of this kind as motivating examples: (1) the\ngeneral overlapping-group-lasso penalty, generalized from the group-lasso\npenalty; and (2) the graph-guided-fused-lasso penalty, generalized from the\nfused-lasso penalty. For both types of penalties, due to their nonseparability\nand nonsmoothness, developing an efficient optimization method remains a\nchallenging problem. In this paper we propose a general optimization approach,\nthe smoothing proximal gradient (SPG) method, which can solve structured sparse\nregression problems with any smooth convex loss under a wide spectrum of\nstructured sparsity-inducing penalties. Our approach combines a smoothing\ntechnique with an effective proximal gradient method. It achieves a convergence\nrate significantly faster than the standard first-order methods, subgradient\nmethods, and is much more scalable than the most widely used interior-point\nmethods. The efficiency and scalability of our method are demonstrated on both\nsimulation experiments and real genetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 00:50:17 GMT"}, {"version": "v2", "created": "Sun, 21 Nov 2010 21:24:00 GMT"}, {"version": "v3", "created": "Sat, 26 Mar 2011 01:17:05 GMT"}, {"version": "v4", "created": "Fri, 29 Jun 2012 05:53:50 GMT"}], "update_date": "2012-07-02", "authors_parsed": [["Chen", "Xi", ""], ["Lin", "Qihang", ""], ["Kim", "Seyoung", ""], ["Carbonell", "Jaime G.", ""], ["Xing", "Eric P.", ""]]}, {"id": "1005.4797", "submitter": "Ajay Jasra", "authors": "Ajay Jasra and Pierre Del Moral", "title": "Sequential Monte Carlo Methods for Option Pricing", "comments": "37 Pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper we provide a review and development of sequential\nMonte Carlo (SMC) methods for option pricing. SMC are a class of Monte\nCarlo-based algorithms, that are designed to approximate expectations w.r.t a\nsequence of related probability measures. These approaches have been used,\nsuccessfully, for a wide class of applications in engineering, statistics,\nphysics and operations research. SMC methods are highly suited to many option\npricing problems and sensitivity/Greek calculations due to the nature of the\nsequential simulation. However, it is seldom the case that such ideas are\nexplicitly used in the option pricing literature. This article provides an\nup-to date review of SMC methods, which are appropriate for option pricing. In\naddition, it is illustrated how a number of existing approaches for option\npricing can be enhanced via SMC. Specifically, when pricing the arithmetic\nAsian option w.r.t a complex stochastic volatility model, it is shown that SMC\nmethods provide additional strategies to improve estimation.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2010 12:28:41 GMT"}], "update_date": "2010-05-27", "authors_parsed": [["Jasra", "Ajay", ""], ["Del Moral", "Pierre", ""]]}, {"id": "1005.5085", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "A simple and efficient algorithm for fused lasso signal approximator\n  with convex loss function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the augmented Lagrangian method (ALM) as a solver for the fused\nlasso signal approximator (FLSA) problem. The ALM is a dual method in which\nsquares of the constraint functions are added as penalties to the Lagrangian.\nIn order to apply this method to FLSA, two types of auxiliary variables are\nintroduced to transform the original unconstrained minimization problem into a\nlinearly constrained minimization problem. Each updating in this iterative\nalgorithm consists of just a simple one-dimensional convex programming problem,\nwith closed form solution in many cases. While the existing literature mostly\nfocused on the quadratic loss function, our algorithm can be easily implemented\nfor general convex loss. The most attractive feature of this algorithm is its\nsimplicity in implementation compared to other existing fast solvers. We also\nprovide some convergence analysis of the algorithm. Finally, the method is\nillustrated with some simulation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2010 15:08:14 GMT"}], "update_date": "2010-05-28", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1005.5201", "submitter": "Yanan Fan Dr", "authors": "S. A. Sisson, G. W. Peters, M. Briers, Y. Fan", "title": "A note on target distribution ambiguity of likelihood-free samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for Bayesian simulation in the presence of computationally\nintractable likelihood functions are of growing interest. Termed\nlikelihood-free samplers, standard simulation algorithms such as Markov chain\nMonte Carlo have been adapted for this setting. In this article, by presenting\ngeneralisations of existing algorithms, we demonstrate that likelihood-free\nsamplers can be ambiguous over the form of the target distribution. We also\nconsider the theoretical justification of these samplers. Distinguishing\nbetween the forms of the target distribution may have implications for the\nfuture development of likelihood-free samplers.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2010 02:03:34 GMT"}], "update_date": "2010-05-31", "authors_parsed": [["Sisson", "S. A.", ""], ["Peters", "G. W.", ""], ["Briers", "M.", ""], ["Fan", "Y.", ""]]}]