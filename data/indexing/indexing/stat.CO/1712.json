[{"id": "1712.00182", "submitter": "Furong Sun", "authors": "Furong Sun, Robert B. Gramacy, Benjamin Haaland, Earl Lawrence, and\n  Andrew Walker", "title": "Emulating satellite drag from large simulation experiments", "comments": "44 pages; 16 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining accurate estimates of satellite drag coefficients in low Earth\norbit is a crucial component in positioning and collision avoidance. Simulators\ncan produce accurate estimates, but their computational expense is much too\nlarge for real-time application. A pilot study showed that Gaussian process\n(GP) surrogate models could accurately emulate simulations. However, cubic\nruntime for training GPs means that they could only be applied to a narrow\nrange of input configurations to achieve the desired level of accuracy. In this\npaper we show how extensions to the local approximate Gaussian Process (laGP)\nmethod allow accurate full-scale emulation. The new methodological\ncontributions, which involve a multi-level global/local modeling approach, and\na set-wise approach to local subset selection, are shown to perform well in\nbenchmark and synthetic data settings. We conclude by demonstrating that our\nmethod achieves the desired level of accuracy, besting simpler viable (i.e.,\ncomputationally tractable) global and local modeling approaches, when trained\non seventy thousand core hours of drag simulations for two real-world\nsatellites: the Hubble space telescope (HST) and the gravity recovery and\nclimate experiment (GRACE).\n", "versions": [{"version": "v1", "created": "Fri, 1 Dec 2017 04:07:26 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 14:57:52 GMT"}, {"version": "v3", "created": "Thu, 13 Dec 2018 01:45:04 GMT"}, {"version": "v4", "created": "Thu, 7 Mar 2019 23:31:28 GMT"}, {"version": "v5", "created": "Sat, 22 Jun 2019 19:36:51 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Sun", "Furong", ""], ["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""], ["Lawrence", "Earl", ""], ["Walker", "Andrew", ""]]}, {"id": "1712.00546", "submitter": "Arindam Fadikar", "authors": "Arindam Fadikar, Dave Higdon, Jiangzhuo Chen, Brian Lewis, Srini\n  Venkatramanan, and Madhav Marathe", "title": "Calibrating a Stochastic Agent Based Model Using Quantile-based\n  Emulation", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": "10.1137/17M1161233", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a number of cases, the Quantile Gaussian Process (QGP) has proven\neffective in emulating stochastic, univariate computer model output (Plumlee\nand Tuo, 2014). In this paper, we develop an approach that uses this emulation\napproach within a Bayesian model calibration framework to calibrate an\nagent-based model of an epidemic. In addition, this approach is extended to\nhandle the multivariate nature of the model output, which gives a time series\nof the count of infected individuals. The basic modeling approach is adapted\nfrom Higdon et al. (2008), using a basis representation to capture the\nmultivariate model output. The approach is motivated with an example taken from\nthe 2015 Ebola Challenge workshop which simulated an ebola epidemic to evaluate\nmethodology.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 04:20:54 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Fadikar", "Arindam", ""], ["Higdon", "Dave", ""], ["Chen", "Jiangzhuo", ""], ["Lewis", "Brian", ""], ["Venkatramanan", "Srini", ""], ["Marathe", "Madhav", ""]]}, {"id": "1712.00716", "submitter": "Qing Qu", "authors": "Qing Qu, Yuqian Zhang, Yonina C. Eldar, and John Wright", "title": "Convolutional Phase Retrieval via Gradient Descent", "comments": "64 pages , 9 figures, appeared in NeurIPS 2017. Accepted at IEEE\n  Transactions on Information Theory. This is the final (minor) update: fixed\n  typos and grammar issues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.NA math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convolutional phase retrieval problem, of recovering an unknown\nsignal $\\mathbf x \\in \\mathbb C^n $ from $m$ measurements consisting of the\nmagnitude of its cyclic convolution with a given kernel $\\mathbf a \\in \\mathbb\nC^m $. This model is motivated by applications such as channel estimation,\noptics, and underwater acoustic communication, where the signal of interest is\nacted on by a given channel/filter, and phase information is difficult or\nimpossible to acquire. We show that when $\\mathbf a$ is random and the number\nof observations $m$ is sufficiently large, with high probability $\\mathbf x$\ncan be efficiently recovered up to a global phase shift using a combination of\nspectral initialization and generalized gradient descent. The main challenge is\ncoping with dependencies in the measurement operator. We overcome this\nchallenge by using ideas from decoupling theory, suprema of chaos processes and\nthe restricted isometry property of random circulant matrices, and recent\nanalysis of alternating minimization methods.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 06:04:25 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 03:30:06 GMT"}, {"version": "v3", "created": "Sun, 6 Oct 2019 02:55:12 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Qu", "Qing", ""], ["Zhang", "Yuqian", ""], ["Eldar", "Yonina C.", ""], ["Wright", "John", ""]]}, {"id": "1712.00771", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Kengo Kato", "title": "Randomized incomplete $U$-statistics in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference for the mean vector of a high-dimensional\n$U$-statistic. In the era of Big Data, the dimension $d$ of the $U$-statistic\nand the sample size $n$ of the observations tend to be both large, and the\ncomputation of the $U$-statistic is prohibitively demanding. Data-dependent\ninferential procedures such as the empirical bootstrap for $U$-statistics is\neven more computationally expensive. To overcome such computational bottleneck,\nincomplete $U$-statistics obtained by sampling fewer terms of the $U$-statistic\nare attractive alternatives. In this paper, we introduce randomized incomplete\n$U$-statistics with sparse weights whose computational cost can be made\nindependent of the order of the $U$-statistic. We derive non-asymptotic\nGaussian approximation error bounds for the randomized incomplete\n$U$-statistics in high dimensions, namely in cases where the dimension $d$ is\npossibly much larger than the sample size $n$, for both non-degenerate and\ndegenerate kernels. In addition, we propose generic bootstrap methods for the\nincomplete $U$-statistics that are computationally much less-demanding than\nexisting bootstrap methods, and establish finite sample validity of the\nproposed bootstrap methods. Our methods are illustrated on the application to\nnonparametric testing for the pairwise independence of a high-dimensional\nrandom vector under weaker assumptions than those appearing in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 14:01:42 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 16:34:38 GMT"}, {"version": "v3", "created": "Wed, 13 Jun 2018 06:26:12 GMT"}, {"version": "v4", "created": "Sun, 27 Jan 2019 20:11:03 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Chen", "Xiaohui", ""], ["Kato", "Kengo", ""]]}, {"id": "1712.00849", "submitter": "Alexander Terenin", "authors": "David Draper and Alexander Terenin", "title": "Comment: A brief survey of the current state of play for Bayesian\n  computation in data science at Big-Data scale", "comments": null, "journal-ref": "Brazilian Journal of Probability and Statistics 31(4):686-691,\n  2017", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to contribute to the discussion of \"Comparing Consensus Monte Carlo\nStrategies for Distributed Bayesian Computation\" by offering our views on the\ncurrent best methods for Bayesian computation, both at big-data scale and with\nsmaller data sets, as summarized in Table 1. This table is certainly an\nover-simplification of a highly complicated area of research in constant\n(present and likely future) flux, but we believe that constructing summaries of\nthis type is worthwhile despite their drawbacks, if only to facilitate further\ndiscussion.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 22:59:57 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 18:50:40 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Draper", "David", ""], ["Terenin", "Alexander", ""]]}, {"id": "1712.00993", "submitter": "Nicolas Langren\\'e", "authors": "Nicolas Langren\\'e, Xavier Warin", "title": "Fast and stable multivariate kernel density estimation by fast sum\n  updating", "comments": "38 pages, 29 figures", "journal-ref": "Journal of Computational and Graphical Statistics 28(3) 596-608\n  (2019)", "doi": "10.1080/10618600.2018.1549052", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation and kernel regression are powerful but\ncomputationally expensive techniques: a direct evaluation of kernel density\nestimates at $M$ evaluation points given $N$ input sample points requires a\nquadratic $\\mathcal{O}(MN)$ operations, which is prohibitive for large scale\nproblems. For this reason, approximate methods such as binning with Fast\nFourier Transform or the Fast Gauss Transform have been proposed to speed up\nkernel density estimation. Among these fast methods, the Fast Sum Updating\napproach is an attractive alternative, as it is an exact method and its speed\nis independent of the input sample and the bandwidth. Unfortunately, this\nmethod, based on data sorting, has for the most part been limited to the\nunivariate case. In this paper, we revisit the fast sum updating approach and\nextend it in several ways. Our main contribution is to extend it to the general\nmultivariate case for general input data and rectilinear evaluation grid. Other\ncontributions include its extension to a wider class of kernels, including the\ntriangular, cosine and Silverman kernels, its combination with parsimonious\nadditive multivariate kernels, and its combination with a fast approximate\nk-nearest-neighbors bandwidth for multivariate datasets. Our numerical tests of\nmultivariate regression and density estimation confirm the speed, accuracy and\nstability of the method. We hope this paper will renew interest for the fast\nsum updating approach and help solve large-scale practical density estimation\nand regression problems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 10:31:44 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 01:51:13 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Langren\u00e9", "Nicolas", ""], ["Warin", "Xavier", ""]]}, {"id": "1712.01231", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri", "title": "Sub-clustering in decomposable graphs and size-varying junction trees", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel representation of decomposable graphs based on\nsemi-latent tree-dependent bipartite graphs. The novel representation has two\nmain benefits. First, it enables a form of sub-clustering within maximal\ncliques of the graph, adding informational richness to the general use of\ndecomposable graphs that could be harnessed in applications with behavioural\ntype of data. Second, it allows for a new node-driven Markov chain Monte Carlo\nsampler of decomposable graphs that can easily parallelize and scale. The\nproposed sampler also benefits from the computational efficiency of\njunction-tree-based samplers of decomposable graphs.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 18:12:24 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Elmasri", "Mohamad", ""]]}, {"id": "1712.01410", "submitter": "Boxiang Liu", "authors": "Boxiang Liu, Thomas Quertermous", "title": "Approximating the Sum of Independent Non-Identical Binomial Random\n  Variables", "comments": "11 pages, 6 figures, submitted to the R journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of sum of independent non-identical binomial random\nvariables is frequently encountered in areas such as genomics, healthcare, and\noperations research. Analytical solutions to the density and distribution are\nusually cumbersome to find and difficult to compute. Several methods have been\ndeveloped to approximate the distribution, and among these is the saddlepoint\napproximation. However, implementation of the saddlepoint approximation is\nnon-trivial and, to our knowledge, an R package is still lacking. In this\npaper, we implemented the saddlepoint approximation in the \\textbf{sinib}\npackage. We provide two examples to illustrate its usage. One example uses\nsimulated data while the other uses real-world healthcare data. The\n\\textbf{sinib} package addresses the gap between the theory and the\nimplementation of approximating the sum of independent non-identical binomials.\n", "versions": [{"version": "v1", "created": "Mon, 4 Dec 2017 23:20:38 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Liu", "Boxiang", ""], ["Quertermous", "Thomas", ""]]}, {"id": "1712.01521", "submitter": "Wei Xiao", "authors": "Wei Xiao", "title": "An Online Algorithm for Nonparametric Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric correlations such as Spearman's rank correlation and Kendall's\ntau correlation are widely applied in scientific and engineering fields. This\npaper investigates the problem of computing nonparametric correlations on the\nfly for streaming data. Standard batch algorithms are generally too slow to\nhandle real-world big data applications. They also require too much memory\nbecause all the data need to be stored in the memory before processing. This\npaper proposes a novel online algorithm for computing nonparametric\ncorrelations. The algorithm has O(1) time complexity and O(1) memory cost and\nis quite suitable for edge devices, where only limited memory and processing\npower are available. You can seek a balance between speed and accuracy by\nchanging the number of cutpoints specified in the algorithm. The online\nalgorithm can compute the nonparametric correlations 10 to 1,000 times faster\nthan the corresponding batch algorithm, and it can compute them based either on\nall past observations or on fixed-size sliding windows.\n", "versions": [{"version": "v1", "created": "Tue, 5 Dec 2017 08:19:11 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Xiao", "Wei", ""]]}, {"id": "1712.02009", "submitter": "Adityanand Guntuboyina", "authors": "Sujayam Saha and Adityanand Guntuboyina", "title": "On the nonparametric maximum likelihood estimator for Gaussian location\n  mixture densities with application to Gaussian denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Nonparametric Maximum Likelihood Estimator (NPMLE) for\nestimating Gaussian location mixture densities in $d$-dimensions from\nindependent observations. Unlike usual likelihood-based methods for fitting\nmixtures, NPMLEs are based on convex optimization. We prove finite sample\nresults on the Hellinger accuracy of every NPMLE. Our results imply, in\nparticular, that every NPMLE achieves near parametric risk (up to logarithmic\nmultiplicative factors) when the true density is a discrete Gaussian mixture\nwithout any prior information on the number of mixture components. NPMLEs can\nnaturally be used to yield empirical Bayes estimates of the Oracle Bayes\nestimator in the Gaussian denoising problem. We prove bounds for the accuracy\nof the empirical Bayes estimate as an approximation to the Oracle Bayes\nestimator. Here our results imply that the empirical Bayes estimator performs\nat nearly the optimal level (up to logarithmic multiplicative factors) for\ndenoising in clustering situations without any prior knowledge of the number of\nclusters.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 02:30:08 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 23:36:07 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Saha", "Sujayam", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "1712.02070", "submitter": "Jiangjiang Zhang", "authors": "Jiangjiang Zhang, Jun Man, Guang Lin, Laosheng Wu, Lingzao Zeng", "title": "Inverse modeling of hydrologic systems with adaptive multi-fidelity\n  Markov chain Monte Carlo simulations", "comments": "57 pages,16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) simulation methods are widely used to assess\nparametric uncertainties of hydrologic models conditioned on measurements of\nobservable state variables. However, when the model is CPU-intensive and\nhigh-dimensional, the computational cost of MCMC simulation will be\nprohibitive. In this situation, a CPU-efficient while less accurate\nlow-fidelity model (e.g., a numerical model with a coarser discretization, or a\ndata-driven surrogate) is usually adopted. Nowadays, multi-fidelity simulation\nmethods that can take advantage of both the efficiency of the low-fidelity\nmodel and the accuracy of the high-fidelity model are gaining popularity. In\nthe MCMC simulation, as the posterior distribution of the unknown model\nparameters is the region of interest, it is wise to distribute most of the\ncomputational budget (i.e., the high-fidelity model evaluations) therein. Based\non this idea, in this paper we propose an adaptive multi-fidelity MCMC\nalgorithm for efficient inverse modeling of hydrologic systems. In this method,\nwe evaluate the high-fidelity model mainly in the posterior region through\niteratively running MCMC based on a Gaussian process (GP) system that is\nadaptively constructed with multi-fidelity simulation. The error of the GP\nsystem is rigorously considered in the MCMC simulation and gradually reduced to\na negligible level in the posterior region. Thus, the proposed method can\nobtain an accurate estimate of the posterior distribution with a small number\nof the high-fidelity model evaluations. The performance of the proposed method\nis demonstrated by three numerical case studies in inverse modeling of\nhydrologic systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 07:50:31 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 04:21:58 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2018 04:36:50 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Zhang", "Jiangjiang", ""], ["Man", "Jun", ""], ["Lin", "Guang", ""], ["Wu", "Laosheng", ""], ["Zeng", "Lingzao", ""]]}, {"id": "1712.02195", "submitter": "Ranjan Maitra", "authors": "Alejandro Murua and Ranjan Maitra", "title": "Fast spatial inference in the homogeneous Ising model", "comments": "18 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is important in statistical modeling and inference in many\napplications, however its normalizing constant, mean number of active vertices\nand mean spin interaction are intractable. We provide accurate approximations\nthat make it possible to calculate these quantities numerically. Simulation\nstudies indicate good performance when compared to Markov Chain Monte Carlo\nmethods and at a tiny fraction of the time. The methodology is also used to\nperform Bayesian inference in a functional Magnetic Resonance Imaging\nactivation detection experiment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Dec 2017 14:24:34 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 00:10:33 GMT"}], "update_date": "2018-02-02", "authors_parsed": [["Murua", "Alejandro", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.02214", "submitter": "Chaojie Wang", "authors": "Chaojie Wang, Linghao Shen, Han Li and Xiaodan Fan", "title": "Nonparametric Statistical Inference and Imputation for Incomplete\n  Categorical Data", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missingness in categorical data is a common problem in various real\napplications. Traditional approaches either utilize only the complete\nobservations or impute the missing data by some ad hoc methods rather than the\ntrue conditional distribution of the missing data, thus losing or distorting\nthe rich information in the partial observations. In this paper, we propose the\nDirichlet Process Mixture of Collapsed Product-Multinomials (DPMCPM) to model\nthe full data jointly and compute the model efficiently. By fitting an infinite\nmixture of product-multinomial distributions, DPMCPM is applicable for any\ncategorical data regardless of the true distribution, which may contain complex\nassociation among variables. Under the framework of latent class analysis, we\nshow that DPMCPM can model general missing mechanisms by creating an extra\ncategory to denote missingness, which implicitly integrates out the missing\npart with regard to their true conditional distribution. Through simulation\nstudies and a real application, we demonstrate that DPMCPM outperforms existing\napproaches on statistical inference and imputation for incomplete categorical\ndata of various missing mechanisms. DPMCPM is implemented as the R package\n\\texttt{MMDai}, which is available from the Comprehensive R Archive Network at\nhttps://cran.r-project.org/web/packages/MMDai/index.html.\n", "versions": [{"version": "v1", "created": "Sat, 2 Dec 2017 08:51:36 GMT"}, {"version": "v2", "created": "Thu, 11 Jul 2019 06:22:21 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Chaojie", ""], ["Shen", "Linghao", ""], ["Li", "Han", ""], ["Fan", "Xiaodan", ""]]}, {"id": "1712.02469", "submitter": "Chunlin Wang", "authors": "Chunlin Wang, Paul Marriott and Pengfei Li", "title": "Asymptotic coverage probabilities of bootstrap percentile confidence\n  intervals for constrained parameters", "comments": "22 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic behaviour of the commonly used bootstrap percentile confidence\ninterval is investigated when the parameters are subject to linear inequality\nconstraints. We concentrate on the important one- and two-sample problems with\ndata generated from general parametric distributions in the natural exponential\nfamily. The focus of this paper is on quantifying the coverage probabilities of\nthe parametric bootstrap percentile confidence intervals, in particular their\nlimiting behaviour near boundaries. We propose a local asymptotic framework to\nstudy this subtle coverage behaviour. Under this framework, we discover that\nwhen the true parameters are on, or close to, the restriction boundary, the\nasymptotic coverage probabilities can always exceed the nominal level in the\none-sample case; however, they can be, remarkably, both under and over the\nnominal level in the two-sample case. Using illustrative examples, we show that\nthe results provide theoretical justification and guidance on applying the\nbootstrap percentile method to constrained inference problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 01:51:35 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Wang", "Chunlin", ""], ["Marriott", "Paul", ""], ["Li", "Pengfei", ""]]}, {"id": "1712.02749", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster, Paul G. Constantine, T.J. Sullivan", "title": "Exact active subspace Metropolis-Hastings, with applications to the\n  Lorenz-96 system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of active subspaces to inform a\nMetropolis-Hastings algorithm, thereby aggressively reducing the computational\ndimension of the sampling problem. We show that the original formulation, as\nproposed by Constantine, Kent, and Bui-Thanh (SIAM J. Sci. Comput.,\n38(5):A2779-A2805, 2016), possesses asymptotic bias. Using pseudo-marginal\narguments, we develop an asymptotically unbiased variant. Our algorithm is\napplied to a synthetic multimodal target distribution as well as a Bayesian\nformulation of a parameter inference problem for a Lorenz-96 system.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:53:57 GMT"}], "update_date": "2017-12-08", "authors_parsed": [["Schuster", "Ingmar", ""], ["Constantine", "Paul G.", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1712.02750", "submitter": "Martin Lysy", "authors": "Masoud Asgharian, Martin Lysy, and Vahid Partovi Nia", "title": "A Convergence Diagnostic for Bayesian Clustering", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of Bayesian clustering, posterior sampling on the\ndiscrete state space of cluster allocations is achieved via Markov chain Monte\nCarlo (MCMC) techniques. As it is typically challenging to design transition\nkernels to explore this state space efficiently, MCMC convergence diagnostics\nfor clustering applications is especially important. For general MCMC problems,\nstate-of-the-art convergence diagnostics involve comparisons across multiple\nchains. However, single-chain alternatives can be appealing for computationally\nintensive and slowly-mixing MCMC, as is typically the case for Bayesian\nclustering. Thus, we propose here a single-chain convergence diagnostic\nspecifically tailored to discrete-space MCMC. Namely, we consider a\nHotelling-type statistic on the highest probability states, and use\nregenerative sampling theory to derive its equilibrium distribution. By\nleveraging information from the unnormalized posterior, our diagnostic protects\nagainst seemingly convergent chains in which the relative frequency of visited\nstates is incorrect. The methodology is illustrated with a Bayesian clustering\nanalysis of genetic mutants of the flowering plant Arabidopsis thaliana.\n", "versions": [{"version": "v1", "created": "Thu, 7 Dec 2017 17:59:40 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 19:40:44 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Asgharian", "Masoud", ""], ["Lysy", "Martin", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1712.02964", "submitter": "Amir Nikooienejad", "authors": "Amir Nikooienejad, Wenyi Wang and Valen E. Johnson", "title": "Bayesian Variable Selection For Survival Data Using Inverse Moment\n  Priors", "comments": "33 pages, 11 figures", "journal-ref": "Annals of Applied Statistics 14.2 (2020): 809-828", "doi": "10.1214/20-AOAS1325", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient variable selection in high-dimensional cancer genomic studies is\ncritical for discovering genes associated with specific cancer types and for\npredicting response to treatment. Censored survival data is prevalent in such\nstudies. In this article we introduce a Bayesian variable selection procedure\nthat uses a mixture prior composed of a point mass at zero and an inverse\nmoment prior in conjunction with the partial likelihood defined by the Cox\nproportional hazard model. The procedure is implemented in the R package\nBVSNLP, which supports parallel computing and uses a stochastic search method\nto explore the model space. Bayesian model averaging is used for prediction.\nThe proposed algorithm provides better performance than other variable\nselection procedures in simulation studies, and appears to provide more\nconsistent variable selection when applied to actual genomic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 8 Dec 2017 06:52:52 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 22:19:47 GMT"}, {"version": "v3", "created": "Tue, 22 May 2018 07:45:30 GMT"}, {"version": "v4", "created": "Tue, 29 May 2018 20:55:07 GMT"}, {"version": "v5", "created": "Mon, 4 Jun 2018 21:52:26 GMT"}, {"version": "v6", "created": "Wed, 9 Oct 2019 04:47:34 GMT"}, {"version": "v7", "created": "Sat, 14 Mar 2020 20:18:17 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Nikooienejad", "Amir", ""], ["Wang", "Wenyi", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1712.03807", "submitter": "Moritz Schauer", "authors": "Marcin Mider, Moritz Schauer and Frank van der Meulen", "title": "Continuous-discrete smoothing of diffusions", "comments": "Revised article with additional author Marcin Mider. Article contains\n  an animated figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose X is a multivariate diffusion process that is observed discretely in\ntime. At each observation time, a transformation of the state of the process is\nobserved with noise. The smoothing problem consists of recovering the path of\nthe process, consistent with the observations. We derive a novel Markov Chain\nMonte Carlo algorithm to sample from the exact smoothing distribution. The\nresulting algorithm is called the Backward Filtering Forward Guiding (BFFG)\nalgorithm. We extend the algorithm to include parameter estimation. The\nproposed method relies on guided proposals introduced in Schauer et al. (2017).\nWe illustrate its efficiency in a number of challenging problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:04:57 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 16:59:22 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 08:56:19 GMT"}, {"version": "v4", "created": "Tue, 22 Jun 2021 11:58:02 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Mider", "Marcin", ""], ["Schauer", "Moritz", ""], ["van der Meulen", "Frank", ""]]}, {"id": "1712.03852", "submitter": "Ryan Martin", "authors": "Minwoo Chae and Ryan Martin and Stephen G. Walker", "title": "Fast nonparametric near-maximum likelihood estimation of a mixing\n  density", "comments": "11 pages, 3 figures", "journal-ref": "Statistics & Probability Letters, 2018, volume 140, pages 142--146", "doi": "10.1016/j.spl.2018.05.012", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are regularly used in density estimation applications, but the\nproblem of estimating the mixing distribution remains a challenge.\nNonparametric maximum likelihood produce estimates of the mixing distribution\nthat are discrete, and these may be hard to interpret when the true mixing\ndistribution is believed to have a smooth density. In this paper, we\ninvestigate an algorithm that produces a sequence of smooth estimates that has\nbeen conjectured to converge to the nonparametric maximum likelihood estimator.\nHere we give a rigorous proof of this conjecture, and propose a new data-driven\nstopping rule that produces smooth near-maximum likelihood estimates of the\nmixing density, and simulations demonstrate the quality empirical performance\nof this estimator.\n", "versions": [{"version": "v1", "created": "Mon, 11 Dec 2017 15:55:18 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chae", "Minwoo", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1712.04088", "submitter": "Manoel Santos Neto", "authors": "Danillo Xavier and Manoel Santos-Neto and Marcelo Bourguignon and Vera\n  Tomazella", "title": "Zero-Modified Poisson-Lindley distribution with applications in\n  zero-inflated and zero-deflated count data", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main object of this article is to present an extension of the\nzero-inflated Poisson-Lindley distribution, called of zero-modified\nPoisson-Lindley. The additional parameter $\\pi$ of the zero-modified\nPoisson-Lindley has a natural interpretation in terms of either\nzero-deflated/inflated proportion. Inference is dealt with by using the\nlikelihood approach. In particular the maximum likelihood estimators of the\ndistribution's parameter are compared in small and large samples. We also\nconsider an alternative bias-correction mechanism based on Efron's bootstrap\nresampling. The model is applied to real data sets and found to perform better\nthan other competing models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 01:18:15 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 11:03:12 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Xavier", "Danillo", ""], ["Santos-Neto", "Manoel", ""], ["Bourguignon", "Marcelo", ""], ["Tomazella", "Vera", ""]]}, {"id": "1712.04200", "submitter": "Bram Thijssen", "authors": "Bram Thijssen, Lodewyk F.A. Wessels", "title": "Approximating multivariate posterior distribution functions from Monte\n  Carlo samples for sequential Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important feature of Bayesian statistics is the opportunity to do\nsequential inference: the posterior distribution obtained after seeing a\ndataset can be used as prior for a second inference. However, when Monte Carlo\nsampling methods are used for inference, we only have a set of samples from the\nposterior distribution. To do sequential inference, we then either have to\nevaluate the second posterior at only these locations and reweight the samples\naccordingly, or we can estimate a functional description of the posterior\nprobability distribution from the samples and use that as prior for the second\ninference. Here, we investigated to what extent we can obtain an accurate joint\nposterior from two datasets if the inference is done sequentially rather than\njointly, under the condition that each inference step is done using Monte Carlo\nsampling. To test this, we evaluated the accuracy of kernel density estimates,\nGaussian mixtures, vine copulas and Gaussian processes in approximating\nposterior distributions, and then tested whether these approximations can be\nused in sequential inference. In low dimensionality, Gaussian processes are\nmore accurate, whereas in higher dimensionality Gaussian mixtures or vine\ncopulas perform better. In our test cases, posterior approximations are\npreferable over direct sample reweighting, although joint inference is still\npreferable over sequential inference. Since the performance is case-specific,\nwe provide an R package mvdens with a unified interface for the density\napproximation methods.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 10:09:27 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 07:44:01 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Thijssen", "Bram", ""], ["Wessels", "Lodewyk F. A.", ""]]}, {"id": "1712.04542", "submitter": "Arun Venkitaraman", "authors": "Arun Venkitaraman and Dave Zachariah", "title": "Learning Sparse Graphs for Prediction and Filtering of Multivariate Data\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of prediction of multivariate data process using an\nunderlying graph model. We develop a method that learns a sparse partial\ncorrelation graph in a tuning-free and computationally efficient manner.\nSpecifically, the graph structure is learned recursively without the need for\ncross-validation or parameter tuning by building upon a hyperparameter-free\nframework. Our approach does not require the graph to be undirected and also\naccommodates varying noise levels across different nodes.Experiments using\nreal-world datasets show that the proposed method offers significant\nperformance gains in prediction, in comparison with the graphs frequently\nassociated with these datasets.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 21:39:18 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 19:17:23 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Venkitaraman", "Arun", ""], ["Zachariah", "Dave", ""]]}, {"id": "1712.04712", "submitter": "Sorawit Saengkyongam", "authors": "Sorawit Saengkyongam, Anthony Hayter, Seksan Kiatsupaibul, Wei Liu", "title": "Efficient Computation of the Stochastic Behavior of Partial Sum\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the computational aspects of probability calculations for\ndynamical partial sum expressions are discussed. Such dynamical partial sum\nexpressions have many important applications, and examples are provided in the\nfields of reliability, product quality assessment, and stochastic control.\nWhile these probability calculations are ostensibly of a high dimension, and\nconsequently intractable in general, it is shown how a recursive integration\nmethodology can be implemented to obtain exact calculations as a series of\ntwo-dimensional calculations. The computational aspects of the implementaion of\nthis methodology, with the adoption of Fast Fourier Transforms, are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 11:28:04 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Saengkyongam", "Sorawit", ""], ["Hayter", "Anthony", ""], ["Kiatsupaibul", "Seksan", ""], ["Liu", "Wei", ""]]}, {"id": "1712.04723", "submitter": "Jialiang Mao", "authors": "Jialiang Mao, Yuhan Chen, Li Ma", "title": "Bayesian graphical compositional regression for microbiome data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in microbiome studies is to test the existence of and give\ncharacterization to differences in the microbiome composition across groups of\nsamples. Important challenges of this problem include the large within-group\nheterogeneities among samples and the existence of potential confounding\nvariables that, when ignored, increase the chance of false discoveries and\nreduce the power for identifying true differences. We propose a probabilistic\nframework to overcome these issues by combining three ideas: (i) a phylogenetic\ntree-based decomposition of the cross-group comparison problem into a series of\nlocal tests, (ii) a graphical model that links the local tests to allow\ninformation sharing across taxa, and (iii) a Bayesian testing strategy that\nincorporates covariates and integrates out the within-group variation, avoiding\npotentially unstable point estimates. We derive an efficient inference\nalgorithm based on numerical integration and junction-tree message passing,\nconduct extensive simulation studies to investigate the performance of our\napproach, and compare it to state-of-the-art methods in a number of\nrepresentative settings. We then apply our method to the American Gut data to\nanalyze the association of dietary habits and human's gut microbiome\ncomposition in the presence of covariates, and illustrate the importance of\nincorporating covariates in microbiome cross-group comparison.\n", "versions": [{"version": "v1", "created": "Wed, 13 Dec 2017 12:06:21 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 19:08:17 GMT"}, {"version": "v3", "created": "Fri, 3 May 2019 20:46:00 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Mao", "Jialiang", ""], ["Chen", "Yuhan", ""], ["Ma", "Li", ""]]}, {"id": "1712.05358", "submitter": "Lampros Bouranis", "authors": "Lampros Bouranis, Nial Friel, Florian Maire", "title": "Model comparison for Gibbs random fields using noisy reversible jump\n  Markov chain Monte Carlo", "comments": "Accepted for publication in Computational Statistics and Data\n  Analysis", "journal-ref": "Computational Statistics and Data Analysis 128 (2018) 221-241", "doi": "10.1016/j.csda.2018.07.005", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reversible jump Markov chain Monte Carlo (RJMCMC) method offers an\nacross-model simulation approach for Bayesian estimation and model comparison,\nby exploring the sampling space that consists of several models of possibly\nvarying dimensions. A naive implementation of RJMCMC to models like Gibbs\nrandom fields suffers from computational difficulties: the posterior\ndistribution for each model is termed doubly-intractable since computation of\nthe likelihood function is rarely available. Consequently, it is simply\nimpossible to simulate a transition of the Markov chain in the presence of\nlikelihood intractability. A variant of RJMCMC is presented, called noisy\nRJMCMC, where the underlying transition kernel is replaced with an\napproximation based on unbiased estimators. Based on previous theoretical\ndevelopments, convergence guarantees for the noisy RJMCMC algorithm are\nprovided. The experiments show that the noisy RJMCMC algorithm can be much more\nefficient than other exact methods, provided that an estimator with controlled\nMonte Carlo variance is used, a fact which is in agreement with the theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 17:37:52 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 14:20:51 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 22:51:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Bouranis", "Lampros", ""], ["Friel", "Nial", ""], ["Maire", "Florian", ""]]}, {"id": "1712.05460", "submitter": "John Lombard", "authors": "John Lombard", "title": "Honey from the Hives: A Theoretical and Computational Exploration of\n  Combinatorial Hives", "comments": "25 pages, 20 figures", "journal-ref": null, "doi": "10.1080/10586458.2018.1473822", "report-no": null, "categories": "math.CO math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the first half of this manuscript, we begin with a brief review of\ncombinatorial hives as introduced by Knutson and Tao, and focus on a conjecture\nby Danilov and Koshevoy for generating such a hive from Hermitian matrix pairs\nthrough an optimization scheme. We examine a proposal by Appleby and Whitehead\nin the spirit of this conjecture and analytically elucidate an obstruction in\ntheir construction for guaranteeing hive generation, while detailing stronger\nconditions under which we can produce hives with almost certain probability. We\nprovide the first mapping of this prescription onto a practical algorithmic\nspace that enables us to produce affirming computational results and open a new\narea of research into the analysis of the random geometries and curvatures of\nhive surfaces from select matrix ensembles.\n  The second part of this manuscript concerns Littlewood-Richardson\ncoefficients and methods of estimating them from the hive construction. We\nillustrate experimental confirmation of two numerical algorithms that we\nprovide as tools for the community: one as a rounded estimator on the\ncontinuous hive polytope volume following a proposal by Narayanan, and the\nother as a novel construction using a coordinate hit-and-run on the hive\nlattice itself. We compare the advantages of each, and include numerical\nresults on their accuracies for some tested cases.\n", "versions": [{"version": "v1", "created": "Thu, 14 Dec 2017 21:26:59 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Lombard", "John", ""]]}, {"id": "1712.05767", "submitter": "Jane Liang", "authors": "Jane W. Liang and Saunak Sen", "title": "Sparse matrix linear models for structured high-throughput data", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent technological advancements have led to the rapid generation of\nhigh-throughput biological data, which can be used to address novel scientific\nquestions in broad areas of research. These data can be thought of as a large\nmatrix with covariates annotating both rows and columns of this matrix. Matrix\nlinear models provide a convenient way for modeling such data. In many\nsituations, sparse estimation of these models is desired. We present fast,\ngeneral methods for fitting sparse matrix linear models to structured\nhigh-throughput data. We induce model sparsity using an L$_1$ penalty and\nconsider the case when the response matrix and the covariate matrices are\nlarge. Due to data size, standard methods for estimation of these penalized\nregression models fail if the problem is converted to the corresponding\nunivariate regression scenario. By leveraging matrix properties in the\nstructure of our model, we develop several fast estimation algorithms\n(coordinate descent, FISTA, and ADMM) and discuss their trade-offs. We evaluate\nour method's performance on simulated data, E. coli chemical genetic screening\ndata, and two Arabidopsis genetic datasets with multivariate responses. Our\nalgorithms have been implemented in the Julia programming language and are\navailable at https://github.com/senresearch/MatrixLMnet.jl.\n", "versions": [{"version": "v1", "created": "Fri, 15 Dec 2017 17:53:29 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 01:28:07 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 00:31:15 GMT"}, {"version": "v4", "created": "Fri, 26 Feb 2021 04:05:37 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Liang", "Jane W.", ""], ["Sen", "Saunak", ""]]}, {"id": "1712.05907", "submitter": "Erin Conlon", "authors": "Zheng Wei and Erin M. Conlon", "title": "Parallel Markov Chain Monte Carlo for Bayesian Hierarchical Models with\n  Big Data, in Two Stages", "comments": "30 pages, 2 figures. New simulation example for logistic regression.\n  MCMC efficiency measure added. Details of convergence diagnostics added. One\n  additional table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the escalating growth of big data sets in recent years, new Bayesian\nMarkov chain Monte Carlo (MCMC) parallel computing methods have been developed.\nThese methods partition large data sets by observations into subsets. However,\nfor Bayesian nested hierarchical models, typically only a few parameters are\ncommon for the full data set, with most parameters being group-specific. Thus,\nparallel Bayesian MCMC methods that take into account the structure of the\nmodel and split the full data set by groups rather than by observations are a\nmore natural approach for analysis. Here, we adapt and extend a recently\nintroduced two-stage Bayesian hierarchical modeling approach, and we partition\ncomplete data sets by groups. In stage 1, the group-specific parameters are\nestimated independently in parallel. The stage 1 posteriors are used as\nproposal distributions in stage 2, where the target distribution is the full\nmodel. Using three-level and four-level models, we show in both simulation and\nreal data studies that results of our method agree closely with the full data\nanalysis, with greatly increased MCMC efficiency and greatly reduced\ncomputation times. The advantages of our method versus existing parallel MCMC\ncomputing methods are also described.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 06:14:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 22:07:54 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Wei", "Zheng", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1712.06006", "submitter": "Ryan Turner", "authors": "Ryan Turner, Brady Neal", "title": "How well does your sampler really work?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new data-driven benchmark system to evaluate the performance of\nnew MCMC samplers. Taking inspiration from the COCO benchmark in optimization,\nwe view this task as having critical importance to machine learning and\nstatistics given the rate at which new samplers are proposed. The common\nhand-crafted examples to test new samplers are unsatisfactory; we take a\nmeta-learning-like approach to generate benchmark examples from a large corpus\nof data sets and models. Surrogates of posteriors found in real problems are\ncreated using highly flexible density models including modern neural network\nbased approaches. We provide new insights into the real effective sample size\nof various samplers per unit time and the estimation efficiency of the samplers\nper sample. Additionally, we provide a meta-analysis to assess the predictive\nutility of various MCMC diagnostics and perform a nonparametric regression to\ncombine them.\n", "versions": [{"version": "v1", "created": "Sat, 16 Dec 2017 18:56:20 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Turner", "Ryan", ""], ["Neal", "Brady", ""]]}, {"id": "1712.06201", "submitter": "Krzysztof Latuszynski", "authors": "Paul Fearnhead, Krzystof Latuszynski, Gareth O. Roberts, Giorgos\n  Sermaidis", "title": "Continious-time Importance Sampling: Monte Carlo Methods which Avoid\n  Time-discretisation Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a continuous-time sequential importance sampling\n(CIS) algorithm which eliminates time-discretisation errors and provides online\nunbiased estimation for continuous time Markov processes, in particular for\ndiffusions. Our work removes the strong conditions imposed by the EA and thus\nextends significantly the class of discretisation error-free MC methods for\ndiffusions. The reason that CIS can be applied more generally than EA is that\nit no longer works on the path space of the SDE. Instead it uses proposal\ndistributions for the transition density of the diffusion, and proposal\ndistributions that are absolutely continuous with respect to the true\ntransition density exist for general SDEs.\n", "versions": [{"version": "v1", "created": "Sun, 17 Dec 2017 22:55:50 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Fearnhead", "Paul", ""], ["Latuszynski", "Krzystof", ""], ["Roberts", "Gareth O.", ""], ["Sermaidis", "Giorgos", ""]]}, {"id": "1712.06947", "submitter": "Jianfeng Lu", "authors": "Jianfeng Lu and Eric Vanden-Eijnden", "title": "Methodological and computational aspects of parallel tempering methods\n  in the infinite swapping limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.chem-ph math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variant of the parallel tempering method is proposed in terms of a\nstochastic switching process for the coupled dynamics of replica configuration\nand temperature permutation. This formulation is shown to facilitate the\nanalysis of the convergence properties of parallel tempering by large deviation\ntheory, which indicates that the method should be operated in the infinite\nswapping limit to maximize sampling efficiency. The effective equation for the\nreplica alone that arises in this infinite swapping limit simply involves\nreplacing the original potential by a mixture potential. The analysis of the\ngeometric properties of this potential offers a new perspective on the issues\nof how to choose of temperature ladder, and why many temperatures should\ntypically be introduced to boost the sampling efficiency. It is also shown how\nto simulate the effective equation in this many temperature regime using\nmultiscale integrators. Finally, similar ideas are also used to discuss\nextensions of the infinite swapping limits to the technique of simulated\ntempering.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 14:30:16 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Lu", "Jianfeng", ""], ["Vanden-Eijnden", "Eric", ""]]}, {"id": "1712.07223", "submitter": "Dimitrios Loukrezis", "authors": "Dimitrios Loukrezis, Ulrich R\\\"omer, and Herbert De Gersem", "title": "Assessing the Performance of Leja and Clenshaw-Curtis Collocation for\n  Computational Electromagnetics with Random Input Data", "comments": "27 pages, 11 figures, 2 tables", "journal-ref": null, "doi": "10.1615/Int.J.UncertaintyQuantification.2018025234", "report-no": null, "categories": "cs.CE cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of quantifying uncertainty regarding the output of an\nelectromagnetic field problem in the presence of a large number of uncertain\ninput parameters. In order to reduce the growth in complexity with the number\nof dimensions, we employ a dimension-adaptive stochastic collocation method\nbased on nested univariate nodes. We examine the accuracy and performance of\ncollocation schemes based on Clenshaw-Curtis and Leja rules, for the cases of\nuniform and bounded, non-uniform random inputs, respectively. Based on\nnumerical experiments with an academic electromagnetic field model, we compare\nthe two rules in both the univariate and multivariate case and for both\nquadrature and interpolation purposes. Results for a real-world electromagnetic\nfield application featuring high-dimensional input uncertainty are also\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 19 Dec 2017 21:28:26 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:07:20 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Loukrezis", "Dimitrios", ""], ["R\u00f6mer", "Ulrich", ""], ["De Gersem", "Herbert", ""]]}, {"id": "1712.07270", "submitter": "Dustin Pluta", "authors": "Dustin Pluta, Hernando Ombao, Chuansheng Chen, Gui Xue, Robert Moyzis,\n  Zhaoxia Yu", "title": "Adaptive Mantel Test for AssociationTesting in Imaging Genetics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mantel's test (MT) for association is conducted by testing the linear\nrelationship of similarity of all pairs of subjects between two observational\ndomains. Motivated by applications to neuroimaging and genetics data, and\nfollowing the succes of shrinkage and kernel methods for prediction with\nhigh-dimensional data, we here introduce the adaptive Mantel test as an\nextension of the MT. By utilizing kernels and penalized similarity measures,\nthe adaptive Mantel test is able to achieve higher statistical power relative\nto the classical MT in many settings. Furthermore, the adaptive Mantel test is\ndesigned to simultaneously test over multiple similarity measures such that the\ncorrect type I error rate under the null hypothesis is maintained without the\nneed to directly adjust the significance threshold for multiple testing. The\nperformance of the adaptive Mantel test is evaluated on simulated data, and is\nused to investigate associations between genetics markers related to\nAlzheimer's Disease and heatlhy brain physiology with data from a working\nmemory study of 350 college students from Beijing Normal University.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 00:06:22 GMT"}, {"version": "v2", "created": "Sun, 16 Dec 2018 03:45:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Pluta", "Dustin", ""], ["Ombao", "Hernando", ""], ["Chen", "Chuansheng", ""], ["Xue", "Gui", ""], ["Moyzis", "Robert", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "1712.07325", "submitter": "Lingzhou Xue", "authors": "Kevin H. Lee, Lingzhou Xue and David R. Hunter", "title": "Model-Based Clustering of Time-Evolving Networks through Temporal\n  Exponential-Family Random Graph Models", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks are a general language for describing time-evolving complex\nsystems, and discrete time network models provide an emerging statistical\ntechnique for various applications. It is a fundamental research question to\ndetect the community structure in time-evolving networks. However, due to\nsignificant computational challenges and difficulties in modeling communities\nof time-evolving networks, there is little progress in the current literature\nto effectively find communities in time-evolving networks. In this work, we\npropose a novel model-based clustering framework for time-evolving networks\nbased on discrete time exponential-family random graph models. To choose the\nnumber of communities, we use conditional likelihood to construct an effective\nmodel selection criterion. Furthermore, we propose an efficient variational\nexpectation-maximization (EM) algorithm to find approximate maximum likelihood\nestimates of network parameters and mixing proportions. By using variational\nmethods and minorization-maximization (MM) techniques, our method has appealing\nscalability for large-scale time-evolving networks. The power of our method is\ndemonstrated in simulation studies and empirical applications to international\ntrade networks and the collaboration networks of a large American research\nuniversity.\n", "versions": [{"version": "v1", "created": "Wed, 20 Dec 2017 05:36:00 GMT"}], "update_date": "2017-12-21", "authors_parsed": [["Lee", "Kevin H.", ""], ["Xue", "Lingzhou", ""], ["Hunter", "David R.", ""]]}, {"id": "1712.07764", "submitter": "Madeleine Thompson", "authors": "Madeleine B. Thompson", "title": "Wave function representation of probability distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Orthogonal decomposition of the square root of a probability density function\nin the Hermite basis is a useful low-dimensional parameterization of continuous\nprobability distributions over the reals. This representation is formally\nsimilar to the representation of quantum mechanical states as wave functions,\nwhose squared modulus is a probability density.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 01:40:29 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 17:49:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Thompson", "Madeleine B.", ""]]}, {"id": "1712.07800", "submitter": "Lingzhou Xue", "authors": "Amal Agarwal and Lingzhou Xue", "title": "Model-Based Clustering of Nonparametric Weighted Networks with\n  Application to Water Pollution Analysis", "comments": "29 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water pollution is a major global environmental problem, and it poses a great\nenvironmental risk to public health and biological diversity. This work is\nmotivated by assessing the potential environmental threat of coal mining\nthrough increased sulfate concentrations in river networks, which do not belong\nto any simple parametric distribution. However, existing network models mainly\nfocus on binary or discrete networks and weighted networks with known\nparametric weight distributions. We propose a principled nonparametric weighted\nnetwork model based on exponential-family random graph models and local\nlikelihood estimation and study its model-based clustering with application to\nlarge-scale water pollution network analysis. We do not require any parametric\ndistribution assumption on network weights. The proposed method greatly extends\nthe methodology and applicability of statistical network models. Furthermore,\nit is scalable to large and complex networks in large-scale environmental\nstudies. The power of our proposed methods is demonstrated in simulation\nstudies and a real application to sulfate pollution network analysis in Ohio\nwatershed located in Pennsylvania, United States.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 05:36:10 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 03:31:53 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Agarwal", "Amal", ""], ["Xue", "Lingzhou", ""]]}, {"id": "1712.08211", "submitter": "Eric Tramel", "authors": "Baptiste Goujaud, Eric W. Tramel, Pierre Courtiol, Mikhail Zaslavskiy,\n  Gilles Wainrib", "title": "Robust Detection of Covariate-Treatment Interactions in Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of interactions between treatment effects and patient descriptors\nin clinical trials is critical for optimizing the drug development process. The\nincreasing volume of data accumulated in clinical trials provides a unique\nopportunity to discover new biomarkers and further the goal of personalized\nmedicine, but it also requires innovative robust biomarker detection methods\ncapable of detecting non-linear, and sometimes weak, signals. We propose a set\nof novel univariate statistical tests, based on the theory of random walks,\nwhich are able to capture non-linear and non-monotonic covariate-treatment\ninteractions. We also propose a novel combined test, which leverages the power\nof all of our proposed univariate tests into a single general-case tool. We\npresent results for both synthetic trials as well as real-world clinical\ntrials, where we compare our method with state-of-the-art techniques and\ndemonstrate the utility and robustness of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Dec 2017 21:09:13 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Goujaud", "Baptiste", ""], ["Tramel", "Eric W.", ""], ["Courtiol", "Pierre", ""], ["Zaslavskiy", "Mikhail", ""], ["Wainrib", "Gilles", ""]]}, {"id": "1712.08364", "submitter": "Line K\\\"uhnel", "authors": "Line K\\\"uhnel, Alexis Arnaudon, Stefan Sommer", "title": "Differential geometry and stochastic dynamics with deep learning\n  numerics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate how deterministic and stochastic dynamics on\nmanifolds, as well as differential geometric constructions can be implemented\nconcisely and efficiently using modern computational frameworks that mix\nsymbolic expressions with efficient numerical computations. In particular, we\nuse the symbolic expression and automatic differentiation features of the\npython library Theano, originally developed for high-performance computations\nin deep learning. We show how various aspects of differential geometry and Lie\ngroup theory, connections, metrics, curvature, left/right invariance, geodesics\nand parallel transport can be formulated with Theano using the automatic\ncomputation of derivatives of any order. We will also show how symbolic\nstochastic integrators and concepts from non-linear statistics can be\nformulated and optimized with only a few lines of code. We will then give\nexplicit examples on low-dimensional classical manifolds for visualization and\ndemonstrate how this approach allows both a concise implementation and\nefficient scaling to high dimensional problems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 09:23:06 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["K\u00fchnel", "Line", ""], ["Arnaudon", "Alexis", ""], ["Sommer", "Stefan", ""]]}, {"id": "1712.08466", "submitter": "Johan Westerborn Alenl\\\"ov", "authors": "Jimmy Olsson and Johan Westerborn Alenl\\\"ov", "title": "Particle-based, online estimation of tangent filters with application to\n  parameter estimation in nonlinear state-space models", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel algorithm for efficient online estimation of the\nfilter derivatives in general hidden Markov models. The algorithm, which has a\nlinear computational complexity and very limited memory requirements, is\nfurnished with a number of convergence results, including a central limit\ntheorem with an asymptotic variance that can be shown to be uniformly bounded\nin time. Using the proposed filter derivative estimator we design a recursive\nmaximum likelihood algorithm updating the parameters according the gradient of\nthe one-step predictor log-likelihood. The efficiency of this online parameter\nestimation scheme is illustrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 14:40:56 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 07:26:57 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 11:59:59 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Olsson", "Jimmy", ""], ["Alenl\u00f6v", "Johan Westerborn", ""]]}, {"id": "1712.08664", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "A Mixture of Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years data has become increasingly higher dimensional, which has\nprompted an increased need for dimension reduction techniques. This is perhaps\nespecially true for clustering (unsupervised classification) as well as\nsemi-supervised and supervised classification. Although dimension reduction in\nthe area of clustering for multivariate data has been quite thoroughly\ndiscussed within the literature, there is relatively little work in the area of\nthree-way, or matrix variate, data. Herein, we develop a mixture of matrix\nvariate bilinear factor analyzers (MMVBFA) model for use in clustering\nhigh-dimensional matrix variate data. This work can be considered both the\nfirst matrix variate bilinear factor analysis model as well as the first MMVBFA\nmodel. Parameter estimation is discussed, and the MMVBFA model is illustrated\nusing simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 22 Dec 2017 21:26:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 16:45:03 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2018 23:41:55 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1712.08786", "submitter": "Ranjan Maitra", "authors": "Anna D. Peterson and Arka P. Ghosh and Ranjan Maitra", "title": "Merging $K$-means with hierarchical clustering for identifying\n  general-shaped groups", "comments": "16 pages, 1 table, 9 figures; accepted for publication in Stat", "journal-ref": null, "doi": "10.1002/sta4.172", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering partitions a dataset such that observations placed together in a\ngroup are similar but different from those in other groups. Hierarchical and\n$K$-means clustering are two approaches but have different strengths and\nweaknesses. For instance, hierarchical clustering identifies groups in a\ntree-like structure but suffers from computational complexity in large datasets\nwhile $K$-means clustering is efficient but designed to identify homogeneous\nspherically-shaped clusters. We present a hybrid non-parametric clustering\napproach that amalgamates the two methods to identify general-shaped clusters\nand that can be applied to larger datasets. Specifically, we first partition\nthe dataset into spherical groups using $K$-means. We next merge these groups\nusing hierarchical methods with a data-driven distance measure as a stopping\ncriterion. Our proposal has the potential to reveal groups with general shapes\nand structure in a dataset. We demonstrate good performance on several\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 15:07:00 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Peterson", "Anna D.", ""], ["Ghosh", "Arka P.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1712.08837", "submitter": "Ze Jin", "authors": "Ze Jin, Benjamin B. Risk, David S. Matteson", "title": "Optimization and Testing in Linear Non-Gaussian Component Analysis", "comments": "33 pages, 3 tables, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) decomposes multivariate data into\nmutually independent components (ICs). The ICA model is subject to a constraint\nthat at most one of these components is Gaussian, which is required for model\nidentifiability. Linear non-Gaussian component analysis (LNGCA) generalizes the\nICA model to a linear latent factor model with any number of both non-Gaussian\ncomponents (signals) and Gaussian components (noise), where observations are\nlinear combinations of independent components. Although the individual Gaussian\ncomponents are not identifiable, the Gaussian subspace is identifiable. We\nintroduce an estimator along with its optimization approach in which\nnon-Gaussian and Gaussian components are estimated simultaneously, maximizing\nthe discrepancy of each non-Gaussian component from Gaussianity while\nminimizing the discrepancy of each Gaussian component from Gaussianity. When\nthe number of non-Gaussian components is unknown, we develop a statistical test\nto determine it based on resampling and the discrepancy of estimated\ncomponents. Through a variety of simulation studies, we demonstrate the\nimprovements of our estimator over competing estimators, and we illustrate the\neffectiveness of the test to determine the number of non-Gaussian components.\nFurther, we apply our method to real data examples and demonstrate its\npractical value.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 20:37:26 GMT"}, {"version": "v2", "created": "Fri, 29 Dec 2017 20:23:30 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Risk", "Benjamin B.", ""], ["Matteson", "David S.", ""]]}, {"id": "1712.08929", "submitter": "V. Roshan Joseph", "authors": "V. Roshan Joseph, Dianpeng Wang, Li Gu, Shiji Lv, Rui Tuo", "title": "Deterministic Sampling of Expensive Posteriors Using Minimum Energy\n  Designs", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2018.1552203", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods require a large number of samples to\napproximate a posterior distribution, which can be costly when the likelihood\nor prior is expensive to evaluate. The number of samples can be reduced if we\ncan avoid repeated samples and those that are close to each other. This is the\nidea behind deterministic sampling methods such as Quasi-Monte Carlo (QMC).\nHowever, the existing QMC methods aim at sampling from a uniform hypercube,\nwhich can miss the high probability regions of the posterior distribution and\nthus the approximation can be poor. Minimum energy design (MED) is a recently\nproposed deterministic sampling method, which makes use of the posterior\nevaluations to obtain a weighted space-filling design in the region of\ninterest. However, the existing implementation of MED is inefficient because it\nrequires several global optimizations and thus numerous evaluations of the\nposterior. In this article, we develop an efficient algorithm that can generate\nMED with few posterior evaluations. We also make several improvements to the\nMED criterion to make it perform better in high dimensions. The advantages of\nMED over MCMC and QMC are illustrated using an example of calibrating a\nfriction drilling process.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 14:25:10 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Joseph", "V. Roshan", ""], ["Wang", "Dianpeng", ""], ["Gu", "Li", ""], ["Lv", "Shiji", ""], ["Tuo", "Rui", ""]]}, {"id": "1712.09566", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio Gomez-Rubio", "title": "Mixture model fitting using conditional models and modal Gibbs sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to fitting mixture models based on\nestimating first the posterior distribution of the auxiliary variables that\nassign each observation to a group in the mixture. The posterior distributions\nof the remainder of the parameters in the mixture is obtained by averaging over\ntheir conditional posterior marginals on the auxiliary variables using Bayesian\nmodel averaging.\n  A new algorithm based on Gibbs sampling is used to approximate the posterior\ndistribution of the auxiliary variables without sampling any other parameter in\nthe model. In particular, the modes of the full conditionals of the parameters\nof the densities in the mixture are computed and these are plugged-in to the\nfull conditional of the auxiliary variables to draw samples. This\napproximation, that we have called 'modal' Gibbs sampling, reduces the\ncomputational burden in the Gibbs sampling algorithm and still provides very\ngood estimates of the posterior distribution of the auxiliary variables.\nConditional models on the auxiliary variables are fitted using the Integrated\nNested Laplace Approximation (INLA) to obtain the conditional posterior\ndistributions, including modes, of the distributional parameters in the\nmixtures.\n  This approach is general enough to consider mixture models with discrete or\ncontinuous outcomes from a wide range of distributions and latent models as\nconditional model fitting is done with INLA. This presents several other\nadvantages, such as fast fitting of the conditional models, not being\nrestricted to the use of conjugate priors on the model parameters and being\nless prone to label switching. Within this framework, computing the marginal\nlikelihood of the mixture model when the number of groups in the mixture is\nknown is easy and it can be used to tackle selection of the number of\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 12:45:53 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Gomez-Rubio", "Virgilio", ""]]}, {"id": "1712.09694", "submitter": "Haolei Weng", "authors": "Haolei Weng and Yang Feng", "title": "On the estimation of correlation in a binary sequence model", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary sequence generated by thresholding a hidden continuous\nsequence. The hidden variables are assumed to have a compound symmetry\ncovariance structure with a single parameter characterizing the common\ncorrelation. We study the parameter estimation problem under such one-parameter\nmodels. We demonstrate that maximizing the likelihood function does not yield\nconsistent estimates for the correlation. We then formally prove the\nnonestimability of the parameter by deriving a non-vanishing minimax lower\nbound. This counter-intuitive phenomenon provides an interesting insight that\none-bit information of each latent variable is not sufficient to consistently\nrecover their common correlation. On the other hand, we further show that\ntrinary data generated from the hidden variables can consistently estimate the\ncorrelation with parametric convergence rate. Thus we reveal a phase transition\nphenomenon regarding the discretization of latent continuous variables while\npreserving the estimability of the correlation. Numerical experiments are\nperformed to validate the conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Dec 2017 22:19:19 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 19:27:54 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "1712.09718", "submitter": "Igor Gilitschenski", "authors": "Gerhard Kurz, Igor Gilitschenski, Florian Pfaff, Lukas Drude, Uwe D.\n  Hanebeck, Reinhold Haeb-Umbach, Roland Y. Siegwart", "title": "Directional Statistics and Filtering Using libDirectional", "comments": "Version accepted for Publication in the Journal of Statistical\n  Software", "journal-ref": null, "doi": "10.18637/jss.v089.i04", "report-no": null, "categories": "stat.CO cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present libDirectional, a MATLAB library for directional\nstatistics and directional estimation. It supports a variety of commonly used\ndistributions on the unit circle, such as the von Mises, wrapped normal, and\nwrapped Cauchy distributions. Furthermore, various distributions on\nhigher-dimensional manifolds such as the unit hypersphere and the hypertorus\nare available. Based on these distributions, several recursive filtering\nalgorithms in libDirectional allow estimation on these manifolds. The\nfunctionality is implemented in a clear, well-documented, and object-oriented\nstructure that is both easy to use and easy to extend.\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 00:36:38 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Kurz", "Gerhard", ""], ["Gilitschenski", "Igor", ""], ["Pfaff", "Florian", ""], ["Drude", "Lukas", ""], ["Hanebeck", "Uwe D.", ""], ["Haeb-Umbach", "Reinhold", ""], ["Siegwart", "Roland Y.", ""]]}, {"id": "1712.10131", "submitter": "Alireza Doostan", "authors": "Paul Diaz, Alireza Doostan, and Jerrad Hampton", "title": "Sparse Polynomial Chaos Expansions via Compressed Sensing and D-optimal\n  Design", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2018.03.020", "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of uncertainty quantification, sparse polynomial chaos (PC)\nexpansions are commonly used by researchers for a variety of purposes, such as\nsurrogate modeling. Ideas from compressed sensing may be employed to exploit\nthis sparsity in order to reduce computational costs. A class of greedy\ncompressed sensing algorithms use least squares minimization to approximate PC\ncoefficients. This least squares problem lends itself to the theory of optimal\ndesign of experiments (ODE). Our work focuses on selecting an experimental\ndesign that improves the accuracy of sparse PC approximations for a fixed\ncomputational budget. We propose DSP, a novel sequential design, greedy\nalgorithm for sparse PC approximation. The algorithm sequentially augments an\nexperimental design according to a set of the basis polynomials deemed\nimportant by the magnitude of their coefficients, at each iteration. Our\nalgorithm incorporates topics from ODE to estimate the PC coefficients. A\nvariety of numerical simulations are performed on three physical models and\nmanufactured sparse PC expansions to provide a comparative study between our\nproposed algorithm and other non-adaptive methods. Further, we examine the\nimportance of sampling by comparing different strategies in terms of their\nability to generate a candidate pool from which an optimal experimental design\nis chosen. It is demonstrated that the most accurate PC coefficient\napproximations, with the least variability, are produced with our\ndesign-adaptive greedy algorithm and the use of a studied importance sampling\nstrategy. We provide theoretical and numerical results which show that using an\noptimal sampling strategy for the candidate pool is key, both in terms of\naccuracy in the approximation, but also in terms of constructing an optimal\ndesign.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 07:09:13 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Diaz", "Paul", ""], ["Doostan", "Alireza", ""], ["Hampton", "Jerrad", ""]]}]