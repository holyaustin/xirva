[{"id": "1408.0241", "submitter": "Mathieu Rosenbaum", "authors": "Alexandre Belloni, Mathieu Rosenbaum and Alexandre Tsybakov", "title": "Linear and Conic Programming Estimators in High-Dimensional\n  Errors-in-variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression model with observation error in the design.\nIn this setting, we allow the number of covariates to be much larger than the\nsample size. Several new estimation methods have been recently introduced for\nthis model. Indeed, the standard Lasso estimator or Dantzig selector turn out\nto become unreliable when only noisy regressors are available, which is quite\ncommon in practice. We show in this work that under suitable sparsity\nassumptions, the procedure introduced in Rosenbaum and Tsybakov (2013) is\nalmost optimal in a minimax sense and, despite non-convexities, can be\nefficiently computed by a single linear programming problem. Furthermore, we\nprovide an estimator attaining the minimax efficiency bound. This estimator is\nwritten as a second order cone programming minimisation problem which can be\nsolved numerically in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Aug 2014 17:30:00 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 21:26:44 GMT"}, {"version": "v3", "created": "Sun, 3 Jul 2016 17:34:36 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Rosenbaum", "Mathieu", ""], ["Tsybakov", "Alexandre", ""]]}, {"id": "1408.0251", "submitter": "Bello Oyedele Adeshina", "authors": "Adeshina Oyedele Bello", "title": "Modeling Cassava Yield: A Response Surface Approach", "comments": null, "journal-ref": "International Journal on Computational Sciences & Applications\n  (IJCSA) Vol.4, No.3, June 2014", "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports on application of theory of experimental design using\ngraphical techniques in R programming language and application of nonlinear\nbootstrap regression method to demonstrate the invariant property of parameter\nestimates of the Inverse polynomial Model (IPM) in a nonlinear surface.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2014 10:29:00 GMT"}, {"version": "v2", "created": "Wed, 18 Mar 2015 13:12:42 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Bello", "Adeshina Oyedele", ""]]}, {"id": "1408.0777", "submitter": "Daniel Cervone", "authors": "Daniel Cervone, Alex D'Amour, Luke Bornn, and Kirk Goldsberry", "title": "A Multiresolution Stochastic Process Model for Predicting Basketball\n  Possession Outcomes", "comments": "31 pages, 9 figures", "journal-ref": "Journal Of The American Statistical Association Vol. 111, Iss.\n  514, 2016", "doi": "10.1080/01621459.2016.1141685", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basketball games evolve continuously in space and time as players constantly\ninteract with their teammates, the opposing team, and the ball. However,\ncurrent analyses of basketball outcomes rely on discretized summaries of the\ngame that reduce such interactions to tallies of points, assists, and similar\nevents. In this paper, we propose a framework for using optical player tracking\ndata to estimate, in real time, the expected number of points obtained by the\nend of a possession. This quantity, called \\textit{expected possession value}\n(EPV), derives from a stochastic process model for the evolution of a\nbasketball possession; we model this process at multiple levels of resolution,\ndifferentiating between continuous, infinitesimal movements of players, and\ndiscrete events such as shot attempts and turnovers. Transition kernels are\nestimated using hierarchical spatiotemporal models that share information\nacross players while remaining computationally tractable on very large data\nsets. In addition to estimating EPV, these models reveal novel insights on\nplayers' decision-making tendencies as a function of their spatial strategy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Aug 2014 19:22:47 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 20:52:53 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2016 18:38:38 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Cervone", "Daniel", ""], ["D'Amour", "Alex", ""], ["Bornn", "Luke", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1408.1182", "submitter": "Visar Berisha", "authors": "Visar Berisha and Alfred O. Hero", "title": "Empirical non-parametric estimation of the Fisher Information", "comments": "12 pages", "journal-ref": null, "doi": "10.1109/LSP.2014.2378514", "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) is a foundational concept in statistical\nsignal processing. The FIM depends on the probability distribution, assumed to\nbelong to a smooth parametric family. Traditional approaches to estimating the\nFIM require estimating the probability distribution function (PDF), or its\nparameters, along with its gradient or Hessian. However, in many practical\nsituations the PDF of the data is not known but the statistician has access to\nan observation sample for any parameter value. Here we propose a method of\nestimating the FIM directly from sampled data that does not require knowledge\nof the underlying PDF. The method is based on non-parametric estimation of an\n$f$-divergence over a local neighborhood of the parameter space and a relation\nbetween curvature of the $f$-divergence and the FIM. Thus we obtain an\nempirical estimator of the FIM that does not require density estimation and is\nasymptotically consistent. We empirically evaluate the validity of our approach\nusing two experiments.\n", "versions": [{"version": "v1", "created": "Wed, 6 Aug 2014 04:35:58 GMT"}, {"version": "v2", "created": "Sun, 16 Nov 2014 22:32:29 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Berisha", "Visar", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1408.1554", "submitter": "Colin  Gillespie", "authors": "Colin S. Gillespie", "title": "A complete data frame work for fitting power law distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few decades power law distributions have been suggested as\nforming generative mechanisms in a variety of disparate fields, such as,\nastrophysics, criminology and database curation. However, fitting these heavy\ntailed distributions requires care, especially since the power law behaviour\nmay only be present in the distributional tail. Current state of the art\nmethods for fitting these models rely on estimating the cut-off parameter\n$x_{\\min}$. This results in the majority of collected data being discarded.\nThis paper provides an alternative, principled approached for fitting heavy\ntailed distributions. By directly modelling the deviation from the power law\ndistribution, we can fit and compare a variety of competing models in a single\nunified framework.\n", "versions": [{"version": "v1", "created": "Thu, 7 Aug 2014 12:02:39 GMT"}, {"version": "v2", "created": "Sun, 24 Aug 2014 18:03:05 GMT"}], "update_date": "2014-08-26", "authors_parsed": [["Gillespie", "Colin S.", ""]]}, {"id": "1408.1742", "submitter": "Josef Dick", "authors": "Houying Zhu and Josef Dick", "title": "Discrepancy Estimates for Acceptance-Rejection Samplers Using Stratified\n  Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an acceptance-rejection sampler using stratified\ninputs as diver sequence. We estimate the discrepancy of the points generated\nby this algorithm. First we show an upper bound on the star discrepancy of\norder $N^{-1/2-1/(2s)}$. Further we prove an upper bound on the $q$-th moment\nof the $L_q$-discrepancy $(\\mathbb{E}[N^{q}L^{q}_{q,N}])^{1/q}$ for $2\\le q\\le\n\\infty$, which is of order $N^{(1-1/s)(1-1/q)}$. We also present an improved\nconvergence rate for a deterministic acceptance-rejection algorithm using\n$(t,m,s)-$nets as driver sequence.\n", "versions": [{"version": "v1", "created": "Fri, 8 Aug 2014 01:15:36 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Zhu", "Houying", ""], ["Dick", "Josef", ""]]}, {"id": "1408.2128", "submitter": "Paul McNicholas", "authors": "Antonio Punzo, Martin Blostein and Paul D. McNicholas", "title": "High-dimensional unsupervised classification via parsimonious\n  contaminated mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The contaminated Gaussian distribution represents a simple heavy-tailed\nelliptical generalization of the Gaussian distribution; unlike the\noften-considered t-distribution, it also allows for automatic detection of mild\noutlying or \"bad\" points in the same way that observations are typically\nassigned to the groups in the finite mixture model context. Starting from this\ndistribution, we propose the contaminated factor analysis model as a method for\ndimensionality reduction and detection of bad points in higher dimensions. A\nmixture of contaminated Gaussian factor analyzers (MCGFA) model follows\ntherefrom, and extends the recently proposed mixture of contaminated Gaussian\ndistributions to high-dimensional data. We introduce a family of 32\nparsimonious models formed by introducing constraints on the covariance and\ncontamination structures of the general MCGFA model. We outline a variant of\nthe expectation-maximization algorithm for parameter estimation. Various\nimplementation issues are discussed, and the novel family of models is compared\nto well-established approaches on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Aug 2014 15:39:13 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 22:13:32 GMT"}, {"version": "v3", "created": "Mon, 22 Jul 2019 21:23:37 GMT"}, {"version": "v4", "created": "Wed, 28 Aug 2019 18:33:38 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Punzo", "Antonio", ""], ["Blostein", "Martin", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1408.2698", "submitter": "Radoslav Harman", "authors": "Radoslav Harman, Eva Benkov\\'a", "title": "Approximate D-optimal Experimental Design with Simultaneous Size and\n  Cost Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an experiment with a finite set of design points representing\npermissible trial conditions. Suppose that each trial is associated with a cost\nthat depends on the selected design point. In this paper, we study the problem\nof constructing an approximate D-optimal experimental design with simultaneous\nrestrictions on the size and on the total cost. For the problem of\nsize-and-cost constrained D-optimality, we formulate an equivalence theorem and\nrules for the removal of redundant design points. We also propose a simple\nmonotonically convergent \"barycentric\" algorithm that allows us to numerically\ncompute a size-and-cost constrained approximate D-optimal design.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 12:02:31 GMT"}], "update_date": "2014-08-13", "authors_parsed": [["Harman", "Radoslav", ""], ["Benkov\u00e1", "Eva", ""]]}, {"id": "1408.2773", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber", "title": "On Integration Methods Based on Scrambled Nets of Arbitrary Size", "comments": "27 pages, 2 figures (final version, to appear in The Journal of\n  Complexity)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of evaluating $I(\\varphi):=\\int_{[0,1)^s}\\varphi(x)\ndx$ for a function $\\varphi \\in L^2[0,1)^{s}$. In situations where $I(\\varphi)$\ncan be approximated by an estimate of the form\n$N^{-1}\\sum_{n=0}^{N-1}\\varphi(x^n)$, with $\\{x^n\\}_{n=0}^{N-1}$ a point set in\n$[0,1)^s$, it is now well known that the $O_P(N^{-1/2})$ Monte Carlo\nconvergence rate can be improved by taking for $\\{x^n\\}_{n=0}^{N-1}$ the first\n$N=\\lambda b^m$ points, $\\lambda\\in\\{1,\\dots,b-1\\}$, of a scrambled\n$(t,s)$-sequence in base $b\\geq 2$. In this paper we derive a bound for the\nvariance of scrambled net quadrature rules which is of order $o(N^{-1})$\nwithout any restriction on $N$. As a corollary, this bound allows us to provide\nsimple conditions to get, for any pattern of $N$, an integration error of size\n$o_P(N^{-1/2})$ for functions that depend on the quadrature size $N$. Notably,\nwe establish that sequential quasi-Monte Carlo (M. Gerber and N. Chopin, 2015,\n\\emph{J. R. Statist. Soc. B, to appear.}) reaches the $o_P(N^{-1/2})$\nconvergence rate for any values of $N$. In a numerical study, we show that for\nscrambled net quadrature rules we can relax the constraint on $N$ without any\nloss of efficiency when the integrand $\\varphi$ is a discontinuous function\nwhile, for sequential quasi-Monte Carlo, taking $N=\\lambda b^m$ may only\nprovide moderate gains.\n", "versions": [{"version": "v1", "created": "Tue, 12 Aug 2014 16:46:49 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 17:35:33 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 17:23:41 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Gerber", "Mathieu", ""]]}, {"id": "1408.2923", "submitter": "Panos Toulis", "authors": "Panos Toulis and Edoardo M. Airoldi", "title": "Asymptotic and finite-sample properties of estimators based on\n  stochastic gradients", "comments": "Annals of Statistics, 2016, forthcoming; 71 pages, 37-page main body;\n  9 figures; 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent procedures have gained popularity for parameter\nestimation from large data sets. However, their statistical properties are not\nwell understood, in theory. And in practice, avoiding numerical instability\nrequires careful tuning of key parameters. Here, we introduce implicit\nstochastic gradient descent procedures, which involve parameter updates that\nare implicitly defined. Intuitively, implicit updates shrink standard\nstochastic gradient descent updates. The amount of shrinkage depends on the\nobserved Fisher information matrix, which does not need to be explicitly\ncomputed; thus, implicit procedures increase stability without increasing the\ncomputational burden. Our theoretical analysis provides the first full\ncharacterization of the asymptotic behavior of both standard and implicit\nstochastic gradient descent-based estimators, including finite-sample error\nbounds. Importantly, analytical expressions for the variances of these\nstochastic gradient-based estimators reveal their exact loss of efficiency. We\nalso develop new algorithms to compute implicit stochastic gradient\ndescent-based estimators for generalized linear models, Cox proportional\nhazards, M-estimators, in practice, and perform extensive experiments. Our\nresults suggest that implicit stochastic gradient descent procedures are poised\nto become a workhorse for approximate inference from large data sets\n", "versions": [{"version": "v1", "created": "Wed, 13 Aug 2014 06:47:25 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 16:43:26 GMT"}, {"version": "v3", "created": "Fri, 31 Oct 2014 00:27:00 GMT"}, {"version": "v4", "created": "Thu, 17 Sep 2015 17:17:20 GMT"}, {"version": "v5", "created": "Sun, 4 Oct 2015 21:11:17 GMT"}, {"version": "v6", "created": "Wed, 28 Sep 2016 15:29:27 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1408.3807", "submitter": "David Barber", "authors": "David Barber", "title": "On solving Ordinary Differential Equations using Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a set of Gaussian Process based approaches that can be used to\nsolve non-linear Ordinary Differential Equations. We suggest an explicit\nprobabilistic solver and two implicit methods, one analogous to Picard\niteration and the other to gradient matching. All methods have greater accuracy\nthan previously suggested Gaussian Process approaches. We also suggest a\ngeneral approach that can yield error estimates from any standard ODE solver.\n", "versions": [{"version": "v1", "created": "Sun, 17 Aug 2014 09:52:06 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Barber", "David", ""]]}, {"id": "1408.3969", "submitter": "Yiming Hu", "authors": "Yi-Ming Hu, Martin Hendry, Ik Siong Heng", "title": "Efficient Exploration of Multi-Modal Posterior Distributions", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Markov Chain Monte Carlo (MCMC) algorithm is a widely recognised as an\nefficient method for sampling a specified posterior distribution. However, when\nthe posterior is multi-modal, conventional MCMC algorithms either tend to\nbecome stuck in one local mode, become non-Markovian or require an excessively\nlong time to explore the global properties of the distribution. We propose a\nnovel variant of MCMC, mixed MCMC, which exploits a specially designed proposal\ndensity to allow the generation candidate points from any of a number of\ndifferent modes. This new method is efficient by design, and is strictly\nMarkovian. We present our method and apply it to a toy model inference problem\nto demonstrate its validity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 10:39:06 GMT"}], "update_date": "2014-08-19", "authors_parsed": [["Hu", "Yi-Ming", ""], ["Hendry", "Martin", ""], ["Heng", "Ik Siong", ""]]}, {"id": "1408.4140", "submitter": "Leo Duan", "authors": "Leo L. Duan, John P. Clancy and Rhonda D. Szczesniak", "title": "BET: Bayesian Ensemble Trees for Clustering and Prediction in\n  Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel \"tree-averaging\" model that utilizes the ensemble of\nclassification and regression trees (CART). Each constituent tree is estimated\nwith a subset of similar data. We treat this grouping of subsets as Bayesian\nensemble trees (BET) and model them as an infinite mixture Dirichlet process.\nWe show that BET adapts to data heterogeneity and accurately estimates each\ncomponent. Compared with the bootstrap-aggregating approach, BET shows improved\nprediction performance with fewer trees. We develop an efficient estimating\nprocedure with improved sampling strategies in both CART and mixture models. We\ndemonstrate these advantages of BET with simulations, classification of breast\ncancer and regression of lung function measurement of cystic fibrosis patients.\n  Keywords: Bayesian CART; Dirichlet Process; Ensemble Approach; Heterogeneity;\nMixture of Trees.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 20:12:09 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Duan", "Leo L.", ""], ["Clancy", "John P.", ""], ["Szczesniak", "Rhonda D.", ""]]}, {"id": "1408.4158", "submitter": "Christian Mueller", "authors": "Zachary D. Kurtz, Christian L. Mueller, Emily R. Miraldi, Dan R.\n  Littman, Martin J. Blaser, Richard A. Bonneau", "title": "Sparse and compositionally robust inference of microbial ecological\n  networks", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1004226", "report-no": null, "categories": "stat.AP q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  16S-ribosomal sequencing and other metagonomic techniques provide snapshots\nof microbial communities, revealing phylogeny and the abundances of microbial\npopulations across diverse ecosystems. While changes in microbial community\nstructure are demonstrably associated with certain environmental conditions,\nidentification of underlying mechanisms requires new statistical tools, as\nthese datasets present several technical challenges. First, the abundances of\nmicrobial operational taxonomic units (OTUs) from 16S datasets are\ncompositional, and thus, microbial abundances are not independent. Secondly,\nmicrobial sequencing-based studies typically measure hundreds of OTUs on only\ntens to hundreds of samples; thus, inference of OTU-OTU interaction networks is\nseverely under-powered, and additional assumptions are required for accurate\ninference. Here, we present SPIEC-EASI (SParse InversE Covariance Estimation\nfor Ecological Association Inference), a statistical method for the inference\nof microbial ecological interactions from metagenomic datasets that addresses\nboth of these issues. SPIEC-EASI combines data transformations developed for\ncompositional data analysis with a graphical model inference framework that\nassumes the underlying ecological interaction network is sparse. To reconstruct\nthe interaction network, SPIEC-EASI relies on algorithms for sparse\nneighborhood and inverse covariance selection. Because no large-scale microbial\necological networks have been experimentally validated, SPIEC-EASI comprises\ncomputational tools to generate realistic OTU count data from a set of diverse\nunderlying network topologies. SPIEC-EASI outperforms state-of-the-art methods\nin terms of edge recovery and network properties on realistic synthetic data\nunder a variety of scenarios. SPIEC-EASI also reproducibly predicts previously\nunknown microbial interactions using data from the American Gut project.\n", "versions": [{"version": "v1", "created": "Mon, 18 Aug 2014 21:01:32 GMT"}, {"version": "v2", "created": "Mon, 25 Aug 2014 18:39:14 GMT"}, {"version": "v3", "created": "Sat, 14 Feb 2015 01:15:09 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Kurtz", "Zachary D.", ""], ["Mueller", "Christian L.", ""], ["Miraldi", "Emily R.", ""], ["Littman", "Dan R.", ""], ["Blaser", "Martin J.", ""], ["Bonneau", "Richard A.", ""]]}, {"id": "1408.4236", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo and Ibrahim Hoteit", "title": "Ensemble Kalman filtering with a divided state-space strategy for\n  coupled data assimilation problems", "comments": "To appear in Monthly Weather Review; Please note that there is a\n  supplementary file associated with the paper", "journal-ref": null, "doi": "10.1175/MWR-D-13-00402.1", "report-no": null, "categories": "physics.ao-ph math.NA nlin.CD physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the data assimilation problem in coupled systems, which\nconsists of two components (sub-systems) interacting with each other through\ncertain coupling terms. A straightforward way to tackle the assimilation\nproblem in such systems is to concatenate the states of the sub-systems into\none augmented state vector, so that a standard ensemble Kalman filter (EnKF)\ncan be directly applied. In this work we present a divided state-space\nestimation strategy, in which data assimilation is carried out with respect to\neach individual sub-system, involving quantities from the sub-system itself and\ncorrelated quantities from other coupled sub-systems. On top of the divided\nstate-space estimation strategy, we also consider the possibility to run the\nsub-systems separately. Combining these two ideas, a few variants of the EnKF\nare derived. The introduction of these variants is mainly inspired by the\ncurrent status and challenges in coupled data assimilation problems, and thus\nmight be of interest from a practical point of view. Numerical experiments with\na multi-scale Lorentz 96 model are conducted to evaluate the performance of\nthese variants against that of the conventional EnKF. In addition, specific for\ncoupled data assimilation problems, two prototypes of extensions of the\npresented methods are also developed in order to achieve a trade-off between\nefficiency and accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 07:51:06 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Luo", "Xiaodong", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1408.4344", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock", "title": "Optimal scaling for the pseudo-marginal random walk Metropolis:\n  insensitivity to the noise generating mechanism", "comments": "New version: simulation study now on a real statistical example\n  (confusing typos corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the optimal scaling and the efficiency of the pseudo-marginal\nrandom walk Metropolis algorithm using a recently-derived result on the\nlimiting efficiency as the dimension, $d\\rightarrow \\infty$. We prove that the\noptimal scaling for a given target varies by less than $20\\%$ across a wide\nrange of distributions for the noise in the estimate of the target, and that\nany scaling that is within $20\\%$ of the optimal one will be at least $70\\%$\nefficient. We demonstrate that this phenomenon occurs even outside the range of\ndistributions for which we rigorously prove it. We then conduct a simulation\nstudy on an example with $d=10$ where importance sampling is used to estimate\nthe target density; we also examine results available from an existing\nsimulations study with $d=5$ and where a particle filter was used. Our key\nconclusions are found to hold in these examples also.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 13:57:18 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 09:57:28 GMT"}, {"version": "v3", "created": "Thu, 23 Apr 2015 08:22:02 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Sherlock", "Chris", ""]]}, {"id": "1408.4438", "submitter": "Do L Paul Minh", "authors": "David D. L. Minh and Do Le (Paul) Minh", "title": "Understanding the Hastings Algorithm", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation 44(2):\n  332-349 (2014)", "doi": "10.1080/03610918.2013.777455", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hastings algorithm is a key tool in computational science. While\nmathematically justified by detailed balance, it can be conceptually difficult\nto grasp. Here, we present two complementary and intuitive ways to derive and\nunderstand the algorithm. In our framework, it is straightforward to see that\nthe celebrated Metropolis-Hastings algorithm has the highest acceptance\nprobability of all Hastings algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 19 Aug 2014 19:37:29 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Minh", "David D. L.", "", "Paul"], ["Le", "Do", "", "Paul"], ["Minh", "", ""]]}, {"id": "1408.4542", "submitter": "Daniel Kosiorowski", "authors": "Daniel Kosiorowski and Zygmunt Zawadzki", "title": "DepthProc An R Package for Robust Exploration of Multidimensional\n  Economic Phenomena", "comments": "the package can be obtained from ## FOR DEVELOPER VERSION:\n  require(devtools) # for windows we need RTools install_github(\"DepthProc\",\n  \"zzawadz\", subdir = \"pkg\") ##and from CRAN ## We recommend the developer\n  version in a context of your comments, suggestions, recommendations, our\n  future collaboration; The paper to appear in Journal of Statistical Software\n  in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data depth concept offers a variety of powerful and user friendly tools for\nrobust exploration and inference for multivariate socio-economic phenomena. The\noffered techniques may be successfully used in cases of lack of our knowledge\non parametric models generating data due to their nonparametric nature. This\npaper presents the R package DepthProc, which is available under GPL-2 licence\non CRAN and R-forge servers for Windows, Linux and OS X platform. The package\nconsist of among others successful implementations of several data depth\ntechniques involving multivariate quantile-quantile plots, multivariate scatter\nestimators, local Wilcoxon tests for multivariate as well as for functional\ndata, robust regressions. In order to show the package capabilities, real\ndatasets concerning United Nations Fourth Millennium Goal and the Internet\nusers activity are used.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 06:41:39 GMT"}, {"version": "v10", "created": "Sat, 31 Mar 2018 16:58:07 GMT"}, {"version": "v11", "created": "Tue, 19 Feb 2019 09:12:49 GMT"}, {"version": "v2", "created": "Thu, 21 Aug 2014 07:58:39 GMT"}, {"version": "v3", "created": "Sun, 12 Oct 2014 14:22:40 GMT"}, {"version": "v4", "created": "Sun, 5 Feb 2017 14:16:54 GMT"}, {"version": "v5", "created": "Wed, 8 Feb 2017 18:50:47 GMT"}, {"version": "v6", "created": "Sun, 12 Feb 2017 11:14:04 GMT"}, {"version": "v7", "created": "Fri, 24 Feb 2017 09:09:23 GMT"}, {"version": "v8", "created": "Fri, 21 Apr 2017 07:39:10 GMT"}, {"version": "v9", "created": "Mon, 4 Sep 2017 15:55:00 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Zawadzki", "Zygmunt", ""]]}, {"id": "1408.4559", "submitter": "Arnab Bhattacharya", "authors": "Arnab Bhattacharya and Simon Wilson", "title": "Sequential Bayesian inference for static parameters in dynamic state\n  space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for sequential Bayesian inference of the static parameters of a\ndynamic state space model is proposed. The method is based on the observation\nthat many dynamic state space models have a relatively small number of static\nparameters (or hyper-parameters), so that in principle the posterior can be\ncomputed and stored on a discrete grid of practical size which can be tracked\ndynamically. Further to this, this approach is able to use any existing\nmethodology which computes the filtering and prediction distributions of the\nstate process. Kalman filter and its extensions to non-linear/non-Gaussian\nsituations have been used in this paper. This is illustrated using several\napplications: linear Gaussian model, Binomial model, stochastic volatility\nmodel and the extremely non-linear univariate non-stationary growth model.\nPerformance has been compared to both existing on-line method and off-line\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 08:23:13 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 07:47:20 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Bhattacharya", "Arnab", ""], ["Wilson", "Simon", ""]]}, {"id": "1408.4622", "submitter": "Emmanuel Vazquez", "authors": "Emmanuel Vazquez and Julien Bect", "title": "A new integral loss function for Bayesian optimization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a real-valued continuous function $f$\nusing a Bayesian approach. Since the early work of Jonas Mockus and Antanas\n\\v{Z}ilinskas in the 70's, the problem of optimization is usually formulated by\nconsidering the loss function $\\max f - M_n$ (where $M_n$ denotes the best\nfunction value observed after $n$ evaluations of $f$). This loss function puts\nemphasis on the value of the maximum, at the expense of the location of the\nmaximizer. In the special case of a one-step Bayes-optimal strategy, it leads\nto the classical Expected Improvement (EI) sampling criterion. This is a\nspecial case of a Stepwise Uncertainty Reduction (SUR) strategy, where the risk\nassociated to a certain uncertainty measure (here, the expected loss) on the\nquantity of interest is minimized at each step of the algorithm. In this\narticle, assuming that $f$ is defined over a measure space $(\\mathbb{X},\n\\lambda)$, we propose to consider instead the integral loss function\n$\\int_{\\mathbb{X}} (f - M_n)_{+}\\, d\\lambda$, and we show that this leads, in\nthe case of a Gaussian process prior, to a new numerically tractable sampling\ncriterion that we call $\\rm EI^2$ (for Expected Integrated Expected\nImprovement). A numerical experiment illustrates that a SUR strategy based on\nthis new sampling criterion reduces the error on both the value and the\nlocation of the maximizer faster than the EI-based strategy.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 12:16:54 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Vazquez", "Emmanuel", ""], ["Bect", "Julien", ""]]}, {"id": "1408.4663", "submitter": "Chris Oates", "authors": "Nial Friel, Antonietta Mira, Chris. J. Oates", "title": "Exploiting Multi-Core Architectures for Reduced-Variance Estimation with\n  Intractable Likelihoods", "comments": "To appear in Bayesian Analysis, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular statistical models for complex phenomena are intractable, in the\nsense that the likelihood function cannot easily be evaluated. Bayesian\nestimation in this setting remains challenging, with a lack of computational\nmethodology to fully exploit modern processing capabilities. In this paper we\nintroduce novel control variates for intractable likelihoods that can\ndramatically reduce the Monte Carlo variance of Bayesian estimators. We prove\nthat our control variates are well-defined and provide a positive variance\nreduction. Furthermore we show how to optimise these control variates for\nvariance reduction. The methodology is highly parallel and offers a route to\nexploit multi-core processing architectures that complements recent research in\nthis direction. Indeed, our work shows that it may not be necessary to\nparallelise the sampling process itself in order to harness the potential of\nmassively multi-core architectures. Simulation results presented on the Ising\nmodel, exponential random graph models and non-linear stochastic differential\nequation models support our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 14:12:05 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2015 09:27:36 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Friel", "Nial", ""], ["Mira", "Antonietta", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1408.4834", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio and Antonello Maruotti and Giovanna Jona Lasinio", "title": "Bayesian Hidden Markov Modelling Using Circular-Linear General Projected\n  Normal Distribution", "comments": "Environmetrics, (2015)", "journal-ref": null, "doi": "10.1002/env.2326", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multivariate hidden Markov model to jointly cluster\ntime-series observations with different support, i.e. circular and linear.\nRelying on the general projected normal distribution, our approach allows for\nbimodal and/or skewed cluster-specific distributions for the circular variable.\nFurthermore, we relax the independence assumption between the circular and\nlinear components observed at the same time. Such an assumption is generally\nused to alleviate the computational burden involved in the parameter estimation\nstep, but it is hard to justify in empirical applications. We carry out a\nsimulation study using different data-generation schemes to investigate model\nbehavior, focusing on well recovering the hidden structure. Finally, the model\nis used to fit a real data example on a bivariate time series of wind speed and\ndirection.\n", "versions": [{"version": "v1", "created": "Wed, 20 Aug 2014 23:17:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 11:10:01 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Maruotti", "Antonello", ""], ["Lasinio", "Giovanna Jona", ""]]}, {"id": "1408.5801", "submitter": "Ryan Tibshirani", "authors": "Ryan J. Tibshirani", "title": "A General Framework for Fast Stagewise Algorithms", "comments": "56 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forward stagewise regression follows a very simple strategy for constructing\na sequence of sparse regression estimates: it starts with all coefficients\nequal to zero, and iteratively updates the coefficient (by a small amount\n$\\epsilon$) of the variable that achieves the maximal absolute inner product\nwith the current residual. This procedure has an interesting connection to the\nlasso: under some conditions, it is known that the sequence of forward\nstagewise estimates exactly coincides with the lasso path, as the step size\n$\\epsilon$ goes to zero. Furthermore, essentially the same equivalence holds\noutside of least squares regression, with the minimization of a differentiable\nconvex loss function subject to an $\\ell_1$ norm constraint (the stagewise\nalgorithm now updates the coefficient corresponding to the maximal absolute\ncomponent of the gradient).\n  Even when they do not match their $\\ell_1$-constrained analogues, stagewise\nestimates provide a useful approximation, and are computationally appealing.\nTheir success in sparse modeling motivates the question: can a simple,\neffective strategy like forward stagewise be applied more broadly in other\nregularization settings, beyond the $\\ell_1$ norm and sparsity? The current\npaper is an attempt to do just this. We present a general framework for\nstagewise estimation, which yields fast algorithms for problems such as\ngroup-structured learning, matrix completion, image denoising, and more.\n", "versions": [{"version": "v1", "created": "Mon, 25 Aug 2014 15:21:24 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 15:49:17 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "1408.6043", "submitter": "Giovanni Fasano", "authors": "Giovanni Fasano", "title": "A Framework of Conjugate Direction Methods for Symmetric Linear Systems\n  in Optimization", "comments": "31 pages", "journal-ref": null, "doi": "10.1007/s10957-014-0600-0", "report-no": null, "categories": "math.NA math.OC q-fin.CP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a parameter dependent class of Krylov-based\nmethods, namely CD, for the solution of symmetric linear systems. We give\nevidence that in our proposal we generate sequences of conjugate directions,\nextending some properties of the standard Conjugate Gradient (CG) method, in\norder to preserve the conjugacy. For specific values of the parameters in our\nframework we obtain schemes equivalent to both the CG and the scaled-CG. We\nalso prove the finite convergence of the algorithms in CD, and we provide some\nerror analysis. Finally, preconditioning is introduced for CD, and we show that\nstandard error bounds for the preconditioned CG also hold for the\npreconditioned CD.\n", "versions": [{"version": "v1", "created": "Tue, 26 Aug 2014 08:02:45 GMT"}], "update_date": "2014-08-27", "authors_parsed": [["Fasano", "Giovanni", ""]]}, {"id": "1408.6288", "submitter": "Damon McDougall", "authors": "Damon McDougall, Chris K.R.T. Jones", "title": "Decreasing flow uncertainty in Bayesian inverse problems through\n  Lagrangian drifter control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonplace in oceanography is the collection of ocean drifter positions.\nOcean drifters are devices that sit on the surface of the ocean and move with\nthe flow, transmitting their position via GPS to stations on land. Using\ndrifter data, it is possible to obtain a posterior on the underlying flow. This\nproblem, however, is highly underdetermined. Through controlling an ocean\ndrifter, we attempt to improve our knowledge of the underlying flow. We do this\nby instructing the drifter to explore parts of the flow currently uncharted,\nthereby obtaining fresh observations. The efficacy of a control is determined\nby its effect on the variance of the posterior distribution. A smaller variance\nis interpreted as a better understanding of the flow. We show a systematic\nreduction in variance can be achieved by utilising controls that allow the\ndrifter to navigate new or interesting flow structures, a good example of which\nare eddies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 00:56:54 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["McDougall", "Damon", ""], ["Jones", "Chris K. R. T.", ""]]}, {"id": "1408.6317", "submitter": "Adam Persing", "authors": "Adam Persing, Ajay Jasra, Alexandros Beskos, David Balding, Maria De\n  Iorio", "title": "A simulation approach for change-points on phylogenetic trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe $n$ sequences at each of $m$ sites, and assume that they have\nevolved from an ancestral sequence that forms the root of a binary tree of\nknown topology and branch lengths, but the sequence states at internal nodes\nare unknown. The topology of the tree and branch lengths are the same for all\nsites, but the parameters of the evolutionary model can vary over sites. We\nassume a piecewise constant model for these parameters, with an unknown number\nof change-points and hence a trans-dimensional parameter space over which we\nseek to perform Bayesian inference. We propose two novel ideas to deal with the\ncomputational challenges of such inference. Firstly, we approximate the model\nbased on the time machine principle: the top nodes of the binary tree (near the\nroot) are replaced by an approximation of the true distribution; as more nodes\nare removed from the top of the tree, the cost of computing the likelihood is\nreduced linearly in $n$. The approach introduces a bias, which we investigate\nempirically. Secondly, we develop a particle marginal Metropolis-Hastings\n(PMMH) algorithm, that employs a sequential Monte Carlo (SMC) sampler and can\nuse the first idea. Our time-machine PMMH algorithm copes well with one of the\nbottle-necks of standard computational algorithms: the trans-dimensional nature\nof the posterior distribution. The algorithm is implemented on simulated and\nreal data examples, and we empirically demonstrate its potential to outperform\ncompeting methods based on approximate Bayesian computation (ABC) techniques.\n", "versions": [{"version": "v1", "created": "Wed, 27 Aug 2014 05:59:00 GMT"}], "update_date": "2014-08-28", "authors_parsed": [["Persing", "Adam", ""], ["Jasra", "Ajay", ""], ["Beskos", "Alexandros", ""], ["Balding", "David", ""], ["De Iorio", "Maria", ""]]}, {"id": "1408.6755", "submitter": "Tobias Kley", "authors": "Tobias Kley", "title": "Quantile-Based Spectral Analysis in an Object-Oriented Framework and a\n  Reference Implementation in R: The quantspec Package", "comments": "27 pages, 11 figures, R package available via CRAN\n  (http://cran.r-project.org/web/packages/quantspec) or GitHub\n  (https://github.com/tobiaskley/quantspec)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile-based approaches to the spectral analysis of time series have\nrecently attracted a lot of attention. Despite a growing literature that\ncontains various estimation proposals, no systematic methods for computing the\nnew estimators are available to date. This paper contains two main\ncontributions. First, an extensible framework for quantile-based spectral\nanalysis of time series is developed and documented using object-oriented\nmodels. A comprehensive, open source, reference implementation of this\nframework, the R package quantspec, was recently contributed to CRAN by the\nauthor of this paper. The second contribution of the present paper is to\nprovide a detailed tutorial, with worked examples, to this R package. A reader\nwho is already familiar with quantile-based spectral analysis and whose primary\ninterest is not the design of the quantspec package, but how to use it, can\nread the tutorial and worked examples (Sections 3 and 4) independently.\n", "versions": [{"version": "v1", "created": "Thu, 28 Aug 2014 15:34:56 GMT"}], "update_date": "2014-08-29", "authors_parsed": [["Kley", "Tobias", ""]]}, {"id": "1408.6980", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Loukia Meligkotsidou", "title": "Augmentation Schemes for Particle MCMC", "comments": null, "journal-ref": "Statistics and Computing (November 2016), 26, 1293-1306", "doi": "10.1007/s11222-015-9603-4", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle MCMC involves using a particle filter within an MCMC algorithm. For\ninference of a model which involves an unobserved stochastic process, the\nstandard implementation uses the particle filter to propose new values for the\nstochastic process, and MCMC moves to propose new values for the parameters. We\nshow how particle MCMC can be generalised beyond this. Our key idea is to\nintroduce new latent variables. We then use the MCMC moves to update the latent\nvariables, and the particle filter to propose new values for the parameters and\nstochastic process given the latent variables. A generic way of defining these\nlatent variables is to model them as pseudo-observations of the parameters or\nof the stochastic process. By choosing the amount of information these latent\nvariables have about the parameters and the stochastic process we can often\nimprove the mixing of the particle MCMC algorithm by trading off the Monte\nCarlo error of the particle filter and the mixing of the MCMC moves. We show\nthat using pseudo-observations within particle MCMC can improve its efficiency\nin certain scenarios: dealing with initialisation problems of the particle\nfilter; speeding up the mixing of particle Gibbs when there is strong\ndependence between the parameters and the stochastic process; and enabling\nfurther MCMC steps to be used within the particle filter.\n", "versions": [{"version": "v1", "created": "Fri, 29 Aug 2014 11:02:41 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Fearnhead", "Paul", ""], ["Meligkotsidou", "Loukia", ""]]}]