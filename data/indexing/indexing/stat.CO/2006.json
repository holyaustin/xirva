[{"id": "2006.00032", "submitter": "Andrew Davis", "authors": "Andrew D. Davis and Youssef Marzouk and Aaron Smith and Natesh Pillai", "title": "Rate-optimal refinement strategies for local approximation MCMC", "comments": "32 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Bayesian inference problems involve target distributions whose density\nfunctions are computationally expensive to evaluate. Replacing the target\ndensity with a local approximation based on a small number of carefully chosen\ndensity evaluations can significantly reduce the computational expense of\nMarkov chain Monte Carlo (MCMC) sampling. Moreover, continual refinement of the\nlocal approximation can guarantee asymptotically exact sampling. We devise a\nnew strategy for balancing the decay rate of the bias due to the approximation\nwith that of the MCMC variance. We prove that the error of the resulting local\napproximation MCMC (LA-MCMC) algorithm decays at roughly the expected\n$1/\\sqrt{T}$ rate, and we demonstrate this rate numerically. We also introduce\nan algorithmic parameter that guarantees convergence given very weak tail\nbounds, significantly strengthening previous convergence results. Finally, we\napply LA-MCMC to a computationally intensive Bayesian inverse problem arising\nin groundwater hydrology.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 18:45:33 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 20:33:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Davis", "Andrew D.", ""], ["Marzouk", "Youssef", ""], ["Smith", "Aaron", ""], ["Pillai", "Natesh", ""]]}, {"id": "2006.00135", "submitter": "Waleed Almutiry", "authors": "Waleed Almutiry, Vineetha Warriyar K V and Rob Deardon", "title": "Continuous Time Individual-Level Models of Infectious Disease: a Package\n  EpiILMCT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the R package EpiILMCT, which allows users to study the\nspread of infectious disease using continuous time individual level models\n(ILMs). The package provides tools for simulation from continuous time ILMs\nthat are based on either spatial demographic, contact network, or a combination\nof both of them, and for the graphical summarization of epidemics. Model\nfitting is carried out within a Bayesian Markov Chain Monte Carlo (MCMC)\nframework. The continuous time ILMs can be implemented within either\nsusceptible-infected-removed (SIR) or susceptible-infected-notified-removed\n(SINR) compartmental frameworks. As infectious disease data is often partially\nobserved, data uncertainties in the form of missing infection times - and in\nsome situations missing removal times - are accounted for using data\naugmentation techniques. The package is illustrated using both simulated and an\nexperimental data set on the spread of the tomato spotted wilt virus (TSWV)\ndisease.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 00:06:17 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Almutiry", "Waleed", ""], ["K", "Vineetha Warriyar", "V"], ["Deardon", "Rob", ""]]}, {"id": "2006.00383", "submitter": "Victor Freguglia", "authors": "Victor Freguglia and Nancy Lopes Garcia", "title": "Inference tools for Markov Random Fields on lattices: The R package\n  mrf2d", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov random fields on two-dimensional lattices are behind many image\nanalysis methodologies. mrf2d provides tools for statistical inference on a\nclass of discrete stationary Markov random field models with pairwise\ninteraction, which includes many of the popular models such as the Potts model\nand texture image models. The package introduces representations of dependence\nstructures and parameters, visualization functions and efficient (C++ based)\nimplementations of sampling algorithms, common estimation methods and other key\nfeatures of the model, providing a useful framework to implement algorithms and\nworking with the model in general. This paper presents a description and\ndetails of the package, as well as some reproducible examples of usage.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 22:43:35 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 19:09:35 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 17:22:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Freguglia", "Victor", ""], ["Garcia", "Nancy Lopes", ""]]}, {"id": "2006.00535", "submitter": "Fernando Llorente Fern\\'andez", "authors": "F. Llorente, L. Martino, V. Elvira, D. Delgado, J. L\\'opez-Santiago", "title": "Adaptive quadrature schemes for Bayesian inference via active learning", "comments": "Keywords: Numerical integration; emulation; Monte Carlo methods;\n  Bayesian quadrature; experimental design; active learning", "journal-ref": "IEEE Access 8 (2020) 208462-208483", "doi": "10.1109/ACCESS.2020.3038333", "report-no": null, "categories": "stat.CO cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical integration and emulation are fundamental topics across scientific\nfields. We propose novel adaptive quadrature schemes based on an active\nlearning procedure. We consider an interpolative approach for building a\nsurrogate posterior density, combining it with Monte Carlo sampling methods and\nother quadrature rules. The nodes of the quadrature are sequentially chosen by\nmaximizing a suitable acquisition function, which takes into account the\ncurrent approximation of the posterior and the positions of the nodes. This\nmaximization does not require additional evaluations of the true posterior. We\nintroduce two specific schemes based on Gaussian and Nearest Neighbors (NN)\nbases. For the Gaussian case, we also provide a novel procedure for fitting the\nbandwidth parameter, in order to build a suitable emulator of a density\nfunction. With both techniques, we always obtain a positive estimation of the\nmarginal likelihood (a.k.a., Bayesian evidence). An equivalent importance\nsampling interpretation is also described, which allows the design of extended\nschemes. Several theoretical results are provided and discussed. Numerical\nresults show the advantage of the proposed approach, including a challenging\ninference problem in an astronomic dynamical model, with the goal of revealing\nthe number of planets orbiting a star.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 15:02:32 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2020 16:25:26 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 16:44:59 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Llorente", "F.", ""], ["Martino", "L.", ""], ["Elvira", "V.", ""], ["Delgado", "D.", ""], ["L\u00f3pez-Santiago", "J.", ""]]}, {"id": "2006.00595", "submitter": "Lu Zhang", "authors": "Lu Zhang, Sudipto Banerjee", "title": "Spatial Factor Modeling: A Bayesian Matrix-Normal Approach for\n  Misaligned Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatially-oriented data sets are prevalent in the environmental\nand physical sciences. Scientists seek to jointly model multiple variables,\neach indexed by a spatial location, to capture any underlying spatial\nassociation for each variable and associations among the different dependent\nvariables. Multivariate latent spatial process models have proved effective in\ndriving statistical inference and rendering better predictive inference at\narbitrary locations for the spatial process. High-dimensional multivariate\nspatial data, which is the theme of this article, refers to data sets where the\nnumber of spatial locations and the number of spatially dependent variables is\nvery large. The field has witnessed substantial developments in scalable models\nfor univariate spatial processes, but such methods for multivariate spatial\nprocesses, especially when the number of outcomes are moderately large, are\nlimited in comparison. Here, we extend scalable modeling strategies for a\nsingle process to multivariate processes. We pursue Bayesian inference which is\nattractive for full uncertainty quantification of the latent spatial process.\nOur approach exploits distribution theory for the Matrix-Normal distribution,\nwhich we use to construct scalable versions of a hierarchical linear model of\ncoregionalization (LMC) and spatial factor models that deliver inference over a\nhigh-dimensional parameter space including the latent spatial process. We\nillustrate the computational and inferential benefits of our algorithms over\ncompeting methods using simulation studies and an analysis of a massive\nvegetation index data set.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 19:43:42 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 02:07:12 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Zhang", "Lu", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2006.00783", "submitter": "Cheng Li", "authors": "Rajarshi Guhaniyogi, Cheng Li, Terrance D. Savitsky, Sanvesh\n  Srivastava", "title": "Distributed Bayesian Varying Coefficient Modeling Using a Gaussian\n  Process Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient models (VCMs) are widely used for estimating nonlinear\nregression functions in functional data models. Their Bayesian variants using\nGaussian process (GP) priors on the functional coefficients, however, have\nreceived limited attention in massive data applications. This is primarily due\nto the prohibitively slow posterior computations using Markov chain Monte Carlo\n(MCMC) algorithms. We address this problem using a divide-and-conquer Bayesian\napproach that operates in three steps. The first step creates a large number of\ndata subsets with much smaller sample sizes by sampling without replacement\nfrom the full data. The second step formulates VCM as a linear mixed-effects\nmodel and develops a data augmentation (DA)-type algorithm for obtaining MCMC\ndraws of the parameters and predictions on all the subsets in parallel. The\nDA-type algorithm appropriately modifies the likelihood such that every subset\nposterior distribution is an accurate approximation of the corresponding true\nposterior distribution. The third step develops a combination algorithm for\naggregating MCMC-based estimates of the subset posterior distributions into a\nsingle posterior distribution called the Aggregated Monte Carlo (AMC)\nposterior. Theoretically, we derive minimax optimal posterior convergence rates\nfor the AMC posterior distributions of both the varying coefficients and the\nmean regression function. We provide quantification on the orders of subset\nsample sizes and the number of subsets according to the smoothness properties\nof the multivariate GP. The empirical results show that the combination schemes\nthat satisfy our theoretical assumptions, including the one in the AMC\nalgorithm, have better nominal coverage, shorter credible intervals, smaller\nmean square errors, and higher effective sample size than their main\ncompetitors across diverse simulations and in a real data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 08:16:45 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Li", "Cheng", ""], ["Savitsky", "Terrance D.", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "2006.01017", "submitter": "Nabil Kahale", "authors": "Nabil Kahale", "title": "Improved SVRG for quadratic functions", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse an iterative algorithm to minimize quadratic functions whose\nHessian matrix $H$ is the expectation of a random symmetric $d\\times d$ matrix.\nThe algorithm is a variant of the stochastic variance reduced gradient (SVRG).\nIn several applications, including least-squares regressions, ridge\nregressions, linear discriminant analysis and regularized linear discriminant\nanalysis, the running time of each iteration is proportional to $d$. Under\nsmoothness and convexity conditions, the algorithm has linear convergence. When\napplied to quadratic functions, our analysis improves the state-of-the-art\nperformance of SVRG up to a logarithmic factor. Furthermore, for\nwell-conditioned quadratic problems, our analysis improves the state-of-the-art\nrunning times of accelerated SVRG, and is better than the known matching lower\nbound, by a logarithmic factor. Our theoretical results are backed with\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 15:28:08 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 07:36:41 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kahale", "Nabil", ""]]}, {"id": "2006.01101", "submitter": "Ian Grooms", "authors": "Ian Grooms", "title": "Analog ensemble data assimilation and a method for constructing analogs\n  with variational autoencoders", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": "10.1002/qj.3910", "report-no": null, "categories": "physics.comp-ph cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proposed to use analogs of the forecast mean to generate an ensemble of\nperturbations for use in ensemble optimal interpolation (EnOI) or ensemble\nvariational (EnVar) methods. A new method of constructing analogs using\nvariational autoencoders (VAEs; a machine learning method) is proposed. The\nresulting analog methods using analogs from a catalog (AnEnOI), and using\nconstructed analogs (cAnEnOI), are tested in the context of a multiscale\nLorenz-`96 model, with standard EnOI and an ensemble square root filter for\ncomparison. The use of analogs from a modestly-sized catalog is shown to\nimprove the performance of EnOI, with limited marginal improvements resulting\nfrom increases in the catalog size. The method using constructed analogs\n(cAnEnOI) is found to perform as well as a full ensemble square root filter,\nand to be robust over a wide range of tuning parameters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 17:39:16 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Grooms", "Ian", ""]]}, {"id": "2006.01584", "submitter": "Yang Liu", "authors": "Yang Liu, Robert J.B. Goudie", "title": "Stochastic Approximation Cut Algorithm for Inference in Modularized\n  Bayesian Models", "comments": "31 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian modelling enables us to accommodate complex forms of data and make a\ncomprehensive inference, but the effect of partial misspecification of the\nmodel is a concern. One approach in this setting is to modularize the model,\nand prevent feedback from suspect modules, using a cut model. After observing\ndata, this leads to the cut distribution which normally does not have a\nclosed-form. Previous studies have proposed algorithms to sample from this\ndistribution, but these algorithms have unclear theoretical convergence\nproperties. To address this, we propose a new algorithm called the Stochastic\nApproximation Cut algorithm (SACut) as an alternative. The algorithm is divided\ninto two parallel chains. The main chain targets an approximation to the cut\ndistribution; the auxiliary chain is used to form an adaptive proposal\ndistribution for the main chain. We prove convergence of the samples drawn by\nthe proposed algorithm and present the exact limit. Although SACut is biased,\nsince the main chain does not target the exact cut distribution, we prove this\nbias can be reduced geometrically by increasing a user-chosen tuning parameter.\nIn addition, parallel computing can be easily adopted for SACut, which greatly\nreduces computation time.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 13:19:15 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 03:19:21 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 12:20:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liu", "Yang", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2006.01635", "submitter": "Sven Serneels", "authors": "Emmanuel Jordy Menvouta, Sven Serneels, Tim Verdonck", "title": "direpack: A Python 3 package for state-of-the-art statistical dimension\n  reduction methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The direpack package aims to establish a set of modern statistical dimension\nreduction techniques into the Python universe as a single, consistent package.\nThe dimension reduction methods included resort into three categories:\nprojection pursuit based dimension reduction, sufficient dimension reduction,\nand robust M estimators for dimension reduction. As a corollary, regularized\nregression estimators based on these reduced dimension spaces are provided as\nwell, ranging from classical principal component regression up to sparse\npartial robust M regression. The package also contains a set of classical and\nrobust pre-processing utilities, including generalized spatial signs, as well\nas dedicated plotting functionality and cross-validation utilities. Finally,\ndirepack has been written consistent with the scikit-learn API, such that the\nestimators can flawlessly be included into (statistical and/or machine)\nlearning pipelines in that framework.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 23:42:36 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Menvouta", "Emmanuel Jordy", ""], ["Serneels", "Sven", ""], ["Verdonck", "Tim", ""]]}, {"id": "2006.01671", "submitter": "Juan C. Laria", "authors": "Juan C. Laria and Line H. Clemmensen and Bjarne K. Ersb{\\o}ll", "title": "A generalized linear joint trained framework for semi-supervised\n  learning of sparse features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elastic-net is among the most widely used types of regularization\nalgorithms, commonly associated with the problem of supervised generalized\nlinear model estimation via penalized maximum likelihood. Its nice properties\noriginate from a combination of $\\ell_1$ and $\\ell_2$ norms, which endow this\nmethod with the ability to select variables taking into account the\ncorrelations between them. In the last few years, semi-supervised approaches,\nthat use both labeled and unlabeled data, have become an important component in\nthe statistical research. Despite this interest, however, few researches have\ninvestigated semi-supervised elastic-net extensions. This paper introduces a\nnovel solution for semi-supervised learning of sparse features in the context\nof generalized linear model estimation: the generalized semi-supervised\nelastic-net (s2net), which extends the supervised elastic-net method, with a\ngeneral mathematical formulation that covers, but is not limited to, both\nregression and classification problems. We develop a flexible and fast\nimplementation for s2net in R, and its advantages are illustrated using both\nreal and synthetic data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 14:44:48 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 12:24:09 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Laria", "Juan C.", ""], ["Clemmensen", "Line H.", ""], ["Ersb\u00f8ll", "Bjarne K.", ""]]}, {"id": "2006.01786", "submitter": "Yingying Ma", "authors": "Yingying Ma and Hansheng Wang", "title": "Hyperparameter Selection for Subsampling Bootstraps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive data analysis becomes increasingly prevalent, subsampling methods\nlike BLB (Bag of Little Bootstraps) serves as powerful tools for assessing the\nquality of estimators for massive data. However, the performance of the\nsubsampling methods are highly influenced by the selection of tuning parameters\n( e.g., the subset size, number of resamples per subset ). In this article we\ndevelop a hyperparameter selection methodology, which can be used to select\ntuning parameters for subsampling methods. Specifically, by a careful\ntheoretical analysis, we find an analytically simple and elegant relationship\nbetween the asymptotic efficiency of various subsampling estimators and their\nhyperparameters. This leads to an optimal choice of the hyperparameters. More\nspecifically, for an arbitrarily specified hyperparameter set, we can improve\nit to be a new set of hyperparameters with no extra CPU time cost, but the\nresulting estimator's statistical efficiency can be much improved. Both\nsimulation studies and real data analysis demonstrate the superior advantage of\nour method.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:10:45 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Ma", "Yingying", ""], ["Wang", "Hansheng", ""]]}, {"id": "2006.01882", "submitter": "Marta Cousido-Rocha", "authors": "Marta Cousido-Rocha, Jacobo de U\\~na-\\'Alvarez, Sebastian D\\\"ohler", "title": "Improved $q$-values for discrete uniform and homogeneous tests: a\n  comparative study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale discrete uniform and homogeneous $P$-values often arise in\napplications with multiple testing. For example, this occurs in genome wide\nassociation studies whenever a nonparametric one-sample (or two-sample) test is\napplied throughout the gene loci. In this paper we consider $q$-values for such\nscenarios based on several existing estimators for the proportion of true null\nhypothesis, $\\pi_0$, which take the discreteness of the $P$-values into\naccount. The theoretical guarantees of the several approaches with respect to\nthe estimation of $\\pi_0$ and the false discovery rate control are reviewed.\nThe performance of the discrete $q$-values is investigated through intensive\nMonte Carlo simulations, including location, scale and omnibus nonparametric\ntests, and possibly dependent $P$-values. The methods are applied to genetic\nand financial data for illustration purposes too. Since the particular\nestimator of $\\pi_0$ used to compute the $q$-values may influence the power,\nrelative advantages and disadvantages of the reviewed procedures are discussed.\nPractical recommendations are given.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 19:01:27 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 11:15:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Cousido-Rocha", "Marta", ""], ["de U\u00f1a-\u00c1lvarez", "Jacobo", ""], ["D\u00f6hler", "Sebastian", ""]]}, {"id": "2006.02043", "submitter": "Mahdi Abolghasemi", "authors": "Evangelos Spiliotis, Mahdi Abolghasemi, Rob J Hyndman, Fotios\n  Petropoulos, Vassilios Assimakopoulos", "title": "Hierarchical forecast reconciliation with machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical forecasting methods have been widely used to support aligned\ndecision-making by providing coherent forecasts at different aggregation\nlevels. Traditional hierarchical forecasting approaches, such as the bottom-up\nand top-down methods, focus on a particular aggregation level to anchor the\nforecasts. During the past decades, these have been replaced by a variety of\nlinear combination approaches that exploit information from the complete\nhierarchy to produce more accurate forecasts. However, the performance of these\ncombination methods depends on the particularities of the examined series and\ntheir relationships. This paper proposes a novel hierarchical forecasting\napproach based on machine learning that deals with these limitations in three\nimportant ways. First, the proposed method allows for a non-linear combination\nof the base forecasts, thus being more general than the linear approaches.\nSecond, it structurally combines the objectives of improved post-sample\nempirical forecasting accuracy and coherence. Finally, due to its non-linear\nnature, our approach selectively combines the base forecasts in a direct and\nautomated way without requiring that the complete information must be used for\nproducing reconciled forecasts for each series and level. The proposed method\nis evaluated both in terms of accuracy and bias using two different data sets\ncoming from the tourism and retail industries. Our results suggest that the\nproposed method gives superior point forecasts than existing approaches,\nespecially when the series comprising the hierarchy are not characterized by\nthe same patterns.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 04:49:39 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Spiliotis", "Evangelos", ""], ["Abolghasemi", "Mahdi", ""], ["Hyndman", "Rob J", ""], ["Petropoulos", "Fotios", ""], ["Assimakopoulos", "Vassilios", ""]]}, {"id": "2006.02077", "submitter": "Nicklas Werge", "authors": "Nicklas Werge (LPSM), Olivier Wintenberger (LPSM)", "title": "AdaVol: An Adaptive Recursive Volatility Prediction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Maximum Likelihood (QML) procedures are theoretically appealing and\nwidely used for statistical inference. While there are extensive references on\nQML estimation in batch settings, it has attracted little attention in\nstreaming settings until recently. An investigation of the convergence\nproperties of the QML procedure in a general conditionally heteroscedastic time\nseries model is conducted, and the classical batch optimization routines\nextended to the framework of streaming and large-scale problems. An adaptive\nrecursive estimation routine for GARCH models named AdaVol is presented. The\nAdaVol procedure relies on stochastic approximations combined with the\ntechnique of Variance Targeting Estimation (VTE). This recursive method has\ncomputationally efficient properties, while VTE alleviates some convergence\ndifficulties encountered by the usual QML estimation due to a lack of\nconvexity. Empirical results demonstrate a favorable trade-off between AdaVol's\nstability and the ability to adapt to time-varying estimates for real-life\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:28:31 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:11:49 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 15:22:43 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 09:38:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Werge", "Nicklas", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2006.02238", "submitter": "Santosh Kumar", "authors": "Peter J. Forrester, Santosh Kumar", "title": "Computable structural formulas for the distribution of the\n  $\\beta$-Jacobi edge eigenvalues", "comments": "22 pages, 3 figures. Mathematica files included. To view these files,\n  please download and extract the zipped source file listed under \"Other\n  formats\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP math.PR physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Jacobi ensemble is one of the classical ensembles of random matrix\ntheory. Prominent in applications are properties of the eigenvalues at the\nspectrum edge, specifically the distribution of the largest (e.g. Roy's largest\nroot test in multivariate statistics) and smallest (e.g.~condition numbers of\nlinear systems) eigenvalues. We identify three ranges of parameter values for\nwhich the gap probability determining these distributions is a finite sum with\nrespect to particular bases, and moreover make use of a certain\ndifferential-difference system fundamental in the theory of the Selberg\nintegral to provide a recursive scheme to compute the corresponding\ncoefficients.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 12:46:02 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Forrester", "Peter J.", ""], ["Kumar", "Santosh", ""]]}, {"id": "2006.02397", "submitter": "Jordan Awan", "authors": "Jordan Awan and Zhanrui Cai", "title": "Approximate Co-Sufficient Sampling for Goodness-of-fit Tests and\n  Synthetic Data", "comments": "35 pages double spaced, before references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-sufficient sampling refers to resampling the data conditional on a\nsufficient statistic, a useful technique for statistical problems such as\ngoodness-of-fit tests, model selection, and confidence interval construction;\nit is also a powerful tool to generate synthetic data which limits the\ndisclosure risk of sensitive data. However, sampling from such conditional\ndistributions is both technically and computationally challenging, and is\ninapplicable in models without low-dimensional sufficient statistics.\n  We study an indirect inference approach to approximate co-sufficient\nsampling, which only requires an efficient statistic rather than a sufficient\nstatistic. Given an efficient estimator, we prove that the expected KL\ndivergence goes to zero between the true conditional distribution and the\nresulting approximate distribution. We also propose a one-step approximate\nsolution to the optimization problem that preserves the original estimator with\nan error of $o_p(n^{-1/2})$, which suffices for asymptotic optimality. The\none-step method is easily implemented, highly computationally efficient, and\napplicable to a wide variety of models, only requiring the ability to sample\nfrom the model and compute an efficient statistic. We implement our methods via\nsimulations to tackle problems in synthetic data, hypothesis testing, and\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:12:11 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:15:28 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 18:12:09 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Awan", "Jordan", ""], ["Cai", "Zhanrui", ""]]}, {"id": "2006.02554", "submitter": "Hengrui Luo", "authors": "Hengrui Luo, Alice Patania, Jisu Kim, Mikael Vejdemo-Johansson", "title": "Generalized Penalty for Circular Coordinate Representation", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological Data Analysis (TDA) provides novel approaches that allow us to\nanalyze the geometrical shapes and topological structures of a dataset. As one\nimportant application, TDA can be used for data visualization and dimension\nreduction. We follow the framework of circular coordinate representation, which\nallows us to perform dimension reduction and visualization for high-dimensional\ndatasets on a torus using persistent cohomology. In this paper, we propose a\nmethod to adapt the circular coordinate framework to take into account the\nroughness of circular coordinates in change-point and high-dimensional\napplications. We use a generalized penalty function instead of an $L_{2}$\npenalty in the traditional circular coordinate algorithm. We provide simulation\nexperiments and real data analysis to support our claim that circular\ncoordinates with generalized penalty will detect the change in high-dimensional\ndatasets under different sampling schemes while preserving the topological\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 22:01:10 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 05:58:09 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Luo", "Hengrui", ""], ["Patania", "Alice", ""], ["Kim", "Jisu", ""], ["Vejdemo-Johansson", "Mikael", ""]]}, {"id": "2006.02700", "submitter": "Noirrit Kiran Chandra", "authors": "Noirrit Kiran Chandra, Antonio Canale and David B. Dunson", "title": "Escaping the curse of dimensionality in Bayesian model based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, there is interest in clustering very high-dimensional\ndata. A common strategy is first stage dimensionality reduction followed by a\nstandard clustering algorithm, such as k-means. This approach does not target\ndimension reduction to the clustering objective, and fails to quantify\nuncertainty. Model-based Bayesian approaches provide an appealing alternative,\nbut often have poor performance in high-dimensions, producing too many or too\nfew clusters. This article provides an explanation for this behavior through\nstudying the clustering posterior in a non-standard setting with fixed sample\nsize and increasing dimensionality. We show that the finite sample posterior\ntends to either assign every observation to a different cluster or all\nobservations to the same cluster as dimension grows, depending on the kernels\nand prior specification but not on the true data-generating model. To find\nmodels avoiding this pitfall, we define a Bayesian oracle for clustering, with\nthe oracle clustering posterior based on the true values of low-dimensional\nlatent variables. We define a class of LAtent Mixtures for Bayesian (Lamb)\nclustering that have equivalent behavior to this oracle as dimension grows.\nLamb is shown to have good performance in simulation studies and an application\nto inferring cell types based on scRNAseq.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:40:42 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 01:52:51 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 15:55:44 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Canale", "Antonio", ""], ["Dunson", "David B.", ""]]}, {"id": "2006.02941", "submitter": "Ian Grooms", "authors": "Ian Grooms", "title": "A note on the formulation of the Ensemble Adjustment Kalman Filter", "comments": "3 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.comp-ph stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ensemble adjustment Kalman filter (EAKF; Anderson, 2001) is one of the\nearliest ensemble square root filters. This note clarifies the correct\nformulation of the EAKF, which depends on a careful treatment of an\neigen-decomposition of one of the matrices involved in the formulation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 15:25:35 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Grooms", "Ian", ""]]}, {"id": "2006.02983", "submitter": "Yu Tang", "authors": "E Chen, Ying Miao and Yu Tang", "title": "Median regression with differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Median regression analysis has robustness properties which make it attractive\ncompared with regression based on the mean, while differential privacy can\nprotect individual privacy during statistical analysis of certain datasets. In\nthis paper, three privacy preserving methods are proposed for median\nregression. The first algorithm is based on a finite smoothing method, the\nsecond provides an iterative way and the last one further employs the greedy\ncoordinate descent approach. Privacy preserving properties of these three\nmethods are all proved. Accuracy bound or convergence properties of these\nalgorithms are also provided. Numerical calculation shows that the first method\nhas better accuracy than the others when the sample size is small. When the\nsample size becomes larger, the first method needs more time while the second\nmethod needs less time with well-matched accuracy. For the third method, it\ncosts less time in both cases, while it highly depends on step size.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 16:14:39 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Chen", "E", ""], ["Miao", "Ying", ""], ["Tang", "Yu", ""]]}, {"id": "2006.02985", "submitter": "L\\'eo Grinsztajn", "authors": "L\\'eo Grinsztajn, Elizaveta Semenova, Charles C. Margossian, Julien\n  Riou", "title": "Bayesian workflow for disease transmission modeling in Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This tutorial shows how to build, fit, and criticize disease transmission\nmodels in Stan, and should be useful to researchers interested in modeling the\nSARS-CoV-2 pandemic and other infectious diseases in a Bayesian framework.\nBayesian modeling provides a principled way to quantify uncertainty and\nincorporate both data and prior knowledge into the model estimates. Stan is an\nexpressive probabilistic programming language that abstracts the inference and\nallows users to focus on the modeling. As a result, Stan code is readable and\neasily extensible, which makes the modeler's work more transparent.\nFurthermore, Stan's main inference engine, Hamiltonian Monte Carlo sampling, is\namiable to diagnostics, which means the user can verify whether the obtained\ninference is reliable. In this tutorial, we demonstrate how to formulate, fit,\nand diagnose a compartmental transmission model in Stan, first with a simple\nSusceptible-Infected-Recovered (SIR) model, then with a more elaborate\ntransmission model used during the SARS-CoV-2 pandemic. We also cover advanced\ntopics which can further help practitioners fit sophisticated models; notably,\nhow to use simulations to probe the model and priors, and computational\ntechniques to scale-up models based on ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 11:19:30 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 13:04:24 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Grinsztajn", "L\u00e9o", ""], ["Semenova", "Elizaveta", ""], ["Margossian", "Charles C.", ""], ["Riou", "Julien", ""]]}, {"id": "2006.03005", "submitter": "Gherardo Varando", "authors": "Gherardo Varando", "title": "Learning DAGs without imposing acyclicity", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore if it is possible to learn a directed acyclic graph (DAG) from\ndata without imposing explicitly the acyclicity constraint. In particular, for\nGaussian distributions, we frame structural learning as a sparse matrix\nfactorization problem and we empirically show that solving an\n$\\ell_1$-penalized optimization yields to good recovery of the true graph and,\nin general, to almost-DAG graphs. Moreover, this approach is computationally\nefficient and is not affected by the explosion of combinatorial complexity as\nin classical structural learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 16:52:01 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Varando", "Gherardo", ""]]}, {"id": "2006.03332", "submitter": "Riko Kelter", "authors": "Riko Kelter", "title": "fbst: An R package for the Full Bayesian Significance Test for testing a\n  sharp null hypothesis against its alternative via the e-value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing is a central statistical method in psychology and the\ncognitive sciences. However, the problems of null hypothesis significance\ntesting (NHST) and p-values have been debated widely, but few attractive\nalternatives exist. This article introduces the fbst R package, which\nimplements the Full Bayesian Significance Test (FBST) to test a sharp null\nhypothesis against its alternative via the e-value. The statistical theory of\nthe FBST has been introduced by Pereira et al. (1999) more than two decades ago\nand since then, the FBST has shown to be a Bayesian alternative to NHST and\np-values with both theoretical and practical highly appealing properties. The\nalgorithm provided in the fbst package is applicable to any Bayesian model as\nlong as the posterior distribution can be obtained at least numerically. The\ncore function of the package provides the Bayesian evidence against the null\nhypothesis, the e-value. Additionally, p-values based on asymptotic arguments\ncan be computed and rich visualisations for communication and interpretation of\nthe results can be produced. Three examples of frequently used statistical\nprocedures in the cognitive sciences are given in this paper which demonstrate\nhow to apply the FBST in practice using the fbst package. Based on the success\nof the FBST in statistical science, the fbst package should be of interest to a\nbroad range of researchers in psychology and the cognitive sciences and\nhopefully will encourage researchers to consider the FBST as a possible\nalternative when conducting hypothesis tests of a sharp null hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 09:32:27 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Kelter", "Riko", ""]]}, {"id": "2006.03371", "submitter": "Andrew Fowlie Assoc. Prof.", "authors": "Andrew Fowlie, Will Handley, Liangliang Su", "title": "Nested sampling cross-checks using order statistics", "comments": "minor changes & clarifications. closely matches published version", "journal-ref": null, "doi": "10.1093/mnras/staa2345", "report-no": null, "categories": "stat.CO astro-ph.CO astro-ph.IM hep-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling (NS) is an invaluable tool in data analysis in modern\nastrophysics, cosmology, gravitational wave astronomy and particle physics. We\nidentify a previously unused property of NS related to order statistics: the\ninsertion indexes of new live points into the existing live points should be\nuniformly distributed. This observation enabled us to create a novel\ncross-check of single NS runs. The tests can detect when an NS run failed to\nsample new live points from the constrained prior and plateaus in the\nlikelihood function, which break an assumption of NS and thus leads to\nunreliable results. We applied our cross-check to NS runs on toy functions with\nknown analytic results in 2 - 50 dimensions, showing that our approach can\ndetect problematic runs on a variety of likelihoods, settings and dimensions.\nAs an example of a realistic application, we cross-checked NS runs performed in\nthe context of cosmological model selection. Since the cross-check is simple,\nwe recommend that it become a mandatory test for every applicable NS run.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:19:03 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 03:52:06 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Fowlie", "Andrew", ""], ["Handley", "Will", ""], ["Su", "Liangliang", ""]]}, {"id": "2006.03452", "submitter": "Guillaume Kon Kam King", "authors": "Guillaume Kon Kam King, Omiros Papaspiliopoulos, Matteo Ruggiero", "title": "Exact inference for a class of non-linear hidden Markov models on\n  general state spaces", "comments": "39 pages, 10 figures in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact inference for hidden Markov models requires the evaluation of all\ndistributions of interest - filtering, prediction, smoothing and likelihood -\nwith a finite computational effort. This article provides sufficient conditions\nfor exact inference for a class of hidden Markov models on general state spaces\ngiven a set of discretely collected indirect observations linked non linearly\nto the signal, and a set of practical algorithms for inference. The conditions\nwe obtain are concerned with the existence of a certain type of dual process,\nwhich is an auxiliary process embedded in the time reversal of the signal, that\nin turn allows to represent the distributions and functions of interest as\nfinite mixtures of elementary densities or products thereof. We describe\nexplicitly how to update recursively the parameters involved, yielding\nqualitatively similar results to those obtained with Baum--Welch filters on\nfinite state spaces. We then provide practical algorithms for implementing the\nrecursions, as well as approximations thereof via an informed pruning of the\nmixtures, and we show superior performance to particle filters both in accuracy\nand computational efficiency. The code for optimal filtering, smoothing and\nparameter inference is made available in the Julia package\nDualOptimalFiltering.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:48:57 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 13:29:30 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["King", "Guillaume Kon Kam", ""], ["Papaspiliopoulos", "Omiros", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "2006.03459", "submitter": "Rouven Schmidt", "authors": "Rouven Schmidt, Thomas Kneib", "title": "Analytic expressions for the Cumulative Distribution Function of the\n  Composed Error Term in Stochastic Frontier Analysis with Truncated Normal and\n  Exponential Inefficiencies", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the stochastic frontier model, the composed error term consists of the\nmeasurement error and the inefficiency term. A general assumption is that the\ninefficiency term follows a truncated normal or exponential distribution. In a\nwide variety of models evaluating the cumulative distribution function of the\ncomposed error term is required. This work introduces and proves four\nrepresentation theorems for these distributions - two for each distributional\nassumptions. These representations can be utilized for a fast and accurate\nevaluation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:03:58 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Schmidt", "Rouven", ""], ["Kneib", "Thomas", ""]]}, {"id": "2006.03936", "submitter": "Ranjan Maitra", "authors": "Karin S. Dorman and Ranjan Maitra", "title": "An Efficient $k$-modes Algorithm for Clustering Categorical Datasets", "comments": "16 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining clusters from data is an important endeavor in many applications. The\n$k$-means method is a popular, efficient, and distribution-free approach for\nclustering numerical-valued data, but does not apply for categorical-valued\nobservations. The $k$-modes method addresses this lacuna by replacing the\nEuclidean with the Hamming distance and the means with the modes in the\n$k$-means objective function. We provide a novel, computationally efficient\nimplementation of $k$-modes, called OTQT. We prove that OTQT finds updates to\nimprove the objective function that are undetectable to existing $k$-modes\nalgorithms. Although slightly slower per iteration due to algorithmic\ncomplexity, OTQT is always more accurate per iteration and almost always faster\n(and only barely slower on some datasets) to the final optimum. Thus, we\nrecommend OTQT as the preferred, default algorithm for $k$-modes optimization.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 18:41:36 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2020 05:32:31 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 20:18:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Dorman", "Karin S.", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2006.03970", "submitter": "Tobia Boschi", "authors": "Tobia Boschi, Matthew Reimherr, Francesca Chiaromonte", "title": "An Efficient Semi-smooth Newton Augmented Lagrangian Method for Elastic\n  Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important and active research area in statistics and\nmachine learning. The Elastic Net is often used to perform selection when the\nfeatures present non-negligible collinearity or practitioners wish to\nincorporate additional known structure. In this article, we propose a new\nSemi-smooth Newton Augmented Lagrangian Method to efficiently solve the Elastic\nNet in ultra-high dimensional settings. Our new algorithm exploits both the\nsparsity induced by the Elastic Net penalty and the sparsity due to the second\norder information of the augmented Lagrangian. This greatly reduces the\ncomputational cost of the problem. Using simulations on both synthetic and real\ndatasets, we demonstrate that our approach outperforms its best competitors by\nat least an order of magnitude in terms of CPU time. We also apply our approach\nto a Genome Wide Association Study on childhood obesity.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 20:42:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Boschi", "Tobia", ""], ["Reimherr", "Matthew", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "2006.04565", "submitter": "Farzana Jahan Mrs", "authors": "Farzana Jahan, Insha Ullah, Kerrie L Mengersen", "title": "A Survey of Bayesian Statistical Approaches for Big Data", "comments": null, "journal-ref": "In Mengersen K., Pudlo P., Robert C. (2020) Case Studies in\n  Applied Bayesian Data Science. Lecture Notes in Mathematics, vol 2259. (pp.\n  17-44) Springer, Cham", "doi": "10.1007/978-3-030-42553-1", "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern era is characterised as an era of information or Big Data. This\nhas motivated a huge literature on new methods for extracting information and\ninsights from these data. A natural question is how these approaches differ\nfrom those that were available prior to the advent of Big Data. We present a\nreview of published studies that present Bayesian statistical approaches\nspecifically for Big Data and discuss the reported and perceived benefits of\nthese approaches. We conclude by addressing the question of whether focusing\nonly on improving computational algorithms and infrastructure will be enough to\nface the challenges of Big Data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:16:02 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Jahan", "Farzana", ""], ["Ullah", "Insha", ""], ["Mengersen", "Kerrie L", ""]]}, {"id": "2006.04576", "submitter": "Patrick J. Laub", "authors": "Patrick J. Laub, Nicole El Karoui, St\\'ephane Loisel, Yahia Salhi", "title": "Quickest detection in practice in presence of seasonality: An\n  illustration with call center data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter, we explain how quickest detection algorithms can be useful\nfor risk management in presence of seasonality. We investigate the problem of\ndetecting fast enough cases when a call center will need extra staff in a near\nfuture with a high probability. We illustrate our findings on real data\nprovided by a French insurer. We also discuss the relevance of the CUSUM\nalgorithm and of some machine-learning type competitor for this applied\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 13:26:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Laub", "Patrick J.", ""], ["Karoui", "Nicole El", ""], ["Loisel", "St\u00e9phane", ""], ["Salhi", "Yahia", ""]]}, {"id": "2006.04877", "submitter": "Vahid Partovi Nia", "authors": "Vahid Partovi Nia, Xinlin Li, Masoud Asgharian, Shoubo Hu, Zhitang\n  Chen, Yanhui Geng", "title": "Clustering Causal Additive Noise Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive noise models are commonly used to infer the causal direction for a\ngiven set of observed data. Most causal models assume a single homogeneous\npopulation. However, observations may be collected under different conditions\nin practice. Such data often require models that can accommodate possible\nheterogeneity caused by different conditions under which data have been\ncollected. We propose a clustering algorithm inspired by the $k$-means\nalgorithm, but with unknown $k$. Using the proposed algorithm, both the labels\nand the number of components are estimated from the collected data. The\nestimated labels are used to adjust the causal direction test statistic. The\nadjustment significantly improves the performance of the test statistic in\nidentifying the correct causal direction.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:59:14 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Nia", "Vahid Partovi", ""], ["Li", "Xinlin", ""], ["Asgharian", "Masoud", ""], ["Hu", "Shoubo", ""], ["Chen", "Zhitang", ""], ["Geng", "Yanhui", ""]]}, {"id": "2006.05061", "submitter": "Xueying Tang", "authors": "Xueying Tang, Susu Zhang, Zhi Wang, Jingchen Liu, Zhiliang Ying", "title": "ProcData: An R Package for Process Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process data refer to data recorded in the log files of computer-based items.\nThese data, represented as timestamped action sequences, keep track of\nrespondents' response processes of solving the items. Process data analysis\naims at enhancing educational assessment accuracy and serving other assessment\npurposes by utilizing the rich information contained in response processes. The\nR package ProcData presented in this article is designed to provide tools for\nprocessing, describing, and analyzing process data. We define an S3 class\n\"proc\" for organizing process data and extend generic methods summary and print\nfor class \"proc\". Two feature extraction methods for process data are\nimplemented in the package for compressing information in the irregular\nresponse processes into regular numeric vectors. ProcData also provides\nfunctions for fitting and making predictions from a neural-network-based\nsequence model. These functions call relevant functions in package keras for\nconstructing and training neural networks. In addition, several response\nprocess generators and a real dataset of response processes of the climate\ncontrol item in the 2012 Programme for International Student Assessment are\nincluded in the package.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 05:44:57 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Tang", "Xueying", ""], ["Zhang", "Susu", ""], ["Wang", "Zhi", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "2006.05145", "submitter": "Brendan O'Donoghue", "authors": "Brendan O'Donoghue, Tor Lattimore, Ian Osband", "title": "Matrix games with bandit feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a version of the classical zero-sum matrix game with unknown payoff\nmatrix and bandit feedback, where the players only observe each others actions\nand a noisy payoff. This generalizes the usual matrix game, where the payoff\nmatrix is known to the players. Despite numerous applications, this problem has\nreceived relatively little attention. Although adversarial bandit algorithms\nachieve low regret, they do not exploit the matrix structure and perform poorly\nrelative to the new algorithms. The main contributions are regret analyses of\nvariants of UCB and K-learning that hold for any opponent, e.g., even when the\nopponent adversarially plays the best-response to the learner's mixed strategy.\nAlong the way, we show that Thompson fails catastrophically in this setting and\nprovide empirical comparison to existing algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 09:36:21 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 10:23:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["O'Donoghue", "Brendan", ""], ["Lattimore", "Tor", ""], ["Osband", "Ian", ""]]}, {"id": "2006.05381", "submitter": "Yifan Cui", "authors": "Yifan Cui, Jan Hannig", "title": "A fiducial approach to nonparametric deconvolution problem: discrete\n  case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial inference, as generalized by Hannig et al. (2016), is applied to\nnonparametric g-modeling (Efron, 2016) in the discrete case. We propose a\ncomputationally efficient algorithm to sample from the fiducial distribution,\nand use generated samples to construct point estimates and confidence\nintervals. We study the theoretical properties of the fiducial distribution and\nperform extensive simulations in various scenarios. The proposed approach gives\nrise to surprisingly good statistical performance in terms of the mean squared\nerror of point estimators and coverage of confidence intervals. Furthermore, we\napply the proposed fiducial method to estimate the probability of each\nsatellite site being malignant using gastric adenocarcinoma data with 844\npatients (Efron, 2016).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:29:24 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 23:53:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 00:33:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cui", "Yifan", ""], ["Hannig", "Jan", ""]]}, {"id": "2006.05496", "submitter": "Felipe Uribe", "authors": "Felipe Uribe, Iason Papaioannou, Youssef M. Marzouk, Daniel Straub", "title": "Cross-entropy-based importance sampling with failure-informed dimension\n  reduction for rare event simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of rare event or failure probabilities in high dimensions is\nof interest in many areas of science and technology. We consider problems where\nthe rare event is expressed in terms of a computationally costly numerical\nmodel. Importance sampling with the cross-entropy method offers an efficient\nway to address such problems provided that a suitable parametric family of\nbiasing densities is employed. Although some existing parametric distribution\nfamilies are designed to perform efficiently in high dimensions, their\napplicability within the cross-entropy method is limited to problems with\ndimension of O(1e2). In this work, rather than directly building sampling\ndensities in high dimensions, we focus on identifying the intrinsic\nlow-dimensional structure of the rare event simulation problem. To this end, we\nexploit a connection between rare event simulation and Bayesian inverse\nproblems. This allows us to adapt dimension reduction techniques from Bayesian\ninference to construct new, effectively low-dimensional, biasing distributions\nwithin the cross-entropy method. In particular, we employ the approach in [47],\nas it enables control of the error in the approximation of the optimal biasing\ndistribution. We illustrate our method using two standard high-dimensional\nreliability benchmark problems and one structural mechanics application\ninvolving random fields.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 20:38:25 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Uribe", "Felipe", ""], ["Papaioannou", "Iason", ""], ["Marzouk", "Youssef M.", ""], ["Straub", "Daniel", ""]]}, {"id": "2006.05527", "submitter": "Lutz Duembgen", "authors": "Alexander Henzi and Alexandre Moesching and Lutz Duembgen", "title": "Accelerating the pool-adjacent-violators algorithm for isotonic\n  distributional regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of estimating stochastically ordered distribution functions,\nthe pool-adjacent-violators algorithm (PAVA) can be modified such that the\ncomputation times are reduced substantially. This is achieved by studying the\ndependence of antitonic weighted least squares fits on the response vector to\nbe approximated.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:58:47 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 12:06:33 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 12:46:12 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Henzi", "Alexander", ""], ["Moesching", "Alexandre", ""], ["Duembgen", "Lutz", ""]]}, {"id": "2006.06102", "submitter": "Mateusz B. Majka", "authors": "Mateusz B. Majka, Marc Sabate-Vidales, {\\L}ukasz Szpruch", "title": "Multi-index Antithetic Stochastic Gradient Algorithm", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational\nstatistics, machine learning and optimisation. Recent years have brought an\ninflux of interest in SGAs and the non-asymptotic analysis of their bias is by\nnow well-developed. However, in order to fully understand the efficiency of\nMonte Carlo algorithms utilizing stochastic gradients, one also needs to carry\nout the analysis of their variance, which turns out to be problem-specific. For\nthis reason, there is no systematic theory that would specify the optimal\nchoice of the random approximation of the gradient in SGAs for a given data\nregime. Furthermore, while there have been numerous attempts to reduce the\nvariance of SGAs, these typically exploit a particular structure of the sampled\ndistribution. In this paper we use the Multi-index Monte Carlo apparatus\ncombined with the antithetic approach to construct the Multi-index Antithetic\nStochastic Gradient Algorithm (MASGA), which can be used to sample from any\nprobability distribution. This, to our knowledge, is the first SGA that, for\nall data regimes and without relying on any specific structure of the target\nmeasure, achieves performance on par with Monte Carlo estimators that have\naccess to unbiased samples from the distribution of interest. In other words,\nMASGA is an optimal estimator from the error-computational cost perspective\nwithin the class of Monte Carlo estimators.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 22:59:23 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Majka", "Mateusz B.", ""], ["Sabate-Vidales", "Marc", ""], ["Szpruch", "\u0141ukasz", ""]]}, {"id": "2006.06172", "submitter": "Xinyu Zhang", "authors": "Xinyu Zhang", "title": "Safe Screening Rules for Generalized Double Sparsity Learning", "comments": "22pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a high-dimensional setting, sparse model has shown its power in\ncomputational and statistical efficiency. We consider variables selection\nproblem with a broad class of simultaneous sparsity regularization, enforcing\nboth feature-wise and group-wise sparsity at the same time. The analysis\nleverages an introduction of $\\epsilon q$-norm in vector space, which is proved\nto has close connection with the mixture regularization and naturally leads to\na dual formulation. Properties of primal/dual optimal solution and optimal\nvalues are discussed, which motivates the design of screening rules. We several\nfast safe screening rules in the general framework, rules that discard inactive\nfeatures/groups at an early stage that are guaranteed to be inactive in the\nexact solution, leading to a significant gain in computation speed.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 03:47:29 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zhang", "Xinyu", ""]]}, {"id": "2006.06706", "submitter": "Alan Heavens", "authors": "Alan Heavens, Elena Sellentin, Andrew Jaffe", "title": "Extreme data compression while searching for new physics", "comments": "12 pages, 12 figures. Accepted for publication in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/staa2589", "report-no": null, "categories": "astro-ph.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bringing a high-dimensional dataset into science-ready shape is a formidable\nchallenge that often necessitates data compression. Compression has accordingly\nbecome a key consideration for contemporary cosmology, affecting public data\nreleases, and reanalyses searching for new physics. However, data compression\noptimized for a particular model can suppress signs of new physics, or even\nremove them altogether. We therefore provide a solution for exploring new\nphysics \\emph{during} data compression. In particular, we store additional\nagnostic compressed data points, selected to enable precise constraints of\nnon-standard physics at a later date. Our procedure is based on the maximal\ncompression of the MOPED algorithm, which optimally filters the data with\nrespect to a baseline model. We select additional filters, based on a\ngeneralised principal component analysis, which are carefully constructed to\nscout for new physics at high precision and speed. We refer to the augmented\nset of filters as MOPED-PC. They enable an analytic computation of Bayesian\nevidences that may indicate the presence of new physics, and fast analytic\nestimates of best-fitting parameters when adopting a specific non-standard\ntheory, without further expensive MCMC analysis. As there may be large numbers\nof non-standard theories, the speed of the method becomes essential. Should no\nnew physics be found, then our approach preserves the precision of the standard\nparameters. As a result, we achieve very rapid and maximally precise\nconstraints of standard and non-standard physics, with a technique that scales\nwell to large dimensional datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 18:04:57 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 16:10:06 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Heavens", "Alan", ""], ["Sellentin", "Elena", ""], ["Jaffe", "Andrew", ""]]}, {"id": "2006.06729", "submitter": "Carlos Mora", "authors": "Carlos M. Mora, Juan Carlos Jimenez, Monica Selva", "title": "Weak variable step-size Euler schemes for stochastic differential\n  equations based on controlling conditional moments", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the weak numerical solution of stochastic differential equations\ndriven by independent Brownian motions (SDEs for short). This paper develops a\nnew methodology to design adaptive strategies for determining automatically the\nstep-sizes of the numerical schemes that compute the mean values of smooth\nfunctions of the solutions of SDEs. First, we introduce a general method for\nconstructing variable step-size weak schemes for SDEs, which is based on\ncontrolling the matching between the first conditional moments of the\nincrements of the numerical integrator and the ones corresponding to an\nadditional weak approximation. To this end, we use certain local discrepancy\nfunctions that do not involve sampling random variables. Precise directions for\ndesigning suitable discrepancy functions and for selecting starting step-sizes\nare given. Second, we introduce three variable step-size Euler schemes derived\nfrom three different discrepancy functions, as well as four variable step-size\nhigher order weak schemes are proposed. Third, to compute the expectation of\nfunctionals of diffusion processes a general procedure for designing adaptive\nschemes with variable step-size and sample-size is presented, which combines a\nconventional Monte Carlo technique for estimating the total number of\nsimulations with the new variable step-size weak schemes. Finally, a variety of\nnumerical simulations are presented to show the potential of the introduced\nvariable step-size strategies and adaptive schemes to overcome known\ninstability problems of the conventional fixed step-size schemes in the\ncomputation of diffusion functional expectations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 18:41:29 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Mora", "Carlos M.", ""], ["Jimenez", "Juan Carlos", ""], ["Selva", "Monica", ""]]}, {"id": "2006.06755", "submitter": "Bamdad Hosseini Dr.", "authors": "Nikola Kovachki, Ricardo Baptista, Bamdad Hosseini, Youssef Marzouk", "title": "Conditional Sampling With Monotone GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach for sampling conditional probability measures,\nenabling consistent uncertainty quantification in supervised learning tasks. We\nconstruct a mapping that transforms a reference measure to the measure of the\noutput conditioned on new inputs. The mapping is trained via a modification of\ngenerative adversarial networks (GANs), called monotone GANs, that imposes\nmonotonicity and a block triangular structure. We present theoretical\nguarantees for the consistency of our proposed method, as well as numerical\nexperiments demonstrating the ability of our method to accurately sample\nconditional measures in applications ranging from inverse problems to image\nin-painting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 19:15:43 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 16:36:59 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Kovachki", "Nikola", ""], ["Baptista", "Ricardo", ""], ["Hosseini", "Bamdad", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2006.07435", "submitter": "Zhanhao Peng", "authors": "Zhanhao Peng and Qing Zhou", "title": "An Empirical Bayes Approach to Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The graphon (W-graph), including the stochastic block model as a special\ncase, has been widely used in modeling and analyzing network data. This random\ngraph model is well-characterized by its graphon function, and estimation of\nthe graphon function has gained a lot of recent research interests. Most\nexisting works focus on community detection in the latent space of the model,\nwhile adopting simple maximum likelihood or Bayesian estimates for the graphon\nor connectivity parameters given the identified communities. In this work, we\npropose a hierarchical Binomial model and develop a novel empirical Bayes\nestimate of the connectivity matrix of a stochastic block model to approximate\nthe graphon function. Based on the likelihood of our hierarchical model, we\nfurther introduce a model selection criterion for choosing the number of\ncommunities. Numerical results on extensive simulations and two well-annotated\nsocial networks demonstrate the superiority of our approach in terms of\nestimation accuracy and model selection.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 19:29:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Peng", "Zhanhao", ""], ["Zhou", "Qing", ""]]}, {"id": "2006.07561", "submitter": "Somak Dutta", "authors": "Dongjin Li, Somak Dutta and Vivekananda Roy", "title": "Model Based Screening Embedded Bayesian Variable Selection for\n  Ultra-high Dimensional Settings", "comments": "54 pages including supplementary,4 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian variable selection method, called SVEN, based on a\nhierarchical Gaussian linear model with priors placed on the regression\ncoefficients as well as on the model space. Sparsity is achieved by using\ndegenerate spike priors on inactive variables, whereas Gaussian slab priors are\nplaced on the coefficients for the important predictors making the posterior\nprobability of a model available in explicit form (up to a normalizing\nconstant). The strong model selection consistency is shown to be attained when\nthe number of predictors grows nearly exponentially with the sample size and\neven when the norm of mean effects solely due to the unimportant variables\ndiverge, which is a novel attractive feature. An appealing byproduct of SVEN is\nthe construction of novel model weight adjusted prediction intervals. Embedding\na unique model based screening and using fast Cholesky updates, SVEN produces a\nhighly scalable computational framework to explore gigantic model spaces,\nrapidly identify the regions of high posterior probabilities and make fast\ninference and prediction. A temperature schedule guided by our model selection\nconsistency derivations is used to further mitigate multimodal posterior\ndistributions. The performance of SVEN is demonstrated through a number of\nsimulation experiments and a real data example from a genome wide association\nstudy with over half a million markers.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 04:43:07 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 19:06:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Li", "Dongjin", ""], ["Dutta", "Somak", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2006.07571", "submitter": "Masahiro Fujisawa", "authors": "Masahiro Fujisawa, Takeshi Teshima, Issei Sato, Masashi Sugiyama", "title": "$\\gamma$-ABC: Outlier-Robust Approximate Bayesian Computation Based on a\n  Robust Divergence Estimator", "comments": "The 24th International Conference on Artificial Intelligence and\n  Statistics (AISTATS 2021); 48 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a likelihood-free inference method\nthat has been employed in various applications. However, ABC can be sensitive\nto outliers if a data discrepancy measure is chosen inappropriately. In this\npaper, we propose to use a nearest-neighbor-based $\\gamma$-divergence estimator\nas a data discrepancy measure. We show that our estimator possesses a suitable\ntheoretical robustness property called the redescending property. In addition,\nour estimator enjoys various desirable properties such as high flexibility,\nasymptotic unbiasedness, almost sure convergence, and linear-time computational\ncomplexity. Through experiments, we demonstrate that our method achieves\nsignificantly higher robustness than existing discrepancy measures.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 06:09:27 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 08:06:41 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 05:16:43 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Fujisawa", "Masahiro", ""], ["Teshima", "Takeshi", ""], ["Sato", "Issei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2006.07687", "submitter": "Neil A. Spencer", "authors": "Neil A. Spencer, Brian Junker, Tracy M. Sweet", "title": "Faster MCMC for Gaussian Latent Position Network Models", "comments": "61 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent position network models are a versatile tool in network science;\napplications include clustering entities, controlling for causal confounders,\nand defining priors over unobserved graphs. Estimating each node's latent\nposition is typically framed as a Bayesian inference problem, with Metropolis\nwithin Gibbs being the most popular tool for approximating the posterior\ndistribution. However, it is well-known that Metropolis within Gibbs is\ninefficient for large networks; the acceptance ratios are expensive to compute,\nand the resultant posterior draws are highly correlated. In this article, we\npropose an alternative Markov chain Monte Carlo strategy---defined using a\ncombination of split Hamiltonian Monte Carlo and Firefly Monte Carlo---that\nleverages the posterior distribution's functional form for more efficient\nposterior computation. We demonstrate that these strategies outperform\nMetropolis within Gibbs and other algorithms on synthetic networks, as well as\non real information-sharing networks of teachers and staff in a school\ndistrict.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 17:38:34 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Spencer", "Neil A.", ""], ["Junker", "Brian", ""], ["Sweet", "Tracy M.", ""]]}, {"id": "2006.07904", "submitter": "Lu Yu", "authors": "Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A.\n  Erdogdu", "title": "An Analysis of Constant Step Size SGD in the Non-convex Regime:\n  Asymptotic Normality and Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured non-convex learning problems, for which critical points have\nfavorable statistical properties, arise frequently in statistical machine\nlearning. Algorithmic convergence and statistical estimation rates are\nwell-understood for such problems. However, quantifying the uncertainty\nassociated with the underlying training algorithm is not well-studied in the\nnon-convex setting. In order to address this shortcoming, in this work, we\nestablish an asymptotic normality result for the constant step size stochastic\ngradient descent (SGD) algorithm--a widely used algorithm in practice.\nSpecifically, based on the relationship between SGD and Markov Chains [DDB19],\nwe show that the average of SGD iterates is asymptotically normally distributed\naround the expected value of their unique invariant distribution, as long as\nthe non-convex and non-smooth objective function satisfies a dissipativity\nproperty. We also characterize the bias between this expected value and the\ncritical points of the objective function under various local regularity\nconditions. Together, the above two results could be leveraged to construct\nconfidence intervals for non-convex problems that are trained using the SGD\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 13:58:44 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 01:27:47 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yu", "Lu", ""], ["Balasubramanian", "Krishnakumar", ""], ["Volgushev", "Stanislav", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "2006.07998", "submitter": "Kevin O'Connor", "authors": "Kevin O'Connor, Kevin McGoff, Andrew B. Nobel", "title": "Optimal Transport for Stationary Markov Chains via Policy Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal transport problem for pairs of stationary finite-state\nMarkov chains, with an emphasis on the computation of optimal transition\ncouplings. Transition couplings are a constrained family of transport plans\nthat capture the dynamics of Markov chains. Solutions of the optimal transition\ncoupling (OTC) problem correspond to alignments of the two chains that minimize\nlong-term average cost. We establish a connection between the OTC problem and\nMarkov decision processes, and show that solutions of the OTC problem can be\nobtained via an adaptation of policy iteration. For settings with large state\nspaces, we develop a fast approximate algorithm based on an entropy-regularized\nversion of the OTC problem, and provide bounds on its per-iteration complexity.\nWe establish a stability result for both the regularized and unregularized\nalgorithms, from which a statistical consistency result follows as a corollary.\nWe validate our theoretical results empirically through a simulation study,\ndemonstrating that the approximate algorithm exhibits faster overall runtime\nwith low error. Finally, we extend the setting and application of our methods\nto hidden Markov models, and illustrate the potential use of the proposed\nalgorithms in practice with an application to computer-generated music.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 19:55:58 GMT"}, {"version": "v2", "created": "Mon, 22 Jun 2020 20:32:32 GMT"}, {"version": "v3", "created": "Tue, 20 Oct 2020 17:37:46 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 21:15:43 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["O'Connor", "Kevin", ""], ["McGoff", "Kevin", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "2006.08036", "submitter": "Marcos Prates O", "authors": "Victor H. Lachos Davila, Marcos O. Prates, Dipak K. Dey", "title": "Heckman selection-t model: parameter estimation via the EM-algorithm", "comments": "19 pages, 5 Tables, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heckman selection model is perhaps the most popular econometric model in the\nanalysis of data with sample selection. The analyses of this model are based on\nthe normality assumption for the error terms, however, in some applications,\nthe distribution of the error term departs significantly from normality, for\ninstance, in the presence of heavy tails and/or atypical observation. In this\npaper, we explore the Heckman selection-t model where the random errors follow\na bivariate Student's-t distribution. We develop an analytically tractable and\nefficient EM-type algorithm for iteratively computing maximum likelihood\nestimates of the parameters, with standard errors as a by-product. The\nalgorithm has closed-form expressions at the E-step, that rely on formulas for\nthe mean and variance of the truncated Student's-t distributions. Simulations\nstudies show the vulnerability of the Heckman selection-normal model, as well\nas the robustness aspects of the Heckman selection-t model. Two real examples\nare analyzed, illustrating the usefulness of the proposed methods. The proposed\nalgorithms and methods are implemented in the new R package HeckmanEM.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 21:59:20 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Davila", "Victor H. Lachos", ""], ["Prates", "Marcos O.", ""], ["Dey", "Dipak K.", ""]]}, {"id": "2006.08350", "submitter": "Bertrand Hassani", "authors": "Bertrand K. Hassani", "title": "Societal biases reinforcement through machine learning: A credit scoring\n  perspective", "comments": "14 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Does machine learning and AI ensure that social biases thrive ? This paper\naims to analyse this issue. Indeed, as algorithms are informed by data, if\nthese are corrupted, from a social bias perspective, good machine learning\nalgorithms would learn from the data provided and reverberate the patterns\nlearnt on the predictions related to either the classification or the\nregression intended. In other words, the way society behaves whether positively\nor negatively, would necessarily be reflected by the models. In this paper, we\nanalyse how social biases are transmitted from the data into banks loan\napprovals by predicting either the gender or the ethnicity of the customers\nusing the exact same information provided by customers through their\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 12:40:21 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 12:48:35 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Hassani", "Bertrand K.", ""]]}, {"id": "2006.08553", "submitter": "Chi Zhang", "authors": "Chi Zhang, Jennifer Ahern, Mark J. van der Laan, Oleg Sofrygin", "title": "tmleCommunity: A R Package Implementing Target Maximum Likelihood\n  Estimation for Community-level Data", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past years, many applications aim to assess the causal effect of\ntreatments assigned at the community level, while data are still collected at\nthe individual level among individuals of the community. In many cases, one\nwants to evaluate the effect of a stochastic intervention on the community,\nwhere all communities in the target population receive probabilistically\nassigned treatments based on a known specified mechanism (e.g., implementing a\ncommunity-level intervention policy that target stochastic changes in the\nbehavior of a target population of communities). The tmleCommunity package is\nrecently developed to implement targeted minimum loss-based estimation (TMLE)\nof the effect of community-level intervention(s) at a single time point on an\nindividual-based outcome of interest, including the average causal effect.\nImplementations of the inverse-probability-of-treatment-weighting (IPTW) and\nthe G-computation formula (GCOMP) are also available. The package supports\nmultivariate arbitrary (i.e., static, dynamic or stochastic) interventions with\na binary or continuous outcome. Besides, it allows user-specified data-adaptive\nmachine learning algorithms through SuperLearner, sl3 and h2oEnsemble packages.\nThe usage of the tmleCommunity package, along with a few examples, will be\ndescribed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:16:40 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhang", "Chi", ""], ["Ahern", "Jennifer", ""], ["van der Laan", "Mark J.", ""], ["Sofrygin", "Oleg", ""]]}, {"id": "2006.08655", "submitter": "Luigi Acerbi", "authors": "Luigi Acerbi", "title": "Variational Bayesian Monte Carlo with Noisy Likelihoods", "comments": "To appear in Advances in Neural Information Processing Systems 33\n  (NeurIPS 2020). 26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.NC q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayesian Monte Carlo (VBMC) is a recently introduced framework\nthat uses Gaussian process surrogates to perform approximate Bayesian inference\nin models with black-box, non-cheap likelihoods. In this work, we extend VBMC\nto deal with noisy log-likelihood evaluations, such as those arising from\nsimulation-based models. We introduce new `global' acquisition functions, such\nas expected information gain (EIG) and variational interquantile range (VIQR),\nwhich are robust to noise and can be efficiently evaluated within the VBMC\nsetting. In a novel, challenging, noisy-inference benchmark comprising of a\nvariety of models with real datasets from computational and cognitive\nneuroscience, VBMC+VIQR achieves state-of-the-art performance in recovering the\nground-truth posteriors and model evidence. In particular, our method vastly\noutperforms `local' acquisition functions and other surrogate-based inference\nmethods while keeping a small algorithmic cost. Our benchmark corroborates VBMC\nas a general-purpose technique for sample-efficient black-box Bayesian\ninference also with noisy models.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 18:06:18 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 15:30:43 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2020 05:54:29 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Acerbi", "Luigi", ""]]}, {"id": "2006.08804", "submitter": "Mingyuan Zhou", "authors": "Hao Zhang, Bo Chen, Yulai Cong, Dandan Guo, Hongwei Liu, Mingyuan Zhou", "title": "Deep Autoencoding Topic Model with Scalable Hybrid Bayesian Inference", "comments": "To appear in IEEE Transactions on Pattern Analysis and Machine\n  Intelligence. arXiv admin note: text overlap with arXiv:1803.01328", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build a flexible and interpretable model for document analysis, we develop\ndeep autoencoding topic model (DATM) that uses a hierarchy of gamma\ndistributions to construct its multi-stochastic-layer generative network. In\norder to provide scalable posterior inference for the parameters of the\ngenerative network, we develop topic-layer-adaptive stochastic gradient\nRiemannian MCMC that jointly learns simplex-constrained global parameters\nacross all layers and topics, with topic and layer specific learning rates.\nGiven a posterior sample of the global parameters, in order to efficiently\ninfer the local latent representations of a document under DATM across all\nstochastic layers, we propose a Weibull upward-downward variational encoder\nthat deterministically propagates information upward via a deep neural network,\nfollowed by a Weibull distribution based stochastic downward generative model.\nTo jointly model documents and their associated labels, we further propose\nsupervised DATM that enhances the discriminative power of its latent\nrepresentations. The efficacy and scalability of our models are demonstrated on\nboth unsupervised and supervised learning tasks on big corpora.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 22:22:56 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Zhang", "Hao", ""], ["Chen", "Bo", ""], ["Cong", "Yulai", ""], ["Guo", "Dandan", ""], ["Liu", "Hongwei", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2006.08855", "submitter": "Ye Tian", "authors": "Ye Tian and Yang Feng", "title": "RaSE: Random Subspace Ensemble Classification", "comments": "93 pages, 13 figures", "journal-ref": "Journal of Machine Learning Research 22, no. 45 (2021): 1-93", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible ensemble classification framework, Random Subspace\nEnsemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate\nmany weak learners, where each weak learner is a base classifier trained in a\nsubspace optimally selected from a collection of random subspaces. To conduct\nsubspace selection, we propose a new criterion, ratio information criterion\n(RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis\nincludes the risk and Monte-Carlo variance of the RaSE classifier, establishing\nthe screening consistency and weak consistency of RIC, and providing an upper\nbound for the misclassification rate of the RaSE classifier. In addition, we\nshow that in a high-dimensional framework, the number of random subspaces needs\nto be very large to guarantee that a subspace covering signals is selected.\nTherefore, we propose an iterative version of the RaSE algorithm and prove that\nunder some specific conditions, a smaller number of generated random subspaces\nare needed to find a desirable subspace through iteration. An array of\nsimulations under various models and real-data applications demonstrate the\neffectiveness and robustness of the RaSE classifier and its iterative version\nin terms of low misclassification rate and accurate feature ranking. The RaSE\nalgorithm is implemented in the R package RaSEn on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:14:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 18:59:19 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 15:54:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2006.08965", "submitter": "Shunichi Nomura", "authors": "Atsumori Takahashi and Shunichi Nomura", "title": "Efficient Path Algorithms for Clustered Lasso and OSCAR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimensional regression, feature clustering by their effects on\noutcomes is often as important as feature selection. For that purpose,\nclustered Lasso and octagonal shrinkage and clustering algorithm for regression\n(OSCAR) are used to make feature groups automatically by pairwise $L_1$ norm\nand pairwise $L_\\infty$ norm, respectively. This paper proposes efficient path\nalgorithms for clustered Lasso and OSCAR to construct solution paths with\nrespect to their regularization parameters. Despite too many terms in\nexhaustive pairwise regularization, their computational costs are reduced by\nusing symmetry of those terms. Simple equivalent conditions to check\nsubgradient equations in each feature group are derived by some graph theories.\nThe proposed algorithms are shown to be more efficient than existing algorithms\nin numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 07:43:57 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Takahashi", "Atsumori", ""], ["Nomura", "Shunichi", ""]]}, {"id": "2006.09451", "submitter": "John Russo", "authors": "John D. Russo, Jeremy Copperman, Daniel M. Zuckerman", "title": "Iterative trajectory reweighting for estimation of equilibrium and\n  non-equilibrium observables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two algorithms by which a set of short, unbiased trajectories can\nbe iteratively reweighted to obtain various observables. The first algorithm\nestimates the stationary (steady state) distribution of a system by iteratively\nreweighting the trajectories based on the average probability in each state.\nThe algorithm applies to equilibrium or non-equilibrium steady states,\nexploiting the `left' stationarity of the distribution under dynamics -- i.e.,\nin a discrete setting, when the column vector of probabilities is multiplied by\nthe transition matrix expressed as a left stochastic matrix. The second\nprocedure relies on the `right' stationarity of the committor (splitting\nprobability) expressed as a row vector. The algorithms are unbiased, do not\nrely on computing transition matrices, and make no Markov assumption about\ndiscretized states. Here, we apply the procedures to a one-dimensional\ndouble-well potential, and to a 208$\\mu$s atomistic Trp-cage folding trajectory\nfrom D.E. Shaw Research.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:47:26 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Russo", "John D.", ""], ["Copperman", "Jeremy", ""], ["Zuckerman", "Daniel M.", ""]]}, {"id": "2006.09525", "submitter": "Thi Tuyet Trang Chau", "authors": "Thi Tuyet Trang Chau, Pierre Ailliot, Val\\'erie Monbet", "title": "An algorithm for non-parametric estimation in state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models are ubiquitous in the statistical literature since they\nprovide a flexible and interpretable framework for analyzing many time series.\nIn most practical applications, the state-space model is specified through a\nparametric model. However, the specification of such a parametric model may\nrequire an important modeling effort or may lead to models which are not\nflexible enough to reproduce all the complexity of the phenomenon of interest.\nIn such situations, an appealing alternative consists in inferring the\nstate-space model directly from the data using a non-parametric framework. The\nrecent developments of powerful simulation techniques have permitted to improve\nthe statistical inference for parametric state-space models. It is proposed to\ncombine two of these techniques, namely the Stochastic Expectation-Maximization\n(SEM) algorithm and Sequential Monte Carlo (SMC) approaches, for non-parametric\nestimation in state-space models. The performance of the proposed algorithm is\nassessed through simulations on toy models and an application to environmental\ndata is discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 21:28:52 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Chau", "Thi Tuyet Trang", ""], ["Ailliot", "Pierre", ""], ["Monbet", "Val\u00e9rie", ""]]}, {"id": "2006.09735", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Rathin Desai and Sai Ganesh Nagarajan and\n  Ioannis Panageas", "title": "Efficient Statistics for Sparse Graphical Models from Truncated Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study high-dimensional estimation from truncated samples.\nWe focus on two fundamental and classical problems: (i) inference of sparse\nGaussian graphical models and (ii) support recovery of sparse linear models.\n  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\\bf x}$\nare generated from a Gaussian $N(\\mu,\\Sigma)$ and observed only if they belong\nto a subset $S \\subseteq \\mathbb{R}^d$. We show that ${\\mu}$ and ${\\Sigma}$ can\nbe estimated with error $\\epsilon$ in the Frobenius norm, using\n$\\tilde{O}\\left(\\frac{\\textrm{nz}({\\Sigma}^{-1})}{\\epsilon^2}\\right)$ samples\nfrom a truncated $\\mathcal{N}({\\mu},{\\Sigma})$ and having access to a\nmembership oracle for $S$. The set $S$ is assumed to have non-trivial measure\nunder the unknown distribution but is otherwise arbitrary.\n  (ii) For sparse linear regression, suppose samples $({\\bf x},y)$ are\ngenerated where $y = {\\bf x}^\\top{{\\Omega}^*} + \\mathcal{N}(0,1)$ and $({\\bf\nx}, y)$ is seen only if $y$ belongs to a truncation set $S \\subseteq\n\\mathbb{R}$. We consider the case that ${\\Omega}^*$ is sparse with a support\nset of size $k$. Our main result is to establish precise conditions on the\nproblem dimension $d$, the support size $k$, the number of observations $n$,\nand properties of the samples and the truncation that are sufficient to recover\nthe support of ${\\Omega}^*$. Specifically, we show that under some mild\nassumptions, only $O(k^2 \\log d)$ samples are needed to estimate ${\\Omega}^*$\nin the $\\ell_\\infty$-norm up to a bounded error.\n  For both problems, our estimator minimizes the sum of the finite population\nnegative log-likelihood function and an $\\ell_1$-regularization term.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:21:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Desai", "Rathin", ""], ["Nagarajan", "Sai Ganesh", ""], ["Panageas", "Ioannis", ""]]}, {"id": "2006.10124", "submitter": "Arvind Thiagarajan", "authors": "Arvind Thiagarajan", "title": "Improvements in Computation and Usage of Joint CDFs for the\n  N-Dimensional Order Statistic", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order statistics provide an intuition for combining multiple lists of scores\nover a common index set. This intuition is particularly valuable when the lists\nto be combined cannot be directly compared in a sensible way. We describe here\nthe advantages of a new method for using joint CDFs of such order statistics to\ncombine score lists. We also present, with proof, a new algorithm for computing\nsuch joint CDF values, with runtime linear in the size of the combined list.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:51:45 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Thiagarajan", "Arvind", ""]]}, {"id": "2006.10126", "submitter": "Arvind Thiagarajan", "authors": "Arvind Thiagarajan", "title": "Using Weighted P-Values in Fisher's Method", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's method prescribes a way to combine p-values from multiple\nexperiments into a single p-value. However, the original method can only\ndetermine a combined p-value analytically if all constituent p-values are\nweighted equally. Here we present, with proof, a method to combine p-values\nwith arbitrary weights.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:56:09 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Thiagarajan", "Arvind", ""]]}, {"id": "2006.10256", "submitter": "K. Jarrod Millman", "authors": "Charles R. Harris, K. Jarrod Millman, St\\'efan J. van der Walt, Ralf\n  Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor,\n  Sebastian Berg, Nathaniel J. Smith, Robert Kern, Matti Picus, Stephan Hoyer,\n  Marten H. van Kerkwijk, Matthew Brett, Allan Haldane, Jaime Fern\\'andez del\n  R\\'io, Mark Wiebe, Pearu Peterson, Pierre G\\'erard-Marchant, Kevin Sheppard,\n  Tyler Reddy, Warren Weckesser, Hameer Abbasi, Christoph Gohlke, Travis E.\n  Oliphant", "title": "Array Programming with NumPy", "comments": null, "journal-ref": "Nature 585, 357 (2020)", "doi": "10.1038/s41586-020-2649-2", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Array programming provides a powerful, compact, expressive syntax for\naccessing, manipulating, and operating on data in vectors, matrices, and\nhigher-dimensional arrays. NumPy is the primary array programming library for\nthe Python language. It plays an essential role in research analysis pipelines\nin fields as diverse as physics, chemistry, astronomy, geoscience, biology,\npsychology, material science, engineering, finance, and economics. For example,\nin astronomy, NumPy was an important part of the software stack used in the\ndiscovery of gravitational waves and the first imaging of a black hole. Here we\nshow how a few fundamental array concepts lead to a simple and powerful\nprogramming paradigm for organizing, exploring, and analyzing scientific data.\nNumPy is the foundation upon which the entire scientific Python universe is\nconstructed. It is so pervasive that several projects, targeting audiences with\nspecialized needs, have developed their own NumPy-like interfaces and array\nobjects. Because of its central position in the ecosystem, NumPy increasingly\nplays the role of an interoperability layer between these new array computation\nlibraries.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:39:27 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Harris", "Charles R.", ""], ["Millman", "K. Jarrod", ""], ["van der Walt", "St\u00e9fan J.", ""], ["Gommers", "Ralf", ""], ["Virtanen", "Pauli", ""], ["Cournapeau", "David", ""], ["Wieser", "Eric", ""], ["Taylor", "Julian", ""], ["Berg", "Sebastian", ""], ["Smith", "Nathaniel J.", ""], ["Kern", "Robert", ""], ["Picus", "Matti", ""], ["Hoyer", "Stephan", ""], ["van Kerkwijk", "Marten H.", ""], ["Brett", "Matthew", ""], ["Haldane", "Allan", ""], ["del R\u00edo", "Jaime Fern\u00e1ndez", ""], ["Wiebe", "Mark", ""], ["Peterson", "Pearu", ""], ["G\u00e9rard-Marchant", "Pierre", ""], ["Sheppard", "Kevin", ""], ["Reddy", "Tyler", ""], ["Weckesser", "Warren", ""], ["Abbasi", "Hameer", ""], ["Gohlke", "Christoph", ""], ["Oliphant", "Travis E.", ""]]}, {"id": "2006.10258", "submitter": "Chul Moon", "authors": "Chul Moon and Adel Bedoui", "title": "Bayesian Elastic Net based on Empirical Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Empirical likelihood is a popular nonparametric method for inference and\nestimation. In this article, we propose a Bayesian elastic net model that is\nbased on empirical likelihood for variable selection. The proposed method\nincorporates interpretability and robustness from Bayesian empirical likelihood\napproach. We derive asymptotic distributions of coefficients for credible\nintervals. The posterior distribution of Bayesian empirical likelihood does not\nhave a closed-form analytic expression and has nonconvex domain, which causes\nimplementation of MCMC challenging. To solve this problem, we implement the\nHamiltonian Monte Carlo approach. Simulation studies and real data analysis\ndemonstrate the advantages of the proposed method in variable selection and\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 03:43:00 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Moon", "Chul", ""], ["Bedoui", "Adel", ""]]}, {"id": "2006.10716", "submitter": "Pierre Miasnikof", "authors": "Seo Hong and Pierre Miasnikof and Roy Kwon and Yuri Lawryshyn", "title": "Market Graph Clustering Via QUBO and Digital Annealing", "comments": "9 pages, 1 figure, 4 subfigures", "journal-ref": "https://www.mdpi.com/1911-8074/14/1/34", "doi": null, "report-no": null, "categories": "cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to find representative nodes of a market graph that best\nreplicate the returns of a broader market graph (index), a common task in the\nfinancial industry. We model our reference index as a market graph and express\nthe index tracking problem in a quadratic K-medoids form. We take advantage of\na purpose built hardware architecture, the Fujitsu Digital Annealer, to\ncircumvent the NP-hard nature of the problem and solve our formulation\nefficiently. In this article, we combine three separate areas of the\nliterature, market graph models, K-medoid clustering and quadratic binary\noptimization modeling, to formulate the index-tracking problem as a quadratic\nK-medoid graph-clustering problem. Our initial results show we accurately\nreplicate the returns of a broad market index, using only a small subset of its\nconstituent assets. Moreover, our quadratic formulation allows us to take\nadvantage of recent hardware advances, to overcome the NP-hard nature of the\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:47:45 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hong", "Seo", ""], ["Miasnikof", "Pierre", ""], ["Kwon", "Roy", ""], ["Lawryshyn", "Yuri", ""]]}, {"id": "2006.11182", "submitter": "Uthaipon Tantipongpipat", "authors": "Uthaipon Tantipongpipat", "title": "$\\lambda$-Regularized A-Optimal Design and its Approximation by\n  $\\lambda$-Regularized Proportional Volume Sampling", "comments": "The previous work to which this work extends can be found at\n  arXiv:1802.08318", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the $\\lambda$-regularized $A$-optimal design problem\nand introduce the $\\lambda$-regularized proportional volume sampling algorithm,\ngeneralized from [Nikolov, Singh, and Tantipongpipat, 2019], for this problem\nwith the approximation guarantee that extends upon the previous work. In this\nproblem, we are given vectors $v_1,\\ldots,v_n\\in\\mathbb{R}^d$ in $d$\ndimensions, a budget $k\\leq n$, and the regularizer parameter $\\lambda\\geq0$,\nand the goal is to find a subset $S\\subseteq [n]$ of size $k$ that minimizes\nthe trace of $\\left(\\sum_{i\\in S}v_iv_i^\\top + \\lambda I_d\\right)^{-1}$ where\n$I_d$ is the $d\\times d$ identity matrix. The problem is motivated from optimal\ndesign in ridge regression, where one tries to minimize the expected squared\nerror of the ridge regression predictor from the true coefficient in the\nunderlying linear model. We introduce $\\lambda$-regularized proportional volume\nsampling and give its polynomial-time implementation to solve this problem. We\nshow its $(1+\\frac{\\epsilon}{\\sqrt{1+\\lambda'}})$-approximation for\n$k=\\Omega\\left(\\frac d\\epsilon+\\frac{\\log 1/\\epsilon}{\\epsilon^2}\\right)$ where\n$\\lambda'$ is proportional to $\\lambda$, extending the previous bound in\n[Nikolov, Singh, and Tantipongpipat, 2019] to the case $\\lambda>0$ and\nobtaining asymptotic optimality as $\\lambda\\rightarrow \\infty$.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 15:17:57 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Tantipongpipat", "Uthaipon", ""]]}, {"id": "2006.11228", "submitter": "Hanwen Xing", "authors": "Hanwen Xing, Geoff K. Nicholls, Jeong Eun Lee", "title": "Distortion estimates for approximate Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current literature on posterior approximation for Bayesian inference offers\nmany alternative methods. Does our chosen approximation scheme work well on the\nobserved data? The best existing generic diagnostic tools treating this kind of\nquestion by looking at performance averaged over data space, or otherwise lack\ndiagnostic detail. However, if the approximation is bad for most data, but good\nat the observed data, then we may discard a useful approximation. We give\ngraphical diagnostics for posterior approximation at the observed data. We\nestimate a \"distortion map\" that acts on univariate marginals of the\napproximate posterior to move them closer to the exact posterior, without\nrecourse to the exact posterior.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 17:05:45 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Xing", "Hanwen", ""], ["Nicholls", "Geoff K.", ""], ["Lee", "Jeong Eun", ""]]}, {"id": "2006.11385", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Fakhri Karray, Mark Crowley", "title": "Quantile-Quantile Embedding for Distribution Transformation and Manifold\n  Embedding with Ability to Choose the Embedding Distribution", "comments": "Published in Machine Learning with Applications, Elsevier, Volume 6,\n  Pages 100088, 2021", "journal-ref": "Machine Learning with Applications, Elsevier, Volume 6, Pages\n  100088, 2021", "doi": "10.1016/j.mlwa.2021.100088", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new embedding method, named Quantile-Quantile Embedding (QQE),\nfor distribution transformation and manifold embedding with the ability to\nchoose the embedding distribution. QQE, which uses the concept of\nquantile-quantile plot from visual statistical tests, can transform the\ndistribution of data to any theoretical desired distribution or empirical\nreference sample. Moreover, QQE gives the user a choice of embedding\ndistribution in embedding the manifold of data into the low dimensional\nembedding space. It can also be used for modifying the embedding distribution\nof other dimensionality reduction methods, such as PCA, t-SNE, and deep metric\nlearning, for better representation or visualization of data. We propose QQE in\nboth unsupervised and supervised forms. QQE can also transform a distribution\nto either an exact reference distribution or its shape. We show that QQE allows\nfor better discrimination of classes in some cases. Our experiments on\ndifferent synthetic and image datasets show the effectiveness of the proposed\nembedding method.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 21:09:09 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 03:06:05 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Karray", "Fakhri", ""], ["Crowley", "Mark", ""]]}, {"id": "2006.11715", "submitter": "Shu Wei Chou-Chen", "authors": "Shu Wei Chou-Chen, Pedro A. Morettin", "title": "Indirect inference for locally stationary ARMA processes with stable\n  innovations", "comments": "31 pages, 14 figures. Submitted to the Journal of Statistical\n  Computation and Simulation", "journal-ref": "Journal of Statistical Computation and Simulation, 90:17,\n  3106-3134 (2020)", "doi": "10.1080/00949655.2020.1797030", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of locally stationary processes assumes that there is a\ntime-varying spectral representation, that is, the existence of finite second\nmoment. We propose the $\\alpha$-stable locally stationary process by modifying\nthe innovations into stable distributions and the indirect inference to\nestimate this type of model. Due to the infinite variance, some of interesting\nproperties such as time-varying auto-correlation cannot be defined. However,\nsince the $\\alpha$-stable family of distributions is closed under linear\ncombination which includes the possibility of handling asymmetry and thicker\ntails, the proposed model has the same tail behavior throughout the time. In\nthis paper, we propose this new model, present theoretical properties of the\nprocess and carry out simulations related to the indirect inference in order to\nestimate the parametric form of the model. Finally, an empirical application is\nillustrated.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 05:34:10 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chou-Chen", "Shu Wei", ""], ["Morettin", "Pedro A.", ""]]}, {"id": "2006.11835", "submitter": "Gero Szepannek", "authors": "Gero Szepannek", "title": "An Overview on the Landscape of R Packages for Credit Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The credit scoring industry has a long tradition of using statistical tools\nfor loan default probability prediction and domain specific standards have been\nestablished long before the hype of machine learning. Although several\ncommercial software companies offer specific solutions for credit scorecard\nmodelling in R explicit packages for this purpose have been missing long time.\nIn the recent years this has changed and several packages have been developed\nwhich are dedicated to credit scoring. The aim of this paper is to give a\nstructured overview on these packages. This may guide users to select the\nappropriate functions for a desired purpose and further hopefully will\ncontribute to directing future development activities. The paper is guided by\nthe chain of subsequent modelling steps as they are forming the typical\nscorecard development process.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 15:53:22 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 07:59:13 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Szepannek", "Gero", ""]]}, {"id": "2006.11939", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Noemi Petra, Georg Stadler, and Isaac Sunseri", "title": "Optimal design of large-scale Bayesian linear inverse problems under\n  reducible model uncertainty: good to know what you don't know", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider optimal design of infinite-dimensional Bayesian linear inverse\nproblems governed by partial differential equations that contain secondary\nreducible model uncertainties, in addition to the uncertainty in the inversion\nparameters. By reducible uncertainties we refer to parametric uncertainties\nthat can be reduced through parameter inference. We seek experimental designs\nthat minimize the posterior uncertainty in the primary parameters, while\naccounting for the uncertainty in secondary parameters. We accomplish this by\nderiving a marginalized A-optimality criterion and developing an efficient\ncomputational approach for its optimization. We illustrate our approach for\nestimating an uncertain time-dependent source in a contaminant transport model\nwith an uncertain initial state as secondary uncertainty. Our results indicate\nthat accounting for additional model uncertainty in the experimental design\nprocess is crucial.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 23:35:34 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Alexanderian", "Alen", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Sunseri", "Isaac", ""]]}, {"id": "2006.12180", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Luca Bagnato", "title": "The multivariate tail-inflated normal distribution and its application\n  in finance", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the multivariate tail-inflated normal (MTIN)\ndistribution, an elliptical heavy-tails generalization of the multivariate\nnormal (MN). The MTIN belongs to the family of MN scale mixtures by choosing a\nconvenient continuous uniform as mixing distribution. Moreover, it has a\nclosed-form for the probability density function characterized by only one\nadditional ``inflation'' parameter, with respect to the nested MN, governing\nthe tail-weight. The first four moments are also computed; interestingly, they\nalways exist and the excess kurtosis can assume any positive value. The method\nof moments and maximum likelihood (ML) are considered for estimation. As\nconcerns the latter, a direct approach, as well as a variant of the EM\nalgorithm, are illustrated. The existence of the ML estimates is also\nevaluated. Since the inflation parameter is estimated from the data, robust\nestimates of the mean vector and covariance matrix of the nested MN\ndistribution are automatically obtained by down-weighting. Simulations are\nperformed to compare the estimation methods/algorithms and to investigate the\nability of AIC and BIC to select among a set of candidate elliptical models.\nFor illustrative purposes, the MTIN distribution is finally fitted to\nmultivariate financial data where its usefulness is also shown in comparison\nwith other well-established multivariate elliptical distributions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 12:25:27 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Punzo", "Antonio", ""], ["Bagnato", "Luca", ""]]}, {"id": "2006.12669", "submitter": "Soumya Ghosh", "authors": "Soumya Ghosh and William T. Stephenson and Tin D. Nguyen and Sameer K.\n  Deshpande and Tamara Broderick", "title": "Approximate Cross-Validation for Structured Models", "comments": "25 pages, 8 figures. NeurIPS 2020 camera ready. v2 fixes typos and\n  provides additional empirical results. Code:\n  https://github.com/SoumyaTGhosh/structured-infinitesimal-jackknife", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern data analyses benefit from explicitly modeling dependence\nstructure in data -- such as measurements across time or space, ordered words\nin a sentence, or genes in a genome. A gold standard evaluation technique is\nstructured cross-validation (CV), which leaves out some data subset (such as\ndata within a time interval or data in a geographic region) in each fold. But\nCV here can be prohibitively slow due to the need to re-run already-expensive\nlearning algorithms many times. Previous work has shown approximate\ncross-validation (ACV) methods provide a fast and provably accurate alternative\nin the setting of empirical risk minimization. But this existing ACV work is\nrestricted to simpler models by the assumptions that (i) data across CV folds\nare independent and (ii) an exact initial model fit is available. In structured\ndata analyses, both these assumptions are often untrue. In the present work, we\naddress (i) by extending ACV to CV schemes with dependence structure between\nthe folds. To address (ii), we verify -- both theoretically and empirically --\nthat ACV quality deteriorates smoothly with noise in the initial fit. We\ndemonstrate the accuracy and computational benefits of our proposed methods on\na diverse set of real-world applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 00:06:03 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 17:37:42 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Ghosh", "Soumya", ""], ["Stephenson", "William T.", ""], ["Nguyen", "Tin D.", ""], ["Deshpande", "Sameer K.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2006.12806", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Peter B\\\"uhlmann", "title": "Seeded intervals and noise level estimation in change point detection: A\n  discussion of Fryzlewicz (2020)", "comments": "To appear in the Journal of the Korean Statistical Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this discussion, we compare the choice of seeded intervals and that of\nrandom intervals for change point segmentation from practical, statistical and\ncomputational perspectives. Furthermore, we investigate a novel estimator of\nthe noise level, which improves many existing model selection procedures\n(including the steepest drop to low levels), particularly for challenging\nfrequent change point scenarios with low signal-to-noise ratios.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 07:51:08 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2006.12921", "submitter": "Antonio Figueroa Caceres", "authors": "Antonio Figueroa and Malte Goettsche", "title": "Gaussian Processes for Surrogate Modeling of Discharged Fuel Nuclide\n  Compositions", "comments": "18 pages, 4 figures, 7 tables. Submitted to Annals of Nuclear Energy.\n  Manuscript Accepted for Publication", "journal-ref": null, "doi": "10.1016/j.anucene.2020.108085", "report-no": null, "categories": "physics.comp-ph physics.data-an stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Several applications such as nuclear forensics, nuclear fuel cycle\nsimulations and sensitivity analysis require methods to quickly compute spent\nfuel nuclide compositions for various irradiation histories. Traditionally,\nthis has been done by interpolating between one-group cross-sections that have\nbeen pre-computed from nuclear reactor simulations for a grid of input\nparameters, using fits such as Cubic Spline. We propose the use of Gaussian\nProcesses (GP) to create surrogate models, which not only provide nuclide\ncompositions, but also the gradient and estimates of their prediction\nuncertainty. The former is useful for applications such as forward and inverse\noptimization problems, the latter for uncertainty quantification applications.\nFor this purpose, we compare GP-based surrogate model performance with Cubic-\nSpline-based interpolators based on infinite lattice simulations of a CANDU 6\nnuclear reactor using the SERPENT 2 code, considering burnup and temperature as\ninput parameters. Additionally, we compare the performance of various grid\nsampling schemes to quasirandom sampling based on the Sobol sequence. We find\nthat GP-based models perform significantly better in predicting spent fuel\ncompositions than Cubic-Spline-based models, though requiring longer\ncomputational runtime. Furthermore, we show that the predicted nuclide\nuncertainties are reasonably accurate. While in the studied two-dimensional\ncase, grid- and quasirandom sampling provide similar results, quasirandom\nsampling will be a more effective strategy in higher dimensional cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:57:00 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 13:48:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Figueroa", "Antonio", ""], ["Goettsche", "Malte", ""]]}, {"id": "2006.13107", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Fast, Optimal, and Targeted Predictions using Parametrized Decision\n  Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2021.1891926", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction is critical for decision-making under uncertainty and lends\nvalidity to statistical inference. With targeted prediction, the goal is to\noptimize predictions for specific decision tasks of interest, which we\nrepresent via functionals. Although classical decision analysis extracts\npredictions from a Bayesian model, these predictions are often difficult to\ninterpret and slow to compute. Instead, we design a class of parametrized\nactions for Bayesian decision analysis that produce optimal, scalable, and\nsimple targeted predictions. For a wide variety of action parametrizations and\nloss functions--including linear actions with sparsity constraints for targeted\nvariable selection--we derive a convenient representation of the optimal\ntargeted prediction that yields efficient and interpretable solutions.\nCustomized out-of-sample predictive metrics are developed to evaluate and\ncompare among targeted predictors. Through careful use of the posterior\npredictive distribution, we introduce a procedure that identifies a set of\nnear-optimal, or acceptable targeted predictors, which provide unique insights\ninto the features and level of complexity needed for accurate targeted\nprediction. Simulations demonstrate excellent prediction, estimation, and\nvariable selection capabilities. Targeted predictions are constructed for\nphysical activity data from the National Health and Nutrition Examination\nSurvey (NHANES) to better predict and understand the characteristics of\nintraday physical activity.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:55:47 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 16:39:37 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2006.13131", "submitter": "Maroussia Bojkova Prof.", "authors": "Maroussia Slavtchova-Bojkova, Kaloyan Vitanov", "title": "Computational modelling of cancer evolution by multi-type branching\n  processes", "comments": "7 pages, 4 figures, 62nd ISI world statistics congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metastasis, the spread of cancer cells from a primary tumor to secondary\nlocation(s) in the human organism, is the ultimate cause of death for the\nmajority of cancer patients. That is why, it is crucial to understand\nmetastases evolution in order to successfully combat the disease. We consider a\nmetastasized cancer cell population after medical treatment (e.g.\nchemotherapy). Arriving in a different environment the cancer cells may change\ntheir lifespan and reproduction, thus they may proliferate into different\ntypes. If the treatment is effective, in the context of branching processes it\nmeans, the reproduction of cancer cells is such that the mean offspring of each\ncell is less than one. However, it is possible mutations to occur during cell\ndivision cycle. These mutations can produce a new cancer cell type, which is\nresistant to the treatment. Cancer cells from this new type may lead to the\nrise of a non-extinction branching process. The above scenario leads us to the\nchoice of a reducible multi-type age-dependent branching process as a relevant\nframework for studying the asymptotic behavior of such complex structures. Our\nprevious theoretical results are related to the asymptotic behavior of the\nwaiting time until the first occurrence of a mutant starting a non-extinction\nprocess and the modified hazard function as a measure of immediate recurrence\nof cancer disease. In the present paper these asymptotic results are used for\ndeveloping numerical schemes and algorithms implemented in Python via the NumPy\npackage for approximate calculation of the corresponding quantities. In\nconclusion, our conjecture is that this methodology can be advantageous in\nrevealing the role of the lifespan distribution of the cancer cells in the\ncontext of cancer disease evolution and other complex cell population systems,\nin general.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 16:24:35 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Slavtchova-Bojkova", "Maroussia", ""], ["Vitanov", "Kaloyan", ""]]}, {"id": "2006.13489", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Yehua Li", "title": "Unified Principal Component Analysis for Sparse and Dense Functional\n  Data under Spatial Dependency", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2021.1938085", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spatially dependent functional data collected under a\ngeostatistics setting, where locations are sampled from a spatial point\nprocess. The functional response is the sum of a spatially dependent functional\neffect and a spatially independent functional nugget effect. Observations on\neach function are made on discrete time points and contaminated with\nmeasurement errors. Under the assumption of spatial stationarity and isotropy,\nwe propose a tensor product spline estimator for the spatio-temporal covariance\nfunction. When a coregionalization covariance structure is further assumed, we\npropose a new functional principal component analysis method that borrows\ninformation from neighboring functions. The proposed method also generates\nnonparametric estimators for the spatial covariance functions, which can be\nused for functional kriging. Under a unified framework for sparse and dense\nfunctional data, infill and increasing domain asymptotic paradigms, we develop\nthe asymptotic convergence rates for the proposed estimators. Advantages of the\nproposed approach are demonstrated through simulation studies and two real data\napplications representing sparse and dense functional data, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:26:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 05:30:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Haozhe", ""], ["Li", "Yehua", ""]]}, {"id": "2006.13698", "submitter": "Ick Hoon Jin", "authors": "Jaewoo Park, Yeseul Jeon, Minsuk Shin, Minjeong Jeon, Ick Hoon Jin", "title": "Bayesian Shrinkage for Functional Network Models with Intractable\n  Normalizing Constants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal network models are widely used to study the time-varying\nrelationships between items (nodes), such as analyzing the relations among\nsurvey questions and studying friendship dynamics in school data over time. We\npropose a new model to study these temporal interactions among items by\nembedding the functional parameters within the exponential random graph model\nframework. Inference on such models is difficult because the likelihood\nfunctions contain intractable normalizing constants. Furthermore, the number of\nfunctional parameters grows exponentially as the number of items increases.\nVariable selection for such models is not trivial because standard shrinkage\napproaches do not consider temporal trends in functional parameters. To\novercome these challenges, we develop a novel Bayes approach by combining an\nauxiliary variable MCMC algorithm and a recently developed functional shrinkage\nmethod. We apply our algorithm to two survey data sets and hotel review data,\nillustrating that the proposed approach can avoid the evaluation of intractable\nnormalizing constants as well as detect significant temporal interactions among\nitems. Through a simulation study under different scenarios, we examine the\nperformance of our algorithm. Our method is, to our knowledge, the first\nattempt to select functional variables for models with intractable normalizing\nconstants.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 13:08:12 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Park", "Jaewoo", ""], ["Jeon", "Yeseul", ""], ["Shin", "Minsuk", ""], ["Jeon", "Minjeong", ""], ["Jin", "Ick Hoon", ""]]}, {"id": "2006.13777", "submitter": "Joris Bierkens", "authors": "Joris Bierkens, Sebastiano Grazzi, Kengo Kamatani, Gareth Roberts", "title": "The Boomerang Sampler", "comments": "Accepted for publication in the proceedings of ICML 2020. Code\n  available at https://github.com/jbierkens/ICML-boomerang", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Boomerang Sampler as a novel class of\ncontinuous-time non-reversible Markov chain Monte Carlo algorithms. The\nmethodology begins by representing the target density as a density, $e^{-U}$,\nwith respect to a prescribed (usually) Gaussian measure and constructs a\ncontinuous trajectory consisting of a piecewise elliptical path. The method\nmoves from one elliptical orbit to another according to a rate function which\ncan be written in terms of $U$. We demonstrate that the method is easy to\nimplement and demonstrate empirically that it can out-perform existing\nbenchmark piecewise deterministic Markov processes such as the bouncy particle\nsampler and the Zig-Zag. In the Bayesian statistics context, these competitor\nalgorithms are of substantial interest in the large data context due to the\nfact that they can adopt data subsampling techniques which are exact (ie induce\nno error in the stationary distribution). We demonstrate theoretically and\nempirically that we can also construct a control-variate subsampling boomerang\nsampler which is also exact, and which possesses remarkable scaling properties\nin the large data limit. We furthermore illustrate a factorised version on the\nsimulation of diffusion bridges.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 14:52:22 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 15:54:39 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bierkens", "Joris", ""], ["Grazzi", "Sebastiano", ""], ["Kamatani", "Kengo", ""], ["Roberts", "Gareth", ""]]}, {"id": "2006.13790", "submitter": "Juntao Wang", "authors": "Juntao Wang, Ningzhong Shi, Xue Zhang, Gongjun Xu", "title": "Sequential Gibbs Sampling Algorithm for Cognitive Diagnosis Models with\n  Many Attributes", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive diagnosis models (CDMs) are useful statistical tools to provide\nrich information relevant for intervention and learning. As a popular approach\nto estimate and make inference of CDMs, the Markov chain Monte Carlo (MCMC)\nalgorithm is widely used in practice. However, when the number of attributes,\n$K$, is large, the existing MCMC algorithm may become time-consuming, due to\nthe fact that $O(2^K)$ calculations are usually needed in the process of MCMC\nsampling to get the conditional distribution for each attribute profile. To\novercome this computational issue, motivated by Culpepper and Hudson (2018), we\npropose a computationally efficient sequential Gibbs sampling method, which\nneeds $O(K)$ calculations to sample each attribute profile. We use simulation\nand real data examples to show the good finite-sample performance of the\nproposed sequential Gibbs sampling, and its advantage over existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 15:02:48 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 06:12:04 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Wang", "Juntao", ""], ["Shi", "Ningzhong", ""], ["Zhang", "Xue", ""], ["Xu", "Gongjun", ""]]}, {"id": "2006.13875", "submitter": "Grace Yoon", "authors": "Grace Yoon, Christian L. M\\\"uller, Irina Gaynanova", "title": "Fast computation of latent correlations", "comments": "Main text: 21 pages and 4 figures. Supplementary material: 24 pages\n  and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian copula models provide a powerful means to perform multi-view\ndata integration since these models can seamlessly express dependencies between\nmixed variable types (binary, continuous, zero-inflated) via latent Gaussian\ncorrelations. The estimation of these latent correlations, however, comes at\nconsiderable computational cost, having prevented the routine use of these\nmodels on high-dimensional data. Here, we propose a new computational approach\nfor estimating latent correlations via a hybrid multi-linear interpolation and\noptimization scheme. Our approach speeds up the current state of the art\ncomputation by several orders of magnitude, thus allowing fast computation of\nlatent Gaussian copula models even when the number of variables $p$ is large.\nWe provide theoretical guarantees for the approximation error of our numerical\nscheme and support its excellent performance on simulated and real-world data.\nWe illustrate the practical advantages of our method on high-dimensional sparse\nquantitative and relative abundance microbiome data as well as multi-view data\nfrom The Cancer Genome Atlas Project. Our method is implemented in the R\npackage mixedCCA, available at https://github.com/irinagain/mixedCCA.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:58:35 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 01:37:17 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Yoon", "Grace", ""], ["M\u00fcller", "Christian L.", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2006.13925", "submitter": "Peiyuan Zhu", "authors": "Peiyuan Zhu, Alexandre Bouchard-C\\^ot\\'e, and Trevor Campbell", "title": "Slice Sampling for General Completely Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely random measures provide a principled approach to creating flexible\nunsupervised models, where the number of latent features is infinite and the\nnumber of features that influence the data grows with the size of the data set.\nDue to the infinity the latent features, posterior inference requires either\nmarginalization---resulting in dependence structures that prevent efficient\ncomputation via parallelization and conjugacy---or finite truncation, which\narbitrarily limits the flexibility of the model. In this paper we present a\nnovel Markov chain Monte Carlo algorithm for posterior inference that\nadaptively sets the truncation level using auxiliary slice variables, enabling\nefficient, parallelized computation without sacrificing flexibility. In\ncontrast to past work that achieved this on a model-by-model basis, we provide\na general recipe that is applicable to the broad class of completely random\nmeasure-based priors. The efficacy of the proposed algorithm is evaluated on\nseveral popular nonparametric models, demonstrating a higher effective sample\nsize per second compared to algorithms using marginalization as well as a\nhigher predictive performance compared to models employing fixed truncations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 17:53:53 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 07:30:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Zhu", "Peiyuan", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Campbell", "Trevor", ""]]}, {"id": "2006.14217", "submitter": "Emanuele Aliverti", "authors": "Emanuele Aliverti and Massimiliano Russo", "title": "Stratified stochastic variational inference for high-dimensional network\n  factor model", "comments": "25 pages, 1 figures. Corrected compilation issues and minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable recent interest in Bayesian modeling of\nhigh-dimensional networks via latent space approaches. When the number of nodes\nincreases, estimation based on Markov Chain Monte Carlo can be extremely slow\nand show poor mixing, thereby motivating research on alternative algorithms\nthat scale well in high-dimensional settings. In this article, we focus on the\nlatent factor model, a widely used approach for latent space modeling of\nnetwork data. We develop scalable algorithms to conduct approximate Bayesian\ninference via stochastic optimization. Leveraging sparse representations of\nnetwork data, the proposed algorithms show massive computational and storage\nbenefits, and allow to conduct inference in settings with thousands of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:27:47 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 10:09:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Aliverti", "Emanuele", ""], ["Russo", "Massimiliano", ""]]}, {"id": "2006.14875", "submitter": "Alix Marie d'Avigneau", "authors": "A. Marie d'Avigneau, S. S. Singh, L. M. Murray", "title": "Anytime Parallel Tempering", "comments": "34 Pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing efficient MCMC algorithms is indispensable in Bayesian inference.\nIn parallel tempering, multiple interacting MCMC chains run to more efficiently\nexplore the state space and improve performance. The multiple chains advance\nindependently through local moves, and the performance enhancement steps are\nexchange moves, where the chains pause to exchange their current sample amongst\neach other. To accelerate the independent local moves, they may be performed\nsimultaneously on multiple processors. Another problem is then encountered:\ndepending on the MCMC implementation and inference problem, local moves can\ntake a varying and random amount of time to complete. There may also be\ninfrastructure-induced variations, such as competing jobs on the same\nprocessors, which arises in cloud computing. Before exchanges can occur, all\nchains must complete the local moves they are engaged in to avoid introducing a\npotentially substantial bias (Proposition 2.1). To solve this issue of randomly\nvarying local move completion times in multi-processor parallel tempering, we\nadopt the Anytime Monte Carlo framework of Murray et al. (2016): we impose\nreal-time deadlines on the parallel local moves and perform exchanges at these\ndeadlines without any processor idling. We show our methodology for exchanges\nat real-time deadlines does not introduce a bias and leads to significant\nperformance enhancements over the na\\\"ive approach of idling until every\nprocessor's local moves complete. The methodology is then applied in an ABC\nsetting, where an Anytime ABC parallel tempering algorithm is derived for the\ndifficult task of estimating the parameters of a Lotka-Volterra predator-prey\nmodel, and similar efficiency enhancements are observed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:15:31 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 14:54:38 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["d'Avigneau", "A. Marie", ""], ["Singh", "S. S.", ""], ["Murray", "L. M.", ""]]}, {"id": "2006.14877", "submitter": "Santeri Karppinen", "authors": "Santeri Karppinen and Matti Vihola", "title": "Conditional particle filters with diffuse initial distributions", "comments": "21 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional particle filters (CPFs) are powerful smoothing algorithms for\ngeneral nonlinear/non-Gaussian hidden Markov models. However, CPFs can be\ninefficient or difficult to apply with diffuse initial distributions, which are\ncommon in statistical applications. We propose a simple but generally\napplicable auxiliary variable method, which can be used together with the CPF\nin order to perform efficient inference with diffuse initial distributions. The\nmethod only requires simulatable Markov transitions that are reversible with\nrespect to the initial distribution, which can be improper. We focus in\nparticular on random-walk type transitions which are reversible with respect to\na uniform initial distribution (on some domain), and autoregressive kernels for\nGaussian initial distributions. We propose to use on-line adaptations within\nthe methods. In the case of random-walk transition, our adaptations use the\nestimated covariance and acceptance rate adaptation, and we detail their\ntheoretical validity. We tested our methods with a linear-Gaussian random-walk\nmodel, a stochastic volatility model, and a stochastic epidemic compartment\nmodel with time-varying transmission rate. The experimental findings\ndemonstrate that our method works reliably with little user specification, and\ncan be substantially better mixing than a direct particle Gibbs algorithm that\ntreats initial states as parameters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 09:21:13 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 14:16:41 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Karppinen", "Santeri", ""], ["Vihola", "Matti", ""]]}, {"id": "2006.15442", "submitter": "Andreas Bender", "authors": "Andreas Bender, David R\\\"ugamer, Fabian Scheipl, Bernd Bischl", "title": "A General Machine Learning Framework for Survival Analysis", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-67664-3_10", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of time-to-event data, also known as survival analysis, requires\nspecialized methods that can deal with censoring and truncation, time-varying\nfeatures and effects, and that extend to settings with multiple competing\nevents. However, many machine learning methods for survival analysis only\nconsider the standard setting with right-censored data and proportional hazards\nassumption. The methods that do provide extensions usually address at most a\nsubset of these challenges and often require specialized software that can not\nbe integrated into standard machine learning workflows directly. In this work,\nwe present a very general machine learning framework for time-to-event analysis\nthat uses a data augmentation strategy to reduce complex survival tasks to\nstandard Poisson regression tasks. This reformulation is based on well\ndeveloped statistical theory. With the proposed approach, any algorithm that\ncan optimize a Poisson (log-)likelihood, such as gradient boosted trees, deep\nneural networks, model-based boosting and many more can be used in the context\nof time-to-event analysis. The proposed technique does not require any\nassumptions with respect to the distribution of event times or the functional\nshapes of feature and interaction effects. Based on the proposed framework we\ndevelop new methods that are competitive with specialized state of the art\napproaches in terms of accuracy, and versatility, but with comparatively small\ninvestments of programming effort or requirements for specialized\nmethodological know-how.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 20:57:18 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 18:42:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bender", "Andreas", ""], ["R\u00fcgamer", "David", ""], ["Scheipl", "Fabian", ""], ["Bischl", "Bernd", ""]]}, {"id": "2006.15532", "submitter": "Shaochuan Lu", "authors": "Lu Shaochuan", "title": "Scalable Bayesian Multiple Changepoint Detection via Auxiliary\n  Uniformization", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By attaching auxiliary event times to the chronologically ordered\nobservations, we formulate the Bayesian multiple changepoint problem of\ndiscrete-time observations into that of continuous-time ones. A version of\nforward-filtering backward-sampling (FFBS) algorithm is proposed for the\nsimulation of changepoints within a collapsed Gibbs sampling scheme. Ideally,\nboth the computational cost and memory cost of the FFBS algorithm can be\nquadratically scaled down to the number of changepoints, instead of the number\nof observations, which is otherwise prohibitive for a long sequence of\nobservations. The new formulation allows the number of changepoints accrue\nunboundedly upon the arrivals of new data. Also, a time-varying changepoint\nrecurrence rate across different segments is assumed to characterize diverse\nscales of run lengths of changepoints. We then suggest a continuous-time\nViterbi algorithm for obtaining the Maximum A Posteriori (MAP) estimates of\nchangepoints. We demonstrate the methods through simulation studies and real\ndata analysis.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 07:52:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Shaochuan", "Lu", ""]]}, {"id": "2006.16194", "submitter": "Samuel Thomas", "authors": "Samuel Thomas and Wanzhu Tu", "title": "Learning Hamiltonian Monte Carlo in R", "comments": "45 pages (31 in main paper, 14 in appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a powerful tool for Bayesian computation. In\ncomparison with the traditional Metropolis-Hastings algorithm, HMC offers\ngreater computational efficiency, especially in higher dimensional or more\ncomplex modeling situations. To most statisticians, however, the idea of HMC\ncomes from a less familiar origin, one that is based on the theory of classical\nmechanics. Its implementation, either through Stan or one of its derivative\nprograms, can appear opaque to beginners. A lack of understanding of the inner\nworking of HMC, in our opinion, has hindered its application to a broader range\nof statistical problems. In this article, we review the basic concepts of HMC\nin a language that is more familiar to statisticians, and we describe an HMC\nimplementation in R, one of the most frequently used statistical software\nenvironments. We also present hmclearn, an R package for learning HMC. This\npackage contains a general-purpose HMC function for data analysis. We\nillustrate the use of this package in common statistical models. In doing so,\nwe hope to promote this powerful computational tool for wider use. Example code\nfor common statistical models is presented as supplementary material for online\npublication.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:02:41 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 12:50:36 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Thomas", "Samuel", ""], ["Tu", "Wanzhu", ""]]}, {"id": "2006.16548", "submitter": "Gonzalo Mena E", "authors": "Gonzalo Mena, Amin Nejatbakhsh, Erdem Varol and Jonathan Niles-Weed", "title": "Sinkhorn EM: An Expectation-Maximization algorithm based on entropic\n  optimal transport", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Sinkhorn EM (sEM), a variant of the expectation maximization (EM)\nalgorithm for mixtures based on entropic optimal transport. sEM differs from\nthe classic EM algorithm in the way responsibilities are computed during the\nexpectation step: rather than assign data points to clusters independently, sEM\nuses optimal transport to compute responsibilities by incorporating prior\ninformation about mixing weights. Like EM, sEM has a natural interpretation as\na coordinate ascent procedure, which iteratively constructs and optimizes a\nlower bound on the log-likelihood. However, we show theoretically and\nempirically that sEM has better behavior than EM: it possesses better global\nconvergence guarantees and is less prone to getting stuck in bad local optima.\nWe complement these findings with experiments on simulated data as well as in\nan inference task involving C. elegans neurons and show that sEM learns cell\nlabels significantly better than other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 06:03:37 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Mena", "Gonzalo", ""], ["Nejatbakhsh", "Amin", ""], ["Varol", "Erdem", ""], ["Niles-Weed", "Jonathan", ""]]}, {"id": "2006.16606", "submitter": "Marius Appel", "authors": "Marius Appel and Edzer Pebesma", "title": "Spatiotemporal Multi-Resolution Approximations for Analyzing Global\n  Environmental Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological developments and open data policies have made large, global\nenvironmental datasets accessible to everyone. For analysing such datasets,\nincluding spatiotemporal correlations using traditional models based on\nGaussian processes does not scale with data volume and requires strong\nassumptions about stationarity, separability, and distance measures of\ncovariance functions that are often unrealistic for global data. Only very few\nmodeling approaches suitably model spatiotemporal correlations while addressing\nboth computational scalability as well as flexible covariance models. In this\npaper, we provide an extension to the multi-resolution approximation (MRA)\napproach for spatiotemporal modeling of global datasets. MRA has been shown to\nbe computationally scalable in distributed computing environments and allows\nfor integrating arbitrary user-defined covariance functions. Our extension adds\na spatiotemporal partitioning, and fitting of complex covariance models\nincluding nonstationarity with kernel convolutions and spherical distances. We\nevaluate the effect of the MRA parameters on estimation and spatiotemporal\nprediction using simulated data, where computation times reduced around two\norders of magnitude with an increase of the root-mean-square prediction error\nof around five percent. This allows for trading off computation times against\nprediction errors, and we derive a practical strategy for selecting the MRA\nparameters. We demonstrate how the approach can be practically used for\nanalyzing daily sea surface temperature and precipitation data on global scale\nand compare models with different complexities in the covariance function.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:40:25 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Appel", "Marius", ""], ["Pebesma", "Edzer", ""]]}, {"id": "2006.16653", "submitter": "Kirill Neklyudov", "authors": "Kirill Neklyudov, Max Welling, Evgenii Egorov, Dmitry Vetrov", "title": "Involutive MCMC: a Unifying Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a computational approach to fundamental\nproblems such as inference, integration, optimization, and simulation. The\nfield has developed a broad spectrum of algorithms, varying in the way they are\nmotivated, the way they are applied and how efficiently they sample. Despite\nall the differences, many of them share the same core principle, which we unify\nas the Involutive MCMC (iMCMC) framework. Building upon this, we describe a\nwide range of MCMC algorithms in terms of iMCMC, and formulate a number of\n\"tricks\" which one can use as design principles for developing new MCMC\nalgorithms. Thus, iMCMC provides a unified view of many known MCMC algorithms,\nwhich facilitates the derivation of powerful extensions. We demonstrate the\nlatter with two examples where we transform known reversible MCMC algorithms\ninto more efficient irreversible ones.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 10:21:42 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Neklyudov", "Kirill", ""], ["Welling", "Max", ""], ["Egorov", "Evgenii", ""], ["Vetrov", "Dmitry", ""]]}, {"id": "2006.16761", "submitter": "Andres Colubri", "authors": "Andres Colubri, Kailash Yadav, Abhishek Jha, Pardis C. Sabeti", "title": "Individual-level Modeling of COVID-19 Epidemic Risk", "comments": "16 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1908.06822 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ongoing COVID-19 pandemic calls for a multi-faceted public health\nresponse comprising complementary interventions to control the spread of the\ndisease while vaccines and therapies are developed. Many of these interventions\nneed to be informed by epidemic risk predictions given available data,\nincluding symptoms, contact patterns, and environmental factors. Here we\npropose a novel probabilistic formalism based on Individual-Level Models (ILMs)\nthat offers rigorous formulas for the probability of infection of individuals,\nwhich can be parameterised via Maximum Likelihood Estimation (MLE) applied on\ncompartmental models defined at the population level. We describe an approach\nwhere individual data collected in real-time is integrated with overall case\ncounts to update the a predictor of the susceptibility of infection as a\nfunction of individual risk factors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 14:51:00 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2020 23:54:43 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 06:33:03 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2020 03:34:42 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Colubri", "Andres", ""], ["Yadav", "Kailash", ""], ["Jha", "Abhishek", ""], ["Sabeti", "Pardis C.", ""]]}, {"id": "2006.16901", "submitter": "Marcin Jurek", "authors": "Marcin Jurek and Matthias Katzfuss", "title": "Hierarchical sparse Cholesky decomposition with applications to\n  high-dimensional spatio-temporal filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial statistics often involves Cholesky decomposition of covariance\nmatrices. To ensure scalability to high dimensions, several recent\napproximations have assumed a sparse Cholesky factor of the precision matrix.\nWe propose a hierarchical Vecchia approximation, whose conditional-independence\nassumptions imply sparsity in the Cholesky factors of both the precision and\nthe covariance matrix. This remarkable property is crucial for applications to\nhigh-dimensional spatio-temporal filtering. We present a fast and simple\nalgorithm to compute our hierarchical Vecchia approximation, and we provide\nextensions to non-linear data assimilation with non-Gaussian data based on the\nLaplace approximation. In several numerical comparisons, our methods strongly\noutperformed alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 15:26:10 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Jurek", "Marcin", ""], ["Katzfuss", "Matthias", ""]]}]