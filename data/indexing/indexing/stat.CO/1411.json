[{"id": "1411.0030", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison and Daniel Tarlow and Tom Minka", "title": "A* Sampling", "comments": "V2: - reworded the last paragraph of Section 2 to clarify that the\n  argmax is a sample from the normalized measure. - fixed notation in Algorithm\n  1. - fixed a typo in paragraph 2 of Section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of drawing samples from a discrete distribution can be converted\ninto a discrete optimization problem. In this work, we show how sampling from a\ncontinuous distribution can be converted into an optimization problem over\ncontinuous space. Central to the method is a stochastic process recently\ndescribed in mathematical statistics that we call the Gumbel process. We\npresent a new construction of the Gumbel process and A* sampling, a practical\ngeneric sampling algorithm that searches for the maximum of a Gumbel process\nusing A* search. We analyze the correctness and convergence time of A* sampling\nand demonstrate empirically that it makes more efficient use of bound and\nlikelihood evaluations than the most closely related adaptive rejection\nsampling-based algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 21:40:50 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 13:46:52 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Maddison", "Chris J.", ""], ["Tarlow", "Daniel", ""], ["Minka", "Tom", ""]]}, {"id": "1411.0086", "submitter": "Stefano Castruccio", "authors": "Stefano Castruccio, Rapha\\\"el Huser, Marc Genton", "title": "High-order Composite Likelihood Inference for Max-Stable Distributions\n  and Processes", "comments": "in Journal of Computational and Graphical Statistics, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate or spatial extremes, inference for max-stable processes\nobserved at a large collection of locations is among the most challenging\nproblems in computational statistics, and current approaches typically rely on\nless expensive composite likelihoods constructed from small subsets of data. In\nthis work, we explore the limits of modern state-of-the-art computational\nfacilities to perform full likelihood inference and to efficiently evaluate\nhigh-order composite likelihoods. With extensive simulations, we assess the\nloss of information of composite likelihood estimators with respect to a full\nlikelihood approach for some widely-used multivariate or spatial extreme\nmodels, we discuss how to choose composite likelihood truncation to improve the\nefficiency, and we also provide recommendations for practitioners.\n", "versions": [{"version": "v1", "created": "Sat, 1 Nov 2014 09:17:06 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2015 16:14:15 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2015 13:48:25 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Castruccio", "Stefano", ""], ["Huser", "Rapha\u00ebl", ""], ["Genton", "Marc", ""]]}, {"id": "1411.0306", "submitter": "Ahmed El Alaoui", "authors": "Ahmed El Alaoui, Michael W. Mahoney", "title": "Fast Randomized Kernel Methods With Statistical Guarantees", "comments": "Improved presentation. Technical details fixed. A conference version\n  of this paper appears in NIPS15 under the modified title \"Fast Randomized\n  Kernel Ridge Regression with Statistical Guarantees\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to improving the running time of kernel-based machine learning\nmethods is to build a small sketch of the input and use it in lieu of the full\nkernel matrix in the machine learning task of interest. Here, we describe a\nversion of this approach that comes with running time guarantees as well as\nimproved guarantees on its statistical performance. By extending the notion of\n\\emph{statistical leverage scores} to the setting of kernel ridge regression,\nour main statistical result is to identify an importance sampling distribution\nthat reduces the size of the sketch (i.e., the required number of columns to be\nsampled) to the \\emph{effective dimensionality} of the problem. This quantity\nis often much smaller than previous bounds that depend on the \\emph{maximal\ndegrees of freedom}. Our main algorithmic result is to present a fast algorithm\nto compute approximations to these scores. This algorithm runs in time that is\nlinear in the number of samples---more precisely, the running time is\n$O(np^2)$, where the parameter $p$ depends only on the trace of the kernel\nmatrix and the regularization parameter---and it can be applied to the matrix\nof feature vectors, without having to form the full kernel matrix. This is\nobtained via a variant of length-squared sampling that we adapt to the kernel\nsetting in a way that is of independent interest. Lastly, we provide empirical\nresults illustrating our theory, and we discuss how this new notion of the\nstatistical leverage of a data point captures in a fine way the difficulty of\nthe original statistical learning problem.\n", "versions": [{"version": "v1", "created": "Sun, 2 Nov 2014 19:57:31 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2015 02:20:23 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2015 20:33:45 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Alaoui", "Ahmed El", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1411.0416", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer, Leonhard Held, Michael H\\\"ohle", "title": "Spatio-Temporal Analysis of Epidemic Phenomena Using the R Package\n  surveillance", "comments": "53 pages, 20 figures, package homepage:\n  http://surveillance.r-forge.r-project.org/", "journal-ref": "Journal of Statistical Software (2017); 77 (11): 1-55", "doi": "10.18637/jss.v077.i11", "report-no": null, "categories": "stat.CO cs.CE physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The availability of geocoded health data and the inherent temporal structure\nof communicable diseases have led to an increased interest in statistical\nmodels and software for spatio-temporal data with epidemic features. The open\nsource R package surveillance can handle various levels of aggregation at which\ninfective events have been recorded: individual-level time-stamped\ngeo-referenced data (case reports) in either continuous space or discrete\nspace, as well as counts aggregated by period and region. For each of these\ndata types, the surveillance package implements tools for visualization,\nlikelihoood inference and simulation from recently developed statistical\nregression frameworks capturing endemic and epidemic dynamics. Altogether, this\npaper is a guide to the spatio-temporal modeling of epidemic phenomena,\nexemplified by analyses of public health surveillance data on measles and\ninvasive meningococcal disease.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 10:25:14 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 03:27:52 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Meyer", "Sebastian", ""], ["Held", "Leonhard", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "1411.0560", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang, Antonio Punzo, Paul D. McNicholas, Salvatore\n  Ingrassia, and Ryan P. Browne", "title": "Multivariate response and parsimony for Gaussian cluster-weighted models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious Gaussian cluster-weighted models is presented. This\nfamily concerns a multivariate extension to cluster-weighted modelling that can\naccount for correlations between multivariate responses. Parsimony is attained\nby constraining parts of an eigen-decomposition imposed on the component\ncovariance matrices. A sufficient condition for identifiability is provided and\nan expectation-maximization algorithm is presented for parameter estimation.\nModel performance is investigated on both synthetic and classical real data\nsets and compared with some popular approaches. Finally, accounting for linear\ndependencies in the presence of a linear regression structure is shown to offer\nbetter performance, vis-\\`{a}-vis clustering, over existing methodologies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 16:57:39 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 15:05:55 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Ingrassia", "Salvatore", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1411.0606", "submitter": "Luca Scrucca", "authors": "Luca Scrucca (Universit\\`a degli Studi di Perugia) and Adrian E.\n  Raftery (University of Washington)", "title": "clustvarsel: A Package Implementing Variable Selection for Model-based\n  Clustering in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture modelling provides a framework for cluster analysis based on\nparsimonious Gaussian mixture models. Variable or feature selection is of\nparticular importance in situations where only a subset of the available\nvariables provide clustering information. This enables the selection of a more\nparsimonious model, yielding more efficient estimates, a clearer interpretation\nand, often, improved clustering partitions. This paper describes the R package\nclustvarsel which performs subset selection for model-based clustering. An\nimproved version of the methodology of Raftery and Dean (2006) is implemented\nin the new version 2 of the package to find the (locally) optimal subset of\nvariables with group/cluster information in a dataset. Search over the solution\nspace is performed using either a stepwise greedy search or a headlong\nalgorithm. Adjustments for speeding up these algorithms are discussed, as well\nas a parallel implementation of the stepwise search. Usage of the package is\npresented through the discussion of several data examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Nov 2014 18:52:52 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Scrucca", "Luca", "", "Universit\u00e0 degli Studi di Perugia"], ["Raftery", "Adrian E.", "", "University of Washington"]]}, {"id": "1411.1285", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner, Luigi Boccuto, Markus G\\\"oker", "title": "Controlling false discoveries in high-dimensional situations: Boosting\n  with stability selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biotechnologies often result in high-dimensional data sets with much\nmore variables than observations (n $\\ll$ p). These data sets pose new\nchallenges to statistical analysis: Variable selection becomes one of the most\nimportant tasks in this setting. We assess the recently proposed flexible\nframework for variable selection called stability selection. By the use of\nresampling procedures, stability selection adds a finite sample error control\nto high-dimensional variable selection procedures such as Lasso or boosting. We\nconsider the combination of boosting and stability selection and present\nresults from a detailed simulation study that provides insights into the\nusefulness of this combination. Limitations are discussed and guidance on the\nspecification and tuning of stability selection is given. The interpretation of\nthe used error bounds is elaborated and insights for practical data analysis\nare given. The results will be used to detect differentially expressed\nphenotype measurements in patients with autism spectrum disorders. All methods\nare implemented in the freely available R package stabs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 14:47:56 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Hofner", "Benjamin", ""], ["Boccuto", "Luigi", ""], ["G\u00f6ker", "Markus", ""]]}, {"id": "1411.1292", "submitter": "Ma\\\"elle Salmon", "authors": "Salmon Ma\\\"elle, Schumacher Dirk, H\\\"ohle Michael", "title": "Monitoring Count Time Series in R: Aberration Detection in Public Health\n  Surveillance", "comments": null, "journal-ref": null, "doi": "10.18637/jss.v070.i10", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public health surveillance aims at lessening disease burden, e.g., in case of\ninfectious diseases by timely recognizing emerging outbreaks. Seen from a\nstatistical perspective, this implies the use of appropriate methods for\nmonitoring time series of aggregated case reports. This paper presents the\ntools for such automatic aberration detection offered by the R package\nsurveillance. We introduce the functionality for the visualization, modelling\nand monitoring of surveillance time series. With respect to modelling we focus\non univariate time series modelling based on generalized linear models (GLMs),\nmultivariate GLMs, generalized additive models and generalized additive models\nfor location, shape and scale. This ranges from illustrating implementational\nimprovements and extensions of the well-known Farrington algorithm, e.g, by\nspline-modelling or by treating it in a Bayesian context. Furthermore, we look\nat categorical time series and address overdispersion using beta-binomial or\nDirichlet-Multinomial modelling. With respect to monitoring we consider\ndetectors based on either a Shewhart-like single timepoint comparison between\nthe observed count and the predictive distribution or by likelihood-ratio based\ncumulative sum methods. Finally, we illustrate how surveillance can support\naberration detection in practice by integrating it into the monitoring workflow\nof a public health institution. Altogether, the present article shows how well\nsurveillance can support automatic aberration detection in a public health\nsurveillance context.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 15:10:19 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Ma\u00eblle", "Salmon", ""], ["Dirk", "Schumacher", ""], ["Michael", "H\u00f6hle", ""]]}, {"id": "1411.1314", "submitter": "James Ridgway", "authors": "James Ridgway", "title": "Computation of Gaussian orthant probabilities in high dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computation of Gaussian orthant probabilities, i.e. the\nprobability that a Gaussian falls inside a quadrant. The\nGeweke-Hajivassiliou-Keane (GHK) algorithm [Genz, 1992; Geweke, 1991;\nHajivassiliou et al., 1996; Keane, 1993], is currently used for integrals of\ndimension greater than 10. In this paper we show that for Markovian covariances\nGHK can be interpreted as the estimator of the normalizing constant of a state\nspace model using sequential importance sampling (SIS). We show for an AR(1)\nthe variance of the GHK, properly normalized, diverges exponentially fast with\nthe dimension. As an improvement we propose using a particle filter (PF). We\nthen generalize this idea to arbitrary covariance matrices using Sequential\nMonte Carlo (SMC) with properly tailored MCMC moves. We show empirically that\nthis can lead to drastic improvements on currently used algorithms. We also\nextend the framework to orthants of mixture of Gaussians (Student, Cauchy\netc.), and to the simulation of truncated Gaussians.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 16:27:42 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 20:30:47 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Ridgway", "James", ""]]}, {"id": "1411.1451", "submitter": "Scott Sisson", "authors": "Robert Erhardt and Scott A. Sisson", "title": "Modelling extremes using approximate Bayesian Computation", "comments": "To appear in Extreme Value Modelling and Risk Analysis: Methods and\n  Applications. Eds. D. Dey and J. Yan. Chapman & Hall/CRC Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By the nature of their construction, many statistical models for extremes\nresult in likelihood functions that are computationally prohibitive to\nevaluate. This is consequently problematic for the purposes of likelihood-based\ninference. With a focus on the Bayesian framework, this chapter examines the\nuse of approximate Bayesian computation (ABC) techniques for the fitting and\nanalysis of statistical models for extremes. After introducing the ideas behind\nABC algorithms and methods, we demonstrate their application to extremal models\nin stereology and spatial extremes.\n", "versions": [{"version": "v1", "created": "Wed, 5 Nov 2014 23:45:52 GMT"}], "update_date": "2014-11-07", "authors_parsed": [["Erhardt", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1411.1552", "submitter": "Minho Park", "authors": "M. Park and M.V. Tretyakov", "title": "A Block Circulant Embedding Method for Simulation of Stationary Gaussian\n  Random Fields on Block-regular Grids", "comments": "[17 pages, 8 figures] We added Remarks 2.1, 3.1, 3.2, and Example 1.3\n  and removed the Appendix which is now summarized in Remark 2.1", "journal-ref": "Int. J. Uncertainty Quantification, V. 5, No. 6 (2015), pp.\n  527-544", "doi": "10.1615/Int.J.UncertaintyQuantification.2015013781", "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for sampling from stationary Gaussian random field on\na grid which is not regular but has a regular block structure which is often\nthe case in applications. The introduced block circulant embedding method\n(BCEM) can outperform the classical circulant embedding method (CEM) which\nrequires a regularization of the irregular grid before its application.\nComparison of BCEM vs CEM is performed on some typical model problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 10:18:43 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 10:09:24 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Park", "M.", ""], ["Tretyakov", "M. V.", ""]]}, {"id": "1411.1702", "submitter": "Andrea Arnold", "authors": "Andrea Arnold, Daniela Calvetti and Erkki Somersalo", "title": "Vectorized and Parallel Particle Filter SMC Parameter Estimation for\n  Stiff ODEs", "comments": "12 pages, 2 figures", "journal-ref": "AIMS Proceedings (2015) 75-84", "doi": "10.3934/proc.2015.0075", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filter (PF) sequential Monte Carlo (SMC) methods are very attractive\nfor the estimation of parameters of time dependent systems where the data is\neither not all available at once, or the range of time constants is wide enough\nto create problems in the numerical time propagation of the states. The need to\nevolve a large number of particles makes PF-based methods computationally\nchallenging, the main bottlenecks being the time propagation of each particle\nand the large number of particles. While parallelization is typically advocated\nto speed up the computing time, vectorization of the algorithm on a single\nprocessor may result in even larger speedups for certain problems. In this\npaper we present a formulation of the PF-SMC class of algorithms proposed in\nArnold et al. (2013), which is particularly amenable to a parallel or\nvectorized computing environment, and we illustrate the performance with a few\ncomputed examples in MATLAB.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 19:04:22 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Arnold", "Andrea", ""], ["Calvetti", "Daniela", ""], ["Somersalo", "Erkki", ""]]}, {"id": "1411.1830", "submitter": "Fabrizio Lecci", "authors": "Brittany Terese Fasy, Jisu Kim, Fabrizio Lecci, Cl\\'ement Maria", "title": "Introduction to the R package TDA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS cs.CG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a short tutorial and introduction to using the R package TDA,\nwhich provides some tools for Topological Data Analysis. In particular, it\nincludes implementations of functions that, given some data, provide\ntopological information about the underlying space, such as the distance\nfunction, the distance to a measure, the kNN density estimator, the kernel\ndensity estimator, and the kernel distance. The salient topological features of\nthe sublevel sets (or superlevel sets) of these functions can be quantified\nwith persistent homology. We provide an R interface for the efficient\nalgorithms of the C++ libraries GUDHI, Dionysus and PHAT, including a function\nfor the persistent homology of the Rips filtration, and one for the persistent\nhomology of sublevel sets (or superlevel sets) of arbitrary functions evaluated\nover a grid of points. The significance of the features in the resulting\npersistence diagrams can be analyzed with functions that implement recently\ndeveloped statistical methods. The R package TDA also includes the\nimplementation of an algorithm for density clustering, which allows us to\nidentify the spatial organization of the probability mass associated to a\ndensity function and visualize it by means of a dendrogram, the cluster tree.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 05:10:34 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2015 17:21:36 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Fasy", "Brittany Terese", ""], ["Kim", "Jisu", ""], ["Lecci", "Fabrizio", ""], ["Maria", "Cl\u00e9ment", ""]]}, {"id": "1411.1869", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "Efficient estimation of high-dimensional multivariate normal copula\n  models with discrete spatial responses", "comments": "arXiv admin note: text overlap with arXiv:1304.0905", "journal-ref": "Stochastic Environmental Research and Risk Assessment, 2016,\n  30(2):493--505", "doi": "10.1007/s00477-015-1060-2", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distributional transform (DT) is amongst the computational methods used\nfor estimation of high-dimensional multivariate normal copula models with\ndiscrete responses. Its advantage is that the likelihood can be derived\nconveniently under the theory for copula models with continuous margins, but\nthere has not been a clear analysis of the adequacy of this method. We\ninvestigate the small-sample and asymptotic efficiency of the method for\nestimating high-dimensional multivariate normal copula models with univariate\nBernoulli, Poisson, and negative binomial margins, and show that the DT\napproximation leads to biased estimates when there is more discretisation. For\na high-dimensional discrete response, we implement a maximum simulated\nlikelihood method, which is based on evaluating the multidimensional integrals\nof the likelihood with randomized quasi Monte Carlo methods. Efficiency\ncalculations show that our method is nearly as efficient as maximum likelihood\nfor fully specified high-dimensional multivariate normal copula models. Both\nmethods are illustrated with spatially aggregated count data sets, and it is\nshown that there is a substantial gain on efficiency via the maximum simulated\nlikelihood method.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 09:48:51 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 19:21:24 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1411.1993", "submitter": "Rosemary Braun", "authors": "Rosemary Braun and Sahil Shah", "title": "Network Methods for Pathway Analysis of Genomic Data", "comments": "Review article", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid advances in high-throughput technologies have led to considerable\ninterest in analyzing genome-scale data in the context of biological pathways,\nwith the goal of identifying functional systems that are involved in a given\nphenotype. In the most common approaches, biological pathways are modeled as\nsimple sets of genes, neglecting the network of interactions comprising the\npathway and treating all genes as equally important to the pathway's function.\nRecently, a number of new methods have been proposed to integrate pathway\ntopology in the analyses, harnessing existing knowledge and enabling more\nnuanced models of complex biological systems. However, there is little guidance\navailable to researches choosing between these methods. In this review, we\ndiscuss eight topology-based methods, comparing their methodological approaches\nand appropriate use cases. In addition, we present the results of the\napplication of these methods to a curated set of ten gene expression profiling\nstudies using a common set of pathway annotations. We report the computational\nefficiency of the methods and the consistency of the results across methods and\nstudies to help guide users in choosing a method. We also discuss the\nchallenges and future outlook for improved network analysis methodologies.\n", "versions": [{"version": "v1", "created": "Fri, 7 Nov 2014 17:33:53 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Braun", "Rosemary", ""], ["Shah", "Sahil", ""]]}, {"id": "1411.2547", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Statistical distributions of sequencing by synthesis with probabilistic\n  nucleotide incorporation", "comments": "25 pages, 2 figures", "journal-ref": "Journal of Computational Biology. June 2009, 16(6): 817-827", "doi": "10.1089/cmb.2008.0215", "report-no": null, "categories": "q-bio.GN cs.DM math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequencing by synthesis is used in many next-generation DNA sequencing\ntechnologies. Some of the technologies, especially those exploring the\nprinciple of single-molecule sequencing, allow incomplete nucleotide\nincorporation in each cycle. We derive statistical distributions for sequencing\nby synthesis by taking into account the possibility that nucleotide\nincorporation may not be complete in each flow cycle. The statistical\ndistributions are expressed in terms of nucleotide probabilities of the target\nsequences and the nucleotide incorporation probabilities for each nucleotide.\nWe give exact distributions both for fixed number of flow cycles and for fixed\nsequence length. Explicit formulas are derived for the mean and variance of\nthese distributions. The results are generalizations of our previous work for\npyrosequencing. Incomplete nucleotide incorporation leads to significant change\nin the mean and variance of the distributions, but still they can be\napproximated by normal distributions with the same mean and variance. The\nresults are also generalized to handle sequence context dependent\nincorporation. The statistical distributions will be useful for instrument and\nsoftware development for sequencing by synthesis platforms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:06:46 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1411.2548", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Length distribution of sequencing by synthesis: fixed flow cycle model", "comments": "27 pages, 5 figures", "journal-ref": "Journal of mathematical biology 67 (2), 389-410, 2013", "doi": "10.1007/s00285-012-0556-3", "report-no": null, "categories": "q-bio.GN cs.DM math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequencing by synthesis is the underlying technology for many next-generation\nDNA sequencing platforms. We developed a new model, the fixed flow cycle model,\nto derive the distributions of sequence length for a given number of flow\ncycles under the general conditions where the nucleotide incorporation is\nprobabilistic and may be incomplete, as in some single-molecule sequencing\ntechnologies. Unlike the previous model, the new model yields the probability\ndistribution for the sequence length. Explicit closed form formulas are derived\nfor the mean and variance of the distribution.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:04:16 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1411.2549", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Distributions of positive signals in pyrosequencing", "comments": "19 pages, 2 figures", "journal-ref": "Journal of mathematical biology 69 (1), 39-54, 2014", "doi": "10.1007/s00285-013-0691-5", "report-no": null, "categories": "q-bio.GN cs.DM math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyrosequencing is one of the important next-generation sequencing\ntechnologies. We derive the distribution of the number of positive signals in\npyrograms of this sequencing technology as a function of flow cycle numbers and\nnucleotide probabilities of the target sequences. As for the distribution of\nsequence length, we also derive the distribution of positive signals for the\nfixed flow cycle model. Explicit formulas are derived for the mean and variance\nof the distributions. A simple result for the mean of the distribution is that\nthe mean number of positive signals in a pyrogram is approximately twice the\nnumber of flow cycles, regardless of nucleotide probabilities. The statistical\ndistributions will be useful for instrument and software development for\npyrosequencing and other related platforms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:18:19 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1411.2624", "submitter": "Theodore  Kypraios", "authors": "Edward S. Knock and Theodore Kypraios", "title": "Bayesian Non-Parametric Inference for Infectious Disease Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Bayesian non-parametric estimation of the rate at\nwhich new infections occur assuming that the epidemic is partially observed.\nThe developed methodology relies on modelling the rate at which new infections\noccur as a function which only depends on time. Two different types of prior\ndistributions are proposed namely using step-functions and B-splines. The\nmethodology is illustrated using both simulated and real datasets and we show\nthat certain aspects of the epidemic such as seasonality and super-spreading\nevents are picked up without having to explicitly incorporate them into a\nparametric model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Nov 2014 21:26:10 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 10:10:42 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Knock", "Edward S.", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1411.2668", "submitter": "Yong Kong", "authors": "Yong Kong", "title": "Statistical distributions of pyrosequencing", "comments": "26 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1411.2547, arXiv:1411.2549", "journal-ref": "Journal of Computational Biology 16 (1), 31-42, 2009", "doi": "10.1089/cmb.2008.0106", "report-no": null, "categories": "q-bio.GN cs.DM math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pyrosequencing is emerging as one of the important next-generation sequencing\ntechnologies. We derive the statistical distributions of this technique in\nterms of nucleotide probabilities of the target sequences. We give exact\ndistributions both for fixed number of flow cycles and for fixed sequence\nlength. Explicit formulas are derived for the mean and variance of these\ndistributions. In both cases, the distributions can be approximated accurately\nby normal distributions with the same mean and variance. The statistical\ndistributions will be useful for instrument and software development for\npyrosequencing platforms.\n", "versions": [{"version": "v1", "created": "Thu, 6 Nov 2014 03:06:09 GMT"}], "update_date": "2014-11-12", "authors_parsed": [["Kong", "Yong", ""]]}, {"id": "1411.3013", "submitter": "Kevin H. Knuth", "authors": "Kevin H. Knuth, Michael Habeck, Nabin K. Malakar, Asim M. Mubeen, Ben\n  Placek", "title": "Bayesian Evidence and Model Selection", "comments": "Arxiv version consists of 58 pages and 9 figures. Features theory,\n  numerical methods and four applications", "journal-ref": "Digital Signal Processing, 47:50-67 (2015)", "doi": "10.1016/j.dsp.2015.06.012", "report-no": null, "categories": "stat.ME astro-ph.IM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we review the concepts of Bayesian evidence and Bayes factors,\nalso known as log odds ratios, and their application to model selection. The\ntheory is presented along with a discussion of analytic, approximate and\nnumerical techniques. Specific attention is paid to the Laplace approximation,\nvariational Bayes, importance sampling, thermodynamic integration, and nested\nsampling and its recent variants. Analogies to statistical physics, from which\nmany of these techniques originate, are discussed in order to provide readers\nwith deeper insights that may lead to new techniques. The utility of Bayesian\nmodel testing in the domain sciences is demonstrated by presenting four\nspecific practical examples considered within the context of signal processing\nin the areas of signal detection, sensor characterization, scientific model\nselection and molecular force characterization.\n", "versions": [{"version": "v1", "created": "Tue, 11 Nov 2014 23:08:54 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:13:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Knuth", "Kevin H.", ""], ["Habeck", "Michael", ""], ["Malakar", "Nabin K.", ""], ["Mubeen", "Asim M.", ""], ["Placek", "Ben", ""]]}, {"id": "1411.3688", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Kody J.H. Law, Youssef M. Marzouk", "title": "Dimension-independent likelihood-informed MCMC", "comments": null, "journal-ref": "Journal of Computational Physics, 304, 109-137 (2016)", "doi": "10.1016/j.jcp.2015.10.008", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Bayesian inference problems require exploring the posterior distribution\nof high-dimensional parameters that represent the discretization of an\nunderlying function. This work introduces a family of Markov chain Monte Carlo\n(MCMC) samplers that can adapt to the particular structure of a posterior\ndistribution over functions. Two distinct lines of research intersect in the\nmethods developed here. First, we introduce a general class of\noperator-weighted proposal distributions that are well defined on function\nspace, such that the performance of the resulting MCMC samplers is independent\nof the discretization of the function. Second, by exploiting local Hessian\ninformation and any associated low-dimensional structure in the change from\nprior to posterior distributions, we develop an inhomogeneous discretization\nscheme for the Langevin stochastic differential equation that yields\noperator-weighted proposals adapted to the non-Gaussian structure of the\nposterior. The resulting dimension-independent, likelihood-informed (DILI) MCMC\nsamplers may be useful for a large class of high-dimensional problems where the\ntarget probability measure has a density with respect to a Gaussian reference\nmeasure. Two nonlinear inverse problems are used to demonstrate the efficiency\nof these DILI samplers: an elliptic PDE coefficient inverse problem and path\nreconstruction in a conditioned diffusion.\n", "versions": [{"version": "v1", "created": "Thu, 13 Nov 2014 20:00:24 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2015 05:11:55 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Law", "Kody J. H.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1411.3921", "submitter": "Brendon Brewer", "authors": "Brendon J. Brewer", "title": "Inference for Trans-dimensional Bayesian Models with Diffusive Nested\n  Sampling", "comments": "Only published here for the time being. 17 pages, 10 figures.\n  Software available at https://github.com/eggplantbren/RJObject", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems involve inferring the number $N$ of components in\nsome region, along with their properties $\\{\\mathbf{x}_i\\}_{i=1}^N$, from a\ndataset $\\mathcal{D}$. A common statistical example is finite mixture\nmodelling. In the Bayesian framework, these problems are typically solved using\none of the following two methods: i) by executing a Monte Carlo algorithm (such\nas Nested Sampling) once for each possible value of $N$, and calculating the\nmarginal likelihood or evidence as a function of $N$; or ii) by doing a single\nrun that allows the model dimension $N$ to change (such as Markov Chain Monte\nCarlo with birth/death moves), and obtaining the posterior for $N$ directly. In\nthis paper we present a general approach to this problem that uses\ntrans-dimensional MCMC embedded within a Nested Sampling algorithm, allowing us\nto explore the posterior distribution and calculate the marginal likelihood\n(summed over $N$) even if the problem contains a phase transition or other\ndifficult features such as multimodality. We present two example problems,\nfinding sinusoidal signals in noisy data, and finding and measuring galaxies in\na noisy astronomical image. Both of the examples demonstrate phase transitions\nin the relationship between the likelihood and the cumulative prior mass,\nhighlighting the need for Nested Sampling.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 14:40:54 GMT"}, {"version": "v2", "created": "Mon, 17 Nov 2014 03:06:47 GMT"}, {"version": "v3", "created": "Wed, 14 Jan 2015 20:31:53 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Brewer", "Brendon J.", ""]]}, {"id": "1411.3954", "submitter": "Art Owen", "authors": "Hera Y. He and Art B. Owen", "title": "Optimal mixture weights in multiple importance sampling", "comments": "23 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiple importance sampling we combine samples from a finite list of\nproposal distributions. When those proposal distributions are used to create\ncontrol variates, it is possible (Owen and Zhou, 2000) to bound the ratio of\nthe resulting variance to that of the unknown best proposal distribution in our\nlist. The minimax regret arises by taking a uniform mixture of proposals, but\nthat is conservative when there are many components. In this paper we optimize\nthe mixture component sampling rates to gain further efficiency. We show that\nthe sampling variance of mixture importance sampling with control variates is\njointly convex in the mixture probabilities and control variate regression\ncoefficients. We also give a sequential importance sampling algorithm to\nestimate the optimal mixture from the sample data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Nov 2014 16:21:37 GMT"}], "update_date": "2014-11-18", "authors_parsed": [["He", "Hera Y.", ""], ["Owen", "Art B.", ""]]}, {"id": "1411.4134", "submitter": "Federico G. Poloni", "authors": "Federico Poloni and Giacomo Sbrana", "title": "A note on forecasting demand using the multivariate exponential\n  smoothing framework", "comments": null, "journal-ref": "Int. J. of Prod. Econ., volume 162, 2015", "doi": "10.1016/j.ijpe.2015.01.017", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple exponential smoothing is widely used in forecasting economic time\nseries. This is because it is quick to compute and it generally delivers\naccurate forecasts. On the other hand, its multivariate version has received\nlittle attention due to the complications arising with the estimation. Indeed,\nstandard multivariate maximum likelihood methods are affected by numerical\nconvergence issues and bad complexity, growing with the dimensionality of the\nmodel. In this paper, we introduce a new estimation strategy for multivariate\nexponential smoothing, based on aggregating its observations into scalar models\nand estimating them. The original high-dimensional maximum likelihood problem\nis broken down into several univariate ones, which are easier to solve.\nContrary to the multivariate maximum likelihood approach, the suggested\nalgorithm does not suffer heavily from the dimensionality of the model. The\nmethod can be used for time series forecasting. In addition, simulation results\nshow that our approach performs at least as well as a maximum likelihood\nestimator on the underlying VMA(1) representation, at least in our test\nproblems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Nov 2014 10:40:31 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Poloni", "Federico", ""], ["Sbrana", "Giacomo", ""]]}, {"id": "1411.4257", "submitter": "Riccardo Rastelli", "authors": "Marco Bertoletti, Nial Friel, Riccardo Rastelli", "title": "Choosing the number of clusters in a finite mixture model using an exact\n  Integrated Completed Likelihood criterion", "comments": "23 pages, to appear in Metron", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integrated completed likelihood (ICL) criterion has proven to be a very\npopular approach in model-based clustering through automatically choosing the\nnumber of clusters in a mixture model. This approach effectively maximises the\ncomplete data likelihood, thereby including the allocation of observations to\nclusters in the model selection criterion. However for practical implementation\none needs to introduce an approximation in order to estimate the ICL. Our\ncontribution here is to illustrate that through the use of conjugate priors one\ncan derive an exact expression for ICL and so avoiding any approximation.\nMoreover, we illustrate how one can find both the number of clusters and the\nbest allocation of observations in one algorithmic framework. The performance\nof our algorithm is presented on several simulated and real examples.\n", "versions": [{"version": "v1", "created": "Sun, 16 Nov 2014 13:27:20 GMT"}, {"version": "v2", "created": "Mon, 25 May 2015 17:10:37 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Bertoletti", "Marco", ""], ["Friel", "Nial", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1411.4598", "submitter": "Yiyuan She", "authors": "Yiyuan She, Yuejia He, Shijie Li, Dapeng Wu", "title": "Joint Association Graph Screening and Decomposition for Large-scale\n  Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2373315", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies large-scale dynamical networks where the current state of\nthe system is a linear transformation of the previous state, contaminated by a\nmultivariate Gaussian noise. Examples include stock markets, human brains and\ngene regulatory networks. We introduce a transition matrix to describe the\nevolution, which can be translated to a directed Granger transition graph, and\nuse the concentration matrix of the Gaussian noise to capture the second-order\nrelations between nodes, which can be translated to an undirected conditional\ndependence graph. We propose regularizing the two graphs jointly in topology\nidentification and dynamics estimation. Based on the notion of joint\nassociation graph (JAG), we develop a joint graphical screening and estimation\n(JGSE) framework for efficient network learning in big data. In particular, our\nmethod can pre-determine and remove unnecessary edges based on the joint\ngraphical structure, referred to as JAG screening, and can decompose a large\nnetwork into smaller subnetworks in a robust manner, referred to as JAG\ndecomposition. JAG screening and decomposition can reduce the problem size and\nsearch space for fine estimation at a later stage. Experiments on both\nsynthetic data and real-world applications show the effectiveness of the\nproposed framework in large-scale network topology identification and dynamics\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 19:21:27 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["She", "Yiyuan", ""], ["He", "Yuejia", ""], ["Li", "Shijie", ""], ["Wu", "Dapeng", ""]]}, {"id": "1411.4691", "submitter": "Yiyuan She", "authors": "Yiyuan She, Zhifeng Wang and He Jiang", "title": "Group Regularized Estimation under Structural Hierarchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection for models including interactions between explanatory\nvariables often needs to obey certain hierarchical constraints. The weak or\nstrong structural hierarchy requires that the existence of an interaction term\nimplies at least one or both associated main effects to be present in the\nmodel. Lately, this problem has attracted a lot of attention, but existing\ncomputational algorithms converge slow even with a moderate number of\npredictors. Moreover, in contrast to the rich literature on ordinary variable\nselection, there is a lack of statistical theory to show reasonably low error\nrates of hierarchical variable selection.\n  This work investigates a new class of estimators that make use of multiple\ngroup penalties to capture structural parsimony. We give the minimax lower\nbounds for strong and weak hierarchical variable selection and show that the\nproposed estimators enjoy sharp rate oracle inequalities. A general-purpose\nalgorithm is developed with guaranteed convergence and global optimality.\nSimulations and real data experiments demonstrate the efficiency and efficacy\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Nov 2014 23:03:11 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 03:51:40 GMT"}, {"version": "v3", "created": "Tue, 8 Nov 2016 21:00:25 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["She", "Yiyuan", ""], ["Wang", "Zhifeng", ""], ["Jiang", "He", ""]]}, {"id": "1411.4911", "submitter": "Chavent Marie", "authors": "Marie Chavent, Vanessa Kuentz-Simonet, Amaury Labenne and J\\'er\\^ome\n  Saracco", "title": "Multivariate Analysis of Mixed Data: The R Package PCAmixdata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed data arise when observations are described by a mixture of numerical\nand categorical variables. The R package PCAmixdata extends standard\nmultivariate analysis methods to incorporate this type of data. The key\ntechniques/methods included in the package are principal component analysis for\nmixed data (PCAmix), varimax-like orthogonal rotation for PCAmix, and multiple\nfactor analysis for mixed multi-table data. This paper gives a synthetic\npresentation of the three algorithms with details to help the user understand\ngraphical and numerical outputs of the corresponding R functions. The three\nmain methods are illustrated on a real dataset composed of four data tables\ncharacterizing living conditions in different municipalities in the Gironde\nregion of southwest France.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 17:02:06 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 17:42:21 GMT"}, {"version": "v3", "created": "Thu, 4 Dec 2014 09:21:30 GMT"}, {"version": "v4", "created": "Fri, 8 Dec 2017 17:54:35 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Chavent", "Marie", ""], ["Kuentz-Simonet", "Vanessa", ""], ["Labenne", "Amaury", ""], ["Saracco", "J\u00e9r\u00f4me", ""]]}, {"id": "1411.4944", "submitter": "Matteo Ruggiero", "authors": "Omiros Papaspiliopoulos, Matteo Ruggiero and Dario Span\\`o", "title": "Filtering hidden Markov measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning two families of time-evolving random\nmeasures from indirect observations. In the first model, the signal is a\nFleming--Viot diffusion, which is reversible with respect to the law of a\nDirichlet process, and the data is a sequence of random samples from the state\nat discrete times. In the second model, the signal is a Dawson--Watanabe\ndiffusion, which is reversible with respect to the law of a gamma random\nmeasure, and the data is a sequence of Poisson point configurations whose\nintensity is given by the state at discrete times. A common methodology is\ndeveloped to obtain the filtering distributions in a computable form, which is\nbased on the projective properties of the signals and duality properties of\ntheir projections. The filtering distributions take the form of mixtures of\nDirichlet processes and gamma random measures for each of the two families\nrespectively, and an explicit algorithm is provided to compute the parameters\nof the mixtures. Hence, our results extend classic characterisations of the\nposterior distribution under Dirichlet process and gamma random measures priors\nto a dynamic framework.\n", "versions": [{"version": "v1", "created": "Tue, 18 Nov 2014 18:06:41 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Ruggiero", "Matteo", ""], ["Span\u00f2", "Dario", ""]]}, {"id": "1411.6144", "submitter": "Wesley Tansey", "authors": "Wesley Tansey and Oluwasanmi Koyejo and Russell A. Poldrack and James\n  G. Scott", "title": "False discovery rate smoothing", "comments": "Added misspecification analysis, added pathological scenario\n  discussions, additional comparisons, new graph fused lasso algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present false discovery rate smoothing, an empirical-Bayes method for\nexploiting spatial structure in large multiple-testing problems. FDR smoothing\nautomatically finds spatially localized regions of significant test statistics.\nIt then relaxes the threshold of statistical significance within these regions,\nand tightens it elsewhere, in a manner that controls the overall\nfalse-discovery rate at a given level. This results in increased power and\ncleaner spatial separation of signals from noise. The approach requires solving\na non-standard high-dimensional optimization problem, for which an efficient\naugmented-Lagrangian algorithm is presented. In simulation studies, FDR\nsmoothing exhibits state-of-the-art performance at modest computational cost.\nIn particular, it is shown to be far more robust than existing methods for\nspatially dependent multiple testing. We also apply the method to a data set\nfrom an fMRI experiment on spatial working memory, where it detects patterns\nthat are much more biologically plausible than those detected by standard\nFDR-controlling methods. All code for FDR smoothing is publicly available in\nPython and R.\n", "versions": [{"version": "v1", "created": "Sat, 22 Nov 2014 17:17:46 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 18:32:39 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tansey", "Wesley", ""], ["Koyejo", "Oluwasanmi", ""], ["Poldrack", "Russell A.", ""], ["Scott", "James G.", ""]]}, {"id": "1411.6370", "submitter": "Jun Zhu", "authors": "Jun Zhu, Jianfei Chen, Wenbo Hu, Bo Zhang", "title": "Big Learning with Bayesian Methods", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explosive growth in data and availability of cheap computing resources have\nsparked increasing interest in Big learning, an emerging subfield that studies\nscalable machine learning algorithms, systems, and applications with Big Data.\nBayesian methods represent one important class of statistic methods for machine\nlearning, with substantial recent developments on adaptive, flexible and\nscalable Bayesian learning. This article provides a survey of the recent\nadvances in Big learning with Bayesian methods, termed Big Bayesian Learning,\nincluding nonparametric Bayesian methods for adaptively inferring model\ncomplexity, regularized Bayesian inference for improving the flexibility via\nposterior regularization, and scalable algorithms and systems based on\nstochastic subsampling and distributed computing for dealing with large-scale\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 07:28:51 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 14:07:26 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Zhu", "Jun", ""], ["Chen", "Jianfei", ""], ["Hu", "Wenbo", ""], ["Zhang", "Bo", ""]]}, {"id": "1411.6927", "submitter": "Pavlo Mozharovskyi", "authors": "Rainer Dyckerhoff, Pavlo Mozharovskyi", "title": "Exact computation of the halfspace depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For computing the exact value of the halfspace depth of a point w.r.t. a data\ncloud of $n$ points in arbitrary dimension, a theoretical framework is\nsuggested. Based on this framework a whole class of algorithms can be derived.\nIn all of these algorithms the depth is calculated as the minimum over a finite\nnumber of depth values w.r.t. proper projections of the data cloud. Three\nvariants of this class are studied in more detail. All of these algorithms are\ncapable of dealing with data that are not in general position and even with\ndata that contain ties. As is shown by simulations, all proposed algorithms\nprove to be very efficient.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 17:20:06 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 15:21:03 GMT"}, {"version": "v3", "created": "Tue, 12 Jan 2016 15:31:38 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Dyckerhoff", "Rainer", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1411.7009", "submitter": "Shaan Qamar", "authors": "Shaan Qamar, Surya T. Tokdar", "title": "Additive Gaussian Process Regression", "comments": "28 pages; 9 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive-interactive regression has recently been shown to offer attractive\nminimax error rates over traditional nonparametric multivariate regression in a\nwide variety of settings, including cases where the predictor count is much\nlarger than the sample size and many of the predictors have important effects\non the response, potentially through complex interactions. We present a\nBayesian implementation of additive-interactive regression using an additive\nGaussian process (AGP) prior and develop an efficient Markov chain sampler that\nextends stochastic search variable selection in this setting. Careful prior and\nhyper-parameter specification are developed in light of performance and\ncomputational considerations, and key innovations address difficulties in\nexploring a joint posterior distribution over multiple subsets of high\ndimensional predictor inclusion vectors. The method offers state-of-the-art\nsupport and interaction recovery while improving dramatically over competitors\nin terms of prediction accuracy on a diverse set of simulated and real data.\nResults from real data studies provide strong evidence that the\nadditive-interactive framework is an attractive modeling platform for\nhigh-dimensional nonparametric regression.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:28:28 GMT"}], "update_date": "2014-11-26", "authors_parsed": [["Qamar", "Shaan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1411.7013", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi, Eric C. Chi, Richard G. Baraniuk", "title": "$k$-POD: A Method for $k$-Means Clustering of Missing Data", "comments": "26 pages, 7 tables", "journal-ref": "The American Statistician 70(1):91-99, 2016", "doi": "10.1080/00031305.2015.1086685", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is often used in clustering applications but its\nusage requires a complete data matrix. Missing data, however, is common in many\napplications. Mainstream approaches to clustering missing data reduce the\nmissing data problem to a complete data formulation through either deletion or\nimputation but these solutions may incur significant costs. Our $k$-POD method\npresents a simple extension of $k$-means clustering for missing data that works\neven when the missingness mechanism is unknown, when external information is\nunavailable, and when there is significant missingness in the data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Nov 2014 20:37:58 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2015 21:13:59 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 18:45:25 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Chi", "Eric C.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1411.7888", "submitter": "Theodore  Kypraios", "authors": "Philip D. O'Neill and Theodore Kypraios", "title": "Bayesian model choice via mixture distributions with application to\n  epidemics and population process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method for evaluating Bayes factors. The key idea is to\nintroduce a hypermodel in which the competing models are components of a\nmixture distribution. Inference for the mixing probabilities then yields\nestimates of the Bayes factors. Our motivation is the setting where the\nobserved data are a partially observed realisation of a stochastic population\nprocess, although the methods have far wider applicability. The methods allow\nfor missing data and for parameters to be shared between models. Illustrative\nexamples including epidemics, population processes and regression models are\ngiven, showing that the methods are competitive compared to existing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Nov 2014 14:50:48 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 21:00:42 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["O'Neill", "Philip D.", ""], ["Kypraios", "Theodore", ""]]}]