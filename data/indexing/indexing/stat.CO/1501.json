[{"id": "1501.00179", "submitter": "Peter Bubenik", "authors": "Peter Bubenik and Pawel Dlotko", "title": "A persistence landscapes toolbox for topological statistics", "comments": "24 pages", "journal-ref": "Journal of Symbolic Computation, Volume 78, January-February 2017,\n  Pages 91-114", "doi": "10.1016/j.jsc.2016.03.009", "report-no": null, "categories": "cs.CG cs.MS math.AT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis provides a multiscale description of the geometry\nand topology of quantitative data. The persistence landscape is a topological\nsummary that can be easily combined with tools from statistics and machine\nlearning. We give efficient algorithms for calculating persistence landscapes,\ntheir averages, and distances between such averages. We discuss an\nimplementation of these algorithms and some related procedures. These are\nintended to facilitate the combination of statistics and machine learning with\ntopological data analysis. We present an experiment showing that the\nlow-dimensional persistence landscapes of points sampled from spheres (and\nboxes) of varying dimensions differ.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 17:34:59 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2015 14:58:20 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2015 17:16:40 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Bubenik", "Peter", ""], ["Dlotko", "Pawel", ""]]}, {"id": "1501.00622", "submitter": "Zizhuo Wang", "authors": "Yichen Chen, Dongdong Ge, Mengdi Wang, Zizhuo Wang, Yinyu Ye, Hao Yin", "title": "Strong NP-Hardness for Sparse Optimization with Concave Penalty\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the regularized sparse minimization problem, which involves\nempirical sums of loss functions for $n$ data points (each of dimension $d$)\nand a nonconvex sparsity penalty. We prove that finding an\n$\\mathcal{O}(n^{c_1}d^{c_2})$-optimal solution to the regularized sparse\noptimization problem is strongly NP-hard for any $c_1, c_2\\in [0,1)$ such that\n$c_1+c_2<1$. The result applies to a broad class of loss functions and sparse\npenalty functions. It suggests that one cannot even approximately solve the\nsparse optimization problem in polynomial time, unless P $=$ NP.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 01:38:23 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 16:00:11 GMT"}, {"version": "v3", "created": "Tue, 21 Jun 2016 14:38:24 GMT"}, {"version": "v4", "created": "Mon, 19 Jun 2017 01:46:43 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Chen", "Yichen", ""], ["Ge", "Dongdong", ""], ["Wang", "Mengdi", ""], ["Wang", "Zizhuo", ""], ["Ye", "Yinyu", ""], ["Yin", "Hao", ""]]}, {"id": "1501.00926", "submitter": "Claudio Fantacci", "authors": "C. Fantacci, B.-T. Vo, F. Papi and B.-N. Vo", "title": "The Marginalized $\\delta$-GLMB Filter", "comments": "With the consent of the authors, part of this work has been reworded\n  and published in the IEEE Signal Processing Letters, vol. 23, no. 6, pp.\n  863-867, with the title \"Scalable Multisensor Multitarget Tracking Using the\n  Marginalized \\delta-GLMB Density\". DOI: 10.1109/LSP.2016.2557078. IEEEXplore\n  link: http://ieeexplore.ieee.org/abstract/document/7457284/", "journal-ref": null, "doi": "10.1109/LSP.2016.2557078", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-target Bayes filter proposed by Mahler is a principled solution to\nrecursive Bayesian tracking based on RFS or FISST. The $\\delta$-GLMB filter is\nan exact closed form solution to the multi-target Bayes recursion which yields\njoint state and label or trajectory estimates in the presence of clutter,\nmissed detections and association uncertainty. Due to presence of explicit data\nassociations in the $\\delta$-GLMB filter, the number of components in the\nposterior grows without bound in time. In this work we propose an efficient\napproximation to the $\\delta$-GLMB filter which preserves both the PHD and\ncardinality distribution of the labeled posterior. This approximation also\nfacilitates efficient multi-sensor tracking with detection-based measurements.\nSimulation results are presented to verify the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 17:13:37 GMT"}, {"version": "v2", "created": "Thu, 6 Apr 2017 15:40:04 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Fantacci", "C.", ""], ["Vo", "B. -T.", ""], ["Papi", "F.", ""], ["Vo", "B. -N.", ""]]}, {"id": "1501.01579", "submitter": "Claudio Fantacci", "authors": "C. Fantacci, B.-N. Vo, B.-T. Vo, G. Battistelli and L. Chisci", "title": "Consensus Labeled Random Finite Set Filtering for Distributed\n  Multi-Object Tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses distributed multi-object tracking over a network of\nheterogeneous and geographically dispersed nodes with sensing, communication\nand processing capabilities. The main contribution is an approach to\ndistributed multi-object estimation based on labeled Random Finite Sets (RFSs)\nand dynamic Bayesian inference, which enables the development of two novel\nconsensus tracking filters, namely a Consensus Marginalized\n$\\delta$-Generalized Labeled Multi-Bernoulli and Consensus Labeled\nMulti-Bernoulli tracking filter. The proposed algorithms provide fully\ndistributed, scalable and computationally efficient solutions for multi-object\ntracking. Simulation experiments via Gaussian mixture implementations confirm\nthe effectiveness of the proposed approach on challenging scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 18:08:03 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2016 07:58:59 GMT"}], "update_date": "2016-06-10", "authors_parsed": [["Fantacci", "C.", ""], ["Vo", "B. -N.", ""], ["Vo", "B. -T.", ""], ["Battistelli", "G.", ""], ["Chisci", "L.", ""]]}, {"id": "1501.01613", "submitter": "Benjamin Baumer", "authors": "Dana Udwin and Ben Baumer", "title": "R Markdown", "comments": "16 pages", "journal-ref": "Wiley Interdisciplinary Reviews: Computational Statistics, 7(3),\n  pp. 167-177, 2015", "doi": "10.1002/wics.1348", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducibility is increasingly important to statistical research, but many\ndetails are often omitted from the published version of complex statistical\nanalyses. A reader's comprehension is limited to what the author concludes,\nwithout exposure to the computational process. Often, the industrious reader\ncannot expand upon or validate the author's results. Even the author may\nstruggle to reproduce their own results upon revisiting them. R Markdown is an\nauthoring syntax that combines the ease of Markdown with the statistical\nprogramming language R. An R Markdown document or presentation interweaves\ncomputation, output and written analysis to the effect of transparency, clarity\nand an inherent invitation to reproduce (especially as sharing data is now as\neasy as the click of a button). It is an open-source tool that can be used\neither on its own or through the RStudio integrated development environment\n(IDE). In addition to facilitating reproducible research, R Markdown is a boon\nto collaboratively-minded data analysts, whose workflow can be streamlined by\nsharing only one master document that contains both code and content.\nStatistics educators may also find that R Markdown is helpful as a homework\ntemplate, for both ease-of-use and in discouraging students from\ncopy-and-pasting results from classmates. Training students in R Markdown will\nintroduce to the workforce a new class of data analysts with an ingrained,\nfoundational inclination toward reproducible research.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 20:27:16 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Udwin", "Dana", ""], ["Baumer", "Ben", ""]]}, {"id": "1501.01898", "submitter": "Jia  Liu", "authors": "Jia Liu, Dario Gasbarra, Juha Railavo", "title": "Fast Estimation of Diffusion Tensors under Rician noise by the EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast computational method, the Expectation Maximization\nalgorithm, for Maximum Likelihood (ML) estimation in diffusion tensor imaging\nunder the Rice noise model. We further extend the ML framework to the maximum a\nposterior (MAP) estimation and describe the numerical similarities of both ML\nand MAP estimators. This novel method is implemented and applied using both\nsynthetic and real data in a wide range of b amplitudes. The comparison with\nother popular methods are made in accuracy, methodology and computation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 16:29:16 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Liu", "Jia", ""], ["Gasbarra", "Dario", ""], ["Railavo", "Juha", ""]]}, {"id": "1501.02248", "submitter": "Francesco Papi", "authors": "Francesco Papi and Du Yong Kim", "title": "A Particle Multi-Target Tracker for Superpositional Measurements using\n  Labeled Random Finite Sets", "comments": "arXiv admin note: text overlap with arXiv:1312.2372 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a general solution for multi-target tracking with\nsuperpositional measurements. Measurements that are functions of the sum of the\ncontributions of the targets present in the surveillance area are called\nsuperpositional measurements. We base our modelling on Labeled Random Finite\nSet (RFS) in order to jointly estimate the number of targets and their\ntrajectories. This modelling leads to a labeled version of Mahler's\nmulti-target Bayes filter. However, a straightforward implementation of this\ntracker using Sequential Monte Carlo (SMC) methods is not feasible due to the\ndifficulties of sampling in high dimensional spaces. We propose an efficient\nmulti-target sampling strategy based on Superpositional Approximate CPHD\n(SA-CPHD) filter and the recently introduced Labeled Multi-Bernoulli (LMB) and\nVo-Vo densities. The applicability of the proposed approach is verified through\nsimulation in a challenging radar application with closely spaced targets and\nlow signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 07:32:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 02:42:01 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Papi", "Francesco", ""], ["Kim", "Du Yong", ""]]}, {"id": "1501.02284", "submitter": "Gabriel Becker", "authors": "Gabriel Becker, Cory Barr, Robert Gentleman, Michael Lawrence", "title": "Enhancing reproducibility and collaboration via management of R package\n  cohorts", "comments": "Submitted to the Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Science depends on collaboration, result reproduction, and the development of\nsupporting software tools. Each of these requires careful management of\nsoftware versions. We present a unified model for installing, managing, and\npublishing software contexts in R. It introduces the package manifest as a\ncentral data structure for representing version specific, decentralized package\ncohorts. The manifest points to package sources on arbitrary hosts and in\nvarious forms, including tarballs and directories under version control. We\nprovide a high-level interface for creating and switching between side-by-side\npackage libraries derived from manifests. Finally, we extend package\ninstallation to support the retrieval of exact package versions as indicated by\nmanifests, and to maintain provenance for installed packages. The provenance\ninformation enables the user to publish libraries or sessions as manifests,\nhence completing the loop between publication and deployment. We have\nimplemented this model across two software packages, switchr and GRANbase, and\nhave released the source code under the Artistic 2.0 license.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 22:32:34 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 16:38:31 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Becker", "Gabriel", ""], ["Barr", "Cory", ""], ["Gentleman", "Robert", ""], ["Lawrence", "Michael", ""]]}, {"id": "1501.02627", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "A fast numerical method for max-convolution and the application to\n  efficient max-product inference in Bayesian networks", "comments": null, "journal-ref": "Journal of Computational Biology. August 2015, 22(8): 770-783", "doi": "10.1089/cmb.2015.0013", "report-no": null, "categories": "cs.NA math.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations depending on sums of random variables are common throughout many\nfields; however, no efficient solution is currently known for performing\nmax-product inference on these sums of general discrete distributions\n(max-product inference can be used to obtain maximum a posteriori estimates).\nThe limiting step to max-product inference is the max-convolution problem\n(sometimes presented in log-transformed form and denoted as \"infimal\nconvolution\", \"min-convolution\", or \"convolution on the tropical semiring\"),\nfor which no O(k log(k)) method is currently known. Here I present a O(k\nlog(k)) numerical method for estimating the max-convolution of two nonnegative\nvectors (e.g., two probability mass functions), where k is the length of the\nlarger vector. This numerical max-convolution method is then demonstrated by\nperforming fast max-product inference on a convolution tree, a data structure\nfor performing fast inference given information on the sum of n discrete random\nvariables in O(n k log(n k) log(n) ) steps (where each random variable has an\narbitrary prior distribution on k contiguous possible states). The numerical\nmax-convolution method can be applied to specialized classes of hidden Markov\nmodels to reduce the runtime of computing the Viterbi path from n k^2 to n k\nlog(k), and has potential application to the all-pairs shortest paths problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:57:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 01:49:25 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "1501.03150", "submitter": "Richard Norton", "authors": "Richard A. Norton and Colin Fox", "title": "Efficiency and computability of MCMC with Langevin, Hamiltonian, and\n  other matrix-splitting proposals", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse computational efficiency of Metropolis-Hastings algorithms with\nAR(1) process proposals. These proposals include, as a subclass, discretized\nLangevin diffusion (e.g. MALA) and discretized Hamiltonian dynamics (e.g. HMC).\n  By including the effect of Metropolis-Hastings we extend earlier work by Fox\nand Parker, who used matrix splitting techniques to analyse the performance and\nimprove efficiency of AR(1) processes for targeting Gaussian distributions.\n  Our research enables analysis of MCMC methods that draw samples from\nnon-Gaussian target distributions by using AR(1) process proposals in\nMetropolis-Hastings algorithms, by analysing the matrix splitting of the\nprecision matrix for a local Gaussian approximation of the non-Gaussian target.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 20:57:50 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Norton", "Richard A.", ""], ["Fox", "Colin", ""]]}, {"id": "1501.03291", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann and Jukka Corander", "title": "Bayesian Optimization for Likelihood-Free Inference of Simulator-Based\n  Statistical Models", "comments": "In press with the Journal of Machine Learning Research (JMLR).\n  Accepted August 17, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our paper deals with inferring simulator-based statistical models given some\nobserved data. A simulator-based model is a parametrized mechanism which\nspecifies how data are generated. It is thus also referred to as generative\nmodel. We assume that only a finite number of parameters are of interest and\nallow the generative process to be very general; it may be a noisy nonlinear\ndynamical system with an unrestricted number of hidden variables. This weak\nassumption is useful for devising realistic models but it renders statistical\ninference very difficult. The main challenge is the intractability of the\nlikelihood function. Several likelihood-free inference methods have been\nproposed which share the basic idea of identifying the parameters by finding\nvalues for which the discrepancy between simulated and observed data is small.\nA major obstacle to using these methods is their computational cost. The cost\nis largely due to the need to repeatedly simulate data sets and the lack of\nknowledge about how the parameters affect the discrepancy. We propose a\nstrategy which combines probabilistic modeling of the discrepancy with\noptimization to facilitate likelihood-free inference. The strategy is\nimplemented using Bayesian optimization and is shown to accelerate the\ninference through a reduction in the number of required simulations by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 09:34:15 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 09:37:28 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 15:19:21 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1501.03323", "submitter": "Ihab Sraj", "authors": "Ihab Sraj and Olivier P. Le Ma\\^itre and Omar M. Knio and Ibrahim\n  Hoteit", "title": "Coordinate Transformation and Polynomial Chaos for the Bayesian\n  Inference of a Gaussian Process with Parametrized Prior Covariance Function", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses model dimensionality reduction for Bayesian inference\nbased on prior Gaussian fields with uncertainty in the covariance function\nhyper-parameters. The dimensionality reduction is traditionally achieved using\nthe Karhunen-\\Loeve expansion of a prior Gaussian process assuming covariance\nfunction with fixed hyper-parameters, despite the fact that these are uncertain\nin nature. The posterior distribution of the Karhunen-Lo\\`{e}ve coordinates is\nthen inferred using available observations. The resulting inferred field is\ntherefore dependent on the assumed hyper-parameters. Here, we seek to\nefficiently estimate both the field and covariance hyper-parameters using\nBayesian inference. To this end, a generalized Karhunen-Lo\\`{e}ve expansion is\nderived using a coordinate transformation to account for the dependence with\nrespect to the covariance hyper-parameters. Polynomial Chaos expansions are\nemployed for the acceleration of the Bayesian inference using similar\ncoordinate transformations, enabling us to avoid expanding explicitly the\nsolution dependence on the uncertain hyper-parameters. We demonstrate the\nfeasibility of the proposed method on a transient diffusion equation by\ninferring spatially-varying log-diffusivity fields from noisy data. The\ninferred profiles were found closer to the true profiles when including the\nhyper-parameters' uncertainty in the inference formulation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 11:52:42 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2015 18:37:37 GMT"}], "update_date": "2015-10-09", "authors_parsed": [["Sraj", "Ihab", ""], ["Ma\u00eetre", "Olivier P. Le", ""], ["Knio", "Omar M.", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1501.03379", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Mark Girolami", "title": "Control Functionals for Quasi-Monte Carlo Integration", "comments": "To appear at AISTATS 2016 (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) methods are being adopted in statistical applications\ndue to the increasingly challenging nature of numerical integrals that are now\nroutinely encountered. For integrands with $d$-dimensions and derivatives of\norder $\\alpha$, an optimal QMC rule converges at a best-possible rate\n$O(N^{-\\alpha/d})$. However, in applications the value of $\\alpha$ can be\nunknown and/or a rate-optimal QMC rule can be unavailable. Standard practice is\nto employ $\\alpha_L$-optimal QMC where the lower bound $\\alpha_L \\leq \\alpha$\nis known, but in general this does not exploit the full power of QMC. One\nsolution is to trade-off numerical integration with functional approximation.\nThis strategy is explored herein and shown to be well-suited to modern\nstatistical computation. A challenging application to robotic arm data\ndemonstrates a substantial variance reduction in predictions for mechanical\ntorques.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 15:54:54 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 16:54:19 GMT"}, {"version": "v3", "created": "Sat, 31 Jan 2015 16:23:29 GMT"}, {"version": "v4", "created": "Thu, 4 Jun 2015 01:34:43 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2015 00:59:20 GMT"}, {"version": "v6", "created": "Tue, 8 Dec 2015 14:33:28 GMT"}, {"version": "v7", "created": "Fri, 1 Apr 2016 04:05:57 GMT"}], "update_date": "2016-04-04", "authors_parsed": [["Oates", "Chris. J.", ""], ["Girolami", "Mark", ""]]}, {"id": "1501.03386", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Daniel Simpson, Mark Girolami", "title": "Discussion of \"Sequential Quasi-Monte Carlo\" by Mathieu Gerber and\n  Nicolas Chopin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A discussion on the possibility of reducing the variance of quasi-Monte Carlo\nestimators in applications. Further details are provided in the accompanying\npaper \"Variance Reduction for Quasi-Monte Carlo\".\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 16:04:05 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Oates", "Chris. J.", ""], ["Simpson", "Daniel", ""], ["Girolami", "Mark", ""]]}, {"id": "1501.03659", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (IMSV), Julien Bect (GdR MASCOT-NUM, L2S), Cl\\'ement\n  Chevalier (UNINE), David Ginsbourger (Idiap, IMSV)", "title": "Quantifying uncertainties on excursion sets under a Gaussian random\n  field prior", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, 4(1):850-874, 2016", "doi": "10.1137/141000749", "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of estimating and quantifying uncertainties on the\nexcursion set of a function under a limited evaluation budget. We adopt a\nBayesian approach where the objective function is assumed to be a realization\nof a Gaussian random field. In this setting, the posterior distribution on the\nobjective function gives rise to a posterior distribution on excursion sets.\nSeveral approaches exist to summarize the distribution of such sets based on\nrandom closed set theory. While the recently proposed Vorob'ev approach\nexploits analytical formulae, further notions of variability require Monte\nCarlo estimators relying on Gaussian random field conditional simulations. In\nthe present work we propose a method to choose Monte Carlo simulation points\nand obtain quasi-realizations of the conditional field at fine designs through\naffine predictors. The points are chosen optimally in the sense that they\nminimize the posterior expected distance in measure between the excursion set\nand its reconstruction. The proposed method reduces the computational costs due\nto Monte Carlo simulations and enables the computation of quasi-realizations on\nfine designs in large dimensions. We apply this reconstruction approach to\nobtain realizations of an excursion set on a fine grid which allow us to give a\nnew measure of uncertainty based on the distance transform of the excursion\nset. Finally we present a safety engineering test case where the simulation\nmethod is employed to compute a Monte Carlo estimate of a contour line.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 12:58:28 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2016 11:24:30 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Azzimonti", "Dario", "", "IMSV"], ["Bect", "Julien", "", "GdR MASCOT-NUM, L2S"], ["Chevalier", "Cl\u00e9ment", "", "UNINE"], ["Ginsbourger", "David", "", "Idiap, IMSV"]]}, {"id": "1501.04222", "submitter": "Salvador Garc\\'ia", "authors": "J. Derrac, S. Garc\\'ia, F. Herrera", "title": "JavaNPST: Nonparametric Statistical Tests in Java", "comments": "19 pages, 1 figure. Statistical Software Library for JAVA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Nonparametric statistical tests are useful procedures that can be applied in\na wide range of situations, such as testing randomness or goodness of fit,\none-sample, two-sample and multiple-sample analysis, association between\nbivariate samples or count data analysis. Their use is often preferred to\nparametric tests due to the fact that they require less restrictive assumptions\nabout the population sampled.\n  In this work, JavaNPST, an open source Java library implementing 40\nnonparametric statistical tests, is presented. It can be helpful for\nprogrammers and practitioners interested in performing nonparametric\nstatistical analyses, providing a quick and easy way of running these tests\ndirectly within any Java code. Some examples of use are also shown,\nhighlighting some of the more remarkable capabilities of the library.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 18:46:52 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Derrac", "J.", ""], ["Garc\u00eda", "S.", ""], ["Herrera", "F.", ""]]}, {"id": "1501.04448", "submitter": "Silvia Pandolfi Dr", "authors": "Francesco Bartolucci, Alessio Farcomeni, Silvia Pandolfi, and Fulvia\n  Pennoni", "title": "LMest: an R package for latent Markov models for categorical\n  longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Markov (LM) models represent an important class of models for the\nanalysis of longitudinal data (Bartolucci et. al., 2013), especially when\nresponse variables are categorical. These models have a great potential of\napplication for the analysis of social, medical, and behavioral data as well as\nin other disciplines. We propose the R package LMest, which is tailored to deal\nwith these types of model. In particular, we consider a general framework for\nextended LM models by including individual covariates and by formulating a\nmixed approach to take into account additional dependence structures in the\ndata. Such extensions lead to a very flexible class of models, which allows us\nto fit different types of longitudinal data. Model parameters are estimated\nthrough the expectation-maximization algorithm, based on the forward-backward\nrecursions, which is implemented in the main functions of the package. The\npackage also allows us to perform local and global decoding and to obtain\nstandard errors for the parameter estimates. We illustrate its use and the most\nimportant features on the basis of examples involving applications in health\nand criminology.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 10:47:37 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Farcomeni", "Alessio", ""], ["Pandolfi", "Silvia", ""], ["Pennoni", "Fulvia", ""]]}, {"id": "1501.04870", "submitter": "Luca Martino", "authors": "J. Read, L. Martino, P. Olmos, D. Luengo", "title": "Scalable Multi-Output Label Prediction: From Classifier Chains to\n  Classifier Trellises", "comments": "(accepted in Pattern Recognition)", "journal-ref": "Pattern Recognition, Volume 48, Issue 6, 2015, Pages 2096-2109", "doi": "10.1016/j.patcog.2015.01.004", "report-no": null, "categories": "stat.ML cs.CV cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-output inference tasks, such as multi-label classification, have become\nincreasingly important in recent years. A popular method for multi-label\nclassification is classifier chains, in which the predictions of individual\nclassifiers are cascaded along a chain, thus taking into account inter-label\ndependencies and improving the overall performance. Several varieties of\nclassifier chain methods have been introduced, and many of them perform very\ncompetitively across a wide range of benchmark datasets. However, scalability\nlimitations become apparent on larger datasets when modeling a fully-cascaded\nchain. In particular, the methods' strategies for discovering and modeling a\ngood chain structure constitutes a mayor computational bottleneck. In this\npaper, we present the classifier trellis (CT) method for scalable multi-label\nclassification. We compare CT with several recently proposed classifier chain\nmethods to show that it occupies an important niche: it is highly competitive\non standard multi-label problems, yet it can also scale up to thousands or even\ntens of thousands of labels.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 16:33:40 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Read", "J.", ""], ["Martino", "L.", ""], ["Olmos", "P.", ""], ["Luengo", "D.", ""]]}, {"id": "1501.05119", "submitter": "Li Yin", "authors": "Xiaoqin Wang, Weimin Ye and Li Yin", "title": "Measuring and estimating interaction between exposures on dichotomous\n  outcome of a population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies for the interaction between exposures on dichotomous\noutcome of a population, one usually uses one parameter of a regression model\nto describe the interaction, leading to one measure of the interaction. In this\narticle, we use the conditional risk of outcome given exposures and covariates\nto describe the interaction and obtain five different measures for the\ninteraction in observational studies, i.e. difference between the marginal risk\ndifferences, ratio of the marginal risk ratios, ratio of the marginal odds\nratios, ratio of the conditional risk ratios, and ratio of the conditional odds\nratios. By using only one regression model for the conditional risk of outcome\ngiven exposures and covariates, we obtain the maximum-likelihood estimates of\nall these measures. By generating approximate distributions of the\nmaximum-likelihood estimates of these measures, we obtain interval estimates of\nthese measures. The method is presented by studying the interaction between a\ntherapy and the environment on eradication of Helicobacter pylori among\nVietnamese children.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 10:45:44 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Wang", "Xiaoqin", ""], ["Ye", "Weimin", ""], ["Yin", "Li", ""]]}, {"id": "1501.05128", "submitter": "Li Yin", "authors": "Li Yin and Xiaoqin Wang", "title": "Estimating confidence regions of common measures of (baseline, treatment\n  effect) on dichotomous outcome of a population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we estimate confidence regions of the common measures of\n(baseline, treatment effect) in observational studies, where the measure of\nbaseline is baseline risk or baseline odds while the measure of treatment\neffect is odds ratio, risk difference, risk ratio or attributable fraction, and\nwhere confounding is controlled in estimation of both baseline and treatment\neffect. To avoid high complexity of the normal approximation method and the\nparametric or non-parametric bootstrap method, we obtain confidence regions for\nmeasures of (baseline, treatment effect) by generating approximate\ndistributions of the ML estimates of these measures based on one logistic\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:07:45 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Yin", "Li", ""], ["Wang", "Xiaoqin", ""]]}, {"id": "1501.05144", "submitter": "Dennis Prangle", "authors": "Dennis Prangle", "title": "Lazier ABC", "comments": "Presented as contributed paper at \"ABC in Montreal\" NIPS workshop in\n  December 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ABC algorithms involve a large number of simulations from the model of\ninterest, which can be very computationally costly. This paper summarises the\nlazy ABC algorithm of Prangle (2015), which reduces the computational demand by\nabandoning many unpromising simulations before completion. By using a random\nstopping decision and reweighting the output sample appropriately, the target\ndistribution is the same as for standard ABC. Lazy ABC is also extended here to\nthe case of non-uniform ABC kernels, which is shown to simplify the process of\ntuning the algorithm effectively.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 11:48:52 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Prangle", "Dennis", ""]]}, {"id": "1501.05242", "submitter": "Bertrand Iooss", "authors": "Micha\\\"el Baudin, Anne Dutfoy (EDF R&D), Bertrand Iooss (M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques), Anne-Laure\n  Popelin", "title": "Open TURNS: An industrial software for uncertainty quantification in\n  simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The needs to assess robust performances for complex systems and to answer\ntighter regulatory processes (security, safety, environmental control, and\nhealth impacts, etc.) have led to the emergence of a new industrial simulation\nchallenge: to take uncertainties into account when dealing with complex\nnumerical simulation frameworks. Therefore, a generic methodology has emerged\nfrom the joint effort of several industrial companies and academic\ninstitutions. EDF R&D, Airbus Group and Phimeca Engineering started a\ncollaboration at the beginning of 2005, joined by IMACS in 2014, for the\ndevelopment of an Open Source software platform dedicated to uncertainty\npropagation by probabilistic methods, named OpenTURNS for Open source Treatment\nof Uncertainty, Risk 'N Statistics. OpenTURNS addresses the specific industrial\nchallenges attached to uncertainties, which are transparency, genericity,\nmodularity and multi-accessibility. This paper focuses on OpenTURNS and\npresents its main features: openTURNS is an open source software under the LGPL\nlicense, that presents itself as a C++ library and a Python TUI, and which\nworks under Linux and Windows environment. All the methodological tools are\ndescribed in the different sections of this paper: uncertainty quantification,\nuncertainty propagation, sensitivity analysis and metamodeling. A section also\nexplains the generic wrappers way to link openTURNS to any external code. The\npaper illustrates as much as possible the methodological tools on an\neducational example that simulates the height of a river and compares it to the\nheight of a dyke that protects industrial facilities. At last, it gives an\noverview of the main developments planned for the next few years.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 17:38:33 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 08:28:36 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Baudin", "Micha\u00ebl", "", "EDF R&D"], ["Dutfoy", "Anne", "", "EDF R&D"], ["Iooss", "Bertrand", "", "M\u00e9thodes\n  d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"], ["Popelin", "Anne-Laure", ""]]}, {"id": "1501.05427", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Raphael Engler", "title": "Enabling scalable stochastic gradient-based inference for Gaussian\n  processes by employing the Unbiased LInear System SolvEr (ULISSE)", "comments": "10 pages - paper accepted at ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of Gaussian processes where quantification of uncertainty is\nof primary interest, it is necessary to accurately characterize the posterior\ndistribution over covariance parameters. This paper proposes an adaptation of\nthe Stochastic Gradient Langevin Dynamics algorithm to draw samples from the\nposterior distribution over covariance parameters with negligible bias and\nwithout the need to compute the marginal likelihood. In Gaussian process\nregression, this has the enormous advantage that stochastic gradients can be\ncomputed by solving linear systems only. A novel unbiased linear systems solver\nbased on parallelizable covariance matrix-vector products is developed to\naccelerate the unbiased estimation of gradients. The results demonstrate the\npossibility to enable scalable and exact (in a Monte Carlo sense)\nquantification of uncertainty in Gaussian processes without imposing any\nspecial structure on the covariance or reducing the number of input vectors.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 08:48:42 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 07:59:49 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 09:21:57 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2015 06:43:32 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Filippone", "Maurizio", ""], ["Engler", "Raphael", ""]]}, {"id": "1501.05478", "submitter": "Francesco Pauli Ph.D.", "authors": "Leonardo Egidi, Roberta Pappad\\`a, Francesco Pauli, Nicola Torelli", "title": "Relabelling in Bayesian mixture models by pivotal units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a simple procedure to deal with label switching when exploring\ncomplex posterior distributions by MCMC algorithms is proposed. Although it\ncannot be generalized to any situation, it may be handy in many applications\nbecause of its simplicity and very low computational burden. A possible area\nwhere it proves to be useful is when deriving a sample for the posterior\ndistribution arising from finite mixture models when no simple or rational\nordering between the components is available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 12:45:32 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 17:06:05 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Egidi", "Leonardo", ""], ["Pappad\u00e0", "Roberta", ""], ["Pauli", "Francesco", ""], ["Torelli", "Nicola", ""]]}, {"id": "1501.05876", "submitter": "Kaniav Kamary", "authors": "Halaleh Kamari and Hossein Bevrani and Kaniav Kamary", "title": "Bayesian estimation of discrete Burr distribution with two parameters", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  So far, various techniques have been implemented for generating discrete\ndistributions based on continuous distributions. The characteristics and\nproperties of this kind of probability distributions have been studied.\nFurthermore, the estimation of related parameters have been computed trough\nclassical methods. However, a few studies addressed the parameter estimate\nissue of these distributions through Bayesian methods. This is essentially\nbecause of the complexity of the model whatever the number of parameter is and\nthe fact that in general they contain a large number of parameters to be\nestimated. This paper deals with computing Bayes estimate of the parameters of\ndiscrete Burr distribution with two parameters. Since the resulting posterior\ndistribution of the parameters is not standard, we apply Metropolis-Hastings\nalgorithm to simulate from the posterior density.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 17:08:39 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 20:37:17 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 12:01:15 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Kamari", "Halaleh", ""], ["Bevrani", "Hossein", ""], ["Kamary", "Kaniav", ""]]}, {"id": "1501.06066", "submitter": "Boxiang Wang", "authors": "Boxiang Wang and Hui Zou", "title": "Sparse Distance Weighted Discrimination", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance weighted discrimination (DWD) was originally proposed to handle the\ndata piling issue in the support vector machine. In this paper, we consider the\nsparse penalized DWD for high-dimensional classification. The state-of-the-art\nalgorithm for solving the standard DWD is based on second-order cone\nprogramming, however such an algorithm does not work well for the sparse\npenalized DWD with high-dimensional data. In order to overcome the challenging\ncomputation difficulty, we develop a very efficient algorithm to compute the\nsolution path of the sparse DWD at a given fine grid of regularization\nparameters. We implement the algorithm in a publicly available R package sdwd.\nWe conduct extensive numerical experiments to demonstrate the computational\nefficiency and classification performance of our method.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jan 2015 17:34:36 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Wang", "Boxiang", ""], ["Zou", "Hui", ""]]}, {"id": "1501.06111", "submitter": "Alireza Mahani", "authors": "Alireza S. Mahani, Mansour T.A. Sharabiani", "title": "Expander Framework for Generating High-Dimensional GLM Gradient and\n  Hessian from Low-Dimensional Base Distributions: R Package RegressionFactory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package RegressionFactory provides expander functions for constructing\nthe high-dimensional gradient vector and Hessian matrix of the log-likelihood\nfunction for generalized linear models (GLMs), from the lower-dimensional\nbase-distribution derivatives. The software follows a modular implementation\nusing the chain rule of derivatives. Such modularity offers a clear separation\nof case-specific components (base distribution functional form and link\nfunctions) from common steps (e.g., matrix algebra operations needed for\nexpansion) in calculating log-likelihood derivatives. In doing so,\nRegressionFactory offers several advantages: 1) It provides a fast and\nconvenient method for constructing log-likelihood and its derivatives by\nrequiring only the low-dimensional, base-distribution derivatives, 2) The\naccompanying definiteness-invariance theorem allows researchers to reason about\nthe negative-definiteness of the log-likelihood Hessian in the much\nlower-dimensional space of the base distributions, 3) The factorized, abstract\nview of regression suggests opportunities to generate novel regression models,\nand 4) Computational techniques for performance optimization can be developed\ngenerically in the abstract framework and be readily applicable across all the\nspecific regression instances. We expect RegressionFactory to facilitate\nresearch and development on optimization and sampling techniques for GLM\nlog-likelihoods as well as construction of composite models from GLM lego\nblocks, such as Hierarchical Bayesian models.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 04:09:22 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Mahani", "Alireza S.", ""], ["Sharabiani", "Mansour T. A.", ""]]}, {"id": "1501.06117", "submitter": "Morteza Amini", "authors": "Morteza Amini and Mahdi Mahdizadeh", "title": "Nonparametric estimation of the entropy using a ranked set sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with non-parametric estimation of the entropy in\nranked set sampling. Theoretical properties of the proposed estimator are\nstudied. The proposed estimator is compared with the rival estimator in simple\nrandom sampling. The applications of the proposed estimator to the mutual\ninformation estimation as well as estimation of the Kullback-Leibler divergence\nare provided. Several Monte-Carlo simulation studies are conducted to examine\nthe performance of the estimator. The results are applied to the long-leaf pine\n(pinus palustris) trees and the body fat percentage data sets to illustrate\napplicability of theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 05:22:10 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 08:40:21 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2015 09:30:58 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Amini", "Morteza", ""], ["Mahdizadeh", "Mahdi", ""]]}, {"id": "1501.06195", "submitter": "Yun Yang", "authors": "Yun Yang, Mert Pilanci, Martin J. Wainwright", "title": "Randomized sketches for kernels: Fast and optimal non-parametric\n  regression", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression (KRR) is a standard method for performing\nnon-parametric regression over reproducing kernel Hilbert spaces. Given $n$\nsamples, the time and space complexity of computing the KRR estimate scale as\n$\\mathcal{O}(n^3)$ and $\\mathcal{O}(n^2)$ respectively, and so is prohibitive\nin many cases. We propose approximations of KRR based on $m$-dimensional\nrandomized sketches of the kernel matrix, and study how small the projection\ndimension $m$ can be chosen while still preserving minimax optimality of the\napproximate KRR estimate. For various classes of randomized sketches, including\nthose based on Gaussian and randomized Hadamard matrices, we prove that it\nsuffices to choose the sketch dimension $m$ proportional to the statistical\ndimension (modulo logarithmic factors). Thus, we obtain fast and minimax\noptimal approximations to the KRR estimate for non-parametric regression.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 19:06:59 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Yang", "Yun", ""], ["Pilanci", "Mert", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1501.07039", "submitter": "Hideyuki Suzuki", "authors": "Hideyuki Suzuki", "title": "Chaotic Boltzmann machines with two elements", "comments": "5 pages, 1 figure", "journal-ref": "Seisan Kenkyu 66 (2014), 315-316 (in Japanese)", "doi": "10.11188/seisankenkyu.66.315", "report-no": null, "categories": "nlin.CD cond-mat.stat-mech math.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief note, we show that chaotic Boltzmann machines truly yield\nsamples from the probabilistic distribution of the corresponding Boltzmann\nmachines if they are composed of only two elements. This note is an English\ntranslation (with slight modifications) of the article originally written in\nJapanese [H. Suzuki, Seisan Kenkyu 66 (2014), 315-316].\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 09:20:45 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Suzuki", "Hideyuki", ""]]}, {"id": "1501.07196", "submitter": "John Ehrlinger", "authors": "John Ehrlinger", "title": "ggRandomForests: Visually Exploring a Random Forest for Regression", "comments": "30 page, 16 figures. R Package vignette for ggRandomForests package\n  (http://cran.r-project.org/package=ggRandomForests) [Document Version 2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forests [Breiman:2001] (RF) are a fully non-parametric statistical\nmethod requiring no distributional assumptions on covariate relation to the\nresponse. RF are a robust, nonlinear technique that optimizes predictive\naccuracy by fitting an ensemble of trees to stabilize model estimates. The\nrandomForestSRC package (http://cran.r-project.org/package=randomForestSRC) is\na unified treatment of Breiman's random forests for survival, regression and\nclassification problems. Predictive accuracy make RF an attractive alternative\nto parametric models, though complexity and interpretability of the forest\nhinder wider application of the method. We introduce the ggRandomForests\npackage (http://cran.r-project.org/package=ggRandomForests), for visually\nunderstand random forest models grown in R with the randomForestSRC package.\n  The vignette is a tutorial for using the ggRandomForests package with the\nrandomForestSRC package for building and post-processing a regression random\nforest. In this tutorial, we explore a random forest model for the Boston\nHousing Data, available in the MASS package. We grow a random forest for\nregression and demonstrate how ggRandomForests can be used when determining\nvariable associations, interactions and how the response depends on predictive\nvariables within the model. The tutorial demonstrates the design and usage of\nmany of ggRandomForests functions and features how to modify and customize the\nresulting ggplot2 graphic objects along the way.\n  A development version of the ggRandomForests package is available on Github.\nWe invite comments, feature requests and bug reports for this package at\n(https://github.com/ehrlinger/ggRandomForests).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 17:03:26 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 21:34:05 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ehrlinger", "John", ""]]}, {"id": "1501.07414", "submitter": "H{\\aa}kon Tjelmeland", "authors": "Haakon Michael Austad, H{\\aa}kon Tjelmeland", "title": "Approximations and bounds for binary Markov random fields", "comments": "33 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete Markov random fields form a natural class of models to represent\nimages and spatial data sets. The use of such models is, however, hampered by a\ncomputationally intractable normalising constant. This makes parameter\nestimation and a fully Bayesian treatment of discrete Markov random fields\ndifficult. We apply approximation theory for pseudo-Boolean functions to binary\nMarkov random fields and construct approximations and upper and lower bounds\nfor the associated computationally intractable normalising constant. As a\nby-product of this process we also get a partially ordered Markov model\napproximation of the binary Markov random field. We present numerical examples\nwith both the pairwise interaction Ising model and with higher-order\ninteraction models, showing the quality of our approximations and bounds. We\nalso present simulation examples and one real data example demonstrating how\nthe approximations and bounds can be applied for parameter estimation and to\nhandle a fully-Bayesian model computationally.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 11:12:18 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 08:13:47 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Austad", "Haakon Michael", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "1501.07454", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Adaptive step size selection for Hessian-based manifold Langevin\n  samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of positive definite metric tensors derived from second derivative\ninformation in the context of the simplified manifold Metropolis adjusted\nLangevin algorithm (MALA) is explored. A new adaptive step length procedure\nthat resolves the shortcomings of such metric tensors in regions where the\nlog-target has near zero curvature in some direction is proposed. The adaptive\nstep length selection also appears to alleviate the need for different tuning\nparameters in transient and stationary regimes that is typical of MALA. The\ncombination of metric tensors derived from second derivative information and\nadaptive step length selection constitute a large step towards developing\nreliable manifold MCMC methods that can be implemented automatically for models\nwith unknown or intractable Fisher information, and even for target\ndistributions that do not admit factorization into prior and likelihood.\nThrough examples of low to moderate dimension, it is shown that proposed\nmethodology performs very well relative to alternative MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 13:49:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 19:03:15 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 09:38:01 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}]