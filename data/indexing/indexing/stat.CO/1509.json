[{"id": "1509.00172", "submitter": "Andrew Golightly", "authors": "Chris Sherlock, Andrew Golightly, Daniel A. Henderson", "title": "Adaptive, delayed-acceptance MCMC for targets with expensive likelihoods", "comments": "50 pages (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting Bayesian inference, delayed acceptance (DA)\nMetropolis-Hastings (MH) algorithms and DA pseudo-marginal MH algorithms can be\napplied when it is computationally expensive to calculate the true posterior or\nan unbiased estimate thereof, but a computationally cheap approximation is\navailable. A first accept-reject stage is applied, with the cheap approximation\nsubstituted for the true posterior in the MH acceptance ratio. Only for those\nproposals which pass through the first stage is the computationally expensive\ntrue posterior (or unbiased estimate thereof) evaluated, with a second\naccept-reject stage ensuring that detailed balance is satisfied with respect to\nthe intended true posterior. In some scenarios there is no obvious\ncomputationally cheap approximation. A weighted average of previous evaluations\nof the computationally expensive posterior provides a generic approximation to\nthe posterior. If only the $k$-nearest neighbours have non-zero weights then\nevaluation of the approximate posterior can be made computationally cheap\nprovided that the points at which the posterior has been evaluated are stored\nin a multi-dimensional binary tree, known as a KD-tree. The contents of the\nKD-tree are potentially updated after every computationally intensive\nevaluation. The resulting adaptive, delayed-acceptance [pseudo-marginal]\nMetropolis-Hastings algorithm is justified both theoretically and empirically.\nGuidance on tuning parameters is provided and the methodology is applied to a\ndiscretely observed Markov jump process characterising predator-prey\ninteractions and an ODE system describing the dynamics of an autoregulatory\ngene network.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 08:17:27 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2016 14:28:49 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Sherlock", "Chris", ""], ["Golightly", "Andrew", ""], ["Henderson", "Daniel A.", ""]]}, {"id": "1509.00349", "submitter": "Alfredo Garbuno-Inigo", "authors": "A. Garbuno-Inigo, F.A. DiazDelaO, K.M. Zuev", "title": "Transitional annealed adaptive slice sampling for Gaussian process\n  hyper-parameter estimation", "comments": "This work is a continuation of our research on arXiv:1506.08010", "journal-ref": null, "doi": "10.1615/Int.J.UncertaintyQuantification.2016018590", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate models have become ubiquitous in science and engineering for their\ncapability of emulating expensive computer codes, necessary to model and\ninvestigate complex phenomena. Bayesian emulators based on Gaussian processes\nadequately quantify the uncertainty that results from the cost of the original\nsimulator, and thus the inability to evaluate it on the whole input space.\nHowever, it is common in the literature that only a partial Bayesian analysis\nis carried out, whereby the underlying hyper-parameters are estimated via\ngradient-free optimisation or genetic algorithms, to name a few methods. On the\nother hand, maximum a posteriori (MAP) estimation could discard important\nregions of the hyper-parameter space. In this paper, we carry out a more\ncomplete Bayesian inference, that combines Slice Sampling with some recently\ndeveloped Sequential Monte Carlo samplers. The resulting algorithm improves the\nmixing in the sampling through delayed-rejection, the inclusion of an annealing\nscheme akin to Asymptotically Independent Markov Sampling and parallelisation\nvia Transitional Markov Chain Monte Carlo. Examples related to the estimation\nof Gaussian process hyper-parameters are presented. For the purpose of\nreproducibility, further development, and use in other applications, the code\nto generate the examples in this paper is freely available for download at\nhttp://github.com/agarbuno/ta2s2_codes\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 15:23:47 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 14:26:59 GMT"}, {"version": "v3", "created": "Thu, 31 Mar 2016 14:04:58 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 14:30:01 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Garbuno-Inigo", "A.", ""], ["DiazDelaO", "F. A.", ""], ["Zuev", "K. M.", ""]]}, {"id": "1509.00394", "submitter": "Anthony Lee", "authors": "Anthony Lee and Nick Whiteley", "title": "Variance estimation in the particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns numerical assessment of Monte Carlo error in particle\nfilters. We show that by keeping track of certain key features of the\ngenealogical structure arising from resampling operations, it is possible to\nestimate variances of a number of standard Monte Carlo approximations which\nparticle filters deliver. All our estimators can be computed from a single run\nof a particle filter with no further simulation. We establish that as the\nnumber of particles grows, our estimators are weakly consistent for asymptotic\nvariances of the Monte Carlo approximations and some of them are also\nnon-asymptotically unbiased. The asymptotic variances can be decomposed into\nterms corresponding to each time step of the algorithm, and we show how to\nconsistently estimate each of these terms. When the number of particles may\nvary over time, this allows approximation of the asymptotically optimal\nallocation of particle numbers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Sep 2015 17:15:44 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2016 16:06:38 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Lee", "Anthony", ""], ["Whiteley", "Nick", ""]]}, {"id": "1509.00602", "submitter": "Safae Laqrichi", "authors": "S Laqrichi, Didier Gourc, Fran\\c{c}ois Marmier", "title": "Toward an effort estimation model for software projects integrating risk", "comments": "in 22nd International Conference on Production Research, 2013,\n  Iguassu Falls, Brazil. 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to a study of The Standish Group International, 44% of software\nprojects cost more and last longer than expected. More accurate the effort\nestimation is; the better the enterprise gets organized and the more the\nsoftware project respects the commitments on budget, time and quality.\nEnhancing the accuracy of effort estimation remains an ongoing challenge to\nsoftware professionals. Several factors can influence the accuracy of effort\nestimation, namely the immaterial aspect of information system projects, new\ntechnologies and the lack of return on experience. However, the most important\nfactor of cost and delay increase is software risks. A software risk is an\nuncertain event with a negative consequence on the software project. In this\narticle, we propose a methodology to take into account risk exposure analysis\nin the effort estimation model. In the literature, this issue is little\naddressed and few approaches are investigated. In this research work, we first\npresent an overview of these approaches and their limits. Then, we propose an\neffort estimation model that improves the accuracy of estimation by integrating\nsoftware risks. We finally apply this model to a case study and compare its\nresults to the results of a classic model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 08:50:16 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Laqrichi", "S", ""], ["Gourc", "Didier", ""], ["Marmier", "Fran\u00e7ois", ""]]}, {"id": "1509.00660", "submitter": "Kasper Kristensen", "authors": "Kasper Kristensen, Anders Nielsen, Casper W. Berg, Hans Skaug, Brad\n  Bell", "title": "TMB: Automatic Differentiation and Laplace Approximation", "comments": null, "journal-ref": "J.Stat.Softw. 70 (2016) 1-21", "doi": "10.18637/jss.v070.i05", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TMB is an open source R package that enables quick implementation of complex\nnonlinear random effect (latent variable) models in a manner similar to the\nestablished AD Model Builder package (ADMB, admb-project.org). In addition, it\noffers easy access to parallel computations. The user defines the joint\nlikelihood for the data and the random effects as a C++ template function,\nwhile all the other operations are done in R; e.g., reading in the data. The\npackage evaluates and maximizes the Laplace approximation of the marginal\nlikelihood where the random effects are automatically integrated out. This\napproximation, and its derivatives, are obtained using automatic\ndifferentiation (up to order three) of the joint likelihood. The computations\nare designed to be fast for problems with many random effects (~10^6) and\nparameters (~10^3). Computation times using ADMB and TMB are compared on a\nsuite of examples ranging from simple models to large spatial models where the\nrandom effects are a Gaussian random field. Speedups ranging from 1.5 to about\n100 are obtained with increasing gains for large problems. The package and\nexamples are available at http://tmb-project.org.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 12:24:51 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Kristensen", "Kasper", ""], ["Nielsen", "Anders", ""], ["Berg", "Casper W.", ""], ["Skaug", "Hans", ""], ["Bell", "Brad", ""]]}, {"id": "1509.00727", "submitter": "Anupama Nandi", "authors": "Joseph Anderson, Navin Goyal, Anupama Nandi, Luis Rademacher", "title": "Heavy-tailed Independent Component Analysis", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is the problem of efficiently recovering\na matrix $A \\in \\mathbb{R}^{n\\times n}$ from i.i.d. observations of $X=AS$\nwhere $S \\in \\mathbb{R}^n$ is a random vector with mutually independent\ncoordinates. This problem has been intensively studied, but all existing\nefficient algorithms with provable guarantees require that the coordinates\n$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem\nwhere we do not make this assumption, about the second moment. This problem\nalso has received considerable attention in the applied literature. In the\npresent work, we first give a provably efficient algorithm that works under the\nassumption that for constant $\\gamma > 0$, each $S_i$ has finite\n$(1+\\gamma)$-moment, thus substantially weakening the moment requirement\ncondition for the ICA problem to be solvable. We then give an algorithm that\nworks under the assumption that matrix $A$ has orthogonal columns but requires\nno moment assumptions. Our techniques draw ideas from convex geometry and\nexploit standard properties of the multivariate spherical Gaussian distribution\nin a novel way.\n", "versions": [{"version": "v1", "created": "Wed, 2 Sep 2015 14:56:22 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Anderson", "Joseph", ""], ["Goyal", "Navin", ""], ["Nandi", "Anupama", ""], ["Rademacher", "Luis", ""]]}, {"id": "1509.00980", "submitter": "Mike Ludkovski", "authors": "Ruimeng Hu and Mike Ludkovski", "title": "Sequential Design for Ranking Response Surfaces", "comments": "26 pages, 7 figures (updated several sections and figures)", "journal-ref": null, "doi": "10.1137/15M1045168", "report-no": null, "categories": "stat.ML q-fin.CP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze sequential design methods for the problem of ranking\nseveral response surfaces. Namely, given $L \\ge 2$ response surfaces over a\ncontinuous input space $\\cal X$, the aim is to efficiently find the index of\nthe minimal response across the entire $\\cal X$. The response surfaces are not\nknown and have to be noisily sampled one-at-a-time. This setting is motivated\nby stochastic control applications and requires joint experimental design both\nin space and response-index dimensions. To generate sequential design\nheuristics we investigate stepwise uncertainty reduction approaches, as well as\nsampling based on posterior classification complexity. We also make connections\nbetween our continuous-input formulation and the discrete framework of pure\nregret in multi-armed bandits. To model the response surfaces we utilize\nkriging surrogates. Several numerical examples using both synthetic data and an\nepidemics control problem are provided to illustrate our approach and the\nefficacy of respective adaptive designs.\n", "versions": [{"version": "v1", "created": "Thu, 3 Sep 2015 08:27:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jul 2016 22:17:24 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Hu", "Ruimeng", ""], ["Ludkovski", "Mike", ""]]}, {"id": "1509.01745", "submitter": "Alexander Gribov", "authors": "Alexander Gribov", "title": "A Turning Band Approach to Kernel Convolution for Arbitrary Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most efficient ways to produce unconditional simulations is with\nthe spectral method using fast Fourier transform (FFT) [1]. But this approach\nis not applicable to arbitrary surfaces because no regular grid exists.\nHowever, points on the arbitrary surface can be generated randomly using\nuniform distribution to replace a regular grid. This paper will describe a\nnonstationary kernel convolution approach for data on arbitrary surfaces.\n", "versions": [{"version": "v1", "created": "Sat, 5 Sep 2015 22:31:05 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Gribov", "Alexander", ""]]}, {"id": "1509.02069", "submitter": "Sharon Lee", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "EMMIXcskew: an R Package for the Fitting of a Mixture of Canonical\n  Fundamental Skew t-Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an R package EMMIXcskew for the fitting of the canonical\nfundamental skew t-distribution (CFUST) and finite mixtures of this\ndistribution (FM-CFUST) via maximum likelihood (ML). The CFUST distribution\nprovides a flexible family of models to handle non-normal data, with parameters\nfor capturing skewness and heavy-tails in the data. It formally encompasses the\nnormal, t, and skew-normal distributions as special and/or limiting cases. A\nfew other versions of the skew t-distributions are also nested within the CFUST\ndistribution. In this paper, an Expectation-Maximization (EM) algorithm is\ndescribed for computing the ML estimates of the parameters of the FM-CFUST\nmodel, and different strategies for initializing the algorithm are discussed\nand illustrated. The methodology is implemented in the EMMIXcskew package, and\nexamples are presented using two real datasets. The EMMIXcskew package contains\nfunctions to fit the FM-CFUST model, including procedures for generating\ndifferent initial values. Additional features include random sample generation\nand contour visualization in 2D and 3D.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 15:00:55 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:25:13 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1509.02230", "submitter": "David Huijser", "authors": "David Huijser and Jesse Goodman and Brendon J. Brewer", "title": "Properties of the Affine Invariant Ensemble Sampler in high dimensions", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present theoretical and practical properties of the affine-invariant\nensemble sampler Markov chain Monte Carlo method. In high dimensions the\naffine-invariant ensemble sampler shows unusual and undesirable properties. We\ndemonstrate this with an $n$-dimensional correlated Gaussian toy problem with a\nknown mean and covariance structure, and analyse the burn-in period. The\nburn-in period seems to be short, however upon closer inspection we discover\nthe mean and the variance of the target distribution do not match the expected,\nknown values. This problem becomes greater as $n$ increases. We therefore\nconclude that the affine-invariant ensemble sampler should be used with caution\nin high dimensional problems. We also present some theoretical results\nexplaining this behaviour.\n", "versions": [{"version": "v1", "created": "Mon, 7 Sep 2015 23:55:57 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 03:37:48 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Huijser", "David", ""], ["Goodman", "Jesse", ""], ["Brewer", "Brendon J.", ""]]}, {"id": "1509.02957", "submitter": "Congrui Yi", "authors": "Congrui Yi and Jian Huang", "title": "Semismooth Newton Coordinate Descent Algorithm for Elastic-Net Penalized\n  Huber Loss Regression and Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm, semismooth Newton coordinate descent (SNCD), for the\nelastic-net penalized Huber loss regression and quantile regression in high\ndimensional settings. Unlike existing coordinate descent type algorithms, the\nSNCD updates each regression coefficient and its corresponding subgradient\nsimultaneously in each iteration. It combines the strengths of the coordinate\ndescent and the semismooth Newton algorithm, and effectively solves the\ncomputational challenges posed by dimensionality and nonsmoothness. We\nestablish the convergence properties of the algorithm. In addition, we present\nan adaptive version of the \"strong rule\" for screening predictors to gain extra\nefficiency. Through numerical experiments, we demonstrate that the proposed\nalgorithm is very efficient and scalable to ultra-high dimensions. We\nillustrate the application via a real data example.\n", "versions": [{"version": "v1", "created": "Wed, 9 Sep 2015 21:26:39 GMT"}, {"version": "v2", "created": "Fri, 20 May 2016 21:09:17 GMT"}], "update_date": "2016-05-24", "authors_parsed": [["Yi", "Congrui", ""], ["Huang", "Jian", ""]]}, {"id": "1509.03253", "submitter": "Yolanda Hagar", "authors": "Yolanda Hagar and Vanja Dukic", "title": "Comparison of hazard rate estimation in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of eight different software packages and functions\navailable in R for semi- or non-parametric estimation of the hazard rate for\nright-censored survival data. Of particular interest is the accuracy of the\nestimation of the hazard rate in the presence of covariates, as well as the\nuser-friendliness of the packages. In addition, we investigate the ability to\nincorporate covariates under both the proportional and the non-proportional\nhazards assumptions. We contrast the robustness, variability and precision of\nthe functions through simulations, and then further compare differences between\nthe functions by analyzing the \"cancer\" and \"TRACE\" survival data sets\navailable in R, including covariates under the proportional and\nnon-proportional hazards settings.\n", "versions": [{"version": "v1", "created": "Thu, 10 Sep 2015 18:11:48 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Hagar", "Yolanda", ""], ["Dukic", "Vanja", ""]]}, {"id": "1509.03495", "submitter": "Olivier Feron", "authors": "Olivier F\\'eron and Fran\\c{c}ois Orieux and Jean-Fran\\c{c}ois\n  Giovannelli", "title": "Gradient Scan Gibbs Sampler: an efficient algorithm for high-dimensional\n  Gaussian distributions", "comments": "18 pages", "journal-ref": null, "doi": "10.1109/JSTSP.2015.2510961", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with Gibbs samplers that include high dimensional\nconditional Gaussian distributions. It proposes an efficient algorithm that\navoids the high dimensional Gaussian sampling and relies on a random excursion\nalong a small set of directions. The algorithm is proved to converge, i.e. the\ndrawn samples are asymptotically distributed according to the target\ndistribution. Our main motivation is in inverse problems related to general\nlinear observation models and their solution in a hierarchical Bayesian\nframework implemented through sampling algorithms. It finds direct applications\nin semi-blind/unsupervised methods as well as in some non-Gaussian methods. The\npaper provides an illustration focused on the unsupervised estimation for\nsuper-resolution methods.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 13:14:31 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["F\u00e9ron", "Olivier", ""], ["Orieux", "Fran\u00e7ois", ""], ["Giovannelli", "Jean-Fran\u00e7ois", ""]]}, {"id": "1509.03808", "submitter": "Jascha Sohl-Dickstein", "authors": "Andrew B. Berger, Mayur Mudigonda, Michael R. DeWeese, Jascha\n  Sohl-Dickstein", "title": "A Markov Jump Process for More Efficient Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most sampling algorithms, including Hamiltonian Monte Carlo, transition\nrates between states correspond to the probability of making a transition in a\nsingle time step, and are constrained to be less than or equal to 1. We derive\na Hamiltonian Monte Carlo algorithm using a continuous time Markov jump\nprocess, and are thus able to escape this constraint. Transition rates in a\nMarkov jump process need only be non-negative. We demonstrate that the new\nalgorithm leads to improved mixing for several example problems, both by\nevaluating the spectral gap of the Markov operator, and by computing\nautocorrelation as a function of compute time. We release the algorithm as an\nopen source Python package.\n", "versions": [{"version": "v1", "created": "Sun, 13 Sep 2015 04:29:13 GMT"}, {"version": "v2", "created": "Thu, 17 Sep 2015 00:45:44 GMT"}, {"version": "v3", "created": "Sun, 11 Oct 2015 18:10:58 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Berger", "Andrew B.", ""], ["Mudigonda", "Mayur", ""], ["DeWeese", "Michael R.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1509.04229", "submitter": "Mike Ludkovski", "authors": "Michael Ludkovski and Katherine Shatskikh", "title": "Bayesian Epidemic Detection in Multiple Populations", "comments": "37 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional epidemic detection algorithms make decisions using only local\ninformation. We propose a novel approach that explicitly models spatial\ninformation fusion from several metapopulations. Our method also takes into\naccount cost-benefit considerations regarding the announcement of epidemic. We\nutilize a compartmental stochastic model within a Bayesian detection framework\nwhich leads to a dynamic optimization problem. The resulting adaptive,\nnon-parametric detection strategy optimally balances detection delay vis-a-vis\nprobability of false alarms. Taking advantage of the underlying state-space\nstructure, we represent the stopping rule in terms of a detection map which\nvisualizes the relationship between the multivariate system state and policy\nmaking. It also allows us to obtain an efficient simulation-based solution\nalgorithm that is based on the Sequential Regression Monte Carlo (SRMC)\napproach of Gramacy and Ludkovski (SIFIN, 2015). We illustrate our results on\nsynthetic examples and also quantify the advantages of our adaptive detection\nrelative to conventional threshold-based strategies.\n", "versions": [{"version": "v1", "created": "Mon, 14 Sep 2015 18:16:48 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Ludkovski", "Michael", ""], ["Shatskikh", "Katherine", ""]]}, {"id": "1509.04613", "submitter": "JInglai Li", "authors": "Hongqiao Wang, Guang Lin, Jinglai Li", "title": "Gaussian process surrogates for failure detection: a Bayesian\n  experimental design approach", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2016.02.053", "report-no": null, "categories": "stat.CO math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task of uncertainty quantification is to identify {the\nprobability of} undesired events, in particular, system failures, caused by\nvarious sources of uncertainties. In this work we consider the construction of\nGaussian {process} surrogates for failure detection and failure probability\nestimation. In particular, we consider the situation that the underlying\ncomputer models are extremely expensive, and in this setting, determining the\nsampling points in the state space is of essential importance. We formulate the\nproblem as an optimal experimental design for Bayesian inferences of the limit\nstate (i.e., the failure boundary) and propose an efficient numerical scheme to\nsolve the resulting optimization problem. In particular, the proposed\nlimit-state inference method is capable of determining multiple sampling points\nat a time, and thus it is well suited for problems where multiple computer\nsimulations can be performed in parallel. The accuracy and performance of the\nproposed method is demonstrated by both academic and practical examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Sep 2015 04:50:09 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Wang", "Hongqiao", ""], ["Lin", "Guang", ""], ["Li", "Jinglai", ""]]}, {"id": "1509.04752", "submitter": "Michael Riis Andersen", "authors": "Michael Riis Andersen, Aki Vehtari, Ole Winther, Lars Kai Hansen", "title": "Bayesian inference for spatio-temporal spike-and-slab priors", "comments": "58 pages, 17 figures", "journal-ref": "Journal of Machine Learning Research, 18(139):1-58, 2017", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the problem of solving a series of underdetermined\nlinear inverse problems subject to a sparsity constraint. We generalize the\nspike-and-slab prior distribution to encode a priori correlation of the support\nof the solution in both space and time by imposing a transformed Gaussian\nprocess on the spike-and-slab probabilities. An expectation propagation (EP)\nalgorithm for posterior inference under the proposed model is derived. For\nlarge scale problems, the standard EP algorithm can be prohibitively slow. We\ntherefore introduce three different approximation schemes to reduce the\ncomputational complexity. Finally, we demonstrate the proposed model using\nnumerical experiments based on both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Sep 2015 21:58:12 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2015 18:10:07 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 13:38:56 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Andersen", "Michael Riis", ""], ["Vehtari", "Aki", ""], ["Winther", "Ole", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1509.04879", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Joaqu\\'in M\\'iguez, Petar M. Djuri\\'c", "title": "Adapting the Number of Particles in Sequential Monte Carlo Methods\n  through an Online Scheme for Convergence Assessment", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 7, pp.\n  1781-1794, April 2017", "doi": "10.1109/TSP.2016.2637324", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are broadly used to approximate posterior distributions of\nhidden states in state-space models by means of sets of weighted particles.\nWhile the convergence of the filter is guaranteed when the number of particles\ntends to infinity, the quality of the approximation is usually unknown but\nstrongly dependent on the number of particles. In this paper, we propose a\nnovel method for assessing the convergence of particle filters online manner,\nas well as a simple scheme for the online adaptation of the number of particles\nbased on the convergence assessment. The method is based on a sequential\ncomparison between the actual observations and their predictive probability\ndistributions approximated by the filter. We provide a rigorous theoretical\nanalysis of the proposed methodology and, as an example of its practical use,\nwe present simulations of a simple algorithm for the dynamic and online\nadaption of the number of particles during the operation of a particle filter\non a stochastic version of the Lorenz system.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 10:49:38 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 08:55:54 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["M\u00edguez", "Joaqu\u00edn", ""], ["Djuri\u0107", "Petar M.", ""]]}, {"id": "1509.05005", "submitter": "Alejandro Frery", "authors": "M. Magdalena Lucini and Alejandro C. Frery", "title": "Comments on \"Detecting Outliers in Gamma Distribution\" by M. Jabbari\n  Nooghabi et al. (2010)", "comments": "Accepted for publication in Communications in Statistics - Theory and\n  Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note shows that the results presented by Jabbari Nooghabi et al. (2010)\ndo not hold in all expected cases. With this, the technique proposed by Kumar\nand Lalhita (2012) for detecting upper outliers in Gamma samples is also not\nvalid. Specifically, this note shows that the probability density functions\n(pdf) under the null hypothesis of the test statistics therein proposed are not\nalways valid.\n", "versions": [{"version": "v1", "created": "Wed, 16 Sep 2015 19:22:19 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Lucini", "M. Magdalena", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1509.05305", "submitter": "Carlo Albert", "authors": "Carlo Albert, Simone Ulzega, Ruedi Stoop", "title": "Boosting Bayesian Parameter Inference of Nonlinear Stochastic\n  Differential Equation Models by Hamiltonian Scale Separation", "comments": "15 pages, 8 figures", "journal-ref": "Phys. Rev. E 93, 043313, 15 April 2016", "doi": "10.1103/PhysRevE.93.043313", "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference is a fundamental problem in data-driven modeling. Given\nobserved data that is believed to be a realization of some parameterized model,\nthe aim is to find parameter values that are able to explain the observed data.\nIn many situations, the dominant sources of uncertainty must be included into\nthe model, for making reliable predictions. This naturally leads to stochastic\nmodels. Stochastic models render parameter inference much harder, as the aim\nthen is to find a distribution of likely parameter values. In Bayesian\nstatistics, which is a consistent framework for data-driven learning, this\nso-called posterior distribution can be used to make probabilistic predictions.\nWe propose a novel, exact and very efficient approach for generating posterior\nparameter distributions, for stochastic differential equation models calibrated\nto measured time-series. The algorithm is inspired by re-interpreting the\nposterior distribution as a statistical mechanics partition function of an\nobject akin to a polymer, where the measurements are mapped on heavier beads\ncompared to those of the simulated data. To arrive at distribution samples, we\nemploy a Hamiltonian Monte Carlo approach combined with a multiple time-scale\nintegration. A separation of time scales naturally arises if either the number\nof measurement points or the number of simulation points becomes large.\nFurthermore, at least for 1D problems, we can decouple the harmonic modes\nbetween measurement points and solve the fastest part of their dynamics\nanalytically. Our approach is applicable to a wide range of inference problems\nand is highly parallelizable.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 15:52:46 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2016 09:04:31 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Albert", "Carlo", ""], ["Ulzega", "Simone", ""], ["Stoop", "Ruedi", ""]]}, {"id": "1509.05315", "submitter": "Carlo Albert", "authors": "Carlo Albert", "title": "A Simulated Annealing Approach to Bayesian Inference", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generic algorithm for the extraction of probabilistic (Bayesian)\ninformation about model parameters from data is presented. The algorithm\npropagates an ensemble of particles in the product space of model parameters\nand outputs. Each particle update consists of a random jump in parameter space\nfollowed by a simulation of a model output and a Metropolis\nacceptance/rejection step based on a comparison of the simulated output to the\ndata. The distance of a particle to the data is interpreted as an energy and\nthe algorithm is reducing the associated temperature of the ensemble such that\nentropy production is minimized. If this simulated annealing is not too fast\ncompared to the mixing speed in parameter space, the parameter marginal of the\nensemble approaches the Bayesian posterior distribution. Annealing is adaptive\nand depends on certain extensive thermodynamic quantities that can easily be\nmeasured throughout run-time. In the general case, we propose annealing with a\nconstant entropy production rate, which is optimal as long as annealing is not\ntoo fast. For the practically relevant special case of no prior knowledge, we\nderive an optimal fast annealing schedule with a non-constant entropy\nproduction rate. The algorithm does not require the calculation of the density\nof the model likelihood, which makes it interesting for Bayesian parameter\ninference with stochastic models, whose likelihood functions are typically very\nhigh dimensional integrals.\n", "versions": [{"version": "v1", "created": "Thu, 17 Sep 2015 16:17:58 GMT"}], "update_date": "2015-09-18", "authors_parsed": [["Albert", "Carlo", ""]]}, {"id": "1509.06075", "submitter": "Claudia Solis-Lemus", "authors": "Claudia Sol\\'is-Lemus and C\\'ecile An\\'e", "title": "Inferring phylogenetic networks with maximum pseudolikelihood under\n  incomplete lineage sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic networks are necessary to represent the tree of life expanded by\nedges to represent events such as horizontal gene transfers, hybridizations or\ngene flow. Not all species follow the paradigm of vertical inheritance of their\ngenetic material. While a great deal of research has flourished into the\ninference of phylogenetic trees, statistical methods to infer phylogenetic\nnetworks are still limited and under development. The main disadvantage of\nexisting methods is a lack of scalability. Here, we present a statistical\nmethod to infer phylogenetic networks from multi-locus genetic data in a\npseudolikelihood framework. Our model accounts for incomplete lineage sorting\nthrough the coalescent model, and for horizontal inheritance of genes through\nreticulation nodes in the network. Computation of the pseudolikelihood is fast\nand simple, and it avoids the burdensome calculation of the full likelihood\nwhich can be intractable with many species. Moreover, estimation at the\nquartet-level has the added computational benefit that it is easily\nparallelizable. Simulation studies comparing our method to a full likelihood\napproach show that our pseudolikelihood approach is much faster without\ncompromising accuracy. We applied our method to reconstruct the evolutionary\nrelationships among swordtails and platyfishes ($Xiphophorus$: Poeciliidae),\nwhich is characterized by widespread hybridizations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Sep 2015 23:41:03 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2016 20:48:47 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2016 19:23:35 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Sol\u00eds-Lemus", "Claudia", ""], ["An\u00e9", "C\u00e9cile", ""]]}, {"id": "1509.06310", "submitter": "Aixin Tan", "authors": "Vivekananda Roy, Aixin Tan, and James M. Flegal", "title": "Estimating standard errors for importance sampling estimators with\n  multiple Markov chains", "comments": "49 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The naive importance sampling estimator, based on samples from a single\nimportance density, can be numerically unstable. Instead, we consider\ngeneralized importance sampling estimators where samples from more than one\nprobability distribution are combined. We study this problem in the Markov\nchain Monte Carlo context, where independent samples are replaced with Markov\nchain samples. If the chains converge to their respective target distributions\nat a polynomial rate, then under two finite moment conditions, we show a\ncentral limit theorem holds for the generalized estimators. Further, we develop\nan easy to implement method to calculate valid asymptotic standard errors based\non batch means. We also provide a batch means estimator for calculating\nasymptotically valid standard errors of Geyer(1994) reverse logistic estimator.\nWe illustrate the method using a Bayesian variable selection procedure in\nlinear regression. In particular, the generalized importance sampling estimator\nis used to perform empirical Bayes variable selection and the batch means\nestimator is used to obtain standard errors in a high-dimensional setting where\ncurrent methods are not applicable.\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 17:18:34 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 15:10:30 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Roy", "Vivekananda", ""], ["Tan", "Aixin", ""], ["Flegal", "James M.", ""]]}, {"id": "1509.06365", "submitter": "Andrew Clark", "authors": "Andrew Clark", "title": "Expanding the Computation of Mixture Models by the use of Hermite\n  Polynomials and Ideals", "comments": "The use of algebraic geometry to the solution of the mixture problem\n  expands the application of algebra to statistics. The algebraic method used\n  is a well known. It is its application to statistics that is different", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models have found uses in many areas. To list a few: unsupervised\nlearning, empirical Bayes, latent class and trait models. The current\napplications of mixture models to empirical data is limited to computing a\nmixture model from the same parametric family, e.g. Gaussians or Poissons. In\nthis paper it is shown that by using Hermite polynomials and ideals, the\nmodeling of a mixture process can be extended to include different families in\nterms of their cumulative distribution functions (cdfs)\n", "versions": [{"version": "v1", "created": "Mon, 21 Sep 2015 14:56:35 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Clark", "Andrew", ""]]}, {"id": "1509.06459", "submitter": "Dustin Tran", "authors": "Dustin Tran, Panos Toulis, Edoardo M. Airoldi", "title": "Stochastic gradient descent methods for estimation with large data sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for parameter estimation in settings with large-scale data\nsets, where traditional methods are no longer tenable. Our methods rely on\nstochastic approximations, which are computationally efficient as they maintain\none iterate as a parameter estimate, and successively update that iterate based\non a single data point. When the update is based on a noisy gradient, the\nstochastic approximation is known as standard stochastic gradient descent,\nwhich has been fundamental in modern applications with large data sets.\nAdditionally, our methods are numerically stable because they employ implicit\nupdates of the iterates. Intuitively, an implicit update is a shrinked version\nof a standard one, where the shrinkage factor depends on the observed Fisher\ninformation at the corresponding data point. This shrinkage prevents numerical\ndivergence of the iterates, which can be caused either by excess noise or\noutliers. Our sgd package in R offers the most extensive and robust\nimplementation of stochastic gradient descent methods. We demonstrate that sgd\ndominates alternative software in runtime for several estimation problems with\nmassive data sets. Our applications include the wide class of generalized\nlinear models as well as M-estimation for robust regression.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 04:25:54 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Tran", "Dustin", ""], ["Toulis", "Panos", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1509.06519", "submitter": "Renata Modzelewska", "authors": "A. Wawrzynczak, R. Modzelewska, A. Gil", "title": "A stochastic method of solution of the Parker transport equation", "comments": "8 pages, 7 figures, presented on 24th European Cosmic Ray Symposium\n  2014", "journal-ref": "IOP Publishing, Journal of Physics: Conference Series, 2015,\n  1742-6596, 632, 012084, (Web of Science)", "doi": "10.1088/1742-6596/632/1/012084", "report-no": null, "categories": "astro-ph.SR math.NA physics.space-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the stochastic model of the galactic cosmic ray (GCR) particles\ntransport in the heliosphere. Based on the solution of the Parker transport\nequation we developed models of the short-time variation of the GCR intensity,\ni.e. the Forbush decrease (Fd) and the 27-day variation of the GCR intensity.\nParker transport equation being the Fokker-Planck type equation delineates\nnon-stationary transport of charged particles in the turbulent medium. The\npresented approach of the numerical solution is grounded on solving of the set\nof equivalent stochastic differential equations (SDEs). We demonstrate the\nmethod of deriving from Parker transport equation the corresponding SDEs in the\nheliocentric spherical coordinate system for the backward approach. Features\nindicative the preeminence of the backward approach over the forward is\nstressed. We compare the outcomes of the stochastic model of the Fd and 27-day\nvariation of the GCR intensity with our former models established by the finite\ndifference method. Both models are in an agreement with the experimental data.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 09:18:46 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Wawrzynczak", "A.", ""], ["Modzelewska", "R.", ""], ["Gil", "A.", ""]]}, {"id": "1509.06523", "submitter": "Renata Modzelewska", "authors": "A. Wawrzynczak, R. Modzelewska, A. Gil", "title": "Stochastic approach to the numerical solution of the non-stationary\n  Parker's transport equation", "comments": "4 pages, 2 figures, presented on International Conference on\n  Mathematical Modeling in Physical Sciences, 2014", "journal-ref": "IOP Publishing, Journal of Physics: Conference Series, 574,\n  012078, 2015, (Web of Science)", "doi": "10.1088/1742-6596/574/1/012078", "report-no": null, "categories": "astro-ph.SR math.NA physics.plasm-ph physics.space-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the newly developed stochastic model of the galactic cosmic ray\n(GCR) particles transport in the heliosphere. Mathematically Parker transport\nequation (PTE) describing non-stationary transport of charged particles in the\nturbulent medium is the Fokker-Planck type. It is the second order parabolic\ntime-dependent 4-dimensional (3 spatial coordinates and particles\nenergy/rigidity) partial differential equation. It is worth to mention that, if\nwe assume the stationary case it remains as the 3-D parabolic type problem with\nrespect to the particles rigidity R. If we fix the energy it still remains as\nthe 3-D parabolic type problem with respect to time. The proposed method of\nnumerical solution is based on the solution of the system of stochastic\ndifferential equations (SDEs) being equivalent to the Parker's transport\nequation. We present the method of deriving from PTE the equivalent SDEs in the\nheliocentric spherical coordinate system for the backward approach. The\nobtained stochastic model of the Forbush decrease of the GCR intensity is in an\nagreement with the experimental data. The advantages and disadvantages of the\nforward and the backward solution of the PTE are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Sep 2015 09:34:20 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Wawrzynczak", "A.", ""], ["Modzelewska", "R.", ""], ["Gil", "A.", ""]]}, {"id": "1509.06890", "submitter": "Renata Modzelewska", "authors": "A. Wawrzynczak, R. Modzelewska, M. Kluczek", "title": "Numerical methods for solution of the stochastic differential equations\n  equivalent to the non-stationary Parker's transport equation", "comments": "4 pages, 2 figures, presented on 4th International Conference on\n  Mathematical Modeling in Physical Sciences, 2015", "journal-ref": "IOP Publishing Ltd., Journal of Physics: Conference Series, Volume\n  633, conference 1, 012058, 2015", "doi": "10.1088/1742-6596/633/1/012058", "report-no": null, "categories": "astro-ph.SR math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the numerical schemes for the strong order integration of the set\nof the stochastic differential equations (SDEs) corresponding to the\nnon-stationary Parker transport equation (PTE). PTE is 5-dimensional (3 spatial\ncoordinates, particles energy and time) Fokker- Planck type equation describing\nthe non-stationary the galactic cosmic ray (GCR) particles transport in the\nheliosphere. We present the formulas for the numerical solution of the obtained\nset of SDEs driven by a Wiener process in the case of the full\nthree-dimensional diffusion tensor. We introduce the solution applying the\nstrong order Euler-Maruyama, Milstein and stochastic Runge-Kutta methods. We\ndiscuss the advantages and disadvantages of the presented numerical methods in\nthe context of increasing the accuracy of the solution of the PTE.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 08:59:39 GMT"}], "update_date": "2015-09-24", "authors_parsed": [["Wawrzynczak", "A.", ""], ["Modzelewska", "R.", ""], ["Kluczek", "M.", ""]]}, {"id": "1509.07138", "submitter": "Peng Ding", "authors": "Xinran Li, Peng Ding", "title": "Exact confidence intervals for the average causal effect on a binary\n  outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the physical randomization of completely randomized experiments,\nRigdon and Hudgens (2015) propose two approaches to obtaining exact confidence\nintervals for the average causal effect on a binary outcome. They construct the\nfirst confidence interval by combining, with the Bonferroni adjustment, the\nprediction sets for treatment effects among treatment and control groups, and\nthe second one by inverting a series of randomization tests. With sample size\n$n$, their second approach requires performing $O(n^4)$ randomization tests. We\ndemonstrate that the physical randomization also justifies other ways to\nconstructing exact confidence intervals that are more computationally\nefficient. By exploiting recent advances in hypergeometric confidence intervals\nand the stochastic order information of randomization tests, we propose\napproaches that either do not need to invoke Monte Carlo, or require performing\nat most $O(n^2)$ randomization tests. We provide technical details and R code\nin the Supplementary Material.\n", "versions": [{"version": "v1", "created": "Wed, 23 Sep 2015 20:15:35 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""]]}, {"id": "1509.07185", "submitter": "Zachary Weller", "authors": "Zachary D. Weller", "title": "spTest: An R Package Implementing Nonparametric Tests of Isotropy", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important step of modeling spatially-referenced data is appropriately\nspecifying the second order properties of the random field. A scientist\ndeveloping a model for spatial data has a number of options regarding the\nnature of the dependence between observations. One of these options is deciding\nwhether or not the dependence between observations depends on direction, or, in\nother words, whether or not the spatial covariance function is isotropic.\nIsotropy implies that spatial dependence is a function of only the distance and\nnot the direction of the spatial separation between sampling locations. A\nresearcher may use graphical techniques, such as directional sample\nsemivariograms, to determine whether an assumption of isotropy holds. These\ngraphical diagnostics can be difficult to assess, subject to personal\ninterpretation, and potentially misleading as they typically do not include a\nmeasure of uncertainty. In order to escape these issues, a hypothesis test of\nthe assumption of isotropy may be more desirable. To avoid specification of the\ncovariance function, a number of nonparametric tests of isotropy have been\ndeveloped using both the spatial and spectral representations of random fields.\nSeveral of these nonparametric tests are implemented in the R package spTest,\navailable on CRAN. We demonstrate how graphical techniques and the hypothesis\ntests programmed in spTest can be used in practice to assess isotropy\nproperties.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 00:04:06 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 20:49:26 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Weller", "Zachary D.", ""]]}, {"id": "1509.07376", "submitter": "Maria Lomeli Miss", "authors": "Maria Lomeli, Stefano Favaro and Yee Whye Teh", "title": "A hybrid sampler for Poisson-Kingman mixture models", "comments": null, "journal-ref": "NIPS 2015", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the introduction of a new Markov Chain Monte Carlo scheme\nfor posterior sampling in Bayesian nonparametric mixture models with priors\nthat belong to the general Poisson-Kingman class. We present a novel compact\nway of representing the infinite dimensional component of the model such that\nwhile explicitly representing this infinite component it has less memory and\nstorage requirements than previous MCMC schemes. We describe comparative\nsimulation results demonstrating the efficacy of the proposed MCMC algorithm\nagainst existing marginal and conditional MCMC samplers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 13:58:17 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Lomeli", "Maria", ""], ["Favaro", "Stefano", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1509.07426", "submitter": "Hua Zhou", "authors": "Hua Zhou and Liuyi Hu and Jin Zhou and Kenneth Lange", "title": "MM Algorithms for Variance Components Models", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variance components estimation and mixed model analysis are central themes in\nstatistics with applications in numerous scientific disciplines. Despite the\nbest efforts of generations of statisticians and numerical analysts, maximum\nlikelihood estimation and restricted maximum likelihood estimation of variance\ncomponent models remain numerically challenging. Building on the\nminorization-maximization (MM) principle, this paper presents a novel iterative\nalgorithm for variance components estimation. MM algorithm is trivial to\nimplement and competitive on large data problems. The algorithm readily extends\nto more complicated problems such as linear mixed models, multivariate response\nmodels possibly with missing data, maximum a posteriori estimation, penalized\nestimation, and generalized estimating equations (GEE). We establish the global\nconvergence of the MM algorithm to a KKT point and demonstrate, both\nnumerically and theoretically, that it converges faster than the classical EM\nalgorithm when the number of variance components is greater than two and all\ncovariance matrices are positive definite.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2015 16:38:16 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Zhou", "Hua", ""], ["Hu", "Liuyi", ""], ["Zhou", "Jin", ""], ["Lange", "Kenneth", ""]]}, {"id": "1509.07751", "submitter": "Josef H\\\"o\\\"ok", "authors": "Lars Josef H\\\"o\\\"ok and Erik Lindstr\\\"om", "title": "Efficient Computation of the Quasi Likelihood function for Discretely\n  Observed Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple method for nearly simultaneous computation of all\nmoments needed for quasi maximum likelihood estimation of parameters in\ndiscretely observed stochastic differential equations commonly seen in finance.\nThe method proposed in this papers is not restricted to any particular dynamics\nof the differential equation and is virtually insensitive to the sampling\ninterval. The key contribution of the paper is that computational complexity is\nsublinear in the number of observations as we compute all moments through a\nsingle operation. Furthermore, that operation can be done offline. The\nsimulations show that the method is unbiased for all practical purposes for any\nsampling design, including random sampling, and that the computational cost is\ncomparable (actually faster for moderate and large data sets) to the simple,\noften severely biased, Euler-Maruyama approximation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 15:17:37 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["H\u00f6\u00f6k", "Lars Josef", ""], ["Lindstr\u00f6m", "Erik", ""]]}, {"id": "1509.07900", "submitter": "Tiep Mai", "authors": "Tiep Mai and Simon Wilson", "title": "Bayesian sequential parameter estimation with a Laplace type\n  approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method for sequential inference of the fixed parameters of a dynamic latent\nGaussian models is proposed and evaluated that is based on the iterated Laplace\napproximation. The method provides a useful trade-off between computational\nperformance and the accuracy of the approximation to the true posterior\ndistribution. Approximation corrections are shown to improve the accuracy of\nthe approximation in simulation studies. A population-based approach is also\nshown to provide a more robust inference method.\n", "versions": [{"version": "v1", "created": "Fri, 25 Sep 2015 21:23:07 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Mai", "Tiep", ""], ["Wilson", "Simon", ""]]}, {"id": "1509.07985", "submitter": "Luca Martino", "authors": "L. Martino, F. Louzada", "title": "Adaptive Rejection Sampling with fixed number of nodes", "comments": "(to appear) Communications in Statistics - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive rejection sampling (ARS) algorithm is a universal random\ngenerator for drawing samples efficiently from a univariate log-concave target\nprobability density function (pdf). ARS generates independent samples from the\ntarget via rejection sampling with high acceptance rates. Indeed, ARS yields a\nsequence of proposal functions that converge toward the target pdf, so that the\nprobability of accepting a sample approaches one. However, sampling from the\nproposal pdf becomes more computational demanding each time it is updated. In\nthis work, we propose a novel ARS scheme, called Cheap Adaptive Rejection\nSampling (CARS), where the computational effort for drawing from the proposal\nremains constant, decided in advance by the user. For generating a large number\nof desired samples, CARS is faster than ARS.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 14:16:25 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 11:10:23 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Martino", "L.", ""], ["Louzada", "F.", ""]]}, {"id": "1509.07993", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, D. Luengo, F. Louzada", "title": "Parallel Metropolis chains with cooperative adaptation", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP.2016.7472423", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods, such as Markov chain Monte Carlo (MCMC) algorithms, have\nbecome very popular in signal processing over the last years. In this work, we\nintroduce a novel MCMC scheme where parallel MCMC chains interact, adapting\ncooperatively the parameters of their proposal functions. Furthermore, the\nnovel algorithm distributes the computational effort adaptively, rewarding the\nchains which are providing better performance and, possibly even stopping other\nones. These extinct chains can be reactivated if the algorithm considers\nnecessary. Numerical simulations shows the benefits of the novel scheme.\n", "versions": [{"version": "v1", "created": "Sat, 26 Sep 2015 15:16:50 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Luengo", "D.", ""], ["Louzada", "F.", ""]]}, {"id": "1509.08165", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder, Arkopal Choudhury, Garud Iyengar, Bodhisattva Sen", "title": "A Computational Framework for Multivariate Convex Regression and its\n  Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the nonparametric least squares estimator (LSE) of a multivariate\nconvex regression function. The LSE, given as the solution to a quadratic\nprogram with $O(n^2)$ linear constraints ($n$ being the sample size), is\ndifficult to compute for large problems. Exploiting problem specific structure,\nwe propose a scalable algorithmic framework based on the augmented Lagrangian\nmethod to compute the LSE. We develop a novel approach to obtain smooth convex\napproximations to the fitted (piecewise affine) convex LSE and provide formal\nbounds on the quality of approximation. When the number of samples is not too\nlarge compared to the dimension of the predictor, we propose a regularization\nscheme --- Lipschitz convex regression --- where we constrain the norm of the\nsubgradients, and study the rates of convergence of the obtained LSE. Our\nalgorithmic framework is simple and flexible and can be easily adapted to\nhandle variants: estimation of a non-decreasing/non-increasing convex/concave\n(with or without a Lipschitz bound) function. We perform numerical studies\nillustrating the scalability of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Sep 2015 00:34:02 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Mazumder", "Rahul", ""], ["Choudhury", "Arkopal", ""], ["Iyengar", "Garud", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1509.08581", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Optimization over Sparse Symmetric Sets via a Nonmonotone Projected\n  Gradient Method", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing a Lipschitz differentiable function\nover a class of sparse symmetric sets that has wide applications in engineering\nand science. For this problem, it is known that any accumulation point of the\nclassical projected gradient (PG) method with a constant stepsize $1/L$\nsatisfies the $L$-stationarity optimality condition that was introduced in [3].\nIn this paper we introduce a new optimality condition that is stronger than the\n$L$-stationarity optimality condition. We also propose a nonmonotone projected\ngradient (NPG) method for this problem by incorporating some support-changing\nand coordintate-swapping strategies into a projected gradient method with\nvariable stepsizes. It is shown that any accumulation point of NPG satisfies\nthe new optimality condition and moreover it is a coordinatewise stationary\npoint. Under some suitable assumptions, we further show that it is a global or\na local minimizer of the problem. Numerical experiments are conducted to\ncompare the performance of PG and NPG. The computational results demonstrate\nthat NPG has substantially better solution quality than PG, and moreover, it is\nat least comparable to, but sometimes can be much faster than PG in terms of\nspeed.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 03:39:01 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2015 22:19:11 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 18:47:57 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1509.08775", "submitter": "Daniel Paulin", "authors": "Daniel Paulin, Ajay Jasra, Alexandre Thiery", "title": "Error Bounds for Sequential Monte Carlo Samplers for Multimodal\n  Distributions", "comments": "42 pages, 6 figures. Some minor corrections in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide bounds on the asymptotic variance for a class of\nsequential Monte Carlo (SMC) samplers designed for approximating multimodal\ndistributions. Such methods combine standard SMC methods and Markov chain Monte\nCarlo (MCMC) kernels. Our bounds improve upon previous results, and unlike some\nearlier work, they also apply in the case when the MCMC kernels can move\nbetween the modes. We apply our results to the Potts model from statistical\nphysics. In this case, the problem of sharp peaks is encountered. Earlier\nmethods, such as parallel tempering, are only able to sample from it at an\nexponential (in an important parameter of the model) cost. We propose a\nsequence of interpolating distributions called interpolation to independence,\nand show that the SMC sampler based on it is able to sample from this target\ndistribution at a polynomial cost. We believe that our method is generally\napplicable to many other distributions as well.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 14:33:59 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 12:09:35 GMT"}, {"version": "v3", "created": "Mon, 20 Feb 2017 15:52:04 GMT"}, {"version": "v4", "created": "Wed, 24 Jan 2018 06:53:21 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Paulin", "Daniel", ""], ["Jasra", "Ajay", ""], ["Thiery", "Alexandre", ""]]}, {"id": "1509.08787", "submitter": "Jeremy Heng", "authors": "Jeremy Heng, Arnaud Doucet and Yvo Pokern", "title": "Gibbs flow for approximate transport with applications to Bayesian\n  computation", "comments": "Significantly revised with new methodology and numerical examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\pi_{0}$ and $\\pi_{1}$ be two distributions on the Borel space\n$(\\mathbb{R}^{d},\\mathcal{B}(\\mathbb{R}^{d}))$. Any measurable function\n$T:\\mathbb{R}^{d}\\rightarrow\\mathbb{R}^{d}$ such that $Y=T(X)\\sim\\pi_{1}$ if\n$X\\sim\\pi_{0}$ is called a transport map from $\\pi_{0}$ to $\\pi_{1}$. For any\n$\\pi_{0}$ and $\\pi_{1}$, if one could obtain an analytical expression for a\ntransport map from $\\pi_{0}$ to $\\pi_{1}$, then this could be straightforwardly\napplied to sample from any distribution. One would map draws from an\neasy-to-sample distribution $\\pi_{0}$ to the target distribution $\\pi_{1}$\nusing this transport map. Although it is usually impossible to obtain an\nexplicit transport map for complex target distributions, we show here how to\nbuild a tractable approximation of a novel transport map. This is achieved by\nmoving samples from $\\pi_{0}$ using an ordinary differential equation with a\nvelocity field that depends on the full conditional distributions of the\ntarget. Even when this ordinary differential equation is time-discretized and\nthe full conditional distributions are numerically approximated, the resulting\ndistribution of mapped samples can be efficiently evaluated and used as a\nproposal within sequential Monte Carlo samplers. We demonstrate significant\ngains over state-of-the-art sequential Monte Carlo samplers at a fixed\ncomputational complexity on a variety of applications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 14:59:01 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 22:22:09 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 08:32:15 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Heng", "Jeremy", ""], ["Doucet", "Arnaud", ""], ["Pokern", "Yvo", ""]]}, {"id": "1509.08870", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "Posterior Exploration based Sequential Monte Carlo for Global\n  Optimization", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a global optimization algorithm based on the Sequential Monte\nCarlo (SMC) sampling framework. In this framework, the objective function is\nnormalized to be a probabilistic density function (pdf), based on which a\nsequence of annealed target pdfs is designed to asymptotically converge on the\nset of global optima. A sequential importance sampling (SIS) procedure is\nperformed to simulate the resulting targets, and the maxima of the objective\nfunction is assessed from the yielded samples. The disturbing issue lies in the\ndesign of the importance sampling (IS) pdf, which crucially influences the IS\nefficiency. We propose an approach to design the IS pdf online by embedding a\nposterior exploration (PE) procedure into each iteration of the SMC framework.\nThe PE procedure can also explore the important regions of the parameter space\nsupported by the target pdf. A byproduct of the PE procedure is an adaptive\nmechanism to design the annealing temperature schedule online. We compare the\nperformance of the proposed algorithm with those of several existing related\nalternatives by applying them to over a dozen standard benchmark functions. The\nresult demonstrates the appealing properties of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2015 18:06:20 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 07:15:52 GMT"}, {"version": "v3", "created": "Thu, 14 Jul 2016 02:07:08 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "1509.08999", "submitter": "Alexander Terenin", "authors": "Alexander Terenin, Daniel Simpson, and David Draper", "title": "Asynchronous Gibbs Sampling", "comments": null, "journal-ref": "Artificial Intelligence and Statistics, 2020", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method often used in\nBayesian learning. MCMC methods can be difficult to deploy on parallel and\ndistributed systems due to their inherently sequential nature. We study\nasynchronous Gibbs sampling, which achieves parallelism by simply ignoring\nsequential requirements. This method has been shown to produce good empirical\nresults for some hierarchical models, and is popular in the topic modeling\ncommunity, but was also shown to diverge for other targets. We introduce a\ntheoretical framework for analyzing asynchronous Gibbs sampling and other\nextensions of MCMC that do not possess the Markov property. We prove that\nasynchronous Gibbs can be modified so that it converges under appropriate\nregularity conditions -- we call this the exact asynchronous Gibbs algorithm.\nWe study asynchronous Gibbs on a set of examples by comparing the exact and\napproximate algorithms, including two where it works well, and one where it\nfails dramatically. We conclude with a set of heuristics to describe settings\nwhere the algorithm can be effectively used.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 02:51:09 GMT"}, {"version": "v2", "created": "Sat, 23 Apr 2016 19:19:06 GMT"}, {"version": "v3", "created": "Sun, 9 Apr 2017 20:30:46 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 22:01:37 GMT"}, {"version": "v5", "created": "Fri, 1 Jun 2018 22:16:31 GMT"}, {"version": "v6", "created": "Sun, 22 Sep 2019 20:41:25 GMT"}, {"version": "v7", "created": "Sat, 29 Feb 2020 23:46:37 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Terenin", "Alexander", ""], ["Simpson", "Daniel", ""], ["Draper", "David", ""]]}, {"id": "1509.09120", "submitter": "Andrew Golightly", "authors": "Gavin A. Whitaker, Andrew Golightly, Richard J. Boys and Chris\n  Sherlock", "title": "Improved bridge constructs for stochastic differential equations", "comments": "22 pages, 7 figures. Accepted for publication in Statistics and\n  Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of generating discrete-time realisations of a nonlinear\nmultivariate diffusion process satisfying an It\\^o stochastic differential\nequation conditional on an observation taken at a fixed future time-point. Such\nrealisations are typically termed diffusion bridges. Since, in general, no\nclosed form expression exists for the transition densities of the process of\ninterest, a widely adopted solution works with the Euler-Maruyama\napproximation, by replacing the intractable transition densities with Gaussian\napproximations. However, the density of the conditioned discrete-time process\nremains intractable, necessitating the use of computationally intensive methods\nsuch as Markov chain Monte Carlo. Designing an efficient proposal mechanism\nwhich can be applied to a noisy and partially observed system that exhibits\nnonlinear dynamics is a challenging problem, and is the focus of this paper. By\npartitioning the process into two parts, one that accounts for nonlinear\ndynamics in a deterministic way, and another as a residual stochastic process,\nwe develop a class of novel constructs that bridge the residual process via a\nlinear approximation. In addition, we adapt a recently proposed construct to a\npartial and noisy observation regime. We compare the performance of each new\nconstruct with a number of existing approaches, using three applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 10:57:51 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 09:56:57 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Whitaker", "Gavin A.", ""], ["Golightly", "Andrew", ""], ["Boys", "Richard J.", ""], ["Sherlock", "Chris", ""]]}, {"id": "1509.09175", "submitter": "Nick Whiteley Dr", "authors": "Juha Ala-Luhtala, Nick Whiteley, Kari Heine, Robert Piche", "title": "An Introduction to Twisted Particle Filters and Parameter Estimation in\n  Non-linear State-space Models", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": "10.1109/TSP.2016.2563387", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Twisted particle filters are a class of sequential Monte Carlo methods\nrecently introduced by Whiteley and Lee to improve the efficiency of marginal\nlikelihood estimation in state-space models. The purpose of this article is to\nextend the twisted particle filtering methodology, establish accessible\ntheoretical results which convey its rationale, and provide a demonstration of\nits practical performance within particle Markov chain Monte Carlo for\nestimating static model parameters. We derive twisted particle filters that\nincorporate systematic or multinomial resampling and information from\nhistorical particle states, and a transparent proof which identifies the\noptimal algorithm for marginal likelihood estimation. We demonstrate how to\napproximate the optimal algorithm for nonlinear state-space models with\nGaussian noise and we apply such approximations to two examples: a range and\nbearing tracking problem and an indoor positioning problem with Bluetooth\nsignal strength measurements. We demonstrate improvements over standard\nalgorithms in terms of variance of marginal likelihood estimates and Markov\nchain autocorrelation for given CPU time, and improved tracking performance\nusing estimated parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2015 13:53:04 GMT"}, {"version": "v2", "created": "Thu, 7 Apr 2016 15:40:22 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Ala-Luhtala", "Juha", ""], ["Whiteley", "Nick", ""], ["Heine", "Kari", ""], ["Piche", "Robert", ""]]}]