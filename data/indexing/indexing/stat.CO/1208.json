[{"id": "1208.0065", "submitter": "Hatef Monajemi Mr.", "authors": "Hatef Monajemi and Peter K. Kitanidis", "title": "An Improved Data Assimilation Scheme for High Dimensional Nonlinear\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC nlin.CD physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear/non-Gaussian filtering has broad applications in many areas of life\nsciences where either the dynamic is nonlinear and/or the probability density\nfunction of uncertain state is non-Gaussian. In such problems, the accuracy of\nthe estimated quantities depends highly upon how accurately their posterior pdf\ncan be approximated. In low dimensional state spaces, methods based on\nSequential Importance Sampling (SIS) can suitably approximate the posterior\npdf. For higher dimensional problems, however, these techniques are usually\ninappropriate since the required number of particles to achieve satisfactory\nestimates grows exponentially with the dimension of state space. On the other\nhand, ensemble Kalman filter (EnKF) and its variants are more suitable for\nlarge-scale problems due to transformation of particles in the Bayesian update\nstep. It has been shown that the latter class of methods may lead to suboptimal\nsolutions for strongly nonlinear problems due to the Gaussian assumption in the\nupdate step. In this paper, we introduce a new technique based on the Gaussian\nsum expansion which captures the non-Gaussian features more accurately while\nthe required computational effort remains within reason for high dimensional\nproblems. We demonstrate the performance of the method for non-Gaussian\nprocesses through several examples including the strongly nonlinear Lorenz\nmodels. Results show a remarkable improvement in the mean square error compared\nto EnKF, and a desirable convergence behavior as the number of particles\nincreases.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2012 01:38:14 GMT"}], "update_date": "2012-08-02", "authors_parsed": [["Monajemi", "Hatef", ""], ["Kitanidis", "Peter K.", ""]]}, {"id": "1208.0651", "submitter": "Muhammad Salman Asif", "authors": "M. Salman Asif, Justin Romberg", "title": "Fast and Accurate Algorithms for Re-Weighted L1-Norm Minimization", "comments": "Submitted to IEEE Trans. Signal Process", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  To recover a sparse signal from an underdetermined system, we often solve a\nconstrained L1-norm minimization problem. In many cases, the signal sparsity\nand the recovery performance can be further improved by replacing the L1 norm\nwith a \"weighted\" L1 norm. Without any prior information about nonzero elements\nof the signal, the procedure for selecting weights is iterative in nature.\nCommon approaches update the weights at every iteration using the solution of a\nweighted L1 problem from the previous iteration.\n  In this paper, we present two homotopy-based algorithms that efficiently\nsolve reweighted L1 problems. First, we present an algorithm that quickly\nupdates the solution of a weighted L1 problem as the weights change. Since the\nsolution changes only slightly with small changes in the weights, we develop a\nhomotopy algorithm that replaces the old weights with the new ones in a small\nnumber of computationally inexpensive steps. Second, we propose an algorithm\nthat solves a weighted L1 problem by adaptively selecting the weights while\nestimating the signal. This algorithm integrates the reweighting into every\nstep along the homotopy path by changing the weights according to the changes\nin the solution and its support, allowing us to achieve a high quality signal\nreconstruction by solving a single homotopy problem. We compare the performance\nof both algorithms, in terms of reconstruction accuracy and computational\ncomplexity, against state-of-the-art solvers and show that our methods have\nsmaller computational cost. In addition, we will show that the adaptive\nselection of the weights inside the homotopy often yields reconstructions of\nhigher quality.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2012 04:06:32 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Asif", "M. Salman", ""], ["Romberg", "Justin", ""]]}, {"id": "1208.0945", "submitter": "Marc Suchard", "authors": "Marc A. Suchard, Shawn E. Simpson, Ivan Zorych, Patrick Ryan, David\n  Madigan", "title": "Massive parallelization of serial inference algorithms for a complex\n  generalized linear model", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following a series of high-profile drug safety disasters in recent years,\nmany countries are redoubling their efforts to ensure the safety of licensed\nmedical products. Large-scale observational databases such as claims databases\nor electronic health record systems are attracting particular attention in this\nregard, but present significant methodological and computational concerns. In\nthis paper we show how high-performance statistical computation, including\ngraphics processing units, relatively inexpensive highly parallel computing\ndevices, can enable complex methods in large databases. We focus on\noptimization and massive parallelization of cyclic coordinate descent\napproaches to fit a conditioned generalized linear model involving tens of\nmillions of observations and thousands of predictors in a Bayesian context. We\nfind orders-of-magnitude improvement in overall run-time. Coordinate descent\napproaches are ubiquitous in high-dimensional statistics and the algorithms we\npropose open up exciting new methodological possibilities with the potential to\nsignificantly improve drug safety.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2012 16:36:14 GMT"}], "update_date": "2012-08-07", "authors_parsed": [["Suchard", "Marc A.", ""], ["Simpson", "Shawn E.", ""], ["Zorych", "Ivan", ""], ["Ryan", "Patrick", ""], ["Madigan", "David", ""]]}, {"id": "1208.1065", "submitter": "Elif Vural", "authors": "Hemant Tyagi, Elif Vural and Pascal Frossard", "title": "Tangent space estimation for smooth embeddings of Riemannian manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous dimensionality reduction problems in data analysis involve the\nrecovery of low-dimensional models or the learning of manifolds underlying sets\nof data. Many manifold learning methods require the estimation of the tangent\nspace of the manifold at a point from locally available data samples. Local\nsampling conditions such as (i) the size of the neighborhood (sampling width)\nand (ii) the number of samples in the neighborhood (sampling density) affect\nthe performance of learning algorithms. In this work, we propose a theoretical\nanalysis of local sampling conditions for the estimation of the tangent space\nat a point P lying on a m-dimensional Riemannian manifold S in R^n. Assuming a\nsmooth embedding of S in R^n, we estimate the tangent space T_P S by performing\na Principal Component Analysis (PCA) on points sampled from the neighborhood of\nP on S. Our analysis explicitly takes into account the second order properties\nof the manifold at P, namely the principal curvatures as well as the higher\norder terms. We consider a random sampling framework and leverage recent\nresults from random matrix theory to derive conditions on the sampling width\nand the local sampling density for an accurate estimation of tangent subspaces.\nWe measure the estimation accuracy by the angle between the estimated tangent\nspace and the true tangent space T_P S and we give conditions for this angle to\nbe bounded with high probability. In particular, we observe that the local\nsampling conditions are highly dependent on the correlation between the\ncomponents in the second-order local approximation of the manifold. We finally\nprovide numerical simulations to validate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2012 23:50:26 GMT"}, {"version": "v2", "created": "Fri, 17 May 2013 10:18:05 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Tyagi", "Hemant", ""], ["Vural", "Elif", ""], ["Frossard", "Pascal", ""]]}, {"id": "1208.1259", "submitter": "Ping Li", "authors": "Ping Li and Art Owen and Cun-Hui Zhang", "title": "One Permutation Hashing for Efficient Search and Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the method of b-bit minwise hashing has been applied to large-scale\nlinear learning and sublinear time near-neighbor search. The major drawback of\nminwise hashing is the expensive preprocessing cost, as the method requires\napplying (e.g.,) k=200 to 500 permutations on the data. The testing time can\nalso be expensive if a new data point (e.g., a new document or image) has not\nbeen processed, which might be a significant issue in user-facing applications.\n  We develop a very simple solution based on one permutation hashing.\nConceptually, given a massive binary data matrix, we permute the columns only\nonce and divide the permuted columns evenly into k bins; and we simply store,\nfor each data vector, the smallest nonzero location in each bin. The\ninteresting probability analysis (which is validated by experiments) reveals\nthat our one permutation scheme should perform very similarly to the original\n(k-permutation) minwise hashing. In fact, the one permutation scheme can be\neven slightly more accurate, due to the \"sample-without-replacement\" effect.\n  Our experiments with training linear SVM and logistic regression on the\nwebspam dataset demonstrate that this one permutation hashing scheme can\nachieve the same (or even slightly better) accuracies compared to the original\nk-permutation scheme. To test the robustness of our method, we also experiment\nwith the small news20 dataset which is very sparse and has merely on average\n500 nonzeros in each data vector. Interestingly, our one permutation scheme\nnoticeably outperforms the k-permutation scheme when k is not too small on the\nnews20 dataset. In summary, our method can achieve at least the same accuracy\nas the original k-permutation scheme, at merely 1/k of the original\npreprocessing cost.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2012 12:28:06 GMT"}], "update_date": "2012-08-08", "authors_parsed": [["Li", "Ping", ""], ["Owen", "Art", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1208.1717", "submitter": "Daniel Simpson", "authors": "Erlend Aune and Daniel Simpson", "title": "The use of systems of stochastic PDEs as priors for multivariate models\n  with discrete structures", "comments": "22 Pages, 17 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A challenge in multivariate problems with discrete structures is the\ninclusion of prior information that may differ in each separate structure. A\nparticular example of this is seismic amplitude versus angle (AVA) inversion to\nelastic parameters, where the discrete structures are geologic layers.\nRecently, the use of systems of linear stocastic partial differential equations\n(SPDEs) have become a popular tool for specifying priors in latent Gaussian\nmodels. This approach allows for flexible incorporation of nonstationarity and\nanisotropy in the prior model. Another advantage is that the prior field is\nMarkovian and therefore the precision matrix is very sparse, introducing huge\ncomputational and memory benefits. We present a novel approach for\nparametrising correlations that differ in the different discrete structures,\nand additionally a geodesic blending approach for quantifying fuzziness of\ninterfaces between the structures. Keywords: Gaussian distribution,\nmultivariate, stochastic PDEs, discrete structures\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 17:30:18 GMT"}], "update_date": "2012-08-09", "authors_parsed": [["Aune", "Erlend", ""], ["Simpson", "Daniel", ""]]}, {"id": "1208.1720", "submitter": "Mathukumalli Vidyasagar", "authors": "Mehmet Eren Ahsen and Mathukumalli Vidyasagar", "title": "Mixing Coefficients Between Discrete and Real Random Variables:\n  Computation and Properties", "comments": "36 pages. Accepted for publication in IEEE Transactions on Automatic\n  Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of estimating the alpha-, beta- and\nphi-mixing coefficients between two random variables, that can either assume\nvalues in a finite set or the set of real numbers. In either case, explicit\nclosed-form formulas for the beta-mixing coefficient are already known.\nTherefore for random variables assuming values in a finite set, our\ncontributions are two-fold: (i) In the case of the alpha-mixing coefficient, we\nshow that determining whether or not it exceeds a prespecified threshold is\nNP-complete, and provide efficiently computable upper and lower bounds. (ii) We\nderive an exact closed-form formula for the phi-mixing coefficient. Next, we\nprove analogs of the data-processing inequality from information theory for\neach of the three kinds of mixing coefficients. Then we move on to real-valued\nrandom variables, and show that by using percentile binning and allowing the\nnumber of bins to increase more slowly than the number of samples, we can\ngenerate empirical estimates that are consistent, i.e., converge to the true\nvalues as the number of samples approaches infinity.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 17:46:51 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 17:48:57 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2012 19:54:09 GMT"}, {"version": "v4", "created": "Sun, 26 May 2013 11:11:28 GMT"}, {"version": "v5", "created": "Wed, 3 Jul 2013 04:45:34 GMT"}], "update_date": "2013-07-04", "authors_parsed": [["Ahsen", "Mehmet Eren", ""], ["Vidyasagar", "Mathukumalli", ""]]}, {"id": "1208.1728", "submitter": "Javier Contreras-Reyes JCR", "authors": "Javier E. Contreras-Reyes, Wilfredo Palma", "title": "Statistical Analysis of Autoregressive Fractionally Integrated Moving\n  Average Models", "comments": "20 pages, 8 figures", "journal-ref": "Computational Statistics (2013), 28(5), 2309-2331", "doi": "10.1007/s00180-013-0408-7", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, several time series exhibit long-range dependence or persistence\nin their observations, leading to the development of a number of estimation and\nprediction methodologies to account for the slowly decaying autocorrelations.\nThe autoregressive fractionally integrated moving average (ARFIMA) process is\none of the best-known classes of long-memory models. In the package afmtools\nfor R, we have implemented some of these statistical tools for analyzing ARFIMA\nmodels. In particular, this package contains functions for parameter\nestimation, exact autocovariance calculation, predictive ability testing, and\nimpulse response function, amongst others. Finally, the implemented methods are\nillustrated with applications to real-life time series.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2012 18:28:31 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Contreras-Reyes", "Javier E.", ""], ["Palma", "Wilfredo", ""]]}, {"id": "1208.2157", "submitter": "Carlo Albert", "authors": "Carlo Albert and Hans R. Kuensch and Andreas Scheidegger", "title": "A Simulated Annealing Approach to Approximate Bayes Computations", "comments": "20 pages, 2 figures", "journal-ref": "Stat. Comput. 25, 1217 (2015)", "doi": "10.1007/s11222-014-9507-8", "report-no": null, "categories": "stat.CO physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayes Computations (ABC) are used for parameter inference when\nthe likelihood function of the model is expensive to evaluate but relatively\ncheap to sample from. In particle ABC, an ensemble of particles in the product\nspace of model outputs and parameters is propagated in such a way that its\noutput marginal approaches a delta function at the data and its parameter\nmarginal approaches the posterior distribution. Inspired by Simulated\nAnnealing, we present a new class of particle algorithms for ABC, based on a\nsequence of Metropolis kernels, associated with a decreasing sequence of\ntolerances w.r.t. the data. Unlike other algorithms, our class of algorithms is\nnot based on importance sampling. Hence, it does not suffer from a loss of\neffective sample size due to re-sampling. We prove convergence under a\ncondition on the speed at which the tolerance is decreased. Furthermore, we\npresent a scheme that adapts the tolerance and the jump distribution in\nparameter space according to some mean-fields of the ensemble, which preserves\nthe statistical independence of the particles, in the limit of infinite sample\nsize. This adaptive scheme aims at converging as close as possible to the\ncorrect result with as few system updates as possible via minimizing the\nentropy production in the system. The performance of this new class of\nalgorithms is compared against two other recent algorithms on two toy examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2012 12:43:20 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2013 09:12:43 GMT"}, {"version": "v3", "created": "Fri, 13 Jun 2014 14:52:35 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Albert", "Carlo", ""], ["Kuensch", "Hans R.", ""], ["Scheidegger", "Andreas", ""]]}, {"id": "1208.2651", "submitter": "Manuel J. A. Eugster", "authors": "Anne-Laure Boulesteix and Manuel J. A. Eugster", "title": "A Plea for Neutral Comparison Studies in Computational Sciences", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0061562", "report-no": null, "categories": "stat.CO cs.CV stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a context where most published articles are devoted to the development of\n\"new methods\", comparison studies are generally appreciated by readers but\nsurprisingly given poor consideration by many scientific journals. In\nconnection with recent articles on over-optimism and epistemology published in\nBioinformatics, this letter stresses the importance of neutral comparison\nstudies for the objective evaluation of existing methods and the establishment\nof standards by drawing parallels with clinical research.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2012 18:01:17 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Boulesteix", "Anne-Laure", ""], ["Eugster", "Manuel J. A.", ""]]}, {"id": "1208.4118", "submitter": "Ari Pakman", "authors": "Ari Pakman and Liam Paninski", "title": "Exact Hamiltonian Monte Carlo for Truncated Multivariate Gaussians", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Hamiltonian Monte Carlo algorithm to sample from multivariate\nGaussian distributions in which the target space is constrained by linear and\nquadratic inequalities or products thereof. The Hamiltonian equations of motion\ncan be integrated exactly and there are no parameters to tune. The algorithm\nmixes faster and is more efficient than Gibbs sampling. The runtime depends on\nthe number and shape of the constraints but the algorithm is highly\nparallelizable. In many cases, we can exploit special structure in the\ncovariance matrices of the untruncated Gaussian to further speed up the\nruntime. A simple extension of the algorithm permits sampling from\ndistributions whose log-density is piecewise quadratic, as in the \"Bayesian\nLasso\" model.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2012 20:40:22 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2013 15:07:33 GMT"}, {"version": "v3", "created": "Wed, 5 Jun 2013 19:07:54 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Pakman", "Ari", ""], ["Paninski", "Liam", ""]]}, {"id": "1208.4275", "submitter": "Xin Gao Dr.", "authors": "Xin Gao and Helene Massam", "title": "Composite likelihood estimation of sparse Gaussian graphical models with\n  symmetry", "comments": "1 figure 4 tables 28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss the composite likelihood estimation of sparse\nGaussian graphical models. When there are symmetry constraints on the\nconcentration matrix or partial correlation matrix, the likelihood estimation\ncan be computational intensive. The composite likelihood offers an alternative\nformulation of the objective function and yields consistent estimators. When a\nsparse model is considered, the penalized composite likelihood estimation can\nyield estimates satisfying both the symmetry and sparsity constraints and\npossess ORACLE property. Application of the proposed method is demonstrated\nthrough simulation studies and a network analysis of a biological data set.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2012 14:20:05 GMT"}], "update_date": "2012-08-22", "authors_parsed": [["Gao", "Xin", ""], ["Massam", "Helene", ""]]}, {"id": "1208.4818", "submitter": "Vinayak Rao", "authors": "Vinayak Rao, Yee Whye Teh", "title": "Fast MCMC sampling for Markov jump processes and extensions", "comments": "Accepted at the Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov jump processes (or continuous-time Markov chains) are a simple and\nimportant class of continuous-time dynamical systems. In this paper, we tackle\nthe problem of simulating from the posterior distribution over paths in these\nmodels, given partial and noisy observations. Our approach is an auxiliary\nvariable Gibbs sampler, and is based on the idea of uniformization. This sets\nup a Markov chain over paths by alternately sampling a finite set of virtual\njump times given the current path and then sampling a new path given the set of\nextant and virtual jump times using a standard hidden Markov model forward\nfiltering-backward sampling algorithm. Our method is exact and does not involve\napproximations like time-discretization. We demonstrate how our sampler extends\nnaturally to MJP-based models like Markov-modulated Poisson processes and\ncontinuous-time Bayesian networks and show significant computational benefits\nover state-of-the-art MCMC samplers for these models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2012 18:07:49 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2012 02:12:20 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2013 20:03:03 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Rao", "Vinayak", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1208.5595", "submitter": "Manuel Koller", "authors": "Manuel Koller", "title": "Nonsingular subsampling for S-estimators with categorical predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An integral part of many algorithms for S-estimators of linear regression is\nrandom subsampling. For problems with only continuous predictors simple random\nsubsampling is a reliable method to generate initial coefficient estimates that\ncan then be further refined. For data with categorical predictors, however,\nrandom subsampling often does not work, thus limiting the use of an otherwise\nfine estimator. This also makes the choice of estimator for robust linear\nregression dependent on the type of predictors, which is an unnecessary\nnuisance in practice. For data with categorical predictors random subsampling\noften generates singular subsamples. Since these subsamples cannot be used to\ncalculate coefficient estimates, they have to be discarded. This makes random\nsubsampling slow, especially if some levels of categorical predictors have low\nfrequency, and renders the algorithms infeasible for such problems. This paper\nintroduces an improved subsampling algorithm that only generates nonsingular\nsubsamples. We call it nonsingular subsampling. For data with continuous\nvariables it is as fast as simple random subsampling but much faster for data\nwith categorical predictors. This is achieved by using a modified LU\ndecomposition algorithm that combines the generation of a sample and the\nsolving of the least squares problem.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 09:23:01 GMT"}], "update_date": "2012-08-29", "authors_parsed": [["Koller", "Manuel", ""]]}, {"id": "1208.5600", "submitter": "Joaquin Miguez", "authors": "Eugenia Koblents and Joaqu\\'in M\\'iguez", "title": "A population Monte Carlo scheme with transformed weights and its\n  application to stochastic kinetic models", "comments": "35 pages, 8 figures", "journal-ref": "Statistics and Computing, 25(2), pp. 407-425, 2015", "doi": "10.1007/s11222-013-9440-2", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of Monte Carlo approximation of posterior\nprobability distributions. In particular, we have considered a recently\nproposed technique known as population Monte Carlo (PMC), which is based on an\niterative importance sampling approach. An important drawback of this\nmethodology is the degeneracy of the importance weights when the dimension of\neither the observations or the variables of interest is high. To alleviate this\ndifficulty, we propose a novel method that performs a nonlinear transformation\non the importance weights. This operation reduces the weight variation, hence\nit avoids their degeneracy and increases the efficiency of the importance\nsampling scheme, specially when drawing from a proposal functions which are\npoorly adapted to the true posterior.\n  For the sake of illustration, we have applied the proposed algorithm to the\nestimation of the parameters of a Gaussian mixture model. This is a very simple\nproblem that enables us to clearly show and discuss the main features of the\nproposed technique. As a practical application, we have also considered the\npopular (and challenging) problem of estimating the rate parameters of\nstochastic kinetic models (SKM). SKMs are highly multivariate systems that\nmodel molecular interactions in biological and chemical problems. We introduce\na particularization of the proposed algorithm to SKMs and present numerical\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2012 09:39:35 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Koblents", "Eugenia", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1208.6550", "submitter": "Luis David Garcia-Puente", "authors": "Luis David Garc\\'ia-Puente, Sonja Petrovi\\'c and Seth Sullivant", "title": "Graphical models in Macaulay2", "comments": "Several changes to address referee comments and suggestions. We will\n  eventually include this package in the standard distribution of Macaulay2.\n  But until then, the associated Macaulay2 file can be found at\n  http://www.shsu.edu/~ldg005/papers.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Macaulay2 package GraphicalModels contains algorithms for the algebraic\nstudy of graphical models associated to undirected, directed and mixed graphs,\nand associated collections of conditional independence statements. Among the\nalgorithms implemented are procedures for computing the vanishing ideal of\ngraphical models, for generating conditional independence ideals of families of\nindependence statements associated to graphs, and for checking for identifiable\nparameters in Gaussian mixed graph models. These procedures can be used to\nstudy fundamental problems about graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2012 16:54:48 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 20:32:46 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2013 19:11:45 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Garc\u00eda-Puente", "Luis David", ""], ["Petrovi\u0107", "Sonja", ""], ["Sullivant", "Seth", ""]]}]