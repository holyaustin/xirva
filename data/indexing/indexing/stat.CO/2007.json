[{"id": "2007.00096", "submitter": "Suzie Brown", "authors": "Suzie Brown, Paul A. Jenkins, Adam M. Johansen, Jere Koskela", "title": "Simple conditions for convergence of sequential Monte Carlo genealogies\n  with applications", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple conditions under which the limiting genealogical process\nassociated with a class of interacting particle systems with non-neutral\nselection mechanisms, as the number of particles grows, is a time-rescaled\nKingman coalescent. Sequential Monte Carlo algorithms are popular methods for\napproximating integrals in problems such as non-linear filtering and smoothing\nwhich employ this type of particle system. Their performance depends strongly\non the properties of the induced genealogical process. We verify the conditions\nof our main result for standard sequential Monte Carlo algorithms with a broad\nclass of low-variance resampling schemes, as well as for conditional sequential\nMonte Carlo with multinomial resampling.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 20:28:02 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 13:08:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Brown", "Suzie", ""], ["Jenkins", "Paul A.", ""], ["Johansen", "Adam M.", ""], ["Koskela", "Jere", ""]]}, {"id": "2007.00180", "submitter": "Hamed Nikbakht", "authors": "Konstantinos G. Papakonstantinou and Hamed Nikbakht", "title": "Hamiltonian MCMC methods for estimating rare events probabilities in\n  high-dimensional problems", "comments": "arXiv admin note: text overlap with arXiv:1909.03575", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and efficient estimation of rare events probabilities is of\nsignificant importance, since often the occurrences of such events have\nwidespread impacts. The focus in this work is on precisely quantifying these\nprobabilities, often encountered in reliability analysis of complex engineering\nsystems, based on an introduced framework termed Approximate Sampling Target\nwith Post-processing Adjustment (ASTPA), which herein is integrated with and\nsupported by gradient-based Hamiltonian Markov Chain Monte Carlo (HMCMC)\nmethods. The basic idea is to construct a relevant target distribution by\nweighting the high-dimensional random variable space through a one-dimensional\noutput likelihood model, using the limit-state function. To sample from this\ntarget distribution, we exploit HMCMC algorithms, a family of MCMC methods that\nadopts physical system dynamics, rather than solely using a proposal\nprobability distribution, to generate distant sequential samples, and we\ndevelop a new Quasi-Newton mass preconditioned HMCMC scheme (QNp-HMCMC), which\nis particularly efficient and suitable for high-dimensional spaces. To\neventually compute the rare event probability, an original post-sampling step\nis devised using an inverse importance sampling procedure based on the already\nobtained samples. The statistical properties of the estimator are analyzed as\nwell, and the performance of the proposed methodology is examined in detail and\ncompared against Subset Simulation in a series of challenging low- and\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 01:56:51 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Papakonstantinou", "Konstantinos G.", ""], ["Nikbakht", "Hamed", ""]]}, {"id": "2007.00187", "submitter": "Yi Liu", "authors": "Yi Liu and Veronika Rockova", "title": "Variable Selection via Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling is a heuristic algorithm for the multi-armed bandit problem\nwhich has a long tradition in machine learning. The algorithm has a Bayesian\nspirit in the sense that it selects arms based on posterior samples of reward\nprobabilities of each arm. By forging a connection between combinatorial binary\nbandits and spike-and-slab variable selection, we propose a stochastic\noptimization approach to subset selection called Thompson Variable Selection\n(TVS). TVS is a framework for interpretable machine learning which does not\nrely on the underlying model to be linear. TVS brings together Bayesian\nreinforcement and machine learning in order to extend the reach of Bayesian\nsubset selection to non-parametric models and large datasets with very many\npredictors and/or very many observations. Depending on the choice of a reward,\nTVS can be deployed in offline as well as online setups with streaming data\nbatches. Tailoring multiplay bandits to variable selection, we provide regret\nbounds without necessarily assuming that the arm mean rewards be unrelated. We\nshow a very strong empirical performance on both simulated and real data.\nUnlike deterministic optimization methods for spike-and-slab variable\nselection, the stochastic nature makes TVS less prone to local convergence and\nthereby more robust.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 02:22:53 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 19:31:04 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Liu", "Yi", ""], ["Rockova", "Veronika", ""]]}, {"id": "2007.00248", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Andrew Zammit-Mangion", "title": "Non-Homogeneous Poisson Process Intensity Modeling and Estimation using\n  Measure Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-homogeneous Poisson processes are used in a wide range of scientific\ndisciplines, ranging from the environmental sciences to the health sciences.\nOften, the central object of interest in a point process is the underlying\nintensity function. Here, we present a general model for the intensity function\nof a non-homogeneous Poisson process using measure transport. The model is\nbuilt from a flexible bijective mapping that maps from the underlying intensity\nfunction of interest to a simpler reference intensity function. We enforce\nbijectivity by modeling the map as a composition of multiple simple bijective\nmaps, and show that the model exhibits an important approximation property.\nEstimation of the flexible mapping is accomplished within an optimization\nframework, wherein computations are efficiently done using recent technological\nadvances in deep learning and a graphics processing unit. Although we find that\nintensity function estimates obtained with our method are not necessarily\nsuperior to those obtained using conventional methods, the modeling\nrepresentation brings with it other advantages such as facilitated point\nprocess simulation and uncertainty quantification. Modeling point processes in\nhigher dimensions is also facilitated using our approach. We illustrate the use\nof our model on both simulated data, and a real data set containing the\nlocations of seismic events near Fiji since 1964.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 05:16:57 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Zammit-Mangion", "Andrew", ""]]}, {"id": "2007.00400", "submitter": "Mikkel Bue Lykkegaard", "authors": "Mikkel B. Lykkegaard, Tim J. Dodwell, David Moxey", "title": "Accelerating Uncertainty Quantification of Groundwater Flow Modelling\n  Using a Deep Neural Network Proxy", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.cma.2021.113895", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the uncertainty in model parameters and output is a critical\ncomponent in model-driven decision support systems for groundwater management.\nThis paper presents a novel algorithmic approach which fuses Markov Chain Monte\nCarlo (MCMC) and Machine Learning methods to accelerate uncertainty\nquantification for groundwater flow models. We formulate the governing\nmathematical model as a Bayesian inverse problem, considering model parameters\nas a random process with an underlying probability distribution. MCMC allows us\nto sample from this distribution, but it comes with some limitations: it can be\nprohibitively expensive when dealing with costly likelihood functions,\nsubsequent samples are often highly correlated, and the standard\nMetropolis-Hastings algorithm suffers from the curse of dimensionality. This\npaper designs a Metropolis-Hastings proposal which exploits a deep neural\nnetwork (DNN) approximation of a groundwater flow model, to significantly\naccelerate MCMC sampling. We modify a delayed acceptance (DA) model hierarchy,\nwhereby proposals are generated by running short subchains using an inexpensive\nDNN approximation, resulting in a decorrelation of subsequent fine model\nproposals. Using a simple adaptive error model, we estimate and correct the\nbias of the DNN approximation with respect to the posterior distribution\non-the-fly. The approach is tested on two synthetic examples; a isotropic\ntwo-dimensional problem, and an anisotropic three-dimensional problem. The\nresults show that the cost of uncertainty quantification can be reduced by up\nto 50% compared to single-level MCMC, depending on the precomputation cost and\naccuracy of the employed DNN.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:40:31 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 18:31:57 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Lykkegaard", "Mikkel B.", ""], ["Dodwell", "Tim J.", ""], ["Moxey", "David", ""]]}, {"id": "2007.00402", "submitter": "Christian Agrell", "authors": "Christian Agrell and Kristina Rognlien Dahl", "title": "Sequential Bayesian optimal experimental design for structural\n  reliability analysis", "comments": "27 pages, 13 figures", "journal-ref": "Statistics and Computing, vol. 31, no. 27 (2021)", "doi": "10.1007/s11222-021-10000-2", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural reliability analysis is concerned with estimation of the\nprobability of a critical event taking place, described by $P(g(\\textbf{X})\n\\leq 0)$ for some $n$-dimensional random variable $\\textbf{X}$ and some\nreal-valued function $g$. In many applications the function $g$ is practically\nunknown, as function evaluation involves time consuming numerical simulation or\nsome other form of experiment that is expensive to perform. The problem we\naddress in this paper is how to optimally design experiments, in a Bayesian\ndecision theoretic fashion, when the goal is to estimate the probability\n$P(g(\\textbf{X}) \\leq 0)$ using a minimal amount of resources. As opposed to\nexisting methods that have been proposed for this purpose, we consider a\ngeneral structural reliability model given in hierarchical form. We therefore\nintroduce a general formulation of the experimental design problem, where we\ndistinguish between the uncertainty related to the random variable $\\textbf{X}$\nand any additional epistemic uncertainty that we want to reduce through\nexperimentation. The effectiveness of a design strategy is evaluated through a\nmeasure of residual uncertainty, and efficient approximation of this quantity\nis crucial if we want to apply algorithms that search for an optimal strategy.\nThe method we propose is based on importance sampling combined with the\nunscented transform for epistemic uncertainty propagation. We implement this\nfor the myopic (one-step look ahead) alternative, and demonstrate the\neffectiveness through a series of numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 11:47:15 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Agrell", "Christian", ""], ["Dahl", "Kristina Rognlien", ""]]}, {"id": "2007.00483", "submitter": "Ryuki Suzuki", "authors": "Ryuki Suzuki, Ryosuke Kataoka, Yonghoon Ji, Hiromitsu Fujii, Hitoshi\n  Kono and Kazunori Umeda", "title": "SLAM using ICP and graph optimization considering physical properties of\n  environment", "comments": "5 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel SLAM (simultaneous localization and mapping)\nscheme based on scan matching in an environment including various physical\nproperties.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 13:39:13 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Suzuki", "Ryuki", ""], ["Kataoka", "Ryosuke", ""], ["Ji", "Yonghoon", ""], ["Fujii", "Hiromitsu", ""], ["Kono", "Hitoshi", ""], ["Umeda", "Kazunori", ""]]}, {"id": "2007.00596", "submitter": "Fan Chen", "authors": "Fan Chen and Karl Rohe", "title": "A New Basis for Sparse PCA", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The statistical and computational performance of sparse principal component\nanalysis (PCA) can be dramatically improved when the principal components are\nallowed to be sparse in a rotated eigenbasis. For this, we propose a new method\nfor sparse PCA. In the simplest version of the algorithm, the component scores\nand loadings are initialized with a low-rank singular value decomposition.\nThen, the singular vectors are rotated with orthogonal rotations to make them\napproximately sparse. Finally, soft-thresholding is applied to the rotated\nsingular vectors. This approach differs from prior approaches because it uses\nan orthogonal rotation to approximate a sparse basis. Our sparse PCA framework\nis versatile; for example, it extends naturally to the two-way analysis of a\ndata matrix for simultaneous dimensionality reduction of rows and columns. We\nidentify the close relationship between sparse PCA and independent component\nanalysis for separating sparse signals. We provide empirical evidence showing\nthat for the same level of sparsity, the proposed sparse PCA method is more\nstable and can explain more variance compared to alternative methods. Through\nthree applications---sparse coding of images, analysis of transcriptome\nsequencing data, and large-scale clustering of Twitter accounts, we demonstrate\nthe usefulness of sparse PCA in exploring modern multivariate data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 16:32:22 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Chen", "Fan", ""], ["Rohe", "Karl", ""]]}, {"id": "2007.00715", "submitter": "Jacky Zhang", "authors": "Jacky Y. Zhang, Rajiv Khanna, Anastasios Kyrillidis, Oluwasanmi Koyejo", "title": "Bayesian Coresets: Revisiting the Nonconvex Optimization Perspective", "comments": "AISTATS 2021 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian coresets have emerged as a promising approach for implementing\nscalable Bayesian inference. The Bayesian coreset problem involves selecting a\n(weighted) subset of the data samples, such that the posterior inference using\nthe selected subset closely approximates the posterior inference using the full\ndataset. This manuscript revisits Bayesian coresets through the lens of\nsparsity constrained optimization. Leveraging recent advances in accelerated\noptimization methods, we propose and analyze a novel algorithm for coreset\nselection. We provide explicit convergence rate guarantees and present an\nempirical evaluation on a variety of benchmark datasets to highlight our\nproposed algorithm's superior performance compared to state-of-the-art on speed\nand accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jul 2020 19:34:59 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:04:24 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Zhang", "Jacky Y.", ""], ["Khanna", "Rajiv", ""], ["Kyrillidis", "Anastasios", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "2007.00980", "submitter": "Caio Beojone", "authors": "Caio Vitor Beojone and Nikolas Geroliminis (\\'Ecole Polytechnique\n  F\\'ed\\'erale de Lausanne)", "title": "On the inefficiency of ride-sourcing services towards urban congestion", "comments": "Submitted to Transportation Research Part C", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SY eess.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of shared-economy and smartphones made on-demand transportation\nservices possible, which created additional opportunities, but also more\ncomplexity to urban mobility. Companies that offer these services are called\nTransportation Network Companies (TNCs) due to their internet-based nature.\nAlthough ride-sourcing is the most notorious service TNCs provide, little is\nknown about to what degree its operations can interfere in traffic conditions,\nwhile replacing other transportation modes, or when a large number of idle\nvehicles is cruising for passengers. We experimentally analyze the efficiency\nof TNCs using taxi trip data from a Chinese megacity and a agent-based\nsimulation with a trip-based MFD model for determining the speed. We\ninvestigate the effect of expanding fleet sizes for TNCs, passengers'\ninclination towards sharing rides, and strategies to alleviate urban\ncongestion. We show that the lack of coordination of objectives between TNCs\nand society can create 37% longer travel times and significant congestion.\nMoreover, allowing shared rides is not capable of decreasing total distance\ntraveled due to higher empty kilometers traveled. Elegant parking management\nstrategies can prevent idle vehicles from cruising without assigned passengers\nand lower to 7% the impacts of the absence of coordination.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 09:29:07 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Beojone", "Caio Vitor", "", "\u00c9cole Polytechnique\n  F\u00e9d\u00e9rale de Lausanne"], ["Geroliminis", "Nikolas", "", "\u00c9cole Polytechnique\n  F\u00e9d\u00e9rale de Lausanne"]]}, {"id": "2007.00996", "submitter": "Bruno Sudret", "authors": "X. Zhu and B. Sudret", "title": "Emulation of stochastic simulators using generalized lambda models", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2020-006B", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulators are ubiquitous in many fields of applied sciences and\nengineering. In the context of uncertainty quantification and optimization, a\nlarge number of simulations are usually necessary, which become intractable for\nhigh-fidelity models. Thus surrogate models of stochastic simulators have been\nintensively investigated in the last decade. In this paper, we present a novel\napproach to surrogate the response distribution of a stochastic simulator which\nuses generalized lambda distributions, whose parameters are represented by\npolynomial chaos expansions of the model inputs. As opposed to most existing\napproaches, this new method does not require replicated runs of the simulator\nat each point of the experimental design. We propose a new fitting procedure\nwhich combines maximum conditional likelihood estimation with (modified)\nfeasible generalized least-squares. We compare our method with state-of-the-art\nnonparametric kernel estimation on four different applications stemming from\nmathematical finance and epidemiology. Its performance is illustrated in terms\nof the accuracy of both the mean/variance of the stochastic simulator and the\nresponse distribution. As the proposed approach can also be used with\nexperimental designs containing replications, we carry out a comparison on two\nof the examples, that show that replications do not help to get a better\noverall accuracy, and may even worsen the results (at fixed total number of\nruns of the simulator).\n", "versions": [{"version": "v1", "created": "Thu, 2 Jul 2020 10:08:34 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 16:27:32 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Zhu", "X.", ""], ["Sudret", "B.", ""]]}, {"id": "2007.01485", "submitter": "Jacob Priddle", "authors": "Jacob W. Priddle, and Christopher Drovandi", "title": "Transformations in Semi-Parametric Bayesian Synthetic Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is a popular method for performing\napproximate Bayesian inference when the likelihood function is intractable. In\nsynthetic likelihood methods, the likelihood function is approximated\nparametrically via model simulations, and then standard likelihood-based\ntechniques are used to perform inference. The Gaussian synthetic likelihood\nestimator has become ubiquitous in BSL literature, primarily for its simplicity\nand ease of implementation. However, it is often too restrictive and may lead\nto poor posterior approximations. Recently, a more flexible semi-parametric\nBayesian synthetic likelihood (semiBSL) estimator has been introduced, which is\nsignificantly more robust to irregularly distributed summary statistics. In\nthis work, we propose a number of extensions to semiBSL. First, we consider\neven more flexible estimators of the marginal distributions using\ntransformation kernel density estimation. Second, we propose whitening semiBSL\n(wsemiBSL) -- a method to significantly improve the computational efficiency of\nsemiBSL. wsemiBSL uses an approximate whitening transformation to decorrelate\nsummary statistics at each algorithm iteration. The methods developed herein\nsignificantly improve the versatility and efficiency of BSL algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 03:58:53 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Priddle", "Jacob W.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "2007.01578", "submitter": "Apostolos Chalkis", "authors": "Apostolos Chalkis, Vissarion Fisikopoulos", "title": "volesti: Volume Approximation and Sampling for Convex Polytopes in R", "comments": "44 pages, 13 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from high dimensional distributions and volume approximation of\nconvex bodies are fundamental operations that appear in optimization, finance,\nengineering and machine learning. In this paper we present volesti, a C++\npackage with an R interface that provides efficient, scalable algorithms for\nvolume estimation, uniform and Gaussian sampling from convex polytopes. volesti\nscales to hundreds of dimensions, handles efficiently three different types of\npolyhedra and provides non existing sampling routines to R. We demonstrate the\npower of volesti by solving several challenging problems using the R language.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 09:47:14 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 08:19:28 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Chalkis", "Apostolos", ""], ["Fisikopoulos", "Vissarion", ""]]}, {"id": "2007.01958", "submitter": "Alon Kipnis", "authors": "David L. Donoho and Alon Kipnis", "title": "Higher Criticism to Compare Two Large Frequency Tables, with sensitivity\n  to Possible Rare and Weak Differences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt Higher Criticism (HC) to the comparison of two frequency tables\nwhich may -- or may not -- exhibit moderate differences between the tables in\nsome unknown, relatively small subset out of a large number of categories.\n  Our analysis of the power of the proposed HC test quantifies the rarity and\nsize of assumed differences and applies moderate deviations-analysis to\ndetermine the asymptotic powerfulness/powerlessness of our proposed HC\nprocedure. Our analysis considers the null hypothesis of no difference in\nunderlying generative model against a rare/weak perturbation alternative, in\nwhich the frequencies of $N^{1-\\beta}$ out of the $N$ categories are perturbed\nby $r(\\log N)/2n$ in the Hellinger distance; here $n$ is the size of each\nsample. Our proposed Higher Criticism (HC) test \\newtext{for} this setting uses\nP-values obtained from $N$ exact binomial tests. We characterize the asymptotic\nperformance of the HC-based test in terms of the sparsity parameter $\\beta$ and\nthe perturbation intensity parameter $r$. Specifically, we derive a region in\nthe $(\\beta,r)$-plane where the test asymptotically has maximal power, while\nhaving asymptotically no power outside this region. Our analysis distinguishes\nbetween cases in which the counts in both tables are low, versus cases in which\ncounts are high, corresponding to the cases of sparse and dense frequency\ntables. The phase transition curve of HC in the high-counts regime matches\nformally the curve delivered by HC in a two-sample normal means model.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jul 2020 22:38:28 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 18:53:38 GMT"}, {"version": "v3", "created": "Mon, 9 Nov 2020 22:52:08 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 07:41:46 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Donoho", "David L.", ""], ["Kipnis", "Alon", ""]]}, {"id": "2007.02006", "submitter": "Zehong Zhang", "authors": "Zehong Zhang, Fei Lu", "title": "Cluster Prediction for Opinion Dynamics from Partial Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian approach to predict the clustering of opinions for a\nsystem of interacting agents from partial observations. The Bayesian\nformulation overcomes the unobservability of the system and quantifies the\nuncertainty in the prediction. We characterize the clustering by the posterior\nof the clusters' sizes and centers, and we represent the posterior by samples.\nTo overcome the challenge in sampling the high-dimensional posterior, we\nintroduce an auxiliary implicit sampling (AIS) algorithm using two-step\nobservations. Numerical results show that the AIS algorithm leads to accurate\npredictions of the sizes and centers for the leading clusters, in both cases of\nnoiseless and noisy observations. In particular, the centers are predicted with\nhigh success rates, but the sizes exhibit a considerable uncertainty that is\nsensitive to observation noise and the observation ratio.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 04:00:29 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 02:11:15 GMT"}, {"version": "v3", "created": "Sun, 20 Dec 2020 03:46:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Zehong", ""], ["Lu", "Fei", ""]]}, {"id": "2007.02156", "submitter": "Cong Mu", "authors": "Cong Mu, Angelo Mele, Lingxin Hao, Joshua Cape, Avanti Athreya, Carey\n  E. Priebe", "title": "On spectral algorithms for community detection in stochastic blockmodel\n  graphs with vertex covariates", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In network inference applications, it is often desirable to detect community\nstructure, namely to cluster vertices into groups, or blocks, according to some\nmeasure of similarity. Beyond mere adjacency matrices, many real networks also\ninvolve vertex covariates that carry key information about underlying block\nstructure in graphs. To assess the effects of such covariates on block\nrecovery, we present a comparative analysis of two model-based spectral\nalgorithms for clustering vertices in stochastic blockmodel graphs with vertex\ncovariates. The first algorithm uses only the adjacency matrix, and directly\nestimates the induced block assignments. The second algorithm incorporates both\nthe adjacency matrix and the vertex covariates into the estimation of block\nassignments, and moreover quantifies the explicit impact of the vertex\ncovariates on the resulting estimate of the block assignments. We employ\nChernoff information to analytically compare the algorithms' performance and\nderive the Chernoff ratio for certain models of interest. Analytic results and\nsimulations suggest that the second algorithm is often preferred: we can often\nbetter estimate the induced block assignments by first estimating the effect of\nvertex covariates. In addition, real data examples on diffusion MRI connectome\ndatasets and social network datasets also indicate that the second algorithm\nhas the advantages of revealing underlying block structure and taking observed\nvertex heterogeneity into account in real applications. Our findings emphasize\nthe importance of distinguishing between observed and unobserved factors that\ncan affect block structure in graphs.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 18:22:22 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 00:31:53 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Mu", "Cong", ""], ["Mele", "Angelo", ""], ["Hao", "Lingxin", ""], ["Cape", "Joshua", ""], ["Athreya", "Avanti", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2007.02192", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee, Debdeep Pati, Bani K. Mallick", "title": "Tail-adaptive Bayesian shrinkage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern genomic studies are increasingly focused on discovering more and more\ninteresting genes associated with a health response. Traditional shrinkage\npriors are primarily designed to detect a handful of signals from tens and\nthousands of predictors. Under diverse sparsity regimes, the nature of signal\ndetection is associated with a tail behaviour of a prior. A desirable tail\nbehaviour is called tail-adaptive shrinkage property where tail-heaviness of a\nprior gets adaptively larger (or smaller) as a sparsity level increases (or\ndecreases) to accommodate more (or less) signals. We propose a\nglobal-local-tail (GLT) Gaussian mixture distribution to ensure this property\nand provide accurate inference under diverse sparsity regimes. Incorporating a\npeaks-over-threshold method in extreme value theory, we develop an automated\ntail learning algorithm for the GLT prior. We compare the performance of the\nGLT prior to the Horseshoe in two gene expression datasets and numerical\nexamples. Results suggest that varying tail rule is advantageous over fixed\ntail rule under diverse sparsity domains.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jul 2020 21:40:12 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2021 03:47:49 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lee", "Se Yoon", ""], ["Pati", "Debdeep", ""], ["Mallick", "Bani K.", ""]]}, {"id": "2007.02310", "submitter": "Zhongyi Hu", "authors": "Zhongyi Hu, Robin Evans", "title": "Faster algorithms for Markov equivalence", "comments": "Accepted", "journal-ref": "36th Conference on Uncertainty in Artificial Intelligence (UAI),\n  2020", "doi": null, "report-no": null, "categories": "math.CO math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximal ancestral graphs (MAGs) have many desirable properties; in particular\nthey can fully describe conditional independences from directed acyclic graphs\n(DAGs) in the presence of latent and selection variables. However, different\nMAGs may encode the same conditional independences, and are said to be\n\\emph{Markov equivalent}. Thus identifying necessary and sufficient conditions\nfor equivalence is essential for structure learning. Several criteria for this\nalready exist, but in this paper we give a new non-parametric characterization\nin terms of the heads and tails that arise in the parameterization for discrete\nmodels. We also provide a polynomial time algorithm ($O(ne^{2})$, where $n$ and\n$e$ are the number of vertices and edges respectively) to verify equivalence.\nMoreover, we extend our criterion to ADMGs and summary graphs and propose an\nalgorithm that converts an ADMG or summary graph to an equivalent MAG in\npolynomial time ($O(n^{2}e)$). Hence by combining both algorithms, we can also\nverify equivalence between two summary graphs or ADMGs.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 12:24:19 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Hu", "Zhongyi", ""], ["Evans", "Robin", ""]]}, {"id": "2007.02392", "submitter": "Alkis Kalavasis", "authors": "Dimitris Fotakis, Alkis Kalavasis, Christos Tzamos", "title": "Efficient Parameter Estimation of Truncated Boolean Product\n  Distributions", "comments": "33 pages, 33rd Conference on Learning Theory (COLT 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the parameters of a Boolean product\ndistribution in $d$ dimensions, when the samples are truncated by a set $S\n\\subset \\{0, 1\\}^d$ accessible through a membership oracle. This is the first\ntime that the computational and statistical complexity of learning from\ntruncated samples is considered in a discrete setting.\n  We introduce a natural notion of fatness of the truncation set $S$, under\nwhich truncated samples reveal enough information about the true distribution.\nWe show that if the truncation set is sufficiently fat, samples from the true\ndistribution can be generated from truncated samples. A stunning consequence is\nthat virtually any statistical task (e.g., learning in total variation\ndistance, parameter estimation, uniformity or identity testing) that can be\nperformed efficiently for Boolean product distributions, can also be performed\nfrom truncated samples, with a small increase in sample complexity. We\ngeneralize our approach to ranking distributions over $d$ alternatives, where\nwe show how fatness implies efficient parameter estimation of Mallows models\nfrom truncated samples.\n  Exploring the limits of learning discrete models from truncated samples, we\nidentify three natural conditions that are necessary for efficient\nidentifiability: (i) the truncation set $S$ should be rich enough; (ii) $S$\nshould be accessible through membership queries; and (iii) the truncation by\n$S$ should leave enough randomness in all directions. By carefully adapting the\nStochastic Gradient Descent approach of (Daskalakis et al., FOCS 2018), we show\nthat these conditions are also sufficient for efficient learning of truncated\nBoolean product distributions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 17:20:39 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Fotakis", "Dimitris", ""], ["Kalavasis", "Alkis", ""], ["Tzamos", "Christos", ""]]}, {"id": "2007.03031", "submitter": "Barry Loneck PhD", "authors": "Barry Loneck and Igor Zurbenko", "title": "Theoretical and Practical Limits of Kolmogorov-Zurbenko Periodograms\n  with DiRienzo-Zurbenko Algorithm Smoothing in the Spectral Analysis of Time\n  Series Data", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kolomogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm\nsmoothing is the state-of-the-art method for spectral analysis of time series\ndata. Because this approach assumes that a sinusoidal model underlies\ntime-series data and because its algorithms are adaptive in nature, it is\nsuperior to traditional use of autoregressive integral moving average (ARIMA)\nalgorithms. This article begins with a presentation of its statistical\nderivation and development followed by instructions for accessing and utilizing\nthis approach within the R statistical program platform. The discussion then\nturns to a presentation of its theoretical and practical limits with regard to\nsensitivity (i.e., ability to detect weak signals), accuracy (i.e., ability to\ncorrectly identify signal frequencies), resolution (i.e., ability to resolve\nsignals with close frequencies), and robustness with respect to missing data\n(i.e., sensitivity and accuracy despite high levels of missingness). Next using\na simulated time series in which two signals close in frequency are embedded in\nsignificant amounts of random noise, the predictive power of this approach is\ncompared to the traditional ARIMA approach, with support also garnered for its\nbeing robust even in the face of significant levels of missing data. The\narticle concludes with brief descriptions of studies across a range of\nscientific disciplines that have capitalized on the power of the\nKolmogorov-Zurbenko periodogram with DiRienzo-Zurbenko algorithm smoothing.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jul 2020 19:28:55 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Loneck", "Barry", ""], ["Zurbenko", "Igor", ""]]}, {"id": "2007.03238", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, P. Fearnhead", "title": "Innovative And Additive Outlier Robust Kalman Filtering With A Robust\n  Particle Filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose CE-BASS, a particle mixture Kalman filter which is\nrobust to both innovative and additive outliers, and able to fully capture\nmulti-modality in the distribution of the hidden state. Furthermore, the\nparticle sampling approach re-samples past states, which enables CE-BASS to\nhandle innovative outliers which are not immediately visible in the\nobservations, such as trend changes. The filter is computationally efficient as\nwe derive new, accurate approximations to the optimal proposal distributions\nfor the particles. The proposed algorithm is shown to compare well with\nexisting approaches and is applied to both machine temperature and server data.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 07:11:09 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "P.", ""]]}, {"id": "2007.03303", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Simon N. Wood, Margaux Zaffran, Rapha\\\"el Nedellec,\n  Yannig Goude", "title": "qgam: Bayesian non-parametric quantile regression modelling in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) are flexible non-linear regression models,\nwhich can be fitted efficiently using the approximate Bayesian methods provided\nby the mgcv R package. While the GAM methods provided by mgcv are based on the\nassumption that the response distribution is modelled parametrically, here we\ndiscuss more flexible methods that do not entail any parametric assumption. In\nparticular, this article introduces the qgam package, which is an extension of\nmgcv providing fast calibrated Bayesian methods for fitting quantile GAMs\n(QGAMs) in R. QGAMs are based on a smooth version of the pinball loss of\nKoenker (2005), rather than on a likelihood function, hence jointly achieving\nsatisfactory accuracy of the quantile point estimates and coverage of the\ncorresponding credible intervals requires adopting the specialized Bayesian\nfitting framework of Fasiolo, Wood, Zaffran, Nedellec, and Goude (2020b). Here\nwe detail how this framework is implemented in qgam and we provide examples\nillustrating how the package should be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 09:32:11 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Wood", "Simon N.", ""], ["Zaffran", "Margaux", ""], ["Nedellec", "Rapha\u00ebl", ""], ["Goude", "Yannig", ""]]}, {"id": "2007.03722", "submitter": "David Ginsbourger", "authors": "Trygve Olav Fossum, C\\'edric Travelletti, Jo Eidsvik, David\n  Ginsbourger, Kanna Rajan", "title": "Learning excursion sets of vector-valued Gaussian random fields for\n  autonomous ocean sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving and optimizing oceanographic sampling is a crucial task for marine\nscience and maritime resource management. Faced with limited resources in\nunderstanding processes in the water-column, the combination of statistics and\nautonomous systems provide new opportunities for experimental design. In this\nwork we develop efficient spatial sampling methods for characterizing regions\ndefined by simultaneous exceedances above prescribed thresholds of several\nresponses, with an application focus on mapping coastal ocean phenomena based\non temperature and salinity measurements. Specifically, we define a design\ncriterion based on uncertainty in the excursions of vector-valued Gaussian\nrandom fields, and derive tractable expressions for the expected integrated\nBernoulli variance reduction in such a framework. We demonstrate how this\ncriterion can be used to prioritize sampling efforts at locations that are\nambiguous, making exploration more effective. We use simulations to study and\ncompare properties of the considered approaches, followed by results from field\ndeployments with an autonomous underwater vehicle as part of a study mapping\nthe boundary of a river plume. The results demonstrate the potential of\ncombining statistical methods and robotic platforms to effectively inform and\nexecute data-driven environmental sampling.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jul 2020 18:23:46 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 13:32:27 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Fossum", "Trygve Olav", ""], ["Travelletti", "C\u00e9dric", ""], ["Eidsvik", "Jo", ""], ["Ginsbourger", "David", ""], ["Rajan", "Kanna", ""]]}, {"id": "2007.03833", "submitter": "Patrick J. Laub", "authors": "Pierre-Olivier Goffard and Patrick J. Laub", "title": "Approximate Bayesian Computations to fit and compare insurance loss\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a statistical learning technique to\ncalibrate and select models by comparing observed data to simulated data. This\ntechnique bypasses the use of the likelihood and requires only the ability to\ngenerate synthetic data from the models of interest. We apply ABC to fit and\ncompare insurance loss models using aggregated data. A state-of-the-art ABC\nimplementation in Python is proposed. It uses sequential Monte Carlo to sample\nfrom the posterior distribution and the Wasserstein distance to compare the\nobserved and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 00:41:34 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 03:41:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Goffard", "Pierre-Olivier", ""], ["Laub", "Patrick J.", ""]]}, {"id": "2007.03942", "submitter": "Bruno Sudret", "authors": "H. M. Kroetz, M. Moustapha, A.T. Beck and B. Sudret", "title": "A two-level Kriging-based approach with active learning for solving\n  time-variant risk optimization problems", "comments": null, "journal-ref": "Reliability Engineering & System Safety, 203, November 2020,\n  107033", "doi": "10.1016/j.ress.2020.107033", "report-no": "RSUQ-2020-007", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed in the literature to solve\nreliability-based optimization problems, where failure probabilities are design\nconstraints. However, few methods address the problem of life-cycle cost or\nrisk optimization, where failure probabilities are part of the objective\nfunction. Moreover, few papers in the literature address time-variant\nreliability problems in life-cycle cost or risk optimization formulations; in\nparticular, because most often computationally expensive Monte Carlo simulation\nis required. This paper proposes a numerical framework for solving general risk\noptimization problems involving time-variant reliability analysis. To alleviate\nthe computational burden of Monte Carlo simulation, two adaptive coupled\nsurrogate models are used: the first one to approximate the objective function,\nand the second one to approximate the quasi-static limit state function. An\niterative procedure is implemented for choosing additional support points to\nincrease the accuracy of the surrogate models. Three application problems are\nused to illustrate the proposed approach. Two examples involve random load and\nrandom resistance degradation processes. The third problem is related to\nload-path dependent failures. This subject had not yet been addressed in the\ncontext of risk-based optimization. It is shown herein that accurate solutions\nare obtained, with extremely limited numbers of objective function and limit\nstate functions calls.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 08:00:02 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Kroetz", "H. M.", ""], ["Moustapha", "M.", ""], ["Beck", "A. T.", ""], ["Sudret", "B.", ""]]}, {"id": "2007.04229", "submitter": "Dootika Vats", "authors": "Kushagra Gupta and Dootika Vats", "title": "Estimating Monte Carlo variance from multiple Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ever-increasing power of the personal computer has led to easy parallel\nimplementations of Markov chain Monte Carlo (MCMC). However, almost all work in\nestimating the variance of Monte Carlo averages, including the efficient batch\nmeans (BM) estimator, focuses on a single-chain MCMC run. We demonstrate that\nsimply averaging covariance matrix estimators from multiple chains (average BM)\ncan yield critical underestimates in small sample sizes, especially for slow\nmixing Markov chains. We propose a multivariate replicated batch means (RBM)\nestimator that utilizes information from parallel chains, thereby correcting\nfor the underestimation. Under weak conditions on the mixing rate of the\nprocess, the RBM and ABM estimator are both strongly consistent and exhibit\nsimilar large-sample bias and variance. However, in small runs the RBM\nestimator can be dramatically superior. This is demonstrated through a variety\nof examples, including a two-variable Gibbs sampler for a bivariate Gaussian\ntarget distribution. Here, we obtain a closed-form expression for the\nasymptotic covariance matrix of the Monte Carlo estimator, a useful result for\nbenchmarking in the future.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 16:14:08 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 08:49:48 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 16:57:15 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Gupta", "Kushagra", ""], ["Vats", "Dootika", ""]]}, {"id": "2007.04285", "submitter": "Gang Li", "authors": "Gang Li, Jan Hannig", "title": "Deep Fiducial Inference", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the mid-2000s, there has been a resurrection of interest in modern\nmodifications of fiducial inference. To date, the main computational tool to\nextract a generalized fiducial distribution is Markov chain Monte Carlo (MCMC).\nWe propose an alternative way of computing a generalized fiducial distribution\nthat could be used in complex situations. In particular, to overcome the\ndifficulty when the unnormalized fiducial density (needed for MCMC), we design\na fiducial autoencoder (FAE). The fitted autoencoder is used to generate\ngeneralized fiducial samples of the unknown parameters. To increase accuracy,\nwe then apply an approximate fiducial computation (AFC) algorithm, by rejecting\nsamples that when plugged into a decoder do not replicate the observed data\nwell enough. Our numerical experiments show the effectiveness of our FAE-based\ninverse solution and the excellent coverage performance of the AFC corrected\nFAE solution.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 17:33:14 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Li", "Gang", ""], ["Hannig", "Jan", ""]]}, {"id": "2007.04358", "submitter": "Owen Thomas", "authors": "Owen Thomas and Henri Pesonen and Jukka Corander", "title": "Generalised Bayes Updates with $f$-divergences through Probabilistic\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stream of algorithmic advances has steadily increased the popularity of the\nBayesian approach as an inference paradigm, both from the theoretical and\napplied perspective. Even with apparent successes in numerous application\nfields, a rising concern is the robustness of Bayesian inference in the\npresence of model misspecification, which may lead to undesirable extreme\nbehavior of the posterior distributions for large sample sizes. Generalized\nbelief updating with a loss function represents a central principle to making\nBayesian inference more robust and less vulnerable to deviations from the\nassumed model. Here we consider such updates with $f$-divergences to quantify a\ndiscrepancy between the assumed statistical model and the probability\ndistribution which generated the observed data. Since the latter is generally\nunknown, estimation of the divergence may be viewed as an intractable problem.\nWe show that the divergence becomes accessible through the use of probabilistic\nclassifiers that can leverage an estimate of the ratio of two probability\ndistributions even when one or both of them is unknown. We demonstrate the\nbehavior of generalized belief updates for various specific choices under the\n$f$-divergence family. We show that for specific divergence functions such an\napproach can even improve on methods evaluating the correct model likelihood\nfunction analytically.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 18:34:12 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["Corander", "Jukka", ""]]}, {"id": "2007.04446", "submitter": "Brian Lucena", "authors": "Brian Lucena", "title": "StructureBoost: Efficient Gradient Boosting for Structured Categorical\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient boosting methods based on Structured Categorical Decision Trees\n(SCDT) have been demonstrated to outperform numerical and one-hot-encodings on\nproblems where the categorical variable has a known underlying structure.\nHowever, the enumeration procedure in the SCDT is infeasible except for\ncategorical variables with low or moderate cardinality. We propose and\nimplement two methods to overcome the computational obstacles and efficiently\nperform Gradient Boosting on complex structured categorical variables. The\nresulting package, called StructureBoost, is shown to outperform established\npackages such as CatBoost and LightGBM on problems with categorical predictors\nthat contain sophisticated structure. Moreover, we demonstrate that\nStructureBoost can make accurate predictions on unseen categorical values due\nto its knowledge of the underlying structure.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 21:37:15 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lucena", "Brian", ""]]}, {"id": "2007.04586", "submitter": "He Jiang", "authors": "He Jiang, Ery Arias-Castro", "title": "$K$-Means and Gaussian Mixture Modeling with a Separation Constraint", "comments": "16 pages, 6 tables, 1 figure with 3 subfigures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering with $K$-means and Gaussian mixture\nmodels with a constraint on the separation between the centers in the context\nof real-valued data. We first propose a dynamic programming approach to solving\nthe $K$-means problem with a separation constraint on the centers, building on\n(Wang and Song, 2011). In the context of fitting a Gaussian mixture model, we\nthen propose an EM algorithm that incorporates such a constraint. A separation\nconstraint can help regularize the output of a clustering algorithm, and we\nprovide both simulated and real data examples to illustrate this point.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 06:49:49 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Jiang", "He", ""], ["Arias-Castro", "Ery", ""]]}, {"id": "2007.04697", "submitter": "Anastasija Nikiforova", "authors": "Anastasija Nikiforova", "title": "Open Data Quality Evaluation: A Comparative Analysis of Open Data in\n  Latvia", "comments": "24 pages, 2 tables, 3 figures, Baltic J. Modern Computing", "journal-ref": "Baltic J. Modern Computing, Vol. 6(2018), No. 4, 363-386", "doi": "10.22364/bjmc.2018.6.4.04", "report-no": null, "categories": "cs.DB cs.CY cs.IR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays open data is entering the mainstream - it is free available for\nevery stakeholder and is often used in business decision-making. It is\nimportant to be sure data is trustable and error-free as its quality problems\ncan lead to huge losses. The research discusses how (open) data quality could\nbe assessed. It also covers main points which should be considered developing a\ndata quality management solution. One specific approach is applied to several\nLatvian open data sets. The research provides a step-by-step open data sets\nanalysis guide and summarizes its results. It is also shown there could exist\ndifferences in data quality depending on data supplier (centralized and\ndecentralized data releases) and, unfortunately, trustable data supplier cannot\nguarantee data quality problems absence. There are also underlined common data\nquality problems detected not only in Latvian open data but also in open data\nof 3 European countries.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 10:43:28 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Nikiforova", "Anastasija", ""]]}, {"id": "2007.04791", "submitter": "Charlotte Baey", "authors": "Charlotte Baey and Estelle Kuhn", "title": "varTestnlme: an R package for Variance Components Testing in Linear and\n  Nonlinear Mixed-effects Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of variance components testing arises naturally when building\nmixed-effects models, to decide which effects should be modeled as fixed or\nrandom. While tests for fixed effects are available in R for models fitted with\nlme4, tools are missing when it comes to random effects. The varTestnlme\npackage for R aims at filling this gap. It allows to test whether any subset of\nthe variances and covariances corresponding to any subset of the random\neffects, are equal to zero using asymptotic property of the likelihood ratio\ntest statistic. It also offers the possibility to test simultaneously for fixed\neffects and variance components. It can be used for linear, generalized linear\nor nonlinear mixed-effects models fitted via lme4, nlme or saemix. Theoretical\nproperties of the used likelihood ratio test are recalled, numerical methods\nused to implement the test procedure are detailed and examples based on\ndifferent real datasets using different mixed models are provided.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 13:32:12 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 16:21:54 GMT"}, {"version": "v3", "created": "Mon, 17 May 2021 14:25:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Baey", "Charlotte", ""], ["Kuhn", "Estelle", ""]]}, {"id": "2007.04803", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Randal Douc", "title": "A Global Stochastic Optimization Particle Filter Algorithm", "comments": "67 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm to learn on the fly the parameter value\n$\\theta_\\star:=\\mathrm{argmax}_{\\theta\\in\\Theta}\\mathbb{E}[\\log f_\\theta(Y_0)]$\nfrom a sequence $(Y_t)_{t\\geq 1}$ of independent copies of $Y_0$, with\n$\\{f_\\theta,\\,\\theta\\in\\Theta\\subseteq\\mathbb{R}^d\\}$ a parametric model. The\nmain idea of the proposed approach is to define a sequence\n$(\\tilde{\\pi}_t)_{t\\geq 1}$ of probability distributions on $\\Theta$ which (i)\nis shown to concentrate on $\\theta_\\star$ as $t\\rightarrow\\infty$ and (ii) can\nbe estimated in an online fashion by means of a standard particle filter (PF)\nalgorithm. The sequence $(\\tilde{\\pi}_t)_{t\\geq 1}$ depends on a learning rate\n$h_t\\rightarrow 0$, with the slower $h_t$ converges to zero the greater is the\nability of the PF approximation $\\tilde{\\pi}_t^N$ of $\\tilde{\\pi}_t$ to escape\nfrom a local optimum of the objective function, but the slower is the rate at\nwhich $\\tilde{\\pi}_t$ concentrates on $\\theta_\\star$. To conciliate ability to\nescape from a local optimum and fast convergence towards $\\theta_\\star$ we\nexploit the acceleration property of averaging, well-known in the stochastic\ngradient descent literature, by letting $\\bar{\\theta}_t^N:=t^{-1}\\sum_{s=1}^t\n\\int_{\\Theta}\\theta\\ \\tilde{\\pi}_s^N(\\mathrm{d} \\theta)$ be the proposed\nestimator of $\\theta_\\star$. Our numerical experiments suggest that\n$\\bar{\\theta}_t^N$ converges to $\\theta_\\star$ at the optimal $t^{-1/2}$ rate\nin challenging models and in situations where $\\tilde{\\pi}_t^N$ concentrates on\nthis parameter value at a slower rate. We illustrate the practical usefulness\nof the proposed optimization algorithm for online parameter learning and for\ncomputing the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 14:17:43 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 13:33:23 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 14:49:53 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 11:14:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Gerber", "Mathieu", ""], ["Douc", "Randal", ""]]}, {"id": "2007.04956", "submitter": "Isaac Lavine", "authors": "Isaac Lavine, Andrew Cron, and Mike West", "title": "Bayesian Computation in Dynamic Latent Factor Models", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian computation for filtering and forecasting analysis is developed for\na broad class of dynamic models. The ability to scale-up such analyses in\nnon-Gaussian, nonlinear multivariate time series models is advanced through the\nintroduction of a novel copula construction in sequential filtering of coupled\nsets of dynamic generalized linear models. The new copula approach is\nintegrated into recently introduced multiscale models in which univariate time\nseries are coupled via nonlinear forms involving dynamic latent factors\nrepresenting cross-series relationships. The resulting methodology offers\ndramatic speed-up in online Bayesian computations for sequential filtering and\nforecasting in this broad, flexible class of multivariate models. Two examples\nin nonlinear models for very heterogeneous time series of non-negative counts\ndemonstrate massive computational efficiencies relative to existing\nsimulation-based methods, while defining similar filtering and forecasting\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 17:37:35 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Lavine", "Isaac", ""], ["Cron", "Andrew", ""], ["West", "Mike", ""]]}, {"id": "2007.05074", "submitter": "Boumediene Hamzi", "authors": "Boumediene Hamzi and Houman Owhadi", "title": "Learning dynamical systems from data: a simple cross-validation\n  perspective", "comments": "File uploaded on arxiv on Sunday, July 5th, 2020. Got delayed due to\n  tex problems on ArXiv. Original version at\n  https://www.researchgate.net/publication/342693818_Learning_dynamical_systems_from_data_a_simple_cross-validation_perspective", "journal-ref": null, "doi": "10.1016/j.physd.2020.132817", "report-no": null, "categories": "cs.LG math.DS nlin.CD stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regressing the vector field of a dynamical system from a finite number of\nobserved states is a natural way to learn surrogate models for such systems. We\npresent variants of cross-validation (Kernel Flows \\cite{Owhadi19} and its\nvariants based on Maximum Mean Discrepancy and Lyapunov exponents) as simple\napproaches for learning the kernel used in these emulators.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jul 2020 21:26:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Hamzi", "Boumediene", ""], ["Owhadi", "Houman", ""]]}, {"id": "2007.05114", "submitter": "Andrea Arnold", "authors": "Leah Mitchell, Andrea Arnold", "title": "Analyzing the Effects of Observation Function Selection in Ensemble\n  Kalman Filtering for Epidemic Models", "comments": "29 pages, 13 figures. This is the accepted manuscript of an article\n  published in Mathematical Biosciences. The published journal article is\n  available online at https://doi.org/10.1016/j.mbs.2021.108655", "journal-ref": "Mathematical Biosciences 339 (2021) 108655", "doi": "10.1016/j.mbs.2021.108655", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The Ensemble Kalman Filter (EnKF) is a popular sequential data assimilation\nmethod that has been increasingly used for parameter estimation and forecast\nprediction in epidemiological studies. The observation function plays a\ncritical role in the EnKF framework, connecting the unknown system variables\nwith the observed data. Key differences in observed data and modeling\nassumptions have led to the use of different observation functions in the\nepidemic modeling literature. In this work, we present a novel computational\nanalysis demonstrating the effects of observation function selection when using\nthe EnKF for state and parameter estimation in this setting. In examining the\nuse of four epidemiologically-inspired observation functions of different forms\nin connection with the classic Susceptible-Infectious-Recovered (SIR) model, we\nshow how incorrect observation modeling assumptions (i.e., fitting incidence\ndata with a prevalence model, or neglecting under-reporting) can lead to\ninaccurate filtering estimates and forecast predictions. Results demonstrate\nthe importance of choosing an observation function that well interprets the\navailable data on the corresponding EnKF estimates in several filtering\nscenarios, including state estimation with known parameters, and combined state\nand parameter estimation with both constant and time-varying parameters.\nNumerical experiments further illustrate how modifying the observation noise\ncovariance matrix in the filter can help to account for uncertainty in the\nobservation function in certain cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jul 2020 00:11:48 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:57:27 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mitchell", "Leah", ""], ["Arnold", "Andrea", ""]]}, {"id": "2007.05860", "submitter": "Sait Cakmak", "authors": "Sait Cakmak, Di Wu and Enlu Zhou", "title": "Solving Bayesian Risk Optimization via Nested Stochastic Gradient\n  Estimation", "comments": "The paper is 20 pages with 3 figures. The supplement is an additional\n  15 pages. The paper is currently under review at IISE Transactions. Updated\n  formatting in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to solve Bayesian Risk Optimization (BRO), which is a\nrecently proposed framework that formulates simulation optimization under input\nuncertainty. In order to efficiently solve the BRO problem, we derive nested\nstochastic gradient estimators and propose corresponding stochastic\napproximation algorithms. We show that our gradient estimators are\nasymptotically unbiased and consistent, and that the algorithms converge\nasymptotically. We demonstrate the empirical performance of the algorithms on a\ntwo-sided market model. Our estimators are of independent interest in extending\nthe literature of stochastic gradient estimation to the case of nested risk\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jul 2020 21:51:46 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 02:45:49 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Cakmak", "Sait", ""], ["Wu", "Di", ""], ["Zhou", "Enlu", ""]]}, {"id": "2007.06072", "submitter": "Jules Depersin", "authors": "Jules Depersin", "title": "A spectral algorithm for robust regression with subgaussian rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new linear up to quadratic time algorithm for linear regression in\nthe absence of strong assumptions on the underlying distributions of samples,\nand in the presence of outliers. The goal is to design a procedure which comes\nwith actual working code that attains the optimal sub-gaussian error bound even\nthough the data have only finite moments (up to $L_4$) and in the presence of\npossibly adversarial outliers. A polynomial-time solution to this problem has\nbeen recently discovered but has high runtime due to its use of Sum-of-Square\nhierarchy programming. At the core of our algorithm is an adaptation of the\nspectral method introduced for the mean estimation problem to the linear\nregression problem. As a by-product we established a connection between the\nlinear regression problem and the furthest hyperplane problem. From a\nstochastic point of view, in addition to the study of the classical quadratic\nand multiplier processes we introduce a third empirical process that comes\nnaturally in the study of the statistical properties of the algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 19:33:50 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Depersin", "Jules", ""]]}, {"id": "2007.06101", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Olanrewaju Akande and Quanli Wang", "title": "Multiple Imputation and Synthetic Data Generation with the R package\n  NPBayesImputeCat", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many contexts, missing data and disclosure control are ubiquitous and\nchallenging issues. In particular at statistical agencies, the respondent-level\ndata they collect from surveys and censuses can suffer from high rates of\nmissingness. Furthermore, agencies are obliged to protect respondents' privacy\nwhen publishing the collected data for public use. The NPBayesImputeCat R\npackage, introduced in this paper, provides routines to i) create multiple\nimputations for missing data, and ii) create synthetic data for statistical\ndisclosure control, for multivariate categorical data, with or without\nstructural zeros. We describe the Dirichlet process mixture of products of\nmultinomial distributions model used in the package, and illustrate various\nuses of the package using data samples from the American Community Survey\n(ACS). We also compare results of the missing data imputation to the mice R\npackage and those of the synthetic data generation to the synthpop R package.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jul 2020 21:44:27 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 17:34:47 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Hu", "Jingchen", ""], ["Akande", "Olanrewaju", ""], ["Wang", "Quanli", ""]]}, {"id": "2007.06157", "submitter": "Tyler Ward", "authors": "Tyler Ward", "title": "Implementing the ICE Estimator in Multilayer Perceptron Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the techniques used to implement the ICE estimator for a\nmultilayer perceptron model, and reviews the performance of the resulting\nmodels. The ICE estimator is implemented in the Apache Spark\nMultilayerPerceptronClassifier, and shown in cross-validation to outperform the\nstock MultilayerPerceptronClassifier that uses unadjusted MLE (cross-entropy)\nloss. The resulting models have identical runtime performance, and similar\nfitting performance to the stock MLP implementations. Additionally, this\napproach requires no hyper-parameters, and is therefore viable as a drop-in\nreplacement for cross-entropy optimizing multilayer perceptron classifiers\nwherever overfitting may be a concern.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 02:49:51 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ward", "Tyler", ""]]}, {"id": "2007.06298", "submitter": "Mehdi Dagdoug", "authors": "Mehdi Dagdoug, Camelia Goga and David Haziza", "title": "Imputation procedures in surveys using nonparametric and machine\n  learning methods: an empirical comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric and machine learning methods are flexible methods for obtaining\naccurate predictions. Nowadays, data sets with a large number of predictors and\ncomplex structures are fairly common. In the presence of item nonresponse,\nnonparametric and machine learning procedures may thus provide a useful\nalternative to traditional imputation procedures for deriving a set of imputed\nvalues. In this paper, we conduct an extensive empirical investigation that\ncompares a number of imputation procedures in terms of bias and efficiency in a\nwide variety of settings, including high-dimensional data sets. The results\nsuggest that a number of machine learning procedures perform very well in terms\nof bias and efficiency.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 10:31:24 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:29:52 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dagdoug", "Mehdi", ""], ["Goga", "Camelia", ""], ["Haziza", "David", ""]]}, {"id": "2007.06635", "submitter": "Mehrdad Naderi Dr", "authors": "Elham Mirfarah and Mehrdad Naderi and Ding-Geng Chen", "title": "Mixture of linear experts model for censored data: A novel approach with\n  scale-mixture of normal distributions", "comments": "21 pages,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical mixture of linear experts (MoE) model is one of the widespread\nstatistical frameworks for modeling, classification, and clustering of data.\nBuilt on the normality assumption of the error terms for mathematical and\ncomputational convenience, the classical MoE model has two challenges: 1) it is\nsensitive to atypical observations and outliers, and 2) it might produce\nmisleading inferential results for censored data. The paper is then aimed to\nresolve these two challenges, simultaneously, by proposing a novel robust MoE\nmodel for model-based clustering and discriminant censored data with the\nscale-mixture of normal class of distributions for the unobserved error terms.\nBased on this novel model, we develop an analytical expectation-maximization\n(EM) type algorithm to obtain the maximum likelihood parameter estimates.\nSimulation studies are carried out to examine the performance, effectiveness,\nand robustness of the proposed methodology. Finally, real data is used to\nillustrate the superiority of the new model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 19:15:39 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Mirfarah", "Elham", ""], ["Naderi", "Mehrdad", ""], ["Chen", "Ding-Geng", ""]]}, {"id": "2007.06944", "submitter": "Augusto Fasano", "authors": "Augusto Fasano and Daniele Durante", "title": "A Class of Conjugate Priors for Multinomial Probit Models which Includes\n  the Multivariate Normal One", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multinomial probit models are widely-implemented representations which allow\nboth classification and inference by learning changes in vectors of class\nprobabilities with a set of p observed predictors. Although various frequentist\nmethods have been developed for estimation, inference and classification within\nsuch a class of models, Bayesian inference is still lagging behind. This is due\nto the apparent absence of a tractable class of conjugate priors, that may\nfacilitate posterior inference on the multinomial probit coefficients. Such an\nissue has motivated increasing efforts toward the development of effective\nMarkov chain Monte Carlo methods, but state-of-the-art solutions still face\nsevere computational bottlenecks, especially in large p settings. In this\narticle, we prove that the entire class of unified skew-normal (SUN)\ndistributions is conjugate to a wide variety of multinomial probit models, and\nwe exploit the SUN properties to improve upon state-of-art-solutions for\nposterior inference and classification both in terms of closed-form results for\nkey functionals of interest, and also by developing novel computational methods\nrelying either on independent and identically distributed samples from the\nexact posterior or on scalable and accurate variational approximations based on\nblocked partially-factorized representations. As illustrated in a\ngastrointestinal lesions application, the magnitude of the improvements\nrelative to current methods is particularly evident, in practice, when the\nfocus is on large p applications.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 10:08:23 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Fasano", "Augusto", ""], ["Durante", "Daniele", ""]]}, {"id": "2007.06968", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Sergey Dolgov", "title": "Deep Composition of Tensor Trains using Squared Inverse Rosenblatt\n  Transports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterising intractable high-dimensional random variables is one of the\nfundamental challenges in stochastic computation. The recent surge of transport\nmaps offers a mathematical foundation and new insights for tackling this\nchallenge by coupling intractable random variables with tractable reference\nrandom variables. This paper generalises a recently developed functional\ntensor-train (FTT) approximation of the inverse Rosenblatt transport [14] to a\nwide class of high-dimensional nonnegative functions, such as unnormalised\nprobability density functions. First, we extend the inverse Rosenblatt\ntransform to enable the transport to general reference measures other than the\nuniform measure. We develop an efficient procedure to compute this transport\nfrom a squared FTT decomposition which preserves the monotonicity. More\ncrucially, we integrate the proposed monotonicity-preserving FTT transport into\na nested variable transformation framework inspired by deep neural networks.\nThe resulting deep inverse Rosenblatt transport significantly expands the\ncapability of tensor approximations and transport maps to random variables with\ncomplicated nonlinear interactions and concentrated density functions. We\ndemonstrate the efficacy of the proposed approach on a range of applications in\nstatistical learning and uncertainty quantification, including parameter\nestimation for dynamical systems and inverse problems constrained by partial\ndifferential equations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 11:04:18 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Cui", "Tiangang", ""], ["Dolgov", "Sergey", ""]]}, {"id": "2007.07724", "submitter": "Aritz Adin", "authors": "E. Orozco-Acosta, A. Adin, M. D. Ugarte", "title": "Scalable Bayesian modeling for smoothing disease risks in large spatial\n  data sets", "comments": null, "journal-ref": "Spatial Statistics (2021), 41, 100496", "doi": "10.1016/j.spasta.2021.100496", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several methods have been proposed in the spatial statistics literature for\nthe analysis of big data sets in continuous domains. However, new methods for\nanalyzing high-dimensional areal data are still scarce. Here, we propose a\nscalable Bayesian modeling approach for smoothing mortality (or incidence)\nrisks in high-dimensional data, that is, when the number of small areas is very\nlarge. The method is implemented in the R add-on package bigDM. Model fitting\nand inference is based on the idea of \"divide and conquer\" and use integrated\nnested Laplace approximations and numerical integration. We analyze the\nproposal's empirical performance in a comprehensive simulation study that\nconsider two model-free settings. Finally, the methodology is applied to\nanalyze male colorectal cancer mortality in Spanish municipalities showing its\nbenefits with regard to the standard approach in terms of goodness of fit and\ncomputational time.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 14:50:20 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Orozco-Acosta", "E.", ""], ["Adin", "A.", ""], ["Ugarte", "M. D.", ""]]}, {"id": "2007.07930", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Philipp F.M. Baumann, Sonja Greven", "title": "Selective Inference for Additive and Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work addresses the problem of conducting valid inference for additive\nand linear mixed models after model selection. One possible solution to\novercome overconfident inference results after model selection is selective\ninference, which constitutes a post-selection inference framework, yielding\nvalid inference statements by conditioning on the selection event. We extend\nrecent work on selective inference to the class of additive and linear mixed\nmodels for any type of model selection mechanism that can be expressed as a\nfunction of the outcome variable (and potentially on covariates on which it\nconditions). We investigate the properties of our proposal in simulation\nstudies and apply the framework to a data set in monetary economics. Due to the\ngenerality of our proposed approach, the presented approach also works for\nnon-standard selection procedures, which we demonstrate in our application.\nHere, the final additive mixed model is selected using a hierarchical selection\nprocedure, which is based on the conditional Akaike information criterion and\ninvolves varying data set sizes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 18:18:12 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2020 14:08:01 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Baumann", "Philipp F. M.", ""], ["Greven", "Sonja", ""]]}, {"id": "2007.07953", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad and Adam J. Rothman", "title": "A likelihood-based approach for multivariate categorical response\n  regression in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to fit the bivariate categorical\nresponse regression model. Our method allows practitioners to estimate which\npredictors are irrelevant, which predictors only affect the marginal\ndistributions of the bivariate response, and which predictors affect both the\nmarginal distributions and log odds ratios. To compute our estimator, we\npropose an efficient first order algorithm which we extend to settings where\nsome subjects have only one response variable measured, i.e., the\nsemi-supervised setting. We derive an asymptotic error bound which illustrates\nthe performance of our estimator in high-dimensional settings. Generalizations\nto the multivariate categorical response regression model are proposed.\nFinally, simulation studies and an application in pan-cancer risk prediction\ndemonstrate the usefulness of our method in terms of interpretability and\nprediction accuracy. An R package implementing the proposed method is available\nfor download at github.com/ajmolstad/BvCategorical.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 19:10:34 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 04:15:09 GMT"}, {"version": "v3", "created": "Sat, 19 Sep 2020 19:32:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "2007.07976", "submitter": "Michael Chiu", "authors": "Michael Chiu, Kenneth R. Jackson, Alexander Kreinin", "title": "Backward Simulation of Multivariate Mixed Poisson Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Backward Simulation (BS) approach was developed to generate, simply and\nefficiently, sample paths of correlated multivariate Poisson process with\nnegative correlation coefficients between their components. In this paper, we\nextend the BS approach to model multivariate Mixed Poisson processes which have\nmany important applications in Insurance, Finance, Geophysics and many other\nareas of Applied Probability. We also extend the Forward Continuation approach,\nintroduced in our earlier work, to multivariate Mixed Poisson processes.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 20:19:24 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 02:45:49 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Chiu", "Michael", ""], ["Jackson", "Kenneth R.", ""], ["Kreinin", "Alexander", ""]]}, {"id": "2007.08016", "submitter": "Pavlo Mozharovskyi", "authors": "Rainer Dyckerhoff, Pavlo Mozharovskyi, Stanislav Nagy", "title": "Approximate computation of projection depths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Data depth is a concept in multivariate statistics that measures the\ncentrality of a point in a given data cloud in $\\IR^d$. If the depth of a point\ncan be represented as the minimum of the depths with respect to all\none-dimensional projections of the data, then the depth satisfies the so-called\nprojection property. Such depths form an important class that includes many of\nthe depths that have been proposed in literature. For depths that satisfy the\nprojection property an approximate algorithm can easily be constructed since\ntaking the minimum of the depths with respect to only a finite number of\none-dimensional projections yields an upper bound for the depth with respect to\nthe multivariate data. Such an algorithm is particularly useful if no exact\nalgorithm exists or if the exact algorithm has a high computational complexity,\nas is the case with the halfspace depth or the projection depth. To compute\nthese depths in high dimensions, the use of an approximate algorithm with\nbetter complexity is surely preferable. Instead of focusing on a single method\nwe provide a comprehensive and fair comparison of several methods, both already\ndescribed in the literature and original.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jul 2020 22:08:05 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Dyckerhoff", "Rainer", ""], ["Mozharovskyi", "Pavlo", ""], ["Nagy", "Stanislav", ""]]}, {"id": "2007.08458", "submitter": "Tom\\'a\\v{s} Rub\\'in", "authors": "Tom\\'a\\v{s} Rub\\'in, Victor M. Panaretos", "title": "Spectral Simulation of Functional Time Series", "comments": "29 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methodology allowing to simulate a stationary functional time\nseries defined by means of its spectral density operators. Our framework is\ngeneral, in that it encompasses any such stationary functional time series,\nwhether linear or not. The methodology manifests particularly significant\ncomputational gains if the spectral density operators are specified by means of\ntheir eigendecomposition or as a filtering of white noise. In the special case\nof linear processes, we determine the analytical expressions for the spectral\ndensity operators of functional autoregressive (fractionally integrated) moving\naverage processes, and leverage these as part of our spectral approach, leading\nto substantial improvements over time-domain simulation methods in some cases.\nThe methods are implemented as an R package (specsimfts) accompanied by several\ndemo files that are easy to modify and can be easily used by researchers aiming\nto probe the finite-sample performance of their functional time series\nmethodology by means of simulation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 16:51:43 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Rub\u00edn", "Tom\u00e1\u0161", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2007.08945", "submitter": "Prateek Bansal", "authors": "Prateek Bansal, Vahid Keshavarzzadeh, Angelo Guevara, Ricardo A.\n  Daziano, Shanjun Li", "title": "Designed Quadrature to Approximate Integrals in Maximum Simulated\n  Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum simulated likelihood estimation of mixed multinomial logit (MMNL) or\nprobit models requires evaluation of a multidimensional integral. Quasi-Monte\nCarlo (QMC) methods such as shuffled and scrambled Halton sequences and\nmodified Latin hypercube sampling (MLHS) are workhorse methods for integral\napproximation. A few earlier studies explored the potential of sparse grid\nquadrature (SGQ), but this approximation suffers from negative weights. As an\nalternative to QMC and SGQ, we looked into the recently developed designed\nquadrature (DQ) method. DQ requires fewer nodes to get the same level of\naccuracy as of QMC and SGQ, is as easy to implement, ensures positivity of\nweights, and can be created on any general polynomial spaces. We benchmarked DQ\nagainst QMC in a Monte Carlo study under different data generating processes\nwith a varying number of random parameters (3, 5, and 10) and\nvariance-covariance structures (diagonal and full). Whereas DQ significantly\noutperformed QMC in the diagonal variance-covariance scenario, it could also\nachieve a better model fit and recover true parameters with fewer nodes (i.e.,\nrelatively lower computation time) in the full variance-covariance scenario.\nFinally, we evaluated the performance of DQ in a case study to understand\npreferences for mobility-on-demand services in New York City. In estimating\nMMNL with five random parameters, DQ achieved better fit and statistical\nsignificance of parameters with just 200 nodes as compared to 1000 QMC draws,\nmaking DQ around five times faster than QMC methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jul 2020 16:25:02 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 02:28:29 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Bansal", "Prateek", ""], ["Keshavarzzadeh", "Vahid", ""], ["Guevara", "Angelo", ""], ["Daziano", "Ricardo A.", ""], ["Li", "Shanjun", ""]]}, {"id": "2007.09114", "submitter": "Alvaro Tejero-Cantero", "authors": "Alvaro Tejero-Cantero (1), Jan Boelts (1), Michael Deistler (1),\n  Jan-Matthis Lueckmann (1), Conor Durkan (2), Pedro J. Gon\\c{c}alves (1, 3),\n  David S. Greenberg (1, 4) and Jakob H. Macke (1, 5, 6) ((1) Computational\n  Neuroengineering, Department of Electrical and Computer Engineering,\n  Technical University of Munich, (2) School of Informatics, University of\n  Edinburgh, (3) Neural Systems Analysis, Center of Advanced European Studies\n  and Research (caesar), Bonn, (4) Model-Driven Machine Learning, Centre for\n  Materials and Coastal Research, Helmholtz-Zentrum Geesthacht, (5) Machine\n  Learning in Science, University of T\\\"ubingen, (6) Empirical Inference, Max\n  Planck Institute for Intelligent Systems, T\\\"ubingen)", "title": "SBI -- A toolkit for simulation-based inference", "comments": "Alvaro Tejero-Cantero, Jan Boelts, Michael Deistler, Jan-Matthis\n  Lueckmann and Conor Durkan contributed equally in shared first authorship.\n  This manuscript has been submitted for consideration to the Journal of Open\n  Source Software (JOSS). 4 pages, no figures; v2: added link to sbi home", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-bio.QM stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Scientists and engineers employ stochastic numerical simulators to model\nempirically observed phenomena. In contrast to purely statistical models,\nsimulators express scientific principles that provide powerful inductive\nbiases, improve generalization to new data or scenarios and allow for fewer,\nmore interpretable and domain-relevant parameters. Despite these advantages,\ntuning a simulator's parameters so that its outputs match data is challenging.\nSimulation-based inference (SBI) seeks to identify parameter sets that a) are\ncompatible with prior knowledge and b) match empirical observations.\nImportantly, SBI does not seek to recover a single 'best' data-compatible\nparameter set, but rather to identify all high probability regions of parameter\nspace that explain observed data, and thereby to quantify parameter\nuncertainty. In Bayesian terminology, SBI aims to retrieve the posterior\ndistribution over the parameters of interest. In contrast to conventional\nBayesian inference, SBI is also applicable when one can run model simulations,\nbut no formula or algorithm exists for evaluating the probability of data given\nparameters, i.e. the likelihood. We present $\\texttt{sbi}$, a PyTorch-based\npackage that implements SBI algorithms based on neural networks. $\\texttt{sbi}$\nfacilitates inference on black-box simulators for practising scientists and\nengineers by providing a unified interface to state-of-the-art algorithms\ntogether with documentation and tutorials.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 16:53:51 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 15:43:36 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Tejero-Cantero", "Alvaro", ""], ["Boelts", "Jan", ""], ["Deistler", "Michael", ""], ["Lueckmann", "Jan-Matthis", ""], ["Durkan", "Conor", ""], ["Gon\u00e7alves", "Pedro J.", ""], ["Greenberg", "David S.", ""], ["Macke", "Jakob H.", ""]]}, {"id": "2007.09118", "submitter": "Hans Kersting", "authors": "Hans Kersting, Maren Mahsereci", "title": "A Fourier State Space Model for Bayesian ODE Filters", "comments": "5 pages, 2 figures, ICML Workshop on Invertible Neural Networks,\n  Normalizing Flows, and Explicit Likelihood Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian ODE filtering is a probabilistic numerical method to solve ordinary\ndifferential equations (ODEs). It computes a Bayesian posterior over the\nsolution from evaluations of the vector field defining the ODE. Its most\npopular version, which employs an integrated Brownian motion prior, uses Taylor\nexpansions of the mean to extrapolate forward and has the same convergence\nrates as classical numerical methods. As the solution of many important ODEs\nare periodic functions (oscillators), we raise the question whether Fourier\nexpansions can also be brought to bear within the framework of Gaussian ODE\nfiltering. To this end, we construct a Fourier state space model for ODEs and a\n`hybrid' model that combines a Taylor (Brownian motion) and Fourier state space\nmodel. We show by experiments how the hybrid model might become useful in\ncheaply predicting until the end of the time domain.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jul 2020 17:14:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 12:04:02 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Kersting", "Hans", ""], ["Mahsereci", "Maren", ""]]}, {"id": "2007.09539", "submitter": "Moo K. Chung", "authors": "Moo K. Chung", "title": "Gaussian kernel smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Image acquisition and segmentation are likely to introduce noise. Further\nimage processing such as image registration and parameterization can introduce\nadditional noise. It is thus imperative to reduce noise measurements and boost\nsignal. In order to increase the signal-to-noise ratio (SNR) and smoothness of\ndata required for the subsequent random field theory based statistical\ninference, some type of smoothing is necessary. Among many image smoothing\nmethods, Gaussian kernel smoothing has emerged as a de facto smoothing\ntechnique among brain imaging researchers due to its simplicity in numerical\nimplementation. Gaussian kernel smoothing also increases statistical\nsensitivity and statistical power as well as Gausianness. Gaussian kernel\nsmoothing can be viewed as weighted averaging of voxel values. Then from the\ncentral limit theorem, the weighted average should be more Gaussian.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 00:19:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 00:37:44 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chung", "Moo K.", ""]]}, {"id": "2007.09577", "submitter": "Feng Li", "authors": "Xiaoqian Wang, Yanfei Kang, Rob J Hyndman, and Feng Li", "title": "Distributed ARIMA Models for Ultra-long Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing forecasts for ultra-long time series plays a vital role in various\nactivities, such as investment decisions, industrial production arrangements,\nand farm management. This paper develops a novel distributed forecasting\nframework to tackle challenges associated with forecasting ultra-long time\nseries by utilizing the industry-standard MapReduce framework. The proposed\nmodel combination approach facilitates distributed time series forecasting by\ncombining the local estimators of ARIMA (AutoRegressive Integrated Moving\nAverage) models delivered from worker nodes and minimizing a global loss\nfunction. In this way, instead of unrealistically assuming the data generating\nprocess (DGP) of an ultra-long time series stays invariant, we make assumptions\nonly on the DGP of subseries spanning shorter time periods. We investigate the\nperformance of the proposed distributed ARIMA models on an electricity demand\ndataset. Compared to ARIMA models, our approach results in significantly\nimproved forecasting accuracy and computational efficiency both in point\nforecasts and prediction intervals, especially for longer forecast horizons.\nMoreover, we explore some potential factors that may affect the forecasting\nperformance of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 03:23:40 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wang", "Xiaoqian", ""], ["Kang", "Yanfei", ""], ["Hyndman", "Rob J", ""], ["Li", "Feng", ""]]}, {"id": "2007.09672", "submitter": "Zachary Fisher", "authors": "Zachary F. Fisher, Sy-Miin Chow, Peter C. M. Molenaar, Barbara L.\n  Fredrickson, Vladas Pipiras and Kathleen M. Gates", "title": "A Square-Root Second-Order Extended Kalman Filtering Approach for\n  Estimating Smoothly Time-Varying Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers collecting intensive longitudinal data (ILD) are increasingly\nlooking to model psychological processes, such as emotional dynamics, that\norganize and adapt across time in complex and meaningful ways. This is also the\ncase for researchers looking to characterize the impact of an intervention on\nindividual behavior. To be useful, statistical models must be capable of\ncharacterizing these processes as complex, time-dependent phenomenon, otherwise\nonly a fraction of the system dynamics will be recovered. In this paper we\nintroduce a Square-Root Second-Order Extended Kalman Filtering approach for\nestimating smoothly time-varying parameters. This approach is capable of\nhandling dynamic factor models where the relations between variables underlying\nthe processes of interest change in a manner that may be difficult to specify\nin advance. We examine the performance of our approach in a Monte Carlo\nsimulation and show the proposed algorithm accurately recovers the unobserved\nstates in the case of a bivariate dynamic factor model with time-varying\ndynamics and treatment effects. Furthermore, we illustrate the utility of our\napproach in characterizing the time-varying effect of a meditation intervention\non day-to-day emotional experiences.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jul 2020 13:47:57 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Fisher", "Zachary F.", ""], ["Chow", "Sy-Miin", ""], ["Molenaar", "Peter C. M.", ""], ["Fredrickson", "Barbara L.", ""], ["Pipiras", "Vladas", ""], ["Gates", "Kathleen M.", ""]]}, {"id": "2007.09865", "submitter": "Jeong-Soo Park", "authors": "Yun Am Seo, Youngsaeng Lee, Jeong-Soo Park", "title": "Iterative Method for Tuning Complex Simulation Code", "comments": null, "journal-ref": "Communications in Statistics -- Simulation and Computation 2020", "doi": "10.1080/03610918.2020.1728317", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tuning a complex simulation code refers to the process of improving the\nagreement of a code calculation with respect to a set of experimental data by\nadjusting parameters implemented in the code. This process belongs to the class\nof inverse problems or model calibration. For this problem, the approximated\nnonlinear least squares (ANLS) method based on a Gaussian process (GP)\nmetamodel has been employed by some researchers. A potential drawback of the\nANLS method is that the metamodel is built only once and not updated\nthereafter. To address this difficulty, we propose an iterative algorithm in\nthis study. In the proposed algorithm, the parameters of the simulation code\nand GP metamodel are alternatively re-estimated and updated by maximum\nlikelihood estimation and the ANLS method. This algorithm uses both computer\nand experimental data repeatedly until convergence. A study using toy-models\nincluding inexact computer code with bias terms reveals that the proposed\nalgorithm performs better than the ANLS method and the\nconditional-likelihood-based approach. Finally, an application to a nuclear\nfusion simulation code is illustrated.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:32:27 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Seo", "Yun Am", ""], ["Lee", "Youngsaeng", ""], ["Park", "Jeong-Soo", ""]]}, {"id": "2007.09871", "submitter": "Marco Cusumano-Towner", "authors": "Marco Cusumano-Towner, Alexander K. Lew, Vikash K. Mansinghka", "title": "Automating Involutive MCMC using Probabilistic and Differentiable\n  Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Involutive MCMC is a unifying mathematical construction for MCMC kernels that\ngeneralizes many classic and state-of-the-art MCMC algorithms, from reversible\njump MCMC to kernels based on deep neural networks. But as with MCMC samplers\nmore generally, implementing involutive MCMC kernels is often tedious and\nerror-prone, especially when sampling on complex state spaces. This paper\ndescribes a technique for automating the implementation of involutive MCMC\nkernels given (i) a pair of probabilistic programs defining the target\ndistribution and an auxiliary distribution respectively and (ii) a\ndifferentiable program that transforms the execution traces of these\nprobabilistic programs. The technique, which is implemented as part of the Gen\nprobabilistic programming system, also automatically detects user errors in the\nspecification of involutive MCMC kernels and exploits sparsity in the kernels\nfor improved efficiency. The paper shows example Gen code for a split-merge\nreversible jump move in an infinite Gaussian mixture model and a\nstate-dependent mixture of proposals on a combinatorial space of covariance\nfunctions for a Gaussian process.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 03:46:36 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 01:24:19 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Cusumano-Towner", "Marco", ""], ["Lew", "Alexander K.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2007.10079", "submitter": "Souhaib Douass", "authors": "Souhaib Douass and M'hamed Ait Kbir", "title": "Flood zones detection using a runoff model built on Hexagonal shape\n  based cellular automata", "comments": "7 pages, 19 figures, Published with International Journal of\n  Engineering Trends and Technology (IJETT)", "journal-ref": "International Journal of Engineering Trends and Technology\n  68.6(2020):68-74", "doi": "10.14445/22315381/IJETT-V68I6P211S", "report-no": null, "categories": "cs.CE nlin.CG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a 3D geographic information systems (GIS) modeling and\nsimulation of water flow in a landscape defined by a digital terrain model,\nprovided by some available geolocation APIs. The proposed approach uses a\ncellular automata based algorithm to calculate water flow dynamic. The\nmethodology was tested on a case study area of 27kmx19km located in Tangier,\nnorth of Morocco. In fact, we aim to detect flood zones in order to prevent\nproblems related to space occupation in urban and rural regions. Some indices\ncan be deduced from the stream shape using Cellular Automata (CA) based\napproach that can reduce the complexity related to space structures with\nmultiple changes. A spatiotemporal simulation of the runoff process is provided\nusing 3D visualization that we can pair with geographical information system\ntools (GIS). The 3D GIS modeling approach that was developed for the analyses\nof flood zones detection using a runoff model based on cellular automata was\ncomprised of three main steps: Input (collection of data), calculation (CA\ntool) and visualization (3D simulation).\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 23:42:31 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Douass", "Souhaib", ""], ["Kbir", "M'hamed Ait", ""]]}, {"id": "2007.10112", "submitter": "Hanwen Huang", "authors": "Hanwen Huang and Qinglong Yang", "title": "Large scale analysis of generalization error in learning using margin\n  based classification methods", "comments": "31 pages, 7 figures. arXiv admin note: substantial text overlap with\n  arXiv:1901.08057", "journal-ref": null, "doi": "10.1088/1742-5468/abbed5", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-margin classifiers are popular methods for classification. We derive\nthe asymptotic expression for the generalization error of a family of\nlarge-margin classifiers in the limit of both sample size $n$ and dimension $p$\ngoing to $\\infty$ with fixed ratio $\\alpha=n/p$. This family covers a broad\nrange of commonly used classifiers including support vector machine, distance\nweighted discrimination, and penalized logistic regression. Our result can be\nused to establish the phase transition boundary for the separability of two\nclasses. We assume that the data are generated from a single multivariate\nGaussian distribution with arbitrary covariance structure. We explore two\nspecial choices for the covariance matrix: spiked population model and two\nlayer neural networks with random first layer weights. The method we used for\nderiving the closed-form expression is from statistical physics known as the\nreplica method. Our asymptotic results match simulations already when $n,p$ are\nof the order of a few hundreds. For two layer neural networks, we reproduce the\nrecently developed `double descent' phenomenology for several classification\nmodels. We also discuss some statistical insights that can be drawn from these\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jul 2020 20:31:26 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huang", "Hanwen", ""], ["Yang", "Qinglong", ""]]}, {"id": "2007.10183", "submitter": "Linyi Zou", "authors": "Linyi Zou, Hui Guo and Carlo Berzuini", "title": "Overlapping-sample Mendelian randomisation with multiple exposures: A\n  Bayesian approach", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Mendelian randomization (MR) has been widely applied to causal\ninference in medical research. It uses genetic variants as instrumental\nvariables (IVs) to investigate putative causal relationship between an exposure\nand an outcome. Traditional MR methods have dominantly focussed on a two-sample\nsetting in which IV-exposure association study and IV-outcome association study\nare independent. However, it is not uncommon that participants from the two\nstudies fully overlap (one-sample) or partly overlap (overlapping-sample).\nMethods: We proposed a method that is applicable to all the three sample\nsettings. In essence, we converted a two- or overlapping- sample problem to a\none-sample problem where data of some or all of the individuals were\nincomplete. Assume that all individuals were drawn from the same population and\nunmeasured data were missing at random. Then the unobserved data were treated\nau pair with the model parameters as unknown quantities, and thus, could be\nimputed iteratively conditioning on the observed data and estimated parameters\nusing Markov chain Monte Carlo. We generalised our model to allow for\npleiotropy and multiple exposures and assessed its performance by a number of\nsimulations using four metrics: mean, standard deviation, coverage and power.\nResults: Higher sample overlapping rate and stronger instruments led to\nestimates with higher precision and power. Pleiotropy had a notably negative\nimpact on the estimates. Nevertheless, overall the coverages were high and our\nmodel performed well in all the sample settings. Conclusions: Our model offers\nthe flexibility of being applicable to any of the sample settings, which is an\nimportant addition to the MR literature which has restricted to one- or two-\nsample scenarios. Given the nature of Bayesian inference, it can be easily\nextended to more complex MR analysis in medical research.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jul 2020 15:18:33 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 14:48:54 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Zou", "Linyi", ""], ["Guo", "Hui", ""], ["Berzuini", "Carlo", ""]]}, {"id": "2007.10612", "submitter": "Art Owen", "authors": "Swarnadip Ghosh and Trevor Hastie and Art B. Owen", "title": "Backfitting for large scale crossed random effects regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with crossed random effect errors can be very expensive to\ncompute. The cost of both generalized least squares and Gibbs sampling can\neasily grow as $N^{3/2}$ (or worse) for $N$ observations. Papaspiliopoulos et\nal. (2020) present a collapsed Gibbs sampler that costs $O(N)$, but under an\nextremely stringent sampling model. We propose a backfitting algorithm to\ncompute a generalized least squares estimate and prove that it costs $O(N)$. A\ncritical part of the proof is in ensuring that the number of iterations\nrequired is $O(1)$ which follows from keeping a certain matrix norm below\n$1-\\delta$ for some $\\delta>0$. Our conditions are greatly relaxed compared to\nthose for the collapsed Gibbs sampler, though still strict. Empirically, the\nbackfitting algorithm has a norm below $1-\\delta$ under conditions that are\nless strict than those in our assumptions. We illustrate the new algorithm on a\nratings data set from Stitch Fix.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 06:00:59 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 21:06:38 GMT"}, {"version": "v3", "created": "Thu, 18 Mar 2021 22:20:09 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2007.10642", "submitter": "Fabien Navarro", "authors": "Basile de Loynes, Fabien Navarro, Baptiste Olivier", "title": "Gasper: GrAph Signal ProcEssing in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a short tutorial on to the use of the \\proglang{R} \\pkg{gasper}\npackage. Gasper is a package dedicated to signal processing on graphs. It also\nprovides an interface to the SuiteSparse Matrix Collection.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 07:41:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 08:55:28 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 00:42:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["de Loynes", "Basile", ""], ["Navarro", "Fabien", ""], ["Olivier", "Baptiste", ""]]}, {"id": "2007.10731", "submitter": "Benjamin Guedj", "authors": "Arthur Leroy and Pierre Latouche and Benjamin Guedj and Servane Gey", "title": "MAGMA: Inference and Prediction with Multi-Task Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate the problem of multiple time series forecasting, with the\nobjective to improve multiple-step-ahead predictions. We propose a multi-task\nGaussian process framework to simultaneously model batches of individuals with\na common mean function and a specific covariance structure. This common mean is\ndefined as a Gaussian process for which the hyper-posterior distribution is\ntractable. Therefore an EM algorithm can be derived for simultaneous\nhyper-parameters optimisation and hyper-posterior computation. Unlike previous\napproaches in the literature, we account for uncertainty and handle uncommon\ngrids of observations while maintaining explicit formulations, by modelling the\nmean process in a non-parametric probabilistic framework. We also provide\npredictive formulas integrating this common mean process. This approach greatly\nimproves the predictive performance far from observations, where information\nshared across individuals provides a relevant prior mean. Our overall algorithm\nis called \\textsc{Magma} (standing for Multi tAsk Gaussian processes with\ncommon MeAn), and publicly available as a R package. The quality of the mean\nprocess estimation, predictive performances, and comparisons to alternatives\nare assessed in various simulated scenarios and on real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 11:43:54 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Leroy", "Arthur", ""], ["Latouche", "Pierre", ""], ["Guedj", "Benjamin", ""], ["Gey", "Servane", ""]]}, {"id": "2007.10979", "submitter": "Jeffrey Wong", "authors": "Jeffrey C. Wong", "title": "Computational Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce computational causal inference as an interdisciplinary field\nacross causal inference, algorithms design and numerical computing. The field\naims to develop software specializing in causal inference that can analyze\nmassive datasets with a variety of causal effects, in a performant, general,\nand robust way. The focus on software improves research agility, and enables\ncausal inference to be easily integrated into large engineering systems. In\nparticular, we use computational causal inference to deepen the relationship\nbetween causal inference, online experimentation, and algorithmic decision\nmaking.\n  This paper describes the new field, the demand, opportunities for\nscalability, open challenges, and begins the discussion for how the community\ncan unite to solve challenges for scaling causal inference and decision making.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 17:57:43 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wong", "Jeffrey C.", ""]]}, {"id": "2007.11111", "submitter": "Nikos Pitsianis", "authors": "Dimitris Floros and Nikos Pitsianis and Xiaobai Sun", "title": "Fast Graphlet Transform of Sparse Graphs", "comments": "To appear in the Proceedings of High Performance Extreme Computing\n  (HPEC) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.DM math.AT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the computational problem of graphlet transform of a sparse\nlarge graph. Graphlets are fundamental topology elements of all\ngraphs/networks. They can be used as coding elements to encode\ngraph-topological information at multiple granularity levels for classifying\nvertices on the same graph/network as well as for making differentiation or\nconnection across different networks. Network/graph analysis using graphlets\nhas growing applications. We recognize the universality and increased encoding\ncapacity in using multiple graphlets, we address the arising computational\ncomplexity issues, and we present a fast method for exact graphlet transform.\nThe fast graphlet transform establishes a few remarkable records at once in\nhigh computational efficiency, low memory consumption, and ready translation to\nhigh-performance program and implementation. It is intended to enable and\nadvance network/graph analysis with graphlets, and to introduce the relatively\nnew analysis apparatus to graph theory, high-performance graph computation, and\nbroader applications.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jul 2020 21:58:42 GMT"}, {"version": "v2", "created": "Fri, 31 Jul 2020 03:25:07 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 21:40:18 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Floros", "Dimitris", ""], ["Pitsianis", "Nikos", ""], ["Sun", "Xiaobai", ""]]}, {"id": "2007.11461", "submitter": "Wendy Cho", "authors": "Wendy K. Tam Cho and Yan Y. Liu", "title": "A Parallel Evolutionary Multiple-Try Metropolis Markov Chain Monte Carlo\n  Algorithm for Sampling Spatial Partitions", "comments": null, "journal-ref": "Statistics and Computing 31, Article 10 (2021)", "doi": "10.1007/s11222-020-09977-z", "report-no": null, "categories": "stat.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an Evolutionary Markov Chain Monte Carlo (EMCMC) algorithm for\nsampling spatial partitions that lie within a large and complex spatial state\nspace. Our algorithm combines the advantages of evolutionary algorithms (EAs)\nas optimization heuristics for state space traversal and the theoretical\nconvergence properties of Markov Chain Monte Carlo algorithms for sampling from\nunknown distributions. Local optimality information that is identified via a\ndirected search by our optimization heuristic is used to adaptively update a\nMarkov chain in a promising direction within the framework of a Multiple-Try\nMetropolis Markov Chain model that incorporates a generalized\nMetropolis-Hasting ratio. We further expand the reach of our EMCMC algorithm by\nharnessing the computational power afforded by massively parallel architecture\nthrough the integration of a parallel EA framework that guides Markov chains\nrunning in parallel.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 14:28:44 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Cho", "Wendy K. Tam", ""], ["Liu", "Yan Y.", ""]]}, {"id": "2007.11521", "submitter": "Alexandre M\\\"osching", "authors": "Alexandre M\\\"osching and Lutz D\\\"umbgen", "title": "Estimation of a Likelihood Ratio Ordered Family of Distributions -- with\n  a Connection to Total Positivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider bivariate observations $(X_1,Y_1), \\ldots, (X_n,Y_n) \\in\n\\mathbb{R}\\times \\mathbb{R}$ with unknown conditional distributions $Q_x$ of\n$Y$, given that $X = x$. The goal is to estimate these distributions under the\nsole assumption that $Q_x$ is isotonic in $x$ with respect to likelihood ratio\norder. If the observations are identically distributed, a related goal is to\nestimate the joint distribution $\\mathcal{L}(X,Y)$ under the sole assumption\nthat it is totally positive of order two in a certain sense. After reviewing\nand generalizing the concepts of likelihood ratio order and total positivity of\norder two, an algorithm is developed which estimates the unknown family of\ndistributions $(Q_x)_x$ via empirical likelihood. The benefit of the stronger\nregularization imposed by likelihood ratio order over the usual stochastic\norder is evaluated in terms of estimation and predictive performances on\nsimulated as well as real data.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 16:28:32 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 16:18:46 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["M\u00f6sching", "Alexandre", ""], ["D\u00fcmbgen", "Lutz", ""]]}, {"id": "2007.11612", "submitter": "Murat A. Erdogdu", "authors": "Murat A. Erdogdu, Rasa Hosseinzadeh, Matthew S. Zhang", "title": "Convergence of Langevin Monte Carlo in Chi-Squared and Renyi Divergence", "comments": "v1: There was an error in the proof of Lemma 1. Authors thank Andre\n  Wibisono for noticing this and letting us know. v2: Paper is updated with an\n  opaque condition, in order not to mislead researchers. v3: Opaque condition\n  in the previous version is proved under LSI and strong dissipativity. v4:\n  Results on Renyi divergence are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sampling from a target distribution $\\nu_* = e^{-f}$ using the\nunadjusted Langevin Monte Carlo (LMC) algorithm when the potential $f$\nsatisfies a strong dissipativity condition and it is first-order smooth with a\nLipschitz gradient. We prove that, initialized with a Gaussian random vector\nthat has sufficiently small variance, iterating the LMC algorithm for\n$\\widetilde{\\mathcal{O}}(\\lambda^2 d\\epsilon^{-1})$ steps is sufficient to\nreach $\\epsilon$-neighborhood of the target in both Chi-squared and Renyi\ndivergence, where $\\lambda$ is the logarithmic Sobolev constant of $\\nu_*$. Our\nresults do not require warm-start to deal with the exponential dimension\ndependency in Chi-squared divergence at initialization. In particular, for\nstrongly convex and first-order smooth potentials, we show that the LMC\nalgorithm achieves the rate estimate $\\widetilde{\\mathcal{O}}(d\\epsilon^{-1})$\nwhich improves the previously known rates in both of these metrics, under the\nsame assumptions. Translating this rate to other metrics, our results also\nrecover the state-of-the-art rate estimates in KL divergence, total variation\nand $2$-Wasserstein distance in the same setup. Finally, as we rely on the\nlogarithmic Sobolev inequality, our framework covers a range of non-convex\npotentials that are first-order smooth and exhibit strong convexity outside of\na compact region.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jul 2020 18:18:28 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 16:23:22 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 17:57:14 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 06:45:09 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Erdogdu", "Murat A.", ""], ["Hosseinzadeh", "Rasa", ""], ["Zhang", "Matthew S.", ""]]}, {"id": "2007.11838", "submitter": "Alexander Lew", "authors": "Alexander K. Lew, Monica Agrawal, David Sontag, Vikash K. Mansinghka", "title": "PClean: Bayesian Data Cleaning at Scale with Domain-Specific\n  Probabilistic Programming", "comments": "Correct formatting error", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data cleaning can be naturally framed as probabilistic inference in a\ngenerative model, combining a prior distribution over ground-truth databases\nwith a likelihood that models the noisy channel by which the data are filtered\nand corrupted to yield incomplete, dirty, and denormalized datasets. Based on\nthis view, we present PClean, a probabilistic programming language for\nleveraging dataset-specific knowledge to clean and normalize dirty data. PClean\nis powered by three modeling and inference contributions: (1) a non-parametric\nmodel of relational database instances, customizable via probabilistic\nprograms, (2) a sequential Monte Carlo inference algorithm that exploits the\nmodel's structure, and (3) near-optimal SMC proposals and blocked Gibbs\nrejuvenation moves constructed on a per-dataset basis. We show empirically that\nshort (< 50-line) PClean programs can be faster and more accurate than generic\nPPL inference on multiple data-cleaning benchmarks; perform comparably in terms\nof accuracy and runtime to state-of-the-art data-cleaning systems (unlike\ngeneric PPL inference given the same runtime); and scale to real-world datasets\nwith millions of records.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 08:01:47 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 05:19:18 GMT"}, {"version": "v3", "created": "Sun, 25 Oct 2020 21:07:53 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 18:41:52 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Lew", "Alexander K.", ""], ["Agrawal", "Monica", ""], ["Sontag", "David", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2007.11919", "submitter": "Pedro Delicado", "authors": "Pedro Delicado and Cristian Pachon-Garcia", "title": "Multidimensional Scaling for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a set of algorithms for Multidimensional Scaling (MDS) to be used\nwith large datasets. MDS is a statistic tool for reduction of dimensionality,\nusing as input a distance matrix of dimensions $n \\times n$. When $n$ is large,\nclassical algorithms suffer from computational problems and MDS configuration\ncan not be obtained. In this paper we address these problems by means of three\nalgorithms: Divide and Conquer MDS, Fast MDS and MDS based on Gower\ninterpolation (the first and the last being original proposals). The main ideas\nof these methods are based on partitioning the dataset into small pieces, where\nclassical MDS methods can work. In order to check the performance of the\nalgorithms as well as to compare them, we do a simulation study. This study\npoints out that Fast MDS and MDS based on Gower interpolation are appropriated\nto use when $n$ is large. Although Divide and Conquer MDS is not as fast as the\nother two algorithms, it is the best method that captures the variance of the\noriginal data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 10:49:00 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 15:29:54 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Delicado", "Pedro", ""], ["Pachon-Garcia", "Cristian", ""]]}, {"id": "2007.11936", "submitter": "Pierre E. Jacob", "authors": "Chenguang Dai, Jeremy Heng, Pierre E. Jacob, Nick Whiteley", "title": "An invitation to sequential Monte Carlo samplers", "comments": "review article, 34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo samplers provide consistent approximations of\nsequences of probability distributions and of their normalizing constants, via\nparticles obtained with a combination of importance weights and Markov\ntransitions. This article presents this class of methods and a number of recent\nadvances, with the goal of helping statisticians assess the applicability and\nusefulness of these methods for their purposes. Our presentation emphasizes the\nrole of bridging distributions for computational and statistical purposes.\nNumerical experiments are provided on simple settings such as multivariate\nNormals, logistic regression and a basic susceptible-infected-recovered model,\nillustrating the impact of the dimension, the ability to perform inference\nsequentially and the estimation of normalizing constants.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 11:26:01 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Dai", "Chenguang", ""], ["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""], ["Whiteley", "Nick", ""]]}, {"id": "2007.12175", "submitter": "Tomas Masak", "authors": "Tomas Masak, Soham Sarkar, Victor M. Panaretos", "title": "Principal Separable Component Analysis via the Partial Inner Product", "comments": "23 pages + appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The non-parametric estimation of covariance lies at the heart of functional\ndata analysis, whether for curve or surface-valued data. The case of a\ntwo-dimensional domain poses both statistical and computational challenges,\nwhich are typically alleviated by assuming separability. However, separability\nis often questionable, sometimes even demonstrably inadequate. We propose a\nframework for the analysis of covariance operators of random surfaces that\ngeneralises separability, while retaining its major advantages. Our approach is\nbased on the additive decomposition of the covariance into a series of\nseparable components. The decomposition is valid for any covariance over a\ntwo-dimensional domain. Leveraging the key notion of the partial inner product,\nwe generalise the power iteration method to general Hilbert spaces and show how\nthe aforementioned decomposition can be efficiently constructed in practice.\nTruncation of the decomposition and retention of the principal separable\ncomponents automatically induces a non-parametric estimator of the covariance,\nwhose parsimony is dictated by the truncation level. The resulting estimator\ncan be calculated, stored and manipulated with little computational overhead\nrelative to separability. The framework and estimation method are genuinely\nnon-parametric, since the considered decomposition holds for any covariance.\nConsistency and rates of convergence are derived under mild regularity\nassumptions, illustrating the trade-off between bias and variance regulated by\nthe truncation level. The merits and practical performance of the proposed\nmethodology are demonstrated in a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 12:21:21 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Masak", "Tomas", ""], ["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2007.12249", "submitter": "Ines Wilms", "authors": "Stephan Smeekes and Ines Wilms", "title": "bootUR: An R Package for Bootstrap Unit Root Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unit root tests form an essential part of any time series analysis. We\nprovide practitioners with a single, unified framework for comprehensive and\nreliable unit root testing in the R package bootUR. The package's backbone is\nthe popular augmented Dickey-Fuller (ADF) test paired with a union of\nrejections principle, which can be performed directly on single time series or\nmultiple (including panel) time series. Accurate inference is ensured through\nthe use of bootstrap methods.The package addresses the needs of both novice\nusers, by providing user-friendly and easy-to-implement functions with sensible\ndefault options, as well as expert users, by giving full user-control to adjust\nthe tests to one's desired settings. Our OpenMP-parallelized efficient C++\nimplementation ensures that all unit root tests are scalable to datasets\ncontaining many time series.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jul 2020 20:40:52 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 06:45:04 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Smeekes", "Stephan", ""], ["Wilms", "Ines", ""]]}, {"id": "2007.12364", "submitter": "Dana Lahat", "authors": "Dana Lahat, Yanbin Lang, Vincent Y. F. Tan, C\\'edric F\\'evotte", "title": "Positive Semidefinite Matrix Factorization: A Connection with Phase\n  Retrieval and Affine Rank Minimization", "comments": "18 pages (16 paper + 2 supplementary material), 9 figures, accepted\n  for publication in the IEEE Transactions on Signal Processing. This is a\n  revised version: there is a new additional PSDMF algorithm based on CGIHT,\n  more numerical experiments, and some background material moved to\n  Supplementary Material (pages 17 and 18 in this document). Supplementary\n  Material also contains some extra figures", "journal-ref": null, "doi": "10.1109/TSP.2021.3071293", "report-no": null, "categories": "eess.SP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Positive semidefinite matrix factorization (PSDMF) expresses each entry of a\nnonnegative matrix as the inner product of two positive semidefinite (psd)\nmatrices. When all these psd matrices are constrained to be diagonal, this\nmodel is equivalent to nonnegative matrix factorization. Applications include\ncombinatorial optimization, quantum-based statistical models, and recommender\nsystems, among others. However, despite the increasing interest in PSDMF, only\na few PSDMF algorithms were proposed in the literature. In this work, we\nprovide a collection of tools for PSDMF, by showing that PSDMF algorithms can\nbe designed based on phase retrieval (PR) and affine rank minimization (ARM)\nalgorithms. This procedure allows a shortcut in designing new PSDMF algorithms,\nas it allows to leverage some of the useful numerical properties of existing PR\nand ARM methods to the PSDMF framework. Motivated by this idea, we introduce a\nnew family of PSDMF algorithms based on iterative hard thresholding (IHT). This\nfamily subsumes previously-proposed projected gradient PSDMF methods. We show\nthat there is high variability among PSDMF optimization problems that makes it\nbeneficial to try a number of methods based on different principles to tackle\ndifficult problems. In certain cases, our proposed methods are the only\nalgorithms able to find a solution. In certain other cases, they converge\nfaster. Our results support our claim that the PSDMF framework can inherit\ndesired numerical properties from PR and ARM algorithms, leading to more\nefficient PSDMF algorithms, and motivate further study of the links between\nthese models.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jul 2020 06:10:19 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 21:26:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Lahat", "Dana", ""], ["Lang", "Yanbin", ""], ["Tan", "Vincent Y. F.", ""], ["F\u00e9votte", "C\u00e9dric", ""]]}, {"id": "2007.12786", "submitter": "Siva Rajesh Kasa", "authors": "Siva Rajesh Kasa, Vaibhav Rajan", "title": "Model-based Clustering using Automatic Differentiation: Confronting\n  Misspecification and High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study two practically important cases of model based clustering using\nGaussian Mixture Models: (1) when there is misspecification and (2) on high\ndimensional data, in the light of recent advances in Gradient Descent (GD)\nbased optimization using Automatic Differentiation (AD). Our simulation studies\nshow that EM has better clustering performance, measured by Adjusted Rand\nIndex, compared to GD in cases of misspecification, whereas on high dimensional\ndata GD outperforms EM. We observe that both with EM and GD there are many\nsolutions with high likelihood but poor cluster interpretation. To address this\nproblem we design a new penalty term for the likelihood based on the Kullback\nLeibler divergence between pairs of fitted components. Closed form expressions\nfor the gradients of this penalized likelihood are difficult to derive but AD\ncan be done effortlessly, illustrating the advantage of AD-based optimization.\nExtensions of this penalty for high dimensional data and for model selection\nare discussed. Numerical experiments on synthetic and real datasets demonstrate\nthe efficacy of clustering using the proposed penalized likelihood approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jul 2020 10:56:05 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kasa", "Siva Rajesh", ""], ["Rajan", "Vaibhav", ""]]}, {"id": "2007.12852", "submitter": "Mingyuan Zhou", "authors": "Rahi Kalantari and Mingyuan Zhou", "title": "Graph Gamma Process Generalized Linear Dynamical Systems", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce graph gamma process (GGP) linear dynamical systems to model\nreal-valued multivariate time series. For temporal pattern discovery, the\nlatent representation under the model is used to decompose the time series into\na parsimonious set of multivariate sub-sequences. In each sub-sequence,\ndifferent data dimensions often share similar temporal patterns but may exhibit\ndistinct magnitudes, and hence allowing the superposition of all sub-sequences\nto exhibit diverse behaviors at different data dimensions. We further\ngeneralize the proposed model by replacing the Gaussian observation layer with\nthe negative binomial distribution to model multivariate count time series.\nGenerated from the proposed GGP is an infinite dimensional directed sparse\nrandom graph, which is constructed by taking the logical OR operation of\ncountably infinite binary adjacency matrices that share the same set of\ncountably infinite nodes. Each of these adjacency matrices is associated with a\nweight to indicate its activation strength, and places a finite number of edges\nbetween a finite subset of nodes belonging to the same node community. We use\nthe generated random graph, whose number of nonzero-degree nodes is finite, to\ndefine both the sparsity pattern and dimension of the latent state transition\nmatrix of a (generalized) linear dynamical system. The activation strength of\neach node community relative to the overall activation strength is used to\nextract a multivariate sub-sequence, revealing the data pattern captured by the\ncorresponding community. On both synthetic and real-world time series, the\nproposed nonparametric Bayesian dynamic models, which are initialized at\nrandom, consistently exhibit good predictive performance in comparison to a\nvariety of baseline models, revealing interpretable latent state transition\npatterns and decomposing the time series into distinctly behaved sub-sequences.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jul 2020 04:16:34 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Kalantari", "Rahi", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2007.13140", "submitter": "Wenyang Wang", "authors": "Wenyang Wang, Dongchu Sun, Zhuoqiong He", "title": "Fully Bayesian Analysis of the Relevance Vector Machine Classification\n  for Imbalanced Data", "comments": "24 Pages, 3 figures, preprint to submit to Electronic Journal of\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relevance Vector Machine (RVM) is a supervised learning algorithm extended\nfrom Support Vector Machine (SVM) based on the Bayesian sparsity model.\nCompared with the regression problem, RVM classification is difficult to be\nconducted because there is no closed-form solution for the weight parameter\nposterior. Original RVM classification algorithm used Newton's method in\noptimization to obtain the mode of weight parameter posterior then approximated\nit by a Gaussian distribution in Laplace's method. It would work but just\napplied the frequency methods in a Bayesian framework. This paper proposes a\nGeneric Bayesian approach for the RVM classification. We conjecture that our\nalgorithm achieves convergent estimates of the quantities of interest compared\nwith the nonconvergent estimates of the original RVM classification algorithm.\nFurthermore, a Fully Bayesian approach with the hierarchical hyperprior\nstructure for RVM classification is proposed, which improves the classification\nperformance, especially in the imbalanced data problem. By the numeric studies,\nour proposed algorithms obtain high classification accuracy rates. The Fully\nBayesian hierarchical hyperprior method outperforms the Generic one for the\nimbalanced data classification.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:53:36 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Wang", "Wenyang", ""], ["Sun", "Dongchu", ""], ["He", "Zhuoqiong", ""]]}, {"id": "2007.13869", "submitter": "Ruda Zhang", "authors": "Ruda Zhang and Roger Ghanem", "title": "Normal-bundle Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.DG math.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models of data sets often exhibit salient geometric structure.\nSuch a phenomenon is summed up in the manifold distribution hypothesis, and can\nbe exploited in probabilistic learning. Here we present normal-bundle bootstrap\n(NBB), a method that generates new data which preserve the geometric structure\nof a given data set. Inspired by algorithms for manifold learning and concepts\nin differential geometry, our method decomposes the underlying probability\nmeasure into a marginalized measure on a learned data manifold and conditional\nmeasures on the normal spaces. The algorithm estimates the data manifold as a\ndensity ridge, and constructs new data by bootstrapping projection vectors and\nadding them to the ridge. We apply our method to the inference of density ridge\nand related statistics, and data augmentation to reduce overfitting.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 21:14:19 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Zhang", "Ruda", ""], ["Ghanem", "Roger", ""]]}, {"id": "2007.13930", "submitter": "Shanyin Tong", "authors": "Shanyin Tong, Eric Vanden-Eijnden and Georg Stadler", "title": "Extreme event probability estimation using PDE-constrained optimization\n  and large deviation theory, with application to tsunamis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.NA math.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and compare methods for the analysis of extreme events in complex\nsystems governed by PDEs that involve random parameters, in situations where we\nare interested in quantifying the probability that a scalar function of the\nsystem's solution is above a threshold. If the threshold is large, this\nprobability is small and its accurate estimation is challenging. To tackle this\ndifficulty, we blend theoretical results from large deviation theory (LDT) with\nnumerical tools from PDE-constrained optimization. Our methods first compute\nparameters that minimize the LDT-rate function over the set of parameters\nleading to extreme events, using adjoint methods to compute the gradient of\nthis rate function. The minimizers give information about the mechanism of the\nextreme events as well as estimates of their probability. We then propose a\nseries of methods to refine these estimates, either via importance sampling or\ngeometric approximation of the extreme event sets. Results are formulated for\ngeneral parameter distributions and detailed expressions are provided when\nGaussian distributions. We give theoretical and numerical arguments showing\nthat the performance of our methods is insensitive to the extremeness of the\nevents we are interested in. We illustrate the application of our approach to\nquantify the probability of extreme tsunami events on shore. Tsunamis are\ntypically caused by a sudden, unpredictable change of the ocean floor elevation\nduring an earthquake. We model this change as a random process, which takes\ninto account the underlying physics. We use the one-dimensional shallow water\nequation to model tsunamis numerically. In the context of this example, we\npresent a comparison of our methods for extreme event probability estimation,\nand find which type of ocean floor elevation change leads to the largest\ntsunamis on shore.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 01:19:26 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 14:28:20 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 17:59:45 GMT"}, {"version": "v4", "created": "Sun, 14 Mar 2021 23:52:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Tong", "Shanyin", ""], ["Vanden-Eijnden", "Eric", ""], ["Stadler", "Georg", ""]]}, {"id": "2007.14080", "submitter": "Shuang Song", "authors": "Wei Jiang, Shuang Song, Lin Hou, Hongyu Zhao", "title": "A set of efficient methods to generate high-dimensional binary data with\n  specified correlation structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional correlated binary data arise in many areas, such as observed\ngenetic variations in biomedical research. Data simulation can help researchers\nevaluate efficiency and explore properties of different computational and\nstatistical methods. Also, some statistical methods, such as Monte-Carlo\nmethods, rely on data simulation. Lunn and Davies (1998) proposed linear time\ncomplexity methods to generate correlated binary variables with three common\ncorrelation structures. However, it is infeasible to specify unequal\nprobabilities in their methods. In this manuscript, we introduce several\ncomputationally efficient algorithms that generate high-dimensional binary data\nwith specified correlation structures and unequal probabilities. Our algorithms\nhave linear time complexity with respect to the dimension for three commonly\nstudied correlation structures, namely exchangeable, decaying-product and\nK-dependent correlation structures. In addition, we extend our algorithms to\ngenerate binary data of general non-negative correlation matrices with\nquadratic time complexity. We provide an R package, CorBin, to implement our\nsimulation methods. Compared to the existing packages for binary data\ngeneration, the time cost to generate a 100-dimensional binary vector with the\ncommon correlation structures and general correlation matrices can be reduced\nup to $10^5$ folds and $10^3$ folds, respectively, and the efficiency can be\nfurther improved with the increase of dimensions. The R package CorBin is\navailable on CRAN at https://cran.r-project.org/.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 09:22:46 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Jiang", "Wei", ""], ["Song", "Shuang", ""], ["Hou", "Lin", ""], ["Zhao", "Hongyu", ""]]}, {"id": "2007.14109", "submitter": "Alessandro Gasparini", "authors": "Emma C. Martin and Alessandro Gasparini and Michael J. Crowther", "title": "merlin: An R package for Mixed Effects Regression for Linear, Nonlinear\n  and User-defined models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package merlin performs flexible joint modelling of hierarchical\nmulti-outcome data. Increasingly, multiple longitudinal biomarker measurements,\npossibly censored time-to-event outcomes and baseline characteristics are\navailable. However, there is limited software that allows all of this\ninformation to be incorporated into one model. In this paper, we present merlin\nwhich allows for the estimation of models with unlimited numbers of continuous,\nbinary, count and time-to-event outcomes, with unlimited levels of nested\nrandom effects. A wide variety of link functions, including the expected value,\nthe gradient and shared random effects, are available in order to link the\ndifferent outcomes in a biologically plausible way. The accompanying\npredict.merlin function allows for individual and population level predictions\nto be made from even the most complex models. There is the option to specify\nuser-defined families, making merlin ideal for methodological research. The\nflexibility of merlin is illustrated using an example in patients followed up\nafter heart valve replacement, beginning with a linear model, and finishing\nwith a joint multiple longitudinal and competing risks survival model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 10:26:00 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Martin", "Emma C.", ""], ["Gasparini", "Alessandro", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2007.14220", "submitter": "Krzysztof Podgorski", "authors": "Georg Lindgren, Krzysztof Podgorski, Igor Rychlik", "title": "Effective computations of joint excursion times for stationary Gaussian\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is to popularize the method of computing the distribution of the\nexcursion times for a Gaussian process that involves extended and multivariate\nRice's formula. The approach was used in numerical implementations of the\nhigh-dimensional integration routine and in earlier work it was shown that the\ncomputations are more effective and thus more precise than those based on Rice\nexpansions.\n  The joint distribution of excursion times is related to the distribution of\nthe number of level crossings, a problem that can be attacked via the Rice\nseries expansion, based on the moments of the number of crossings. Another\npoint of attack is the \"Independent Interval Approximation\" intensively studied\nfor the persistence of physical systems. It treats the lengths of successive\ncrossing intervals as statistically independent. A renewal type argument leads\nto an expression that provides the approximate interval distribution via its\nLaplace transform.\n  However, independence is not valid in typical situations. Even if it leads to\nacceptable results for the persistency exponent, rigorous assessment of the\napproximation error is not available. Moreover, we show that the IIA approach\ncannot deliver properly defined probability distributions and thus the method\nis limited to persistence studies.\n  This paper presents an alternative approach that is both more general, more\naccurate, and relatively unknown. It is based on exact expressions for the\nprobability density for one and for two successive excursion lengths. The\nnumerical routine RIND computes the densities using recent advances in\nscientific computing and is easily accessible via a simple Matlab interface.\nThe result solves the problem of two-step excursion dependence for a general\nstationary differentiable Gaussian process. The work offers also some\nanalytical results that explain the effectiveness of the implemented method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 13:48:21 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Lindgren", "Georg", ""], ["Podgorski", "Krzysztof", ""], ["Rychlik", "Igor", ""]]}, {"id": "2007.14299", "submitter": "Rapha\\\"elle Momal", "authors": "Rapha\\\"elle Momal, St\\'ephane Robin, Christophe Ambroise", "title": "Accounting for missing actors in interaction network inference from\n  abundance data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network inference aims at unraveling the dependency structure relating\njointly observed variables. Graphical models provide a general framework to\ndistinguish between marginal and conditional dependency. Unobserved variables\n(missing actors) may induce apparent conditional dependencies.In the context of\ncount data, we introduce a mixture of Poisson log-normal distributions with\ntree-shaped graphical models, to recover the dependency structure, including\nmissing actors. We design a variational EM algorithm and assess its performance\non synthetic data. We demonstrate the ability of our approach to recover\nenvironmental drivers on two ecological datasets. The corresponding R package\nis available from github.com/Rmomal/nestor.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 15:15:19 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Momal", "Rapha\u00eblle", ""], ["Robin", "St\u00e9phane", ""], ["Ambroise", "Christophe", ""]]}, {"id": "2007.14440", "submitter": "Hillary Fairbanks", "authors": "Hillary R. Fairbanks, Umberto Villa, and Panayot S. Vassilevski", "title": "Multilevel Hierarchical Decomposition of Finite Element White Noise with\n  Application to Multilevel Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a new hierarchical multilevel approach to generate\nGaussian random field realizations in an algorithmically scalable manner that\nis well-suited to incorporate into multilevel Markov chain Monte Carlo (MCMC)\nalgorithms. This approach builds off of other partial differential equation\n(PDE) approaches for generating Gaussian random field realizations; in\nparticular, a single field realization may be formed by solving a\nreaction-diffusion PDE with a spatial white noise source function as the\nrighthand side. While these approaches have been explored to accelerate forward\nuncertainty quantification tasks, e.g. multilevel Monte Carlo, the previous\nconstructions are not directly applicable to multilevel MCMC frameworks which\nbuild fine scale random fields in a hierarchical fashion from coarse scale\nrandom fields. Our new hierarchical multilevel method relies on a hierarchical\ndecomposition of the white noise source function in $L^2$ which allows us to\nform Gaussian random field realizations across multiple levels of\ndiscretization in a way that fits into multilevel MCMC algorithmic frameworks.\nAfter presenting our main theoretical results and numerical scaling results to\nshowcase the utility of this new hierarchical PDE method for generating\nGaussian random field realizations, this method is tested on a four-level MCMC\nalgorithm to explore its feasibility.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 19:19:15 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 23:16:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Fairbanks", "Hillary R.", ""], ["Villa", "Umberto", ""], ["Vassilevski", "Panayot S.", ""]]}, {"id": "2007.14476", "submitter": "Ahmed Attia", "authors": "Ahmed Attia and Emil Constantinescu", "title": "Optimal Experimental Design for Inverse Problems in the Presence of\n  Observation Correlations", "comments": "40 pages, 20 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal experimental design (OED) is the general formalism of sensor\nplacement and decisions about the data collection strategy for engineered or\nnatural experiments. This approach is prevalent in many critical fields such as\nbattery design, numerical weather prediction, geosciences, and environmental\nand urban studies. State-of-the-art computational methods for experimental\ndesign, however, do not accommodate correlation structure in observational\nerrors produced by many expensive-to-operate devices such as X-ray machines,\nradars, and satellites. Discarding evident data correlations leads to biased\nresults, higher expenses, and waste of valuable resources. We present a general\nformulation of the OED formalism for model-constrained large-scale Bayesian\nlinear inverse problems, where measurement errors are generally correlated. The\nproposed approach utilizes the Hadamard product of matrices to formulate the\nweighted likelihood and is valid for both finite- and infinite-dimensional\nBayesian inverse problems. Extensive numerical experiments are carried out for\nempirical verification of the proposed approach using an advection-diffusion\nmodel, where the objective is to optimally place a small set of sensors, under\na limited budget, to predict the concentration of a contaminant in a closed and\nbounded domain.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 20:43:55 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Attia", "Ahmed", ""], ["Constantinescu", "Emil", ""]]}, {"id": "2007.14495", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers, Peter J. Rousseeuw, Mia Hubert", "title": "Class maps for visualizing classification results", "comments": "Appeared online, Technometrics", "journal-ref": null, "doi": "10.1080/00401706.2021.1927849", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a major tool of statistics and machine learning. A\nclassification method first processes a training set of objects with given\nclasses (labels), with the goal of afterward assigning new objects to one of\nthese classes. When running the resulting prediction method on the training\ndata or on test data, it can happen that an object is predicted to lie in a\nclass that differs from its given label. This is sometimes called label bias,\nand raises the question whether the object was mislabeled. The proposed class\nmap reflects the probability that an object belongs to an alternative class,\nhow far it is from the other objects in its given class, and whether some\nobjects lie far from all classes. The goal is to visualize aspects of the\nclassification results to obtain insight in the data. The display is\nconstructed for discriminant analysis, the k-nearest neighbor classifier,\nsupport vector machines, logistic regression, and coupling pairwise\nclassifications. It is illustrated on several benchmark datasets, including\nsome about images and texts.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jul 2020 21:27:15 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 15:34:25 GMT"}, {"version": "v3", "created": "Wed, 19 May 2021 13:51:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Hubert", "Mia", ""]]}, {"id": "2007.14900", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis, Lambros Mertzanis, Athina Panotopoulou, Ioannis\n  Papageorgiou, and Maria Skoularidou", "title": "Bayesian Context Trees: Modelling and exact inference for discrete time\n  series", "comments": "50 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new Bayesian modelling framework for the class of higher-order,\nvariable-memory Markov chains, and introduce an associated collection of\nmethodological tools for exact inference with discrete time series. We show\nthat a version of the context tree weighting algorithm can compute the prior\npredictive likelihood exactly (averaged over both models and parameters), and\ntwo related algorithms are introduced, which identify the a posteriori most\nlikely models and compute their exact posterior probabilities. All three\nalgorithms are deterministic and have linear-time complexity. A family of\nvariable-dimension Markov chain Monte Carlo samplers is also provided,\nfacilitating further exploration of the posterior. The performance of the\nproposed methods in model selection, Markov order estimation and prediction is\nillustrated through simulation experiments and real-world applications with\ndata from finance, genetics, neuroscience, and animal communication.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 15:16:49 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Mertzanis", "Lambros", ""], ["Panotopoulou", "Athina", ""], ["Papageorgiou", "Ioannis", ""], ["Skoularidou", "Maria", ""]]}, {"id": "2007.14927", "submitter": "Lihan Wang", "authors": "Jianfeng Lu and Lihan Wang", "title": "On explicit $L^2$-convergence rate estimate for piecewise deterministic\n  Markov processes in MCMC algorithms", "comments": "Under minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish $L^2$-exponential convergence rate for three popular piecewise\ndeterministic Markov processes for sampling: the randomized Hamiltonian Monte\nCarlo method, the zigzag process, and the bouncy particle sampler. Our analysis\nis based on a variational framework for hypocoercivity, which combines a\nPoincar\\'{e}-type inequality in time-augmented state space and a standard $L^2$\nenergy estimate. Our analysis provides explicit convergence rate estimates,\nwhich are more quantitative than existing results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jul 2020 16:00:56 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 20:46:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lu", "Jianfeng", ""], ["Wang", "Lihan", ""]]}, {"id": "2007.15195", "submitter": "Mauricio Nascimento", "authors": "Mauricio Nascimento and Benjamin A. Shaby", "title": "A Vecchia Approximation for High-Dimensional Gaussian Cumulative\n  Distribution Functions Arising from Spatial Data", "comments": "19 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to quickly and accurately approximate the cumulative\ndistribution function of multivariate Gaussian distributions arising from\nspatial Gaussian processes. This approximation is trivially parallelizable and\nsimple to implement using standard software. We demonstrate its accuracy and\ncomputational efficiency in a series of simulation experiments and apply it to\nanalyzing the joint tail of a large precipitation dataset using a\nrecently-proposed scale mixture model for spatial extremes. This dataset is\nmany times larger than what was previously considered possible to fit using\npreferred inferential techniques.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jul 2020 02:52:39 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Nascimento", "Mauricio", ""], ["Shaby", "Benjamin A.", ""]]}, {"id": "2007.15862", "submitter": "Niharika Gauraha", "authors": "Niharika Gauraha", "title": "A Note on Particle Gibbs Method and its Extensions and Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional state trajectories of state-space models pose challenges for\nBayesian inference. Particle Gibbs (PG) methods have been widely used to sample\nfrom the posterior of a state space model. Basically, particle Gibbs is a\nParticle Markov Chain Monte Carlo (PMCMC) algorithm that mimics the Gibbs\nsampler by drawing model parameters and states from their conditional\ndistributions.\n  This tutorial provides an introductory view on Particle Gibbs (PG) method and\nits extensions and variants, and illustrates through several examples of\ninference in non-linear state space models (SSMs). We also implement PG\nSamplers in two different programming languages: Python and Rust. Comparison of\nrun-time performance of Python and Rust programs are also provided for various\nPG methods.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 06:15:03 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 04:39:50 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Gauraha", "Niharika", ""]]}, {"id": "2007.15934", "submitter": "Alvaro K\\\"ohn-Luque", "authors": "Alvaro K\\\"ohn-Luque, Xiaoran Lai and Arnoldo Frigessi", "title": "Towards personalized computer simulations of breast cancer treatment", "comments": "2 pages, 2 figures, VPH2020 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cancer pathology is unique to a given individual, and developing personalized\ndiagnostic and treatment protocols are a primary concern. Mathematical modeling\nand simulation is a promising approach to personalized cancer medicine. Yet,\nthe complexity, heterogeneity and multiscale nature of cancer present severe\nchallenges. One of the major barriers to use mathematical models to predict the\noutcome of therapeutic regimens in a particular patient lies in their\ninitialization and parameterization in order to reflect individual cancer\ncharacteristics accurately. Here we present a study where we used multitype\nmeasurements acquired routinely on a single breast tumor, including\nhistopathology, magnetic resonance imaging (MRI), and molecular profiling, to\npersonalize a multiscale hybrid cellular automaton model of breast cancer\ntreated with chemotherapeutic and antiangiogenic agents. We model drug\npharmacokinetics and pharmacodynamics at the tumor tissue level but with\ncellular and subcellular resolution. We simulate those spatio-temporal dynamics\nin 2D cross-sections of tumor portions over 12-week therapy regimes, resulting\nin complex and computationally intensive simulations. For such computationally\ndemanding systems, formal statistical inference methods to estimate individual\nparameters from data have not been feasible in practice to until most recently,\nafter the emergence of machine learning techniques applied to likelihood-free\ninference methods. Here we use the inference advances provided by Bayesian\noptimization to fit our model to simulated data of individual patients. In this\nway, we investigate if some key parameters can be estimated from a series of\nmeasurements of cell density in the tumor tissue, as well as how often the\nmeasurements need to be taken to allow reliable predictions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 09:57:27 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["K\u00f6hn-Luque", "Alvaro", ""], ["Lai", "Xiaoran", ""], ["Frigessi", "Arnoldo", ""]]}, {"id": "2007.16010", "submitter": "Subhadip Maji", "authors": "Subhadip Maji, Arijit Ghosh Chowdhury, Raghav Bali and Vamsi M\n  Bhandaru", "title": "Exclusion and Inclusion -- A model agnostic approach to feature\n  importance in DNNs", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks in NLP have enabled systems to learn complex non-linear\nrelationships. One of the major bottlenecks towards being able to use DNNs for\nreal world applications is their characterization as black boxes. To solve this\nproblem, we introduce a model agnostic algorithm which calculates phrase-wise\nimportance of input features. We contend that our method is generalizable to a\ndiverse set of tasks, by carrying out experiments for both Regression and\nClassification. We also observe that our approach is robust to outliers,\nimplying that it only captures the essential aspects of the input.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jul 2020 07:50:53 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Maji", "Subhadip", ""], ["Chowdhury", "Arijit Ghosh", ""], ["Bali", "Raghav", ""], ["Bhandaru", "Vamsi M", ""]]}]