[{"id": "0712.2526", "submitter": "Jon McAuliffe", "authors": "Michael Braun and Jon McAuliffe", "title": "Variational inference for large-scale models of discrete choice", "comments": "29 pages, 2 tables, 2 figures", "journal-ref": "Journal of the American Statistical Association (2010) 105(489):\n  324-334", "doi": "10.1198/jasa.2009.tm08030", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": null, "abstract": "  Discrete choice models are commonly used by applied statisticians in numerous\nfields, such as marketing, economics, finance, and operations research. When\nagents in discrete choice models are assumed to have differing preferences,\nexact inference is often intractable. Markov chain Monte Carlo techniques make\napproximate inference possible, but the computational cost is prohibitive on\nthe large data sets now becoming routinely available. Variational methods\nprovide a deterministic alternative for approximation of the posterior\ndistribution. We derive variational procedures for empirical Bayes and fully\nBayesian inference in the mixed multinomial logit model of discrete choice. The\nalgorithms require only that we solve a sequence of unconstrained optimization\nproblems, which are shown to be convex. Extensive simulations demonstrate that\nvariational methods achieve accuracy competitive with Markov chain Monte Carlo,\nat a small fraction of the computational cost. Thus, variational methods permit\ninferences on data sets that otherwise could not be analyzed without\nbias-inducing modifications to the underlying model.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2007 16:16:18 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2007 18:46:25 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2008 18:03:40 GMT"}], "update_date": "2010-06-04", "authors_parsed": [["Braun", "Michael", ""], ["McAuliffe", "Jon", ""]]}, {"id": "0712.3056", "submitter": "Alicia Johnson", "authors": "Alicia A. Johnson and Galin L. Jones", "title": "Gibbs Sampling for a Bayesian Hierarchical General Linear Model", "comments": "20 pages, 1 figure, submitted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian hierarchical version of the normal theory general\nlinear model which is practically relevant in the sense that it is general\nenough to have many applications and it is not straightforward to sample\ndirectly from the corresponding posterior distribution. Thus we study a block\nGibbs sampler that has the posterior as its invariant distribution. In\nparticular, we establish that the Gibbs sampler converges at a geometric rate.\nThis allows us to establish conditions for a central limit theorem for the\nergodic averages used to estimate features of the posterior. Geometric\nergodicity is also a key component for using batch means methods to\nconsistently estimate the variance of the asymptotic normal distribution.\nTogether, our results give practitioners the tools to be as confident in\ninferences based on the observations from the Gibbs sampler as they would be\nwith inferences based on random samples from the posterior. Our theoretical\nresults are illustrated with an application to data on the cost of health plans\nissued by health maintenance organizations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2007 21:12:02 GMT"}, {"version": "v2", "created": "Thu, 15 May 2008 15:10:31 GMT"}, {"version": "v3", "created": "Mon, 27 Oct 2008 16:37:29 GMT"}, {"version": "v4", "created": "Sat, 17 Oct 2009 19:53:46 GMT"}, {"version": "v5", "created": "Thu, 21 Jan 2010 23:04:50 GMT"}], "update_date": "2010-01-22", "authors_parsed": [["Johnson", "Alicia A.", ""], ["Jones", "Galin L.", ""]]}, {"id": "0712.3618", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, Jin Cao, and Tian Bu", "title": "Network Tomography: Identifiability and Fourier Domain Estimation", "comments": "21 pages", "journal-ref": "IEEE INFOCOM 2007, p.1875-1883", "doi": "10.1109/INFCOM.2007.218", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": null, "abstract": "  The statistical problem for network tomography is to infer the distribution\nof $\\mathbf{X}$, with mutually independent components, from a measurement model\n$\\mathbf{Y}=A\\mathbf{X}$, where $A$ is a given binary matrix representing the\nrouting topology of a network under consideration. The challenge is that the\ndimension of $\\mathbf{X}$ is much larger than that of $\\mathbf{Y}$ and thus the\nproblem is often called ill-posed. This paper studies some statistical aspects\nof network tomography. We first address the identifiability issue and prove\nthat the $\\mathbf{X}$ distribution is identifiable up to a shift parameter\nunder mild conditions. We then use a mixture model of characteristic functions\nto derive a fast algorithm for estimating the distribution of $\\mathbf{X}$\nbased on the General method of Moments. Through extensive model simulation and\nreal Internet trace driven simulation, the proposed approach is shown to be\nfavorable comparing to previous methods using simple discretization for\ninferring link delays in a heterogeneous network.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 03:47:28 GMT"}], "update_date": "2007-12-24", "authors_parsed": [["Chen", "Aiyou", ""], ["Cao", "Jin", ""], ["Bu", "Tian", ""]]}, {"id": "0712.3744", "submitter": "Julien Bect", "authors": "Emmanuel Vazquez and Julien Bect", "title": "Convergence properties of the expected improvement algorithm", "comments": "This paper has been withdrawn", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn from the arXiv. It is now published by Elsevier\nin the Journal of Statistical Planning and Inference, under the modified title\n\"Convergence properties of the expected improvement algorithm with fixed mean\nand covariance functions\". See http://dx.doi.org/10.1016/j.jspi.2010.04.018\n  An author-generated post-print version is available from the HAL repository\nof SUPELEC at http://hal-supelec.archives-ouvertes.fr/hal-00217562\n  Abstract : \"This paper deals with the convergence of the expected improvement\nalgorithm, a popular global optimization algorithm based on a Gaussian process\nmodel of the function to be optimized. The first result is that under some mild\nhypotheses on the covariance function k of the Gaussian process, the expected\nimprovement algorithm produces a dense sequence of evaluation points in the\nsearch domain, when the function to be optimized is in the reproducing kernel\nHilbert space generated by k. The second result states that the density\nproperty also holds for P-almost all continuous functions, where P is the\n(prior) probability distribution induced by the Gaussian process.\"\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2007 16:39:42 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2008 12:29:40 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2009 10:46:33 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2010 07:14:37 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Vazquez", "Emmanuel", ""], ["Bect", "Julien", ""]]}, {"id": "0712.4166", "submitter": "Peter Hoff", "authors": "Peter Hoff", "title": "Simulation of the matrix Bingham-von Mises-Fisher distribution, with\n  applications to multivariate and relational data", "comments": "17 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  Orthonormal matrices play an important role in reduced-rank matrix\napproximations and the analysis of matrix-valued data. A matrix Bingham-von\nMises-Fisher distribution is a probability distribution on the set of\northonormal matrices that includes linear and quadratic terms, and arises as a\nposterior distribution in latent factor models for multivariate and relational\ndata. This article describes rejection and Gibbs sampling algorithms for\nsampling from this family of distributions, and illustrates their use in the\nanalysis of a protein-protein interaction network.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2007 19:59:41 GMT"}], "update_date": "2007-12-28", "authors_parsed": [["Hoff", "Peter", ""]]}, {"id": "0712.4273", "submitter": "Olivier Cappe", "authors": "Olivier Capp\\'e (LTCI), Eric Moulines (LTCI)", "title": "Online EM Algorithm for Latent Data Models", "comments": "Version that includes the corrigendum published in volume 73, part 5\n  (2011), of the Journal of the Royal Statistical Society, Series B + the\n  correction of a typo in Eqs. (32-33)", "journal-ref": "Journal of the Royal Statistical Society: Series B, Royal\n  Statistical Society, 2009, 71 (3), pp.593-613", "doi": "10.1111/j.1467-9868.2009.00698.x", "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we propose a generic online (also sometimes called\nadaptive or recursive) version of the Expectation-Maximisation (EM) algorithm\napplicable to latent variable models of independent observations. Compared to\nthe algorithm of Titterington (1984), this approach is more directly connected\nto the usual EM algorithm and does not rely on integration with respect to the\ncomplete data distribution. The resulting algorithm is usually simpler and is\nshown to achieve convergence to the stationary points of the Kullback-Leibler\ndivergence between the marginal distribution of the observation and the model\ndistribution at the optimal rate, i.e., that of the maximum likelihood\nestimator. In addition, the proposed approach is also suitable for conditional\n(or regression) models, as illustrated in the case of the mixture of linear\nregressions model.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2007 19:44:34 GMT"}, {"version": "v2", "created": "Thu, 4 Sep 2008 14:36:55 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2011 14:59:41 GMT"}, {"version": "v4", "created": "Wed, 1 Mar 2017 13:40:32 GMT"}], "update_date": "2017-03-02", "authors_parsed": [["Capp\u00e9", "Olivier", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"]]}]