[{"id": "1010.0124", "submitter": "Florian Frommlet", "authors": "Florian Frommlet, Felix Ruhaltinger, Piotr Twarog and Malgorzata\n  Bogdan", "title": "A model selection approach to genome wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the vast majority of genome wide association studies (GWAS) published so\nfar, statistical analysis was performed by testing markers individually. In\nthis article we present some elementary statistical considerations which\nclearly show that in case of complex traits the approach based on multiple\nregression or generalized linear models is preferable to multiple testing. We\nintroduce a model selection approach to GWAS based on modifications of Bayesian\nInformation Criterion (BIC) and develop some simple search strategies to deal\nwith the huge number of potential models. Comprehensive simulations based on\nreal SNP data confirm that model selection has larger power than multiple\ntesting to detect causal SNPs in complex models. On the other hand multiple\ntesting has substantial problems with proper ranking of causal SNPs and tends\nto detect a certain number of false positive SNPs, which are not linked to any\nof the causal mutations. We show that this behavior is typical in GWAS for\ncomplex traits and can be explained by an aggregated influence of many small\nrandom sample correlations between genotypes of a SNP under investigation and\nother causal SNPs. We believe that our findings at least partially explain\nproblems with low power and nonreplicability of results in many real data GWAS.\nFinally, we discuss the advantages of our model selection approach in the\ncontext of real data analysis, where we consider publicly available gene\nexpression data as traits for individuals from the HapMap project.\n", "versions": [{"version": "v1", "created": "Fri, 1 Oct 2010 11:20:42 GMT"}], "update_date": "2010-10-04", "authors_parsed": [["Frommlet", "Florian", ""], ["Ruhaltinger", "Felix", ""], ["Twarog", "Piotr", ""], ["Bogdan", "Malgorzata", ""]]}, {"id": "1010.0300", "submitter": "Jean-Michel Marin", "authors": "Gilles Celeux, Mohammed El Anbari, Jean-Michel Marin and Christian P.\n  Robert", "title": "Regularization in regression: comparing Bayesian and frequentist methods\n  in a poorly informative situation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a collection of simulated an real benchmarks, we compare Bayesian and\nfrequentist regularization approaches under a low informative constraint when\nthe number of variables is almost equal to the number of observations on\nsimulated and real datasets. This comparison includes new global noninformative\napproaches for Bayesian variable selection built on Zellner's g-priors that are\nsimilar to Liang et al. (2008). The interest of those calibration-free\nproposals is discussed. The numerical experiments we present highlight the\nappeal of Bayesian regularization methods, when compared with non-Bayesian\nalternatives. They dominate frequentist methods in the sense that they provide\nsmaller prediction errors while selecting the most relevant variables in a\nparsimonious way.\n", "versions": [{"version": "v1", "created": "Sat, 2 Oct 2010 07:48:09 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2011 06:20:24 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2011 08:37:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Celeux", "Gilles", ""], ["Anbari", "Mohammed El", ""], ["Marin", "Jean-Michel", ""], ["Robert", "Christian P.", ""]]}, {"id": "1010.0703", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney and Lorenzo Orecchia", "title": "Implementing regularization implicitly via approximate eigenvector\n  computation", "comments": "11 pages; a few clarifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is a powerful technique for extracting useful information from\nnoisy data. Typically, it is implemented by adding some sort of norm constraint\nto an objective function and then exactly optimizing the modified objective\nfunction. This procedure often leads to optimization problems that are\ncomputationally more expensive than the original problem, a fact that is\nclearly problematic if one is interested in large-scale applications. On the\nother hand, a large body of empirical work has demonstrated that heuristics,\nand in some cases approximation algorithms, developed to speed up computations\nsometimes have the side-effect of performing regularization implicitly. Thus,\nwe consider the question: What is the regularized optimization objective that\nan approximation algorithm is exactly optimizing?\n  We address this question in the context of computing approximations to the\nsmallest nontrivial eigenvector of a graph Laplacian; and we consider three\nrandom-walk-based procedures: one based on the heat kernel of the graph, one\nbased on computing the the PageRank vector associated with the graph, and one\nbased on a truncated lazy random walk. In each case, we provide a precise\ncharacterization of the manner in which the approximation method can be viewed\nas implicitly computing the exact solution to a regularized problem.\nInterestingly, the regularization is not on the usual vector form of the\noptimization problem, but instead it is on a related semidefinite program.\n", "versions": [{"version": "v1", "created": "Mon, 4 Oct 2010 20:49:15 GMT"}, {"version": "v2", "created": "Wed, 27 Apr 2011 03:52:25 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Mahoney", "Michael W.", ""], ["Orecchia", "Lorenzo", ""]]}, {"id": "1010.0842", "submitter": "Merrilee Hurn Dr", "authors": "Gundula Behrens and Nial Friel and Merrilee Hurn", "title": "Tuning Tempered Transitions", "comments": "To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-010-9206-z", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of tempered transitions was proposed by Neal (1996) for tackling\nthe difficulties arising when using Markov chain Monte Carlo to sample from\nmultimodal distributions. In common with methods such as simulated tempering\nand Metropolis-coupled MCMC, the key idea is to utilise a series of\nsuccessively easier to sample distributions to improve movement around the\nstate space. Tempered transitions does this by incorporating moves through\nthese less modal distributions into the MCMC proposals. Unfortunately the\nimproved movement between modes comes at a high computational cost with a low\nacceptance rate of expensive proposals. We consider how the algorithm may be\ntuned to increase the acceptance rates for a given number of temperatures. We\nfind that the commonly assumed geometric spacing of temperatures is reasonable\nin many but not all applications.\n", "versions": [{"version": "v1", "created": "Tue, 5 Oct 2010 11:06:15 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Behrens", "Gundula", ""], ["Friel", "Nial", ""], ["Hurn", "Merrilee", ""]]}, {"id": "1010.1195", "submitter": "Etienne Bernard P", "authors": "Etienne P. Bernard, C\\'edric Chanal, Werner Krauth", "title": "Damage spreading and coupling in Markov chains", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": "10.1209/0295-5075/92/60004", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn cond-mat.soft stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we relate the coupling of Markov chains, at the basis of\nperfect sampling methods, with damage spreading, which captures the chaotic\nnature of stochastic dynamics. For two-dimensional spin glasses and hard\nspheres we point out that the obstacle to the application of perfect-sampling\nschemes is posed by damage spreading rather than by the survey problem of the\nentire configuration space. We find dynamical damage-spreading transitions\ndeeply inside the paramagnetic and liquid phases, and show that critical values\nof the transition temperatures and densities depend on the coupling scheme. We\ndiscuss our findings in the light of a classic proof that for arbitrary Monte\nCarlo algorithms damage spreading can be avoided through non-Markovian coupling\nschemes.\n", "versions": [{"version": "v1", "created": "Wed, 6 Oct 2010 16:36:30 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 17:27:45 GMT"}], "update_date": "2015-05-20", "authors_parsed": [["Bernard", "Etienne P.", ""], ["Chanal", "C\u00e9dric", ""], ["Krauth", "Werner", ""]]}, {"id": "1010.1595", "submitter": "Christian P. Robert", "authors": "Pierre Jacob (Universite Paris-Dauphine and CREST, France), Christian\n  P. Robert (Universite Paris-Dauphine, IuF, and CREST, France), Murray H.\n  Smith (NIWA, Wellington, New Zealand)", "title": "Using parallel computation to improve Independent Metropolis--Hastings\n  based estimation", "comments": "19 pages, 8 figures, to appear in Journal of Computational and\n  Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the implications of the fact that parallel\nraw-power can be exploited by a generic Metropolis--Hastings algorithm if the\nproposed values are independent. In particular, we present improvements to the\nindependent Metropolis--Hastings algorithm that significantly decrease the\nvariance of any estimator derived from the MCMC output, for a null computing\ncost since those improvements are based on a fixed number of target density\nevaluations. Furthermore, the techniques developed in this paper do not\njeopardize the Markovian convergence properties of the algorithm, since they\nare based on the Rao--Blackwell principles of Gelfand and Smith (1990), already\nexploited in Casella and Robert (1996), Atchade and Perron (2005) and Douc and\nRobert (2010). We illustrate those improvements both on a toy normal example\nand on a classical probit regression model, but stress the fact that they are\napplicable in any case where the independent Metropolis-Hastings is applicable.\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 05:43:27 GMT"}, {"version": "v2", "created": "Mon, 14 Mar 2011 11:37:42 GMT"}, {"version": "v3", "created": "Thu, 24 Mar 2011 10:01:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Jacob", "Pierre", "", "Universite Paris-Dauphine and CREST, France"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine, IuF, and CREST, France"], ["Smith", "Murray H.", "", "NIWA, Wellington, New Zealand"]]}, {"id": "1010.1609", "submitter": "Michael Mahoney", "authors": "Michael W. Mahoney", "title": "Algorithmic and Statistical Perspectives on Large-Scale Data Analysis", "comments": "33 pages. To appear in Uwe Naumann and Olaf Schenk, editors,\n  \"Combinatorial Scientific Computing,\" Chapman and Hall/CRC Press, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, ideas from statistics and scientific computing have begun to\ninteract in increasingly sophisticated and fruitful ways with ideas from\ncomputer science and the theory of algorithms to aid in the development of\nimproved worst-case algorithms that are useful for large-scale scientific and\nInternet data analysis problems. In this chapter, I will describe two recent\nexamples---one having to do with selecting good columns or features from a (DNA\nSingle Nucleotide Polymorphism) data matrix, and the other having to do with\nselecting good clusters or communities from a data graph (representing a social\nor information network)---that drew on ideas from both areas and that may serve\nas a model for exploiting complementary algorithmic and statistical\nperspectives in order to solve applied large-scale data analysis problems.\n", "versions": [{"version": "v1", "created": "Fri, 8 Oct 2010 07:02:11 GMT"}], "update_date": "2010-10-11", "authors_parsed": [["Mahoney", "Michael W.", ""]]}, {"id": "1010.2310", "submitter": "Cinzia Viroli", "authors": "Cinzia Viroli", "title": "Stochastic model selection for Mixtures of Matrix-Normals", "comments": "This paper has been withdrawn by the author. Some content of the work\n  paper has been extended", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of matrix normal distributions are a powerful tool for\nclassifying three-way data in unsupervised problems. The distribution of each\ncomponent is assumed to be a matrix variate normal density. The mixture model\ncan be estimated through the EM algorithm under the assumption that the number\nof components is known and fixed. In this work we introduce, develop and\nexplore a Bayesian analysis of the model in order to provide a tool for\nsimultaneous model estimation and model selection. The effectiveness of the\nproposed method is illustrated on a simulation study and on a real example.\n", "versions": [{"version": "v1", "created": "Tue, 12 Oct 2010 07:32:42 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 16:07:57 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2013 08:43:10 GMT"}], "update_date": "2013-03-07", "authors_parsed": [["Viroli", "Cinzia", ""]]}, {"id": "1010.2846", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori and Atsumi Ohara", "title": "A Bregman Extension of quasi-Newton updates II: Convergence and\n  Robustness Properties", "comments": "39 pages, 1 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of quasi-Newton methods, and investigate the\nconvergence and the robustness properties of the proposed update formulae for\nthe approximate Hessian matrix. Fletcher has studied a variational problem\nwhich derives the approximate Hessian update formula of the quasi-Newton\nmethods. We point out that the variational problem is identical to optimization\nof the Kullback-Leibler divergence, which is a discrepancy measure between two\nprobability distributions. Then, we introduce the Bregman divergence as an\nextension of the Kullback-Leibler divergence, and derive extended quasi-Newton\nupdate formulae based on the variational problem with the Bregman divergence.\nThe proposed update formulae belong to a class of self-scaling quasi-Newton\nmethods. We study the convergence property of the proposed quasi-Newton method,\nand moreover, we apply the tools in the robust statistics to analyze the\nrobustness property of the Hessian update formulae against the numerical\nrounding errors included in the line search for the step length. As the result,\nwe found that the influence of the inexact line search is bounded only for the\nstandard BFGS formula for the Hessian approximation. Numerical studies are\nconducted to verify the usefulness of the tools borrowed from robust\nstatistics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 08:09:48 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Ohara", "Atsumi", ""]]}, {"id": "1010.2847", "submitter": "Takafumi Kanamori Dr.", "authors": "Takafumi Kanamori and Atsumi Ohara", "title": "A Bregman Extension of quasi-Newton updates I: An Information\n  Geometrical framework", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study quasi-Newton methods from the viewpoint of information geometry\ninduced associated with Bregman divergences. Fletcher has studied a variational\nproblem which derives the approximate Hessian update formula of the\nquasi-Newton methods. We point out that the variational problem is identical to\noptimization of the Kullback-Leibler divergence, which is a discrepancy measure\nbetween two probability distributions. The Kullback-Leibler divergence for the\nmultinomial normal distribution corresponds to the objective function Fletcher\nhas considered. We introduce the Bregman divergence as an extension of the\nKullback-Leibler divergence, and derive extended quasi-Newton update formulae\nbased on the variational problem with the Bregman divergence. As well as the\nKullback-Leibler divergence, the Bregman divergence introduces the information\ngeometrical structure on the set of positive definite matrices. From the\ngeometrical viewpoint, we study the approximation Hessian update, the\ninvariance property of the update formulae, and the sparse quasi-Newton\nmethods. Especially, we point out that the sparse quasi-Newton method is\nclosely related to statistical methods such as the EM-algorithm and the\nboosting algorithm. Information geometry is useful tool not only to better\nunderstand the quasi-Newton methods but also to design new update formulae.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 08:11:32 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Kanamori", "Takafumi", ""], ["Ohara", "Atsumi", ""]]}, {"id": "1010.3043", "submitter": "Tamara Kolda", "authors": "Eric C. Chi and Tamara G. Kolda", "title": "Making Tensor Factorizations Robust to Non-Gaussian Noise", "comments": "Contributed presentation at the NIPS Workshop on Tensors, Kernels,\n  and Machine Learning, Whistler, BC, Canada, December 10, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensors are multi-way arrays, and the Candecomp/Parafac (CP) tensor\nfactorization has found application in many different domains. The CP model is\ntypically fit using a least squares objective function, which is a maximum\nlikelihood estimate under the assumption of i.i.d. Gaussian noise. We\ndemonstrate that this loss function can actually be highly sensitive to\nnon-Gaussian noise. Therefore, we propose a loss function based on the 1-norm\nbecause it can accommodate both Gaussian and grossly non-Gaussian\nperturbations. We also present an alternating majorization-minimization\nalgorithm for fitting a CP model using our proposed loss function.\n", "versions": [{"version": "v1", "created": "Thu, 14 Oct 2010 23:21:30 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Chi", "Eric C.", ""], ["Kolda", "Tamara G.", ""]]}, {"id": "1010.4406", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Aaron D. Byrnes and Pavel V. Shevchenko", "title": "Impact of Insurance for Operational Risk: Is it worthwhile to insure or\n  be insured for severe losses?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Basel II standards, the Operational Risk (OpRisk) advanced\nmeasurement approach allows a provision for reduction of capital as a result of\ninsurance mitigation of up to 20%. This paper studies the behaviour of\ndifferent insurance policies in the context of capital reduction for a range of\npossible extreme loss models and insurance policy scenarios in a multi-period,\nmultiple risk settings. A Loss Distributional Approach (LDA) for modelling of\nthe annual loss process, involving homogeneous compound Poisson processes for\nthe annual losses, with heavy tailed severity models comprised of alpha-stable\nseverities is considered. There has been little analysis of such models to date\nand it is believed, insurance models will play more of a role in OpRisk\nmitigation and capital reduction in future. The first question of interest is\nwhen would it be equitable for a bank or financial institution to purchase\ninsurance for heavy tailed OpRisk losses under different insurance policy\nscenarios? The second question then pertains to Solvency II and addresses what\nthe insurers capital would be for such operational risk scenarios under\ndifferent policy offerings. In addition we consider the insurers perspective\nwith respect to fair premium as a percentage above the expected annual claim\nfor each insurance policy. The intention being to address questions related to\nVaR reduction under Basel II, SCR under Solvency II and fair insurance premiums\nin OpRisk for different extreme loss scenarios. In the process we provide\nclosed form solutions for the distribution of loss process and claims process\nin an LDA structure as well as closed form analytic solutions for the Expected\nShortfall, SCR and MCR under Basel II and Solvency II. We also provide closed\nform analytic solutions for the annual loss distribution of multiple risks\nincluding insurance mitigation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Oct 2010 09:42:57 GMT"}, {"version": "v2", "created": "Wed, 3 Nov 2010 00:19:24 GMT"}], "update_date": "2010-11-04", "authors_parsed": [["Peters", "Gareth W.", ""], ["Byrnes", "Aaron D.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1010.5265", "submitter": "James Scott", "authors": "James G. Scott", "title": "Parameter expansion in local-shrinkage models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of using MCMC to fit sparse Bayesian models\nbased on normal scale-mixture priors. Examples of this framework include the\nBayesian LASSO and the horseshoe prior. We study the usefulness of parameter\nexpansion (PX) for improving convergence in such models, which is notoriously\nslow when the global variance component is near zero. Our conclusion is that\nparameter expansion does improve matters in LASSO-type models, but only\nmodestly. In most cases this improvement, while noticeable, is less than what\nmight be expected, especially compared to the improvements that PX makes\npossible for models very similar to those considered here. We give some\nexamples, and we attempt to provide some intuition as to why this is so. We\nalso describe how slice sampling may be used to update the global variance\ncomponent. In practice, this approach seems to perform almost as well as\nparameter expansion. As a practical matter, however, it is perhaps best viewed\nnot as a replacement for PX, but as a tool for expanding the class of models to\nwhich PX is applicable.\n", "versions": [{"version": "v1", "created": "Mon, 25 Oct 2010 21:09:59 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Scott", "James G.", ""]]}, {"id": "1010.5503", "submitter": "Jiangang Hao", "authors": "Jiangang Hao, Timothy A. McKay, Benjamin P. Koester, Eli S. Rykoff,\n  Eduardo Rozo, James Annis, Risa H. Wechsler, August Evrard, Seth R. Siegel,\n  Matthew Becker, Michael Busha, David Gerdes, David E. Johnston and Erin\n  Sheldon", "title": "A GMBCG Galaxy Cluster Catalog of 55,424 Rich Clusters from SDSS DR7", "comments": "Updated to match the published version. The catalog can be accessed\n  from: http://home.fnal.gov/~jghao/gmbcg_sdss_catalog.html", "journal-ref": "Astrophys.J.Suppl.191:254-274,2010", "doi": "10.1088/0067-0049/191/2/254", "report-no": "FERMILAB-PUB-10-287-A", "categories": "astro-ph.CO stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We present a large catalog of optically selected galaxy clusters from the\napplication of a new Gaussian Mixture Brightest Cluster Galaxy (GMBCG)\nalgorithm to SDSS Data Release 7 data. The algorithm detects clusters by\nidentifying the red sequence plus Brightest Cluster Galaxy (BCG) feature, which\nis unique for galaxy clusters and does not exist among field galaxies. Red\nsequence clustering in color space is detected using an Error Corrected\nGaussian Mixture Model. We run GMBCG on 8240 square degrees of photometric data\nfrom SDSS DR7 to assemble the largest ever optical galaxy cluster catalog,\nconsisting of over 55,000 rich clusters across the redshift range from 0.1 < z\n< 0.55. We present Monte Carlo tests of completeness and purity and perform\ncross-matching with X-ray clusters and with the maxBCG sample at low redshift.\nThese tests indicate high completeness and purity across the full redshift\nrange for clusters with 15 or more members.\n", "versions": [{"version": "v1", "created": "Tue, 26 Oct 2010 20:01:37 GMT"}, {"version": "v2", "created": "Tue, 21 Dec 2010 19:59:29 GMT"}, {"version": "v3", "created": "Wed, 22 Dec 2010 03:38:34 GMT"}], "update_date": "2010-12-23", "authors_parsed": [["Hao", "Jiangang", ""], ["McKay", "Timothy A.", ""], ["Koester", "Benjamin P.", ""], ["Rykoff", "Eli S.", ""], ["Rozo", "Eduardo", ""], ["Annis", "James", ""], ["Wechsler", "Risa H.", ""], ["Evrard", "August", ""], ["Siegel", "Seth R.", ""], ["Becker", "Matthew", ""], ["Busha", "Michael", ""], ["Gerdes", "David", ""], ["Johnston", "David E.", ""], ["Sheldon", "Erin", ""]]}, {"id": "1010.6098", "submitter": "Juan Jose Fernandez-Duran", "authors": "Fern\\'andez-Dur\\'an, J.J. and Gregorio-Dom\\'inguez, M.M", "title": "Maximum Likelihood Estimation of Nonnegative Trigonometric Sum Models\n  Using a Newton-like Algorithm on Manifolds", "comments": "9 pages 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Fern\\'andez-Dur\\'an (2004), a new family of circular distributions based\non nonnegative trigonometric sums (NNTS models) is developed. Because the\nparameter space of this family is the surface of the hypersphere, an efficient\nNewton-like algorithm on manifolds is generated in order to obtain the maximum\nlikelihood estimates of the parameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Oct 2010 21:25:31 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Fern\u00e1ndez-Dur\u00e1n", "", ""], ["J.", "J.", ""], ["Gregorio-Dom\u00ednguez", "", ""], ["M", "M.", ""]]}, {"id": "1010.6113", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine and CREST) and Kerrie\n  L. Mengersen (Queensland University of Technology, Brisbane)", "title": "Exact Bayesian Analysis of Mixtures", "comments": "2 figures, 3 tables, 2 R codes. Chapter to appear in Mixtures:\n  Estimation and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how a complete and exact Bayesian analysis of a\nparametric mixture model is possible in some cases when components of the\nmixture are taken from exponential families and when conjugate priors are used.\nThis restricted set-up allows us to show the relevance of the Bayesian approach\nas well as to exhibit the limitations of a complete analysis, namely that it is\nimpossible to conduct this analysis when the sample size is too large, when the\ndata are not from an exponential family, or when priors that are more complex\nthan conjugate priors are used.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 00:18:55 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine and CREST"], ["Mengersen", "Kerrie L.", "", "Queensland University of Technology, Brisbane"]]}, {"id": "1010.6118", "submitter": "Nevena Maric", "authors": "Vanja Dukic and Nevena Maric", "title": "On minimum correlation in construction of multivariate distributions", "comments": "Major changes made compared to previous versions. Minimum\n  correlations and Multivariate methods added. 10 pages, 5 figures", "journal-ref": "Phys. Rev. E , Volume 87, Issue 3: 032114 (2013)", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a method for exact generation of multivariate\nsamples with pre-specified marginal distributions and a given correlation\nmatrix, based on a mixture of Fr\\'echet-Hoeffding bounds and marginal products.\nThe bivariate algorithm can accommodate any among the theoretically possible\ncorrelation coefficients, and explicitly provides a connection between\nsimulation and the minimum correlation attainable for different distribution\nfamilies. We calculate the minimum correlations in several common\ndistributional examples, including in some that have not been looked at before.\nAs an illustration, we provide the details and results of implementing the\nalgorithm for generating three-dimensional negatively and positively correlated\nBeta random variables, making it the only non-copula algorithm for correlated\nBeta simulation in dimensions greater than two. This work has potential for\nimpact in a variety of fields where simulation of multivariate stochastic\ncomponents is desired.\n", "versions": [{"version": "v1", "created": "Fri, 29 Oct 2010 02:41:43 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 02:17:53 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 22:30:29 GMT"}], "update_date": "2013-03-18", "authors_parsed": [["Dukic", "Vanja", ""], ["Maric", "Nevena", ""]]}]