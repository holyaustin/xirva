[{"id": "0803.0054", "submitter": "Julien Cornebise", "authors": "Julien Cornebise (LTCI), Eric Moulines (LTCI), Jimmy Olsson", "title": "Adaptive methods for sequential importance sampling with application to\n  state space models", "comments": "Preprint of the article to be published in Statistics and Comptuing.\n  36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss new adaptive proposal strategies for sequential\nMonte Carlo algorithms--also known as particle filters--relying on criteria\nevaluating the quality of the proposed particles. The choice of the proposal\ndistribution is a major concern and can dramatically influence the quality of\nthe estimates. Thus, we show how the long-used coefficient of variation of the\nweights can be used for estimating the chi-square distance between the target\nand instrumental distributions of the auxiliary particle filter. As a\nby-product of this analysis we obtain an auxiliary adjustment multiplier weight\ntype for which this chi-square distance is minimal. Moreover, we establish an\nempirical estimate of linear complexity of the Kullback-Leibler divergence\nbetween the involved distributions. Guided by these results, we discuss\nadaptive designing of the particle filter proposal distribution and illustrate\nthe methods on a numerical example.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2008 08:33:24 GMT"}, {"version": "v2", "created": "Sat, 23 Aug 2008 06:46:35 GMT"}], "update_date": "2008-08-25", "authors_parsed": [["Cornebise", "Julien", "", "LTCI"], ["Moulines", "Eric", "", "LTCI"], ["Olsson", "Jimmy", ""]]}, {"id": "0803.0525", "submitter": "Sophie Lebre", "authors": "Sophie L\\`ebre (SG), Pierre-Yves Bourguinon (SG)", "title": "An EM algorithm for estimation in the Mixture Transition Distribution\n  model", "comments": "22 pages", "journal-ref": "Journal of Statistical Computation and Simulation (2008) ?", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mixture Transition Distribution (MTD) model was introduced by Raftery to\nface the need for parsimony in the modeling of high-order Markov chains in\ndiscrete time. The particularity of this model comes from the fact that the\neffect of each lag upon the present is considered separately and additively, so\nthat the number of parameters required is drastically reduced. However, the\nefficiency for the MTD parameter estimations proposed up to date still remains\nproblematic on account of the large number of constraints on the parameters. In\nthis paper, an iterative procedure, commonly known as Expectation-Maximization\n(EM) algorithm, is developed cooperating with the principle of Maximum\nLikelihood Estimation (MLE) to estimate the MTD parameters. Some applications\nof modeling MTD show the proposed EM algorithm is easier to be used than the\nalgorithm developed by Berchtold. Moreover, the EM Estimations of parameters\nfor high-order MTD models led on DNA sequences outperform the corresponding\nfully parametrized Markov chain in terms of Bayesian Information Criterion. A\nsoftware implementation of our algorithm is available in the library seq++ at\nhttp://stat.genopole.cnrs.fr/seqpp\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2008 19:31:23 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["L\u00e8bre", "Sophie", "", "SG"], ["Bourguinon", "Pierre-Yves", "", "SG"]]}, {"id": "0803.2173", "submitter": "Russell Zaretzki", "authors": "Artin Armagan, Russell Zaretzki", "title": "Adaptive Ridge Selector (ARiS)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new shrinkage variable selection operator for linear models\nwhich we term the \\emph{adaptive ridge selector} (ARiS). This approach is\ninspired by the \\emph{relevance vector machine} (RVM), which uses a Bayesian\nhierarchical linear setup to do variable selection and model estimation.\nExtending the RVM algorithm, we include a proper prior distribution for the\nprecisions of the regression coefficients, $v_{j}^{-1} \\sim\nf(v_{j}^{-1}|\\eta)$, where $\\eta$ is a scalar hyperparameter. A novel fitting\napproach which utilizes the full set of posterior conditional distributions is\napplied to maximize the joint posterior distribution\n$p(\\boldsymbol\\beta,\\sigma^{2},\\mathbf{v}^{-1}|\\mathbf{y},\\eta)$ given the\nvalue of the hyper-parameter $\\eta$. An empirical Bayes method is proposed for\nchoosing $\\eta$. This approach is contrasted with other regularized least\nsquares estimators including the lasso, its variants, nonnegative garrote and\nordinary ridge regression. Performance differences are explored for various\nsimulated data examples. Results indicate superior prediction and model\nselection accuracy under sparse setups and drastic improvement in accuracy of\nmodel choice with increasing sample size.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2008 19:11:10 GMT"}, {"version": "v2", "created": "Wed, 28 May 2008 15:20:18 GMT"}], "update_date": "2008-05-28", "authors_parsed": [["Armagan", "Artin", ""], ["Zaretzki", "Russell", ""]]}, {"id": "0803.2394", "submitter": "Alexey Koloydenko", "authors": "J\\\"uri Lember, Alexey Koloydenko", "title": "The adjusted Viterbi training for hidden Markov models", "comments": "Published in at http://dx.doi.org/10.3150/07-BEJ105 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2008, Vol. 14, No. 1, 180-206", "doi": "10.3150/07-BEJ105", "report-no": "IMS-BEJ-BEJ105", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM procedure is a principal tool for parameter estimation in the hidden\nMarkov models. However, applications replace EM by Viterbi extraction, or\ntraining (VT). VT is computationally less intensive, more stable and has more\nof an intuitive appeal, but VT estimation is biased and does not satisfy the\nfollowing fixed point property. Hypothetically, given an infinitely large\nsample and initialized to the true parameters, VT will generally move away from\nthe initial values. We propose adjusted Viterbi training (VA), a new method to\nrestore the fixed point property and thus alleviate the overall imprecision of\nthe VT estimators, while preserving the computational advantages of the\nbaseline VT algorithm. Simulations elsewhere have shown that VA appreciably\nimproves the precision of estimation in both the special case of mixture models\nand more general HMMs. However, being entirely analytic, the VA correction\nrelies on infinite Viterbi alignments and associated limiting probability\ndistributions. While explicit in the mixture case, the existence of these\nlimiting measures is not obvious for more general HMMs. This paper proves that\nunder certain mild conditions, the required limiting distributions for general\nHMMs do exist.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2008 06:44:24 GMT"}], "update_date": "2008-12-18", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Koloydenko", "Alexey", ""]]}, {"id": "0803.2931", "submitter": "Lutz D\\\"umbgen", "authors": "Lutz Duembgen, Arne Kovac", "title": "Extensions of smoothing via taut strings", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS216 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2009, Vol. 3, 41-75", "doi": "10.1214/08-EJS216", "report-no": "IMS-EJS-EJS_2008_216", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe independent random pairs $(X_1,Y_1)$, $(X_2,Y_2)$,\n>..., $(X_n,Y_n)$. Our goal is to estimate regression functions such as the\nconditional mean or $\\beta$--quantile of $Y$ given $X$, where $0<\\beta <1$. In\norder to achieve this we minimize criteria such as, for instance, $$\n\\sum_{i=1}^n \\rho(f(X_i) - Y_i) + \\lambda \\cdot \\mathop TV\\nolimits (f) $$\namong all candidate functions $f$. Here $\\rho$ is some convex function\ndepending on the particular regression function we have in mind, $\\mathop {\\rm\nTV}\\nolimits (f)$ stands for the total variation of $f$, and $\\lambda >0$ is\nsome tuning parameter. This framework is extended further to include binary or\nPoisson regression, and to include localized total variation penalties. The\nlatter are needed to construct estimators adapting to inhomogeneous smoothness\nof $f$. For the general framework we develop noniterative algorithms for the\nsolution of the minimization problems which are closely related to the taut\nstring algorithm (cf. Davies and Kovac, 2001). Further we establish a\nconnection between the present setting and monotone regression, extending\nprevious work by Mammen and van de Geer (1997). The algorithmic considerations\nand numerical examples are complemented by two consistency results.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2008 16:02:23 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2009 10:57:55 GMT"}], "update_date": "2009-01-29", "authors_parsed": [["Duembgen", "Lutz", ""], ["Kovac", "Arne", ""]]}]