[{"id": "1902.00075", "submitter": "Rakshith Sharma Srinivasa", "authors": "Rakshith Sharma Srinivasa, Mark A. Davenport, Justin Romberg", "title": "Trading beams for bandwidth: Imaging with randomized beamforming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of actively imaging a range-limited far-field scene\nusing an antenna array. We describe how the range limit imposes structure in\nthe measurements across multiple wavelengths. This structure allows us to\nintroduce a novel trade-off: the number of spatial array measurements (i.e.,\nbeams that have to be formed) can be reduced significantly lower than the\nnumber array elements if the scene is illuminated with a broadband source. To\ntake advantage of this trade-off, we use a small number of \"generic\" linear\ncombinations of the array outputs, instead of the phase offsets used in\nconventional beamforming. We provide theoretical justification for the proposed\ntrade-off without making any strong structural assumptions on the target scene\n(such as sparsity) except that it is range limited. In proving our theoretical\nresults, we take inspiration from the sketching literature. We also provide\nsimulation results to establish the merit of the proposed signal acquisition\nstrategy. Our proposed method results in a reduction in the number of required\nspatial measurements in an array imaging system and hence can directly impact\ntheir speed and cost of operation.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 21:17:20 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Srinivasa", "Rakshith Sharma", ""], ["Davenport", "Mark A.", ""], ["Romberg", "Justin", ""]]}, {"id": "1902.00412", "submitter": "Matti Vihola", "authors": "Matti Vihola and Jordan Franks", "title": "On the use of approximate Bayesian computation Markov chain Monte Carlo\n  with inflated tolerance and post-correction", "comments": "27 pages, 8 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation allows for inference of complicated\nprobabilistic models with intractable likelihoods using model simulations. The\nMarkov chain Monte Carlo implementation of approximate Bayesian computation is\noften sensitive to the tolerance parameter: low tolerance leads to poor mixing\nand large tolerance entails excess bias. We consider an approach using a\nrelatively large tolerance for the Markov chain Monte Carlo sampler to ensure\nits sufficient mixing, and post-processing the output leading to estimators for\na range of finer tolerances. We introduce an approximate confidence interval\nfor the related post-corrected estimators, and propose an adaptive approximate\nBayesian computation Markov chain Monte Carlo, which finds a `balanced'\ntolerance level automatically, based on acceptance rate optimisation. Our\nexperiments show that post-processing based estimators can perform better than\ndirect Markov chain targetting a fine tolerance, that our confidence intervals\nare reliable, and that our adaptive algorithm leads to reliable inference with\nlittle user specification.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 15:47:19 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 07:36:23 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Vihola", "Matti", ""], ["Franks", "Jordan", ""]]}, {"id": "1902.00608", "submitter": "Mark Tygert", "authors": "Aaron Defazio and Mark Tygert", "title": "Methods of interpreting error estimates for grayscale image\n  reconstructions", "comments": "23 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One representation of possible errors in a grayscale image reconstruction is\nas another grayscale image estimating potentially worrisome differences between\nthe reconstruction and the actual \"ground-truth\" reality. Visualizations and\nsummary statistics can aid in the interpretation of such a representation of\nerror estimates. Visualizations include suitable colorizations of the\nreconstruction, as well as the obvious \"correction\" of the reconstruction by\nsubtracting off the error estimates. The canonical summary statistic would be\nthe root-mean-square of the error estimates. Numerical examples involving\ncranial magnetic-resonance imaging clarify the relative merits of the various\nmethods in the context of compressed sensing. Unfortunately, the colorizations\nappear likely to be too distracting for actual clinical practice, and the\nroot-mean-square gets swamped by background noise in the error estimates.\nFortunately, straightforward displays of the error estimates and of the\n\"corrected\" reconstruction are illuminating, and the root-mean-square improves\ngreatly after mild blurring of the error estimates; the blurring is barely\nperceptible to the human eye yet smooths away background noise that would\notherwise overwhelm the root-mean-square.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 01:14:58 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Defazio", "Aaron", ""], ["Tygert", "Mark", ""]]}, {"id": "1902.01011", "submitter": "Pritam Ranjan", "authors": "Feng Yang, C. Devon Lin, Pritam Ranjan", "title": "Global Fitting of the Response Surface via Estimating Multiple Contours\n  of a Simulator", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulators are nowadays widely used to understand complex physical\nsystems in many areas such as aerospace, renewable energy, climate modeling,\nand manufacturing. One fundamental issue in the study of computer simulators is\nknown as experimental design, that is, how to select the input settings where\nthe computer simulator is run and the corresponding response is collected.\nExtra care should be taken in the selection process because computer simulators\ncan be computationally expensive to run. The selection shall acknowledge and\nachieve the goal of the analysis. This article focuses on the goal of producing\nmore accurate prediction which is important for risk assessment and decision\nmaking. We propose two new methods of design approaches that sequentially\nselect input settings to achieve this goal. The approaches make novel\napplications of simultaneous and sequential contour estimations. Numerical\nexamples are employed to demonstrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 02:29:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yang", "Feng", ""], ["Lin", "C. Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1902.01542", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh and Rahul Mazumder", "title": "Learning Hierarchical Interactions at Scale: A Convex Optimization\n  Approach", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many learning settings, it is beneficial to augment the main features with\npairwise interactions. Such interaction models can be often enhanced by\nperforming variable selection under the so-called strong hierarchy constraint:\nan interaction is non-zero only if its associated main features are non-zero.\nExisting convex optimization based algorithms face difficulties in handling\nproblems where the number of main features $p \\sim 10^3$ (with total number of\nfeatures $\\sim p^2$). In this paper, we study a convex relaxation which\nenforces strong hierarchy and develop a highly scalable algorithm based on\nproximal gradient descent. We introduce novel screening rules that allow for\nsolving the complicated proximal problem in parallel. In addition, we introduce\na specialized active-set strategy with gradient screening for avoiding costly\ngradient computations. The framework can handle problems having dense design\nmatrices, with $p = 50,000$ ($\\sim 10^9$ interactions)---instances that are\nmuch larger than current state of the art. Experiments on real and synthetic\ndata suggest that our toolkit hierScale outperforms the state of the art in\nterms of prediction and variable selection and can achieve over a 4900x\nspeed-up.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 04:47:54 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 23:48:45 GMT"}, {"version": "v3", "created": "Sun, 26 May 2019 03:29:03 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 00:54:39 GMT"}, {"version": "v5", "created": "Tue, 14 Jul 2020 01:13:02 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1902.01781", "submitter": "Lawrence Middleton", "authors": "Lawrence Middleton, George Deligiannidis, Arnaud Doucet, Pierre E.\n  Jacob", "title": "Unbiased Smoothing using Particle Independent Metropolis-Hastings", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the approximation of expectations with respect to the\ndistribution of a latent Markov process given noisy measurements. This is known\nas the smoothing problem and is often approached with particle and Markov chain\nMonte Carlo (MCMC) methods. These methods provide consistent but biased\nestimators when run for a finite time. We propose a simple way of coupling two\nMCMC chains built using Particle Independent Metropolis-Hastings (PIMH) to\nproduce unbiased smoothing estimators. Unbiased estimators are appealing in the\ncontext of parallel computing, and facilitate the construction of confidence\nintervals. The proposed scheme only requires access to off-the-shelf Particle\nFilters (PF) and is thus easier to implement than recently proposed unbiased\nsmoothers. The approach is demonstrated on a L\\'evy-driven stochastic\nvolatility model and a stochastic kinetic model.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 16:41:00 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Middleton", "Lawrence", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1902.01981", "submitter": "Saurav Prakash", "authors": "Amirhossein Reisizadeh, Saurav Prakash, Ramtin Pedarsani, Amir Salman\n  Avestimehr", "title": "CodedReduce: A Fast and Robust Framework for Gradient Aggregation in\n  Distributed Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the commonly used synchronous Gradient Descent paradigm for\nlarge-scale distributed learning, for which there has been a growing interest\nto develop efficient and robust gradient aggregation strategies that overcome\ntwo key system bottlenecks: communication bandwidth and stragglers' delays. In\nparticular, Ring-AllReduce (RAR) design has been proposed to avoid bandwidth\nbottleneck at any particular node by allowing each worker to only communicate\nwith its neighbors that are arranged in a logical ring. On the other hand,\nGradient Coding (GC) has been recently proposed to mitigate stragglers in a\nmaster-worker topology by allowing carefully designed redundant allocation of\nthe data set to the workers. We propose a joint communication topology design\nand data set allocation strategy, named CodedReduce (CR), that combines the\nbest of both RAR and GC. That is, it parallelizes the communications over a\ntree topology leading to efficient bandwidth utilization, and carefully designs\na redundant data set allocation and coding strategy at the nodes to make the\nproposed gradient aggregation scheme robust to stragglers. In particular, we\nquantify the communication parallelization gain and resiliency of the proposed\nCR scheme, and prove its optimality when the communication topology is a\nregular tree. Furthermore, we empirically evaluate the performance of our\nproposed CR design over Amazon EC2 and demonstrate that it achieves speedups of\nup to 27.2x and 7.0x, respectively over the benchmarks GC and RAR.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 00:05:21 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 06:06:19 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Reisizadeh", "Amirhossein", ""], ["Prakash", "Saurav", ""], ["Pedarsani", "Ramtin", ""], ["Avestimehr", "Amir Salman", ""]]}, {"id": "1902.02623", "submitter": "Mark van de Wiel", "authors": "Jurre R. Veerman, Gwenael G.R. Leday, Mark A. van de Wiel", "title": "Estimation of variance components, heritability and the ridge penalty in\n  high-dimensional generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For high-dimensional linear regression models, we review and compare several\nestimators of variances $\\tau^2$ and $\\sigma^2$ of the random slopes and\nerrors, respectively. These variances relate directly to ridge regression\npenalty $\\lambda$ and heritability index $h^2$, often used in genetics. Direct\nand indirect estimators of these, either based on cross-validation (CV) or\nmaximum marginal likelihood (MML), are also discussed. The comparisons include\nseveral cases of covariate matrix $\\mathbf{X}_{n \\times p}$, with $p \\gg n$,\nsuch as multi-collinear covariates and data-derived ones. In addition, we study\nrobustness against departures from the model such as sparse instead of dense\neffects and non-Gaussian errors.\n  An example on weight gain data with genomic covariates confirms the good\nperformance of MML compared to CV. Several extensions are presented. First, to\nthe high-dimensional linear mixed effects model, with REML as an alternative to\nMML. Second, to the conjugate Bayesian setting, which proves to be a good\nalternative. Third, and most prominently, to generalized linear models for\nwhich we derive a computationally efficient MML estimator by re-writing the\nmarginal likelihood as an $n$-dimensional integral. For Poisson and Binomial\nridge regression, we demonstrate the superior accuracy of the resulting MML\nestimator of $\\lambda$ as compared to CV. Software is provided to enable\nreproduction of all results presented here.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 14:15:58 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Veerman", "Jurre R.", ""], ["Leday", "Gwenael G. R.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1902.03316", "submitter": "Youngseok Kim", "authors": "Youngseok Kim, Chao Gao", "title": "Bayesian Model Selection with Graph Structured Sparsity", "comments": null, "journal-ref": "Journal of Machine Learning Research 21(109):1-61, 2020", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general algorithmic framework for Bayesian model selection. A\nspike-and-slab Laplacian prior is introduced to model the underlying structural\nassumption. Using the notion of effective resistance, we derive an EM-type\nalgorithm with closed-form iterations to efficiently explore possible\ncandidates for Bayesian model selection. The deterministic nature of the\nproposed algorithm makes it more scalable to large-scale and high-dimensional\ndata sets compared with existing stochastic search algorithms. When applied to\nsparse linear regression, our framework recovers the EMVS algorithm [Rockova\nand George, 2014] as a special case. We also discuss extensions of our\nframework using tools from graph algebra to incorporate complex Bayesian models\nsuch as biclustering and submatrix localization. Extensive simulation studies\nand real data applications are conducted to demonstrate the superior\nperformance of our methods over its frequentist competitors such as $\\ell_0$ or\n$\\ell_1$ penalization.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 22:51:03 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 03:17:07 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Kim", "Youngseok", ""], ["Gao", "Chao", ""]]}, {"id": "1902.03335", "submitter": "Hien Nguyen", "authors": "H D Nguyen and F Forbes and G J McLachlan", "title": "Mini-batch learning of exponential family finite mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mini-batch algorithms have become increasingly popular due to the requirement\nfor solving optimization problems, based on large-scale data sets. Using an\nexisting online expectation-{}-maximization (EM) algorithm framework, we\ndemonstrate how mini-batch (MB) algorithms may be constructed, and propose a\nscheme for the stochastic stabilization of the constructed mini-batch\nalgorithms. Theoretical results regarding the convergence of the mini-batch EM\nalgorithms are presented. We then demonstrate how the mini-batch framework may\nbe applied to conduct maximum likelihood (ML) estimation of mixtures of\nexponential family distributions, with emphasis on ML estimation for mixtures\nof normal distributions. Via a simulation study, we demonstrate that the\nmini-batch algorithm for mixtures of normal distributions can outperform the\nstandard EM algorithm. Further evidence of the performance of the mini-batch\nframework is provided via an application to the famous MNIST data set.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 00:16:00 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 21:17:46 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Nguyen", "H D", ""], ["Forbes", "F", ""], ["McLachlan", "G J", ""]]}, {"id": "1902.03586", "submitter": "Alastair Gregory", "authors": "Alastair Gregory and Kaushik Jana", "title": "Space-efficient estimation of empirical tail dependence coefficients for\n  bivariate data streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a space-efficient approximation to empirical tail\ndependence coefficients of an indefinite bivariate stream of data. The\napproximation, which has stream-length invariant error bounds, utilises recent\nwork on the development of a summary for bivariate empirical copula functions.\nThe work in this paper accurately approximates a bivariate empirical copula in\nthe tails of each marginal distribution, therefore modelling the tail\ndependence between the two variables observed in the data stream. Copulas\nevaluated at these marginal tails can be used to estimate the tail dependence\ncoefficients. Modifications to the space-efficient bivariate copula\napproximation, presented in this paper, allow the error of approximations to\nthe tail dependence coefficients to remain stream-length invariant. Theoretical\nand numerical evidence of this, including a case-study using the Los Alamos\nNational Laboratory netflow data-set, is provided within this article.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 12:43:13 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 20:32:38 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 19:14:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Gregory", "Alastair", ""], ["Jana", "Kaushik", ""]]}, {"id": "1902.03999", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "KTBoost: Combined Kernel and Tree Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel boosting algorithm called `KTBoost' which combines\nkernel boosting and tree boosting. In each boosting iteration, the algorithm\nadds either a regression tree or reproducing kernel Hilbert space (RKHS)\nregression function to the ensemble of base learners. Intuitively, the idea is\nthat discontinuous trees and continuous RKHS regression functions complement\neach other, and that this combination allows for better learning of functions\nthat have parts with varying degrees of regularity such as discontinuities and\nsmooth parts. We empirically show that KTBoost significantly outperforms both\ntree and kernel boosting in terms of predictive accuracy in a comparison on a\nwide array of data sets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:02:10 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 07:02:40 GMT"}, {"version": "v3", "created": "Fri, 24 Jul 2020 14:28:32 GMT"}, {"version": "v4", "created": "Wed, 18 Nov 2020 14:38:38 GMT"}, {"version": "v5", "created": "Mon, 8 Feb 2021 07:06:15 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "1902.04023", "submitter": "Ted Dunning", "authors": "Ted Dunning and Otmar Ertl", "title": "Computing Extremely Accurate Quantiles Using t-Digests", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present on-line algorithms for computing approximations of rank-based\nstatistics that give high accuracy, particularly near the tails of a\ndistribution, with very small sketches. Notably, the method allows a quantile\n$q$ to be computed with an accuracy relative to $\\max(q, 1-q)$ rather than\nabsolute accuracy as with most other methods. This new algorithm is robust with\nrespect to skewed distributions or ordered datasets and allows separately\ncomputed summaries to be combined with no loss in accuracy.\n  An open-source Java implementation of this algorithm is available from the\nauthor. Independent implementations in Go and Python are also available.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 17:57:38 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Dunning", "Ted", ""], ["Ertl", "Otmar", ""]]}, {"id": "1902.04212", "submitter": "John Maclean", "authors": "John Maclean, Erik S Van Vleck", "title": "Particle filters for data assimilation based on reduced order data\n  models", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for Data Assimilation (DA) in which the data is\nsplit into multiple sets corresponding to low-rank projections of the state\nspace. Algorithms are developed that assimilate some or all of the projected\ndata, including an algorithm compatible with any generic DA method. The major\napplication explored here is PROJ-PF, a projected Particle Filter. The PROJ-PF\nimplementation assimilates highly informative but low-dimensional observations.\nThe implementation considered here is based upon using projections\ncorresponding to Assimilation in the Unstable Subspace (AUS). In the context of\nparticle filtering, the projected approach mitigates the collapse of particle\nensembles in high dimensional DA problems while preserving as much relevant\ninformation as possible, as the unstable and neutral modes correspond to the\nmost uncertain model predictions. In particular we formulate and numerically\nimplement a projected Optimal Proposal Particle Filter (PROJ-OP-PF) and compare\nto the standard optimal proposal and to the Ensemble Transform Kalman Filter.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 02:04:21 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 00:21:27 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 04:31:53 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 01:38:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Maclean", "John", ""], ["Van Vleck", "Erik S", ""]]}, {"id": "1902.04630", "submitter": "Alen Alexanderian", "authors": "Helen L. Cleaves, Alen Alexanderian, Hayley Guy, Ralph C. Smith, and\n  Meilin Yu", "title": "Derivative-based global sensitivity analysis for models with\n  high-dimensional inputs and functional outputs", "comments": "27 pages; minor revisions; accepted for publication in SIAM Journal\n  on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for derivative-based global sensitivity analysis (GSA)\nfor models with high-dimensional input parameters and functional outputs. We\ncombine ideas from derivative-based GSA, random field representation via\nKarhunen--Lo\\`{e}ve expansions, and adjoint-based gradient computation to\nprovide a scalable computational framework for computing the proposed\nderivative-based GSA measures. We illustrate the strategy for a nonlinear ODE\nmodel of cholera epidemics and for elliptic PDEs with application examples from\ngeosciences and biotransport.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 20:58:02 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 23:38:22 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 15:04:26 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Cleaves", "Helen L.", ""], ["Alexanderian", "Alen", ""], ["Guy", "Hayley", ""], ["Smith", "Ralph C.", ""], ["Yu", "Meilin", ""]]}, {"id": "1902.04827", "submitter": "David  Nott", "authors": "David T. Frazier, David J. Nott, Christopher Drovandi and Robert Kohn", "title": "Bayesian inference using synthetic likelihood: asymptotics and\n  adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implementing Bayesian inference is often computationally challenging in\napplications involving complex models, and sometimes calculating the likelihood\nitself is difficult. Synthetic likelihood is one approach for carrying out\ninference when the likelihood is intractable, but it is straightforward to\nsimulate from the model. The method constructs an approximate likelihood by\ntaking a vector summary statistic as being multivariate normal, with the\nunknown mean and covariance matrix estimated by simulation for any given\nparameter value. Our article makes three contributions. The first shows that if\nthe summary statistic satisfies a central limit theorem, then the synthetic\nlikelihood posterior is asymptotically normal and yields credible sets with the\ncorrect level of frequentist coverage. This result is similar to that obtained\nby approximate Bayesian computation. The second contribution compares the\ncomputational efficiency of Bayesian synthetic likelihood and approximate\nBayesian computation using the acceptance probability for rejection and\nimportance sampling algorithms with a \"good\" proposal distribution. We show\nthat Bayesian synthetic likelihood is computationally more efficient than\napproximate Bayesian computation, and behaves similarly to regression-adjusted\napproximate Bayesian computation. Based on the asymptotic results, the third\ncontribution proposes using adjusted inference methods when a possibly\nmisspecified form is assumed for the covariance matrix of the synthetic\nlikelihood, such as diagonal or a factor model, to speed up the computation.\nThe methodology is illustrated with some simulated and real examples.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 10:02:25 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 10:10:21 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 08:07:02 GMT"}, {"version": "v4", "created": "Fri, 12 Mar 2021 06:01:17 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Frazier", "David T.", ""], ["Nott", "David J.", ""], ["Drovandi", "Christopher", ""], ["Kohn", "Robert", ""]]}, {"id": "1902.04931", "submitter": "Guillaume Damblin", "authors": "Guillaume Damblin, Pierre Gaillard", "title": "Bayesian inference and non-linear extensions of the CIRCE method for\n  quantifying the uncertainty of closure relationships integrated into\n  thermal-hydraulic system codes", "comments": "37 pages, 5 figures", "journal-ref": "Nuclear Engineering and Design, 2020, Volume 359, 1 April 2020,\n  110391", "doi": "10.1016/j.nucengdes.2019.110391", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty Quantification of closure relationships integrated into\nthermal-hydraulic system codes is a critical prerequisite in applying the\nBest-Estimate Plus Uncertainty (BEPU) methodology for nuclear safety and\nlicensing processes.The purpose of the CIRCE method is to estimate the\n(log)-Gaussian probability distribution of a multiplicative factor applied to a\nreference closure relationship in order to assess its uncertainty. Even though\nthis method has been implemented with success in numerous physical scenarios,\nit can still suffer from substantial limitations such as the linearity\nassumption and the difficulty of properly taking into account the inherent\nstatistical uncertainty. In the paper, we will extend the CIRCE method in two\naspects. On the one hand, we adopt the Bayesian setting putting prior\nprobability distributions on the parameters of the (log)-Gaussian distribution.\nThe posterior distribution of the parameters is then computed with respect to\nan experimental database by means of Markov Chain Monte Carlo (MCMC)\nalgorithms. On the other hand, we tackle the more general setting where the\nsimulations do not move linearly against the multiplicative factor(s). MCMC\nalgorithms then become time-prohibitive when the thermal-hydraulic simulations\nexceed a few minutes. This handicap is overcome by using Gaussian process (GP)\nemulators which can yield both reliable and fast predictions of the\nsimulations. The GP-based MCMC algorithms will be applied to quantify the\nuncertainty of two condensation closure relationships at a safety injection\nwith respect to a database of experimental tests. The thermal-hydraulic\nsimulations will be run with the CATHARE 2 computer code.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:43:40 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 08:51:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Damblin", "Guillaume", ""], ["Gaillard", "Pierre", ""]]}, {"id": "1902.05658", "submitter": "Mahdi Teimouri Yanesari", "authors": "Sahar Sadani, Kamel Abdollahnezhad, Mahdi Teimouri, and Vahid Ranjbar", "title": "A new estimator for Weibull distribution parameters: Comprehensive\n  comparative study for Weibull Distribution", "comments": "2 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weibull distribution has received a wide range of applications in engineering\nand science. The utility and usefulness of an estimator is highly subject to\nthe field of practitioner's study. In practice users looking for their desired\nestimator under different setting of parameters and sample sizes. In this paper\nwe focus on two topics. Firstly, we propose $U$-statistics for the Weibull\ndistribution parameters. The consistency and asymptotically normality of the\nintroduced $U$-statistics are proved theoretically and by simulations. Several\nof methods have been proposed for estimating the parameters of Weibull\ndistribution in the literature. These methods include: the generalized least\nsquare type 1, the generalized least square type 2, the $L$-moments, the\nLogarithmic moments, the maximum likelihood estimation, the method of moments,\nthe percentile method, the weighted least square, and weighted maximum\nlikelihood estimation. Secondary, due to lack of a comprehensive comparison\nbetween the Weibull distribution parameters estimators, a comprehensive\ncomparison study is made between our proposed $U$-statistics and above nine\nestimators. Based on simulations, it turns out that the our proposed\n$U$-statistics show the best performance in terms of bias for estimating the\nshape and scale parameters when the sample size is large.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:39:09 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Sadani", "Sahar", ""], ["Abdollahnezhad", "Kamel", ""], ["Teimouri", "Mahdi", ""], ["Ranjbar", "Vahid", ""]]}, {"id": "1902.05717", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Giorgio M. Vitetta, Pasquale Di Viesti and Emilio Sirignano", "title": "A New Smoothing Technique based on the Parallel Concatenation of\n  Forward/Backward Bayesian Filters: Turbo Smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel method for developing filtering algorithms, based on the\nparallel concatenation of Bayesian filters and called turbo filtering, has been\nproposed. In this manuscript we show how the same conceptual approach can be\nexploited to devise a new smoothing method, called turbo smoothing. A turbo\nsmoother combines a turbo filter, employed in its forward pass, with the\nparallel concatenation of two backward information filters used in its backward\npass. As a specific application of our general theory, a detailed derivation of\ntwo turbo smoothing algorithms for conditionally linear Gaussian systems is\nillustrated. Numerical results for a specific dynamic system evidence that\nthese algorithms can achieve a better complexity-accuracy tradeoff than other\nsmoothing techniques recently appeared in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 08:21:22 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Vitetta", "Giorgio M.", ""], ["Di Viesti", "Pasquale", ""], ["Sirignano", "Emilio", ""]]}, {"id": "1902.06020", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Luis Nieto-Barajas and Gabriel Nu\\~nez-Antonio", "title": "Projected P\\'olya Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way of defining probability distributions for circular variables\n(directions in two dimensions) is to radially project probability\ndistributions, originally defined on $\\mathbb{R}^2$, to the unit circle.\nProjected distributions have proved to be useful in the study of circular and\ndirectional data. Although any bivariate distribution can be used to produce a\nprojected circular model, these distributions are typically parametric. In this\narticle we consider a bivariate P\\'olya tree on $\\mathbb{R}^2$ and project it\nto the unit circle to define a new Bayesian nonparametric model for circular\ndata. We study the properties of the proposed model, obtain its posterior\ncharacterisation and show its performance with simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 01:16:37 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 22:37:05 GMT"}, {"version": "v3", "created": "Thu, 5 Nov 2020 00:49:19 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Nieto-Barajas", "Luis", ""], ["Nu\u00f1ez-Antonio", "Gabriel", ""]]}, {"id": "1902.06131", "submitter": "Jiayang Sun", "authors": "Jang Ik Cho, Xiaofeng Wang, Yifan Xu and Jiayang Sun", "title": "LISA: a MATLAB package for Longitudinal Image Sequence Analysis", "comments": "18 pages, 17 figures made from 35 png files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large sequences of images (or movies) can now be obtained on an unprecedented\nscale, which poses fundamental challenges to the existing image analysis\ntechniques. The challenges include heterogeneity, (automatic) alignment,\nmultiple comparisons, potential artifacts, and hidden noises. This paper\nintroduces our MATLAB package, Longitudinal Image Sequence Analysis (LISA), as\na one-stop ensemble of image processing and analysis tool for comparing a\ngeneral class of images from either different times, sessions, or subjects.\nGiven two contrasting sequences of images, the image processing in LISA starts\nwith selecting a region of interest in two representative images, followed by\nautomatic or manual segmentation and registration. Automatic segmentation\nde-noises an image using a mixture of Gaussian distributions of the pixel\nintensity values, while manual segmentation applies a user-chosen intensity\ncut-off value to filter out noises. Automatic registration aligns the\ncontrasting images based on a mid-line regression whereas manual registration\nlines up the images along a reference line formed by two user-selected points.\nThe processed images are then rendered for simultaneous statistical comparisons\nto generate D, S, T, and P-maps. The D map represents a curated difference of\ncontrasting images, the S map is the non-parametrically smoothed differences,\nthe T map presents the variance-adjusted, smoothed differences, and the P-map\nprovides multiplicity-controlled p-values. These maps reveal the regions with\nsignificant differences due to either longitudinal, subject-specific, or\ntreatment changes. A user can skip the image processing step to dive directly\ninto the statistical analysis step if the images have already been processed.\nHence, LISA offers flexibility in applying other image pre-processing tools.\nLISA also has a parallel computing option for high definition images.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 17:50:19 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Cho", "Jang Ik", ""], ["Wang", "Xiaofeng", ""], ["Xu", "Yifan", ""], ["Sun", "Jiayang", ""]]}, {"id": "1902.06603", "submitter": "Jeffrey Negrea", "authors": "Jeffrey Negrea", "title": "Optimal Scaling and Shaping of Random Walk Metropolis via Diffusion\n  Limits of Block-I.I.D. Targets", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends Roberts et al. (1997) by considering limits of Random Walk\nMetropolis (RWM) applied to block IID target distributions, with corresponding\nblock-independent proposals. The extension verifies the robustness of the\noptimal scaling heuristic, to tune the acceptance rate to $\\approx0.234$, for\nany choice of proposal shaping. We upgrade the form of weak convergence from a\nfinite-dimensional subprocess to the infinite dimensional process. We show that\nthe optimal shaping (in terms of the decay of autocorrelations of linear\nfunctions) is the variance of the target distribution. We show that this choice\ncoincides with the optimal shaping in terms of spectral gaps in special cases\nwhere they can be computed. Lastly, we provide some negative guarantees,\nshowing that RWM performance degrades with higher-order dependence. In such\ncases, no tuning of RWM will yield performance comparable to an IID target.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 15:05:50 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Negrea", "Jeffrey", ""]]}, {"id": "1902.06641", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer", "title": "Is a single unique Bayesian network enough to accurately represent your\n  data?", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network (BN) modelling is extensively used in systems epidemiology.\nUsually it consists in selecting and reporting the best-fitting structure\nconditional to the data. A major practical concern is avoiding overfitting, on\naccount of its extreme flexibility and its modelling richness. Many approaches\nhave been proposed to control for overfitting. Unfortunately, they essentially\nall rely on very crude decisions that result in too simplistic approaches for\nsuch complex systems. In practice, with limited data sampled from complex\nsystem, this approach seems too simplistic. An alternative would be to use the\nMonte Carlo Markov chain model choice (MC3) over the network to learn the\nlandscape of reasonably supported networks, and then to present all possible\narcs with their MCMC support. This paper presents an R implementation, called\nmcmcabn, of a flexible structural MC3 that is accessible to non-specialists.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 16:43:18 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1902.06861", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Nishika Ranathunga", "title": "Computation of the expected value of a function of a chi-distributed\n  random variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of numerically evaluating the expected value of a\nsmooth bounded function of a chi-distributed random variable, divided by the\nsquare root of the number of degrees of freedom. This problem arises in the\ncontexts of simultaneous inference, the selection and ranking of populations\nand in the evaluation of multivariate t probabilities. It also arises in the\nassessment of the coverage probability and expected volume properties of the\nsome non-standard confidence regions. We use a transformation put forward by\nMori, followed by the application of the trapezoidal rule. This rule has the\nremarkable property that, for suitable integrands, it is exponentially\nconvergent. We use it to create a nested sequence of quadrature rules, for the\nestimation of the approximation error, so that previous evaluations of the\nintegrand are not wasted. The application of the trapezoidal rule requires the\napproximation of an infinite sum by a finite sum. We provide a new easily\ncomputed upper bound on the error of this approximation. Our overall conclusion\nis that this method is a very suitable candidate for the computation of the\ncoverage and expected volume properties of non-standard confidence regions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 02:23:36 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 04:46:30 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 23:25:51 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Kabaila", "Paul", ""], ["Ranathunga", "Nishika", ""]]}, {"id": "1902.07706", "submitter": "Malcolm Itter", "authors": "Malcolm S. Itter, Jarno Vanhatalo, Andrew O. Finley", "title": "EcoMem: An R package for quantifying ecological memory", "comments": "11 pages, 3 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological processes may exhibit memory to past disturbances affecting the\nresilience of ecosystems to future disturbance. Understanding the role of\necological memory in shaping ecosystem responses to disturbance under global\nchange is a critical step toward developing effective adaptive management\nstrategies to maintain ecosystem function and biodiversity. We developed\nEcoMem, an R package for quantifying ecological memory functions using common\nenvironmental time series data (continuous, count, proportional) applying a\nBayesian hierarchical framework. The package estimates memory functions for\ncontinuous and binary (e.g., disturbance chronology) variables making no a\npriori assumption on the form of the functions. EcoMem allows users to quantify\necological memory for a wide range of ecosystem processes and responses. The\nutility of the package to advance understanding of the memory of ecosystems to\nenvironmental drivers is demonstrated using a simulated dataset and a case\nstudy assessing the memory of boreal tree growth to insect defoliation.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:59:55 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Itter", "Malcolm S.", ""], ["Vanhatalo", "Jarno", ""], ["Finley", "Andrew O.", ""]]}, {"id": "1902.07770", "submitter": "Yunzhang Zhu", "authors": "Shanshan Tu, Yunzhang Zhu, Yoonkyung Lee", "title": "Cross Validation for Penalized Quantile Regression with a Case-Weight\n  Adjusted Solution Path", "comments": "54 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation is widely used for selecting tuning parameters in\nregularization methods, but it is computationally intensive in general. To\nlessen its computational burden, approximation schemes such as generalized\napproximate cross validation (GACV) are often employed. However, such\napproximations may not work well when non-smooth loss functions are involved.\nAs a case in point, approximate cross validation schemes for penalized quantile\nregression do not work well for extreme quantiles. In this paper, we propose a\nnew algorithm to compute the leave-one-out cross validation scores exactly for\nquantile regression with ridge penalty through a case-weight adjusted solution\npath. Resorting to the homotopy technique in optimization, we introduce a case\nweight for each individual data point as a continuous embedding parameter and\ndecrease the weight gradually from one to zero to link the estimators based on\nthe full data and those with a case deleted. This allows us to design a\nsolution path algorithm to compute all leave-one-out estimators very\nefficiently from the full-data solution. We show that the case-weight adjusted\nsolution path is piecewise linear in the weight parameter, and using the\nsolution path, we examine case influences comprehensively and observe that\ndifferent modes of case influences emerge, depending on the specified\nquantiles, data dimensions and penalty parameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 20:35:41 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Tu", "Shanshan", ""], ["Zhu", "Yunzhang", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "1902.07953", "submitter": "Roel Ceballos", "authors": "Empha Grace Perez and Roel F. Ceballos", "title": "Malaria Incidence in the Philippines: Prediction using the\n  Autoregressive Moving Average Models", "comments": null, "journal-ref": "International Journal of Engineering and Future Technology, 16(4),\n  2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study was conducted to develop an appropriate model that could predict\nthe weekly reported Malaria incidence in the Philippines using the Box-Jenkins\nmethod.The data were retrieved from the Department of Health(DOH) website in\nthe Philippines. It contains 70 data points of which 60 data points were used\nin model building and the remaining 10 data points were used for forecast\nevaluation. The R Statistical Software was used to do all the necessary\ncomputations in the study. Box-Cox Transformation and Differencing was done to\nmake the series stationary. Based on the results of the analysis, ARIMA (2, 1,\n0) is the appropriate model for the weekly Malaria incidence in the\nPhilippines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:47:46 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Perez", "Empha Grace", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1902.08179", "submitter": "Holden Lee", "authors": "Holden Lee, Oren Mangoubi, Nisheeth K. Vishnoi", "title": "Online Sampling from Log-Concave Distributions", "comments": "42 pages", "journal-ref": "NeurIPS 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sequence of convex functions $f_0, f_1, \\ldots, f_T$, we study the\nproblem of sampling from the Gibbs distribution $\\pi_t \\propto\ne^{-\\sum_{k=0}^tf_k}$ for each epoch $t$ in an online manner. Interest in this\nproblem derives from applications in machine learning, Bayesian statistics, and\noptimization where, rather than obtaining all the observations at once, one\nconstantly acquires new data, and must continuously update the distribution.\nOur main result is an algorithm that generates roughly independent samples from\n$\\pi_t$ for every epoch $t$ and, under mild assumptions, makes\n$\\mathrm{polylog}(T)$ gradient evaluations per epoch. All previous results\nimply a bound on the number of gradient or function evaluations which is at\nleast linear in $T$. Motivated by real-world applications, we assume that\nfunctions are smooth, their associated distributions have a bounded second\nmoment, and their minimizer drifts in a bounded manner, but do not assume they\nare strongly convex. In particular, our assumptions hold for online Bayesian\nlogistic regression, when the data satisfy natural regularity properties,\ngiving a sampling algorithm with updates that are poly-logarithmic in $T$. In\nsimulations, our algorithm achieves accuracy comparable to an algorithm\nspecialized to logistic regression. Key to our algorithm is a novel stochastic\ngradient Langevin dynamics Markov chain with a carefully designed variance\nreduction step and constant batch size. Technically, lack of strong convexity\nis a significant barrier to analysis and, here, our main contribution is a\nmartingale exit time argument that shows our Markov chain remains in a ball of\nradius roughly poly-logarithmic in $T$ for enough time to reach within\n$\\varepsilon$ of $\\pi_t$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 18:42:14 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 01:25:58 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2019 15:50:02 GMT"}, {"version": "v4", "created": "Wed, 4 Dec 2019 23:52:58 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Lee", "Holden", ""], ["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1902.08432", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Marta Crispino, Qinghua Liu, Valeria Vitelli", "title": "BayesMallows: An R Package for the Bayesian Mallows Model", "comments": null, "journal-ref": null, "doi": "10.32614/RJ-2020-026", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BayesMallows is an R package for analyzing data in the form of rankings or\npreferences with the Mallows rank model, and its finite mixture extension, in a\nBayesian probabilistic framework.\n  The Mallows model is a well-known model, grounded on the idea that the\nprobability density of an observed ranking decreases exponentially fast as its\ndistance to the location parameter increases. Despite the model being quite\npopular, this is the first Bayesian implementation that allows a wide choice of\ndistances, and that works well with a large amount of items to be ranked.\nBayesMallows supports footrule, Spearman, Kendall, Cayley, Hamming and Ulam\ndistances, allowing full use of the rich expressiveness of the Mallows model.\nThis is possible thanks to the implementation of fast algorithms for\napproximating the partition function of the model under various distances.\nAlthough developed for being used in computing the posterior distribution of\nthe model, these algorithms may be of interest in their own right.\n  BayesMallows handles non-standard data: partial rankings and pairwise\ncomparisons, even in cases including non-transitive preference patterns. The\nadvantage of the Bayesian paradigm in this context comes from its ability to\ncoherently quantify posterior uncertainties of estimates of any quantity of\ninterest. These posteriors are fully available to the user, and the package\ncomes with convienient tools for summarizing and visualizing the posterior\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 10:46:02 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Crispino", "Marta", ""], ["Liu", "Qinghua", ""], ["Vitelli", "Valeria", ""]]}, {"id": "1902.08452", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Nonconvex sampling with the Metropolis-adjusted Langevin algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Langevin Markov chain algorithms are widely deployed methods to sample\nfrom distributions in challenging high-dimensional and non-convex statistics\nand machine learning applications. Despite this, current bounds for the\nLangevin algorithms are slower than those of competing algorithms in many\nimportant situations, for instance when sampling from weakly log-concave\ndistributions, or when sampling or optimizing non-convex log-densities. In this\npaper, we obtain improved bounds in many of these situations, showing that the\nMetropolis-adjusted Langevin algorithm (MALA) is faster than the best bounds\nfor its competitor algorithms when the target distribution satisfies weak\nthird- and fourth- order regularity properties associated with the input data.\nIn many settings, our regularity conditions are weaker than the usual Euclidean\noperator norm regularity properties, allowing us to show faster bounds for a\nmuch larger class of distributions than would be possible with the usual\nEuclidean operator norm approach, including in statistics and machine learning\napplications where the data satisfy a certain incoherence condition. In\nparticular, we show that using our regularity conditions one can obtain faster\nbounds for applications which include sampling problems in Bayesian logistic\nregression with weakly convex priors, and the nonconvex optimization problem of\nlearning linear classifiers with zero-one loss functions.\n  Our main technical contribution in this paper is our analysis of the\nMetropolis acceptance probability of MALA in terms of its \"energy-conservation\nerror,\" and our bound for this error in terms of third- and fourth- order\nregularity conditions. Our combination of this higher-order analysis of the\nenergy conservation error with the conductance method is key to obtaining\nbounds which have a sub-linear dependence on the dimension $d$ in the\nnon-strongly logconcave setting.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 12:05:58 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 16:40:25 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1902.09029", "submitter": "Yuzhou Chen", "authors": "Yuzhou Chen, Yulia R. Gel, Vyacheslav Lyubchich, and Kusha Nezafati", "title": "Snowboot: Bootstrap Methods for Network Inference", "comments": null, "journal-ref": "The R Journal (2018) 10:2, pages 95-113", "doi": "10.32614/RJ-2018-056", "report-no": null, "categories": "stat.CO cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex networks are used to describe a broad range of disparate social\nsystems and natural phenomena, from power grids to customer segmentation to\nhuman brain connectome. Challenges of parametric model specification and\nvalidation inspire a search for more data-driven and flexible nonparametric\napproaches for inference of complex networks. In this paper we discuss\nmethodology and R implementation of two bootstrap procedures on random\nnetworks, that is, patchwork bootstrap of Thompson et al. (2016) and Gel et al.\n(2017) and vertex bootstrap of Snijders and Borgatti (1999). To our knowledge,\nthe new R package snowboot is the first implementation of the vertex and\npatchwork bootstrap inference on networks in R. Our new package is accompanied\nwith a detailed user's manual, and is compatible with the popular R package on\nnetwork studies igraph. We evaluate the patchwork bootstrap and vertex\nbootstrap with extensive simulation studies and illustrate their utility in\napplication to analysis of real world networks.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 22:31:43 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Chen", "Yuzhou", ""], ["Gel", "Yulia R.", ""], ["Lyubchich", "Vyacheslav", ""], ["Nezafati", "Kusha", ""]]}, {"id": "1902.09046", "submitter": "David Warne", "authors": "David J. Warne (1), Scott A. Sisson (2), Christopher Drovandi (1) ((1)\n  Queensland University of Technology, (2) University of New South Wales)", "title": "Vector operations for accelerating expensive Bayesian computations -- a\n  tutorial guide", "comments": null, "journal-ref": null, "doi": "10.1214/21-BA1265", "report-no": null, "categories": "stat.CO cs.DC cs.MS cs.PF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications in Bayesian statistics are extremely computationally\nintensive. However, they are often inherently parallel, making them prime\ntargets for modern massively parallel processors. Multi-core and distributed\ncomputing is widely applied in the Bayesian community, however, very little\nattention has been given to fine-grain parallelisation using single instruction\nmultiple data (SIMD) operations that are available on most modern commodity\nCPUs and is the basis of GPGPU computing. In this work, we practically\ndemonstrate, using standard programming libraries, the utility of the SIMD\napproach for several topical Bayesian applications. We show that SIMD can\nimprove the floating point arithmetic performance resulting in up to $6\\times$\nimprovement in serial algorithm performance. Importantly, these improvements\nare multiplicative to any gains achieved through multi-core processing. We\nillustrate the potential of SIMD for accelerating Bayesian computations and\nprovide the reader with techniques for exploiting modern massively parallel\nprocessing environments using standard tools.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 00:38:23 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 08:31:45 GMT"}, {"version": "v3", "created": "Mon, 14 Dec 2020 06:22:53 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Warne", "David J.", ""], ["Sisson", "Scott A.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1902.09615", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Richard K. Crump, Max H. Farrell, Yingjie Feng", "title": "Binscatter Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the \\texttt{Stata} (and \\texttt{R}) package \\textsf{Binsreg},\nwhich implements the binscatter methods developed in\n\\citet*{Cattaneo-Crump-Farrell-Feng_2019_Binscatter}. The package includes the\ncommands \\texttt{binsreg}, \\texttt{binsregtest}, and \\texttt{binsregselect}.\nThe first command (\\texttt{binsreg}) implements binscatter for the regression\nfunction and its derivatives, offering several point estimation, confidence\nintervals and confidence bands procedures, with particular focus on\nconstructing binned scatter plots. The second command (\\texttt{binsregtest})\nimplements hypothesis testing procedures for parametric specification and for\nnonparametric shape restrictions of the unknown regression function. Finally,\nthe third command (\\texttt{binsregselect}) implements data-driven number of\nbins selectors for binscatter implementation using either quantile-spaced or\nevenly-spaced binning/partitioning. All the commands allow for covariate\nadjustment, smoothness restrictions, weighting and clustering, among other\nfeatures. A companion \\texttt{R} package with the same capabilities is also\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:59:23 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Crump", "Richard K.", ""], ["Farrell", "Max H.", ""], ["Feng", "Yingjie", ""]]}, {"id": "1902.09653", "submitter": "Indranil Sahoo", "authors": "Indranil Sahoo, Joseph Guinness and Brian J. Reich", "title": "Estimating Atmospheric Motion Winds from Satellite Image Data using\n  Space-time Drift Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostationary satellites collect high-resolution weather data comprising a\nseries of images which can be used to estimate wind speed and direction at\ndifferent altitudes. The Derived Motion Winds (DMW) Algorithm is commonly used\nto process these data and estimate atmospheric winds by tracking features in\nimages taken by the GOES-R series of the NOAA geostationary meteorological\nsatellites. However, the wind estimates from the DMW Algorithm are sparse and\ndo not come with uncertainty measures. This motivates us to statistically model\nwind motions as a spatial process drifting in time. We propose a covariance\nfunction that depends on spatial and temporal lags and a drift parameter to\ncapture the wind speed and wind direction. We estimate the parameters by local\nmaximum likelihood. Our method allows us to compute standard errors of the\nestimates, enabling spatial smoothing of the estimates using a Gaussian kernel\nweighted by the inverses of the estimated variances. We conduct extensive\nsimulation studies to determine the situations where our method performs well.\nThe proposed method is applied to the GOES-15 brightness temperature data over\nColorado and reduces prediction error of brightness temperature compared to the\nDMW Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 23:08:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:12:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 16:05:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sahoo", "Indranil", ""], ["Guinness", "Joseph", ""], ["Reich", "Brian J.", ""]]}, {"id": "1902.09796", "submitter": "Neelesh Upadhye Dr", "authors": "Aastha M. Sathe and Neelesh. S. Upadhye", "title": "Estimation of the Parameters of Multivariate Stable Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we begin our discussion with some of the well-known methods\navailable in the literature for the estimation of the parameters of a\nunivariate/multivariate stable distribution. Based on the available methods, a\nnew hybrid method is proposed for the estimation of the parameters of a\nunivariate stable distribution. The proposed method is further used for the\nestimation of the parameters of a strictly multivariate stable distribution.\nThe efficiency, accuracy, and simplicity of the new method is shown through\nMonte-Carlo simulation. Finally, we apply the proposed method to the univariate\nand bivariate financial data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 08:31:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sathe", "Aastha M.", ""], ["Upadhye", "Neelesh. S.", ""]]}, {"id": "1902.10283", "submitter": "Behnam Malmir", "authors": "Behnam Malmir and Shing I Chang", "title": "Gait Change Detection Using Parameters Generated from Microsoft Kinect\n  Coordinates", "comments": "This article is an updated version of a paper entitled 'Gait Change\n  Detection Using Parameters Generated from Microsoft Kinect Coordinates'\n  presented at the 2016 Industrial and Systems Engineering Research Conference\n  (ISERC) in Anaheim, California (May 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a method to convert Microsoft Kinect coordinates into\ngait parameters in order to detect a person's gait change. The proposed method\ncan help quantify the progress of physical therapy. Microsoft Kinect, a popular\nplatform for video games, was used to generate 25 joints to form a human\nskeleton, and then the proposed method converted the coordinates of selected\nKinect joints into gait parameters such as spine tilt, hip tilt, and shoulder\ntilt, which were tracked over time. Sample entropy measure was then applied to\nquantify the variability of each gait parameter. Male and female subjects\nwalked a three-meter path multiple times in initial experiments, and their\nwalking patterns were recorded via the proposed Kinect device through the\nfrontal plane. Time series of the gait parameters were generated for subjects\nwith and without knee braces. Sample entropy was used to transform these time\nseries into numerical values for comparison of these two conditions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 00:49:31 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""]]}, {"id": "1902.10412", "submitter": "Alexander Kreuzer", "authors": "Alexander Kreuzer, Claudia Czado", "title": "Efficient Bayesian inference for nonlinear state space models with\n  univariate autoregressive state equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent autoregressive processes are a popular choice to model time varying\nparameters. These models can be formulated as nonlinear state space models for\nwhich inference is not straightforward due to the high number of parameters.\nTherefore maximum likelihood methods are often infeasible and researchers rely\non alternative techniques, such as Gibbs sampling. But conventional Gibbs\nsamplers are often tailored to specific situations and suffer from high\nautocorrelation among repeated draws. We present a Gibbs sampler for general\nnonlinear state space models with an univariate autoregressive state equation.\nFor this we employ an interweaving strategy and elliptical slice sampling to\nexploit the dependence implied by the autoregressive process. Within a\nsimulation study we demonstrate the efficiency of the proposed sampler for\nbivariate dynamic copula models. Further we are interested in modeling the\nvolatility return relationship. Therefore we use the proposed sampler to\nestimate the parameters of stochastic volatility models with skew Student t\nerrors and the parameters of a novel bivariate dynamic mixture copula model.\nThis model allows for dynamic asymmetric tail dependence. Comparison to\nrelevant benchmark models, such as the DCC-GARCH or a Student t copula model,\nwith respect to predictive accuracy shows the superior performance of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 09:44:40 GMT"}, {"version": "v2", "created": "Thu, 28 Feb 2019 07:06:26 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 11:56:29 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Kreuzer", "Alexander", ""], ["Czado", "Claudia", ""]]}, {"id": "1902.10704", "submitter": "Yanzhi Chen", "authors": "Yanzhi Chen, Michael U. Gutmann", "title": "Adaptive Gaussian Copula ABC", "comments": "8 pages, 5 figures, accepted to AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a set of techniques for Bayesian\ninference when the likelihood is intractable but sampling from the model is\npossible. This work presents a simple yet effective ABC algorithm based on the\ncombination of two classical ABC approaches --- regression ABC and sequential\nABC. The key idea is that rather than learning the posterior directly, we first\ntarget another auxiliary distribution that can be learned accurately by\nexisting methods, through which we then subsequently learn the desired\nposterior with the help of a Gaussian copula. During this process, the\ncomplexity of the model changes adaptively according to the data at hand.\nExperiments on a synthetic dataset as well as three real-world inference tasks\ndemonstrates that the proposed method is fast, accurate, and easy to use.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:28:14 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Chen", "Yanzhi", ""], ["Gutmann", "Michael U.", ""]]}]