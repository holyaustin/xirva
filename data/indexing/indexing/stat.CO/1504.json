[{"id": "1504.00053", "submitter": "Michael Habeck", "authors": "Michael Habeck", "title": "Ensemble annealing of complex physical systems", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cond-mat.stat-mech physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms for simulating complex physical systems or solving difficult\noptimization problems often resort to an annealing process. Rather than\nsimulating the system at the temperature of interest, an annealing algorithm\nstarts at a temperature that is high enough to ensure ergodicity and gradually\ndecreases it until the destination temperature is reached. This idea is used in\npopular algorithms such as parallel tempering and simulated annealing. A\ngeneral problem with annealing methods is that they require a temperature\nschedule. Choosing well-balanced temperature schedules can be tedious and\ntime-consuming. Imbalanced schedules can have a negative impact on the\nconvergence, runtime and success of annealing algorithms. This article outlines\na unifying framework, ensemble annealing, that combines ideas from simulated\nannealing, histogram reweighting and nested sampling with concepts in\nthermodynamic control. Ensemble annealing simultaneously simulates a physical\nsystem and estimates its density of states. The temperatures are lowered not\naccording to a prefixed schedule but adaptively so as to maintain a constant\nrelative entropy between successive ensembles. After each step on the\ntemperature ladder an estimate of the density of states is updated and a new\ntemperature is chosen. Ensemble annealing is highly practical and broadly\napplicable. This is illustrated for various systems including Ising, Potts, and\nprotein models.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 21:52:49 GMT"}], "update_date": "2015-04-02", "authors_parsed": [["Habeck", "Michael", ""]]}, {"id": "1504.00202", "submitter": "Deepak Tunuguntla", "authors": "Deepak R. Tunuguntla, Anthony R. Thornton, Thomas Weinhart", "title": "From discrete elements to continuum fields: Extension to bidisperse\n  systems", "comments": "Published in the special issue -Modelling and computational\n  challenges in granular materials- of the journal -Computational Particle\n  Mechanics-", "journal-ref": null, "doi": "10.1007/s40571-015-0087-y", "report-no": null, "categories": "cond-mat.soft stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To develop, calibrate and/or validate continuum models from experimental or\nnumerical data, micro-macro transition methods are required. These methods are\nused to obtain the continuum fields (such as density, momentum, stress) from\nthe discrete data (positions, velocities, forces). This is especially\nchallenging for non-uniform and dynamic situations in the presence of multiple\ncomponents. Here, we present a general method to perform this micro-macro\ntransition, but for simplicity we restrict our attention to two-component\nscenarios, e.g. particulate mixtures containing two types of particles. We\npresent an extension to the micro-macro transition method, called\n\\emph{coarse-graining}, for unsteady two-component flows. By construction, this\nnovel averaging method is advantageous, e.g. when compared to binning methods,\nbecause the obtained macroscopic fields are consistent with the continuum\nequations of mass, momentum, and energy balance. Additionally, boundary\ninteraction forces can be taken into account in a self-consistent way and thus\nallow for the construction of continuous stress fields even within one particle\nradius of the boundaries. Similarly, stress and drag forces can also be\ndetermined for individual constituents of a multi-component mixture, which is\nessential for several continuum applications, \\textit{e.g.} mixture theory\nsegregation models. Moreover, the method does not require ensemble-averaging\nand thus can be efficiently exploited to investigate static, steady, and\ntime-dependent flows. The method presented in this paper is valid for any\ndiscrete data, \\textit{e.g.} particle simulations, molecular dynamics,\nexperimental data, etc.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 12:40:28 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 15:19:44 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2015 12:58:37 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2015 00:18:59 GMT"}], "update_date": "2015-12-04", "authors_parsed": [["Tunuguntla", "Deepak R.", ""], ["Thornton", "Anthony R.", ""], ["Weinhart", "Thomas", ""]]}, {"id": "1504.00298", "submitter": "Richard Everitt", "authors": "Richard G. Everitt, Adam M. Johansen, Ellen Rowing, Melina\n  Evdemon-Hogan", "title": "Bayesian model comparison with un-normalised likelihoods", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9629-2", "report-no": null, "categories": "stat.CO physics.data-an stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for which the likelihood function can be evaluated only up to a\nparameter-dependent unknown normalising constant, such as Markov random field\nmodels, are used widely in computer science, statistical physics, spatial\nstatistics, and network analysis. However, Bayesian analysis of these models\nusing standard Monte Carlo methods is not possible due to the intractability of\ntheir likelihood functions. Several methods that permit exact, or close to\nexact, simulation from the posterior distribution have recently been developed.\nHowever, estimating the evidence and Bayes' factors (BFs) for these models\nremains challenging in general. This paper describes new random weight\nimportance sampling and sequential Monte Carlo methods for estimating BFs that\nuse simulation to circumvent the evaluation of the intractable likelihood, and\ncompares them to existing methods. In some cases we observe an advantage in the\nuse of biased weight estimates. An initial investigation into the theoretical\nand empirical properties of this class of methods is presented. Some support\nfor the use of biased estimates is presented, but we advocate caution in the\nuse of such estimates.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:10:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 15:42:08 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2016 23:33:41 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Everitt", "Richard G.", ""], ["Johansen", "Adam M.", ""], ["Rowing", "Ellen", ""], ["Evdemon-Hogan", "Melina", ""]]}, {"id": "1504.00302", "submitter": "Julio Castrillon PhD", "authors": "Julio E. Castrillon-Candas, Marc G. Genton, Rio Yokota", "title": "Multi-Level Restricted Maximum Likelihood Covariance Estimation and\n  Kriging for Large Non-Gridded Spatial Datasets", "comments": "Spatial Statistics, Available online 10 November 2015", "journal-ref": null, "doi": "10.1016/j.spasta.2015.10.006", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multi-level restricted Gaussian maximum likelihood method for\nestimating the covariance function parameters and computing the best unbiased\npredictor. Our approach produces a new set of multi-level contrasts where the\ndeterministic parameters of the model are filtered out thus enabling the\nestimation of the covariance parameters to be decoupled from the deterministic\ncomponent. Moreover, the multi-level covariance matrix of the contrasts exhibit\nfast decay that is dependent on the smoothness of the covariance function. Due\nto the fast decay of the multi-level covariance matrix coefficients only a\nsmall set is computed with a level dependent criterion. We demonstrate our\napproach on problems of up to 512,000 observations with a Matern covariance\nfunction and highly irregular placements of the observations. In addition,\nthese problems are numerically unstable and hard to solve with traditional\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2015 17:20:30 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 18:07:52 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Castrillon-Candas", "Julio E.", ""], ["Genton", "Marc G.", ""], ["Yokota", "Rio", ""]]}, {"id": "1504.00600", "submitter": "Ashkan Panahi", "authors": "Ashkan Panahi and Mats Viberg", "title": "A Novel Sparsity-Based Approach to Recursive Estimation of Dynamic\n  Parameter Sets", "comments": "The paper is to be submitted to the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a variable number of parameters with a\ndynamic nature. A familiar example is finding the position of moving targets\nusing sensor array observations. The problem is challenging in cases where\neither the observations are not reliable or the parameters evolve rapidly.\nInspired by the sparsity based techniques, we introduce a novel Bayesian model\nfor the problems of interest and study its associated recursive Bayesian\nfilter. We propose an algorithm approximating the Bayesian filter, maintaining\na reasonable amount of calculations. We compare by numerical evaluation the\nresulting technique to state-of-the-art algorithms in different scenarios. In a\nscenario with a low SNR, the proposed method outperforms other complex\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2015 16:05:12 GMT"}], "update_date": "2015-04-03", "authors_parsed": [["Panahi", "Ashkan", ""], ["Viberg", "Mats", ""]]}, {"id": "1504.00781", "submitter": "Bhaveshkumar Dharmani", "authors": "Dharmani Bhaveshkumar C", "title": "The Gram-Charlier A Series based Extended Rule-of-Thumb for Bandwidth\n  Selection in Univariate and Multivariate Kernel Density Estimations", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article derives a novel Gram-Charlier A (GCA) Series based Extended\nRule-of-Thumb (ExROT) for bandwidth selection in Kernel Density Estimation\n(KDE). There are existing various bandwidth selection rules achieving\nminimization of the Asymptotic Mean Integrated Square Error (AMISE) between the\nestimated probability density function (PDF) and the actual PDF. The rules\ndiffer in a way to estimate the integration of the squared second order\nderivative of an unknown PDF $(f(\\cdot))$, identified as the roughness\n$R(f''(\\cdot))$. The simplest Rule-of-Thumb (ROT) estimates $R(f''(\\cdot))$\nwith an assumption that the density being estimated is Gaussian. Intuitively,\nbetter estimation of $R(f''(\\cdot))$ and consequently better bandwidth\nselection rules can be derived, if the unknown PDF is approximated through an\ninfinite series expansion based on a more generalized density assumption. As a\ndemonstration and verification to this concept, the ExROT derived in the\narticle uses an extended assumption that the density being estimated is near\nGaussian. This helps use of the GCA expansion as an approximation to the\nunknown near Gaussian PDF. The ExROT for univariate KDE is extended to that for\nmultivariate KDE. The required multivariate AMISE criteria is re-derived using\nelementary calculus of several variables, instead of Tensor calculus. The\nderivation uses the Kronecker product and the vector differential operator to\nachieve the AMISE expression in vector notations. There is also derived ExROT\nfor kernel based density derivative estimator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 08:42:44 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["C", "Dharmani Bhaveshkumar", ""]]}, {"id": "1504.00917", "submitter": "Mar\\'ia Pilar Fr\\'ias Bustamante", "authors": "Mar\\'ia Pilar Fr\\'ias, Alexander V. Ivanov, Nikolai Leonenko,\n  Francisco Mart\\'inez, Mar\\'ia Dolores Ruiz-Medina", "title": "Detecting hidden periodicities for models with cyclical errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the estimation of parameters in the harmonic regression with\ncyclically dependent errors is addressed. Asymptotic properties of the\nleast-squares estimates are analyzed by simulation experiments. By numerical\nsimulation, we prove that consistency and asymptotic normality of the\nleast-squares parameter estimator studied holds under different scenarios,\nwhere theoretical results do not exist, and have yet to be proven. In\nparticular, these two asymptotic properties are shown by simulations for the\nleast-squares parameter estimator in the non-linear regression model analyzed,\nwhen its error term is defined as a non-linear transformation of a Gaussian\nrandom process displaying long-range dependence.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2015 19:40:00 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Fr\u00edas", "Mar\u00eda Pilar", ""], ["Ivanov", "Alexander V.", ""], ["Leonenko", "Nikolai", ""], ["Mart\u00ednez", "Francisco", ""], ["Ruiz-Medina", "Mar\u00eda Dolores", ""]]}, {"id": "1504.00964", "submitter": "Luca Weihs", "authors": "Luca Weihs, Mathias Drton, Dennis Leung", "title": "Efficient Computation of the Bergsma-Dassios Sign Covariance", "comments": "Improved formatting, added reference to R package implementing the\n  algorithm, and added additional experiments", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an extension of Kendall's $\\tau$, Bergsma and Dassios (2014) introduced a\ncovariance measure $\\tau^*$ for two ordinal random variables that vanishes if\nand only if the two variables are independent. For a sample of size $n$, a\ndirect computation of $t^*$, the empirical version of $\\tau^*$, requires\n$O(n^4)$ operations. We derive an algorithm that computes the statistic using\nonly $O(n^2\\log(n))$ operations.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2015 01:05:36 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2015 21:31:37 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Weihs", "Luca", ""], ["Drton", "Mathias", ""], ["Leung", "Dennis", ""]]}, {"id": "1504.01079", "submitter": "Manuel V\\'azquez", "authors": "Joaquin Miguez, Manuel A. Vazquez", "title": "A proof of uniform convergence over time for a distributed particle\n  filter", "comments": "A preliminary version of this work was presented at the IEEE SAM 2014\n  workshop. This work has been accepted for publication in Elsevier Signal\n  Processing", "journal-ref": "Signal Processing, 122, pp.152-163, 2016", "doi": "10.1016/j.sigpro.2015.11.015", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed signal processing algorithms have become a hot topic during the\npast years. One class of algorithms that have received special attention are\nparticles filters (PFs). However, most distributed PFs involve various\nheuristic or simplifying approximations and, as a consequence, classical\nconvergence theorems for standard PFs do not hold for their distributed\ncounterparts. In this paper, we analyze a distributed PF based on the\nnon-proportional weight-allocation scheme of Bolic {\\em et al} (2005) and prove\nrigorously that, under certain stability assumptions, its asymptotic\nconvergence is guaranteed uniformly over time, in such a way that approximation\nerrors can be kept bounded with a fixed computational budget. To illustrate the\ntheoretical findings, we carry out computer simulations for a target tracking\nproblem. The numerical results show that the distributed PF has a negligible\nperformance loss (compared to a centralized filter) for this problem and enable\nus to empirically validate the key assumptions of the analysis.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2015 03:48:17 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 12:02:23 GMT"}], "update_date": "2016-06-03", "authors_parsed": [["Miguez", "Joaquin", ""], ["Vazquez", "Manuel A.", ""]]}, {"id": "1504.01418", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Babak Shahbaba and Hongkai Zhao", "title": "Precomputing Strategy for Hamiltonian Monte Carlo Method Based on\n  Regularity in Parameter Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) algorithms play an important role in\nstatistical inference problems dealing with intractable probability\ndistributions. Recently, many MCMC algorithms such as Hamiltonian Monte Carlo\n(HMC) and Riemannian Manifold HMC have been proposed to provide distant\nproposals with high acceptance rate. These algorithms, however, tend to be\ncomputationally intensive which could limit their usefulness, especially for\nbig data problems due to repetitive evaluations of functions and statistical\nquantities that depend on the data. This issue occurs in many statistic\ncomputing problems. In this paper, we propose a novel strategy that exploits\nsmoothness (regularity) of parameter space to improve computational efficiency\nof MCMC algorithms. When evaluation of functions or statistical quantities are\nneeded at a point in parameter space, interpolation from precomputed values or\nprevious computed values is used. More specifically, we focus on Hamiltonian\nMonte Carlo (HMC) algorithms that use geometric information for faster\nexploration of probability distributions. Our proposed method is based on\nprecomputing the required geometric information on a set of grids before\nrunning sampling information at nearby grids at each iteration of HMC. Sparse\ngrid interpolation method is used for high dimensional problems. Tests on\ncomputational examples are shown to illustrate the advantages of our method.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2015 21:14:54 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 19:49:23 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Zhang", "Cheng", ""], ["Shahbaba", "Babak", ""], ["Zhao", "Hongkai", ""]]}, {"id": "1504.01661", "submitter": "Youssef Khmou", "authors": "Youssef Khmou, Said Safi", "title": "An original Propagator for large array", "comments": "Fourteen pages and four figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate that when the ratio $n$ of the number of\nantenna elements $N$ to the number $P$ of radiating sources is superior or\nequal to $2$, then it is possible to choose a propagator from a set of\n$n(n+1)/2-1$ operators to compute the Angles of Arrival (AoA) of the narrowband\nincoming waves. This new non eigenbased approach is efficient when the Signal\nto Noise Ratio (SNR) is moderate, and gives multitude of possibilities, that\nare dependent of the random data, to construct the complex sets whose columns\nare orthogonal to the signal subspace generated by the radiating sources.\nElementary examples are given for $n=3$, $n=4$ and $n=6$. The simulation\nresults are presented to illustrate the performance of the proposed\ncomputational methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 14:40:41 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Khmou", "Youssef", ""], ["Safi", "Said", ""]]}, {"id": "1504.01794", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Adam Persing, Alexandros Beskos, Kari Heine, Maria De\n  Iorio", "title": "Bayesian Inference for Duplication-Mutation with Complementarity Network\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe an undirected graph $G$ without multiple edges and self-loops,\nwhich is to represent a protein-protein interaction (PPI) network. We assume\nthat $G$ evolved under the duplication-mutation with complementarity (DMC)\nmodel from a seed graph, $G_0$, and we also observe the binary forest $\\Gamma$\nthat represents the duplication history of $G$. A posterior density for the DMC\nmodel parameters is established, and we outline a sampling strategy by which\none can perform Bayesian inference; that sampling strategy employs a particle\nmarginal Metropolis-Hastings (PMMH) algorithm. We test our methodology on\nnumerical examples to demonstrate a high accuracy and precision in the\ninference of the DMC model's mutation and homodimerization parameters.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 01:13:57 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Jasra", "Ajay", ""], ["Persing", "Adam", ""], ["Beskos", "Alexandros", ""], ["Heine", "Kari", ""], ["De Iorio", "Maria", ""]]}, {"id": "1504.01896", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (U. Paris-Dauphine PSL & U. Warwick)", "title": "The Metropolis-Hastings algorithm", "comments": "15 pages, 7 figures, corrections of errors in R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note is a self-contained and basic introduction to the\nMetropolis-Hastings algorithm, this ubiquitous tool used for producing\ndependent simulations from an arbitrary distribution. The document illustrates\nthe principles of the methodology on simple examples with R codes and provides\nreferences to the recent extensions of the method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2015 10:13:16 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 11:39:46 GMT"}, {"version": "v3", "created": "Wed, 27 Jan 2016 13:21:39 GMT"}], "update_date": "2016-01-28", "authors_parsed": [["Robert", "Christian P.", "", "U. Paris-Dauphine PSL & U. Warwick"]]}, {"id": "1504.02382", "submitter": "Shahab Basiri", "authors": "Shahab Basiri, Esa Ollila and Visa Koivunen", "title": "Robust, scalable and fast bootstrap method for analyzing large scale\n  data", "comments": "This paper is submitted for publication in IEEE Transactions On\n  Signal Processing, 8 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2498121", "report-no": null, "categories": "stat.ME cs.IR cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of performing statistical inference for\nlarge scale data sets i.e., Big Data. The volume and dimensionality of the data\nmay be so high that it cannot be processed or stored in a single computing\nnode. We propose a scalable, statistically robust and computationally efficient\nbootstrap method, compatible with distributed processing and storage systems.\nBootstrap resamples are constructed with smaller number of distinct data points\non multiple disjoint subsets of data, similarly to the bag of little bootstrap\nmethod (BLB) [1]. Then significant savings in computation is achieved by\navoiding the re-computation of the estimator for each bootstrap sample.\nInstead, a computationally efficient fixed-point estimation equation is\nanalytically solved via a smart approximation following the Fast and Robust\nBootstrap method (FRB) [2]. Our proposed bootstrap method facilitates the use\nof highly robust statistical methods in analyzing large scale data sets. The\nfavorable statistical properties of the method are established analytically.\nNumerical examples demonstrate scalability, low complexity and robust\nstatistical performance of the method in analyzing large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2015 16:48:28 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 20:01:28 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Basiri", "Shahab", ""], ["Ollila", "Esa", ""], ["Koivunen", "Visa", ""]]}, {"id": "1504.02661", "submitter": "Sebastian  Dorn", "authors": "Sebastian Dorn and Torsten A. En{\\ss}lin", "title": "Stochastic determination of matrix determinants", "comments": "8 pages, 5 figures", "journal-ref": "Phys. Rev. E 92, 013302 (2015)", "doi": "10.1103/PhysRevE.92.013302", "report-no": null, "categories": "physics.data-an astro-ph.IM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix determinants play an important role in data analysis, in particular\nwhen Gaussian processes are involved. Due to currently exploding data volumes,\nlinear operations - matrices - acting on the data are often not accessible\ndirectly but are only represented indirectly in form of a computer routine.\nSuch a routine implements the transformation a data vector undergoes under\nmatrix multiplication. While efficient probing routines to estimate a matrix's\ndiagonal or trace, based solely on such computationally affordable\nmatrix-vector multiplications, are well known and frequently used in signal\ninference, there is no stochastic estimate for its determinant. We introduce a\nprobing method for the logarithm of a determinant of a linear operator. Our\nmethod rests upon a reformulation of the log-determinant by an integral\nrepresentation and the transformation of the involved terms into stochastic\nexpressions. This stochastic determinant determination enables large-size\napplications in Bayesian inference, in particular evidence calculations, model\ncomparison, and posterior determination.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2015 12:43:15 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 09:08:13 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Dorn", "Sebastian", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1504.02914", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "Representing numeric data in 32 bits while preserving 64-bit precision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data files often consist of numbers having only a few significant decimal\ndigits, whose information content would allow storage in only 32 bits. However,\nwe may require that arithmetic operations involving these numbers be done with\n64-bit floating-point precision, which precludes simply representing the data\nas 32-bit floating-point values. Decimal floating point gives a compact and\nexact representation, but requires conversion with a slow division operation\nbefore it can be used. Here, I show that interesting subsets of 64-bit\nfloating-point values can be compactly and exactly represented by the 32 bits\nconsisting of the sign, exponent, and high-order part of the mantissa, with the\nlower-order 32 bits of the mantissa filled in by table lookup, indexed by bits\nfrom the part of the mantissa retained, and possibly from the exponent. For\nexample, decimal data with 4 or fewer digits to the left of the decimal point\nand 2 or fewer digits to the right of the decimal point can be represented in\nthis way using the lower-order 5 bits of the retained part of the mantissa as\nthe index. Data consisting of 6 decimal digits with the decimal point in any of\nthe 7 positions before or after one of the digits can also be represented this\nway, and decoded using 19 bits from the mantissa and exponent as the index.\nEncoding with such a scheme is a simple copy of half the 64-bit value, followed\nif necessary by verification that the value can be represented, by checking\nthat it decodes correctly. Decoding requires only extraction of index bits and\na table lookup. Lookup in a small table will usually reference cache; even with\nlarger tables, decoding is still faster than conversion from decimal floating\npoint with a division operation. I discuss how such schemes perform on recent\ncomputer systems, and how they might be used to automatically compress large\narrays in interpretive languages such as R.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2015 20:33:06 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Neal", "Radford M.", ""]]}, {"id": "1504.02992", "submitter": "Luca Weihs", "authors": "Mathias Drton and Luca Weihs", "title": "Generic Identifiability of Linear Structural Equation Models by Ancestor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear structural equation models, which relate random variables via linear\ninterdependencies and Gaussian noise, are a popular tool for modeling\nmultivariate joint distributions. These models correspond to mixed graphs that\ninclude both directed and bidirected edges representing the linear\nrelationships and correlations between noise terms, respectively. A question of\ninterest for these models is that of parameter identifiability, whether or not\nit is possible to recover edge coefficients from the joint covariance matrix of\nthe random variables. For the problem of determining generic parameter\nidentifiability, we present an algorithm that extends an algorithm from prior\nwork by Foygel, Draisma, and Drton (2012). The main idea underlying our new\nalgorithm is the use of ancestral subsets of vertices in the graph in\napplication of a decomposition idea of Tian (2005).\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2015 17:24:42 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Drton", "Mathias", ""], ["Weihs", "Luca", ""]]}, {"id": "1504.03152", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Isabella Gollini", "title": "Bayesian computational algorithms for social network analysis", "comments": "Book chapter to appear in \"Challenges of Computational Network\n  Analysis With R\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we review some of the most recent computational advances in\nthe rapidly expanding field of statistical social network analysis using the R\nopen-source software. In particular we will focus on Bayesian estimation for\ntwo important families of models: exponential random graph models (ERGMs) and\nlatent space models (LSMs).\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 12:40:55 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Caimo", "Alberto", ""], ["Gollini", "Isabella", ""]]}, {"id": "1504.03355", "submitter": "Ricky Fok WK", "authors": "Ricky Fok, Aijun An, Xiaogong Wang", "title": "Geodesic and Contour Optimization Using Conformal Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel optimization algorithm for continuous functions using\ngeodesics and contours under conformal mapping.The algorithm can find multiple\noptima by first following a geodesic curve to a local optimum then traveling to\nthe next search area by following a contour curve. To improve the efficiency,\nNewton-Raphson algorithm is also employed in local search steps. A proposed\njumping mechanism based on realized geodesics enables the algorithm to jump to\na nearby region and consequently avoid trapping in local optima. Conformal\nmapping is used to resolve numerical instability associated with solving the\nclassical geodesic equations. Geodesic flows under conformal mapping are\nconstructed numerically by using local quadratic approximation. The parameters\nin the algorithm are adaptively chosen to reflect local geometric features of\nthe objective function. Comparisons with many commonly used optimization\nalgorithms including gradient, trust region, genetic algorithm and global\nsearch methods have shown that the proposed algorithm outperforms most widely\nused methods in almost all test cases with only a couple of exceptions.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2015 20:36:42 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Fok", "Ricky", ""], ["An", "Aijun", ""], ["Wang", "Xiaogong", ""]]}, {"id": "1504.03461", "submitter": "Bj\\\"orn Sprungk", "authors": "Daniel Rudolf and Bj\\\"orn Sprungk", "title": "On a generalization of the preconditioned Crank-Nicolson Metropolis\n  algorithm", "comments": "40 pages, 3 Figures", "journal-ref": null, "doi": "10.1007/s10208-016-9340-x", "report-no": null, "categories": "stat.CO math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metropolis algorithms for approximate sampling of probability measures on\ninfinite dimensional Hilbert spaces are considered and a generalization of the\npreconditioned Crank-Nicolson (pCN) proposal is introduced. The new proposal is\nable to incorporate information of the measure of interest. A numerical\nsimulation of a Bayesian inverse problem indicates that a Metropolis algorithm\nwith such a proposal performs independent of the state space dimension and the\nvariance of the observational noise. Moreover, a qualitative convergence result\nis provided by a comparison argument for spectral gaps. In particular, it is\nshown that the generalization inherits geometric ergodicity from the Metropolis\nalgorithm with pCN proposal.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 08:57:31 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2016 09:19:07 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "1504.03467", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu", "title": "A note on one of the Markov chain Monte Carlo novice's questions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel time-homogeneous Markov embedding of a class of time\ninhomogeneous Markov chains widely used in the context of Monte Carlo sampling\nalgorithms which allows us to answer one of the most basic, yet hard, question\nabout the practical implementation of these techniques. We also show that this\nembedding sheds some light on the recent result of [#maire-douc-olsson2013]. We\ndiscuss further applications of the technique.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2015 09:20:21 GMT"}], "update_date": "2015-04-15", "authors_parsed": [["Andrieu", "Christophe", ""]]}, {"id": "1504.04093", "submitter": "Scott Sisson", "authors": "Jingjing Li and David J. Nott and Yanan Fan and Scott A. Sisson", "title": "Extending approximate Bayesian computation methods to high dimensions\n  via a Gaussian copula model", "comments": "TO appear in CSDA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) refers to a family of inference\nmethods used in the Bayesian analysis of complex models where evaluation of the\nlikelihood is difficult. Conventional ABC methods often suffer from the curse\nof dimensionality, and a marginal adjustment strategy was recently introduced\nin the literature to improve the performance of ABC algorithms in\nhigh-dimensional problems. The marginal adjustment approach is extended using a\nGaussian copula approximation. The method first estimates the bivariate\nposterior for each pair of parameters separately using a 2-dimensional Gaussian\ncopula, and then combines these estimates together to estimate the joint\nposterior. The approximation works well in large sample settings when the\nposterior is approximately normal, but also works well in many cases which are\nfar from that situation due to the nonparametric estimation of the marginal\nposterior distributions. If each bivariate posterior distribution can be well\nestimated with a low-dimensional ABC analysis then this Gaussian copula method\ncan extend ABC methods to problems of high dimension. The method also results\nin an analytic expression for the approximate posterior which is useful for\nmany purposes such as approximation of the likelihood itself. This method is\nillustrated with several examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 03:33:13 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 11:00:24 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Li", "Jingjing", ""], ["Nott", "David J.", ""], ["Fan", "Yanan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1504.04184", "submitter": "Esa Ollila", "authors": "Esa Ollila", "title": "Multichannel sparse recovery of complex-valued signals using Huber's\n  criterion", "comments": "To appear in CoSeRa'15 (Pisa, Italy, June 16-19, 2015). arXiv admin\n  note: text overlap with arXiv:1502.02441", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize Huber's criterion to multichannel sparse\nrecovery problem of complex-valued measurements where the objective is to find\ngood recovery of jointly sparse unknown signal vectors from the given multiple\nmeasurement vectors which are different linear combinations of the same known\nelementary vectors. This requires careful characterization of robust\ncomplex-valued loss functions as well as Huber's criterion function for the\nmultivariate sparse regression problem. We devise a greedy algorithm based on\nsimultaneous normalized iterative hard thresholding (SNIHT) algorithm. Unlike\nthe conventional SNIHT method, our algorithm, referred to as HUB-SNIHT, is\nrobust under heavy-tailed non-Gaussian noise conditions, yet has a negligible\nperformance loss compared to SNIHT under Gaussian noise. Usefulness of the\nmethod is illustrated in source localization application with sensor arrays.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 11:25:40 GMT"}], "update_date": "2015-04-17", "authors_parsed": [["Ollila", "Esa", ""]]}, {"id": "1504.04406", "submitter": "Mark Schmidt", "authors": "Mark Schmidt, Reza Babanezhad, Mohamed Osama Ahmed, Aaron Defazio, Ann\n  Clifton, Anoop Sarkar", "title": "Non-Uniform Stochastic Average Gradient Method for Training Conditional\n  Random Fields", "comments": "AI/Stats 2015, 24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply stochastic average gradient (SAG) algorithms for training\nconditional random fields (CRFs). We describe a practical implementation that\nuses structure in the CRF gradient to reduce the memory requirement of this\nlinearly-convergent stochastic gradient method, propose a non-uniform sampling\nscheme that substantially improves practical performance, and analyze the rate\nof convergence of the SAGA variant under non-uniform sampling. Our experimental\nresults reveal that our method often significantly outperforms existing methods\nin terms of the training objective, and performs as well or better than\noptimally-tuned stochastic gradient methods in terms of test error.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2015 23:26:35 GMT"}], "update_date": "2015-04-20", "authors_parsed": [["Schmidt", "Mark", ""], ["Babanezhad", "Reza", ""], ["Ahmed", "Mohamed Osama", ""], ["Defazio", "Aaron", ""], ["Clifton", "Ann", ""], ["Sarkar", "Anoop", ""]]}, {"id": "1504.04696", "submitter": "Arnak Dalalyan S.", "authors": "Samuel Balmand, Arnak S. Dalalyan", "title": "On estimation of the diagonal elements of a sparse precision matrix", "comments": "Companion R package at\n  http://cran.r-project.org/web/packages/DESP/index.html", "journal-ref": "Electron. J. Statist. Volume 10, Number 1, 1551-1579 (2016)", "doi": "10.1214/16-EJS1148", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present several estimators of the diagonal elements of the\ninverse of the covariance matrix, called precision matrix, of a sample of iid\nrandom vectors. The focus is on high dimensional vectors having a sparse\nprecision matrix. It is now well understood that when the underlying\ndistribution is Gaussian, the columns of the precision matrix can be estimated\nindependently form one another by solving linear regression problems under\nsparsity constraints. This approach leads to a computationally efficient\nstrategy for estimating the precision matrix that starts by estimating the\nregression vectors, then estimates the diagonal entries of the precision matrix\nand, in a final step, combines these estimators for getting estimators of the\noff-diagonal entries. While the step of estimating the regression vector has\nbeen intensively studied over the past decade, the problem of deriving\nstatistically accurate estimators of the diagonal entries has received much\nless attention. The goal of the present paper is to fill this gap by presenting\nfour estimators---that seem the most natural ones---of the diagonal entries of\nthe precision matrix and then performing a comprehensive empirical evaluation\nof these estimators. The estimators under consideration are the residual\nvariance, the relaxed maximum likelihood, the symmetry-enforced maximum\nlikelihood and the penalized maximum likelihood. We show, both theoretically\nand empirically, that when the aforementioned regression vectors are estimated\nwithout error, the symmetry-enforced maximum likelihood estimator has the\nsmallest estimation error. However, in a more realistic setting when the\nregression vector is estimated by a sparsity-favoring computationally efficient\nmethod, the qualities of the estimators become relatively comparable with a\nslight advantage for the residual variance estimator.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 08:56:38 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 10:04:17 GMT"}, {"version": "v3", "created": "Fri, 29 May 2015 12:17:08 GMT"}, {"version": "v4", "created": "Wed, 25 May 2016 14:16:17 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Balmand", "Samuel", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1504.04722", "submitter": "Aleksey Polunchenko", "authors": "Wenyu Du and Aleksey S. Polunchenko and Grigory Sokolov", "title": "On Robustness of the Shiryaev-Roberts Procedure for Quickest\n  Change-Point Detection under Parameter Misspecification in the Post-Change\n  Distribution", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gist of the quickest change-point detection problem is to detect the\npresence of a change in the statistical behavior of a series of sequentially\nmade observations, and do so in an optimal\ndetection-speed-vs.-\"false-positive\"-risk manner. When optimality is understood\neither in the generalized Bayesian sense or as defined in Shiryaev's\nmulti-cyclic setup, the so-called Shiryaev-Roberts (SR) detection procedure is\nknown to be the \"best one can do\", provided, however, that the observations'\npre- and post-change distributions are both fully specified. We consider a more\nrealistic setup, viz. one where the post-change distribution is assumed known\nonly up to a parameter, so that the latter may be \"misspecified\". The question\nof interest is the sensitivity (or robustness) of the otherwise \"best\" SR\nprocedure with respect to a possible misspecification of the post-change\ndistribution parameter. To answer this question, we provide a case study where,\nin a specific Gaussian scenario, we allow the SR procedure to be \"out of tune\"\nin the way of the post-change distribution parameter, and numerically assess\nthe effect of the \"mistuning\" on Shiryaev's (multi-cyclic) Stationary Average\nDetection Delay delivered by the SR procedure. The comprehensive quantitative\nrobustness characterization of the SR procedure obtained in the study can be\nused to develop the respective theory as well as to provide a rational for\npractical design of the SR procedure. The overall qualitative conclusion of the\nstudy is an expected one: the SR procedure is less (more) robust for less\n(more) contrast changes and for lower (higher) levels of the false alarm risk.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2015 14:26:42 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Du", "Wenyu", ""], ["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""]]}, {"id": "1504.04941", "submitter": "Patrick Perry", "authors": "Patrick O. Perry", "title": "Fast Moment-Based Estimation for Hierarchical Models", "comments": "36 pages, 7 figures; includes supplementary material; accepted for\n  publication at JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models allow for heterogeneous behaviours in a population while\nsimultaneously borrowing estimation strength across all subpopulations.\nUnfortunately, existing likelihood-based methods for fitting hierarchical\nmodels have high computational demands, and these demands have limited their\nadoption in large-scale prediction and inference problems. This paper proposes\na moment-based procedure for estimating the parameters of a hierarchical model\nwhich has its roots in a method originally introduced by Cochran in 1937. The\nmethod trades statistical efficiency for computational efficiency. It gives\nconsistent parameter estimates, competitive prediction error performance, and\nsubstantial computational improvements. When applied to a large-scale\nrecommender system application and compared to a standard maximum likelihood\nprocedure, the method delivers competitive prediction performance while\nreducing the sequential computation time from hours to minutes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2015 05:46:13 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 02:18:28 GMT"}, {"version": "v3", "created": "Tue, 15 Dec 2015 18:40:11 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Perry", "Patrick O.", ""]]}, {"id": "1504.05659", "submitter": "ShengLi Tzeng", "authors": "ShengLi Tzeng, Hsin-Cheng Huang", "title": "Multi-Resolution Spatial Random-Effects Models for Irregularly Spaced\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial random-effects model is flexible in modeling spatial covariance\nfunctions, and is computationally efficient for spatial prediction via fixed\nrank kriging. However, the success of this model depends on an appropriate set\nof basis functions. In this research, we propose a class of basis functions\nextracted from thin-plate splines. These functions are ordered in terms of\ntheir degrees of smoothness with a higher-order function corresponding to\nlarger-scale features and a lower-order one corresponding to smaller-scale\ndetails, leading to a parsimonious representation for a nonstationary spatial\ncovariance function. Consequently, only a small to moderate number of functions\nare needed in a spatial random-effects model. The proposed class of basis\nfunctions has several advantages over commonly used ones. First, we do not need\nto concern about the allocation of the basis functions, but simply select the\ntotal number of functions corresponding to a resolution. Second, only a small\nnumber of basis functions is usually required, which facilitates computation.\nThird, estimation variability of model parameters can be considerably reduced,\nand hence more precise covariance function estimates can be obtained. Fourth,\nthe proposed basis functions depend only on the data locations but not the\nmeasurements taken at those locations, and are applicable regardless of whether\nthe data locations are sparse or irregularly spaced. In addition, we derive a\nsimple close-form expression for the maximum likelihood estimates of model\nparameters in the spatial random-effects model. Some numerical examples are\nprovided to demonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 05:44:44 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Tzeng", "ShengLi", ""], ["Huang", "Hsin-Cheng", ""]]}, {"id": "1504.05715", "submitter": "Fran\\c{c}ois Septier", "authors": "Francois Septier, Gareth W. Peters", "title": "Langevin and Hamiltonian based Sequential MCMC for Efficient Bayesian\n  Filtering in High-dimensional Spaces", "comments": null, "journal-ref": null, "doi": "10.1109/JSTSP.2015.2497211", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear non-Gaussian state-space models arise in numerous applications in\nstatistics and signal processing. In this context, one of the most successful\nand popular approximation techniques is the Sequential Monte Carlo (SMC)\nalgorithm, also known as particle filtering. Nevertheless, this method tends to\nbe inefficient when applied to high dimensional problems. In this paper, we\nfocus on another class of sequential inference methods, namely the Sequential\nMarkov Chain Monte Carlo (SMCMC) techniques, which represent a promising\nalternative to SMC methods. After providing a unifying framework for the class\nof SMCMC approaches, we propose novel efficient strategies based on the\nprinciple of Langevin diffusion and Hamiltonian dynamics in order to cope with\nthe increasing number of high-dimensional applications. Simulation results show\nthat the proposed algorithms achieve significantly better performance compared\nto existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 10:13:51 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 17:41:25 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Septier", "Francois", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1504.05723", "submitter": "Saikat Saha", "authors": "Saikat Saha", "title": "Noise Robust Online Inference for Linear Dynamic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.RO cs.SY q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the Bayesian online inference problems for the linear dynamic\nsystems (LDS) under non- Gaussian environment. The noises can naturally be\nnon-Gaussian (skewed and/or heavy tailed) or to accommodate spurious\nobservations, noises can be modeled as heavy tailed. However, at the cost of\nsuch noise robustness, the performance may degrade when such spurious\nobservations are absent. Therefore, any inference engine should not only be\nrobust to noise outlier, but also be adaptive to potentially unknown and time\nvarying noise parameters; yet it should be scalable and easy to implement.\n  To address them, we envisage here a new noise adaptive Rao-Blackwellized\nparticle filter (RBPF), by leveraging a hierarchically Gaussian model as a\nproxy for any non-Gaussian (process or measurement) noise density. This leads\nto a conditionally linear Gaussian model (CLGM), that is tractable. However,\nthis framework requires a valid transition kernel for the intractable state,\ntargeted by the particle filter (PF). This is typically unknown. We outline how\nsuch kernel can be constructed provably, at least for certain classes\nencompassing many commonly occurring non-Gaussian noises, using auxiliary\nlatent variable approach. The efficacy of this RBPF algorithm is demonstrated\nthrough numerical studies.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 10:47:01 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Saha", "Saikat", ""]]}, {"id": "1504.05753", "submitter": "Fran\\c{c}ois Septier", "authors": "Thi Le Thu Nguyen, Francois Septier, Gareth W. Peters, Yves Delignon", "title": "Efficient Sequential Monte-Carlo Samplers for Bayesian Inference", "comments": "arXiv admin note: text overlap with arXiv:1303.3123 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many problems, complex non-Gaussian and/or nonlinear models are required\nto accurately describe a physical system of interest. In such cases, Monte\nCarlo algorithms are remarkably flexible and extremely powerful approaches to\nsolve such inference problems. However, in the presence of a high-dimensional\nand/or multimodal posterior distribution, it is widely documented that standard\nMonte-Carlo techniques could lead to poor performance. In this paper, the study\nis focused on a Sequential Monte-Carlo (SMC) sampler framework, a more robust\nand efficient Monte Carlo algorithm. Although this approach presents many\nadvantages over traditional Monte-Carlo methods, the potential of this emergent\ntechnique is however largely underexploited in signal processing. In this work,\nwe aim at proposing some novel strategies that will improve the efficiency and\nfacilitate practical implementation of the SMC sampler specifically for signal\nprocessing applications. Firstly, we propose an automatic and adaptive strategy\nthat selects the sequence of distributions within the SMC sampler that\nminimizes the asymptotic variance of the estimator of the posterior\nnormalization constant. This is critical for performing model selection in\nmodelling applications in Bayesian signal processing. The second original\ncontribution we present improves the global efficiency of the SMC sampler by\nintroducing a novel correction mechanism that allows the use of the particles\ngenerated through all the iterations of the algorithm (instead of only\nparticles from the last iteration). This is a significant contribution as it\nremoves the need to discard a large portion of the samples obtained, as is\nstandard in standard SMC methods. This will improve estimation performance in\npractical settings where computational budget is important to consider.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 12:24:21 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Nguyen", "Thi Le Thu", ""], ["Septier", "Francois", ""], ["Peters", "Gareth W.", ""], ["Delignon", "Yves", ""]]}, {"id": "1504.05806", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters and Efstathios Panayi and Francois Septier", "title": "SMC-ABC methods for the estimation of stochastic simulation models of\n  the limit order book", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider classes of models that have been recently developed\nfor quantitative finance that involve modelling a highly complex multivariate,\nmulti-attribute stochastic process known as the Limit Order Book (LOB). The LOB\nis the primary data structure recorded each day intra-daily for all assets on\nevery electronic exchange in the world in which trading takes place. As such,\nit represents one of the most important fundamental structures to study from a\nstochastic process perspective if one wishes to characterize features of\nstochastic dynamics for price, volume, liquidity and other important attributes\nfor a traded asset. In this paper we aim to adopt the model structure which\ndevelops a stochastic model framework for the LOB of a given asset and to\nexplain how to perform calibration of this stochastic model to real observed\nLOB data for a range of different assets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2015 13:54:25 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Peters", "Gareth W.", ""], ["Panayi", "Efstathios", ""], ["Septier", "Francois", ""]]}, {"id": "1504.06122", "submitter": "Leo N. Geppert", "authors": "Leo N. Geppert, Katja Ickstadt, Alexander Munteanu, Jens Quedenfeld,\n  Christian Sohler", "title": "Random projections for Bayesian regression", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-015-9608-z", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article deals with random projections applied as a data reduction\ntechnique for Bayesian regression analysis. We show sufficient conditions under\nwhich the entire $d$-dimensional distribution is approximately preserved under\nrandom projections by reducing the number of data points from $n$ to $k\\in\nO(\\operatorname{poly}(d/\\varepsilon))$ in the case $n\\gg d$. Under mild\nassumptions, we prove that evaluating a Gaussian likelihood function based on\nthe projected data instead of the original data yields a\n$(1+O(\\varepsilon))$-approximation in terms of the $\\ell_2$ Wasserstein\ndistance. Our main result shows that the posterior distribution of Bayesian\nlinear regression is approximated up to a small error depending on only an\n$\\varepsilon$-fraction of its defining parameters. This holds when using\narbitrary Gaussian priors or the degenerate case of uniform distributions over\n$\\mathbb{R}^d$ for $\\beta$. Our empirical evaluations involve different\nsimulated settings of Bayesian linear regression. Our experiments underline\nthat the proposed method is able to recover the regression model up to small\nerror while considerably reducing the total running time.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 10:58:34 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2015 16:22:35 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Geppert", "Leo N.", ""], ["Ickstadt", "Katja", ""], ["Munteanu", "Alexander", ""], ["Quedenfeld", "Jens", ""], ["Sohler", "Christian", ""]]}, {"id": "1504.06173", "submitter": "Juho Kokkala", "authors": "Juho Kokkala, Arno Solin and Simo S\\\"arkk\\\"a", "title": "Sigma-Point Filtering and Smoothing Based Parameter Estimation in\n  Nonlinear Dynamic Systems", "comments": "Revised version. 14 pages, 11 figures. Submitted to Journal of\n  Advances in Information Fusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approximate maximum likelihood parameter estimation in nonlinear\nstate-space models. We discuss both direct optimization of the likelihood and\nexpectation--maximization (EM). For EM, we also give closed-form expressions\nfor the maximization step in a class of models that are linear in parameters\nand have additive noise. To obtain approximations to the filtering and\nsmoothing distributions needed in the likelihood-maximization methods, we focus\non using Gaussian filtering and smoothing algorithms that employ sigma-points\nto approximate the required integrals. We discuss different sigma-point schemes\nbased on the third, fifth, seventh, and ninth order unscented transforms and\nthe Gauss--Hermite quadrature rule. We compare the performance of the methods\nin two simulated experiments: a univariate nonlinear growth model as well as\ntracking of a maneuvering target. In the experiments, we also compare against\napproximate likelihood estimates obtained by particle filtering and extended\nKalman filtering based methods. The experiments suggest that the higher-order\nunscented transforms may in some cases provide more accurate estimates\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 13:26:39 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2015 18:28:08 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Kokkala", "Juho", ""], ["Solin", "Arno", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1504.06226", "submitter": "Katar\\'ina Burclov\\'a", "authors": "Katarina Burclova and Andrej Pazman", "title": "Optimal design of experiments via linear programming", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the possibility of extending some results of Pazman and\nPronzato (2014) to a larger set of optimality criteria. Namely, in a linear\nregression model the problem of computing D-, A-, E_k-optimal designs, of\ncombining these optimality criteria, and the \"criterion robust\" problem of\nHarman (2004) are reformulated here as \"infinite-dimensional\" linear\nprogramming problems. Approximate optimum designs can then be computed by a\nmodified cutting-plane method, and this is checked on examples. Finally, the\nexpressions for these criteria are reformulated in terms of the response\nfunction of an even nonlinear model.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 15:39:29 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Burclova", "Katarina", ""], ["Pazman", "Andrej", ""]]}, {"id": "1504.06352", "submitter": "John Tillinghast", "authors": "John Tillinghast", "title": "Fast Functional Integrals with Application to Differential Equations\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is introduced which uses higher-order Laplace approximation to\nevaluate functional integrals much faster than existing methods. An\nimplementation in MATLAB is called SLAM-FIT (Sparse Laplace Approximation\nMethod for Functional Integration on Time) or simply SLAM. In this paper SLAM\nis applied to estimate parameters of mixed models that require functional\nintegration. It is compared with two more general packages which can be used to\ndo functional integration. One is Stan, a recent and very general package for\nintegrating and estimating using hybrid Monte Carlo. The other is INLA, a\nrecent R package which uses Laplace approximations for Gaussian Markov random\nfields. In both cases it is able to get near-identical or equivalent results,\nin significantly less time, even for moderately sized data sets. The\nfundamental speed advantage of the algorithm may be greater than it appears,\nbecause SLAM is running in pure MATLAB while the other two packages use\noptimized compiled code.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2015 22:02:40 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2016 02:24:28 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Tillinghast", "John", ""]]}, {"id": "1504.06870", "submitter": "Arthur White PhD", "authors": "Adrian O'Hagan and Arthur White", "title": "Improved model-based clustering performance using Bayesian\n  initialization averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation-Maximization (EM) algorithm is a commonly used method for\nfinding the maximum likelihood estimates of the parameters in a mixture model\nvia coordinate ascent. A serious pitfall with the algorithm is that in the case\nof multimodal likelihood functions, it can get trapped at a local maximum. This\nproblem often occurs when sub-optimal starting values are used to initialize\nthe algorithm. Bayesian initialization averaging (BIA) is proposed as an\nensemble method to generate high quality starting values for the EM algorithm.\nCompeting sets of trial starting values are combined as a weighted average,\nwhich is then used as the starting position for a full EM run. The method can\nalso be extended to variational Bayes (VB) methods, a class of algorithm\nsimilar to EM that is based on an approximation of the model posterior. The BIA\nmethod is demonstrated on real continuous, categorical and network data sets,\nand the convergent log-likelihoods and associated clustering solutions\npresented. These compare favorably with the output produced using competing\ninitialization methods such as random starts, hierarchical clustering and\ndeterministic annealing, with the highest available maximum likelihood\nestimates obtained in a higher percentage of cases, at reasonable computational\ncost. The implications of the different clustering solutions obtained by local\nmaxima are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2015 19:25:46 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 16:22:54 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2015 14:07:50 GMT"}, {"version": "v4", "created": "Wed, 6 Jul 2016 14:20:56 GMT"}, {"version": "v5", "created": "Thu, 30 Aug 2018 16:14:49 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["O'Hagan", "Adrian", ""], ["White", "Arthur", ""]]}, {"id": "1504.07235", "submitter": "Ping Li", "authors": "Ping Li", "title": "Sign Stable Random Projections for Large-Scale Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the use of \"sign $\\alpha$-stable random projections\" (where\n$0<\\alpha\\leq 2$) for building basic data processing tools in the context of\nlarge-scale machine learning applications (e.g., classification, regression,\nclustering, and near-neighbor search). After the processing by sign stable\nrandom projections, the inner products of the processed data approximate\nvarious types of nonlinear kernels depending on the value of $\\alpha$. Thus,\nthis approach provides an effective strategy for approximating nonlinear\nlearning algorithms essentially at the cost of linear learning. When $\\alpha\n=2$, it is known that the corresponding nonlinear kernel is the arc-cosine\nkernel. When $\\alpha=1$, the procedure approximates the arc-cos-$\\chi^2$ kernel\n(under certain condition). When $\\alpha\\rightarrow0+$, it corresponds to the\nresemblance kernel.\n  From practitioners' perspective, the method of sign $\\alpha$-stable random\nprojections is ready to be tested for large-scale learning applications, where\n$\\alpha$ can be simply viewed as a tuning parameter. What is missing in the\nliterature is an extensive empirical study to show the effectiveness of sign\nstable random projections, especially for $\\alpha\\neq 2$ or 1. The paper\nsupplies such a study on a wide variety of classification datasets. In\nparticular, we compare shoulder-by-shoulder sign stable random projections with\nthe recently proposed \"0-bit consistent weighted sampling (CWS)\" (Li 2015).\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 19:50:40 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1504.07245", "submitter": "Joel Akeret", "authors": "Joel Akeret, Alexandre Refregier, Adam Amara, Sebastian Seehars,\n  Caspar Hasner (ETH Zurich)", "title": "Approximate Bayesian Computation for Forward Modeling in Cosmology", "comments": "19 pages, 5 figures, 1 algorithm. Accepted for publication in JCAP.\n  The code is available at\n  http://www.cosmology.ethz.ch/research/software-lab/abcpmc.html", "journal-ref": null, "doi": "10.1088/1475-7516/2015/08/043", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is often used in cosmology and astrophysics to derive\nconstraints on model parameters from observations. This approach relies on the\nability to compute the likelihood of the data given a choice of model\nparameters. In many practical situations, the likelihood function may however\nbe unavailable or intractable due to non-gaussian errors, non-linear\nmeasurements processes, or complex data formats such as catalogs and maps. In\nthese cases, the simulation of mock data sets can often be made through forward\nmodeling. We discuss how Approximate Bayesian Computation (ABC) can be used in\nthese cases to derive an approximation to the posterior constraints using\nsimulated data sets. This technique relies on the sampling of the parameter\nset, a distance metric to quantify the difference between the observation and\nthe simulations and summary statistics to compress the information in the data.\nWe first review the principles of ABC and discuss its implementation using a\nPopulation Monte-Carlo (PMC) algorithm and the Mahalanobis distance metric. We\ntest the performance of the implementation using a Gaussian toy model. We then\napply the ABC technique to the practical case of the calibration of image\nsimulations for wide field cosmological surveys. We find that the ABC analysis\nis able to provide reliable parameter constraints for this problem and is\ntherefore a promising technique for other applications in cosmology and\nastrophysics. Our implementation of the ABC PMC method is made available via a\npublic code release.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2015 20:01:17 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 15:26:58 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 09:35:50 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Akeret", "Joel", "", "ETH Zurich"], ["Refregier", "Alexandre", "", "ETH Zurich"], ["Amara", "Adam", "", "ETH Zurich"], ["Seehars", "Sebastian", "", "ETH Zurich"], ["Hasner", "Caspar", "", "ETH Zurich"]]}, {"id": "1504.07823", "submitter": "Xiyun Jiao", "authors": "Xiyun Jiao and David A. van Dyk", "title": "A Corrected and More Efficient Suite of MCMC Samplers for the Multinomal\n  Probit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multinomial probit (MNP) model is a useful tool for describing\ndiscrete-choice data and there are a variety of methods for fitting the model.\nAmong them, the algorithms provided by Imai and van Dyk (2005a), based on\nMarginal Data Augmentation, are widely used, because they are efficient in\nterms of convergence and allow the possibly improper prior distribution to be\nspecified directly on identifiable parameters. Burgette and Nordheim (2012)\nmodify a model and algorithm of Imai and van Dyk (2005a) to avoid an arbitrary\nchoice that is often made to establish identifiability. There is an error in\nthe algorithms of Imai and van Dyk (2005a), however, which affects both their\nalgorithms and that of Burgette and Nordheim (2012). This error can alter the\nstationary distribution and the resulting fitted parameters as well as the\nefficiency of these algorithms. We propose a correction and use both a\nsimulation study and a real-data analysis to illustrate the difference between\nthe original and corrected algorithms, both in terms of their estimated\nposterior distributions and their convergence properties. In some cases, the\neffect on the stationary distribution can be substantial.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 12:07:52 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Jiao", "Xiyun", ""], ["van Dyk", "David A.", ""]]}, {"id": "1504.08133", "submitter": "Christopher Yau", "authors": "Michalis K. Titsias and Christopher Yau", "title": "The Hamming Ball Sampler", "comments": "16 pages, 4 figures. No supplementary information included. Corrected\n  Figure 4 and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Hamming Ball Sampler, a novel Markov Chain Monte Carlo\nalgorithm, for efficient inference in statistical models involving\nhigh-dimensional discrete state spaces. The sampling scheme uses an auxiliary\nvariable construction that adaptively truncates the model space allowing\niterative exploration of the full model space in polynomial time. The approach\ngeneralizes conventional Gibbs sampling schemes for discrete spaces and can be\nconsidered as a Big Data-enabled MCMC algorithm that provides an intuitive\nmeans for user-controlled balance between statistical efficiency and\ncomputational tractability. We illustrate the generic utility of our sampling\nalgorithm through application to a range of statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2015 09:21:36 GMT"}, {"version": "v2", "created": "Sun, 3 May 2015 17:37:50 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""]]}]