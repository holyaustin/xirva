[{"id": "1612.00083", "submitter": "Christian Carmona", "authors": "Christian Carmona, Luis Nieto-Barajas and Antonio Canale", "title": "Model based approach for household clustering with mixed scale variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ministry of Social Development in Mexico is in charge of creating and\nassigning social programmes targeting specific needs in the population for the\nimprovement of quality of life. To better target the social programmes, the\nMinistry is aimed to find clusters of households with the same needs based on\ndemographic characteristics as well as poverty conditions of the household.\nAvailable data consists of continuous, ordinal, and nominal variables and the\nobservations are not iid but come from a survey sample based on a complex\ndesign. We propose a Bayesian nonparametric mixture model that jointly models\nthis mixed scale data and accommodates for the different sampling\nprobabilities. The performance of the model is assessed via simulated data. A\nfull analysis of socio-economic conditions in households in the State of Mexico\nis presented.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 00:01:01 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 16:19:22 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Carmona", "Christian", ""], ["Nieto-Barajas", "Luis", ""], ["Canale", "Antonio", ""]]}, {"id": "1612.00259", "submitter": "Maarten Kampert", "authors": "Maarten M. Kampert, Jacqueline J. Meulman, and Jerome H. Friedman", "title": "rCOSA: A Software Package for Clustering Objects on Subsets of\n  Attributes", "comments": "Accepted for publication by the Journal of Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \\texttt{rCOSA} is a software package interfaced to the R language. It\nimplements statistical techniques for clustering objects on subsets of\nattributes in multivariate data. The main output of COSA is a dissimilarity\nmatrix that one can subsequently analyze with a variety of proximity analysis\nmethods. Our package extends the original COSA software (Friedman and Meulman,\n2004) by adding functions for hierarchical clustering methods, least squares\nmultidimensional scaling, partitional clustering, and data visualization. In\nthe many publications that cite the COSA paper by Friedman and Meulman (2004),\nthe COSA program is actually used only a small number of times. This can be\nattributed to the fact that thse original implementation is not very easy to\ninstall and use. Moreover, the available software is out-of-date. Here, we\nintroduce an up-to-date software package and a clear guidance for this advanced\ntechnique. The software package and related links are available for free at:\n\\url{https://github.com/mkampert/rCOSA}\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 14:34:42 GMT"}], "update_date": "2016-12-02", "authors_parsed": [["Kampert", "Maarten M.", ""], ["Meulman", "Jacqueline J.", ""], ["Friedman", "Jerome H.", ""]]}, {"id": "1612.00762", "submitter": "Christopher E. Granade", "authors": "Christopher Granade and Nathan Wiebe", "title": "Structured Filtering", "comments": "22 pages, 14 figures, 3 algorithms, and one video", "journal-ref": null, "doi": "10.1088/1367-2630/aa77cf", "report-no": null, "categories": "quant-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge facing existing sequential Monte-Carlo methods for\nparameter estimation in physics stems from the inability of existing approaches\nto robustly deal with experiments that have different mechanisms that yield the\nresults with equivalent probability. We address this problem here by proposing\na form of particle filtering that clusters the particles that comprise the\nsequential Monte-Carlo approximation to the posterior before applying a\nresampler. Through a new graphical approach to thinking about such models, we\nare able to devise an artificial-intelligence based strategy that automatically\nlearns the shape and number of the clusters in the support of the posterior. We\ndemonstrate the power of our approach by applying it to randomized gap\nestimation and a form of low circuit-depth phase estimation where existing\nmethods from the physics literature either exhibit much worse performance or\neven fail completely.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2016 06:28:29 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Granade", "Christopher", ""], ["Wiebe", "Nathan", ""]]}, {"id": "1612.00807", "submitter": "Renato J Cintra", "authors": "R. J. Cintra, F. M. Bayer, V. A. Coutinho, S. Kulasekera, A.\n  Madanayake", "title": "Energy-efficient 8-point DCT Approximations: Theory and Hardware\n  Architectures", "comments": "21 pages, 7 figures, 5 tables", "journal-ref": "Circuits, Systems, and Signal Processing, November 2016, Volume\n  35, Issue 11, pp 4009-4029", "doi": "10.1007/s00034-015-0233-z", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to its remarkable energy compaction properties, the discrete cosine\ntransform (DCT) is employed in a multitude of compression standards, such as\nJPEG and H.265/HEVC. Several low-complexity integer approximations for the DCT\nhave been proposed for both 1-D and 2-D signal analysis. The increasing demand\nfor low-complexity, energy efficient methods require algorithms with even lower\ncomputational costs. In this paper, new 8-point DCT approximations with very\nlow arithmetic complexity are presented. The new transforms are proposed based\non pruning state-of-the-art DCT approximations. The proposed algorithms were\nassessed in terms of arithmetic complexity, energy retention capability, and\nimage compression performance. In addition, a metric combining performance and\ncomputational complexity measures was proposed. Results showed good performance\nand extremely low computational complexity. Introduced algorithms were mapped\ninto systolic-array digital architectures and physically realized as digital\nprototype circuits using FPGA technology and mapped to 45nm CMOS technology.\nAll hardware-related metrics showed low resource consumption of the proposed\npruned approximate transforms. The best proposed transform according to the\nintroduced metric presents a reduction in power consumption of 21--25%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Dec 2016 19:47:28 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Coutinho", "V. A.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.00951", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Frank Wood", "title": "On the Pitfalls of Nested Monte Carlo", "comments": "Appearing in NIPS Workshop on Advances in Approximate Bayesian\n  Inference 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing interest in estimating expectations outside of the\nclassical inference framework, such as for models expressed as probabilistic\nprograms. Many of these contexts call for some form of nested inference to be\napplied. In this paper, we analyse the behaviour of nested Monte Carlo (NMC)\nschemes, for which classical convergence proofs are insufficient. We give\nconditions under which NMC will converge, establish a rate of convergence, and\nprovide empirical data that suggests that this rate is observable in practice.\nFinally, we prove that general-purpose nested inference schemes are inherently\nbiased. Our results serve to warn of the dangers associated with naive\ncomposition of inference and models.\n", "versions": [{"version": "v1", "created": "Sat, 3 Dec 2016 10:44:50 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1612.01801", "submitter": "Zhibing He", "authors": "Zhibing He, Yichen Qin, Ben-Chang Shia and Yang Li", "title": "Variable Selection with Scalable Bootstrap in Generalized Linear Model\n  for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap is commonly used as a tool for non-parametric statistical inference\nto estimate meaningful parameters in Variable Selection Models. However, for\nmassive dataset that has exponential growth rate, the computation of Bootstrap\nVariable Selection (BootVS) can be a crucial issue. In this paper, we propose\nthe method of Variable Selection with Bag of Little Bootstraps (BLBVS) on\nGeneral Linear Regression and extend it to Generalized Linear Model for\nselecting important parameters and assessing the quality of estimators'\ncomputation efficiency by analyzing results of multiple bootstrap sub-samples.\nThe introduced method best suits large datasets which have parallel and\ndistributed computing structures. To test the performance of BLBVS, we compare\nit with BootVS from different aspects via empirical studies. The results of\nsimulations show our method has excellent performance. A real data analysis,\nRisk Forecast of Credit Cards, is also presented to illustrate the\ncomputational superiority of BLBVS on large scale datasets, and the result\ndemonstrates the usefulness and validity of our proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 13:50:50 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 15:42:41 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["He", "Zhibing", ""], ["Qin", "Yichen", ""], ["Shia", "Ben-Chang", ""], ["Li", "Yang", ""]]}, {"id": "1612.01872", "submitter": "Adam Griffin", "authors": "Adam Griffin, Paul A. Jenkins, Gareth O. Roberts, Simon E.F. Spencer", "title": "Simulation from quasi-stationary distributions on reducible state spaces", "comments": "30 pages, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-stationary distributions (QSDs)arise from stochastic processes that\nexhibit transient equilibrium behaviour on the way to absorption QSDs are often\nmathematically intractable and even drawing samples from them is not\nstraightforward. In this paper the framework of Sequential Monte Carlo samplers\nis utilized to simulate QSDs and several novel resampling techniques are\nproposed to accommodate models with reducible state spaces, with particular\nfocus on preserving particle diversity on discrete spaces. Finally an approach\nis considered to estimate eigenvalues associated with QSDs, such as the decay\nparameter.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 15:40:13 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 10:57:34 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Griffin", "Adam", ""], ["Jenkins", "Paul A.", ""], ["Roberts", "Gareth O.", ""], ["Spencer", "Simon E. F.", ""]]}, {"id": "1612.01907", "submitter": "Jouni Helske", "authors": "Jouni Helske", "title": "KFAS: Exponential Family State Space Models in R", "comments": "39 pages, 7 figures. This is a preprint version of an article to\n  appear in the Journal of Statistical Software. Change to previous version:\n  Added grant number to acknowledgments", "journal-ref": "Journal of Statistical Software, 78(10), 1 - 39 (2017)", "doi": "10.18637/jss.v078.i10", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space modelling is an efficient and flexible method for statistical\ninference of a broad class of time series and other data. This paper describes\nan R package KFAS for state space modelling with the observations from an\nexponential family, namely Gaussian, Poisson, binomial, negative binomial and\ngamma distributions. After introducing the basic theory behind Gaussian and\nnon-Gaussian state space models, an illustrative example of Poisson time series\nforecasting is provided. Finally, a comparison to alternative R packages\nsuitable for non-Gaussian time series modelling is presented.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 17:00:28 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 14:47:30 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Helske", "Jouni", ""]]}, {"id": "1612.01930", "submitter": "Jarno Hartog", "authors": "Jarno Hartog, Harry van Zanten", "title": "Nonparametric Bayesian label prediction on a graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An implementation of a nonparametric Bayesian approach to solving binary\nclassification problems on graphs is described. A hierarchical Bayesian\napproach with a randomly scaled Gaussian prior is considered. The prior uses\nthe graph Laplacian to take into account the underlying geometry of the graph.\nA method based on a theoretically optimal prior and a more flexible variant\nusing partial conjugacy are proposed. Two simulated data examples and two\nexamples using real data are used in order to illustrate the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Dec 2016 18:04:09 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 12:16:29 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Hartog", "Jarno", ""], ["van Zanten", "Harry", ""]]}, {"id": "1612.02195", "submitter": "Jerzy Rydlewski", "authors": "Daniel Kosiorowski and Dominik Mielczarek and Jerzy P. Rydlewski and\n  Ma{\\l}gorzata Snarska", "title": "Generalized Exponential smoothing in prediction of hierarchical time\n  series", "comments": null, "journal-ref": "STATISTICS IN TRANSITION new series, June 2018 Vol. 19, No. 2, pp.\n  331-350", "doi": "10.21307/stattrans-2018-019", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shang and Hyndman (2017) proposed a grouped functional time series\nforecasting approach as a combination of individual forecasts obtained using\ngeneralized least squares method. We modify their methodology using generalized\nexponential smoothing technique for the most disaggregated functional time\nseries in order to obtain more robust predictor. We discuss some properties of\nour proposals basing on results obtained via simulation studies and analysis of\nreal data related to a prediction of a demand for electricity in Australia in\n2016.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 11:06:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 11:54:08 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 16:12:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Kosiorowski", "Daniel", ""], ["Mielczarek", "Dominik", ""], ["Rydlewski", "Jerzy P.", ""], ["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1612.02357", "submitter": "Gonzalo  Garc\\'ia-Donato", "authors": "Anabel Forte and Gonzalo Garcia-Donato and Mark Steel", "title": "Methods and Tools for Bayesian Variable Selection and Model Averaging in\n  Univariate Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we briefly review the main methodological aspects concerned\nwith the application of the Bayesian approach to model choice and model\naveraging in the context of variable selection in regression models. This\nincludes prior elicitation, summaries of the posterior distribution and\ncomputational strategies. We then examine and compare various publicly\navailable {\\tt R}-packages for its practical implementation summarizing and\nexplaining the differences between packages and giving recommendations for\napplied users. We find that all packages reviewed lead to very similar results,\nbut there are potentially important differences in flexibility and efficiency\nof the packages.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 18:19:31 GMT"}], "update_date": "2016-12-08", "authors_parsed": [["Forte", "Anabel", ""], ["Garcia-Donato", "Gonzalo", ""], ["Steel", "Mark", ""]]}, {"id": "1612.02358", "submitter": "Benjamin Crestel", "authors": "Benjamin Crestel and Alen Alexanderian and Georg Stadler and Omar\n  Ghattas", "title": "A-optimal encoding weights for nonlinear inverse problems, with\n  applications to the Helmholtz inverse problem", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aa6d8e", "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The computational cost of solving an inverse problem governed by PDEs, using\nmultiple experiments, increases linearly with the number of experiments. A\nrecently proposed method to decrease this cost uses only a small number of\nrandom linear combinations of all experiments for solving the inverse problem.\nThis approach applies to inverse problems where the PDE solution depends\nlinearly on the right-hand side function that models the experiment. As this\nmethod is stochastic in essence, the quality of the obtained reconstructions\ncan vary, in particular when only a small number of combinations are used. We\ndevelop a Bayesian formulation for the definition and computation of encoding\nweights that lead to a parameter reconstruction with the least uncertainty. We\ncall these weights A-optimal encoding weights. Our framework applies to inverse\nproblems where the governing PDE is nonlinear with respect to the inversion\nparameter field. We formulate the problem in infinite dimensions and follow the\noptimize-then-discretize approach, devoting special attention to the\ndiscretization and the choice of numerical methods in order to achieve a\ncomputational cost that is independent of the parameter discretization. We\nelaborate our method for a Helmholtz inverse problem, and derive the\nadjoint-based expressions for the gradient of the objective function of the\noptimization problem for finding the A-optimal encoding weights. The proposed\nmethod is potentially attractive for real-time monitoring applications, where\none can invest the effort to compute optimal weights offline, to later solve an\ninverse problem repeatedly, over time, at a fraction of the initial cost.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2016 18:23:21 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 22:53:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Crestel", "Benjamin", ""], ["Alexanderian", "Alen", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1612.03054", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa, Sonja Petrovi\\'c and Denis Baji\\'c", "title": "DERGMs: Degeneracy-restricted exponential random graph models", "comments": "Version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models, or ERGMs, are a flexible class of models for\nnetworks. Recent work highlights difficulties related to the models' ill\nbehavior, dubbed `degeneracy', such as most of the probability mass being\nconcentrated on a very small subset of the parameter space. This behavior\nlimits both the applicability of an ERGM as a model for real data and parameter\nestimation via the usual MCMC algorithms.\n  To address this problem, we propose a new exponential family of models for\nrandom graphs that build on the standard ERGM framework. We resolve the\ndegenerate model behavior by an interpretable support restriction. Namely, we\nintroduce a new parameter based on the graph-theoretic notion of degeneracy, a\nmeasure of sparsity whose value is low in real-worlds networks. We prove this\nsupport restriction does not eliminate too many graphs from the support of an\nERGM, and we also prove that degeneracy of a model is captured precisely by\nstability of its sufficient statistics. We show examples of ERGMs that are\ndegenerate whose counterpart DERGMs are not, both theoretically and by\nsimulations, and we test our model class on a set of real world networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 15:21:46 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 15:45:19 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 10:00:12 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Karwa", "Vishesh", ""], ["Petrovi\u0107", "Sonja", ""], ["Baji\u0107", "Denis", ""]]}, {"id": "1612.03233", "submitter": "Amir Sepehri", "authors": "Amir Sepehri", "title": "New Tests of Uniformity on the Compact Classical Groups as Diagnostics\n  for Weak-star Mixing of Markov Chains", "comments": "Accepted for publication in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA math.RT stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces two new families of non-parametric tests of\ngoodness-of-fit on the compact classical groups. One of them is a family of\ntests for the eigenvalue distribution induced by the uniform distribution,\nwhich is consistent against all fixed alternatives. The other is a family of\ntests for the uniform distribution on the entire group, which is again\nconsistent against all fixed alternatives. We find the asymptotic distribution\nunder the null and general alternatives. The tests are proved to be\nasymptotically admissible. Local power is derived and the global properties of\nthe power function against local alternatives are explored.\n  The new tests are validated on two random walks for which the mixing-time is\nstudied in the literature. The new tests, and several others, are applied to\nthe Markov chain sampler proposed by \\cite{jones2011randomized}, providing\nstrong evidence supporting the claim that the sampler mixes quickly.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 01:07:20 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 07:50:15 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Sepehri", "Amir", ""]]}, {"id": "1612.03301", "submitter": "Qi Lei", "authors": "Rashish Tandon, Qi Lei, Alexandros G. Dimakis and Nikos Karampatziakis", "title": "Gradient Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel coding theoretic framework for mitigating stragglers in\ndistributed learning. We show how carefully replicating data blocks and coding\nacross gradients can provide tolerance to failures and stragglers for\nSynchronous Gradient Descent. We implement our schemes in python (using MPI) to\nrun on Amazon EC2, and show how we compare against baseline approaches in\nrunning time and generalization error.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 14:25:00 GMT"}, {"version": "v2", "created": "Wed, 8 Mar 2017 01:00:33 GMT"}], "update_date": "2017-03-09", "authors_parsed": [["Tandon", "Rashish", ""], ["Lei", "Qi", ""], ["Dimakis", "Alexandros G.", ""], ["Karampatziakis", "Nikos", ""]]}, {"id": "1612.03319", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Sumeetpal Singh and Anthony Lee", "title": "Anytime Monte Carlo", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo algorithms simulate some prescribed number of samples, taking\nsome random real time to complete the computations necessary. This work\nconsiders the converse: to impose a real-time budget on the computation, which\nresults in the number of samples simulated being random. To complicate matters,\nthe real time taken for each simulation may depend on the sample produced, so\nthat the samples themselves are not independent of their number, and a length\nbias with respect to compute time is apparent. This is especially problematic\nwhen a Markov chain Monte Carlo (MCMC) algorithm is used and the final state of\nthe Markov chain -- rather than an average over all states -- is required,\nwhich is the case in parallel tempering implementations of MCMC. The length\nbias does not diminish with the compute budget in this case. It also occurs in\nsequential Monte Carlo (SMC) algorithms, which is the focus of this paper. We\npropose an anytime framework to address the concern, using a continuous-time\nMarkov jump process to study the progress of the computation in real time. We\nfirst show that for any MCMC algorithm, the length bias of the final state's\ndistribution due to the imposed real-time computing budget can be eliminated by\nusing a multiple chain construction. The utility of this construction is then\ndemonstrated on a large-scale SMC^2 implementation, using four billion\nparticles distributed across a cluster of 128 graphics processing units on the\nAmazon EC2 service. The anytime framework imposes a real-time budget on the\nMCMC move steps within the SMC$^{2}$ algorithm, ensuring that all processors\nare simultaneously ready for the resampling step, demonstrably reducing\nidleness to due waiting times and providing substantial control over the total\ncompute budget.\n", "versions": [{"version": "v1", "created": "Sat, 10 Dec 2016 16:36:07 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 14:34:35 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:27:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Singh", "Sumeetpal", ""], ["Lee", "Anthony", ""]]}, {"id": "1612.03461", "submitter": "Renato J Cintra", "authors": "V. A. Coutinho, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake", "title": "Low-complexity Pruned 8-point DCT Approximations for Image Encoding", "comments": "13 pages, 6 figures, 3 tables", "journal-ref": null, "doi": "10.1109/CONIELECOMP.2015.7086923", "report-no": null, "categories": "cs.MM cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two multiplierless pruned 8-point discrete cosine transform (DCT)\napproximation are presented. Both transforms present lower arithmetic\ncomplexity than state-of-the-art methods. The performance of such new methods\nwas assessed in the image compression context. A JPEG-like simulation was\nperformed, demonstrating the adequateness and competitiveness of the introduced\nmethods. Digital VLSI implementation in CMOS technology was also considered.\nBoth presented methods were realized in Berkeley Emulation Engine (BEE3).\n", "versions": [{"version": "v1", "created": "Sun, 11 Dec 2016 19:41:44 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Coutinho", "V. A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1612.03930", "submitter": "Kofi Adragni P", "authors": "Sean Martin, Andrew M. Raim, Wen Huang, Kofi P. Adragni", "title": "ManifoldOptim: An R Interface to the ROPTLIB Library for Riemannian\n  Manifold Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Manifold optimization appears in a wide variety of computational problems in\nthe applied sciences. In recent statistical methodologies such as sufficient\ndimension reduction and regression envelopes, estimation relies on the\noptimization of likelihood functions over spaces of matrices such as the\nStiefel or Grassmann manifolds. Recently, Huang, Absil, Gallivan, and Hand\n(2016) have introduced the library ROPTLIB, which provides a framework and\nstate of the art algorithms to optimize real-valued objective functions over\ncommonly used matrix-valued Riemannian manifolds. This article presents\nManifoldOptim, an R package that wraps the C++ library ROPTLIB. ManifoldOptim\nenables users to access functionality in ROPTLIB through R so that optimization\nproblems can easily be constructed, solved, and integrated into larger R codes.\nComputationally intensive problems can be programmed with Rcpp and\nRcppArmadillo, and otherwise accessed through R. We illustrate the practical\nuse of ManifoldOptim through several motivating examples involving dimension\nreduction and envelope methods in regression.\n", "versions": [{"version": "v1", "created": "Mon, 12 Dec 2016 21:20:22 GMT"}, {"version": "v2", "created": "Sun, 25 Dec 2016 02:19:07 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Martin", "Sean", ""], ["Raim", "Andrew M.", ""], ["Huang", "Wen", ""], ["Adragni", "Kofi P.", ""]]}, {"id": "1612.04074", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas", "title": "Spatio-temporal data mining in ecological and veterinary epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the spread of any disease is a highly complex and\ninterdisciplinary exercise as biological, social, geographic, economic, and\nmedical factors may shape the way a disease moves through a population and\noptions for its eventual control or eradication. Disease spread poses a serious\nthreat in animal and plant health and has implications for ecosystem\nfunctioning and species extinctions as well as implications in society through\nfood security and potential disease spread in humans. Space-time epidemiology\nis based on the concept that various characteristics of the pathogenic agents\nand the environment interact in order to alter the probability of disease\noccurrence and form temporal or spatial patterns. Epidemiology aims to identify\nthese patterns and factors, to assess the relevant uncertainty sources, and to\ndescribe disease in the population. Thus disease spread at the population level\ndiffers from the approach traditionally taken by veterinary practitioners that\nare principally concerned with the health status of the individual. Patterns of\ndisease occurrence provide insights into which factors may be affecting the\nhealth of the population, through investigating which individuals are affected,\nwhere are these individuals located and when did they become infected. With the\nrapid development of smart sensors, social networks, as well as digital maps\nand remotely-sensed imagery spatio-temporal data are more ubiquitous and richer\nthan ever before. The availability of such large datasets (Big data) poses\ngreat challenges in data analysis. In addition, increased availability of\ncomputing power facilitates the use of computationally-intensive methods for\nthe analysis of such data. Thus new methods as well as case studies are needed\nto understand veterinary and ecological epidemiology. A special issue aimed to\naddress this topic.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 09:42:39 GMT"}], "update_date": "2016-12-14", "authors_parsed": [["Moustakas", "Aristides", ""]]}, {"id": "1612.04093", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Modified Cholesky Riemann Manifold Hamiltonian Monte Carlo: Exploiting\n  Sparsity for Fast Sampling of High-dimensional Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Riemann manifold Hamiltonian Monte Carlo (RMHMC) has the potential to produce\nhigh-quality Markov chain Monte Carlo-output even for very challenging target\ndistributions. To this end, a symmetric positive definite scaling matrix for\nRMHMC, which derives, via a modified Cholesky factorization, from the\npotentially indefinite negative Hessian of the target log-density is proposed.\nThe methodology is able to exploit the sparsity of the Hessian, stemming from\nconditional independence modeling assumptions, and thus admit fast\nimplementation of RMHMC even for high-dimensional target distributions.\nMoreover, the methodology can exploit log-concave conditional target densities,\noften encountered in Bayesian hierarchical models, for faster sampling and more\nstraight forward tuning. The proposed methodology is compared to alternatives\nfor some challenging targets, and is illustrated by applying a state space\nmodel to real data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 10:45:06 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 07:44:52 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "1612.04101", "submitter": "David Bolin", "authors": "David Bolin and Finn Lindgren", "title": "Calculating probabilistic excursion sets and related quantities using\n  excursions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R software package excursions contains methods for calculating\nprobabilistic excursion sets, contour credible regions, and simultaneous\nconfidence bands for latent Gaussian stochastic processes and fields. It also\ncontains methods for uncertainty quantification of contour maps and computation\nof Gaussian integrals. This article describes the theoretical and computational\nmethods used in the package. The main functions of the package are introduced\nand two examples illustrate how the package can be used.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 11:20:58 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 10:16:15 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Bolin", "David", ""], ["Lindgren", "Finn", ""]]}, {"id": "1612.04271", "submitter": "Nicholas Syring", "authors": "Nicholas Syring, Meng Li", "title": "BayesBD: An R Package for Bayesian Inference on Image Boundaries", "comments": "14 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the BayesBD package providing Bayesian inference for boundaries of\nnoisy images. The BayesBD package implements flexible Gaussian process priors\nindexed by the circle to recover the boundary in a binary or Gaussian noised\nimage, with the benefits of guaranteed geometric restrictions on the estimated\nboundary, (nearly) minimax optimal and smoothness adaptive convergence rates,\nand convenient joint inferences under certain assumptions. The core sampling\ntasks for our model have linear complexity, and our implementation in c++ using\npackages Rcpp and RcppArmadillo is computationally efficient. Users can access\nthe full functionality of the package in both Rgui and the corresponding shiny\napplication. Additionally, the package includes numerous utility functions to\naid users in data preparation and analysis of results. We compare BayesBD with\nselected existing packages using both simulations and real data applications,\nand demonstrate the excellent performance and flexibility of BayesBD even when\nthe observation contains complicated structural information that may violate\nits assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 16:38:37 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 18:50:51 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Syring", "Nicholas", ""], ["Li", "Meng", ""]]}, {"id": "1612.05053", "submitter": "Guillaume Dehaene", "authors": "Guillaume P. Dehaene", "title": "Expectation Propagation performs a smoothed gradient descent", "comments": "This article was submitted and accepted to the Advances in\n  Approximate Bayesian Inference NIPS 2016 workshop\n  (www.approximateinference.org)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference is a popular method to build learning algorithms but it is\nhampered by the fact that its key object, the posterior probability\ndistribution, is often uncomputable. Expectation Propagation (EP) (Minka\n(2001)) is a popular algorithm that solves this issue by computing a parametric\napproximation (e.g: Gaussian) to the density of the posterior. However, while\nit is known empirically to quickly compute fine approximations, EP is extremely\npoorly understood which prevents it from being adopted by a larger fraction of\nthe community.\n  The object of the present article is to shed intuitive light on EP, by\nrelating it to other better understood methods. More precisely, we link it to\nusing gradient descent to compute the Laplace approximation of a target\nprobability distribution. We show that EP is exactly equivalent to performing\ngradient descent on a smoothed energy landscape: i.e: the original energy\nlandscape convoluted with some smoothing kernel. This also relates EP to\nalgorithms that compute the Gaussian approximation which minimizes the reverse\nKL divergence to the target distribution, a link that has been conjectured\nbefore but has not been proved rigorously yet. These results can help\npractitioners to get a better feel for how EP works, as well as lead to other\nnew results on this important method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 13:16:46 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Dehaene", "Guillaume P.", ""]]}, {"id": "1612.05501", "submitter": "Matthew Friedlander D", "authors": "Matthew Friedlander", "title": "The Bayesian analysis of contingency table data using the bayesloglin R\n  package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For log-linear analysis, the hyper Dirichlet conjugate prior is available to\nwork in the Bayesian paradigm. With this prior, the MC3 algorithm allows for\nexploration of the space of models to try to find those with the highest\nposterior probability. Once top models have been identified, a block Gibbs\nsampler can be constructed to sample from the posterior distribution and to\nestimate parameters of interest. Our aim in this paper, is to introduce the\nbayesloglin R package \\citep{R} which contains functions to carry out these\ntasks.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 15:10:12 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Friedlander", "Matthew", ""]]}, {"id": "1612.05614", "submitter": "Jason Xu", "authors": "Jason Xu, Eric C. Chi, Meng Yang, Kenneth Lange", "title": "An MM Algorithm for Split Feasibility Problems", "comments": "31 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical multi-set split feasibility problem seeks a point in the\nintersection of finitely many closed convex domain constraints, whose image\nunder a linear mapping also lies in the intersection of finitely many closed\nconvex range constraints. Split feasibility generalizes important inverse\nproblems including convex feasibility, linear complementarity, and regression\nwith constraint sets. When a feasible point does not exist, solution methods\nthat proceed by minimizing a proximity function can be used to obtain optimal\napproximate solutions to the problem. We present an extension of the proximity\nfunction approach that generalizes the linear split feasibility problem to\nallow for non-linear mappings. Our algorithm is based on the principle of\nmajorization-minimization, is amenable to quasi-Newton acceleration, and comes\ncomplete with convergence guarantees under mild assumptions. Furthermore, we\nshow that the Euclidean norm appearing in the proximity function of the\nnon-linear split feasibility problem can be replaced by arbitrary Bregman\ndivergences. We explore several examples illustrating the merits of non-linear\nformulations over the linear case, with a focus on optimization for\nintensity-modulated radiation therapy.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2016 20:15:09 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 00:26:34 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Xu", "Jason", ""], ["Chi", "Eric C.", ""], ["Yang", "Meng", ""], ["Lange", "Kenneth", ""]]}, {"id": "1612.06468", "submitter": "Richard Everitt", "authors": "Richard G Everitt and Richard Culliford and Felipe Medina-Aguayo and\n  Daniel J Wilson", "title": "Sequential Monte Carlo with transformations", "comments": null, "journal-ref": "Statistics and Computing, 2020", "doi": "10.1007/s11222-019-09903-y", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces methodology for performing Bayesian inference\nsequentially on a sequence of posteriors on spaces of different dimensions. We\nshow how this may be achieved through the use of sequential Monte Carlo (SMC)\nsamplers (Del Moral et al., 2006, 2007), making use of the full flexibility of\nthis framework in order that the method is computationally efficient. In\nparticular, we introduce the innovation of using deterministic transformations\nto move particles effectively between target distributions with different\ndimensions. This approach, combined with adaptive methods, yields an extremely\nflexible and general algorithm for Bayesian model comparison that is suitable\nfor use in applications where the acceptance rate in reversible jump Markov\nchain Monte Carlo (RJMCMC) is low. We demonstrate this approach on the\nwell-studied problem of model comparison for mixture models, and for the novel\napplication of inferring coalescent trees sequentially, as data arrives.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 00:57:02 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 15:09:36 GMT"}, {"version": "v3", "created": "Wed, 11 Sep 2019 17:47:01 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G", ""], ["Culliford", "Richard", ""], ["Medina-Aguayo", "Felipe", ""], ["Wilson", "Daniel J", ""]]}, {"id": "1612.06492", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Geoffrey J. McLachlan", "title": "Chunked-and-Averaged Estimators for Vector Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A divide-and-conquer method for parameter estimation is the\nchunked-and-averaged (CA) estimator. CA estimators have been studied for\nunivariate parameters under independent and identically distributed (IID)\nsampling. We study the CA estimators of vector parameters and under non-IID\nsampling.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 02:54:51 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 05:26:25 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 13:52:49 GMT"}, {"version": "v4", "created": "Tue, 29 Aug 2017 14:02:27 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1612.06518", "submitter": "Daniel Fischer", "authors": "Daniel Fischer, Alain Berro, Klaus Nordhausen, Anne Ruiz-Gazen", "title": "REPPlab: An R package for detecting clusters and outliers using\n  exploratory projection pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R-package REPPlab is designed to explore multivariate data sets using\none-dimensional unsupervised projection pursuit. It is useful in practice as a\npreprocessing step to find clusters or as an outlier detection tool for\nmultivariate numerical data. Except from the package tourr that implements\nsmooth sequences of projection matrices and rggobi that provides an interface\nto a dynamic graphics package called GGobi, there is no implementation of\nexploratory projection pursuit tools available in R especially in the context\nof outlier detection. REPPlab is an R interface for the Java program EPPlab\nthat implements four projection indices and three biologically inspired\noptimization algorithms. The implemented indices are either adapted to cluster\nor to outlier detection and the optimization algorithms have at most one\nparameter to tune. Following the original software EPPlab, the exploration\nstrategy in REPPlab is divided into two steps. Many potentially interesting\nprojections are calculated at the first step and examined at the second step.\nFor this second step, different tools for plotting and combining the results\nare proposed with specific tools for outlier detection. Compared to EPPlab,\nsome of these tools are new and their performance is illustrated through some\nsimulations and using some real data sets in a clustering context. The\nfunctionalities of the package are also illustrated for outlier detection on a\nnew data set that is provided with the package.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2016 06:18:02 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Fischer", "Daniel", ""], ["Berro", "Alain", ""], ["Nordhausen", "Klaus", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1612.07002", "submitter": "JInglai Li", "authors": "Xinjuan Chen, Jinglai Li", "title": "A subset multicanonical Monte Carlo method for simulating rare failure\n  events", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.04.051", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating failure probabilities of engineering systems is an important\nproblem in many engineering fields. In this work we consider such problems\nwhere the failure probability is extremely small (e.g $\\leq10^{-10}$). In this\ncase, standard Monte Carlo methods are not feasible due to the extraordinarily\nlarge number of samples required. To address these problems, we propose an\nalgorithm that combines the main ideas of two very powerful failure probability\nestimation approaches: the subset simulation (SS) and the multicanonical Monte\nCarlo (MMC) methods. Unlike the standard MMC which samples in the entire domain\nof the input parameter in each iteration, the proposed subset MMC algorithm\nadaptively performs MMC simulations in a subset of the state space and thus\nimproves the sampling efficiency. With numerical examples we demonstrate that\nthe proposed method is significantly more efficient than both of the SS and the\nMMC methods. Moreover, the proposed algorithm can reconstruct the complete\ndistribution function of the parameter of interest and thus can provide more\ninformation than just the failure probabilities of the systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 07:59:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Chen", "Xinjuan", ""], ["Li", "Jinglai", ""]]}, {"id": "1612.07010", "submitter": "Kari Krizak Halle", "authors": "Kari Krizak Halle and Mette Langaas", "title": "Permutation in genetic association studies with covariates: controlling\n  the familywise error rate with score tests in generalized linear models", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genome-wide association (GWA) studies the goal is to detect associations\nbetween genetic markers and a given phenotype. The number of genetic markers\ncan be large and effective methods for control of the overall error rate is a\ncentral topic when analyzing GWA data. The Bonferroni method is known to be\nconservative when the tests are dependent. Permutation methods give exact\ncontrol of the overall error rate when the assumption of exchangeability is\nsatisfied, but are computationally intensive for large datasets. For regression\nmodels the exchangeability assumption is in general not satisfied and there is\nno standard solution on how to do permutation testing, except some approximate\nmethods. In this paper we will discuss permutation methods for control of the\nfamilywise error rate in genetic association studies and present an approximate\nsolution. These methods will be compared using simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2016 08:32:34 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 07:32:39 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Halle", "Kari Krizak", ""], ["Langaas", "Mette", ""]]}, {"id": "1612.07471", "submitter": "Alain Durmus", "authors": "Alain Durmus, Eric Moulines, Marcelo Pereyra", "title": "Efficient Bayesian computation by proximal Markov chain Monte Carlo:\n  when Langevin meets Moreau", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern imaging methods rely strongly on Bayesian inference techniques to\nsolve challenging imaging problems. Currently, the predominant Bayesian\ncomputation approach is convex optimisation, which scales very efficiently to\nhigh dimensional image models and delivers accurate point estimation results.\nHowever, in order to perform more complex analyses, for example image\nuncertainty quantification or model selection, it is necessary to use more\ncomputationally intensive Bayesian computation techniques such as Markov chain\nMonte Carlo methods. This paper presents a new and highly efficient Markov\nchain Monte Carlo methodology to perform Bayesian computation for high\ndimensional models that are log-concave and non-smooth, a class of models that\nis central in imaging sciences. The methodology is based on a regularised\nunadjusted Langevin algorithm that exploits tools from convex analysis, namely\nMoreau-Yoshida envelopes and proximal operators, to construct Markov chains\nwith favourable convergence properties. In addition to scaling efficiently to\nhigh dimensions, the method is straightforward to apply to models that are\ncurrently solved by using proximal optimisation algorithms. We provide a\ndetailed theoretical analysis of the proposed methodology, including asymptotic\nand non-asymptotic convergence results with easily verifiable conditions, and\nexplicit bounds on the convergence rates. The proposed methodology is\ndemonstrated with four experiments related to image deconvolution and\ntomographic reconstruction with total-variation and $\\ell_1$ priors, where we\nconduct a range of challenging Bayesian analyses related to uncertainty\nquantification, hypothesis testing, and model selection in the absence of\nground truth.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 07:31:59 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "1612.07498", "submitter": "Heidi Seibold", "authors": "Heidi Seibold, Torsten Hothorn, Achim Zeileis", "title": "Generalised Linear Model Trees with Global Additive Effects", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-018-0342-1", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based trees are used to find subgroups in data which differ with\nrespect to model parameters. In some applications it is natural to keep some\nparameters fixed globally for all observations while asking if and how other\nparameters vary across subgroups. Existing implementations of model-based trees\ncan only deal with the scenario where all parameters depend on the subgroups.\nWe propose partially additive linear model trees (PALM trees) as an extension\nof (generalised) linear model trees (LM and GLM trees, respectively), in which\nthe model parameters are specified a priori to be estimated either globally\nfrom all observations or locally from the observations within the subgroups\ndetermined by the tree. Simulations show that the method has high power for\ndetecting subgroups in the presence of global effects and reliably recovers the\ntrue parameters. Furthermore, treatment-subgroup differences are detected in an\nempirical application of the method to data from a mathematics exam: the PALM\ntree is able to detect a small subgroup of students that had a disadvantage in\nan exam with two versions while adjusting for overall ability effects.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 09:19:02 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 08:24:03 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seibold", "Heidi", ""], ["Hothorn", "Torsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1612.08141", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Luca Tardella", "title": "PLMIX: An R package for modeling and clustering partially ranked data", "comments": "33 pages, 1 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranking data represent a peculiar form of multivariate ordinal data taking\nvalues in the set of permutations. Despite the numerous methodological\ncontributions to increase the flexibility of ranked data modeling, the\napplication of more sophisticated models is limited by the related\ncomputational issues. The PLMIX package offers a comprehensive framework aimed\nat endowing the R statistical environment with some recent methodological\nadvances in modeling and clustering partially ranked data. The usefulness of\nthe novel PLMIX package can be motivated from several perspectives: (i) it\ncontributes to fill the gap concerning Bayesian estimation of ranking models in\nR, by focusing on the Plackett-Luce model and its extension within the finite\nmixture approach as the generative sampling distribution; (ii) it addresses\ncomputational complexity by combining the flexibility of R routines and the\nspeed of compiled C++ code, with possible parallel execution; (iii) it covers\nthe fundamental phases of ranking data analysis allowing for a more careful and\ncritical application of ranking models in real experiments; (iv) it provides\neffective tools for clustering heterogeneous partially ranked data. The\nfunctionality of the novel package is illustrated with several applications to\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 04:52:52 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 15:37:44 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 23:34:49 GMT"}, {"version": "v4", "created": "Wed, 7 Mar 2018 21:09:00 GMT"}, {"version": "v5", "created": "Mon, 12 Mar 2018 08:15:20 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "1612.08224", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Shiwei Lan, Alexander Vandenberg-Rodes, Babak\n  Shahbaba", "title": "Geodesic Lagrangian Monte Carlo over the space of positive definite\n  matrices: with application to Bayesian spectral density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the application of Hamiltonian Monte Carlo to allow for sampling\nfrom probability distributions defined over symmetric or Hermitian positive\ndefinite matrices. To do so, we exploit the Riemannian structure induced by\nCartan's century-old canonical metric. The geodesics that correspond to this\nmetric are available in closed-form and---within the context of Lagrangian\nMonte Carlo---provide a principled way to travel around the space of positive\ndefinite matrices. Our method improves Bayesian inference on such matrices by\nallowing for a broad range of priors, so we are not limited to conjugate priors\nonly. In the context of spectral density estimation, we use the (non-conjugate)\ncomplex reference prior as an example modeling option made available by the\nalgorithm. Results based on simulated and real-world multivariate time series\nare presented in this context, and future directions are outlined.\n", "versions": [{"version": "v1", "created": "Sat, 24 Dec 2016 23:35:30 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Holbrook", "Andrew", ""], ["Lan", "Shiwei", ""], ["Vandenberg-Rodes", "Alexander", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1612.08709", "submitter": "Mark Tygert", "authors": "Huamin Li, Yuval Kluger, and Mark Tygert", "title": "Randomized algorithms for distributed computation of principal component\n  analysis and singular value decomposition", "comments": "21 pages, 29 tables, 1 figure, 8 algorithms in pseudocode", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms provide solutions to two ubiquitous problems: (1) the\ndistributed calculation of a principal component analysis or singular value\ndecomposition of a highly rectangular matrix, and (2) the distributed\ncalculation of a low-rank approximation (in the form of a singular value\ndecomposition) to an arbitrary matrix. Carefully honed algorithms yield results\nthat are uniformly superior to those of the stock, deterministic\nimplementations in Spark (the popular platform for distributed computation); in\nparticular, whereas the stock software will without warning return left\nsingular vectors that are far from numerically orthonormal, a significantly\nburnished randomized implementation generates left singular vectors that are\nnumerically orthonormal to nearly the machine precision.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 19:06:13 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 22:06:19 GMT"}, {"version": "v3", "created": "Wed, 31 May 2017 23:04:43 GMT"}, {"version": "v4", "created": "Mon, 1 Jan 2018 20:24:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Li", "Huamin", ""], ["Kluger", "Yuval", ""], ["Tygert", "Mark", ""]]}, {"id": "1612.08974", "submitter": "John Ehrlinger", "authors": "John Ehrlinger", "title": "ggRandomForests: Exploring Random Forest Survival", "comments": "39 pages, 23 figures, 4 tables. Draft working document. R package\n  vignette for ggRandomForests package\n  (https://cran.r-project.org/package=ggRandomForests). arXiv admin note: text\n  overlap with arXiv:1501.07196", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forest (Leo Breiman 2001a) (RF) is a non-parametric statistical method\nrequiring no distributional assumptions on covariate relation to the response.\nRF is a robust, nonlinear technique that optimizes predictive accuracy by\nfitting an ensemble of trees to stabilize model estimates. Random survival\nforests (RSF) (Ishwaran and Kogalur 2007; Ishwaran et al. 2008) are an\nextension of Breimans RF techniques allowing efficient nonparametric analysis\nof time to event data. The randomForestSRC package (Ishwaran and Kogalur 2014)\nis a unified treatment of Breimans random forest for survival, regression and\nclassification problems.\n  Predictive accuracy makes RF an attractive alternative to parametric models,\nthough complexity and interpretability of the forest hinder wider application\nof the method. We introduce the ggRandomForests package, tools for visually\nunderstand random forest models grown in R (R Core Team 2014) with the\nrandomForestSRC package. The ggRandomForests package is structured to extract\nintermediate data objects from randomForestSRC objects and generate figures\nusing the ggplot2 (Wickham 2009) graphics package.\n  This document is structured as a tutorial for building random forest for\nsurvival with the randomForestSRC package and using the ggRandomForests package\nfor investigating how the forest is constructed. We analyse the Primary Biliary\nCirrhosis of the liver data from a clinical trial at the Mayo Clinic (Fleming\nand Harrington 1991). Our aim is to demonstrate the strength of using Random\nForest methods for both prediction and information retrieval, specifically in\ntime to event data settings.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2016 20:17:58 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Ehrlinger", "John", ""]]}, {"id": "1612.09162", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and Thomas B. Sch\\\"on", "title": "High-dimensional Filtering using Nested Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) methods comprise one of the most successful\napproaches to approximate Bayesian filtering. However, SMC without good\nproposal distributions struggle in high dimensions. We propose nested\nsequential Monte Carlo (NSMC), a methodology that generalises the SMC framework\nby requiring only approximate, properly weighted, samples from the SMC proposal\ndistribution, while still resulting in a correct SMC algorithm. This way we can\nexactly approximate the locally optimal proposal, and extend the class of\nmodels for which we can perform efficient inference using SMC. We show improved\naccuracy over other state-of-the-art methods on several spatio-temporal state\nspace models.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 14:58:55 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1612.09357", "submitter": "Sen Na", "authors": "Sen Na, Cho-Jui Hsieh", "title": "Sparse Learning with Semi-Proximal-Based Strictly Contractive\n  Peaceman-Rachford Splitting Method", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimizing sum of two functions under a linear constraint is what we called\nsplitting problem. This convex optimization has wide applications in machine\nlearning problems, such as Lasso, Group Lasso and Sparse logistic regression. A\nrecent paper by Gu et al (2015) developed a Semi-Proximal-Based Strictly\nContractive Peaceman-Rachford Splitting Method (SPB-SPRSM), which is an\nextension of Strictly Contractive Peaceman-Rachford Splitting Method (SPRSM)\nproposed by He et al (2014). By introducing semi-proximal terms and using two\ndifferent relaxation factors, SPB-SPRSM showed a more flexiable applicability\ncomparing to its origin SPRSM and widely-used Alternating Direction Method of\nMultipliers (ADMM) algorithm, although all of them have $O(1/t)$ convergence\nrate. In this paper, we develop a stochastic version of SPB-SPRSM algorithm,\nwhere only a subset of samples (even one sample) are used at each iteration.\nThe resulting algorithm, Stochastic SPB-SPRSM, is more flexiable than\nStochastic ADMM and other ADMM-based algorithms on both simulations and real\ndatasets. Moreover, we prove $O(1/\\sqrt{t})$ convergence rate in ergodic sense,\nwhich is the same with Stochastic ADMM algorithm under the same assumption. But\nas shown in He et al (2014) that SPRSM based algorithms will always converge\nfaster than ADMM in apllication, our proposed algorithm will also preserve this\nadvantage.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 00:38:30 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 05:26:03 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Na", "Sen", ""], ["Hsieh", "Cho-Jui", ""]]}]