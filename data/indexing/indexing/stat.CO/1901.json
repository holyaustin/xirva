[{"id": "1901.00533", "submitter": "Maksym Byshkin", "authors": "Alexander Borisenko, Maksym Byshkin, Alessandro Lomi", "title": "A Simple Algorithm for Scalable Monte Carlo Inference", "comments": "15 pages + supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math-ph math.MP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methods of statistical physics are widely used for modelling complex\nnetworks. Building on the recently proposed Equilibrium Expectation approach,\nwe derive a simple and efficient algorithm for maximum likelihood estimation\n(MLE) of parameters of exponential family distributions - a family of\nstatistical models, that includes Ising model, Markov Random Field and\nExponential Random Graph models. Computational experiments and analysis of\nempirical data demonstrate that the algorithm increases by orders of magnitude\nthe size of network data amenable to Monte Carlo based inference. We report\nresults suggesting that the applicability of the algorithm may readily be\nextended to the analysis of large samples of dependent observations commonly\nfound in biology, sociology, astrophysics, and ecology.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2019 20:59:47 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 07:55:02 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 10:55:25 GMT"}, {"version": "v4", "created": "Tue, 11 Feb 2020 14:02:02 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Borisenko", "Alexander", ""], ["Byshkin", "Maksym", ""], ["Lomi", "Alessandro", ""]]}, {"id": "1901.00578", "submitter": "Zheng Zhang", "authors": "Jiali Luan and Zheng Zhang", "title": "Prediction of multi-dimensional spatial variation data via Bayesian\n  tensor completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a multi-dimensional computational method to predict the\nspatial variation data inside and across multiple dies of a wafer. This\ntechnique is based on tensor computation. A tensor is a high-dimensional\ngeneralization of a matrix or a vector. By exploiting the hidden low-rank\nproperty of a high-dimensional data array, the large amount of unknown\nvariation testing data may be predicted from a few random measurement samples.\nThe tensor rank, which decides the complexity of a tensor representation, is\ndecided by an available variational Bayesian approach. Our approach is\nvalidated by a practical chip testing data set, and it can be easily\ngeneralized to characterize the process variations of multiple wafers. Our\napproach is more efficient than the previous virtual probe techniques in terms\nof memory and computational cost when handling high-dimensional chip testing\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2019 02:25:30 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Luan", "Jiali", ""], ["Zhang", "Zheng", ""]]}, {"id": "1901.01477", "submitter": "Michael Weylandt", "authors": "Michael Weylandt and John Nagorski and Genevera I. Allen", "title": "Dynamic Visualization and Fast Computation for Convex Clustering via\n  Algorithmic Regularization", "comments": "To appear in the Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": "10.1080/10618600.2019.1629943", "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Convex clustering is a promising new approach to the classical problem of\nclustering, combining strong performance in empirical studies with rigorous\ntheoretical foundations. Despite these advantages, convex clustering has not\nbeen widely adopted, due to its computationally intensive nature and its lack\nof compelling visualizations. To address these impediments, we introduce\nAlgorithmic Regularization, an innovative technique for obtaining high-quality\nestimates of regularization paths using an iterative one-step approximation\nscheme. We justify our approach with a novel theoretical result, guaranteeing\nglobal convergence of the approximate path to the exact solution under\neasily-checked non-data-dependent assumptions. The application of algorithmic\nregularization to convex clustering yields the Convex Clustering via\nAlgorithmic Regularization Paths (CARP) algorithm for computing the clustering\nsolution path. On example data sets from genomics and text analysis, CARP\ndelivers over a 100-fold speed-up over existing methods, while attaining a\nfiner approximation grid than standard methods. Furthermore, CARP enables\nimproved visualization of clustering solutions: the fine solution grid returned\nby CARP can be used to construct a convex clustering-based dendrogram, as well\nas forming the basis of a dynamic path-wise visualization based on modern web\ntechnologies. Our methods are implemented in the open-source R package\nclustRviz, available at https://github.com/DataSlingers/clustRviz.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2019 00:22:35 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 22:40:47 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 21:36:39 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 18:32:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Weylandt", "Michael", ""], ["Nagorski", "John", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1901.02061", "submitter": "Zunjing Wang", "authors": "Xiao-Feng Xie and Zunjing Jenipher Wang", "title": "Examining Travel Patterns and Characteristics in a Bikesharing Network\n  and Implications for Data-Driven Decision Supports: Case Study in the\n  Washington DC Area", "comments": "33 pages, 14 figures", "journal-ref": "Journal of Transport Geography, 2018, 71: 84-102", "doi": "10.1016/j.jtrangeo.2018.07.010", "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bikesharing has gradually become one adopted sustainable transportation mode\nrecent years to bring us many social, environmental, economic, and\nhealth-related benefits and rewards. There is increased research toward better\nunderstanding of bikesharing systems (BSS) in urban environments. However, our\ncomprehension remains incomplete on the patterns and characteristics of BSS. In\nthis paper, aiming to help improving sustainability in multimodal\ntransportation through BSS, we perform a systematic data analysis to examine\nunderlying patterns and characteristics of the system dynamics in a bikeshare\nnetwork and to acquire implications of the patterns and characteristics for\ndecision making. As a case study, we use trip history data from the Capital\nBikeshare system in the Washington DC area and some additional data sources.\nThe study covers seven important aspects of bikeshare transportation systems,\nwhich are respectively trip demand and flow, operating activities, use and idle\ntimes, trip purpose, origin-destination flows, mobility, and safety. For these\naspects, by using appropriate statistical methods and geographic techniques, we\ninvestigate travel patterns and characteristics of BSS from data to evaluate\nthe qualitative and quantitative impacts of the inputs from key stakeholders on\nmain measures of effectiveness such as trip costs, mobility, safety, quality of\nservice, and operational efficiency, where key stakeholders include road users,\nsystem operators, and city. We also disclose some new patterns and\ncharacteristics of BSS to advance the knowledge on travel behaviors. Finally,\nwe briefly summarize our findings and discuss the implications of the patterns\nand characteristics for data-driven decision supports from the relations\nbetween BSS and key stakeholders for promoting bikeshare utilization and\ntransforming urban transportation to be more sustainable.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 18:29:26 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Xie", "Xiao-Feng", ""], ["Wang", "Zunjing Jenipher", ""]]}, {"id": "1901.02117", "submitter": "Yajuan Si", "authors": "Yajuan Si and Peigen Zhou", "title": "Bayes-raking: Bayesian Finite Population Inference with Known Margins", "comments": null, "journal-ref": null, "doi": "10.1093/jssam/smaa008", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Raking is widely used in categorical data modeling and survey practice but\nfaced with methodological and computational challenges. We develop a Bayesian\nparadigm for raking by incorporating the marginal constraints as a prior\ndistribution via two main strategies: 1) constructing the solution subspaces\nvia basis functions or projection matrix and 2) modeling soft constraints. The\nproposed Bayes-raking estimation integrates the models for the margins, the\nsample selection and response mechanism, and the outcome, with the capability\nto propagate all sources of uncertainty. Computation is done via Stan, and\ncodes are ready for public use. Simulation studies show that Bayes-raking can\nperform as well as raking with large samples and outperform in terms of\nvalidity and efficiency gains, especially with a sparse contingency table or\ndependent raking factors. We apply the new method to the Longitudinal Study of\nWellbeing study and demonstrate that model-based approaches significantly\nimprove inferential reliability and substantive findings as a unified survey\ninference framework.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2019 01:20:28 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 20:08:09 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 19:46:19 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 19:06:11 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Zhou", "Peigen", ""]]}, {"id": "1901.02733", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie", "title": "Marginal Densities, Factor Graph Duality, and High-Temperature Series\n  Expansions", "comments": "Proc. of the 23rd International Conference on Artificial Intelligence\n  and Statistics (AISTATS) 2020, Palermo, Italy. Extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the marginal densities of a global probability mass function in\na primal normal factor graph and the corresponding marginal densities in the\ndual normal factor graph are related via local mappings. The mapping depends on\nthe Fourier transform of the local factors of the models. Details of the\nmapping, including its fixed points, are derived for the Ising model, and then\nextended to the Potts model. By employing the mapping, we can transform\nsimultaneously all the estimated marginal densities from one domain to the\nother, which is advantageous if estimating the marginals can be carried out\nmore efficiently in the dual domain. An example of particular significance is\nthe ferromagnetic Ising model in a positive external field, for which there is\na rapidly mixing Markov chain (called the subgraphs-world process) to generate\nconfigurations in the dual normal factor graph of the model. Our numerical\nexperiments illustrate that the proposed procedure can provide more accurate\nestimates of marginal densities in various settings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2019 03:48:18 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:38:36 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2020 19:45:24 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Molkaraie", "Mehdi", ""]]}, {"id": "1901.02945", "submitter": "Alan Lenarcic", "authors": "Alan Lenarcic and William Valdar", "title": "Algorithmic Bayesian Group Gibbs Selection", "comments": "Code for Group Bayes Estimator available to public in R package\n  format at https://github.com/lenarcica/BayesSpike.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection, with precedents in George and McCulloch (1993) and\nAbramovich et al. (1998), support credibility measures that relate model\nuncertainty, but computation can be costly when sparse priors are approximate.\nWe design an exact selection engine suitable for Gauss noise, t-distributed\nnoise, and logistic learning, benefiting from data-structures derived from\ncoordinate descent lasso. Gibbs sampler chains are stored in a compressed\nbinary format compatible with Equi-Energy (Kou et al., 2006) tempering. We\nachieve a grouped-effects selection model, similar to the setting for group\nlasso, to determine co-entry of coefficients into the model. We derive a\nfunctional integrand for group inclusion, and introduce a new MCMC switching\nstep to avoid numerical integration. Theorems show this step has exponential\nconvergence to target distribution. We demonstrate a role for group selection\nto inform on genetic decomposition in a diallel experiment, and identify\npotential quantitative trait loci in p > 40K Heterogenous Stock\nhaplotype/phenotype studies.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jan 2019 22:02:46 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 15:18:53 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Lenarcic", "Alan", ""], ["Valdar", "William", ""]]}, {"id": "1901.02976", "submitter": "Art Owen", "authors": "Art B. Owen and Yi Zhou", "title": "The square root rule for adaptive importance sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In adaptive importance sampling, and other contexts, we have $K>1$ unbiased\nand uncorrelated estimates $\\hat\\mu_k$ of a common quantity $\\mu$. The optimal\nunbiased linear combination weights them inversely to their variances but those\nweights are unknown and hard to estimate. A simple deterministic square root\nrule based on a working model that $\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-1/2}$\ngives an unbisaed estimate of $\\mu$ that is nearly optimal under a wide range\nof alternative variance patterns. We show that if\n$\\mathrm{Var}(\\hat\\mu_k)\\propto k^{-y}$ for an unknown rate parameter $y\\in\n[0,1]$ then the square root rule yields the optimal variance rate with a\nconstant that is too large by at most $9/8$ for any $0\\le y\\le 1$ and any\nnumber $K$ of estimates. Numerical work shows that rule is similarly robust to\nsome other patterns with mildly decreasing variance as $k$ increases.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 00:15:29 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 02:15:35 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Owen", "Art B.", ""], ["Zhou", "Yi", ""]]}, {"id": "1901.03283", "submitter": "Mario Teixeira Parente", "authors": "Mario Teixeira Parente, Daniel Bittner, Steven Mattis, Gabriele\n  Chiogna, Barbara Wohlmuth", "title": "Bayesian calibration and sensitivity analysis for a karst aquifer model\n  using active subspaces", "comments": "27 pages, 5 figures, 2 tables; 5 pages supplementary information", "journal-ref": "Water Resources Research 55 (8), 7086-7107, 2019", "doi": "10.1029/2019WR024739", "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we perform a parameter study for a recently developed karst\nhydrological model. The study consists of a high-dimensional Bayesian inverse\nproblem and a global sensitivity analysis. For the first time in karst\nhydrology, we use the active subspace method to find directions in the\nparameter space that dominate the Bayesian update from the prior to the\nposterior distribution in order to effectively reduce the dimension of the\nproblem and for computational efficiency. Additionally, the calculated active\nsubspace can be exploited to construct sensitivity metrics on each of the\nindividual parameters and be used to construct a natural model surrogate. The\nmodel consists of 21 parameters to reproduce the hydrological behavior of\nspring discharge in a karst aquifer located in the Kerschbaum spring recharge\narea at Waidhofen a.d. Ybbs in Austria. The experimental spatial and time\nseries data for the inference process were collected by the water works in\nWaidhofen. We show that this case study has implicit low-dimensionality, and we\nrun an adjusted Markov chain Monte Carlo algorithm in a low-dimensional\nsubspace to construct samples of the posterior distribution. The results are\nvisualized and verified by plots of the posterior's push-forward distribution\ndisplaying the uncertainty in predicting discharge values due to the\nexperimental noise in the data. Finally, a discussion provides hydrological\ninterpretation of these results for the Kerschbaum area.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 17:24:04 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 11:12:49 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 11:18:13 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Parente", "Mario Teixeira", ""], ["Bittner", "Daniel", ""], ["Mattis", "Steven", ""], ["Chiogna", "Gabriele", ""], ["Wohlmuth", "Barbara", ""]]}, {"id": "1901.03311", "submitter": "Bruno Sudret", "authors": "M. Moustapha and B. Sudret", "title": "Surrogate-assisted reliability-based design optimization: a survey and a\n  new general framework", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2019-001", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reliability-based design optimization (RBDO) is an active field of research\nwith an ever increasing number of contributions. Numerous methods have been\nproposed for the solution of RBDO, a complex problem that combines optimization\nand reliability analysis. Classical approaches are based on approximation\nmethods and have been classified in review papers. In this paper, we first\nreview classical approaches based on approximation methods such as FORM, and\nalso more recent methods that rely upon surrogate modelling and Monte Carlo\nsimulation. We then propose a general framework for the solution of RBDO\nproblems that includes three independent blocks, namely adaptive surrogate\nmodelling, reliability analysis and optimization. These blocks are\nnon-intrusive with respect to each other and can be plugged independently in\nthe framework. After a discussion on numerical considerations that require\nattention for the framework to yield robust solutions to various types of\nproblems, the latter is applied to three examples (using two analytical\nfunctions and a finite element model). Kriging and support vector machines\ntogether with their own active learning schemes are considered in the surrogate\nmodel block. In terms of reliability analysis, the proposed framework is\nillustrated using both crude Monte Carlo and subset simulation. Finally, the\ncovariance-matrix adaptation - evolution scheme (CMA-ES), a global search\nalgorithm, or sequential quadratic programming (SQP), a local gradient-based\nmethod, are used in the optimization block. The comparison of the results to\nbenchmark studies show the effectiveness and efficiency of the proposed\nframework.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 18:30:13 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Moustapha", "M.", ""], ["Sudret", "B.", ""]]}, {"id": "1901.03352", "submitter": "Abdullah Makkeh", "authors": "Abdullah Makkeh, Daniel Chicharro, Dirk Oliver Theis, Raul Vicente", "title": "MAXENT3D_PID: An Estimator for the Maximum-entropy Trivariate Partial\n  Information Decomposition", "comments": null, "journal-ref": "Entropy 21 (9), 862, 2019", "doi": "10.3390/e21090862", "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chicharro (2017) introduced a procedure to determine multivariate partial\ninformation measures within the maximum entropy framework, separating unique,\nredundant, and synergistic components of information. Makkeh, Theis, and\nVicente (2018) formulated the latter trivariate partial information measure as\nCone Programming. In this paper, we present MAXENT3D_PID, a production-quality\nsoftware that computes the trivariate partial information measure based on the\nCone Programming model. We describe in detail our software, explain how to use\nit, and perform some experiments reflecting its accuracy in estimating the\ntrivariate partial information decomposition.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2019 19:24:42 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Makkeh", "Abdullah", ""], ["Chicharro", "Daniel", ""], ["Theis", "Dirk Oliver", ""], ["Vicente", "Raul", ""]]}, {"id": "1901.03791", "submitter": "Matthew Williams", "authors": "Matthew R. Williams and Terrance D. Savitsky", "title": "Optimization of Survey Weights under a Large Number of Conflicting\n  Constraints", "comments": "23 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survey data, sampling weights are needed for consistent\nestimation of the population. However, the original inverse probability weights\nfrom the survey sample design are typically modified to account for\nnon-response, to increase efficiency by incorporating auxiliary population\ninformation, and to reduce the variability in estimates due to extreme weights.\nIt is often the case that no single set of weights can be found which\nsuccessfully incorporates all of these modifications because together they\ninduce a large number of constraints and restrictions on the feasible solution\nspace. For example, a unique combination of categorical variables may not be\npresent in the sample data, even if the corresponding population level\ninformation is available. Additional requirements for weights to fall within\nspecified ranges may also lead to fewer population level adjustments being\nincorporated. We present a framework and accompanying computational methods to\naddress this issue of constraint achievement or selection within a restricted\nspace that will produce revised weights with reasonable properties. By\ncombining concepts from generalized raking, ridge and lasso regression,\nbenchmarking of small area estimates, augmentation of state-space equations,\npath algorithms, and data-cloning, this framework simultaneously selects\nconstraints and provides diagnostics suggesting why a fully constrained\nsolution is not possible. Combinatoric operations such as brute force\nevaluations of all possible combinations of constraints and restrictions are\navoided. We demonstrate this framework by applying alternative methods to\npost-stratification for the National Survey on Drug Use and Health. We also\ndiscuss strategies for scaling up to even larger data sets. Computations were\nperformed in R and code is available from the authors.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2019 03:28:55 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Williams", "Matthew R.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1901.03958", "submitter": "Bj\\\"orn Sprungk", "authors": "Claudia Schillings, Bj\\\"orn Sprungk, Philipp Wacker", "title": "On the Convergence of the Laplace Approximation and\n  Noise-Level-Robustness of Laplace-based Monte Carlo Methods for Bayesian\n  Inverse Problems", "comments": "50 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian approach to inverse problems provides a rigorous framework for\nthe incorporation and quantification of uncertainties in measurements,\nparameters and models. We are interested in designing numerical methods which\nare robust w.r.t. the size of the observational noise, i.e., methods which\nbehave well in case of concentrated posterior measures. The concentration of\nthe posterior is a highly desirable situation in practice, since it relates to\ninformative or large data. However, it can pose a computational challenge for\nnumerical methods based on the prior or reference measure. We propose to employ\nthe Laplace approximation of the posterior as the base measure for numerical\nintegration in this context. The Laplace approximation is a Gaussian measure\ncentered at the maximum a-posteriori estimate and with covariance matrix\ndepending on the logposterior density. We discuss convergence results of the\nLaplace approximation in terms of the Hellinger distance and analyze the\nefficiency of Monte Carlo methods based on it. In particular, we show that\nLaplace-based importance sampling and Laplace-based quasi-Monte-Carlo methods\nare robust w.r.t. the concentration of the posterior for large classes of\nposterior distributions and integrands whereas prior-based importance sampling\nand plain quasi-Monte Carlo are not. Numerical experiments are presented to\nillustrate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 10:47:12 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 09:13:17 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 08:36:40 GMT"}, {"version": "v4", "created": "Fri, 26 Jun 2020 16:02:28 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Schillings", "Claudia", ""], ["Sprungk", "Bj\u00f6rn", ""], ["Wacker", "Philipp", ""]]}, {"id": "1901.03960", "submitter": "Pai Liu", "authors": "Jingwei Gan, Pai Liu, Rajan K. Chakrabarty", "title": "Introducing a Generative Adversarial Network Model for Lagrangian\n  Trajectory Simulation", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a generative adversarial network (GAN) model to simulate the\n3-dimensional Lagrangian motion of particles trapped in the recirculation zone\nof a buoyancy-opposed flame. The GAN model comprises a stochastic recurrent\nneural network, serving as a generator, and a convoluted neural network,\nserving as a discriminator. Adversarial training was performed to the point\nwhere the best-trained discriminator failed to distinguish the ground truth\nfrom the trajectory produced by the best-trained generator. The model\nperformance was then benchmarked against a statistical analysis performed on\nboth the simulated trajectories and the ground truth, with regard to the\naccuracy and generalization criteria.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jan 2019 11:06:34 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Gan", "Jingwei", ""], ["Liu", "Pai", ""], ["Chakrabarty", "Rajan K.", ""]]}, {"id": "1901.04345", "submitter": "Johan Pensar", "authors": "Johan Pensar, Yingying Xu, Santeri Puranen, Maiju Pesonen, Yoshiyuki\n  Kabashima, Jukka Corander", "title": "High-dimensional structure learning of binary pairwise Markov networks:\n  A comparative numerical study", "comments": null, "journal-ref": "Computational Statistics & Data Analysis 141, 62-76 (2020)", "doi": "10.1016/j.csda.2019.06.012", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the undirected graph structure of a Markov network from data is a\nproblem that has received a lot of attention during the last few decades. As a\nresult of the general applicability of the model class, a myriad of methods\nhave been developed in parallel in several research fields. Recently, as the\nsize of the considered systems has increased, the focus of new methods has been\nshifted towards the high-dimensional domain. In particular, introduction of the\npseudo-likelihood function has pushed the limits of score-based methods which\nwere originally based on the likelihood function. At the same time, methods\nbased on simple pairwise tests have been developed to meet the challenges\narising from increasingly large data sets in computational biology. Apart from\nbeing applicable to high-dimensional problems, methods based on the\npseudo-likelihood and pairwise tests are fundamentally very different. To\ncompare the accuracy of the different types of methods, an extensive numerical\nstudy is performed on data generated by binary pairwise Markov networks. A\nparallelizable Gibbs sampler, based on restricted Boltzmann machines, is\nproposed as a tool to efficiently sample from sparse high-dimensional networks.\nThe results of the study show that pairwise methods can be more accurate than\npseudo-likelihood methods in settings often encountered in high-dimensional\nstructure learning applications.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 14:26:52 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2019 07:28:54 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Pensar", "Johan", ""], ["Xu", "Yingying", ""], ["Puranen", "Santeri", ""], ["Pesonen", "Maiju", ""], ["Kabashima", "Yoshiyuki", ""], ["Corander", "Jukka", ""]]}, {"id": "1901.04454", "submitter": "Byeonghee Yu", "authors": "Uros Seljak and Byeonghee Yu", "title": "Posterior inference unchained with EL_2O", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO cs.LG physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference of analytically non-tractable posteriors is a difficult\nproblem because of marginalization of correlated variables and stochastic\nmethods such as MCMC and VI are commonly used. We argue that stochastic KL\ndivergence minimization used by MCMC and VI is noisy, and we propose instead\nEL_2O, expectation optimization of L_2 distance squared between the approximate\nlog posterior q and the un-normalized log posterior of p. When sampling from q\nthe solutions agree with stochastic KL divergence minimization based VI in the\nlarge sample limit, however EL_2O method is free of sampling noise, has better\noptimization properties, and requires only as many sample evaluations as the\nnumber of parameters we are optimizing if q covers p. As a consequence,\nincreasing the expressivity of q improves both the quality of results and the\nconvergence rate, allowing EL_2O to approach exact inference. Use of automatic\ndifferentiation methods enables us to develop Hessian, gradient and gradient\nfree versions of the method, which can determine M(M+2)/2+1, M+1 and 1\nparameter(s) of q with a single sample, respectively. EL_2O provides a reliable\nestimate of the quality of the approximating posterior, and converges rapidly\non full rank gaussian approximation for q and extensions beyond it, such as\nnonlinear transformations and gaussian mixtures. These can handle general\nposteriors, while still allowing fast analytic marginalizations. We test it on\nseveral examples, including a realistic 13 dimensional galaxy clustering\nanalysis, showing that it is several orders of magnitude faster than MCMC,\nwhile giving smooth and accurate non-gaussian posteriors, often requiring a few\nto a few dozen of iterations only.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 18:38:23 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 18:01:42 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Seljak", "Uros", ""], ["Yu", "Byeonghee", ""]]}, {"id": "1901.04598", "submitter": "Adrian Wong", "authors": "Adrian S. Wong, Kangbo Hao, Zheng Fang, Henry D.I. Abarbanel", "title": "Precision Annealing Monte Carlo Methods for Statistical Data\n  Assimilation: Metropolis-Hastings Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Data Assimilation (SDA) is the transfer of information from field\nor laboratory observations to a user selected model of the dynamical system\nproducing those observations. The data is noisy and the model has errors; the\ninformation transfer addresses properties of the conditional probability\ndistribution of the states of the model conditioned on the observations. The\nquantities of interest in SDA are the conditional expected values of functions\nof the model state, and these require the approximate evaluation of high\ndimensional integrals. We introduce a conditional probability distribution and\nuse the Laplace method with annealing to identify the maxima of the conditional\nprobability distribution. The annealing method slowly increases the precision\nterm of the model as it enters the Laplace method. In this paper, we extend the\nidea of precision annealing (PA) to Monte Carlo calculations of conditional\nexpected values using Metropolis-Hastings methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2019 22:49:21 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Wong", "Adrian S.", ""], ["Hao", "Kangbo", ""], ["Fang", "Zheng", ""], ["Abarbanel", "Henry D. I.", ""]]}, {"id": "1901.04706", "submitter": "Svetlana Dubinkina", "authors": "Sangeetika Ruchi and Svetlana Dubinkina and Marco Iglesias", "title": "Transform-based particle filtering for elliptic Bayesian inverse\n  problems", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/ab30f3", "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce optimal transport based resampling in adaptive SMC. We consider\nelliptic inverse problems of inferring hydraulic conductivity from pressure\nmeasurements. We consider two parametrizations of hydraulic conductivity: by\nGaussian random field, and by a set of scalar (non-)Gaussian distributed\nparameters and Gaussian random fields. We show that for scalar parameters\noptimal transport based SMC performs comparably to monomial based SMC but for\nGaussian high-dimensional random fields optimal transport based SMC outperforms\nmonomial based SMC. When comparing to ensemble Kalman inversion with mutation\n(EKI), we observe that for Gaussian random fields, optimal transport based SMC\ngives comparable or worse performance than EKI depending on the complexity of\nthe parametrization. For non-Gaussian distributed parameters optimal transport\nbased SMC outperforms EKI.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 08:32:32 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 08:26:07 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Ruchi", "Sangeetika", ""], ["Dubinkina", "Svetlana", ""], ["Iglesias", "Marco", ""]]}, {"id": "1901.04779", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen and Steven Stern", "title": "Assessing the accuracy of record linkages with Markov chain based Monte\n  Carlo simulation approach", "comments": "33 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of finding matches and linking records from\ndifferent data sources so that the linked records belong to the same entity.\nThere is an increasing number of applications of record linkage in statistical,\nhealth, government and business organisations to link administrative, survey,\npopulation census and other files to create a complete set of information for\nmore complete and comprehensive analysis. To make valid inferences using a\nlinked file, it is increasingly becoming important to assess the linking\nmethod. It is also important to find techniques to improve the linking process\nto achieve higher accuracy. This motivates to develop a method for assessing\nlinking process and help decide which linking method is likely to be more\naccurate for a linking task. This paper proposes a Markov Chain based Monte\nCarlo simulation approach, MaCSim for assessing a linking method and\nillustrates the utility of the approach using a realistic synthetic dataset\nreceived from the Australian Bureau of Statistics to avoid privacy issues\nassociated with using real personal information. A linking method applied by\nMaCSim is also defined. To assess the defined linking method, correct re-link\nproportions for each record are calculated using our developed simulation\napproach. The accuracy is determined for a number of simulated datasets. The\nanalyses indicated promising performance of the proposed method MaCSim of the\nassessment of accuracy of the linkages. The computational aspects of the\nmethodology are also investigated to assess its feasibility for practical use.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 11:56:54 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 12:28:24 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 09:45:12 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""], ["Stern", "Steven", ""]]}, {"id": "1901.04816", "submitter": "Daniele Ancora", "authors": "Daniele Ancora, Luca Leuzzi", "title": "Learning Direct and Inverse Transmission Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.comp-ph physics.optics stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear problems appear in a variety of disciplines and their application for\nthe transmission matrix recovery is one of the most stimulating challenges in\nbiomedical imaging. Its knowledge turns any random media into an optical tool\nthat can focus or transmit an image through disorder. Here, converting an\ninput-output problem into a statistical mechanical formulation, we investigate\nhow inference protocols can learn the transmission couplings by\npseudolikelihood maximization. Bridging linear regression and thermodynamics\nlet us propose an innovative framework to pursue the solution of the\nscattering-riddle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 13:37:24 GMT"}, {"version": "v2", "created": "Sat, 2 Feb 2019 21:36:54 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Ancora", "Daniele", ""], ["Leuzzi", "Luca", ""]]}, {"id": "1901.04866", "submitter": "James Townsend", "authors": "James Townsend, Tom Bird, David Barber", "title": "Practical Lossless Compression with Latent Variables using Bits Back\n  Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep latent variable models have seen recent success in many data domains.\nLossless compression is an application of these models which, despite having\nthe potential to be highly useful, has yet to be implemented in a practical\nmanner. We present `Bits Back with ANS' (BB-ANS), a scheme to perform lossless\ncompression with latent variable models at a near optimal rate. We demonstrate\nthis scheme by using it to compress the MNIST dataset with a variational\nauto-encoder model (VAE), achieving compression rates superior to standard\nmethods with only a simple VAE. Given that the scheme is highly amenable to\nparallelization, we conclude that with a sufficiently high quality generative\nmodel this scheme could be used to achieve substantial improvements in\ncompression rate with acceptable running time. We make our implementation\navailable open source at https://github.com/bits-back/bits-back .\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 14:45:47 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Townsend", "James", ""], ["Bird", "Tom", ""], ["Barber", "David", ""]]}, {"id": "1901.04884", "submitter": "Michal Valko", "authors": "Jean-Bastien Grill and Michal Valko and R\\'emi Munos", "title": "Optimistic optimization of a Brownian", "comments": "10 pages, 2 figures", "journal-ref": "Neural Information Processing Systems (NeurIPS 2018)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optimizing a Brownian motion. We consider a\n(random) realization $W$ of a Brownian motion with input space in $[0,1]$.\nGiven $W$, our goal is to return an $\\epsilon$-approximation of its maximum\nusing the smallest possible number of function evaluations, the sample\ncomplexity of the algorithm. We provide an algorithm with sample complexity of\norder $\\log^2(1/\\epsilon)$. This improves over previous results of Al-Mharmah\nand Calvin (1996) and Calvin et al. (2017) which provided only polynomial\nrates. Our algorithm is adaptive---each query depends on previous values---and\nis an instance of the optimism-in-the-face-of-uncertainty principle.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 15:37:11 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Grill", "Jean-Bastien", ""], ["Valko", "Michal", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1901.05178", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Jayant Jha and Prajamitra Bhuyan", "title": "Two-stage Circular-circular Regression with Zero-inflation: Application\n  to Medical Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the modeling of zero-inflated circular measurements\nconcerning real case studies from medical sciences. Circular-circular\nregression models have been discussed in the statistical literature and\nillustrated with various real-life applications. However, there are no models\nto deal with zero-inflated response as well as a covariate simultaneously. The\nMobius transformation based two-stage circular-circular regression model is\nproposed, and the Bayesian estimation of the model parameters is suggested\nusing the MCMC algorithm. Simulation results show the superiority of the\nperformance of the proposed method over the existing competitors. The method is\napplied to analyse real datasets on astigmatism due to cataract surgery and\nabnormal gait related to orthopaedic impairment. The methodology proposed can\nassist in efficient decision making during treatment or post-operative care.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 08:44:19 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 14:50:20 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 08:44:29 GMT"}, {"version": "v4", "created": "Fri, 9 Oct 2020 10:09:27 GMT"}, {"version": "v5", "created": "Thu, 17 Dec 2020 11:42:45 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Jha", "Jayant", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "1901.05194", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Kiranmoy Chatterjee and Prajamitra Bhuyan", "title": "On the Estimation of Population Size from a Dependent Triple Record\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation based on capture-recapture experiment under triple\nrecord system is an interesting problem in various fields including\nepidemiology, population studies, etc. In many real life scenarios, there\nexists inherent dependency between capture and recapture attempts. We propose a\nnovel model that successfully incorporates the possible dependency and the\nassociated parameters possess nice interpretations. We provide estimation\nmethodology for the population size and the associated model parameters based\non maximum likelihood method. The proposed model is applied to analyze real\ndata sets from public health and census coverage evaluation study. The\nperformance of the proposed estimate is evaluated through extensive simulation\nstudy and the results are compared with the existing competitors. The results\nexhibit superiority of the proposed model over the existing competitors both in\nreal data analysis and simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2019 09:40:40 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 13:34:02 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "1901.05935", "submitter": "Rocio Joo", "authors": "Rocio Joo, Matthew E. Boone, Thomas A. Clay, Samantha C. Patrick,\n  Susana Clusella-Trullas, Mathieu Basille", "title": "Navigating through the R packages for movement", "comments": "77 pages, 4 figures", "journal-ref": "Journal of Animal Ecology, 2019", "doi": "10.1111/1365-2656.13116", "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of miniaturized biologging devices has provided ecologists with\nunprecedented opportunities to record animal movement across scales, and led to\nthe collection of ever-increasing quantities of tracking data. In parallel,\nsophisticated tools have been developed to process, visualize and analyze\ntracking data, however many of these tools have proliferated in isolation,\nmaking it challenging for users to select the most appropriate method for the\nquestion in hand. Indeed, within the R software alone, we listed 58 packages\ncreated to deal with tracking data or 'tracking packages'. Here we reviewed and\ndescribed each tracking package based on a workflow centered around tracking\ndata (i.e. spatio-temporal locations (x,y,t)), broken down into three stages:\npre-processing, post-processing and analysis, the latter consisting of data\nvisualization, track description, path reconstruction, behavioral pattern\nidentification, space use characterization, trajectory simulation and others.\nSupporting documentation is key to render a package accessible for users. Based\non a user survey, we reviewed the quality of packages' documentation, and\nidentified 11 packages with good or excellent documentation. Links between\npackages were assessed through a network graph analysis. Although a large group\nof packages showed some degree of connectivity (either depending on functions\nor suggesting the use of another tracking package), one third of the packages\nworked in isolation, reflecting a fragmentation in the R movement-ecology\nprogramming community. Finally, we provide recommendations for users when\nchoosing packages, and for developers to maximize the usefulness of their\ncontribution and strengthen the links within the programming community.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 18:13:52 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 20:10:27 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 18:31:49 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Joo", "Rocio", ""], ["Boone", "Matthew E.", ""], ["Clay", "Thomas A.", ""], ["Patrick", "Samantha C.", ""], ["Clusella-Trullas", "Susana", ""], ["Basille", "Mathieu", ""]]}, {"id": "1901.06075", "submitter": "Michael Weylandt", "authors": "Michael Weylandt", "title": "Splitting Methods for Convex Bi-Clustering and Co-Clustering", "comments": "To appear in IEEE DSW 2019", "journal-ref": "DSW 2019: Proceedings of the IEEE Data Science Workshop 2019, pp.\n  237-244", "doi": "10.1109/DSW.2019.8755599", "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Co-Clustering, the problem of simultaneously identifying clusters across\nmultiple aspects of a data set, is a natural generalization of clustering to\nhigher-order structured data. Recent convex formulations of bi-clustering and\ntensor co-clustering, which shrink estimated centroids together using a convex\nfusion penalty, allow for global optimality guarantees and precise theoretical\nanalysis, but their computational properties have been less well studied. In\nthis note, we present three efficient operator-splitting methods for the convex\nco-clustering problem: a standard two-block ADMM, a Generalized ADMM which\navoids an expensive tensor Sylvester equation in the primal update, and a\nthree-block ADMM based on the operator splitting scheme of Davis and Yin.\nTheoretical complexity analysis suggests, and experimental evidence confirms,\nthat the Generalized ADMM is far more efficient for large problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 03:44:51 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 04:33:32 GMT"}, {"version": "v3", "created": "Thu, 30 May 2019 21:41:05 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 18:34:54 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Weylandt", "Michael", ""]]}, {"id": "1901.06300", "submitter": "Jana de Wiljes", "authors": "Jana de Wiljes and Sahani Pathiraja and Sebastian Reich", "title": "Ensemble transform algorithms for nonlinear smoothing problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several numerical tools designed to overcome the challenges of smoothing in a\nnonlinear and non-Gaussian setting are investigated for a class of particle\nsmoothers. The considered family of smoothers is induced by the class of linear\nensemble transform filters which contains classical filters such as the\nstochastic ensemble Kalman filter, the ensemble square root filter and the\nrecently introduced nonlinear ensemble transform filter. Further the ensemble\ntransform particle smoother is introduced and particularly highlighted as it is\nconsistent in the particle limit and does not require assumptions with respect\nto the family of the posterior distribution. The linear update pattern of the\nconsidered class of linear ensemble transform smoothers allows one to implement\nimportant supplementary techniques such as adaptive spread corrections, hybrid\nformulations, and localization in order to facilitate their application to\ncomplex estimation problems. These additional features are derived and\nnumerically investigated for a sequence of increasingly challenging test\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 15:40:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 07:27:06 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 16:07:56 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["de Wiljes", "Jana", ""], ["Pathiraja", "Sahani", ""], ["Reich", "Sebastian", ""]]}, {"id": "1901.06428", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Unreasonable effectiveness of Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on the article \"Probabilistic Integration: A Role in\nStatistical Computation?\" by F.-X. Briol, C. J. Oates, M. Girolami, M. A.\nOsborne and D. Sejdinovic to appear in Statistical Science.\n  There is a role for statistical computation in numerical integration.\nHowever, the competition from incumbent methods looks to be stiffer for this\nproblem than for some of the newer problems being handled by probabilistic\nnumerics. One of the challenges is the unreasonable effectiveness of the\ncentral limit theorem. Another is the unreasonable effectiveness of\npseudorandom number generators. A third is the common $O(n^3)$ cost of methods\nbased on Gaussian processes. Despite these advantages, the classical methods\nare weak in places where probabilistic methods could bring an improvement.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2019 22:01:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1901.07090", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay and Kaijun Wang", "title": "Spectral Graph Analysis: A Unified Explanation and Modern Perspectives", "comments": "The first draft of the paper was written in June 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks or graphs are ubiquitous in sciences and engineering:\nbiological networks, brain networks, transportation networks, social networks,\nand the World Wide Web, to name a few. Spectral graph theory provides a set of\nuseful techniques and models for understanding `patterns of interconnectedness'\nin a graph. Our prime focus in this paper is on the following question: Is\nthere a unified explanation and description of the fundamental spectral graph\nmethods? There are at least two reasons to be interested in this question.\nFirstly, to gain a much deeper and refined understanding of the basic\nfoundational principles, and secondly, to derive rich consequences with\npractical significance for algorithm design. However, despite half a century of\nresearch, this question remains one of the most formidable open issues, if not\nthe core problem in modern network science. The achievement of this paper is to\ntake a step towards answering this question by discovering a simple, yet\nuniversal statistical logic of spectral graph analysis. The prescribed\nviewpoint appears to be good enough to accommodate almost all existing spectral\ngraph techniques as a consequence of just one single formalism and algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 21:41:52 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Wang", "Kaijun", ""]]}, {"id": "1901.07150", "submitter": "Cheng Wang", "authors": "Zhou Tang, Zhangsheng Yu and Cheng Wang", "title": "A Fast Iterative Algorithm for High-dimensional Differential Network", "comments": "13 pages 3 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential network is an important tool to capture the changes of\nconditional correlations under two sample cases. In this paper, we introduce a\nfast iterative algorithm to recover the differential network for\nhigh-dimensional data. The computation complexity of our algorithm is linear in\nthe sample size and the number of parameters, which is optimal in the sense\nthat it is of the same order as computing two sample covariance matrices. The\nproposed method is appealing for high-dimensional data with a small sample\nsize. The experiments on simulated and real data sets show that the proposed\nalgorithm outperforms other existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 02:09:44 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Tang", "Zhou", ""], ["Yu", "Zhangsheng", ""], ["Wang", "Cheng", ""]]}, {"id": "1901.07377", "submitter": "Dan Li", "authors": "Dan Li and Sonia Martinez", "title": "Data assimilation and online optimization with performance guarantees", "comments": "IEEE Transactions on Automatic Control. A preliminary work appeared\n  in 10.1109/CDC.2018.8619159 and arxiv:1803.07984", "journal-ref": null, "doi": "10.1109/TAC.2020.3005681", "report-no": null, "categories": "math.OC cs.SY eess.SP eess.SY math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a class of real-time stochastic optimization problems\ndependent on an unknown probability distribution. In the considered scenario,\ndata is streaming frequently while trying to reach a decision. Thus, we aim to\ndevise a procedure that incorporates samples (data) of the distribution\nsequentially and adjusts decisions accordingly. We approach this problem in a\ndistributionally robust optimization framework and propose a novel Online Data\nAssimilation Algorithm (ONDA Algorithm) for this purpose. This algorithm\nguarantees out-of-sample performance of decisions with high probability, and\ngradually improves the quality of the decisions by incorporating the streaming\ndata. We show that the ONDA Algorithm converges under a sufficiently slow data\nstreaming rate, and provide a criteria for its termination after certain number\nof data have been collected. Simulations illustrate the results.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 22:16:09 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 19:34:26 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 23:58:16 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Li", "Dan", ""], ["Martinez", "Sonia", ""]]}, {"id": "1901.07473", "submitter": "Ricardo Ehlers", "authors": "Ricardo S Ehlers", "title": "A Conway-Maxwell-Poisson GARMA Model for Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible model for count time series which has potential uses\nfor both underdispersed and overdispersed data. The model is based on the\nConway-Maxwell-Poisson (COM-Poisson) distribution with parameters varying along\ntime to take serial correlation into account. Model estimation is challenging\nhowever and require the application of recently proposed methods to deal with\nthe intractable normalising constant as well as efficiently sampling values\nfrom the COM-Poisson distribution.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2019 17:15:53 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Ehlers", "Ricardo S", ""]]}, {"id": "1901.08057", "submitter": "Hanwen Huang", "authors": "Hanwen Huang", "title": "Large dimensional analysis of general margin based classification\n  methods", "comments": "28 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Margin-based classifiers have been popular in both machine learning and\nstatistics for classification problems. Since a large number of classifiers are\navailable, one natural question is which type of classifiers should be used\ngiven a particular classification task. We aim to answering this question by\ninvestigating the asymptotic performance of a family of large-margin\nclassifiers in situations where the data dimension $p$ and the sample $n$ are\nboth large. This family covers a broad range of classifiers including support\nvector machine, distance weighted discrimination, penalized logistic\nregression, and large-margin unified machine as special cases. The asymptotic\nresults are described by a set of nonlinear equations and we observe a close\nmatch of them with Monte Carlo simulation on finite data samples. Our\nanalytical studies shed new light on how to select the best classifier among\nvarious classification methods as well as on how to choose the optimal tuning\nparameters for a given method.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2019 14:45:08 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Huang", "Hanwen", ""]]}, {"id": "1901.08115", "submitter": "Daniel Rudolf", "authors": "Josef Dick, Daniel Rudolf, Houying Zhu", "title": "A weighted Discrepancy Bound of quasi-Monte Carlo Importance Sampling", "comments": "14 pages, 2 figures, shorter version of the manuscript accepted for\n  publication in Stat. Probab. Lett. (for Section 3 see online supplementary\n  material there)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling Monte-Carlo methods are widely used for the approximation\nof expectations with respect to partially known probability measures. In this\npaper we study a deterministic version of such an estimator based on\nquasi-Monte Carlo. We obtain an explicit error bound in terms of the\nstar-discrepancy for this method.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2019 13:04:51 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Dick", "Josef", ""], ["Rudolf", "Daniel", ""], ["Zhu", "Houying", ""]]}, {"id": "1901.08239", "submitter": "Zhimin Chen", "authors": "Zhimin Chen, Darius Parvin, Maedbh King, Susan Hao", "title": "Visualizing Topographic Independent Component Analysis with Movies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Independent component analysis (ICA) has often been used as a tool to model\nnatural image statistics by separating multivariate signals in the image into\ncomponents that are assumed to be independent. However, these estimated\ncomponents oftentimes have higher order dependencies, such as co-activation of\ncomponents, that are not accounted for in the model. Topographic independent\ncomponent analysis(TICA), a modification of ICA, takes into account higher\norder dependencies and orders components topographically as a function of\ndependence. Here, we aim to visualize the time course of TICA basis activations\nto movie stimuli. We find that the activity of TICA bases are often clustered\nand move continuously, potentially resembling activity of topographically\norganized cells in the visual cortex.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 05:47:01 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Chen", "Zhimin", ""], ["Parvin", "Darius", ""], ["King", "Maedbh", ""], ["Hao", "Susan", ""]]}, {"id": "1901.08606", "submitter": "Steve Huntsman", "authors": "Steve Huntsman", "title": "Fast Markov Chain Monte Carlo Algorithms via Lie Groups", "comments": "Accepted to AISTATS 2020; proofs included here as an appendix (but\n  relegated to supplementary info in conference version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.GR math.RA stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From basic considerations of the Lie group that preserves a target\nprobability measure, we derive the Barker, Metropolis, and ensemble Markov\nchain Monte Carlo (MCMC) algorithms, as well as variants of waste-recycling\nMetropolis-Hastings and an altogether new MCMC algorithm. We illustrate these\nconstructions with explicit numerical computations, and we empirically\ndemonstrate on a spin glass that the new algorithm converges more quickly than\nits siblings.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2019 19:01:13 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 20:56:58 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Huntsman", "Steve", ""]]}, {"id": "1901.08855", "submitter": "Jukka Sir\\'en", "authors": "Jukka Sir\\'en, Samuel Kaski", "title": "Local dimension reduction of summary statistics for likelihood-free\n  inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) and other likelihood-free inference\nmethods have gained popularity in the last decade, as they allow rigorous\nstatistical inference for complex models without analytically tractable\nlikelihood functions. A key component for accurate inference with ABC is the\nchoice of summary statistics, which summarize the information in the data, but\nat the same time should be low-dimensional for efficiency. Several dimension\nreduction techniques have been introduced to automatically construct\ninformative and low-dimensional summaries from a possibly large pool of\ncandidate summaries. Projection-based methods, which are based on learning\nsimple functional relationships from the summaries to parameters, are widely\nused and usually perform well, but might fail when the assumptions behind the\ntransformation are not satisfied. We introduce a localization strategy for any\nprojection-based dimension reduction method, in which the transformation is\nestimated in the neighborhood of the observed data instead of the whole space.\nLocalization strategies have been suggested before, but the performance of the\ntransformed summaries outside the local neighborhood has not been guaranteed.\nIn our localization approach the transformation is validated and optimized over\nvalidation datasets, ensuring reliable performance. We demonstrate the\nimprovement in the estimation accuracy for localized versions of linear\nregression and partial least squares, for three different models of varying\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2019 12:45:01 GMT"}, {"version": "v2", "created": "Thu, 20 Jun 2019 10:25:15 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Sir\u00e9n", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1901.09400", "submitter": "Nicolas Papadakis", "authors": "Nicolas Papadakis", "title": "Approximation of Wasserstein distance with Transshipment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for approximating the p-Wasserstein distance between histograms\ndefined on unstructured discrete grids is presented. It is based on the\ncomputation of a barycenter constrained to be supported on a low dimensional\nsubspace, which corresponds to a transshipment problem. A multi-scale strategy\nis also considered. The method provides sparse transport matrices and can be\napplied to large scale and non structured data.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2019 16:31:52 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 09:46:19 GMT"}, {"version": "v3", "created": "Mon, 17 Jun 2019 10:19:31 GMT"}, {"version": "v4", "created": "Wed, 23 Sep 2020 14:58:41 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Papadakis", "Nicolas", ""]]}, {"id": "1901.09828", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli and Michael Fop", "title": "A dynamic stochastic blockmodel for interaction lengths", "comments": "23 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new dynamic stochastic blockmodel that focuses on the analysis\nof interaction lengths in networks. The model does not rely on a discretization\nof the time dimension and may be used to analyze networks that evolve\ncontinuously over time. The framework relies on a clustering structure on the\nnodes, whereby two nodes belonging to the same latent group tend to create\ninteractions and non-interactions of similar lengths. We introduce a fast\nvariational expectation-maximization algorithm to perform inference, and adapt\na widely used clustering criterion to perform model choice. Finally, we test\nour methodology on artificial data, and propose a demonstration on a dataset\nconcerning face-to-face interactions between students in a high-school.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 17:25:34 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Fop", "Michael", ""]]}, {"id": "1901.10082", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos, Zach Kaplan, Daniel Sanz-Alonso", "title": "Variational Characterizations of Local Entropy and Heat Regularization\n  in Deep Learning", "comments": null, "journal-ref": null, "doi": "10.3390/e21050511", "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide new theoretical and computational\nunderstanding on two loss regularizations employed in deep learning, known as\nlocal entropy and heat regularization. For both regularized losses we introduce\nvariational characterizations that naturally suggest a two-step scheme for\ntheir optimization, based on the iterative shift of a probability density and\nthe calculation of a best Gaussian approximation in Kullback-Leibler\ndivergence. Under this unified light, the optimization schemes for local\nentropy and heat regularized loss differ only over which argument of the\nKullback-Leibler divergence is used to find the best Gaussian approximation.\nLocal entropy corresponds to minimizing over the second argument, and the\nsolution is given by moment matching. This allows to replace traditional\nback-propagation calculation of gradients by sampling algorithms, opening an\navenue for gradient-free, parallelizable training of neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 03:12:42 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Kaplan", "Zach", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "1901.10230", "submitter": "Samuel Wiqvist", "authors": "Samuel Wiqvist, Pierre-Alexandre Mattei, Umberto Picchini, Jes\n  Frellsen", "title": "Partially Exchangeable Networks and Architectures for Learning Summary\n  Statistics in Approximate Bayesian Computation", "comments": "Forthcoming on the Proceedings of ICML 2019. New comparisons with\n  several different networks. We now use the Wasserstein distance to produce\n  comparisons. Code available on GitHub. 16 pages, 5 figures, 21 tables", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:6798--6807, 2019", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel family of deep neural architectures, named partially\nexchangeable networks (PENs) that leverage probabilistic symmetries. By design,\nPENs are invariant to block-switch transformations, which characterize the\npartial exchangeability properties of conditionally Markovian processes.\nMoreover, we show that any block-switch invariant function has a PEN-like\nrepresentation. The DeepSets architecture is a special case of PEN and we can\ntherefore also target fully exchangeable data. We employ PENs to learn summary\nstatistics in approximate Bayesian computation (ABC). When comparing PENs to\nprevious deep learning methods for learning summary statistics, our results are\nhighly competitive, both considering time series and static models. Indeed,\nPENs provide more reliable posterior samples even when using less training\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:31:31 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 14:19:59 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Wiqvist", "Samuel", ""], ["Mattei", "Pierre-Alexandre", ""], ["Picchini", "Umberto", ""], ["Frellsen", "Jes", ""]]}, {"id": "1901.10257", "submitter": "Earo Wang", "authors": "Earo Wang, Dianne Cook, Rob J Hyndman", "title": "A new tidy data structure to support exploration and modeling of\n  temporal data", "comments": "Revision on Section 4 and 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mining temporal data for information is often inhibited by a multitude of\nformats: irregular or multiple time intervals, point events that need\naggregating, multiple observational units or repeated measurements on multiple\nindividuals, and heterogeneous data types. On the other hand, the software\nsupporting time series modeling and forecasting, makes strict assumptions on\nthe data to be provided, typically requiring a matrix of numeric data with\nimplicit time indexes. Going from raw data to model-ready data is painful. This\nwork presents a cohesive and conceptual framework for organizing and\nmanipulating temporal data, which in turn flows into visualization, modeling\nand forecasting routines. Tidy data principles are extended to temporal data\nby: (1) mapping the semantics of a dataset into its physical layout; (2)\nincluding an explicitly declared index variable representing time; (3)\nincorporating a \"key\" comprising single or multiple variables to uniquely\nidentify units over time. This tidy data representation most naturally supports\nthinking of operations on the data as building blocks, forming part of a \"data\npipeline\" in time-based contexts. A sound data pipeline facilitates a fluent\nworkflow for analyzing temporal data. The infrastructure of tidy temporal data\nhas been implemented in the R package \"tsibble\".\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 12:48:59 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 05:32:47 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Wang", "Earo", ""], ["Cook", "Dianne", ""], ["Hyndman", "Rob J", ""]]}, {"id": "1901.10275", "submitter": "Mikko Heikkil\\\"a", "authors": "Mikko A. Heikkil\\\"a and Joonas J\\\"alk\\\"o and Onur Dikmen and Antti\n  Honkela", "title": "Differentially Private Markov Chain Monte Carlo", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in differentially private (DP) machine learning and DP\nBayesian learning have enabled learning under strong privacy guarantees for the\ntraining data subjects. In this paper, we further extend the applicability of\nDP Bayesian learning by presenting the first general DP Markov chain Monte\nCarlo (MCMC) algorithm whose privacy-guarantees are not subject to unrealistic\nassumptions on Markov chain convergence and that is applicable to posterior\ninference in arbitrary models. Our algorithm is based on a decomposition of the\nBarker acceptance test that allows evaluating the R\\'enyi DP privacy cost of\nthe accept-reject choice. We further show how to improve the DP guarantee\nthrough data subsampling and approximate acceptance tests.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 13:34:43 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 13:37:26 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Heikkil\u00e4", "Mikko A.", ""], ["J\u00e4lk\u00f6", "Joonas", ""], ["Dikmen", "Onur", ""], ["Honkela", "Antti", ""]]}, {"id": "1901.10399", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Phalguni Nanda, Prajamitra Bhuyan and Anup Dewanji", "title": "Optimal Replacement Policy under Cumulative Damage Model and Strength\n  Degradation with Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life scenarios, system failure depends on dynamic\nstress-strength interference, where strength degrades and stress accumulates\nconcurrently over time. In this paper, we consider the problem of finding an\noptimal replacement strategy that balances the cost of replacement with the\ncost of failure and results in a minimum expected cost per unit time under\ncumulative damage model with strength degradation. The existing recommendations\nare applicable only under restricted distributional assumptions and/or with\nfixed strength. As theoretical evaluation of the expected cost per unit time\nturns out to be very complicated, a simulation-based algorithm is proposed to\nevaluate the expected cost rate and find the optimal replacement strategy. The\nproposed method is easy to implement having wider domain of application. For\nillustration, the proposed method is applied to real case studies on mailbox\nand cell-phone battery experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2019 21:39:33 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 08:18:27 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 15:03:30 GMT"}, {"version": "v4", "created": "Fri, 27 Mar 2020 09:51:45 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Nanda", "Phalguni", ""], ["Bhuyan", "Prajamitra", ""], ["Dewanji", "Anup", ""]]}, {"id": "1901.10543", "submitter": "Jameson Quinn", "authors": "Jameson Quinn", "title": "A High-Dimensional Particle Filter Algorithm", "comments": "17 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Online data assimilation in time series models over a large spatial extent is\nan important problem in both geosciences and robotics. Such models are\nintrinsically high-dimensional, rendering traditional particle filter\nalgorithms ineffective. Though methods that begin to address this problem\nexist, they either rely on additional assumptions or lead to error that is\nspatially inhomogeneous. I present a novel particle-based algorithm for online\napproximation of the filtering problem on such models, using the fact that each\nlocus affects only nearby loci at the next time step. The algorithm is based on\na Metropolis-Hastings-like MCMC for creating hybrid particles at each step. I\nshow simulation results that suggest the error of this algorithm is uniform in\nboth space and time, with a lower bias, though higher variance, as compared to\na previously-proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 20:56:38 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Quinn", "Jameson", ""]]}, {"id": "1901.10568", "submitter": "Christopher Aicher", "authors": "Christopher Aicher, Srshti Putcha, Christopher Nemeth, Paul Fearnhead,\n  and Emily B. Fox", "title": "Stochastic Gradient MCMC for Nonlinear State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State space models (SSMs) provide a flexible framework for modeling complex\ntime series via a latent stochastic process. Inference for nonlinear,\nnon-Gaussian SSMs is often tackled with particle methods that do not scale well\nto long time series. The challenge is two-fold: not only do computations scale\nlinearly with time, as in the linear case, but particle filters additionally\nsuffer from increasing particle degeneracy with longer series. Stochastic\ngradient MCMC methods have been developed to scale inference for hidden Markov\nmodels (HMMs) and linear SSMs using buffered stochastic gradient estimates to\naccount for temporal dependencies. We extend these stochastic gradient\nestimators to nonlinear SSMs using particle methods. We present error bounds\nthat account for both buffering error and particle error in the case of\nnonlinear SSMs that are log-concave in the latent process. We evaluate our\nproposed particle buffered stochastic gradient using SGMCMC for inference on\nboth long sequential synthetic and minute-resolution financial returns data,\ndemonstrating the importance of this class of methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 21:39:56 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 02:59:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Aicher", "Christopher", ""], ["Putcha", "Srshti", ""], ["Nemeth", "Christopher", ""], ["Fearnhead", "Paul", ""], ["Fox", "Emily B.", ""]]}, {"id": "1901.10928", "submitter": "William N Anderson", "authors": "William N. Anderson, Johan Verbeeck", "title": "Exact Bootstrap and Permutation Distribution of Wins and Losses in a\n  Hierarchical Trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finkelstein-Schoenfeld, Buyse, Pocock, and other authors have developed\ngeneralizations of the Mann-Whitney test that allow for pairwise patient\ncomparisons to include a hierarchy of measurements. Various authors present\neither asymptotic or randomized methods for analyzing the wins. We use graph\ntheory concepts to derive exact means and variances for the number of wins, as\na replacement for approximate values obtained from bootstrap analysis or random\nsampling from the permutation distribution. The time complexity of our\nalgorithm is $O(N^2)$, where $N$ is the total number of patients. In any\nsituation where the mean and variance of a bootstrap sample are used to draw\nconclusions, our methodology will be faster and more accurate than the\nrandomized bootstrap or permutation test.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 16:21:27 GMT"}, {"version": "v2", "created": "Sun, 24 Nov 2019 19:43:12 GMT"}], "update_date": "2019-11-26", "authors_parsed": [["Anderson", "William N.", ""], ["Verbeeck", "Johan", ""]]}, {"id": "1901.11269", "submitter": "Simon Cotter", "authors": "Simon L. Cotter and Ioannis G. Kevrekidis and Paul Russell", "title": "Transport map accelerated adaptive importance sampling, and application\n  to inverse problems arising from multiscale stochastic reaction networks", "comments": "44 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, Bayesian inverse problems can give rise to probability\ndistributions which contain complexities due to the Hessian varying greatly\nacross parameter space. This complexity often manifests itself as lower\ndimensional manifolds on which the likelihood function is invariant, or varies\nvery little. This can be due to trying to infer unobservable parameters, or due\nto sloppiness in the model which is being used to describe the data. In such a\nsituation, standard sampling methods for characterising the posterior\ndistribution, which do not incorporate information about this structure, will\nbe highly inefficient.\n  In this paper, we seek to develop an approach to tackle this problem when\nusing adaptive importance sampling methods, by using optimal transport maps to\nsimplify posterior distributions which are concentrated on lower dimensional\nmanifolds. This approach is applicable to a whole range of problems for which\nMonte Carlo Markov chain (MCMC) methods mix slowly.\n  We demonstrate the approach by considering inverse problems arising from\npartially observed stochastic reaction networks. In particular, we consider\nsystems which exhibit multiscale behaviour, but for which only the slow\nvariables in the system are observable. We demonstrate that certain multiscale\napproximations lead to more consistent approximations of the posterior than\nothers. The use of optimal transport maps stabilises the ensemble transform\nadaptive importance sampling (ETAIS) method, and allows for efficient sampling\nwith smaller ensemble sizes. This approach allows us to take advantage of the\nlarge increases of efficiency when using adaptive importance sampling methods\nfor previously intractable Bayesian inverse problems with complex posterior\nstructure.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 08:48:52 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 12:15:20 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Cotter", "Simon L.", ""], ["Kevrekidis", "Ioannis G.", ""], ["Russell", "Paul", ""]]}, {"id": "1901.11491", "submitter": "Gregor Kastner", "authors": "Darjus Hosszejni and Gregor Kastner", "title": "Approaches Toward the Bayesian Estimation of the Stochastic Volatility\n  Model with Leverage", "comments": null, "journal-ref": "In R. Argiento, D. Durante, and S. Wade, editors, Bayesian\n  Statistics and New Generations - Selected Contributions from BAYSM 2018,\n  volume 296 of Springer Proceedings in Mathematics & Statistics, pages 75-83,\n  Cham, 2019. Springer", "doi": "10.1007/978-3-030-30611-3_8", "report-no": null, "categories": "stat.CO econ.EM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sampling efficiency of MCMC methods in Bayesian inference for stochastic\nvolatility (SV) models is known to highly depend on the actual parameter\nvalues, and the effectiveness of samplers based on different parameterizations\nvaries significantly. We derive novel algorithms for the centered and the\nnon-centered parameterizations of the practically highly relevant SV model with\nleverage, where the return process and innovations of the volatility process\nare allowed to correlate. Moreover, based on the idea of\nancillarity-sufficiency interweaving (ASIS), we combine the resulting samplers\nin order to guarantee stable sampling efficiency irrespective of the baseline\nparameterization.We carry out an extensive comparison to already existing\nsampling methods for this model using simulated as well as real world data.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 17:43:13 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2019 15:34:13 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Hosszejni", "Darjus", ""], ["Kastner", "Gregor", ""]]}]