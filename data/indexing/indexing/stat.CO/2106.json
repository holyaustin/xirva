[{"id": "2106.00001", "submitter": "Vikrant Singhal", "authors": "Vikrant Singhal, Thomas Steinke", "title": "Privately Learning Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Private data analysis suffers a costly curse of dimensionality. However, the\ndata often has an underlying low-dimensional structure. For example, when\noptimizing via gradient descent, the gradients often lie in or near a\nlow-dimensional subspace. If that low-dimensional structure can be identified,\nthen we can avoid paying (in terms of privacy or accuracy) for the high ambient\ndimension.\n  We present differentially private algorithms that take input data sampled\nfrom a low-dimensional linear subspace (possibly with a small amount of error)\nand output that subspace (or an approximation to it). These algorithms can\nserve as a pre-processing step for other procedures.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 21:09:23 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Singhal", "Vikrant", ""], ["Steinke", "Thomas", ""]]}, {"id": "2106.00075", "submitter": "Antonio Moretti", "authors": "Antonio Khalil Moretti, Liyi Zhang, Christian A. Naesseth, Hadiah\n  Venner, David Blei, Itsik Pe'er", "title": "Variational Combinatorial Sequential Monte Carlo Methods for Bayesian\n  Phylogenetic Inference", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian phylogenetic inference is often conducted via local or sequential\nsearch over topologies and branch lengths using algorithms such as random-walk\nMarkov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).\nHowever, when MCMC is used for evolutionary parameter learning, convergence\nrequires long runs with inefficient exploration of the state space. We\nintroduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful\nframework that establishes variational sequential search to learn distributions\nover intricate combinatorial structures. We then develop nested CSMC, an\nefficient proposal distribution for CSMC and prove that nested CSMC is an exact\napproximation to the (intractable) locally optimal proposal. We use nested CSMC\nto define a second objective, VNCSMC which yields tighter lower bounds than\nVCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore\nhigher probability spaces than existing methods on a range of tasks.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 19:44:24 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 19:23:23 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Moretti", "Antonio Khalil", ""], ["Zhang", "Liyi", ""], ["Naesseth", "Christian A.", ""], ["Venner", "Hadiah", ""], ["Blei", "David", ""], ["Pe'er", "Itsik", ""]]}, {"id": "2106.00177", "submitter": "Colin Fox", "authors": "Colin Fox, Li-Jen Hsiao, Jeong Eun (Kate) Lee", "title": "Solutions of the Multivariate Inverse Frobenius--Perron Problem", "comments": "Submitted to Entropy. v2: a few typos fixed", "journal-ref": null, "doi": "10.3390/e23070838", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the inverse Frobenius--Perron problem: given a prescribed target\ndistribution $\\rho$, find a deterministic map $M$ such that iterations of $M$\ntend to $\\rho$ in distribution. We show that all solutions may be written in\nterms of a factorization that combines the forward and inverse Rosenblatt\ntransformations with a uniform map, that is, a map under which the uniform\ndistribution on the $d$-dimensional hypercube as invariant. Indeed, every\nsolution is equivalent to the choice of a uniform map. We motivate this\nfactorization via $1$-dimensional examples, and then use the factorization to\npresent solutions in $1$ and $2$ dimensions induced by a range of uniform maps.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:52:06 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 23:49:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Fox", "Colin", "", "Kate"], ["Hsiao", "Li-Jen", "", "Kate"], ["Eun", "Jeong", "", "Kate"], ["Lee", "", ""]]}, {"id": "2106.00577", "submitter": "The Tien Mai", "authors": "The Tien Mai", "title": "Efficient adaptive MCMC implementation for Pseudo-Bayesian quantum\n  tomography", "comments": "ongoing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the Pseudo-Bayesian approach to the problem of estimating density\nmatrix in quantum state tomography in this paper. Pseudo-Bayesian inference has\nbeen shown to offer a powerful paradign for quantum tomography with attractive\ntheoretical and empirical results. However, the computation of\n(Pseudo-)Bayesian estimators, due to sampling from complex and high-dimensional\ndistribution, pose significant challenges that hampers their usages in\npractical settings. To overcome this problem, we present an efficient adaptive\nMCMC sampling method for the Pseudo-Bayesian estimator. We show in simulations\nthat our approach is substantially faster than the previous implementation by\nat least two orders of magnitude which is significant for practical quantum\ntomography.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 15:39:17 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Mai", "The Tien", ""]]}, {"id": "2106.00755", "submitter": "Americo Cunha Jr", "authors": "Americo Cunha Jr, Luis Fernando Figueira da Silva", "title": "Assessment of a transient homogeneous reactor through in situ adaptive\n  tabulation", "comments": null, "journal-ref": "Journal of the Brazilian Society of Mechanical Sciences and\n  Engineering, vol. 36, pp. 377, 2014", "doi": "10.1007/s40430-013-0080-4", "report-no": null, "categories": "physics.flu-dyn cs.CE math.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of computational models for the numerical simulation of\nchemically reacting flows operating in the turbulent regime requires the\nsolution of partial differential equations that represent the balance of mass,\nlinear momentum, chemical species, and energy. The chemical reactions of the\nmodel may involve detailed reaction mechanisms for the description of the\nphysicochemical phenomena. One of the biggest challenges is the stiffness of\nthe numerical simulation of these models and the nonlinear nature of species\nrate of reaction. This work presents a study of in situ adaptive tabulation\n(ISAT) technique, focusing on the accuracy, efficiency, and memory usage in the\nsimulation of homogeneous stirred reactor models using simple and complex\nreaction mechanisms. The combustion of carbon monoxide with oxygen and methane\nwith air mixtures are considered, using detailed reaction mechanisms with 4 and\n53 species, 3 and 325 reactions, respectively. The results of these simulations\nindicate that the developed implementation of ISAT technique has a absolute\nglobal error smaller than 1 %. Moreover, ISAT technique provides gains, in\nterms of computational time, of up to 80% when compared with the direct\nintegration of the full chemical kinetics. However, in terms of memory usage\nthe present implementation of ISAT technique is found to be excessively\ndemanding.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:09:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Cunha", "Americo", "Jr"], ["da Silva", "Luis Fernando Figueira", ""]]}, {"id": "2106.00797", "submitter": "Maxime Vono", "authors": "Maxime Vono, Vincent Plassier, Alain Durmus, Aymeric Dieuleveut, Eric\n  Moulines", "title": "QLSD: Quantised Langevin stochastic dynamics for Bayesian federated\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning aims at conducting inference when data are decentralised\nand locally stored on several clients, under two main constraints: data\nownership and communication overhead. In this paper, we address these issues\nunder the Bayesian paradigm. To this end, we propose a novel Markov chain Monte\nCarlo algorithm coined \\texttt{QLSD} built upon quantised versions of\nstochastic gradient Langevin dynamics. To improve performance in a big data\nregime, we introduce variance-reduced alternatives of our methodology referred\nto as \\texttt{QLSD}$^\\star$ and \\texttt{QLSD}$^{++}$. We provide both\nnon-asymptotic and asymptotic convergence guarantees for the proposed\nalgorithms and illustrate their benefits on several federated learning\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:08:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vono", "Maxime", ""], ["Plassier", "Vincent", ""], ["Durmus", "Alain", ""], ["Dieuleveut", "Aymeric", ""], ["Moulines", "Eric", ""]]}, {"id": "2106.01104", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao, Ron D. Frostig, and Hernando Ombao", "title": "Filtrated Common Functional Principal Components for Multivariate\n  Functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local field potentials (LFPs) are signals that measure electrical activity in\nlocalized cortical regions from multiple implanted tetrodes in the human or\nanimal brain. They can be treated as multivariate functional data (i.e., curves\nobserved at many tetrodes spread across a patch on the surface of the cortex).\nMost multivariate functional data contain both global features (which are\nshared in common to all curves) as well isolated features (common only to a\nsmall subset of curves). The goal is this paper is to develop a procedure for\ncapturing this common features. We propose a novel tree-structured functional\nprincipal component (filt-fPC) model through low-dimensional functional\nrepresentation, specifically via filtration. A popular approach to dimension\nreduction of functional data is functional principal components analysis\n(fPCA). Ordinary fPCA can only capture the major information of one population,\nbut fail to reveal the similarity of variation pattern of different groups,\nwhich is potentially related to functional connectivity of brain. One major\nadvantage of the proposed filt-fPC method is the ability to extracting\ncomponents that are common to multiple groups, and meanwhile preserves the\nidiosyncratic individual features of different groups, leading to a\nparsimonious and interpretable low dimensional representation of multivariate\nfunctional data. Another advantage is that the extracted functional principal\ncomponents satisfy the orthonormal property for each set, making filt-fPC\nscores easy to be obtained. The proposed filt-fPC method was employed to study\nthe impact of a shock (induced stroke) on the functional organization structure\nof the rat brain. Finally we point to further directions as this filtration\nidea can also be generalized to other functional statistical models, such as\nfunctional regression, classification and functional times series models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:11:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jiao", "Shuhao", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2106.01712", "submitter": "Jonas Wallin", "authors": "David Bolin and Jonas Wallin", "title": "Efficient methods for Gaussian Markov random fields under sparse linear\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for inference and simulation of linearly constrained Gaussian Markov\nRandom Fields (GMRF) are computationally prohibitive when the number of\nconstraints is large. In some cases, such as for intrinsic GMRFs, they may even\nbe unfeasible. We propose a new class of methods to overcome these challenges\nin the common case of sparse constraints, where one has a large number of\nconstraints and each only involves a few elements. Our methods rely on a basis\ntransformation into blocks of constrained versus non-constrained subspaces, and\nwe show that the methods greatly outperform existing alternatives in terms of\ncomputational cost. By combining the proposed methods with the stochastic\npartial differential equation approach for Gaussian random fields, we also show\nhow to formulate Gaussian process regression with linear constraints in a GMRF\nsetting to reduce computational cost. This is illustrated in two applications\nwith simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:31:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "2106.01713", "submitter": "Bruno Sudret", "authors": "M. Moustapha, S. Marelli and B. Sudret", "title": "A generalized framework for active learning reliability: survey and\n  benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-002", "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning methods have recently surged in the literature due to their\nability to solve complex structural reliability problems within an affordable\ncomputational cost. These methods are designed by adaptively building an\ninexpensive surrogate of the original limit-state function. Examples of such\nsurrogates include Gaussian process models which have been adopted in many\ncontributions, the most popular ones being the efficient global reliability\nanalysis (EGRA) and the active Kriging Monte Carlo simulation (AK-MCS), two\nmilestone contributions in the field. In this paper, we first conduct a survey\nof the recent literature, showing that most of the proposed methods actually\nspan from modifying one or more aspects of the two aforementioned methods. We\nthen propose a generalized modular framework to build on-the-fly efficient\nactive learning strategies by combining the following four ingredients or\nmodules: surrogate model, reliability estimation algorithm, learning function\nand stopping criterion. Using this framework, we devise 39 strategies for the\nsolution of 20 reliability benchmark problems. The results of this extensive\nbenchmark are analyzed under various criteria leading to a synthesized set of\nrecommendations for practitioners. These may be refined with a priori knowledge\nabout the feature of the problem to solve, i.e., dimensionality and magnitude\nof the failure probability. This benchmark has eventually highlighted the\nimportance of using surrogates in conjunction with sophisticated reliability\nestimation algorithms as a way to enhance the efficiency of the latter.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:33:59 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Moustapha", "M.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "2106.01746", "submitter": "Alix Marie d'Avigneau", "authors": "A. Marie d'Avigneau, S. S. Singh, R. J. Ober", "title": "Limits of accuracy for parameter estimation and localisation in\n  Single-Molecule Microscopy via sequential Monte Carlo methods", "comments": "36 pages (inc. 7 pages appendix), 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the quality of parameter estimates for models describing the motion\nof single molecules in cellular environments is an important problem in\nfluorescence microscopy. We consider the fundamental data model, where\nmolecules emit photons at random times and the photons arrive at random\nlocations on the detector according to complex point spread functions (PSFs).\nThe random, non-Gaussian PSF of the detection process and random trajectory of\nthe molecule make inference challenging. Moreover, the presence of other nearby\nmolecules causes further uncertainty in the origin of the measurements, which\nimpacts the statistical precision of estimates. We quantify the limits of\naccuracy of model parameter estimates and separation distance between closely\nspaced molecules (known as the resolution problem) by computing the Cramer-Rao\nlower bound (CRLB), or equivalently the inverse of the Fisher information\nmatrix (FIM), for the variance of estimates. This fundamental CRLB is crucial,\nas it provides a lower bound for more practical scenarios. While analytic\nexpressions for the FIM can be derived for static molecules, the analytical\ntools to evaluate it for molecules whose trajectories follow SDEs are still\nmostly missing. We address this by presenting a general SMC based methodology\nfor both parameter inference and computing the desired accuracy limits for\nnon-static molecules and a non-Gaussian fundamental detection model. For the\nfirst time, we are able to estimate the FIM for stochastically moving molecules\nobserved through the Airy and Born & Wolf PSF. This is achieved by estimating\nthe score and observed information matrix via SMC. We sum up the outcome of our\nnumerical work by summarising the qualitative behaviours for the accuracy\nlimits as functions of e.g. collected photon count, molecule diffusion, etc. We\nalso verify that we can recover known results from the static molecule case.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 10:54:40 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["d'Avigneau", "A. Marie", ""], ["Singh", "S. S.", ""], ["Ober", "R. J.", ""]]}, {"id": "2106.01906", "submitter": "Jingyu He", "authors": "Jingyu He, Nicholas Polson, Jianeng Xu", "title": "Bayesian Inference for Gamma Models", "comments": "Duplicate submission of arXiv:1905.12141 Please check\n  arXiv:1905.12141 for future update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a data\naugmentation scheme for models that include gamma functions. Our methodology\napplies to many situations in statistics and machine learning, including\nMultinomial-Dirichlet distributions, Negative binomial regression,\nPoisson-Gamma hierarchical models, Extreme value models, to name but a few. All\nof those models include a gamma function which does not admit a natural\nconjugate prior distribution providing a significant challenge to inference and\nprediction. To provide a data augmentation strategy, we construct and develop\nthe theory of the class of Exponential Reciprocal Gamma distributions. This\nallows scalable EM and MCMC algorithms to be developed. We illustrate our\nmethodology on a number of examples, including gamma shape inference, negative\nbinomial regression and Dirichlet allocation. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:58:39 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:48:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["He", "Jingyu", ""], ["Polson", "Nicholas", ""], ["Xu", "Jianeng", ""]]}, {"id": "2106.02127", "submitter": "Antik Chakraborty", "authors": "Antik Chakraborty, Rihui Ou, David B. Dunson", "title": "Bayesian inference on high-dimensional multivariate binary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has become increasingly common to collect high-dimensional binary data;\nfor example, with the emergence of new sampling techniques in ecology. In\nsmaller dimensions, multivariate probit (MVP) models are routinely used for\ninferences. However, algorithms for fitting such models face issues in scaling\nup to high dimensions due to the intractability of the likelihood, involving an\nintegral over a multivariate normal distribution having no analytic form.\nAlthough a variety of algorithms have been proposed to approximate this\nintractable integral, these approaches are difficult to implement and/or\ninaccurate in high dimensions. We propose a two-stage Bayesian approach for\ninference on model parameters while taking care of the uncertainty propagation\nbetween the stages. We use the special structure of latent Gaussian models to\nreduce the highly expensive computation involved in joint parameter estimation\nto focus inference on marginal distributions of model parameters. This\nessentially makes the method embarrassingly parallel for both stages. We\nillustrate performance in simulations and applications to joint species\ndistribution modeling in ecology.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:53:41 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chakraborty", "Antik", ""], ["Ou", "Rihui", ""], ["Dunson", "David B.", ""]]}, {"id": "2106.02175", "submitter": "Haoyue Wang", "authors": "Rahul Mazumder, Haoyue Wang", "title": "Linear regression with partially mismatched data: local search with\n  theoretical guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a fundamental modeling tool in statistics and related\nfields. In this paper, we study an important variant of linear regression in\nwhich the predictor-response pairs are partially mismatched. We use an\noptimization formulation to simultaneously learn the underlying regression\ncoefficients and the permutation corresponding to the mismatches. The\ncombinatorial structure of the problem leads to computational challenges. We\npropose and study a simple greedy local search algorithm for this optimization\nproblem that enjoys strong theoretical guarantees and appealing computational\nperformance. We prove that under a suitable scaling of the number of mismatched\npairs compared to the number of samples and features, and certain assumptions\non problem data; our local search algorithm converges to a nearly-optimal\nsolution at a linear rate. In particular, in the noiseless case, our algorithm\nconverges to the global optimal solution with a linear convergence rate. We\nalso propose an approximate local search step that allows us to scale our\napproach to much larger instances. We conduct numerical experiments to gather\nfurther insights into our theoretical results and show promising performance\ngains compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 23:32:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mazumder", "Rahul", ""], ["Wang", "Haoyue", ""]]}, {"id": "2106.02263", "submitter": "Zhengqing Zhou", "authors": "Zhengqing Zhou, Guanyang Wang, Jose Blanchet, Peter W. Glynn", "title": "Unbiased Optimal Stopping via the MUSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new unbiased estimator for estimating the utility of the optimal\nstopping problem. The MUSE, short for `Multilevel Unbiased Stopping Estimator',\nconstructs the unbiased Multilevel Monte Carlo (MLMC) estimator at every stage\nof the optimal stopping problem in a backward recursive way. In contrast to\ntraditional sequential methods, the MUSE can be implemented in parallel when\nmultiple processors are available. We prove the MUSE has finite variance,\nfinite computational complexity, and achieves $\\varepsilon$-accuracy with\n$O(1/\\varepsilon^2)$ computational cost under mild conditions. We demonstrate\nMUSE empirically in several numerical examples, including an option pricing\nproblem with high-dimensional inputs, which illustrates the use of the MUSE on\ncomputer clusters.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 05:00:26 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Zhou", "Zhengqing", ""], ["Wang", "Guanyang", ""], ["Blanchet", "Jose", ""], ["Glynn", "Peter W.", ""]]}, {"id": "2106.02364", "submitter": "Jakob Dambon", "authors": "Jakob A. Dambon, Fabio Sigrist, Reinhard Furrer", "title": "varycoef: An R Package for Gaussian Process-based Spatially Varying\n  Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are well-known tools for modeling dependent data\nwith applications in spatial statistics, time series analysis, or econometrics.\nIn this article, we present the R package varycoef that implements estimation,\nprediction, and variable selection of linear models with spatially varying\ncoefficients (SVC) defined by GPs, so called GP-based SVC models. Such models\noffer a high degree of flexibility while being relatively easy to interpret.\nUsing varycoef, we show versatile applications of (spatially) varying\ncoefficient models on spatial and time series data. This includes model and\ncoefficient estimation with predictions and variable selection. The package\nuses state-of-the-art computational statistics techniques like parallelization,\nmodel-based optimization, and covariance tapering. This allows the user to work\nwith (S)VC models in a computationally efficient manner, i.e., model estimation\non large data sets is possible in a feasible amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:23:04 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Dambon", "Jakob A.", ""], ["Sigrist", "Fabio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2106.02482", "submitter": "Jason Steffener", "authors": "Jason Steffener", "title": "Power of Mediation Effects Using Bootstrap Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mediation analyses are a statistical tool for testing the hypothesis about\nhow the relationship between two variables may be direct or indirect via a\nthird variable. Assessing statistical significance has been an area of active\nresearch; however, assessment of statistical power has been hampered by the\nlack of closed form calculations and the need for substantial amounts of\ncomputational simulations. The current work provides a detailed explanation of\nimplementing large scale simulation procedures within a shared computing\ncluster environment. In addition, all results and code for implementing these\nprocedures is publicly available. The resulting power analyses compare the\neffects of sample size and strength and direction of the relationships between\nthe three variables. Comparisons of three confidence interval calculation\nmethods demonstrated that the bias-corrected method is optimal and requires\napproximately ten less participants than the percentile method to achieve\nequivalent power. Differing strengths of distal and proximal effects were\ncompared and did not differentially affect the power to detect mediation\neffects. Suppression effects were explored and demonstrate that in the presence\nof no observed relationship between two variables, entrance of the mediating\nvariable into the model can reveal a suppressed relationship. The power to\ndetect suppression effects is similar to unsuppressed mediation. These results\nand their methods provide important information about the power of mediation\nmodels for study planning. Of greater importance is that the methods lay the\ngroundwork for assessment of statistical power of more complicated models\ninvolving multiple mediators and moderators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:41:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Steffener", "Jason", ""]]}, {"id": "2106.02686", "submitter": "Michael Lindsey", "authors": "Michael Lindsey, Jonathan Weare, Anna Zhang", "title": "Ensemble Markov chain Monte Carlo with teleporting walkers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an ensemble Markov chain Monte Carlo approach to sampling from a\nprobability density with known likelihood. This method upgrades an underlying\nMarkov chain by allowing an ensemble of such chains to interact via a process\nin which one chain's state is cloned as another's is deleted. This effective\nteleportation of states can overcome issues of metastability in the underlying\nchain, as the scheme enjoys rapid mixing once the modes of the target density\nhave been populated. We derive a mean-field limit for the evolution of the\nensemble. We analyze the global and local convergence of this mean-field limit,\nshowing asymptotic convergence independent of the spectral gap of the\nunderlying Markov chain, and moreover we interpret the limiting evolution as a\ngradient flow. We explain how interaction can be applied selectively to a\nsubset of state variables in order to maintain advantage on very\nhigh-dimensional problems. Finally we present the application of our\nmethodology to Bayesian hyperparameter estimation for Gaussian process\nregression.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 19:52:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lindsey", "Michael", ""], ["Weare", "Jonathan", ""], ["Zhang", "Anna", ""]]}, {"id": "2106.02805", "submitter": "Hua Zhou", "authors": "Kenneth Lange and Joong-Ho Won and Alfonso Landeros and Hua Zhou", "title": "Nonconvex Optimization via MM Algorithms: Convergence Theory", "comments": "33 pages", "journal-ref": "Wiley StatsRef: Statistics Reference Online, 2021", "doi": "10.1002/9781118445112.stat08295", "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majorization-minimization (MM) principle is an extremely general\nframework for deriving optimization algorithms. It includes the\nexpectation-maximization (EM) algorithm, proximal gradient algorithm,\nconcave-convex procedure, quadratic lower bound algorithm, and proximal\ndistance algorithm as special cases. Besides numerous applications in\nstatistics, optimization, and imaging, the MM principle finds wide applications\nin large scale machine learning problems such as matrix completion,\ndiscriminant analysis, and nonnegative matrix factorizations. When applied to\nnonconvex optimization problems, MM algorithms enjoy the advantages of\nconvexifying the objective function, separating variables, numerical stability,\nand ease of implementation. However, compared to the large body of literature\non other optimization algorithms, the convergence analysis of MM algorithms is\nscattered and problem specific. This survey presents a unified treatment of the\nconvergence of MM algorithms. With modern applications in mind, the results\nencompass non-smooth objective functions and non-asymptotic analysis.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 05:19:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lange", "Kenneth", ""], ["Won", "Joong-Ho", ""], ["Landeros", "Alfonso", ""], ["Zhou", "Hua", ""]]}, {"id": "2106.03012", "submitter": "Zhiqiang Tan", "authors": "Zexi Song and Zhiqiang Tan", "title": "On Irreversible Metropolis Sampling Related to Langevin Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been considerable interest in designing Markov chain Monte Carlo\nalgorithms by exploiting numerical methods for Langevin dynamics, which\nincludes Hamiltonian dynamics as a deterministic case. A prominent approach is\nHamiltonian Monte Carlo (HMC), where a leapfrog discretization of Hamiltonian\ndynamics is employed. We investigate a recently proposed class of irreversible\nsampling algorithms, called Hamiltonian assisted Metropolis sampling (HAMS),\nwhich uses an augmented target density similarly as in HMC, but involves a\nflexible proposal scheme and a carefully formulated acceptance-rejection scheme\nto achieve generalized reversibility. We show that as the step size tends to 0,\nthe HAMS proposal satisfies a class of stochastic differential equations\nincluding Langevin dynamics as a special case. We provide theoretical results\nfor HAMS under the univariate Gaussian setting, including the stationary\nvariance, the expected acceptance rate, and the spectral radius. From these\nresults, we derive default choices of tuning parameters for HAMS, such that\nonly the step size needs to be tuned in applications. Various relatively recent\nalgorithms for Langevin dynamics are also shown to fall in the class of HAMS\nproposals up to negligible differences. Our numerical experiments on sampling\nhigh-dimensional latent variables confirm that the HAMS algorithms consistently\nachieve superior performance, compared with several Metropolis-adjusted\nalgorithms based on popular integrators of Langevin dynamics.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 02:35:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Song", "Zexi", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "2106.03235", "submitter": "Sebastian Ament", "authors": "Sebatian Ament and Carla Gomes", "title": "On the Optimality of Backward Regression: Sparse Recovery and Subset\n  Selection", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9415082", "report-no": null, "categories": "math.OC cs.IT cs.NA math.IT math.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse recovery and subset selection are fundamental problems in varied\ncommunities, including signal processing, statistics and machine learning.\nHerein, we focus on an important greedy algorithm for these problems: Backward\nStepwise Regression. We present novel guarantees for the algorithm, propose an\nefficient, numerically stable implementation, and put forth Stepwise Regression\nwith Replacement (SRR), a new family of two-stage algorithms that employs both\nforward and backward steps for compressed sensing problems. Prior work on the\nbackward algorithm has proven its optimality for the subset selection problem,\nprovided the residual associated with the optimal solution is small enough.\nHowever, the existing bounds on the residual magnitude are NP-hard to compute.\nIn contrast, our main theoretical result includes a bound that can be computed\nin polynomial time, depends chiefly on the smallest singular value of the\nmatrix, and also extends to the method of magnitude pruning. In addition, we\nreport numerical experiments highlighting crucial differences between forward\nand backward greedy algorithms and compare SRR against popular two-stage\nalgorithms for compressed sensing. Remarkably, SRR algorithms generally\nmaintain good sparse recovery performance on coherent dictionaries. Further, a\nparticular SRR algorithm has an edge over Subspace Pursuit.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:12:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ament", "Sebatian", ""], ["Gomes", "Carla", ""]]}, {"id": "2106.03431", "submitter": "Mathias H{\\o}jgaard Jensen", "authors": "Mathias H{\\o}jgaard Jensen and Sarang Joshi and Stefan Sommer", "title": "Bridge Simulation and Metric Estimation on Lie Groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a simulation scheme for simulating Brownian bridges on complete\nand connected Lie groups. We show how this simulation scheme leads to absolute\ncontinuity of the Brownian bridge measure with respect to the guided process\nmeasure. This result generalizes the Euclidean result of Delyon and Hu to Lie\ngroups. We present numerical results of the guided process in the Lie group\n$\\SO(3)$. In particular, we apply importance sampling to estimate the metric on\n$\\SO(3)$ using an iterative maximum likelihood method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:59:20 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Jensen", "Mathias H\u00f8jgaard", ""], ["Joshi", "Sarang", ""], ["Sommer", "Stefan", ""]]}, {"id": "2106.03823", "submitter": "Michael O'Malley", "authors": "Michael O'Malley, Adam M. Sykulski, Rick Lumpkin, Alejandro Schuler", "title": "Multivariate Probabilistic Regression with Natural Gradient Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many single-target regression problems require estimates of uncertainty along\nwith the point predictions. Probabilistic regression algorithms are well-suited\nfor these tasks. However, the options are much more limited when the prediction\ntarget is multivariate and a joint measure of uncertainty is required. For\nexample, in predicting a 2D velocity vector a joint uncertainty would quantify\nthe probability of any vector in the plane, which would be more expressive than\ntwo separate uncertainties on the x- and y- components. To enable joint\nprobabilistic regression, we propose a Natural Gradient Boosting (NGBoost)\napproach based on nonparametrically modeling the conditional parameters of the\nmultivariate predictive distribution. Our method is robust, works\nout-of-the-box without extensive tuning, is modular with respect to the assumed\ntarget distribution, and performs competitively in comparison to existing\napproaches. We demonstrate these claims in simulation and with a case study\npredicting two-dimensional oceanographic velocity data. An implementation of\nour method is available at https://github.com/stanfordmlgroup/ngboost.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:44:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["O'Malley", "Michael", ""], ["Sykulski", "Adam M.", ""], ["Lumpkin", "Rick", ""], ["Schuler", "Alejandro", ""]]}, {"id": "2106.04170", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Sergey Dolgov and Olivier Zahm", "title": "Conditional Deep Inverse Rosenblatt Transports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel offline-online method to mitigate the computational burden\nof the characterization of conditional beliefs in statistical learning. In the\noffline phase, the proposed method learns the joint law of the belief random\nvariables and the observational random variables in the tensor-train (TT)\nformat. In the online phase, it utilizes the resulting order-preserving\nconditional transport map to issue real-time characterization of the\nconditional beliefs given new observed information. Compared with the\nstate-of-the-art normalizing flows techniques, the proposed method relies on\nfunction approximation and is equipped with thorough performance analysis. This\nalso allows us to further extend the capability of transport maps in\nchallenging problems with high-dimensional observations and high-dimensional\nbelief variables. On the one hand, we present novel heuristics to reorder\nand/or reparametrize the variables to enhance the approximation power of TT. On\nthe other, we integrate the TT-based transport maps and the parameter\nreordering/reparametrization into layered compositions to further improve the\nperformance of the resulting transport maps. We demonstrate the efficiency of\nthe proposed method on various statistical learning tasks in ordinary\ndifferential equations (ODEs) and partial differential equations (PDEs).\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:23:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cui", "Tiangang", ""], ["Dolgov", "Sergey", ""], ["Zahm", "Olivier", ""]]}, {"id": "2106.04255", "submitter": "Lily Wang", "authors": "Xinyi Li, Shan Yu, Yueying Wang, Guannan Wang, Li Wang", "title": "Spline Smoothing of 3D Geometric Data", "comments": "43 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past two decades, we have seen an increased demand for 3D\nvisualization and simulation software in medicine, architectural design,\nengineering, and many other areas, which have boosted the investigation of\ngeometric data analysis and raised the demand for further advancement in\nstatistical analytic approaches. In this paper, we propose a class of spline\nsmoother appropriate for approximating geometric data over 3D complex domains,\nwhich can be represented in terms of a linear combination of spline basis\nfunctions with some smoothness constraints. We start with introducing the\ntetrahedral partitions, Barycentric coordinates, Bernstein basis polynomials,\nand trivariate spline on tetrahedra. Then, we propose a penalized spline\nsmoothing method for identifying the underlying signal in a complex 3D domain\nfrom potential noisy observations. Simulation studies are conducted to compare\nthe proposed method with traditional smoothing methods on 3D complex domains.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:29:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Xinyi", ""], ["Yu", "Shan", ""], ["Wang", "Yueying", ""], ["Wang", "Guannan", ""], ["Wang", "Li", ""]]}, {"id": "2106.04453", "submitter": "Max Ehre", "authors": "Max Ehre, Iason Papaioannou, Bruno Sudret, Daniel Straub", "title": "Sequential active learning of low-dimensional model representations for\n  reliability analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, the analysis of high-dimensional, computationally expensive\nengineering models remains a difficult challenge in risk and reliability\nengineering. We use a combination of dimensionality reduction and surrogate\nmodelling termed partial least squares-driven polynomial chaos expansion\n(PLS-PCE) to render such problems feasible. Standalone surrogate models\ntypically perform poorly for reliability analysis. Therefore, in a previous\nwork, we have used PLS-PCEs to reconstruct the intermediate densities of a\nsequential importance sampling approach to reliability analysis. Here, we\nextend this approach with an active learning procedure that allows for improved\nerror control at each importance sampling level. To this end, we formulate an\nestimate of the combined estimation error for both the subspace identified in\nthe dimension reduction step and surrogate model constructed therein. With\nthis, it is possible to adapt the design of experiments so as to optimally\nlearn the subspace representation and the surrogate model constructed therein.\nThe approach is gradient-free and thus can be directly applied to black\nbox-type models. We demonstrate the performance of this approach with a series\nof low- (2 dimensions) to high- (869 dimensions) dimensional example problems\nfeaturing a number of well-known caveats for reliability methods besides high\ndimensions and expensive computational models: strongly nonlinear limit-state\nfunctions, multiple relevant failure regions and small probabilities of\nfailure.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:37:53 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ehre", "Max", ""], ["Papaioannou", "Iason", ""], ["Sudret", "Bruno", ""], ["Straub", "Daniel", ""]]}, {"id": "2106.04636", "submitter": "Andrew Chia", "authors": "Andrew Chia", "title": "Automatically Differentiable Random Coefficient Logistic Demand\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.DC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the random coefficient logistic demand (BLP) model can be phrased\nas an automatically differentiable moment function, including the incorporation\nof numerical safeguards proposed in the literature. This allows gradient-based\nfrequentist and quasi-Bayesian estimation using the Continuously Updating\nEstimator (CUE). Drawing from the machine learning literature, we outline\nhitherto under-utilized best practices in both frequentist and Bayesian\nestimation techniques. Our Monte Carlo experiments compare the performance of\nCUE, 2S-GMM, and LTE estimation. Preliminary findings indicate that the CUE\nestimated using LTE and frequentist optimization has a lower bias but higher\nMAE compared to the traditional 2-Stage GMM (2S-GMM) approach. We also find\nthat using credible intervals from MCMC sampling for the non-linear parameters\ntogether with frequentist analytical standard errors for the concentrated out\nlinear parameters provides empirical coverage closest to the nominal level. The\naccompanying admest Python package provides a platform for replication and\nextensibility.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 18:50:11 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Chia", "Andrew", ""]]}, {"id": "2106.04888", "submitter": "Yiling Jiang", "authors": "Shasha Liua, Yiling Jianga, Ronggui Lua, Xu Cheng, Jia Lia, Yang Chen,\n  Gaofeng Tian", "title": "Cellular Automata Simulation of Grain Growth of Powder Metallurgy\n  Nickel-Based Superalloy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Primary {\\gamma}' phase instead of carbides and borides plays an important\nrole in suppressing grain growth during solution at 1433K of FGH98 nickel-based\npolycrystalline alloys. Results illustrate that as-fabricated FGH98 has\nequiaxed grain structure and after heat treatment, grains remain equiaxed but\ngrow larger. In order to clarify the effects of the size and volume fraction of\nthe primary {\\gamma}' phase on the grain growth during heat treatment, this\npaper establish a 2D Cellular Automata (CA) model based on the thermal\nactivation and the lowest energy principle. The CA results are compared with\nthe experimental results and show a good fit with an error less than 10%. Grain\ngrowth kinetics are depicted and simulations in real time for various sizes and\nvolume fractions of primary {\\gamma}' particles work out well with the Zener\nrelation. The coefficient n value in Zener relation is theoretically calculated\nand its minimum value is 0.23 when the radius of primary {\\gamma}' is\n2.8{\\mu}m.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 08:13:30 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liua", "Shasha", ""], ["Jianga", "Yiling", ""], ["Lua", "Ronggui", ""], ["Cheng", "Xu", ""], ["Lia", "Jia", ""], ["Chen", "Yang", ""], ["Tian", "Gaofeng", ""]]}, {"id": "2106.04997", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (1), David R. Hunter (2), Martina Morris (3), Chad\n  Klumb (3) ((1) University of New South Wales, (2) Penn State University, (3)\n  University of Washington)", "title": "ergm 4.0: New features and improvements", "comments": "52 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ergm package supports the statistical analysis and simulation of network\ndata. It anchors the statnet suite of packages for network analysis in R\nintroduced in a special issue in Journal of Statistical Software in 2008. This\narticle provides an overview of the functionality and performance improvements\nin the 2021 ergm 4.0 release. These include more flexible handling of nodal\ncovariates, operator terms that extend and simplify model specification, new\nmodels for networks with valued edges, improved handling of constraints on the\nsample space of networks, performance enhancements to the Markov chain Monte\nCarlo and maximum likelihood estimation algorithms, broader and faster\nsearching for networks with certain target statistics using simulated\nannealing, and estimation with missing edge data. We also identify the new\npackages in the statnet suite that extend ergm's functionality to other network\ndata types and structural features, and the robust set of online resources that\nsupport the statnet development process and applications.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 11:33:49 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Krivitsky", "Pavel N.", ""], ["Hunter", "David R.", ""], ["Morris", "Martina", ""], ["Klumb", "Chad", ""]]}, {"id": "2106.05204", "submitter": "Yili Hong", "authors": "Khaled F. Bedair and Yili Hong and Hussein R. Al-Khalidi", "title": "Copula-Frailty Models for Recurrent Event Data Based on Monte Carlo EM\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-type recurrent events are often encountered in medical applications\nwhen two or more different event types could repeatedly occur over an\nobservation period. For example, patients may experience recurrences of\nmulti-type nonmelanoma skin cancers in a clinical trial for skin cancer\nprevention. The aims in those applications are to characterize features of the\nmarginal processes, evaluate covariate effects, and quantify both the\nwithin-subject recurrence dependence and the dependence among different event\ntypes. We use copula-frailty models to analyze correlated recurrent events of\ndifferent types. Parameter estimation and inference are carried out by using a\nMonte Carlo expectation-maximization (MCEM) algorithm, which can handle a\nrelatively large (i.e., three or more) number of event types. Performances of\nthe proposed methods are evaluated via extensive simulation studies. The\ndeveloped methods are used to model the recurrences of skin cancer with\ndifferent types.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:46:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bedair", "Khaled F.", ""], ["Hong", "Yili", ""], ["Al-Khalidi", "Hussein R.", ""]]}, {"id": "2106.05396", "submitter": "Naoufal Acharki", "authors": "Naoufal Acharki and Antoine Bertoncello and Josselin Garnier", "title": "Robust Prediction Interval estimation for Gaussian Processes by\n  Cross-Validation method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic regression models typically use the Maximum Likelihood\nEstimation or Cross-Validation to fit parameters. Unfortunately, these methods\nmay give advantage to the solutions that fit observations in average, but they\ndo not pay attention to the coverage and the width of Prediction Intervals. In\nthis paper, we address the question of adjusting and calibrating Prediction\nIntervals for Gaussian Processes Regression. First we determine the model's\nparameters by a standard Cross-Validation or Maximum Likelihood Estimation\nmethod then we adjust the parameters to assess the optimal type II Coverage\nProbability to a nominal level. We apply a relaxation method to choose\nparameters that minimize the Wasserstein distance between the Gaussian\ndistribution of the initial parameters (Cross-Validation or Maximum Likelihood\nEstimation) and the proposed Gaussian distribution among the set of parameters\nthat achieved the desired Coverage Probability.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:19:08 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Acharki", "Naoufal", ""], ["Bertoncello", "Antoine", ""], ["Garnier", "Josselin", ""]]}, {"id": "2106.05807", "submitter": "Minh-Ngoc Tran", "authors": "Anna Lopatnikova and Minh-Ngoc Tran", "title": "Quantum Natural Gradient for Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Bayes (VB) is a critical method in machine learning and\nstatistics, underpinning the recent success of Bayesian deep learning. The\nnatural gradient is an essential component of efficient VB estimation, but it\nis prohibitively computationally expensive in high dimensions. We propose a\nhybrid quantum-classical algorithm to improve the scaling properties of natural\ngradient computation and make VB a truly computationally efficient method for\nBayesian inference in highdimensional settings. The algorithm leverages matrix\ninversion from the linear systems algorithm by Harrow, Hassidim, and Lloyd\n[Phys. Rev. Lett. 103, 15 (2009)] (HHL). We demonstrate that the matrix to be\ninverted is sparse and the classical-quantum-classical handoffs are\nsufficiently economical to preserve computational efficiency, making the\nproblem of natural gradient for VB an ideal application of HHL. We prove that,\nunder standard conditions, the VB algorithm with quantum natural gradient is\nguaranteed to converge. Our regression-based natural gradient formulation is\nalso highly useful for classical VB.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:15:07 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 16:46:58 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Lopatnikova", "Anna", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "2106.05820", "submitter": "Murray Pollock", "authors": "Paul A. Jenkins, Murray Pollock and Gareth O. Roberts", "title": "Bayesian semi-parametric inference for diffusion processes using splines", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-parametric method to simultaneously infer both the drift\nand volatility functions of a discretely observed scalar diffusion. We\nintroduce spline bases to represent these functions and develop a Markov chain\nMonte Carlo algorithm to infer, a posteriori, the coefficients of these\nfunctions in the spline basis. A key innovation is that we use spline bases to\nmodel transformed versions of the drift and volatility functions rather than\nthe functions themselves. The output of the algorithm is a posterior sample of\nplausible drift and volatility functions that are not constrained to any\nparticular parametric family. The flexibility of this approach provides\npractitioners a powerful investigative tool, allowing them to posit parametric\nmodels to better capture the underlying dynamics of their processes of\ninterest. We illustrate the versatility of our method by applying it to\nchallenging datasets from finance, paleoclimatology, and astrophysics. In view\nof the parametric diffusion models widely employed in the literature for those\nexamples, some of our results are surprising since they call into question some\naspects of these models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:37:38 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2106.05824", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, S. Marelli, I. Papaioannou, D. Straub, B. Sudret", "title": "Rare event estimation using stochastic spectral embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-003", "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the probability of rare failure events is an essential step in the\nreliability assessment of engineering systems. Computing this failure\nprobability for complex non-linear systems is challenging, and has recently\nspurred the development of active-learning reliability methods. These methods\napproximate the limit-state function (LSF) using surrogate models trained with\na sequentially enriched set of model evaluations. A recently proposed method\ncalled stochastic spectral embedding (SSE) aims to improve the local\napproximation accuracy of global, spectral surrogate modelling techniques by\nsequentially embedding local residual expansions in subdomains of the input\nspace. In this work we apply SSE to the LSF, giving rise to a stochastic\nspectral embedding-based reliability (SSER) method. The resulting partition of\nthe input space decomposes the failure probability into a set of\neasy-to-compute domain-wise failure probabilities. We propose a set of\nmodifications that tailor the algorithm to efficiently solve rare event\nestimation problems. These modifications include specialized refinement domain\nselection, partitioning and enrichment strategies. We showcase the algorithm\nperformance on four benchmark problems of various dimensionality and complexity\nin the LSF.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:10:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wagner", "P. -R.", ""], ["Marelli", "S.", ""], ["Papaioannou", "I.", ""], ["Straub", "D.", ""], ["Sudret", "B.", ""]]}, {"id": "2106.05828", "submitter": "Housen Li", "authors": "Markus Haltmeier and Housen Li and Axel Munk", "title": "A Variational View on Statistical Multiscale Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying view on various statistical estimation techniques\nincluding penalization, variational and thresholding methods. These estimators\nwill be analyzed in the context of statistical linear inverse problems\nincluding nonparametric and change point regression, and high dimensional\nlinear models as examples. Our approach reveals many seemingly unrelated\nestimation schemes as special instances of a general class of variational\nmultiscale estimators, named MIND (MultIscale Nemirovskii--Dantzig). These\nestimators result from minimizing certain regularization functionals under\nconvex constraints that can be seen as multiple statistical tests for local\nhypotheses.\n  For computational purposes, we recast MIND in terms of simpler unconstraint\noptimization problems via Lagrangian penalization as well as Fenchel duality.\nPerformance of several MINDs is demonstrated on numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:47:16 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Haltmeier", "Markus", ""], ["Li", "Housen", ""], ["Munk", "Axel", ""]]}, {"id": "2106.05840", "submitter": "Mian Adnan", "authors": "Mian Arif Shams Adnan, H. M. Miraz Mahmud", "title": "A Bagging and Boosting Based Convexly Combined Optimum Mixture\n  Probabilistic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unlike previous studies on mixture distributions, a bagging and boosting\nbased convexly combined mixture probabilistic model has been suggested. This\nmodel is a result of iteratively searching for obtaining the optimum\nprobabilistic model that provides the maximum p value.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:20:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Adnan", "Mian Arif Shams", ""], ["Mahmud", "H. M. Miraz", ""]]}, {"id": "2106.05944", "submitter": "Crist\\'obal Guzm\\'an", "authors": "Santiago Armstrong, Crist\\'obal Guzm\\'an, Carlos A. Sing-Long", "title": "An Optimal Algorithm for Strict Circular Seriation", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of circular seriation, where we are given a matrix of\npairwise dissimilarities between $n$ objects, and the goal is to find a {\\em\ncircular order} of the objects in a manner that is consistent with their\ndissimilarity. This problem is a generalization of the classical {\\em linear\nseriation} problem where the goal is to find a {\\em linear order}, and for\nwhich optimal ${\\cal O}(n^2)$ algorithms are known. Our contributions can be\nsummarized as follows. First, we introduce {\\em circular Robinson matrices} as\nthe natural class of dissimilarity matrices for the circular seriation problem.\nSecond, for the case of {\\em strict circular Robinson dissimilarity matrices}\nwe provide an optimal ${\\cal O}(n^2)$ algorithm for the circular seriation\nproblem. Finally, we propose a statistical model to analyze the well-posedness\nof the circular seriation problem for large $n$. In particular, we establish\n${\\cal O}(\\log(n)/n)$ rates on the distance between any circular ordering found\nby solving the circular seriation problem to the underlying order of the model,\nin the Kendall-tau metric.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 17:42:50 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Armstrong", "Santiago", ""], ["Guzm\u00e1n", "Crist\u00f3bal", ""], ["Sing-Long", "Carlos A.", ""]]}, {"id": "2106.06095", "submitter": "Sebastian Ament", "authors": "Sebastian Ament and Carla Gomes", "title": "Sparse Bayesian Learning via Stepwise Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Bayesian Learning (SBL) is a powerful framework for attaining sparsity\nin probabilistic models. Herein, we propose a coordinate ascent algorithm for\nSBL termed Relevance Matching Pursuit (RMP) and show that, as its noise\nvariance parameter goes to zero, RMP exhibits a surprising connection to\nStepwise Regression. Further, we derive novel guarantees for Stepwise\nRegression algorithms, which also shed light on RMP. Our guarantees for Forward\nRegression improve on deterministic and probabilistic results for Orthogonal\nMatching Pursuit with noise. Our analysis of Backward Regression on determined\nsystems culminates in a bound on the residual of the optimal solution to the\nsubset selection problem that, if satisfied, guarantees the optimality of the\nresult. To our knowledge, this bound is the first that can be computed in\npolynomial time and depends chiefly on the smallest singular value of the\nmatrix. We report numerical experiments using a variety of feature selection\nalgorithms. Notably, RMP and its limiting variant are both efficient and\nmaintain strong performance with correlated features.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 00:20:27 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Ament", "Sebastian", ""], ["Gomes", "Carla", ""]]}, {"id": "2106.06137", "submitter": "Edwin Fong", "authors": "Edwin Fong, Chris Holmes", "title": "Conformal Bayesian Computation", "comments": "19 pages, 4 figures, 12 tables; added references and fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop scalable methods for producing conformal Bayesian predictive\nintervals with finite sample calibration guarantees. Bayesian posterior\npredictive distributions, $p(y \\mid x)$, characterize subjective beliefs on\noutcomes of interest, $y$, conditional on predictors, $x$. Bayesian prediction\nis well-calibrated when the model is true, but the predictive intervals may\nexhibit poor empirical coverage when the model is misspecified, under the so\ncalled ${\\cal{M}}$-open perspective. In contrast, conformal inference provides\nfinite sample frequentist guarantees on predictive confidence intervals without\nthe requirement of model fidelity. Using 'add-one-in' importance sampling, we\nshow that conformal Bayesian predictive intervals are efficiently obtained from\nre-weighted posterior samples of model parameters. Our approach contrasts with\nexisting conformal methods that require expensive refitting of models or\ndata-splitting to achieve computational efficiency. We demonstrate the utility\non a range of examples including extensions to partially exchangeable settings\nsuch as hierarchical models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:49:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:16:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fong", "Edwin", ""], ["Holmes", "Chris", ""]]}, {"id": "2106.06271", "submitter": "Joaqu\\'in M\\'iguez", "authors": "Albert L\\'opez-Yela and Joaquin Miguez", "title": "Polynomial propagation of moments in stochastic differential equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the problem of approximating the moments of the solution,\n$\\boldsymbol{X}(t)$, of an It\\^o stochastic differential equation (SDE) with\ndrift and a diffusion terms over a time-grid $t_0, t_1, \\ldots, t_n$. In\nparticular, we assume an explicit numerical scheme for the generation of sample\npaths $\\hat{\\boldsymbol{X}}(t_0), \\ldots, \\hat{\\boldsymbol{X}}(t_n), \\ldots$\nand then obtain recursive equations that yield any desired non-central moment\nof $\\hat{\\boldsymbol{X}}(t_n)$ as a function of the initial condition\n$\\boldsymbol{X}_0$. The core of the methodology is the decomposition of the\nnumerical solution into a \"central part\" and an \"effective noise\" term. The\ncentral term is computed deterministically from the ordinary differential\nequation (ODE) that results from eliminating the diffusion term in the SDE,\nwhile the effective noise accounts for the stochastic deviation from the\nnumerical solution of the ODE. For simplicity, we describe algorithms based on\nan Euler-Maruyama integrator, but other explicit numerical schemes can be\nexploited in the same way. We also apply the moment approximations to construct\nestimates of the 1-dimensional marginal probability density functions of\n$\\hat{\\boldsymbol{X}}(t_n)$ based on a Gram-Charlier expansion. Both for the\napproximation of moments and 1-dimensional densities, we describe how to handle\nthe cases in which the initial condition is fixed (i.e., $\\boldsymbol{X}_0 =\n\\boldsymbol{x}_0$ for some known $\\boldsymbol{x_0}$) or random. In the latter\ncase, we resort to polynomial chaos expansion (PCE) schemes to approximate the\ntarget moments. The methodology has been inspired by the PCE and differential\nalgebra (DA) methods used for uncertainty propagation in astrodynamics\nproblems. Hence, we illustrate its application for the quantification of\nuncertainty in a 2-dimensional Keplerian orbit perturbed by a Wiener noise\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 09:37:52 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["L\u00f3pez-Yela", "Albert", ""], ["Miguez", "Joaquin", ""]]}, {"id": "2106.06300", "submitter": "Maxime Vono", "authors": "Vincent Plassier, Maxime Vono, Alain Durmus and Eric Moulines", "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm\n  via Langevin Monte Carlo within Gibbs", "comments": "77 pages. Accepted for publication at ICML 2021, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing reliable Bayesian inference on a big data scale is becoming a\nkeystone in the modern era of machine learning. A workhorse class of methods to\nachieve this task are Markov chain Monte Carlo (MCMC) algorithms and their\ndesign to handle distributed datasets has been the subject of many works.\nHowever, existing methods are not completely either reliable or computationally\nefficient. In this paper, we propose to fill this gap in the case where the\ndataset is partitioned and stored on computing nodes within a cluster under a\nmaster/slaves architecture. We derive a user-friendly centralised distributed\nMCMC algorithm with provable scaling in high-dimensional settings. We\nillustrate the relevance of the proposed methodology on both synthetic and real\ndata experiments.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:37:14 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 14:21:51 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Plassier", "Vincent", ""], ["Vono", "Maxime", ""], ["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "2106.06430", "submitter": "Laura M. Wolf", "authors": "Laura M. Wolf and Marcus Baum", "title": "Continuous Herded Gibbs Sampling", "comments": "6 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Herding is a technique to sequentially generate deterministic samples from a\nprobability distribution. In this work, we propose a continuous herded Gibbs\nsampler, that combines kernel herding on continuous densities with Gibbs\nsampling. Our algorithm allows for deterministically sampling from\nhigh-dimensional multivariate probability densities, without directly sampling\nfrom the joint density. Experiments with Gaussian mixture densities indicate\nthat the L2 error decreases similarly to kernel herding, while the computation\ntime is significantly lower, i.e., linear in the number of dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 14:37:40 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Wolf", "Laura M.", ""], ["Baum", "Marcus", ""]]}, {"id": "2106.06510", "submitter": "Soumya Ghosh", "authors": "William T. Stephenson, Soumya Ghosh, Tin D. Nguyen, Mikhail Yurochkin,\n  Sameer K. Deshpande, Tamara Broderick", "title": "Measuring the sensitivity of Gaussian processes to kernel choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are used to make medical and scientific decisions,\nincluding in cardiac care and monitoring of carbon dioxide emissions. But the\nchoice of GP kernel is often somewhat arbitrary. In particular, uncountably\nmany kernels typically align with qualitative prior knowledge (e.g. function\nsmoothness or stationarity). But in practice, data analysts choose among a\nhandful of convenient standard kernels (e.g. squared exponential). In the\npresent work, we ask: Would decisions made with a GP differ under other,\nqualitatively interchangeable kernels? We show how to formulate this\nsensitivity analysis as a constrained optimization problem over a\nfinite-dimensional space. We can then use standard optimizers to identify\nsubstantive changes in relevant decisions made with a GP. We demonstrate in\nboth synthetic and real-world examples that decisions made with a GP can\nexhibit substantial sensitivity to kernel choice, even when prior draws are\nqualitatively interchangeable to a user.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:09:53 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Stephenson", "William T.", ""], ["Ghosh", "Soumya", ""], ["Nguyen", "Tin D.", ""], ["Yurochkin", "Mikhail", ""], ["Deshpande", "Sameer K.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2106.06568", "submitter": "Adam Loy", "authors": "Adam Loy and Jenna Korobova", "title": "Bootstrapping Clustered Data in R using lmeresampler", "comments": "15 pages, 3 figures, 2 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed-effects models are commonly used to analyze clustered data\nstructures. There are numerous packages to fit these models in R and conduct\nlikelihood-based inference. The implementation of resampling-based procedures\nfor inference are more limited. In this paper, we introduce the lmeresampler\npackage for bootstrapping nested linear mixed-effects models fit via lme4 or\nnlme. Bootstrap estimation allows for bias correction, adjusted standard errors\nand confidence intervals for small samples sizes and when distributional\nassumptions break down. We will also illustrate how bootstrap resampling can be\nused to diagnose this model class. In addition, lmeresampler makes it easy to\nconstruct interval estimates of functions of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:39:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Loy", "Adam", ""], ["Korobova", "Jenna", ""]]}, {"id": "2106.06608", "submitter": "Nhat Ho", "authors": "Nhat Ho, Stephen G. Walker", "title": "Statistical Analysis from the Fourier Integral Theorem", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the Fourier integral theorem as our starting point, in this paper we\nfocus on natural Monte Carlo and fully nonparametric estimators of multivariate\ndistributions and conditional distribution functions. We do this without the\nneed for any estimated covariance matrix or dependence structure between\nvariables. These aspects arise immediately from the integral theorem. Being\nable to model multivariate data sets using conditional distribution functions\nwe can study a number of problems, such as prediction for Markov processes,\nestimation of mixing distribution functions which depend on covariates, and\ngeneral multivariate data. Estimators are explicit Monte Carlo based and\nrequire no recursive or iterative algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:44:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2106.06787", "submitter": "Hwanwoo Kim", "authors": "John Harlim, Shixiao Jiang, Hwanwoo Kim, Daniel Sanz-Alonso", "title": "Graph-based Prior and Forward Models for Inverse Problems on Manifolds\n  with Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops manifold learning techniques for the numerical solution\nof PDE-constrained Bayesian inverse problems on manifolds with boundaries. We\nintroduce graphical Mat\\'ern-type Gaussian field priors that enable flexible\nmodeling near the boundaries, representing boundary values by superposition of\nharmonic functions with appropriate Dirichlet boundary conditions. We also\ninvestigate the graph-based approximation of forward models from PDE parameters\nto observed quantities. In the construction of graph-based prior and forward\nmodels, we leverage the ghost point diffusion map algorithm to approximate\nsecond-order elliptic operators with classical boundary conditions. Numerical\nresults validate our graph-based approach and demonstrate the need to design\nprior covariance models that account for boundary conditions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 14:41:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Harlim", "John", ""], ["Jiang", "Shixiao", ""], ["Kim", "Hwanwoo", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "2106.06902", "submitter": "Shouto Yonekura", "authors": "Shouto Yonekura and Shonosuke Sugasawa", "title": "Adaptation of the Tuning Parameter in General Bayesian Inference with\n  Robust Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for robust Bayesian estimation with robust\ndivergence (e.g., density power divergence or {\\gamma}-divergence), indexed by\na single tuning parameter. It is well known that the posterior density induced\nby robust divergence gives highly robust estimators against outliers if the\ntuning parameter is appropriately and carefully chosen. In a Bayesian\nframework, one way to find the optimal tuning parameter would be using evidence\n(marginal likelihood). However, we numerically illustrate that evidence induced\nby the density power divergence does not work to select the optimal tuning\nparameter since robust divergence is not regarded as a statistical model. To\novercome the problems, we treat the exponential of robust divergence as an\nunnormalized statistical model, and we estimate the tuning parameter via\nminimizing the Hyvarinen score. We also provide adaptive computational methods\nbased on sequential Monte Carlo (SMC) samplers, which enables us to obtain the\noptimal tuning parameter and samples from posterior distributions\nsimultaneously. The empirical performance of the proposed method through\nsimulations and an application to real data are also provided.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 03:25:37 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 17:25:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yonekura", "Shouto", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2106.07462", "submitter": "Hanwen Xing", "authors": "Hanwen Xing", "title": "Improving Bridge estimators via $f$-GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bridge sampling is a powerful Monte Carlo method for estimating ratios of\nnormalizing constants. Various methods have been introduced to improve its\nefficiency. These methods aim to increase the overlap between the densities by\napplying appropriate transformations to them without changing their normalizing\nconstants. In this paper, we first give a new estimator of the asymptotic\nrelative mean square error (RMSE) of the optimal Bridge estimator by\nequivalently estimating an $f$-divergence between the two densities. We then\nutilize this framework and propose $f$-GAN-Bridge estimator ($f$-GB) based on a\nbijective transformation that maps one density to the other and minimizes the\nasymptotic RMSE of the optimal Bridge estimator with respect to the densities.\nThis transformation is chosen by minimizing a specific $f$-divergence between\nthe densities using an $f$-GAN. We show $f$-GB is optimal in the sense that\nwithin any given set of candidate transformations, the $f$-GB estimator can\nasymptotically achieve an RMSE lower than or equal to that achieved by Bridge\nestimators based on any other transformed densities. Numerical experiments show\nthat $f$-GB outperforms existing methods in simulated and real-world examples.\nIn addition, we discuss how Bridge estimators naturally arise from the problem\nof $f$-divergence estimation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:40:29 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 17:24:43 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Xing", "Hanwen", ""]]}, {"id": "2106.07478", "submitter": "Inder Tecuapetla-G\\'omez", "authors": "Inder Tecuapetla-G\\'omez and Julia Trinidad Reyes", "title": "Modeling satellite-based open water fraction via flexible Beta\n  regression: An application to wetlands in the north-western Pacific coast of\n  Mexico", "comments": "5 pages, 5 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Carbon sequestration and water filtering are two examples of the several\necosystem services provided by wetlands. Open water mapping is an effective\nmeans to measure any wetland extension as these are comprised of many open\nwater bodies. An economical, though indirect, approach towards mapping open\nwater bodies is through applying geo-computational methods to satellite images.\nIn this work we propose the flexible Beta regression (FBR) model to predict\nopen water fraction from measurements of a water index. We focus on\nobservations derived from two MODIS images acquired during the dry season of\n2008 in Marismas Nacionales, a wetland located in the north-western Pacific\ncoast of Mexico. A Bayesian estimation procedure is presented to estimate the\nFBR model; in particular, we provide details of a nested Metropolis-Hastings\nand Gibbs sampling algorithm to carry out parameter estimation. Our results\nshow that the FBR model produces valid predictors of water fraction unlike the\nstandard model. Our work is complemented by software developed in the R\nlanguage and available through a GitHub repository.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:06:24 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Tecuapetla-G\u00f3mez", "Inder", ""], ["Reyes", "Julia Trinidad", ""]]}, {"id": "2106.07523", "submitter": "Robin Evans", "authors": "Robin J. Evans", "title": "Dependency in DAG models with Hidden Variables", "comments": "In Proceedings of the 37th Conference on Artificial Intelligence; 12\n  pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Directed acyclic graph models with hidden variables have been much studied,\nparticularly in view of their computational efficiency and connection with\ncausal methods. In this paper we provide the circumstances under which it is\npossible for two variables to be identically equal, while all other observed\nvariables stay jointly independent of them and mutually of each other. We find\nthat this is possible if and only if the two variables are `densely connected';\nin other words, if applications of identifiable causal interventions on the\ngraph cannot (non-trivially) separate them. As a consequence of this, we can\nalso allow such pairs of random variables have any bivariate joint distribution\nthat we choose. This has implications for model search, since it suggests that\nwe can reduce to only consider graphs in which densely connected vertices are\nalways joined by an edge.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 15:47:07 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Evans", "Robin J.", ""]]}, {"id": "2106.07908", "submitter": "Truong-Vinh Hoang", "authors": "Truong-Vinh Hoang (1), Sebastian Krumscheid (1), Hermann G. Matthies\n  (2) and Ra\\'ul Tempone (1 and 3) ((1) Chair of Mathematics for Uncertainty\n  Quantification, RWTH Aachen University, (2) Technische Universit\\\"at\n  Braunschweig (3) Computer, Electrical and Mathematical Sciences and\n  Engineering, KAUST, and Alexander von Humboldt professor in Mathematics of\n  Uncertainty Quantification, RWTH Aachen University)", "title": "Machine learning-based conditional mean filter: a generalization of the\n  ensemble Kalman filter for nonlinear data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Filtering is a data assimilation technique that performs the sequential\ninference of dynamical systems states from noisy observations. Herein, we\npropose a machine learning-based ensemble conditional mean filter (ML-EnCMF)\nfor tracking possibly high-dimensional non-Gaussian state models with nonlinear\ndynamics based on sparse observations. The proposed filtering method is\ndeveloped based on the conditional expectation and numerically implemented\nusing machine learning (ML) techniques combined with the ensemble method. The\ncontribution of this work is twofold. First, we demonstrate that the ensembles\nassimilated using the ensemble conditional mean filter (EnCMF) provide an\nunbiased estimator of the Bayesian posterior mean, and their variance matches\nthe expected conditional variance. Second, we implement the EnCMF using\nartificial neural networks, which have a significant advantage in representing\nnonlinear functions over high-dimensional domains such as the conditional mean.\nFinally, we demonstrate the effectiveness of the ML-EnCMF for tracking the\nstates of Lorenz-63 and Lorenz-96 systems under the chaotic regime. Numerical\nresults show that the ML-EnCMF outperforms the ensemble Kalman filter.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:40:32 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Hoang", "Truong-Vinh", "", "1 and 3"], ["Krumscheid", "Sebastian", "", "1 and 3"], ["Matthies", "Hermann G.", "", "1 and 3"], ["Tempone", "Ra\u00fal", "", "1 and 3"]]}, {"id": "2106.09367", "submitter": "Lukas Kades", "authors": "Lukas Kades, Martin G\\\"arttner, Thomas Gasenzer, Jan M. Pawlowski", "title": "Towards sampling complex actions", "comments": "29 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-lat hep-th math-ph math.MP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path integrals with complex actions are encountered for many physical systems\nranging from spin- or mass-imbalanced atomic gases and graphene to quantum\nchromo-dynamics at finite density to the non-equilibrium evolution of quantum\nsystems. Many computational approaches have been developed for tackling the\nsign problem emerging for complex actions. Among these, complex Langevin\ndynamics has the appeal of general applicability. One of its key challenges is\nthe potential convergence of the dynamics to unphysical fixed points. The\nstatistical sampling process at such a fixed point is not based on the physical\naction and hence leads to wrong predictions. Moreover, its unphysical nature is\nhard to detect due to the implicit nature of the process. In the present work\nwe set up a general approach based on a Markov chain Monte Carlo scheme in an\nextended state space. In this approach we derive an explicit real sampling\nprocess for generalized complex Langevin dynamics. Subject to a set of\nconstraints, this sampling process is the physical one. These constraints\noriginate from the detailed-balance equations satisfied by the Monte Carlo\nscheme. This allows us to re-derive complex Langevin dynamics from a new\nperspective and establishes a framework for the explicit construction of new\nsampling schemes for complex actions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:37:43 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 10:52:43 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Kades", "Lukas", ""], ["G\u00e4rttner", "Martin", ""], ["Gasenzer", "Thomas", ""], ["Pawlowski", "Jan M.", ""]]}, {"id": "2106.09376", "submitter": "Ossi R\\\"ais\\\"a", "authors": "Ossi R\\\"ais\\\"a, Antti Koskela, Antti Honkela", "title": "Differentially Private Hamiltonian Monte Carlo", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) algorithms have long been the main workhorses\nof Bayesian inference. Among them, Hamiltonian Monte Carlo (HMC) has recently\nbecome very popular due to its efficiency resulting from effective use of the\ngradients of the target distribution. In privacy-preserving machine learning,\ndifferential privacy (DP) has become the gold standard in ensuring that the\nprivacy of data subjects is not violated. Existing DP MCMC algorithms either\nuse random-walk proposals, or do not use the Metropolis--Hastings (MH)\nacceptance test to ensure convergence without decreasing their step size to\nzero. We present a DP variant of HMC using the MH acceptance test that builds\non a recently proposed DP MCMC algorithm called the penalty algorithm, and adds\nnoise to the gradient evaluations of HMC. We prove that the resulting algorithm\nconverges to the correct distribution, and is ergodic. We compare DP-HMC with\nthe existing penalty, DP-SGLD and DP-SGNHT algorithms, and find that DP-HMC has\nbetter or equal performance than the penalty algorithm, and performs more\nconsistently than DP-SGLD or DP-SGNHT.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:50:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["R\u00e4is\u00e4", "Ossi", ""], ["Koskela", "Antti", ""], ["Honkela", "Antti", ""]]}, {"id": "2106.09382", "submitter": "Donghyeon Yu", "authors": "Young-Geun Choi, Seunghwan Lee, Donghyeon Yu", "title": "An efficient parallel block coordinate descent algorithm for large-scale\n  precision matrix estimation using graphics processing units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale sparse precision matrix estimation has attracted wide interest\nfrom the statistics community. The convex partial correlation selection method\n(CONCORD) developed by Khare et al. (2015) has recently been credited with some\ntheoretical properties for estimating sparse precision matrices. The CONCORD\nobtains its solution by a coordinate descent algorithm (CONCORD-CD) based on\nthe convexity of the objective function. However, since a coordinate-wise\nupdate in CONCORD-CD is inherently serial, a scale-up is nontrivial. In this\npaper, we propose a novel parallelization of CONCORD-CD, namely, CONCORD-PCD.\nCONCORD-PCD partitions the off-diagonal elements into several groups and\nupdates each group simultaneously without harming the computational convergence\nof CONCORD-CD. We guarantee this by employing the notion of edge coloring in\ngraph theory. Specifically, we establish a nontrivial correspondence between\nscheduling the updates of the off-diagonal elements in CONCORD-CD and coloring\nthe edges of a complete graph. It turns out that CONCORD-PCD simultaneously\nupdates off-diagonal elements in which the associated edges are colorable with\nthe same color. As a result, the number of steps required for updating\noff-diagonal elements reduces from p(p-1)/2 to p-1 (for even p) or p (for odd\np), where p denotes the number of variables. We prove that the number of such\nsteps is irreducible In addition, CONCORD-PCD is tailored to single-instruction\nmultiple-data (SIMD) parallelism. A numerical study shows that the\nSIMD-parallelized PCD algorithm implemented in graphics processing units (GPUs)\nboosts the CONCORD-CD algorithm multiple times.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 10:55:23 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Choi", "Young-Geun", ""], ["Lee", "Seunghwan", ""], ["Yu", "Donghyeon", ""]]}, {"id": "2106.10107", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox and Wouter Smink", "title": "Assessing an Alternative for `Negative Variance Components': A Gentle\n  Introduction to Bayesian Covariance Structure Modelling for Negative\n  Associations Among Patients with Personalized Treatments", "comments": "4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The multilevel model (MLM) is the popular approach to describe dependences of\nhierarchically clustered observations. A main feature is the capability to\nestimate (cluster-specific) random effect parameters, while their distribution\ndescribes the variation across clusters. However, the MLM can only model\npositive associations among clustered observations, and it is not suitable for\nsmall sample sizes. The limitation of the MLM becomes apparent when estimation\nmethods produce negative estimates for random effect variances, which can be\nseen as an indication that observations are negatively correlated. A gentle\nintroduction to Bayesian Covariance Structure Modelling (BCSM) is given, which\nmakes it possible to model also negatively correlated observations. The BCSM\ndoes not model dependences through random (cluster-specific) effects, but\nthrough a covariance matrix. We show that this makes the BCSM particularly\nuseful for small data samples. We draw specific attention to detect effects of\na personalized intervention. The effect of a personalized treatment can differ\nacross individuals, and this can lead to negative associations among\nmeasurements of individuals who are treated by the same therapist. It is shown\nthat the BCSM enables the modeling of negative associations among clustered\nmeasurements and aids in the interpretation of negative clustering effects.\nThrough a simulation study and by analysis of a real data example, we discuss\nthe suitability of the BCSM for small data sets and for exploring effects of\nindividualized treatments, specifically when (standard) MLM software produces\nnegative or zero variance estimates.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:59:04 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Smink", "Wouter", ""]]}, {"id": "2106.10139", "submitter": "Kamran Pentland", "authors": "Kamran Pentland, Massimiliano Tamborrino, D. Samaddar, L. C. Appel", "title": "Stochastic parareal: an application of probabilistic methods to\n  time-parallelisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Parareal is a well-studied algorithm for numerically integrating systems of\ntime-dependent differential equations by parallelising the temporal domain.\nGiven approximate initial values at each temporal sub-interval, the algorithm\nlocates a solution in a fixed number of iterations using a predictor-corrector,\nstopping once a tolerance is met. This iterative process combines solutions\nlocated by inexpensive (coarse resolution) and expensive (fine resolution)\nnumerical integrators. In this paper, we introduce a stochastic parareal\nalgorithm with the aim of accelerating the convergence of the deterministic\nparareal algorithm. Instead of providing the predictor-corrector with a\ndeterministically located set of initial values, the stochastic algorithm\nsamples initial values from dynamically varying probability distributions in\neach temporal sub-interval. All samples are then propagated by the numerical\nmethod in parallel. The initial values yielding the most continuous (smoothest)\ntrajectory across consecutive sub-intervals are chosen as the new, more\naccurate, set of initial values. These values are fed into the\npredictor-corrector, converging in fewer iterations than the deterministic\nalgorithm with a given probability. The performance of the stochastic\nalgorithm, implemented using various probability distributions, is illustrated\non systems of ordinary differential equations. When the number of sampled\ninitial values is large enough, we show that stochastic parareal converges\nalmost certainly in fewer iterations than the deterministic algorithm while\nmaintaining solution accuracy. Additionally, it is shown that the expected\nvalue of the convergence rate decreases with increasing numbers of samples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:12:05 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 11:47:18 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Pentland", "Kamran", ""], ["Tamborrino", "Massimiliano", ""], ["Samaddar", "D.", ""], ["Appel", "L. C.", ""]]}, {"id": "2106.10144", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox, Konrad Klotzke, Ahmet Salih Simsek", "title": "LNIRT: An R Package for Joint Modeling of Response Accuracy and Times", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In \\textit{computer-based testing} it has become standard to collect response\naccuracy (RA) and response times (RTs) for each test item. IRT models are used\nto measure a latent variable (e.g., ability, intelligence) using the RA\nobservations. The information in the RTs can help to improve routine operations\nin (educational) testing, and provide information about speed of working. In\nmodern applications, the joint models are needed to integrate RT information in\na test analysis. The R-package LNIRT supports fitting joint models through a\nuser-friendly setup which only requires specifying RA, RT data, and the total\nnumber of Gibbs sampling iterations. More detailed specifications of the\nanalysis are optional. The main results can be reported through the summary\nfunctions, but output can also be analysed with Markov chain Monte Carlo (MCMC)\noutput tools (i.e., coda, mcmcse). The main functionality of the LNIRT package\nis illustrated with two real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:18:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Klotzke", "Konrad", ""], ["Simsek", "Ahmet Salih", ""]]}, {"id": "2106.10171", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox, Konrad Klotzke, Duco Veen", "title": "Generalized Linear Randomized Response Modeling using GLMMRR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Randomized response (RR) designs are used to collect response data about\nsensitive behaviors (e.g., criminal behavior, sexual desires). The modeling of\nRR data is more complex, since it requires a description of the RR process. For\nthe class of generalized linear mixed models (GLMMs), the RR process can be\nrepresented by an adjusted link function, which relates the expected RR to the\nlinear predictor, for most common RR designs. The package GLMMRR includes\nmodified link functions for four different cumulative distributions (i.e.,\nlogistic, cumulative normal, gumbel, cauchy) for GLMs and GLMMs, where the\npackage lme4 facilitates ML and REML estimation. The mixed modeling framework\nin GLMMRR can be used to jointly analyse data collected under different designs\n(e.g., dual questioning, multilevel, mixed mode, repeated measurements designs,\nmultiple-group designs). The well-known features of the GLM and GLMM (package\nlme4) software are remained, while adding new model-fit tests, residual\nanalyses, and plot functions to give support to a profound RR data analysis.\nData of H\\\"{o}glinger and Jann (2018) and H\\\"{o}glinger, Jann, and Diekmann\n(2014) is used to illustrate the methodology and software.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:04:37 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Klotzke", "Konrad", ""], ["Veen", "Duco", ""]]}, {"id": "2106.10188", "submitter": "Kirill Neklyudov", "authors": "Kirill Neklyudov, Roberto Bondesan, Max Welling", "title": "Deterministic Gibbs Sampling via Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic dynamics is an essential part of many MCMC algorithms, e.g.\nHybrid Monte Carlo or samplers utilizing normalizing flows. This paper presents\na general construction of deterministic measure-preserving dynamics using\nautonomous ODEs and tools from differential geometry. We show how Hybrid Monte\nCarlo and other deterministic samplers follow as special cases of our theory.\nWe then demonstrate the utility of our approach by constructing a continuous\nnon-sequential version of Gibbs sampling in terms of an ODE flow and extending\nit to discrete state spaces. We find that our deterministic samplers are more\nsample efficient than stochastic counterparts, even if the latter generate\nindependent samples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:36:09 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Neklyudov", "Kirill", ""], ["Bondesan", "Roberto", ""], ["Welling", "Max", ""]]}, {"id": "2106.10238", "submitter": "Fabian Zaiser", "authors": "Carol Mak, Fabian Zaiser, Luke Ong", "title": "Nonparametric Hamiltonian Monte Carlo", "comments": "33 pages, 13 figures. To appear in Proceedings of the 38th\n  International Conference on Machine Learning, PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.PL stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probabilistic programming uses programs to express generative models whose\nposterior probability is then computed by built-in inference engines. A\nchallenging goal is to develop general purpose inference algorithms that work\nout-of-the-box for arbitrary programs in a universal probabilistic programming\nlanguage (PPL). The densities defined by such programs, which may use\nstochastic branching and recursion, are (in general) nonparametric, in the\nsense that they correspond to models on an infinite-dimensional parameter\nspace. However standard inference algorithms, such as the Hamiltonian Monte\nCarlo (HMC) algorithm, target distributions with a fixed number of parameters.\nThis paper introduces the Nonparametric Hamiltonian Monte Carlo (NP-HMC)\nalgorithm which generalises HMC to nonparametric models. Inputs to NP-HMC are a\nnew class of measurable functions called \"tree representable\", which serve as a\nlanguage-independent representation of the density functions of probabilistic\nprograms in a universal PPL. We provide a correctness proof of NP-HMC, and\nempirically demonstrate significant performance improvements over existing\napproaches on several nonparametric examples.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 17:03:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Mak", "Carol", ""], ["Zaiser", "Fabian", ""], ["Ong", "Luke", ""]]}, {"id": "2106.10341", "submitter": "Aur\\'elien Ouattara", "authors": "Aur\\'elien Ouattara, Matthieu Bult\\'e, Wan-Ju Lin, Philipp Scholl,\n  Benedikt Veit, Christos Ziakas, Florian Felice, Julien Virlogeux, George\n  Dikos", "title": "Scalable Econometrics on Big Data -- The Logistic Regression on Spark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extra-large datasets are becoming increasingly accessible, and computing\ntools designed to handle huge amount of data efficiently are democratizing\nrapidly. However, conventional statistical and econometric tools are still\nlacking fluency when dealing with such large datasets. This paper dives into\neconometrics on big datasets, specifically focusing on the logistic regression\non Spark. We review the robustness of the functions available in Spark to fit\nlogistic regression and introduce a package that we developed in PySpark which\nreturns the statistical summary of the logistic regression, necessary for\nstatistical inference.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 20:10:34 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ouattara", "Aur\u00e9lien", ""], ["Bult\u00e9", "Matthieu", ""], ["Lin", "Wan-Ju", ""], ["Scholl", "Philipp", ""], ["Veit", "Benedikt", ""], ["Ziakas", "Christos", ""], ["Felice", "Florian", ""], ["Virlogeux", "Julien", ""], ["Dikos", "George", ""]]}, {"id": "2106.10534", "submitter": "Art Owen", "authors": "Zexin Pan and Art B. Owen", "title": "The nonzero gain coefficients of Sobol's sequences are always powers of\n  two", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When a plain Monte Carlo estimate on $n$ samples has variance $\\sigma^2/n$,\nthen scrambled digital nets attain a variance that is $o(1/n)$ as $n\\to\\infty$.\nFor finite $n$ and an adversarially selected integrand, the variance of a\nscrambled $(t,m,s)$-net can be at most $\\Gamma\\sigma^2/n$ for a maximal gain\ncoefficient $\\Gamma<\\infty$. The most widely used digital nets and sequences\nare those of Sobol'. It was previously known that $\\Gamma\\leqslant 2^t3^s$ for\nSobol' points as well as Niederreiter-Xing points. In this paper we study nets\nin base $2$. We show that $\\Gamma \\leqslant2^{t+s-1}$ for nets. This bound is a\nsimple, but apparently unnoticed, consequence of a microstructure analysis in\nNiederreiter and Pirsic (2001). We obtain a sharper bound that is smaller than\nthis for some digital nets. We also show that all nonzero gain coefficients\nmust be powers of two. A consequence of this latter fact is a simplified\nalgorithm for computing gain coefficients of nets in base $2$.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 17:11:02 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Pan", "Zexin", ""], ["Owen", "Art B.", ""]]}, {"id": "2106.10539", "submitter": "Elan Ness-Cohn", "authors": "Elan Ness-Cohn and Rosemary Braun", "title": "Fasano-Franceschini Test: an Implementation of a 2-Dimensional\n  Kolmogorov-Smirnov test in R", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The univariate Kolmogorov-Smirnov (KS) test is a non-parametric statistical\ntest designed to assess whether a set of data is consistent with a given\nprobability distribution (or, in the two-sample case, whether the two samples\ncome from the same underlying distribution). The versatility of the KS test has\nmade it a cornerstone of statistical analysis and is commonly used across the\nscientific disciplines. However, the test proposed by Kolmogorov and Smirnov\ndoes not naturally extend to multidimensional distributions. Here, we present\nthe fasano.franceschini.test package, an R implementation of the 2-D KS\ntwo-sample test as defined by Fasano and Franceschini (Fasano and Franceschini\n1987). The fasano.franceschini.test package provides three improvements over\nthe current 2-D KS test on the Comprehensive R Archive Network (CRAN): (i) the\nFasano and Franceschini test has been shown to run in $O(n^2)$ versus the\nPeacock implementation which runs in $O(n^3)$; (ii) the package implements a\nprocedure for handling ties in the data; and (iii) the package implements a\nparallelized bootstrapping procedure for improved significance testing.\nUltimately, the fasano.franceschini.test package presents a robust statistical\ntest for analyzing random samples defined in 2-dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 17:28:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ness-Cohn", "Elan", ""], ["Braun", "Rosemary", ""]]}, {"id": "2106.10660", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens", "title": "Bayesian inference for continuous-time hidden Markov models with an\n  unknown number of states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the modeling of data generated by a latent continuous-time Markov\njump process with a state space of finite but unknown dimensions. Typically in\nsuch models, the number of states has to be pre-specified, and Bayesian\ninference for a fixed number of states has not been studied until recently. In\naddition, although approaches to address the problem for discrete-time models\nhave been developed, no method has been successfully implemented for the\ncontinuous-time case. We focus on reversible jump Markov chain Monte Carlo\nwhich allows the trans-dimensional move among different numbers of states in\norder to perform Bayesian inference for the unknown number of states.\nSpecifically, we propose an efficient split-combine move which can facilitate\nthe exploration of the parameter space, and demonstrate that it can be\nimplemented effectively at scale. Subsequently, we extend this algorithm to the\ncontext of model-based clustering, allowing numbers of states and clusters both\ndetermined during the analysis. The model formulation, inference methodology,\nand associated algorithm are illustrated by simulation studies. Finally, We\napply this method to real data from a Canadian healthcare system in Quebec.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 09:13:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""]]}, {"id": "2106.10694", "submitter": "Xiaolei Chu", "authors": "Xiaolei Chu, Hung Nguyen Sinh, Wei Cui, Lin Zhao, Yaojun Ge", "title": "Life-cycle assessment for flutter probability of a long-span suspension\n  bridge based on field monitoring data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Assessment of structural safety status is of paramount importance for\nexisting bridges, where accurate evaluation of flutter probability is essential\nfor long-span bridges. In current engineering practice, at the design stage,\nflutter critical wind speed is usually estimated by the wind tunnel test, which\nis sensitive to modal frequencies and damping ratios. After construction,\nstructural properties of existing structures will change with time due to\nvarious factors, such as structural deteriorations and periodic environments.\nThe structural dynamic properties, such as modal frequencies and damping\nratios, cannot be considered as the same values as the initial ones, and the\ndeteriorations should be included when estimating the life-cycle flutter\nprobability. This paper proposes an evaluation framework to assess the\nlife-cycle flutter probability of long-span bridges considering the\ndeteriorations of structural properties, based on field monitoring data. The\nBayesian approach is employed for modal identification of a suspension bridge\nwith the main span of 1650 m, and the field monitoring data during 2010-2015 is\nanalyzed to determine the deterioration functions of modal frequencies and\ndamping ratios, as well as their inter-seasonal fluctuations. According to the\nhistorical trend, the long-term structural properties can be predicted, and the\nprobability distributions of flutter critical wind speed for each year in the\nlong term are calculated. Consequently, the life-cycle flutter probability is\nestimated, based on the predicted modal frequencies and damping ratios.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 13:35:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chu", "Xiaolei", ""], ["Sinh", "Hung Nguyen", ""], ["Cui", "Wei", ""], ["Zhao", "Lin", ""], ["Ge", "Yaojun", ""]]}, {"id": "2106.10880", "submitter": "Yuling Jiao", "authors": "Jian Huang, Yuling Jiao, Lican Kang, Xu Liao, Jin Liu, Yanyan Liu", "title": "Schr{\\\"o}dinger-F{\\\"o}llmer Sampler: Sampling without Ergodicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from probability distributions is an important problem in statistics\nand machine learning, specially in Bayesian inference when integration with\nrespect to posterior distribution is intractable and sampling from the\nposterior is the only viable option for inference. In this paper, we propose\nSchr\\\"{o}dinger-F\\\"{o}llmer sampler (SFS), a novel approach for sampling from\npossibly unnormalized distributions. The proposed SFS is based on the\nSchr\\\"{o}dinger-F\\\"{o}llmer diffusion process on the unit interval with a time\ndependent drift term, which transports the degenerate distribution at time zero\nto the target distribution at time one. Comparing with the existing Markov\nchain Monte Carlo samplers that require ergodicity, no such requirement is\nneeded for SFS. Computationally, SFS can be easily implemented using the\nEuler-Maruyama discretization. In theoretical analysis, we establish\nnon-asymptotic error bounds for the sampling distribution of SFS in the\nWasserstein distance under suitable conditions. We conduct numerical\nexperiments to evaluate the performance of SFS and demonstrate that it is able\nto generate samples with better quality than several existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 06:39:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 07:11:15 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Kang", "Lican", ""], ["Liao", "Xu", ""], ["Liu", "Jin", ""], ["Liu", "Yanyan", ""]]}, {"id": "2106.10952", "submitter": "Elena Ehrlich", "authors": "Elena Ehrlich, Laurent Callot, Fran\\c{c}ois-Xavier Aubet", "title": "Spliced Binned-Pareto Distribution for Robust Modeling of Heavy-tailed\n  Time Series", "comments": "Accepted at RobustWorkshop@ICLR2021:\n  <https://sites.google.com/connect.hku.hk/robustml-2021/accepted-papers/paper-041>", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a novel method to robustly and accurately model time\nseries with heavy-tailed noise, in non-stationary scenarios. In many practical\napplication time series have heavy-tailed noise that significantly impacts the\nperformance of classical forecasting models; in particular, accurately modeling\na distribution over extreme events is crucial to performing accurate time\nseries anomaly detection. We propose a Spliced Binned-Pareto distribution which\nis both robust to extreme observations and allows accurate modeling of the full\ndistribution. Our method allows the capture of time dependencies in the higher\norder moments of the distribution such as the tail heaviness. We compare the\nrobustness and the accuracy of the tail estimation of our method to other state\nof the art methods on Twitter mentions count time series.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:48:03 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ehrlich", "Elena", ""], ["Callot", "Laurent", ""], ["Aubet", "Fran\u00e7ois-Xavier", ""]]}, {"id": "2106.11043", "submitter": "Rihui Ou", "authors": "Rihui Ou, Deborshee Sen, David Dunson", "title": "Scalable Bayesian inference for time series via divide-and-conquer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian computational algorithms tend to scale poorly as data size\nincreases. This had led to the development of divide-and-conquer-based\napproaches for scalable inference. These divide the data into subsets, perform\ninference for each subset in parallel, and then combine these inferences. While\nappealing theoretical properties and practical performance have been\ndemonstrated for independent observations, scalable inference for dependent\ndata remains challenging. In this work, we study the problem of Bayesian\ninference from very long time series. The literature in this area focuses\nmainly on approximate approaches that lack any theoretical guarantees and may\nprovide arbitrarily poor accuracy in practice. We propose a simple and scalable\ndivide-and-conquer method, and provide accuracy guarantees. Numerical\nsimulations and real data applications demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:29:02 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 20:53:26 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ou", "Rihui", ""], ["Sen", "Deborshee", ""], ["Dunson", "David", ""]]}, {"id": "2106.11188", "submitter": "Riccardo Fogliato", "authors": "Riccardo Fogliato, Shamindra Shrotriya, Arun Kumar Kuchibhotla", "title": "maars: Tidy Inference under the 'Models as Approximations' Framework in\n  R", "comments": "The first two authors contributed equally to this work and are\n  ordered alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression using ordinary least squares (OLS) is a critical part of\nevery statistician's toolkit. In R, this is elegantly implemented via lm() and\nits related functions. However, the statistical inference output from this\nsuite of functions is based on the assumption that the model is well specified.\nThis assumption is often unrealistic and at best satisfied approximately. In\nthe statistics and econometrics literature, this has long been recognized and a\nlarge body of work provides inference for OLS under more practical assumptions.\nThis can be seen as model-free inference. In this paper, we introduce our\npackage maars (\"models as approximations\") that aims at bringing research on\nmodel-free inference to R via a comprehensive workflow. The maars package\ndiffers from other packages that also implement variance estimation, such as\nsandwich, in three key ways. First, all functions in maars follow a consistent\ngrammar and return output in tidy format, with minimal deviation from the\ntypical lm() workflow. Second, maars contains several tools for inference\nincluding empirical, multiplier, residual bootstrap, and subsampling, for easy\ncomparison. Third, maars is developed with pedagogy in mind. For this, most of\nits functions explicitly return the assumptions under which the output is\nvalid. This key innovation makes maars useful in teaching inference under\nmisspecification and also a powerful tool for applied researchers. We hope our\ndefault feature of explicitly presenting assumptions will become a de facto\nstandard for most statistical modeling in R.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:27:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fogliato", "Riccardo", ""], ["Shrotriya", "Shamindra", ""], ["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2106.11209", "submitter": "Nicholas Horton", "authors": "Chelsey Legacy and Andrew Zieffler and Benjamin S. Baumer and Valerie\n  Barr and Nicholas J. Horton", "title": "Facilitating team-based data science: lessons learned from the DSC-WAV\n  project", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While coursework provides undergraduate data science students with some\nrelevant analytic skills, many are not given the rich experiences with data and\ncomputing they need to be successful in the workplace. Additionally, students\noften have lmited exposure to team-based data science and the principles and\ntools of collaboration that are encountered outside of school.\n  In this paper, we describe the DSC-WAV program, an NSF-funded data science\nworkforce development project in which teams of undergraduate sophomores and\njuniors work with a local non-profit organization on a data-focused problem. To\nhelp students develop a sense of agency and improve confidence in their\ntechnical and non-technical data science skills, the project promoted a\nteam-based approach to data science, adopting several processes and tools\nintended to facilitate this collaboration.\n  Evidence from the project evaluation, including participant survey and\ninterview data, is presented to document the degree to which the project was\nsuccessful in engaging students in team-based data science, and how the project\nchanged the students' perceptions of their technical and non-technical skills.\nWe also examine opportunities for improvement and offer insight to provide\nadvice for other data science educators who want to implement something similar\nat their own institutions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:51:34 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 12:15:06 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 17:36:42 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Legacy", "Chelsey", ""], ["Zieffler", "Andrew", ""], ["Baumer", "Benjamin S.", ""], ["Barr", "Valerie", ""], ["Horton", "Nicholas J.", ""]]}, {"id": "2106.11213", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana, Fabio Rapallo, Henry P. Wynn", "title": "Circuits for robust designs", "comments": "21 pages; 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the application of circuit theory to experimental design\nstarted by the first two authors. The theory gives a very special and detailed\nrepresentation of the kernel of the design model matrix. This representation\nturns out to be an appropriate way to study the optimality criteria referred to\nas robustness: the sensitivity of the design to the removal of design points.\nMany examples are given, from classical combinatorial designs to two-level\nfactorial design including interactions. The complexity of the circuit\nrepresentations are useful because the large range of options they offer, but\nconversely require the use of dedicated software. Suggestions for speed\nimprovement are made.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:56:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""], ["Wynn", "Henry P.", ""]]}, {"id": "2106.11338", "submitter": "Damon Pham", "authors": "Damon Pham, John Muschelli, Amanda Mejia", "title": "ciftiTools: A package for reading, writing, visualizing and manipulating\n  CIFTI files in R", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surface-based analysis of MR data has well-recognized advantages, including\nimproved whole-cortex visualization, the ability to perform surface smoothing\nto avoid issues associated with volumetric smoothing, improved inter-subject\nalignment, and reduced dimensionality. The CIFTI ``grayordinate'' file format\nintroduced by the Human Connectome Project further advances surface-based\nanalysis by combining left and right hemispheric surface metric data with\nsubcortical and cerebellar gray matter data into a single file. Analyses\nperformed in grayordinates space are well-suited to leverage information shared\nacross the brain and across subjects through both traditional analysis\ntechniques and more advanced Bayesian statistical methods. The R statistical\nenvironment facilitates the latter given its wealth of advanced statistical\ntechniques for Bayesian computation and spatial modeling. Yet little support\nfor grayordinates analysis has been previously available in R, and few\ncomprehensive programmatic tools for working with CIFTI files have been\navailable in any language. Here, we present the ciftiTools R package, which\nprovides a unified environment for reading, writing, visualizing and\nmanipulating CIFTI and related data formats. We illustrate ciftiTools'\nconvenient and user-friendly suite of tools for working with grayordinates and\nsurface geometry data in R, and we describe how ciftiTools is being utilized to\nadvance the statistical analysis of grayordinates-based functional MRI data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:11:24 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Pham", "Damon", ""], ["Muschelli", "John", ""], ["Mejia", "Amanda", ""]]}, {"id": "2106.11357", "submitter": "Giorgos Vasdekis", "authors": "G. Vasdekis, G. O. Roberts", "title": "A Note on the Polynomial Ergodicity of the One-Dimensional Zig-Zag\n  process", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove polynomial ergodicity for the one-dimensional Zig-Zag process on\nheavy tailed targets and identify the exact order of polynomial convergence of\nthe process when targeting Student distributions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 18:40:19 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Vasdekis", "G.", ""], ["Roberts", "G. O.", ""]]}, {"id": "2106.11561", "submitter": "Johanna Meier", "authors": "Ziang Niu, Johanna Meier, Fran\\c{c}ois-Xavier Briol", "title": "Discrepancy-based Inference for Intractable Generative Models using\n  Quasi-Monte Carlo", "comments": "minor presentation changes and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intractable generative models are models for which the likelihood is\nunavailable but sampling is possible. Most approaches to parameter inference in\nthis setting require the computation of some discrepancy between the data and\nthe generative model. This is for example the case for minimum distance\nestimation and approximate Bayesian computation. These approaches require\nsampling a high number of realisations from the model for different parameter\nvalues, which can be a significant challenge when simulating is an expensive\noperation. In this paper, we propose to enhance this approach by enforcing\n\"sample diversity\" in simulations of our models. This will be implemented\nthrough the use of quasi-Monte Carlo (QMC) point sets. Our key results are\nsample complexity bounds which demonstrate that, under smoothness conditions on\nthe generator, QMC can significantly reduce the number of samples required to\nobtain a given level of accuracy when using three of the most common\ndiscrepancies: the maximum mean discrepancy, the Wasserstein distance, and the\nSinkhorn divergence. This is complemented by a simulation study which\nhighlights that an improved accuracy is sometimes also possible in some\nsettings which are not covered by the theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:36:08 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:03:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Niu", "Ziang", ""], ["Meier", "Johanna", ""], ["Briol", "Fran\u00e7ois-Xavier", ""]]}, {"id": "2106.11597", "submitter": "Carsten Hartmann", "authors": "Robert D. Skeel and Carsten Hartmann", "title": "Choice of Damping Coefficient in Langevin Dynamics", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the application of Langevin dynamics to sampling and\ninvestigates how to choose the damping parameter in Langevin dynamics for the\npurpose of maximizing thoroughness of sampling. Also, it considers the\ncomputation of measures of sampling thoroughness.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:05:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Skeel", "Robert D.", ""], ["Hartmann", "Carsten", ""]]}, {"id": "2106.11617", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Modal clustering on PPGMMGA projection subspace", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PPGMMGA is a Projection Pursuit (PP) algorithm aimed at detecting and\nvisualizing clustering structures in multivariate data. The algorithm uses the\nnegentropy as PP index obtained by fitting Gaussian Mixture Models (GMMs) for\ndensity estimation, and then optimized using Genetic Algorithms (GAs). Since\nthe PPGMMGA algorithm is a dimension reduction technique specifically\nintroduced for visualization purposes, cluster memberships are not explicitly\nprovided. In this paper a modal clustering approach is proposed for estimating\nclusters of projected data points. In particular, a modal EM algorithm is\nemployed to estimate the modes corresponding to the local maxima in the\nprojection subspace of the underlying density estimated using parsimonious\nGMMs. Data points are then clustered according to the domain of attraction of\nthe identified modes. Simulated and real data are discussed to illustrate the\nproposed method and evaluate the clustering performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:57:44 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "2106.11970", "submitter": "Fanhua Shang", "authors": "Lin Kong, Wei Sun, Fanhua Shang, Yuanyuan Liu, Hongying Liu", "title": "Learned Interpretable Residual Extragradient ISTA for Sparse Coding", "comments": "Accepted for presentation at the ICML Workshop on Theoretic\n  Foundation, Criticism, and Application Trend of Explainable AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, the study on learned iterative shrinkage thresholding algorithm\n(LISTA) has attracted increasing attentions. A large number of experiments as\nwell as some theories have proved the high efficiency of LISTA for solving\nsparse coding problems. However, existing LISTA methods are all serial\nconnection. To address this issue, we propose a novel extragradient based LISTA\n(ELISTA), which has a residual structure and theoretical guarantees. In\nparticular, our algorithm can also provide the interpretability for Res-Net to\na certain extent. From a theoretical perspective, we prove that our method\nattains linear convergence. In practice, extensive empirical results verify the\nadvantages of our method.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 16:22:29 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kong", "Lin", ""], ["Sun", "Wei", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""], ["Liu", "Hongying", ""]]}, {"id": "2106.12046", "submitter": "Erik Kusch", "authors": "Erik Kusch, Richard Davy", "title": "KrigR -- A tool for downloading and statistically downscaling climate\n  reanalysis data", "comments": "submitted to Scientific Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present KrigR, an R package for acquiring and statistically downscaling\nstate-of-the-art climate data using kriging. KrigR allows R-users to (1)\ndownload ERA5 and ERA5-Land climate reanalysis data for a user-specified\nregion, and time-length, (2) aggregate these climate products to desired\ntemporal resolutions and metrics, (3) acquire topographical co-variates, and\n(4) statistically downscale spatial data to a user-specified resolution using\nco-variate data via kriging. KrigR can execute all of these tasks in a single\nfunction call, thus enabling the user to obtain any of 83 (ERA5) / 50\n(ERA5-Land) climate variables at high spatial and temporal resolution with a\nsingle R-command. Thereby, KrigR provides a toolbox to obtain a wide range of\ntailored climate data at unprecedented combinations of high temporal and\nspatial resolutions. Additionally, we demonstrate how KrigR is\ncompartmentalised to enable use of any given climate dataset and\nthird-party/user-provided co-variates for the kriging step and brings an\nadvantage over other high-resolution datasets by providing downscaling\nuncertainties, which can explain the difference between several existing\nhigh-resolution climate products.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:03:21 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Kusch", "Erik", ""], ["Davy", "Richard", ""]]}, {"id": "2106.12262", "submitter": "David Frazier", "authors": "David T. Frazier, Ruben Loaiza-Maya and Gael M. Martin", "title": "A Note on the Accuracy of Variational Bayes in State Space Models:\n  Inference and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using theoretical and numerical results, we document the accuracy of commonly\napplied variational Bayes methods across a broad range of state space models.\nThe results demonstrate that, in terms of accuracy on fixed parameters, there\nis a clear hierarchy in terms of the methods, with approaches that do not\napproximate the states yielding superior accuracy over methods that do. We also\ndocument numerically that the inferential discrepancies between the various\nmethods often yield only small discrepancies in predictive accuracy over small\nout-of-sample evaluation periods. Nevertheless, in certain settings, these\npredictive discrepancies can become marked over longer out-of-sample periods.\nThis finding indicates that the invariance of predictive results to inferential\ninaccuracy, which has been an oft-touted point made by practitioners seeking to\njustify the use of variational inference, is not ubiquitous and must be\nassessed on a case-by-case basis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:39:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Frazier", "David T.", ""], ["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""]]}, {"id": "2106.12408", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Tamara Broderick", "title": "The SKIM-FA Kernel: High-Dimensional Variable Selection and Nonlinear\n  Interaction Discovery in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific problems require identifying a small set of covariates that\nare associated with a target response and estimating their effects. Often,\nthese effects are nonlinear and include interactions, so linear and additive\nmethods can lead to poor estimation and variable selection. The Bayesian\nframework makes it straightforward to simultaneously express sparsity,\nnonlinearity, and interactions in a hierarchical model. But, as for the few\nother methods that handle this trifecta, inference is computationally\nintractable - with runtime at least quadratic in the number of covariates, and\noften worse. In the present work, we solve this computational bottleneck. We\nfirst show that suitable Bayesian models can be represented as Gaussian\nprocesses (GPs). We then demonstrate how a kernel trick can reduce computation\nwith these GPs to O(# covariates) time for both variable selection and\nestimation. Our resulting fit corresponds to a sparse orthogonal decomposition\nof the regression function in a Hilbert space (i.e., a functional ANOVA\ndecomposition), where interaction effects represent all variation that cannot\nbe explained by lower-order effects. On a variety of synthetic and real\ndatasets, our approach outperforms existing methods used for large,\nhigh-dimensional datasets while remaining competitive (or being orders of\nmagnitude faster) in runtime.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:53:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""]]}, {"id": "2106.12555", "submitter": "Joel Dyer", "authors": "Joel Dyer, Patrick Cannon, Sebastian M Schmon", "title": "Approximate Bayesian Computation with Path Signatures", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation models of scientific interest often lack a tractable likelihood\nfunction, precluding standard likelihood-based statistical inference. A popular\nlikelihood-free method for inferring simulator parameters is approximate\nBayesian computation, where an approximate posterior is sampled by comparing\nsimulator output and observed data. However, effective measures of closeness\nbetween simulated and observed data are generally difficult to construct,\nparticularly for time series data which are often high-dimensional and\nstructurally complex. Existing approaches typically involve manually\nconstructing summary statistics, requiring substantial domain expertise and\nexperimentation, or rely on unrealistic assumptions such as iid data. Others\nare inappropriate in more complex settings like multivariate or irregularly\nsampled time series data. In this paper, we introduce the use of path\nsignatures as a natural candidate feature set for constructing distances\nbetween time series data for use in approximate Bayesian computation\nalgorithms. Our experiments show that such an approach can generate more\naccurate approximate Bayesian posteriors than existing techniques for time\nseries models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:25:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dyer", "Joel", ""], ["Cannon", "Patrick", ""], ["Schmon", "Sebastian M", ""]]}, {"id": "2106.12652", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar and Shrijita Bhattacharya and Mookyong Son and\n  Tapabrata Maiti", "title": "Black Box Variational Bayes Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many decades now, Bayesian Model Averaging (BMA) has been a popular\nframework to systematically account for model uncertainty that arises in\nsituations when multiple competing models are available to describe the same or\nsimilar physical process. The implementation of this framework, however, comes\nwith multitude of practical challenges including posterior approximation via\nMarkov Chain Monte Carlo and numerical integration. We present a Variational\nBayes Inference approach to BMA as a viable alternative to the standard\nsolutions which avoids many of the aforementioned pitfalls. The proposed method\nis 'black box' in the sense that it can be readily applied to many models with\nlittle to no model-specific derivation. We illustrate the utility of our\nvariational approach on a suite of standard examples and discuss all the\nnecessary implementation details. Fully documented Python code with all the\nexamples is provided as well.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 20:40:27 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Bhattacharya", "Shrijita", ""], ["Son", "Mookyong", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2106.13359", "submitter": "Christopher Paciorek", "authors": "Joshua E. Hug and Christopher J. Paciorek", "title": "A numerically stable online implementation and exploration of WAIC\n  through variations of the predictive density, using NIMBLE", "comments": "27 pages, 9 tables. This is a preprint of the MA in Statistics thesis\n  of Joshua Hug at the University of California, Berkeley, submitted May 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We go through the process of crafting a robust and numerically stable online\nalgorithm for the computation of the Watanabe-Akaike information criteria\n(WAIC). We implement this algorithm in the NIMBLE software. The implementation\nis performed in an online manner and does not require the storage in memory of\nthe complete samples from the posterior distribution. This algorithm allows the\nuser to specify a specific form of the predictive density to be used in the\ncomputation of WAIC, in order to cater to specific prediction goals. We then\ncomment and explore via simulations the use of different forms of the\npredictive density in the context of different predictive goals. We find that\nwhen using marginalized predictive densities, WAIC is sensitive to the grouping\nof the observations into a joint density.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 00:07:04 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hug", "Joshua E.", ""], ["Paciorek", "Christopher J.", ""]]}, {"id": "2106.13508", "submitter": "Defeng Sun", "authors": "Qian LI, Binyan Jiang, Defeng Sun", "title": "MARS: A second-order reduction algorithm for high-dimensional sparse\n  precision matrices estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the precision matrix (or inverse covariance matrix) is of great\nimportance in statistical data analysis. However, as the number of parameters\nscales quadratically with the dimension p, computation becomes very challenging\nwhen p is large. In this paper, we propose an adaptive sieving reduction\nalgorithm to generate a solution path for the estimation of precision matrices\nunder the $\\ell_1$ penalized D-trace loss, with each subproblem being solved by\na second-order algorithm. In each iteration of our algorithm, we are able to\ngreatly reduce the number of variables in the problem based on the\nKarush-Kuhn-Tucker (KKT) conditions and the sparse structure of the estimated\nprecision matrix in the previous iteration. As a result, our algorithm is\ncapable of handling datasets with very high dimensions that may go beyond the\ncapacity of the existing methods. Moreover, for the sub-problem in each\niteration, other than solving the primal problem directly, we develop a\nsemismooth Newton augmented Lagrangian algorithm with global linear convergence\non the dual problem to improve the efficiency. Theoretical properties of our\nproposed algorithm have been established. In particular, we show that the\nconvergence rate of our algorithm is asymptotically superlinear. The high\nefficiency and promising performance of our algorithm are illustrated via\nextensive simulation studies and real data applications, with comparison to\nseveral state-of-the-art solvers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:00:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["LI", "Qian", ""], ["Jiang", "Binyan", ""], ["Sun", "Defeng", ""]]}, {"id": "2106.13683", "submitter": "Chengjing Wang", "authors": "Peipei Tang, Chengjing Wang and Bo Jiang", "title": "A proximal-proximal majorization-minimization algorithm for nonconvex\n  tuning-free robust regression problems", "comments": "31 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we introduce a proximal-proximal majorization-minimization\n(PPMM) algorithm for nonconvex tuning-free robust regression problems. The\nbasic idea is to apply the proximal majorization-minimization algorithm to\nsolve the nonconvex problem with the inner subproblems solved by a sparse\nsemismooth Newton (SSN) method based proximal point algorithm (PPA). We must\nemphasize that the main difficulty in the design of the algorithm lies in how\nto overcome the singular difficulty of the inner subproblem. Furthermore, we\nalso prove that the PPMM algorithm converges to a d-stationary point. Due to\nthe Kurdyka-Lojasiewicz (KL) property of the problem, we present the\nconvergence rate of the PPMM algorithm. Numerical experiments demonstrate that\nour proposed algorithm outperforms the existing state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:07:13 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Tang", "Peipei", ""], ["Wang", "Chengjing", ""], ["Jiang", "Bo", ""]]}, {"id": "2106.13706", "submitter": "Alexander Hagen PhD", "authors": "Alex Hagen, Shane Jackson, James Kahn, Jan Strube, Isabel Haide, Karl\n  Pazdernik, Connor Hainje", "title": "Accelerated Computation of a High Dimensional Kolmogorov-Smirnov\n  Distance", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical testing is widespread and critical for a variety of scientific\ndisciplines. The advent of machine learning and the increase of computing power\nhas increased the interest in the analysis and statistical testing of\nmultidimensional data. We extend the powerful Kolmogorov-Smirnov two sample\ntest to a high dimensional form in a similar manner to Fasano (Fasano, 1987).\nWe call our result the d-dimensional Kolmogorov-Smirnov test (ddKS) and provide\nthree novel contributions therewith: we develop an analytical equation for the\nsignificance of a given ddKS score, we provide an algorithm for computation of\nddKS on modern computing hardware that is of constant time complexity for small\nsample sizes and dimensions, and we provide two approximate calculations of\nddKS: one that reduces the time complexity to linear at larger sample sizes,\nand another that reduces the time complexity to linear with increasing\ndimension. We perform power analysis of ddKS and its approximations on a corpus\nof datasets and compare to other common high dimensional two sample tests and\ndistances: Hotelling's T^2 test and Kullback-Leibler divergence. Our ddKS test\nperforms well for all datasets, dimensions, and sizes tested, whereas the other\ntests and distances fail to reject the null hypothesis on at least one dataset.\nWe therefore conclude that ddKS is a powerful multidimensional two sample test\nfor general use, and can be calculated in a fast and efficient manner using our\nparallel or approximate methods. Open source implementations of all methods\ndescribed in this work are located at https://github.com/pnnl/ddks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:45:18 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Hagen", "Alex", ""], ["Jackson", "Shane", ""], ["Kahn", "James", ""], ["Strube", "Jan", ""], ["Haide", "Isabel", ""], ["Pazdernik", "Karl", ""], ["Hainje", "Connor", ""]]}, {"id": "2106.13718", "submitter": "Onur Teymur", "authors": "Onur Teymur, Christopher N. Foley, Philip G. Breen, Toni Karvonen,\n  Chris. J. Oates", "title": "Black Box Probabilistic Numerics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic numerics casts numerical tasks, such the numerical solution of\ndifferential equations, as inference problems to be solved. One approach is to\nmodel the unknown quantity of interest as a random variable, and to constrain\nthis variable using data generated during the course of a traditional numerical\nmethod. However, data may be nonlinearly related to the quantity of interest,\nrendering the proper conditioning of random variables difficult and limiting\nthe range of numerical tasks that can be addressed. Instead, this paper\nproposes to construct probabilistic numerical methods based only on the final\noutput from a traditional method. A convergent sequence of approximations to\nthe quantity of interest constitute a dataset, from which the limiting quantity\nof interest can be extrapolated, in a probabilistic analogue of Richardson's\ndeferred approach to the limit. This black box approach (1) massively expands\nthe range of tasks to which probabilistic numerics can be applied, (2) inherits\nthe features and performance of state-of-the-art numerical methods, and (3)\nenables provably higher orders of convergence to be achieved. Applications are\npresented for nonlinear ordinary and partial differential equations, as well as\nfor eigenvalue problems-a setting for which no probabilistic numerical methods\nhave yet been developed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:21:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Teymur", "Onur", ""], ["Foley", "Christopher N.", ""], ["Breen", "Philip G.", ""], ["Karvonen", "Toni", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2106.13844", "submitter": "Todd Oliver", "authors": "Todd A. Oliver, Christopher S. Simmons and Robert D. Moser", "title": "Extensions to Multifidelity Monte Carlo Methods for Simulations of\n  Chaotic Systems", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multifidelity Monte Carlo methods often rely on a preprocessing phase\nconsisting of standard Monte Carlo sampling to estimate correlation\ncoefficients between models of different fidelity to determine the weights and\nnumber of samples for each level. For computationally intensive models, as are\noften encountered in simulations of chaotic systems, this up-front cost can be\nprohibitive. In this work, a correlation estimation procedure is developed for\nthe case in which the highest and next highest fidelity models are generated\nvia discretizing the same mathematical model using different resolution. The\nprocedure uses discretization error estimates to estimate the required\ncorrelation coefficient without the need to sample the highest fidelity model,\nwhich can dramatically decrease the cost of the preprocessing phase. The method\nis extended to chaotic problems by using discretization error estimates that\naccount for the statistical nature of common quantities of interest and the\naccompanying finite sampling errors that pollute estimates of such quantities\nof interest. The methodology is then demonstrated on a model problem based on\nthe Kuramoto-Sivashinsky equation.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:51:12 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Oliver", "Todd A.", ""], ["Simmons", "Christopher S.", ""], ["Moser", "Robert D.", ""]]}, {"id": "2106.14045", "submitter": "Ning Ning", "authors": "Ning Ning and Jinwen Qiu", "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models\n  in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:28:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ning", "Ning", ""], ["Qiu", "Jinwen", ""]]}, {"id": "2106.14095", "submitter": "Maikol Sol\\'is", "authors": "Maikol Sol\\'is, Carlos Pasquier", "title": "Using relative weight analysis with residualization to detect relevant\n  nonlinear interaction effects in ordinary and logistic regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative weight analysis is a classic tool to detect if one variable or\ninteraction in a model is relevant or not. In this paper, we will focus on the\nconstruction of relative weights for non-linear interactions using restricted\ncubic splines. Our aim is to provide an accessible method to analyze a\nmultivariate model and identify one subset with the most representative set of\nvariables. Furthermore, we developed a procedure treating control, fixed, free\nand interactions terms at the same time in the residual weight analysis. The\ninteractions are residualized properly against their main effects to keep their\ntrue effect in the model. We test this method with two simulated examples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 21:20:07 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sol\u00eds", "Maikol", ""], ["Pasquier", "Carlos", ""]]}, {"id": "2106.14109", "submitter": "Han Fu", "authors": "Han Fu (1), Shahrul Mt-Isa (2), Richard Baumgartner (3), William\n  Malbecq (2) ((1) The Ohio State University, (2) MSD, (3) Merck)", "title": "Parmsurv: a SAS Macro for Flexible Parametric Survival Analysis with\n  Long-Term Predictions", "comments": "15 pages, 1 figure, 10 tables, accepted by The Clinical Data Science\n  Conference - PHUSE US Connect 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic evaluations often require predictions of survival rates\nbeyond the follow-up period. Parametric survival models can be more convenient\nfor economic modelling than the Cox model. The generalized gamma (GG) and\ngeneralized F (GF) distributions are extensive families that contain almost all\ncommonly used distributions with various hazard shapes and arbitrary\ncomplexity. In this study, we present a new SAS macro for implementing a wide\nvariety of flexible parametric models including the GG and GF distributions and\ntheir special cases, as well as the Gompertz distribution. Proper custom\ndistributions are also supported. Different from existing SAS procedures, this\nmacro not only supports regression on the location parameter but also on\nancillary parameters, which greatly increases model flexibility. In addition,\nthe SAS macro supports weighted regression, stratified regression and robust\ninference. This study demonstrates with several examples how the SAS macro can\nbe used for flexible survival modeling and extrapolation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 23:24:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fu", "Han", "", "The Ohio State University"], ["Mt-Isa", "Shahrul", "", "MSD"], ["Baumgartner", "Richard", "", "Merck"], ["Malbecq", "William", "", "MSD"]]}, {"id": "2106.14110", "submitter": "Amartya Mukherjee", "authors": "Amartya Mukherjee, Yusuf Aydogdu, Thambirajah Ravichandran and\n  Navaratnam Sri Namachchivaya", "title": "Stochastic Parametrization using Compressed Sensing: Application to the\n  Lorenz-96 Atmospheric Model", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.OC physics.ao-ph stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Growing set of optimization and regression techniques, based upon sparse\nrepresentations of signals, to build models from data sets has received\nwidespread attention recently with the advent of compressed sensing. In this\npaper, sparse approximations in high-dimensional spaces are used to build\nmodels (vector fields) to emulate the behavior of the fine-scale process, so\nthat explicit simulations become an online benchmark for parameterization.\nObservations are assimilated during the integration of low-dimensional built\nmodel to provide predictions. We outline how the parameterization schemes\ndeveloped here and the low-dimensional filtering algorithm can be applied to\nthe Lorenz-96 atmospheric model that mimics mid-latitude atmospheric dynamics\nwith microscopic convective processes.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 23:25:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Mukherjee", "Amartya", ""], ["Aydogdu", "Yusuf", ""], ["Ravichandran", "Thambirajah", ""], ["Namachchivaya", "Navaratnam Sri", ""]]}, {"id": "2106.14215", "submitter": "Nina Golyandina", "authors": "Nikita Zvonarev and Nina Golyandina", "title": "Fast and stable modification of the Gauss-Newton method for low-rank\n  signal estimation", "comments": "arXiv admin note: text overlap with arXiv:2101.09779,\n  arXiv:1803.01419", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The weighted nonlinear least-squares problem for low-rank signal estimation\nis considered. The problem of constructing a numerical solution that is stable\nand fast for long time series is addressed. A modified weighted Gauss-Newton\nmethod, which can be implemented through the direct variable projection onto a\nspace of low-rank signals, is proposed. For a weight matrix which provides the\nmaximum likelihood estimator of the signal in the presence of autoregressive\nnoise of order $p$ the computational cost of iterations is $O(N r^2 + N p^2 + r\nN \\log N)$ as $N$ tends to infinity, where $N$ is the time-series length, $r$\nis the rank of the approximating time series. Moreover, the proposed method can\nbe applied to data with missing values, without increasing the computational\ncost. The method is compared with state-of-the-art methods based on the\nvariable projection approach in terms of floating-point numerical stability and\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 12:19:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zvonarev", "Nikita", ""], ["Golyandina", "Nina", ""]]}, {"id": "2106.14258", "submitter": "Jianhao Zhang", "authors": "Jianhao Zhang and Yoonkyung Lee", "title": "Sparse Logistic Tensor Decomposition for Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tensor data are increasingly available in many application domains. We\ndevelop several tensor decomposition methods for binary tensor data. Different\nfrom classical tensor decompositions for continuous-valued data with squared\nerror loss, we formulate logistic tensor decompositions for binary data with a\nBernoulli likelihood. To enhance the interpretability of estimated factors and\nimprove their stability further, we propose sparse formulations of logistic\ntensor decomposition by considering $\\ell_{1}$-norm and $\\ell_{0}$-norm\nregularized likelihood. To handle the resulting optimization problems, we\ndevelop computational algorithms which combine the strengths of tensor power\nmethod and majorization-minimization (MM) algorithm. Through simulation\nstudies, we demonstrate the utility of our methods in analysis of binary tensor\ndata. To illustrate the effectiveness of the proposed methods, we analyze a\ndataset concerning nations and their political relations and perform\nco-clustering of estimated factors to find associations between the nations and\npolitical relations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:22:20 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Jianhao", ""], ["Lee", "Yoonkyung", ""]]}, {"id": "2106.14392", "submitter": "David Gunawan", "authors": "David Gunawan and Robert Kohn and David Nott", "title": "Flexible Variational Bayes based on a Copula of a Mixture of Normals", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes methods approximate the posterior density by a family of\ntractable distributions and use optimisation to estimate the unknown parameters\nof the approximation. Variational approximation is useful when exact inference\nis intractable or very costly. Our article develops a flexible variational\napproximation based on a copula of a mixture of normals, which is implemented\nusing the natural gradient and a variance reduction method. The efficacy of the\napproach is illustrated by using simulated and real datasets to approximate\nmultimodal, skewed and heavy-tailed posterior distributions, including an\napplication to Bayesian deep feedforward neural network regression models. Each\nexample shows that the proposed variational approximation is much more accurate\nthan the corresponding Gaussian copula and a mixture of normals variational\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 04:50:31 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:49:46 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Nott", "David", ""]]}, {"id": "2106.14415", "submitter": "Patrick J. Laub", "authors": "Young Lee, Patrick J. Laub, Thomas Taimre, Hongbiao Zhao, Jiancang\n  Zhuang", "title": "Exact simulation of extrinsic stress-release processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and straightforward algorithm that simulates exact sample\npaths for a generalized stress-release process. The computation of the exact\nlaw of the joint interarrival times is detailed and used to derive this\nalgorithm. Furthermore, the martingale generator of the process is derived and\ninduces theoretical moments which generalize some results of Borovkov &\nVere-Jones (2000) and are used to demonstrate the validity of our simulation\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 06:30:10 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lee", "Young", ""], ["Laub", "Patrick J.", ""], ["Taimre", "Thomas", ""], ["Zhao", "Hongbiao", ""], ["Zhuang", "Jiancang", ""]]}, {"id": "2106.14529", "submitter": "Johannes Buchner", "authors": "Johannes Buchner, Thomas Boller, David Bogensberger, Adam Malyali,\n  Kirpal Nandra, Joern Wilms, Tom Dwelly, Teng Liu", "title": "Systematic evaluation of variability detection methods for eROSITA", "comments": "Resubmitted version after a positive first referee report.\n  Variability analysis tools available\n  https://github.com/JohannesBuchner/bexvar/. 15 min Talk:\n  https://youtu.be/xBC1S9MTH4w. To appear on A&A, Special Issue: The Early Data\n  Release of eROSITA and Mikhail Pavlinsky ART-XC on the SRG Mission", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE astro-ph.IM stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The reliability of detecting source variability in sparsely and irregularly\nsampled X-ray light curves is investigated. This is motivated by the\nunprecedented survey capabilities of eROSITA onboard SRG, providing light\ncurves for many thousand sources in its final-depth equatorial deep field\nsurvey. Four methods for detecting variability are evaluated: excess variance,\namplitude maximum deviations, Bayesian blocks and a new Bayesian formulation of\nthe excess variance. We judge the false detection rate of variability based on\nsimulated Poisson light curves of constant sources, and calibrate significance\nthresholds. Simulations with flares injected favour the amplitude maximum\ndeviation as most sensitive at low false detections. Simulations with white and\nred stochastic source variability favour Bayesian methods. The results are\napplicable also for the million sources expected in eROSITA's all-sky survey.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:08:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Buchner", "Johannes", ""], ["Boller", "Thomas", ""], ["Bogensberger", "David", ""], ["Malyali", "Adam", ""], ["Nandra", "Kirpal", ""], ["Wilms", "Joern", ""], ["Dwelly", "Tom", ""], ["Liu", "Teng", ""]]}, {"id": "2106.14565", "submitter": "Anant Mathur", "authors": "Anant Mathur, Sarat Moka and Zdravko Botev", "title": "Variance Reduction for Matrix Computations with Applications to Gaussian\n  Processes", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In addition to recent developments in computing speed and memory,\nmethodological advances have contributed to significant gains in the\nperformance of stochastic simulation. In this paper, we focus on variance\nreduction for matrix computations via matrix factorization. We provide insights\ninto existing variance reduction methods for estimating the entries of large\nmatrices. Popular methods do not exploit the reduction in variance that is\npossible when the matrix is factorized. We show how computing the square root\nfactorization of the matrix can achieve in some important cases arbitrarily\nbetter stochastic performance. In addition, we propose a factorized estimator\nfor the trace of a product of matrices and numerically demonstrate that the\nestimator can be up to 1,000 times more efficient on certain problems of\nestimating the log-likelihood of a Gaussian process. Additionally, we provide a\nnew estimator of the log-determinant of a positive semi-definite matrix where\nthe log-determinant is treated as a normalizing constant of a probability\ndensity.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:41:22 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 08:19:40 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mathur", "Anant", ""], ["Moka", "Sarat", ""], ["Botev", "Zdravko", ""]]}, {"id": "2106.14599", "submitter": "Chuji Luo", "authors": "Chuji Luo, Michael J. Daniels", "title": "BNPqte: A Bayesian Nonparametric Approach to Causal Inference on\n  Quantiles in R", "comments": "44 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the BNPqte R package which implements the\nBayesian nonparametric approach of Xu, Daniels and Winterstein (2018) for\nestimating quantile treatment effects in observational studies. This approach\nprovides flexible modeling of the distributions of potential outcomes, so it is\ncapable of capturing a variety of underlying relationships among the outcomes,\ntreatments and confounders and estimating multiple quantile treatment effects\nsimultaneously. Specifically, this approach uses a Bayesian additive regression\ntrees (BART) model to estimate the propensity score and a Dirichlet process\nmixture (DPM) of multivariate normals model to estimate the conditional\ndistribution of the potential outcome given the estimated propensity score. The\nBNPqte R package provides a fast implementation for this approach by designing\nefficient R functions for the DPM of multivariate normals model in joint and\nconditional density estimation. These R functions largely improve the\nefficiency of the DPM model in density estimation, compared to the popular\nDPpackage. BART-related R functions in the BNPqte R package are inherited from\nthe BART R package with two modifications on variable importance and split\nprobability. To maximize computational efficiency, the actual sampling and\ncomputation for each model are carried out in C++ code. The Armadillo C++\nlibrary is also used for fast linear algebra calculations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:11:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Luo", "Chuji", ""], ["Daniels", "Michael J.", ""]]}, {"id": "2106.14648", "submitter": "Lucile Ter-Minassian", "authors": "Sahra Ghalebikesabi, Lucile Ter-Minassian, Karla Diaz-Ordaz and Chris\n  Holmes", "title": "On Locality of Local Explanation Models", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shapley values provide model agnostic feature attributions for model outcome\nat a particular instance by simulating feature absence under a global\npopulation distribution. The use of a global population can lead to potentially\nmisleading results when local model behaviour is of interest. Hence we consider\nthe formulation of neighbourhood reference distributions that improve the local\ninterpretability of Shapley values. By doing so, we find that the\nNadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as\na self-normalised importance sampling estimator. Empirically, we observe that\nNeighbourhood Shapley values identify meaningful sparse feature relevance\nattributions that provide insight into local model behaviour, complimenting\nconventional Shapley analysis. They also increase on-manifold explainability\nand robustness to the construction of adversarial classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:20:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ghalebikesabi", "Sahra", ""], ["Ter-Minassian", "Lucile", ""], ["Diaz-Ordaz", "Karla", ""], ["Holmes", "Chris", ""]]}, {"id": "2106.14981", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak", "title": "Fast Bayesian Variable Selection in Binomial and Negative Binomial\n  Regression", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection is a powerful tool for data analysis, as it\noffers a principled method for variable selection that accounts for prior\ninformation and uncertainty. However, wider adoption of Bayesian variable\nselection has been hampered by computational challenges, especially in\ndifficult regimes with a large number of covariates or non-conjugate\nlikelihoods. Generalized linear models for count data, which are prevalent in\nbiology, ecology, economics, and beyond, represent an important special case.\nHere we introduce an efficient MCMC scheme for variable selection in binomial\nand negative binomial regression that exploits Tempered Gibbs Sampling (Zanella\nand Roberts, 2019) and that includes logistic regression as a special case. In\nexperiments we demonstrate the effectiveness of our approach, including on\ncancer data with seventeen thousand covariates.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 20:54:41 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Jankowiak", "Martin", ""]]}, {"id": "2106.15188", "submitter": "Ion Gabriel Ion", "authors": "Ion Gabriel Ion, Christian Wildner, Dimitrios Loukrezis, Heinz Koeppl,\n  Herbert De Gersem", "title": "Tensor-train approximation of the chemical master equation and its\n  application for parameter inference", "comments": null, "journal-ref": null, "doi": "10.1063/5.0045521", "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this work, we perform Bayesian inference tasks for the chemical master\nequation in the tensor-train format. The tensor-train approximation has been\nproven to be very efficient in representing high dimensional data arising from\nthe explicit representation of the chemical master equation solution. An\nadditional advantage of representing the probability mass function in the\ntensor train format is that parametric dependency can be easily incorporated by\nintroducing a tensor product basis expansion in the parameter space. Time is\ntreated as an additional dimension of the tensor and a linear system is derived\nto solve the chemical master equation in time. We exemplify the tensor-train\nmethod by performing inference tasks such as smoothing and parameter inference\nusing the tensor-train framework. A very high compression ratio is observed for\nstoring the probability mass function of the solution. Since all linear algebra\noperations are performed in the tensor-train format, a significant reduction of\nthe computational time is observed as well.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:08:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 12:48:16 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Ion", "Ion Gabriel", ""], ["Wildner", "Christian", ""], ["Loukrezis", "Dimitrios", ""], ["Koeppl", "Heinz", ""], ["De Gersem", "Herbert", ""]]}, {"id": "2106.15812", "submitter": "Patrick Chao", "authors": "Patrick Chao, William Fithian", "title": "AdaPT-GMM: Powerful and robust covariate-assisted multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new empirical Bayes method for covariate-assisted multiple\ntesting with false discovery rate (FDR) control, where we model the local false\ndiscovery rate for each hypothesis as a function of both its covariates and\np-value. Our method refines the adaptive p-value thresholding (AdaPT) procedure\nby generalizing its masking scheme to reduce the bias and variance of its false\ndiscovery proportion estimator, improving the power when the rejection set is\nsmall or some null p-values concentrate near 1. We also introduce a Gaussian\nmixture model for the conditional distribution of the test statistics given\ncovariates, modeling the mixing proportions with a generic user-specified\nclassifier, which we implement using a two-layer neural network. Like AdaPT,\nour method provably controls the FDR in finite samples even if the classifier\nor the Gaussian mixture model is misspecified. We show in extensive simulations\nand real data examples that our new method, which we call AdaPT-GMM,\nconsistently delivers high power relative to competing state-of-the-art\nmethods. In particular, it performs well in scenarios where AdaPT is\nunderpowered, and is especially well-suited for testing composite null\nhypothesis, such as whether the effect size exceeds a practical significance\nthreshold.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 05:06:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chao", "Patrick", ""], ["Fithian", "William", ""]]}, {"id": "2106.15980", "submitter": "Ghassen Jerfel", "authors": "Ghassen Jerfel, Serena Wang, Clara Fannjiang, Katherine A. Heller,\n  Yian Ma, Michael I. Jordan", "title": "Variational Refinement for Importance Sampling Using the Forward\n  Kullback-Leibler Divergence", "comments": "Accepted for the 37th Conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Variational Inference (VI) is a popular alternative to asymptotically exact\nsampling in Bayesian inference. Its main workhorse is optimization over a\nreverse Kullback-Leibler divergence (RKL), which typically underestimates the\ntail of the posterior leading to miscalibration and potential degeneracy.\nImportance sampling (IS), on the other hand, is often used to fine-tune and\nde-bias the estimates of approximate Bayesian inference procedures. The quality\nof IS crucially depends on the choice of the proposal distribution. Ideally,\nthe proposal distribution has heavier tails than the target, which is rarely\nachievable by minimizing the RKL. We thus propose a novel combination of\noptimization and sampling techniques for approximate Bayesian inference by\nconstructing an IS proposal distribution through the minimization of a forward\nKL (FKL) divergence. This approach guarantees asymptotic consistency and a fast\nconvergence towards both the optimal IS estimator and the optimal variational\napproximation. We empirically demonstrate on real data that our method is\ncompetitive with variational boosting and MCMC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 11:00:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jerfel", "Ghassen", ""], ["Wang", "Serena", ""], ["Fannjiang", "Clara", ""], ["Heller", "Katherine A.", ""], ["Ma", "Yian", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2106.16200", "submitter": "Giulio Franzese", "authors": "Giulio Franzese, Dimitrios Milios, Maurizio Filippone, Pietro\n  Michiardi", "title": "A Unified View of Stochastic Hamiltonian Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we revisit the theoretical properties of Hamiltonian stochastic\ndifferential equations (SDEs) for Bayesian posterior sampling, and we study the\ntwo types of errors that arise from numerical SDE simulation: the\ndiscretization error and the error due to noisy gradient estimates in the\ncontext of data subsampling. We consider overlooked results describing the\nergodic convergence rates of numerical integration schemes, and we produce a\nnovel analysis for the effect of mini-batches through the lens of differential\noperator splitting. In our analysis, the stochastic component of the proposed\nHamiltonian SDE is decoupled from the gradient noise, for which we make no\nnormality assumptions. This allows us to derive interesting connections among\ndifferent sampling schemes, including the original Hamiltonian Monte Carlo\n(HMC) algorithm, and explain their performance. We show that for a careful\nselection of numerical integrators, both errors vanish at a rate\n$\\mathcal{O}(\\eta^2)$, where $\\eta$ is the integrator step size. Our\ntheoretical results are supported by an empirical study on a variety of\nregression and classification tasks for Bayesian neural networks.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 16:50:11 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Franzese", "Giulio", ""], ["Milios", "Dimitrios", ""], ["Filippone", "Maurizio", ""], ["Michiardi", "Pietro", ""]]}]