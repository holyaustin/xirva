[{"id": "2003.00371", "submitter": "Bradley Price", "authors": "Bradley S. Price and Aaron J. Molstad and Ben Sherwood", "title": "Estimating Multiple Precision Matrices with Cluster Fusion\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood framework for estimating multiple precision\nmatrices from different classes. Most existing methods either incorporate no\ninformation on relationships between the precision matrices, or require this\ninformation be known a priori. The framework proposed in this article allows\nfor simultaneous estimation of the precision matrices and relationships between\nthe precision matrices, jointly. Sparse and non-sparse estimators are proposed,\nboth of which require solving a non-convex optimization problem. To compute our\nproposed estimators, we use an iterative algorithm which alternates between a\nconvex optimization problem solved by blockwise coordinate descent and a\nk-means clustering problem. Blockwise updates for computing the sparse\nestimator require solving an elastic net penalized precision matrix estimation\nproblem, which we solve using a proximal gradient descent algorithm. We prove\nthat this subalgorithm has a linear rate of convergence. In simulation studies\nand two real data applications, we show that our method can outperform\ncompetitors that ignore relevant relationships between precision matrices and\nperforms similarly to methods which use prior information often uknown in\npractice.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2020 01:03:22 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Price", "Bradley S.", ""], ["Molstad", "Aaron J.", ""], ["Sherwood", "Ben", ""]]}, {"id": "2003.00844", "submitter": "Kirankumar Shiragur", "authors": "Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "A General Framework for Symmetric Property Estimation", "comments": "Published in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a general framework for estimating symmetric\nproperties of distributions from i.i.d. samples. For a broad class of symmetric\nproperties we identify the easy region where empirical estimation works and the\ndifficult region where more complex estimators are required. We show that by\napproximately computing the profile maximum likelihood (PML) distribution\n\\cite{ADOS16} in this difficult region we obtain a symmetric property\nestimation framework that is sample complexity optimal for many properties in a\nbroader parameter regime than previous universal estimation approaches based on\nPML. The resulting algorithms based on these pseudo PML distributions are also\nmore practical.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2020 13:00:04 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "2003.01286", "submitter": "Zheyang Wu", "authors": "Hong Zhang and Zheyang Wu", "title": "Accurate $p$-Value Calculation for Generalized Fisher's Combination\n  Tests Under Dependence", "comments": "53 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining dependent tests of significance has broad applications but the\n$p$-value calculation is challenging. Current moment-matching methods (e.g.,\nBrown's approximation) for Fisher's combination test tend to significantly\ninflate the type I error rate at the level less than 0.05. It could lead to\nsignificant false discoveries in big data analyses. This paper provides several\nmore accurate and computationally efficient $p$-value calculation methods for a\ngeneral family of Fisher type statistics, referred as the GFisher. The GFisher\ncovers Fisher's combination, Good's statistic, Lancaster's statistic, weighted\nZ-score combination, etc. It allows a flexible weighting scheme, as well as an\nomnibus procedure that automatically adapts proper weights and degrees of\nfreedom to a given data. The new $p$-value calculation methods are based on\nnovel ideas of moment-ratio matching and joint-distribution surrogating.\nSystematic simulations show that they are accurate under multivariate Gaussian,\nand robust under the generalized linear model and the multivariate\n$t$-distribution, down to at least $10^{-6}$ level. We illustrate the\nusefulness of the GFisher and the new $p$-value calculation methods in\nanalyzing both simulated and real data of gene-based SNP-set association\nstudies in genetics. Relevant computation has been implemented into R package\n$GFisher$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 01:33:17 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Zhang", "Hong", ""], ["Wu", "Zheyang", ""]]}, {"id": "2003.01804", "submitter": "Piaomu Liu", "authors": "Piaomu Liu", "title": "Prediction of Time-to-terminal Event (TTTE) in a Class of Joint Dynamic\n  Models", "comments": "The title is edited. A real-life scenario where the proposed method\n  can be applied to is added to Introduction. Model evaluation is updated\n  (5-fold cross-validation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In different areas of research, multiple recurrent competing risks (RCR) are\noften observed on the same observational unit. For instance, different types of\ncancer relapses are observed on the same patient and several types of component\nfailures are observed in the same reliability system. When a terminal event\n(TE) such as death is also observed on the same unit, since the RCRs are\ngenerally informative about death, we develop joint dynamic models that\nsimultaneously model the RCRs and the TE. A key interest of such joint dynamic\nmodeling is to predict time-to-terminal event (TTTE) for new units that have\nnot experienced the TE by the end of monitoring period. In this paper, we\npropose a simulation approach to predict TTTE which arises from a class of\njoint dynamic models of RCRs and TE. The proposed approach can be applied to\nproblems in precision medicine and potentially many other settings. The\nsimulation method makes personalized predictions of TTTE and provides an\nempirical predictive distribution of TTTE. Predictions of the RCR occurrences\nbeyond a possibly random monitoring time and leading up to the TE occurrence\nare also produced. The approach is dynamic in that each simulated occurrence of\nRCR increases the amount of knowledge we obtain on an observational unit which\ninforms the simulation of TTTE. We demonstrate the approach on a synthetic\ndataset and evaluate predictive accuracy of the prediction method through\n5-fold cross-validation using empirical Brier score.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2020 21:24:41 GMT"}, {"version": "v2", "created": "Sun, 25 Oct 2020 18:56:50 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Liu", "Piaomu", ""]]}, {"id": "2003.02109", "submitter": "Tadeo Javier Cocucci", "authors": "Tadeo Javier Cocucci, Manuel Pulido, Magdalena Lucini and Pierre\n  Tandeo", "title": "Model error covariance estimation in particle and ensemble Kalman\n  filters using an online expectation-maximization algorithm", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3931", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of ensemble-based data assimilation techniques that estimate\nthe state of a dynamical system from partial observations depends crucially on\nthe prescribed uncertainty of the model dynamics and of the observations. These\nare not usually known and have to be inferred. Many approaches have been\nproposed to tackle this problem, including fully Bayesian, likelihood\nmaximization and innovation-based techniques. This work focuses on maximization\nof the likelihood function via the expectation-maximization (EM) algorithm to\ninfer the model error covariance combined with ensemble Kalman filters and\nparticle filters to estimate the state. The classical application of the EM\nalgorithm in a data assimilation context involves filtering and smoothing a\nfixed batch of observations in order to complete a single iteration. This is an\ninconvenience when using sequential filtering in high-dimensional applications.\nMotivated by this, an adaptation of the algorithm that can process observations\nand update the parameters on the fly, with some underlying simplifications, is\npresented. The proposed technique was evaluated and achieved good performance\nin experiments with the Lorenz-63 and the 40-variable Lorenz-96 dynamical\nsystems designed to represent some common scenarios in data assimilation such\nas non-linearity, chaoticity and model misspecification.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 14:48:13 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cocucci", "Tadeo Javier", ""], ["Pulido", "Manuel", ""], ["Lucini", "Magdalena", ""], ["Tandeo", "Pierre", ""]]}, {"id": "2003.02311", "submitter": "Matteo Croci", "authors": "Matteo Croci, Vegard Vinje, Marie E. Rognes", "title": "Fast uncertainty quantification of tracer distribution in the brain\n  interstitial fluid with multilevel and quasi Monte Carlo", "comments": "Multilevel Monte Carlo, quasi Monte Carlo, brain simulation, brain\n  fluids, finite element method, biomedical computing, random fields,\n  diffusion-convection", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient uncertainty quantification algorithms are key to understand the\npropagation of uncertainty -- from uncertain input parameters to uncertain\noutput quantities -- in high resolution mathematical models of brain\nphysiology. Advanced Monte Carlo methods such as quasi Monte Carlo (QMC) and\nmultilevel Monte Carlo (MLMC) have the potential to dramatically improve upon\nstandard Monte Carlo (MC) methods, but their applicability and performance in\nbiomedical applications is underexplored. In this paper, we design and apply\nQMC and MLMC methods to quantify uncertainty in a convection-diffusion model of\ntracer transport within the brain. We show that QMC outperforms standard MC\nsimulations when the number of random inputs is small. MLMC considerably\noutperforms both QMC and standard MC methods and should therefore be preferred\nfor brain transport models.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 20:10:36 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:46:12 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Croci", "Matteo", ""], ["Vinje", "Vegard", ""], ["Rognes", "Marie E.", ""]]}, {"id": "2003.02344", "submitter": "Guillaume Gautier", "authors": "Guillaume Gautier, R\\'emi Bardenet, and Michal Valko", "title": "Fast sampling from $\\beta$-ensembles", "comments": "37 pages, 8 figures, code at https://github.com/guilgautier/DPPy", "journal-ref": "Stat. Comput. 31 (2021) 7", "doi": "10.1007/s11222-020-09984-0", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sampling algorithms for $\\beta$-ensembles with time complexity less\nthan cubic in the cardinality of the ensemble. Following Dumitriu & Edelman\n(2002), we see the ensemble as the eigenvalues of a random tridiagonal matrix,\nnamely a random Jacobi matrix. First, we provide a unifying and elementary\ntreatment of the tridiagonal models associated to the three classical Hermite,\nLaguerre and Jacobi ensembles. For this purpose, we use simple changes of\nvariables between successive reparametrizations of the coefficients defining\nthe tridiagonal matrix. Second, we derive an approximate sampler for the\nsimulation of $\\beta$-ensembles, and illustrate how fast it can be for\npolynomial potentials. This method combines a Gibbs sampler on Jacobi matrices\nand the diagonalization of these matrices. In practice, even for large\nensembles, only a few Gibbs passes suffice for the marginal distribution of the\neigenvalues to fit the expected theoretical distribution. When the conditionals\nin the Gibbs sampler can be simulated exactly, the same fast empirical\nconvergence is observed for the fluctuations of the largest eigenvalue. Our\nexperimental results support a conjecture by Krishnapur et al. (2016), that the\nGibbs chain on Jacobi matrices of size $N$ mixes in $\\mathcal{O}(\\log(N))$.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2020 21:57:56 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Gautier", "Guillaume", ""], ["Bardenet", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "2003.02842", "submitter": "Patrick Chang", "authors": "Patrick Chang, Etienne Pienaar, Tim Gebbie", "title": "Malliavin-Mancino estimators implemented with non-uniform fast Fourier\n  transforms", "comments": "29 pages, 15 figures, 3 tables, 10 algorithms, link to our supporting\n  Julia code: https://github.com/CHNPAT005/PCEPTG-MM-NUFFT; v3: Accepted\n  submitted version for SISC", "journal-ref": "SIAM J. Sci. Comput., 2020, 42(6), B1378 - B1403", "doi": "10.1137/20M1325903", "report-no": null, "categories": "q-fin.CP q-fin.TR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We implement and test kernel averaging Non-Uniform Fast Fourier Transform\n(NUFFT) methods to enhance the performance of correlation and covariance\nestimation on asynchronously sampled event-data using the Malliavin-Mancino\nFourier estimator. The methods are benchmarked for Dirichlet and Fej\\'{e}r\nFourier basis kernels. We consider test cases formed from Geometric Brownian\nmotions to replicate synchronous and asynchronous data for benchmarking\npurposes. We consider three standard averaging kernels to convolve the\nevent-data for synchronisation via over-sampling for use with the Fast Fourier\nTransform (FFT): the Gaussian kernel, the Kaiser-Bessel kernel, and the\nexponential of semi-circle kernel. First, this allows us to demonstrate the\nperformance of the estimator with different combinations of basis kernels and\naveraging kernels. Second, we investigate and compare the impact of the\naveraging scales explicit in each averaging kernel and its relationship between\nthe time-scale averaging implicit in the Malliavin-Mancino estimator. Third, we\ndemonstrate the relationship between time-scale averaging based on the number\nof Fourier coefficients used in the estimator to a theoretical model of the\nEpps effect. We briefly demonstrate the methods on Trade-and-Quote (TAQ) data\nfrom the Johannesburg Stock Exchange to make an initial visualisation of the\ncorrelation dynamics for various time-scales under market microstructure.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 16:42:45 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 16:06:46 GMT"}, {"version": "v3", "created": "Tue, 10 Nov 2020 16:33:03 GMT"}], "update_date": "2020-11-11", "authors_parsed": [["Chang", "Patrick", ""], ["Pienaar", "Etienne", ""], ["Gebbie", "Tim", ""]]}, {"id": "2003.02929", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik, Florian Frommlet", "title": "Flexible Bayesian Nonlinear Model Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are used in a wide range of applications providing a\npowerful scientific tool for researchers from different fields. Linear models\nare often not sufficient to describe the complex relationship between input\nvariables and a response. This relationship can be better described by\nnon-linearities and complex functional interactions. Deep learning models have\nbeen extremely successful in terms of prediction although they are often\ndifficult to specify and potentially suffer from overfitting. In this paper, we\nintroduce a class of Bayesian generalized nonlinear regression models with a\ncomprehensive non-linear feature space. Non-linear features are generated\nhierarchically, similarly to deep learning, but have additional flexibility on\nthe possible types of features to be considered. This flexibility, combined\nwith variable selection, allows us to find a small set of important features\nand thereby more interpretable models. A genetically modified Markov chain\nMonte Carlo algorithm is developed to make inference. Model averaging is also\npossible within our framework. In various applications, we illustrate how our\napproach is used to obtain meaningful non-linear models. Additionally, we\ncompare its predictive performance with a number of machine learning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:20:55 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "2003.02941", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of test power for Z-test and Chi-square test with\n  auxiliary information", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main goal of this article is to study how an auxiliary information can be\nused to improve the power of two famous statistical tests: the $ Z$-test and\nthe chi-square test. This information can be of any nature - probability of\nsets of partitions, expectation of a function, ... - and is not even required\nto be an exact information, it can be given by an estimate based on a larger\nsample for example. Some definitions of auxiliary information can be found in\nthe statistical literature and will be recalled. In this article, the notion of\nauxiliary information is discussed here from a very general point of view.\nThese two statistical tests are modified so that the auxiliary information is\ntaken into account. One show in particular that the power of these tests is\nincreased exponentially. Some statistical examples are treated to show the\nconcreteness of this method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2020 21:46:21 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2003.03187", "submitter": "Jouni Helske", "authors": "Jouni Helske, Santtu Tikka, Juha Karvanen", "title": "Estimation of causal effects with small data in the presence of trapdoor\n  variables", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating causal effects of interventions from\nobservational data when well-known back-door and front-door adjustments are not\napplicable. We show that when an identifiable causal effect is subject to an\nimplicit functional constraint that is not deducible from conditional\nindependence relations, the estimator of the causal effect can exhibit bias in\nsmall samples. This bias is related to variables that we call trapdoor\nvariables. We use simulated data to study different strategies to account for\ntrapdoor variables and suggest how the related trapdoor bias might be\nminimized. The importance of trapdoor variables in causal effect estimation is\nillustrated with real data from the Life Course 1971-2002 study. Using this\ndataset, we estimate the causal effect of education on income in the Finnish\ncontext. Bayesian modelling allows us to take the parameter uncertainty into\naccount and to present the estimated causal effects as posterior distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 13:28:44 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 17:34:20 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 15:34:04 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Helske", "Jouni", ""], ["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "2003.03258", "submitter": "Llu\\'is Alemany-Puig", "authors": "Llu\\'is Alemany-Puig and Ramon Ferrer-i-Cancho", "title": "Fast calculation of the variance of edge crossings", "comments": "Better connection with graph theory (crossing number). Introduction\n  and discussion substantially rewritten. Minor corrections in other parts of\n  the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DM cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crossing number, i.e. the minimum number of edge crossings arising when\ndrawing a graph on a certain surface, is a very important problem of graph\ntheory. The opposite problem, i.e. the maximum crossing number, is receiving\ngrowing attention. Here we consider a complementary problem of the distribution\nof the number of edge crossings, namely the variance of the number of\ncrossings, when embedding the vertices of an arbitrary graph in some space at\nrandom. In his pioneering research, Moon derived that variance on random linear\narrangements of complete unipartite and bipartite graphs. Given the need of\nefficient algorithms to support this sort of research and given also the\ngrowing interest of the number of edge crossings in spatial networks, networks\nwhere vertices are embedded in some space, here we derive algorithms to\ncalculate the variance in arbitrary graphs in $o(nm^2)$-time, and in forests in\n$O(n)$-time. These algorithms work on a wide range of random layouts (not only\non Moon's) and are based on novel arithmetic expressions for the calculation of\nthe variance that we develop from previous theoretical work. This paves the way\nfor many applications that rely on a fast but exact calculation of the\nvariance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 14:55:28 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 18:21:56 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Alemany-Puig", "Llu\u00eds", ""], ["Ferrer-i-Cancho", "Ramon", ""]]}, {"id": "2003.03341", "submitter": "Juan Pablo Madrigal Cianci", "authors": "Jonas Latz, Juan P. Madrigal-Cianci, Fabio Nobile, Raul Tempone", "title": "Generalized Parallel Tempering on Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current work we present two generalizations of the Parallel Tempering\nalgorithm, inspired by the so-called continuous-time Infinite Swapping\nalgorithm. Such a method, found its origins in the molecular dynamics\ncommunity, and can be understood as the limit case of the continuous-time\nParallel Tempering algorithm, where the (random) time between swaps of states\nbetween two parallel chains goes to zero. Thus, swapping states between chains\noccurs continuously. In the current work, we extend this idea to the context of\ntime-discrete Markov chains and present two Markov chain Monte Carlo algorithms\nthat follow the same paradigm as the continuous-time infinite swapping\nprocedure. We analyze the convergence properties of such discrete-time\nalgorithms in terms of their spectral gap, and implement them to sample from\ndifferent target distributions. Numerical results show that the proposed\nmethods significantly improve over more traditional sampling algorithms such as\nRandom Walk Metropolis and (traditional) Parallel Tempering.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2020 18:13:22 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 08:16:21 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Latz", "Jonas", ""], ["Madrigal-Cianci", "Juan P.", ""], ["Nobile", "Fabio", ""], ["Tempone", "Raul", ""]]}, {"id": "2003.03508", "submitter": "Stephanus Marnus Stoltz", "authors": "Marnus Stoltz, Gene Stoltz, Kazushige Obara, Ting Wang, David Bryant", "title": "A 1000-fold Acceleration of Hidden Markov Model Fitting using Graphical\n  Processing Units, with application to Nonvolcanic Tremor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hidden Markov models (HMMs) are general purpose models for time-series data\nwidely used across the sciences because of their flexibility and elegance.\nHowever fitting HMMs can often be computationally demanding and time consuming,\nparticularly when the the number of hidden states is large or the Markov chain\nitself is long. Here we introduce a new Graphical Processing Unit (GPU) based\nalgorithm designed to fit long chain HMMs, applying our approach to an HMM for\nnonvolcanic tremor events developed by Wang et al.(2018). Even on a modest GPU,\nour implementation resulted in a 1000-fold increase in speed over the standard\nsingle processor algorithm, allowing a full Bayesian inference of uncertainty\nrelated to model parameters. Similar improvements would be expected for HMM\nmodels given large number of observations and moderate state spaces (<80 states\nwith current hardware). We discuss the model, general GPU architecture and\nalgorithms and report performance of the method on a tremor dataset from the\nShikoku region, Japan.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 03:44:21 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Stoltz", "Marnus", ""], ["Stoltz", "Gene", ""], ["Obara", "Kazushige", ""], ["Wang", "Ting", ""], ["Bryant", "David", ""]]}, {"id": "2003.03546", "submitter": "Victor Gallego", "authors": "David Rios Insua, Roi Naveiro, Victor Gallego, Jason Poulos", "title": "Adversarial Machine Learning: Perspectives from Adversarial Risk\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial Machine Learning (AML) is emerging as a major field aimed at the\nprotection of automated ML systems against security threats. The majority of\nwork in this area has built upon a game-theoretic framework by modelling a\nconflict between an attacker and a defender. After reviewing game-theoretic\napproaches to AML, we discuss the benefits that a Bayesian Adversarial Risk\nAnalysis perspective brings when defending ML based systems. A research agenda\nis included.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 10:30:43 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Insua", "David Rios", ""], ["Naveiro", "Roi", ""], ["Gallego", "Victor", ""], ["Poulos", "Jason", ""]]}, {"id": "2003.03636", "submitter": "Filippo Pagani Mr", "authors": "Simon Cotter and Thomas House and Filippo Pagani", "title": "The NuZZ: Numerical ZigZag Sampling for General Models", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Numerical ZigZag (NuZZ) algorithm, a Piecewise Deterministic\nMCMC algorithm that is applicable to general statistical models, without the\nneed for bounds on the gradient of the log posterior. This allows us to\ninvestigate: (i) how the ZigZag process behaves on some test problems with\ncommon challenging features; (ii) the performance of NuZZ compared to other\nnumerical approaches to the ZigZag; (iii) the error between the target and\nsampled distributions as a function of computational effort for different MCMC\nalgorithms including the NuZZ. Through a series of test problems we compare the\nmixing of the ZigZag process against other common methods. We present numerical\nevidence and an analytical argument that the Wasserstein distance between the\ntarget distribution and the invariant distribution of the NuZZ process is\nexpected to exhibit asymptotically linearly dependence on the tolerances of\nboth the numerical integration and root finding schemes used. Notably we\npresent a real-life example which demonstrates that NuZZ can outperform not\nonly the super-efficient version of the ZigZag process with thinning, but also\nwell-established methods such as Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 18:46:13 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Cotter", "Simon", ""], ["House", "Thomas", ""], ["Pagani", "Filippo", ""]]}, {"id": "2003.03668", "submitter": "Richard Samworth", "authors": "Yudong Chen, Tengyao Wang and Richard J. Samworth", "title": "High-dimensional, multiscale online changepoint detection", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new method for high-dimensional, online changepoint detection\nin settings where a $p$-variate Gaussian data stream may undergo a change in\nmean. The procedure works by performing likelihood ratio tests against simple\nalternatives of different scales in each coordinate, and then aggregating test\nstatistics across scales and coordinates. The algorithm is online in the sense\nthat both its storage requirements and worst-case computational complexity per\nnew observation are independent of the number of previous observations; in\npractice, it may even be significantly faster than this. We prove that the\npatience, or average run length under the null, of our procedure is at least at\nthe desired nominal level, and provide guarantees on its response delay under\nthe alternative that depend on the sparsity of the vector of mean change.\nSimulations confirm the practical effectiveness of our proposal, which is\nimplemented in the R package 'ocd', and we also demonstrate its utility on a\nseismology data set.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2020 21:54:09 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2020 15:46:08 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Chen", "Yudong", ""], ["Wang", "Tengyao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2003.03830", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Martin C. Rinard, Vikash K.\n  Mansinghka", "title": "The Fast Loaded Dice Roller: A Near-Optimal Exact Sampler for Discrete\n  Probability Distributions", "comments": "12 pages, 5 figures, 1 table. Appearing in AISTATS 2020", "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics, PMLR 108:1036-1046, 2020", "doi": null, "report-no": null, "categories": "stat.CO cs.DM cs.DS cs.IT math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm for the fundamental problem of\ngenerating a random integer from a discrete probability distribution using a\nsource of independent and unbiased random coin flips. We prove that this\nalgorithm, which we call the Fast Loaded Dice Roller (FLDR), is highly\nefficient in both space and time: (i) the size of the sampler is guaranteed to\nbe linear in the number of bits needed to encode the input distribution; and\n(ii) the expected number of bits of entropy it consumes per sample is at most 6\nbits more than the information-theoretically optimal rate. We present fast\nimplementations of the linear-time preprocessing and near-optimal sampling\nalgorithms using unsigned integer arithmetic. Empirical evaluations on a broad\nset of probability distributions establish that FLDR is 2x-10x faster in both\npreprocessing and sampling than multiple baseline algorithms, including the\nwidely-used alias and interval samplers. It also uses up to 10000x less space\nthan the information-theoretically optimal sampler, at the expense of less than\n1.5x runtime overhead.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2020 19:17:08 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 15:37:39 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2003.03893", "submitter": "You-Gan Wang", "authors": "Jinran Wu and You-Gan Wang", "title": "A working likelihood approach to support vector regression with a\n  data-driven insensitivity parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The insensitive parameter in support vector regression determines the set of\nsupport vectors that greatly impacts the prediction. A data-driven approach is\nproposed to determine an approximate value for this insensitive parameter by\nminimizing a generalized loss function originating from the likelihood\nprinciple. This data-driven support vector regression also statistically\nstandardizes samples using the scale of noises. Nonlinear and linear numerical\nsimulations with three types of noises ($\\epsilon$-Laplacian distribution,\nnormal distribution, and uniform distribution), and in addition, five real\nbenchmark data sets, are used to test the capacity of the proposed method.\nBased on all of the simulations and the five case studies, the proposed support\nvector regression using a working likelihood, data-driven insensitive parameter\nis superior and has lower computational costs.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 02:32:32 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wu", "Jinran", ""], ["Wang", "You-Gan", ""]]}, {"id": "2003.03950", "submitter": "Khai Xiang Au", "authors": "Khai Xiang Au, Matthew M. Graham, Alexandre H. Thiery", "title": "Manifold lifting: scaling MCMC to the vanishing noise regime", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Markov chain Monte Carlo methods struggle to explore distributions\nthat are concentrated in the neighbourhood of low-dimensional structures. These\npathologies naturally occur in a number of situations. For example, they are\ncommon to Bayesian inverse problem modelling and Bayesian neural networks, when\nobservational data are highly informative, or when a subset of the statistical\nparameters of interest are non-identifiable. In this paper, we propose a\nstrategy that transforms the original sampling problem into the task of\nexploring a distribution supported on a manifold embedded in a higher\ndimensional space; in contrast to the original posterior this lifted\ndistribution remains diffuse in the vanishing noise limit. We employ a\nconstrained Hamiltonian Monte Carlo method which exploits the manifold geometry\nof this lifted distribution, to perform efficient approximate inference. We\ndemonstrate in several numerical experiments that, contrarily to competing\napproaches, the sampling efficiency of our proposed methodology does not\ndegenerate as the target distribution to be explored concentrates near low\ndimensional structures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 07:15:57 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Au", "Khai Xiang", ""], ["Graham", "Matthew M.", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2003.04067", "submitter": "Ben Youngman", "authors": "Benjamin D. Youngman", "title": "evgam: An R package for Generalized Additive Extreme Value Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces the R package evgam. The package provides functions\nfor fitting extreme value distributions. These include the generalized extreme\nvalue and generalized Pareto distributions. The former can also be fitted\nthrough a point process representation. evgam supports quantile regression via\nthe asymmetric Laplace distribution, which can be useful for estimating high\nthresholds, sometimes used to discriminate between extreme and non-extreme\nvalues. The main addition of evgam is to let extreme value distribution\nparameters have generalized additive model forms, which can be objectively\nestimated using Laplace's method. Illustrative examples fitting various\ndistributions with various specifications are given. These include daily\nprecipitation accumulations for part of Colorado, US, used to illustrate\nspatial models, and daily maximum temperatures for Fort Collins, Colorado, US,\nused to illustrate temporal models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2020 12:31:55 GMT"}, {"version": "v2", "created": "Sun, 26 Apr 2020 17:27:33 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 16:31:26 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Youngman", "Benjamin D.", ""]]}, {"id": "2003.04787", "submitter": "Kun Chen", "authors": "Yan Li, Chun Yu, Yize Zhao, Robert H. Aseltine, Weixin Yao, Kun Chen", "title": "Pursuing Sources of Heterogeneity in Modeling Clustered Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often have to deal with heterogeneous population with mixed\nregression relationships, increasingly so in the era of data explosion. In such\nproblems, when there are many candidate predictors, it is not only of interest\nto identify the predictors that are associated with the outcome, but also to\ndistinguish the true sources of heterogeneity, i.e., to identify the predictors\nthat have different effects among the clusters and thus are the true\ncontributors to the formation of the clusters. We clarify the concepts of the\nsource of heterogeneity that account for potential scale differences of the\nclusters and propose a regularized finite mixture effects regression to achieve\nheterogeneity pursuit and feature selection simultaneously. As the name\nsuggests, the problem is formulated under an effects-model parameterization, in\nwhich the cluster labels are missing and the effect of each predictor on the\noutcome is decomposed to a common effect term and a set of cluster-specific\nterms. A constrained sparse estimation of these effects leads to the\nidentification of both the variables with common effects and those with\nheterogeneous effects. We propose an efficient algorithm and show that our\napproach can achieve both estimation and selection consistency. Simulation\nstudies further demonstrate the effectiveness of our method under various\npractical scenarios. Three applications are presented, namely, an imaging\ngenetics study for linking genetic factors and brain neuroimaging traits in\nAlzheimer's disease, a public health study for exploring the association\nbetween suicide risk among adolescents and their school district\ncharacteristics, and a sport analytics study for understanding how the salary\nlevels of baseball players are associated with their performance and\ncontractual status.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 14:59:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 20:03:13 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Yan", ""], ["Yu", "Chun", ""], ["Zhao", "Yize", ""], ["Aseltine", "Robert H.", ""], ["Yao", "Weixin", ""], ["Chen", "Kun", ""]]}, {"id": "2003.04873", "submitter": "Haoyun Ying", "authors": "Haoyun Ying, Keheng Mao, Klaus Mosegaard", "title": "Moving Target Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS math.OC math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov Chain Monte Carlo (MCMC) methods are popular when considering\nsampling from a high-dimensional random variable $\\mathbf{x}$ with possibly\nunnormalised probability density $p$ and observed data $\\mathbf{d}$. However,\nMCMC requires evaluating the posterior distribution $p(\\mathbf{x}|\\mathbf{d})$\nof the proposed candidate $\\mathbf{x}$ at each iteration when constructing the\nacceptance rate. This is costly when such evaluations are intractable. In this\npaper, we introduce a new non-Markovian sampling algorithm called Moving Target\nMonte Carlo (MTMC). The acceptance rate at $n$-th iteration is constructed\nusing an iteratively updated approximation of the posterior distribution\n$a_n(\\mathbf{x})$ instead of $p(\\mathbf{x}|\\mathbf{d})$. The true value of the\nposterior $p(\\mathbf{x}|\\mathbf{d})$ is only calculated if the candidate\n$\\mathbf{x}$ is accepted. The approximation $a_n$ utilises these evaluations\nand converges to $p$ as $n \\rightarrow \\infty$. A proof of convergence and\nestimation of convergence rate in different situations are given.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 17:38:36 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Ying", "Haoyun", ""], ["Mao", "Keheng", ""], ["Mosegaard", "Klaus", ""]]}, {"id": "2003.04896", "submitter": "Kody Law", "authors": "Ajay Jasra, Kody J. H. Law, Deng Lu", "title": "Unbiased Estimation of the Gradient of the Log-Likelihood in Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a parameter associated to a Bayesian\ninverse problem. Treating the unknown initial condition as a nuisance\nparameter, typically one must resort to a numerical approximation of gradient\nof the log-likelihood and also adopt a discretization of the problem in space\nand/or time. We develop a new methodology to unbiasedly estimate the gradient\nof the log-likelihood with respect to the unknown parameter, i.e. the\nexpectation of the estimate has no discretization bias. Such a property is not\nonly useful for estimation in terms of the original stochastic model of\ninterest, but can be used in stochastic gradient algorithms which benefit from\nunbiased estimates. Under appropriate assumptions, we prove that our estimator\nis not only unbiased but of finite variance. In addition, when implemented on a\nsingle processor, we show that the cost to achieve a given level of error is\ncomparable to multilevel Monte Carlo methods, both practically and\ntheoretically. However, the new algorithm provides the possibility for parallel\ncomputation on arbitrarily many processors without any loss of efficiency,\nasymptotically. In practice, this means any precision can be achieved in a\nfixed, finite constant time, provided that enough processors are available.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 12:07:00 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 12:33:03 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Lu", "Deng", ""]]}, {"id": "2003.04937", "submitter": "N. Benjamin Erichson", "authors": "Miles E. Lopes and N. Benjamin Erichson and Michael W. Mahoney", "title": "Error Estimation for Sketched SVD via the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to compute fast approximations to the singular value decompositions\n(SVD) of very large matrices, randomized sketching algorithms have become a\nleading approach. However, a key practical difficulty of sketching an SVD is\nthat the user does not know how far the sketched singular vectors/values are\nfrom the exact ones. Indeed, the user may be forced to rely on analytical\nworst-case error bounds, which do not account for the unique structure of a\ngiven problem. As a result, the lack of tools for error estimation often leads\nto much more computation than is really necessary. To overcome these\nchallenges, this paper develops a fully data-driven bootstrap method that\nnumerically estimates the actual error of sketched singular vectors/values. In\nparticular, this allows the user to inspect the quality of a rough initial\nsketched SVD, and then adaptively predict how much extra work is needed to\nreach a given error tolerance. Furthermore, the method is computationally\ninexpensive, because it operates only on sketched objects, and it requires no\npasses over the full matrix being factored. Lastly, the method is supported by\ntheoretical guarantees and a very encouraging set of experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2020 19:14:08 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Lopes", "Miles E.", ""], ["Erichson", "N. Benjamin", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2003.05331", "submitter": "Maximilian Pichler", "authors": "Maximilian Pichler, Florian Hartig", "title": "A new method for faster and more accurate inference of species\n  associations from big community data", "comments": "65 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  1. Joint Species Distribution models (JSDMs) explain spatial variation in\ncommunity composition by contributions of the environment, biotic associations,\nand possibly spatially structured residual covariance. They show great promise\nas a general analytical framework for community ecology and macroecology, but\ncurrent JSDMs, even when approximated by latent variables, scale poorly on\nlarge datasets, limiting their usefulness for currently emerging big (e.g.,\nmetabarcoding and metagenomics) community datasets. 2. Here, we present a\nnovel, more scalable JSDM (sjSDM) that circumvents the need to use latent\nvariables by using a Monte-Carlo integration of the joint JSDM likelihood and\nallows flexible elastic net regularization on all model components. We\nimplemented sjSDM in PyTorch, a modern machine learning framework that can make\nuse of CPU and GPU calculations. Using simulated communities with known\nspecies-species associations and different number of species and sites, we\ncompare sjSDM with state-of-the-art JSDM implementations to determine\ncomputational runtimes and accuracy of the inferred species-species and\nspecies-environmental associations. 3. We find that sjSDM is orders of\nmagnitude faster than existing JSDM algorithms (even when run on the CPU) and\ncan be scaled to very large datasets. Despite the dramatically improved speed,\nsjSDM produces more accurate estimates of species association structures than\nalternative JSDM implementations. We demonstrate the applicability of sjSDM to\nbig community data using eDNA case study with thousands of fungi operational\ntaxonomic units (OTU). 4. Our sjSDM approach makes the analysis of JSDMs to\nlarge community datasets with hundreds or thousands of species possible,\nsubstantially extending the applicability of JSDMs in ecology. We provide our\nmethod in an R package to facilitate its applicability for practical data\nanalysis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 14:37:02 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2020 08:11:01 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2020 13:26:47 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 11:46:42 GMT"}, {"version": "v5", "created": "Fri, 2 Jul 2021 09:24:25 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Pichler", "Maximilian", ""], ["Hartig", "Florian", ""]]}, {"id": "2003.05492", "submitter": "Philippe Gagnon", "authors": "Philippe Gagnon and Florian Maire", "title": "An asymptotic Peskun ordering and its application to lifted samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Peskun ordering between two samplers, implying a dominance of one over the\nother, is known among the Markov chain Monte Carlo community for being a\nremarkably strong result, but it is also known for being one that is notably\ndifficult to establish. Indeed, one has to prove that the probability to reach\na state, using a sampler, is greater than or equal to the probability using the\nother sampler, and this must hold for all states excepting the current state.\nWe provide in this paper a weaker version that does not require an inequality\nbetween the probabilities for all these states: the dominance holds\nasymptotically, as a varying parameter grows without bound, as long as the\nstates for which the probabilities are greater than or equal to belong to a\nmass-concentrating set. The weak ordering turns out to be useful to compare\nlifted samplers for partially-ordered discrete state-spaces with their\nMetropolis-Hastings counterparts. An analysis yields a qualitative conclusion:\nthey asymptotically perform better in certain situations (and we are able to\nidentify these situations), but not necessarily in others (and the reasons why\nare made clear). The difference in performance is evaluated quantitatively in\nimportant applications such as graphical model simulation and variable\nselection. The code to reproduce all numerical experiments is available online.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2020 19:15:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:45:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gagnon", "Philippe", ""], ["Maire", "Florian", ""]]}, {"id": "2003.05686", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Assessing the accuracy of individual link with varying block sizes and\n  cut-off values using MaCSim approach", "comments": "24 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of matching together records from different\ndata sources that belong to the same entity. Record linkage is increasingly\nbeing used by many organizations including statistical, health, government etc.\nto link administrative, survey, and other files to create a robust file for\nmore comprehensive analysis. Therefore, it becomes necessary to assess the\nability of a linking method to achieve high accuracy or compare between methods\nwith respect to accuracy. In this paper, we evaluate the accuracy of individual\nlink using varying block sizes and different cut-off values by utilizing a\nMarkov Chain based Monte Carlo simulation approach (MaCSim). MaCSim utilizes\ntwo linked files to create an agreement matrix. The agreement matrix is\nsimulated to generate re-sampled versions of the agreement matrix. A defined\nlinking method is used in each simulation to link the files and the accuracy of\nthe linking method is assessed. The aim of this paper is to facilitate optimal\nchoice of block size and cut-off value to achieve high accuracy in terms of\nminimizing average False Discovery Rate and False Negative Rate. The analyses\nhave been performed using a synthetic dataset provided by the Australian Bureau\nof Statistics (ABS) and indicated promising results.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:03:20 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 04:46:31 GMT"}, {"version": "v3", "created": "Mon, 23 Nov 2020 22:50:28 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.05994", "submitter": "Kenan \\v{S}ehi\\'c", "authors": "Kenan \\v{S}ehi\\'c (1) and Mirza Karamehmedovi\\'c (1) ((1) Department\n  of Applied Mathematics and Computer Science Technical University of Denmark)", "title": "Estimation of Failure Probabilities via Local Subset Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We here consider the subset simulation method which approaches a failure\nevent using a decreasing sequence of nested intermediate failure events. The\nmethod resembles importance sampling, which actively explores a probability\nspace by conditioning the next evaluation on the previous evaluations using a\nMarkov chain Monte Carlo (MCMC) algorithm. A Markov chain typically requires\nmany steps to estimate the target distribution, which is impractical with\nexpensive numerical models. Therefore, we propose to approximate each step of a\nMarkov chain locally with Gaussian process (GP) regression. Benchmark examples\nof reliability analysis show that local approximations significantly improve\noverall efficiency of subset simulation. They reduce the number of expensive\nlimit-state evaluations by over $80\\%$. However, GP regression becomes\ncomputationally impractical with increasing dimension. Therefore, to make our\nuse of a GP feasible, we employ the partial least squares (PLS) regression, a\ngradient-free reduction method, locally to explore and utilize a\nlow-dimensional subspace within a Markov chain. Numerical experiments\nillustrate a significant computational gain with maintained sufficient\naccuracy.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 19:46:44 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["\u0160ehi\u0107", "Kenan", ""], ["Karamehmedovi\u0107", "Mirza", ""]]}, {"id": "2003.06291", "submitter": "Shovanur Haque", "authors": "Shovanur Haque, Kerrie Mengersen", "title": "Improved assessment of the accuracy of record linkage via an extended\n  MaCSim approach", "comments": "32 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1901.04779", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DB stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Record linkage is the process of bringing together the same entity from\noverlapping data sources while removing duplicates. Huge amounts of data are\nnow being collected by public or private organizations as well as by\nresearchers and individuals. Linking and analysing relevant information from\nthis massive data reservoir can provide new insights into society. However,\nthis increase in the amount of data may also increase the likelihood of\nincorrectly linked records among databases. It has become increasingly\nimportant to have effective and efficient methods for linking data from\ndifferent sources. Therefore, it becomes necessary to assess the ability of a\nlinking method to achieve high accuracy or to compare between methods with\nrespect to accuracy. In this paper, we improve on a Markov Chain based Monte\nCarlo simulation approach (MaCSim) for assessing a linking method. MaCSim\nutilizes two linked files that have been previously linked on similar types of\ndata to create an agreement matrix and then simulates the matrix using a\nproposed algorithm developed to generate re-sampled versions of the agreement\nmatrix. A defined linking method is used in each simulation to link the files\nand the accuracy of the linking method is assessed. The improvement proposed\nhere involves calculation of a similarity weight for every linking variable\nvalue for each record pair, which allows partial agreement of the linking\nvariable values. A threshold is calculated for every linking variable based on\nadjustable parameter \"tolerance\" for that variable. To assess the accuracy of\nlinking method, correctly linked proportions are investigated for each record.\nThe extended MaCSim approach is illustrated using a synthetic dataset provided\nby the Australian Bureau of Statistics (ABS) based on realistic data settings.\nTest results show higher accuracy of the assessment of linkages.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2020 10:41:21 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 05:23:21 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 01:16:17 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Haque", "Shovanur", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2003.06675", "submitter": "Xiaohuan Xue", "authors": "Xiaohuan Xue", "title": "Improved Approximations of Hedges' g*", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hedges' unbiased estimator g* has been broadly used in statistics. We propose\na sequence of polynomials to better approximate the multiplicative correction\nfactor of g* by incorporating analytic estimations to the ratio of gamma\nfunctions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2020 17:45:28 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Xue", "Xiaohuan", ""]]}, {"id": "2003.07132", "submitter": "Aijun Zhang", "authors": "Zebin Yang, Aijun Zhang, Agus Sudjianto", "title": "GAMI-Net: An Explainable Neural Network based on Generalized Additive\n  Models with Structured Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of interpretability is an inevitable problem when using neural\nnetwork models in real applications. In this paper, an explainable neural\nnetwork based on generalized additive models with structured interactions\n(GAMI-Net) is proposed to pursue a good balance between prediction accuracy and\nmodel interpretability. GAMI-Net is a disentangled feedforward network with\nmultiple additive subnetworks; each subnetwork consists of multiple hidden\nlayers and is designed for capturing one main effect or one pairwise\ninteraction. Three interpretability aspects are further considered, including\na) sparsity, to select the most significant effects for parsimonious\nrepresentations; b) heredity, a pairwise interaction could only be included\nwhen at least one of its parent main effects exists; and c) marginal clarity,\nto make main effects and pairwise interactions mutually distinguishable. An\nadaptive training algorithm is developed, where main effects are first trained\nand then pairwise interactions are fitted to the residuals. Numerical\nexperiments on both synthetic functions and real-world datasets show that the\nproposed model enjoys superior interpretability and it maintains competitive\nprediction accuracy in comparison to the explainable boosting machine and other\nclassic machine learning models.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 11:51:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 15:02:15 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yang", "Zebin", ""], ["Zhang", "Aijun", ""], ["Sudjianto", "Agus", ""]]}, {"id": "2003.07364", "submitter": "Alireza Vafaei Sadr", "authors": "A. Vafaei Sadr, S. M. S. Movahed", "title": "Clustering of Local Extrema in Planck CMB maps", "comments": "17 pages, 7 figures, and 3 tables. Including major revision and\n  matched to the accepted version that appeared in MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab368", "report-no": null, "categories": "astro-ph.CO astro-ph.HE astro-ph.IM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering of local extrema will be exploited to examine Gaussianity,\nasymmetry, and the footprint of the cosmic-string network on the CMB observed\nby Planck. The number density of local extrema ($n_{\\rm pk}$ for peak and\n$n_{\\rm tr}$ for trough) and sharp clipping ($n_{\\rm pix}$) statistics support\nthe Gaussianity hypothesis for all component separations. However, the pixel at\nthe threshold reveals a more consistent treatment with respect to end-to-end\nsimulations. A very tiny deviation from associated simulations in the context\nof trough density, in the threshold range $\\theta\\in [-2-0]$ for NILC and CR\ncomponent separations, are detected. The unweighted two-point correlation\nfunction, of the local extrema, illustrates good consistency between different\ncomponent separations and corresponding Gaussian simulations for almost all\navailable thresholds. However, for high thresholds, a small deficit in the\nclustering of peaks is observed with respect to the Planck fiducial\n$\\Lambda$CDM model. To put a significant constraint on the amplitude of the\nmass function based on the value of $\\Psi$ around the Doppler peak\n($\\theta\\approx 70-75$ arcmin), we should consider $\\vartheta\\lesssim 0.0$. The\nscale-independent bias factors for the peak above a threshold for large\nseparation angle and high threshold level are in agreement with the value\nexpected for a pure Gaussian CMB. Applying the $n_{\\rm pk}$, $n_{\\rm tr}$,\n$\\Psi_{\\rm pk-pk}$ and $\\Psi_{\\rm tr-tr}$ measures on the tessellated CMB map\nwith patches of $7.5^2$ deg$^2$ size prove statistical isotropy in the Planck\nmaps. The peak clustering analysis puts the upper bound on the cosmic-string\ntension, $G\\mu^{(\\rm up)} \\lesssim 5.59\\times 10^{-7}$, in SMICA.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 10:37:40 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 09:18:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sadr", "A. Vafaei", ""], ["Movahed", "S. M. S.", ""]]}, {"id": "2003.07398", "submitter": "Jiacong Du", "authors": "Jiacong Du, Jonathan Boss, Peisong Han, Lauren J Beesley, Stephen A\n  Goutman, Stuart Batterman, Eva L Feldman, Bhramar Mukherjee", "title": "Variable selection with multiply-imputed datasets: choosing between\n  stacked and grouped methods", "comments": "23 pages, 6 figures. This paper has been submitted to Statistics in\n  Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods, such as lasso and elastic net, are used in many\nbiomedical applications when simultaneous regression coefficient estimation and\nvariable selection is desired. However, missing data complicates the\nimplementation of these methods, particularly when missingness is handled using\nmultiple imputation. Applying a variable selection algorithm on each imputed\ndataset will likely lead to different sets of selected predictors, making it\ndifficult to ascertain a final active set without resorting to ad hoc\ncombination rules. In this paper we consider a general class of penalized\nobjective functions which, by construction, force selection of the same\nvariables across multiply-imputed datasets. By pooling objective functions\nacross imputations, optimization is then performed jointly over all imputed\ndatasets rather than separately for each dataset. We consider two objective\nfunction formulations that exist in the literature, which we will refer to as\n\"stacked\" and \"grouped\" objective functions. Building on existing work, we (a)\nderive and implement efficient cyclic coordinate descent and\nmajorization-minimization optimization algorithms for both continuous and\nbinary outcome data, (b) incorporate adaptive shrinkage penalties, (c) compare\nthese methods through simulation, and (d) develop an R package miselect for\neasy implementation. Simulations demonstrate that the \"stacked\" objective\nfunction approaches tend to be more computationally efficient and have better\nestimation and selection properties. We apply these methods to data from the\nUniversity of Michigan ALS Patients Repository (UMAPR) which aims to identify\nthe association between persistent organic pollutants and ALS risk.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2020 18:39:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Du", "Jiacong", ""], ["Boss", "Jonathan", ""], ["Han", "Peisong", ""], ["Beesley", "Lauren J", ""], ["Goutman", "Stephen A", ""], ["Batterman", "Stuart", ""], ["Feldman", "Eva L", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2003.07898", "submitter": "Kun Chen", "authors": "Kun Chen, Ruipeng Dong, Wanwan Xu, Zemin Zheng", "title": "Statistically Guided Divide-and-Conquer for Sparse Factorization of\n  Large Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse factorization of a large matrix is fundamental in modern\nstatistical learning. In particular, the sparse singular value decomposition\nand its variants have been utilized in multivariate regression, factor\nanalysis, biclustering, vector time series modeling, among others. The appeal\nof this factorization is owing to its power in discovering a\nhighly-interpretable latent association network, either between samples and\nvariables or between responses and predictors. However, many existing methods\nare either ad hoc without a general performance guarantee, or are\ncomputationally intensive, rendering them unsuitable for large-scale studies.\nWe formulate the statistical problem as a sparse factor regression and tackle\nit with a divide-and-conquer approach. In the first stage of division, we\nconsider both sequential and parallel approaches for simplifying the task into\na set of co-sparse unit-rank estimation (CURE) problems, and establish the\nstatistical underpinnings of these commonly-adopted and yet poorly understood\ndeflation methods. In the second stage of division, we innovate a contended\nstagewise learning technique, consisting of a sequence of simple incremental\nupdates, to efficiently trace out the whole solution paths of CURE. Our\nalgorithm has a much lower computational complexity than alternating convex\nsearch, and the choice of the step size enables a flexible and principled\ntradeoff between statistical accuracy and computational efficiency. Our work is\namong the first to enable stagewise learning for non-convex problems, and the\nidea can be applicable in many multi-convex problems. Extensive simulation\nstudies and an application in genetics demonstrate the effectiveness and\nscalability of our approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 19:12:21 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Chen", "Kun", ""], ["Dong", "Ruipeng", ""], ["Xu", "Wanwan", ""], ["Zheng", "Zemin", ""]]}, {"id": "2003.07953", "submitter": "Shounak Chattopadhyay", "authors": "Shounak Chattopadhyay, Antik Chakraborty, David B. Dunson", "title": "Nearest Neighbor Dirichlet Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There is a rich literature on Bayesian nonparametric methods for unknown\ndensities. The most popular approach relies on Dirichlet process mixture\nmodels. These models characterize the unknown density as a kernel convolution\nwith an unknown almost surely discrete mixing measure, which is given a\nDirichlet process prior. Such models are very flexible and have good\nperformance in many settings, but posterior computation typically relies on\nMarkov chain Monte Carlo algorithms that can be complex and inefficient. As a\nsimple alternative, we propose a class of nearest neighbor-Dirichlet processes.\nThe approach starts by grouping the data into neighborhoods based on standard\nalgorithms. Within each neighborhood, the density is characterized via a\nBayesian parametric model, such as a Gaussian with unknown parameters.\nAssigning a Dirichlet prior to the weights on these local kernels, we obtain a\nsimple pseudo-posterior for the weights and kernel parameters. A simple and\nembarrassingly parallel Monte Carlo algorithm is proposed to sample from the\nresulting pseudo-posterior for the unknown density. Desirable asymptotic\nproperties are shown, and the methods are evaluated in simulation studies and\napplied to a motivating data set in the context of classification.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 21:39:11 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 00:21:27 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Chattopadhyay", "Shounak", ""], ["Chakraborty", "Antik", ""], ["Dunson", "David B.", ""]]}, {"id": "2003.07980", "submitter": "Cecilia Mondaini", "authors": "Nathan E. Glatt-Holtz and Cecilia F. Mondaini", "title": "Mixing Rates for Hamiltonian Monte Carlo Algorithms in Finite and\n  Infinite Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish the geometric ergodicity of the preconditioned Hamiltonian Monte\nCarlo (HMC) algorithm defined on an infinite-dimensional Hilbert space, as\ndeveloped in [Beskos et al., Stochastic Process. Appl., 2011]. This algorithm\ncan be used as a basis to sample from certain classes of target measures which\nare absolutely continuous with respect to a Gaussian measure. Our work\naddresses an open question posed in [Beskos et al., Stochastic Process. Appl.,\n2011], and provides an alternative to a recent proof based on exact coupling\ntechniques given in arXiv:1909.07962. The approach here establishes convergence\nin a suitable Wasserstein distance by using the weak Harris theorem together\nwith a generalized coupling argument. We also show that a law of large numbers\nand central limit theorem can be derived as a consequence of our main\nconvergence result. Moreover, our approach yields a novel proof of mixing rates\nfor the classical finite-dimensional HMC algorithm. As such, the methodology we\ndevelop provides a flexible framework to tackle the rigorous convergence of\nother Markov Chain Monte Carlo algorithms. Additionally, we show that the scope\nof our result includes certain measures that arise in the Bayesian approach to\ninverse PDE problems, cf. [Stuart, Acta Numer., 2010]. Particularly, we verify\nall of the required assumptions for a certain class of inverse problems\ninvolving the recovery of a divergence free vector field from a passive scalar,\narXiv:1808.01084v3.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2020 23:09:47 GMT"}], "update_date": "2020-03-19", "authors_parsed": [["Glatt-Holtz", "Nathan E.", ""], ["Mondaini", "Cecilia F.", ""]]}, {"id": "2003.07991", "submitter": "Yuming Chen", "authors": "Daniele Bigoni, Yuming Chen, Nicolas Garcia Trillos, Youssef Marzouk,\n  Daniel Sanz-Alonso", "title": "Data-Driven Forward Discretizations for Bayesian Inversion", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/abb2fa", "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a framework for the learning of discretizations of\nexpensive forward models in Bayesian inverse problems. The main idea is to\nincorporate the parameters governing the discretization as part of the unknown\nto be estimated within the Bayesian machinery. We numerically show that in a\nvariety of inverse problems arising in mechanical engineering, signal\nprocessing and the geosciences, the observations contain useful information to\nguide the choice of discretization.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 00:08:12 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 20:44:33 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Bigoni", "Daniele", ""], ["Chen", "Yuming", ""], ["Trillos", "Nicolas Garcia", ""], ["Marzouk", "Youssef", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "2003.08009", "submitter": "Marius Hofert", "authors": "Marius Hofert", "title": "Random number generators produce collisions: Why, how many and more", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It seems surprising that when applying widely used random number generators\nto generate one million random numbers on modern architectures, one obtains, on\naverage, about 116 collisions. This article explains why, how to mathematically\ncompute such a number, why they often cannot be obtained in a straightforward\nway, how to numerically compute them in a robust way and, among other things,\nwhat would need to be changed to bring this number below 1. The probability of\nat least one collision is also briefly addressed, which, as it turns out, again\nneeds a careful numerical treatment. Overall, the article provides an\nintroduction to the representation of floating-point numbers on a computer and\ncorresponding implications in statistics and simulation. All computations are\ncarried out in R and are reproducible with the code included in this article.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2020 01:40:56 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 18:39:06 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Hofert", "Marius", ""]]}, {"id": "2003.09379", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse, Christopher Drovandi, Michael U. Gutmann", "title": "Sequential Bayesian Experimental Design for Implicit Models via Mutual\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian experimental design (BED) is a framework that uses statistical\nmodels and decision making under uncertainty to optimise the cost and\nperformance of a scientific experiment. Sequential BED, as opposed to static\nBED, considers the scenario where we can sequentially update our beliefs about\nthe model parameters through data gathered in the experiment. A class of models\nof particular interest for the natural and medical sciences are implicit\nmodels, where the data generating distribution is intractable, but sampling\nfrom it is possible. Even though there has been a lot of work on static BED for\nimplicit models in the past few years, the notoriously difficult problem of\nsequential BED for implicit models has barely been touched upon. We address\nthis gap in the literature by devising a novel sequential design framework for\nparameter estimation that uses the Mutual Information (MI) between model\nparameters and simulated data as a utility function to find optimal\nexperimental designs, which has not been done before for implicit models. Our\napproach uses likelihood-free inference by ratio estimation to simultaneously\nestimate posterior distributions and the MI. During the sequential BED\nprocedure we utilise Bayesian optimisation to help us optimise the MI utility.\nWe find that our framework is efficient for the various implicit models tested,\nyielding accurate parameter estimates after only a few iterations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 16:52:10 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Drovandi", "Christopher", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2003.09436", "submitter": "Anh Tran", "authors": "Anh Tran, Mike Eldred, Tim Wildey, Scott McCann, Jing Sun, Robert J.\n  Visintainer", "title": "aphBO-2GP-3B: A budgeted asynchronous parallel multi-acquisition\n  functions for constrained Bayesian optimization on high-performing computing\n  architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-fidelity complex engineering simulations are highly predictive, but also\ncomputationally expensive and often require substantial computational efforts.\nThe mitigation of computational burden is usually enabled through parallelism\nin high-performance cluster (HPC) architecture. In this paper, an asynchronous\nconstrained batch-parallel Bayesian optimization method is proposed to\nefficiently solve the computationally-expensive simulation-based optimization\nproblems on the HPC platform, with a budgeted computational resource, where the\nmaximum number of simulations is a constant. The advantages of this method are\nthree-fold. First, the efficiency of the Bayesian optimization is improved,\nwhere multiple input locations are evaluated massively parallel in an\nasynchronous manner to accelerate the optimization convergence with respect to\nphysical runtime. This efficiency feature is further improved so that when each\nof the inputs is finished, another input is queried without waiting for the\nwhole batch to complete. Second, the method can handle both known and unknown\nconstraints. Third, the proposed method considers several acquisition functions\nat the same time and sample based on an evolving probability mass distribution\nfunction using a modified GP-Hedge scheme, where parameters are corresponding\nto the performance of each acquisition function. The proposed framework is\ntermed aphBO-2GP-3B, which corresponds to asynchronous parallel hedge Bayesian\noptimization with two Gaussian processes and three batches. The aphBO-2GP-3B\nframework is demonstrated using two high-fidelity expensive industrial\napplications, where the first one is based on finite element analysis (FEA) and\nthe second one is based on computational fluid dynamics (CFD) simulations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2020 18:02:27 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 22:08:53 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Tran", "Anh", ""], ["Eldred", "Mike", ""], ["Wildey", "Tim", ""], ["McCann", "Scott", ""], ["Sun", "Jing", ""], ["Visintainer", "Robert J.", ""]]}, {"id": "2003.10323", "submitter": "Adrien Mazoyer", "authors": "Jean-Fran\\c{c}ois Coeurjolly, Adrien Mazoyer and Pierre-Olivier\n  Amblard", "title": "Monte Carlo integration of non-differentiable functions on\n  $[0,1]^\\iota$, $\\iota=1,\\dots,d$, using a single determinantal point pattern\n  defined on $[0,1]^d$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.CA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the use of a particular class of determinantal point\nprocesses (DPP), a class of repulsive spatial point processes, for Monte Carlo\nintegration. Let $d\\ge 1$, $I\\subseteq \\overline d=\\{1,\\dots,d\\}$ with\n$\\iota=|I|$. Using a single set of $N$ quadrature points $\\{u_1,\\dots,u_N\\}$\ndefined, once for all, in dimension $d$ from the realization of the DPP model,\nwe investigate \"minimal\" assumptions on the integrand in order to obtain\nunbiased Monte Carlo estimates of $\\mu(f_I)=\\int_{[0,1]^\\iota} f_I(u)\n\\mathrm{d} u$ for any known $\\iota$-dimensional integrable function on\n$[0,1]^\\iota$. In particular, we show that the resulting estimator has variance\nwith order $N^{-1-(2s\\wedge 1)/d}$ when the integrand belongs to some Sobolev\nspace with regularity $s > 0$. When $s>1/2$ (which includes a large class of\nnon-differentiable functions), the variance is asymptotically explicit and the\nestimator is shown to satisfy a Central Limit Theorem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 15:07:55 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Mazoyer", "Adrien", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "2003.10374", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Fredrik Lindsten and David Blei", "title": "Markovian Score Climbing: Variational Inference with KL(p||q)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern variational inference (VI) uses stochastic gradients to avoid\nintractable expectations, enabling large-scale probabilistic inference in\ncomplex models. VI posits a family of approximating distributions q and then\nfinds the member of that family that is closest to the exact posterior p.\nTraditionally, VI algorithms minimize the \"exclusive Kullback-Leibler (KL)\"\nKL(q || p), often for computational convenience. Recent research, however, has\nalso focused on the \"inclusive KL\" KL(p || q), which has good statistical\nproperties that makes it more appropriate for certain inference problems. This\npaper develops a simple algorithm for reliably minimizing the inclusive KL\nusing stochastic gradients with vanishing bias. This method, which we call\nMarkovian score climbing (MSC), converges to a local optimum of the inclusive\nKL. It does not suffer from the systematic errors inherent in existing methods,\nsuch as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which\nlead to bias in their final estimates. We illustrate convergence on a toy model\nand demonstrate the utility of MSC on Bayesian probit regression for\nclassification as well as a stochastic volatility model for financial data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 16:38:10 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 19:46:38 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Lindsten", "Fredrik", ""], ["Blei", "David", ""]]}, {"id": "2003.10548", "submitter": "Renato Panaro Sr.", "authors": "Renato Valladares Panaro", "title": "spsurv: An R package for semi-parametric survival analysis", "comments": "140 pages, 25 figures, 21 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software development innovations and advances in computing have enabled more\ncomplex and less costly computations in medical research (survival analysis),\nengineering studies (reliability analysis), and social sciences event analysis\n(historical analysis). As a result, many semi-parametric modeling efforts\nemerged when it comes to time-to-event data analysis. In this context, this\nwork presents a flexible Bernstein polynomial (BP) based framework for survival\ndata modeling. This innovative approach is applied to existing families of\nmodels such as proportional hazards (PH), proportional odds (PO), and\naccelerated failure time (AFT) models to estimate unknown baseline functions.\nAlong with this contribution, this work also presents new automated routines in\nR, taking advantage of algorithms available in Stan. The proposed computation\nroutines are tested and explored through simulation studies based on artificial\ndatasets. The tools implemented to fit the proposed statistical models are\ncombined and organized in an R package. Also, the BP based proportional hazards\n(BPPH), proportional odds (BPPO), and accelerated failure time (BPAFT) models\nare illustrated in real applications related to cancer trial data using maximum\nlikelihood (ML) estimation and Markov chain Monte Carlo (MCMC) methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2020 21:04:43 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Panaro", "Renato Valladares", ""]]}, {"id": "2003.10609", "submitter": "Xinlian Zhang", "authors": "Cheng Meng, Xinlian Zhang, Jingyi Zhang, Wenxuan Zhong, Ping Ma", "title": "More efficient approximation of smoothing splines via space-filling\n  basis selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating smoothing spline estimators in a\nnonparametric regression model. When applied to a sample of size $n$, the\nsmoothing spline estimator can be expressed as a linear combination of $n$\nbasis functions, requiring $O(n^3)$ computational time when the number of\npredictors $d\\geq 2$. Such a sizable computational cost hinders the broad\napplicability of smoothing splines. In practice, the full sample smoothing\nspline estimator can be approximated by an estimator based on $q$\nrandomly-selected basis functions, resulting in a computational cost of\n$O(nq^2)$. It is known that these two estimators converge at the identical rate\nwhen $q$ is of the order $O\\{n^{2/(pr+1)}\\}$, where $p\\in [1,2]$ depends on the\ntrue function $\\eta$, and $r > 1$ depends on the type of spline. Such $q$ is\ncalled the essential number of basis functions. In this article, we develop a\nmore efficient basis selection method. By selecting the ones corresponding to\nroughly equal-spaced observations, the proposed method chooses a set of basis\nfunctions with a large diversity. The asymptotic analysis shows our proposed\nsmoothing spline estimator can decrease $q$ to roughly $O\\{n^{1/(pr+1)}\\}$,\nwhen $d\\leq pr+1$. Applications on synthetic and real-world datasets show the\nproposed method leads to a smaller prediction error compared with other basis\nselection methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 01:46:24 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Meng", "Cheng", ""], ["Zhang", "Xinlian", ""], ["Zhang", "Jingyi", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2003.10702", "submitter": "Michael Sachs", "authors": "Michael C Sachs, Erin E Gabriel, Arvid Sj\\\"olander", "title": "Symbolic Computation of Tight Causal Bounds", "comments": "Submitted to JRSS-B in March 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference involves making a set of assumptions about the nature of\nthings, defining a causal query, and attempting to find estimators of the query\nbased on the distribution of observed variables. When causal queries are not\nidentifiable from the observed data, it still may be possible to derive bounds\nfor these quantities in terms of the distribution of observed variables. We\ndevelop and describe a general approach for computation of bounds, proving that\nif the problem can be stated as a linear program, then the true global extrema\nresult in tight bounds. Building upon previous work in this area, we\ncharacterize a class of problems that can always be stated as a linear\nprogramming problem; we describe a general algorithm for constructing the\nlinear objective and constraints based on the causal model and the causal query\nof interest. These problems therefore can be solved using a vertex enumeration\nalgorithm. We develop an R package implementing this algorithm with a user\nfriendly graphical interface using directed acyclic graphs, which only allows\nfor problems within this class to be depicted. We have implemented additional\nfeatures to help with interpreting and applying the bounds that we illustrate\nin examples.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 07:50:40 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Sachs", "Michael C", ""], ["Gabriel", "Erin E", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2003.10726", "submitter": "Yue Su", "authors": "Yue Su and Patrick Kandege Mwanakatwe", "title": "Model selection criteria of the standard censored regression model based\n  on the bootstrap sample augmentation mechanism", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical regression technique is an extraordinarily essential data\nfitting tool to explore the potential possible generation mechanism of the\nrandom phenomenon. Therefore, the model selection or the variable selection is\nbecoming extremely important so as to identify the most appropriate model with\nthe most optimal explanation effect on the interesting response. In this paper,\nwe discuss and compare the bootstrap-based model selection criteria on the\nstandard censored regression model (Tobit regression model) under the\ncircumstance of limited observation information. The Monte Carlo numerical\nevidence demonstrates that the performances of the model selection criteria\nbased on the bootstrap sample augmentation strategy will become more\ncompetitive than their alternative ones, such as the Akaike Information\nCriterion (AIC) and the Bayesian Information Criterion (BIC) etc. under the\ncircumstance of the inadequate observation information. Meanwhile, the\nnumerical simulation experiments further demonstrate that the model\nidentification risk due to the deficiency of the data information, such as the\nhigh censoring rate and rather limited number of observations, can be\nadequately compensated by increasing the scientific computation cost in terms\nof the bootstrap sample augmentation strategies. We also apply the recommended\nbootstrap-based model selection criterion on the Tobit regression model to fit\nthe real fidelity dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2020 09:19:27 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Su", "Yue", ""], ["Mwanakatwe", "Patrick Kandege", ""]]}, {"id": "2003.11183", "submitter": "Jian Cao Dr.", "authors": "Jian Cao, Marc G. Genton, David E. Keyes, George M. Turkiyyah", "title": "Exploiting Low Rank Covariance Structures for Computing High-Dimensional\n  Normal and Student-$t$ Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a preconditioned Monte Carlo method for computing high-dimensional\nmultivariate normal and Student-$t$ probabilities arising in spatial\nstatistics. The approach combines a tile-low-rank representation of covariance\nmatrices with a block-reordering scheme for efficient Quasi-Monte Carlo\nsimulation. The tile-low-rank representation decomposes the high-dimensional\nproblem into many diagonal-block-size problems and low-rank connections. The\nblock-reordering scheme reorders between and within the diagonal blocks to\nreduce the impact of integration variables from right to left, thus improving\nthe Monte Carlo convergence rate. Simulations up to dimension $65{,}536$\nsuggest that the new method can improve the run time by an order of magnitude\ncompared with the non-reordered tile-low-rank Quasi-Monte Carlo method and two\norders of magnitude compared with the dense Quasi-Monte Carlo method. Our\nmethod also forms a strong substitute for the approximate conditioning methods\nas a more robust estimation with error guarantees. An application study to wind\nstochastic generators is provided to illustrate that the new computational\nmethod makes the maximum likelihood estimation feasible for high-dimensional\nskew-normal random fields.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 02:16:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 11:32:36 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Cao", "Jian", ""], ["Genton", "Marc G.", ""], ["Keyes", "David E.", ""], ["Turkiyyah", "George M.", ""]]}, {"id": "2003.11208", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, Sudipto Banerjee, Andrew O. Finley", "title": "Highly Scalable Bayesian Geostatistical Modeling via Meshed Gaussian\n  Processes on Partitioned Domains", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1833889", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of scalable Bayesian hierarchical models for the\nanalysis of massive geostatistical datasets. The underlying idea combines ideas\non high-dimensional geostatistics by partitioning the spatial domain and\nmodeling the regions in the partition using a sparsity-inducing directed\nacyclic graph (DAG). We extend the model over the DAG to a well-defined spatial\nprocess, which we call the Meshed Gaussian Process (MGP). A major contribution\nis the development of a MGPs on tessellated domains, accompanied by a Gibbs\nsampler for the efficient recovery of spatial random effects. In particular,\nthe cubic MGP (Q-MGP) can harness high-performance computing resources by\nexecuting all large-scale operations in parallel within the Gibbs sampler,\nimproving mixing and computing time compared to sequential updating schemes.\nUnlike some existing models for large spatial data, a Q-MGP facilitates massive\ncaching of expensive matrix operations, making it particularly apt in dealing\nwith spatiotemporal remote-sensing data. We compare Q-MGPs with large synthetic\nand real world data against state-of-the-art methods. We also illustrate using\nNormalized Difference Vegetation Index (NDVI) data from the Serengeti park\nregion to recover latent multivariate spatiotemporal random effects at millions\nof locations. The source code is available at https://github.com/mkln/meshgp.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2020 04:15:23 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:51:18 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Peruzzi", "Michele", ""], ["Banerjee", "Sudipto", ""], ["Finley", "Andrew O.", ""]]}, {"id": "2003.12043", "submitter": "Markus Loecher", "authors": "Markus Loecher", "title": "From unbiased MDI Feature Importance to Explainable AI for Trees", "comments": "arXiv admin note: text overlap with arXiv:2003.02106", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We attempt to give a unifying view of the various recent attempts to (i)\nimprove the interpretability of tree-based models and (ii) debias the the\ndefault variable-importance measure in random Forests, Gini importance. In\nparticular, we demonstrate a common thread among the out-of-bag based bias\ncorrection methods and their connection to local explanation for trees. In\naddition, we point out a bias caused by the inclusion of inbag data in the\nnewly developed explainable AI for trees algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2020 17:16:58 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 17:45:50 GMT"}, {"version": "v3", "created": "Sun, 24 May 2020 15:44:32 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Loecher", "Markus", ""]]}, {"id": "2003.12247", "submitter": "Shouto Yonekura", "authors": "Shouto Yonekura and Alexandros Beskos", "title": "Online Smoothing for Diffusion Processes Observed with Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for online estimation of smoothing expectations\nfor a class of additive functionals, in the context of a rich family of\ndiffusion processes (that may include jumps) -- observed at discrete-time\ninstances. We overcome the unavailability of the transition density of the\nunderlying SDE by working on the augmented pathspace. The new method can be\napplied, for instance, to carry out online parameter inference for the\ndesignated class of models. Algorithms defined on the infinite-dimensional\npathspace have been developed in the last years mainly in the context of MCMC\ntechniques. There, the main benefit is the achievement of mesh-free mixing\ntimes for the practical time-discretised algorithm used on a PC. Our own\nmethodology sets up the framework for infinite-dimensional online filtering --\nan important positive practical consequence is the construct of estimates with\nthe variance that does not increase with decreasing mesh-size. Besides\nregularity conditions, our method is, in principle, applicable under the weak\nassumption -- relatively to restrictive conditions often required in the MCMC\nor filtering literature of methods defined on pathspace -- that the SDE\ncovariance matrix is invertible.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 06:07:51 GMT"}, {"version": "v2", "created": "Sat, 4 Apr 2020 01:56:50 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 13:00:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Yonekura", "Shouto", ""], ["Beskos", "Alexandros", ""]]}, {"id": "2003.12540", "submitter": "Ning Hao", "authors": "Ning Hao, Yue Selena Niu, Feifei Xiao, and Heping Zhang", "title": "A super scalable algorithm for short segment detection", "comments": "To be published in Statistics in Biosciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications such as copy number variant (CNV) detection, the goal is\nto identify short segments on which the observations have different means or\nmedians from the background. Those segments are usually short and hidden in a\nlong sequence, and hence are very challenging to find. We study a super\nscalable short segment (4S) detection algorithm in this paper. This\nnonparametric method clusters the locations where the observations exceed a\nthreshold for segment detection. It is computationally efficient and does not\nrely on Gaussian noise assumption. Moreover, we develop a framework to assign\nsignificance levels for detected segments. We demonstrate the advantages of our\nproposed method by theoretical, simulation, and real data studies.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2020 17:08:22 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Hao", "Ning", ""], ["Niu", "Yue Selena", ""], ["Xiao", "Feifei", ""], ["Zhang", "Heping", ""]]}, {"id": "2003.12890", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar and Tapabrata Maiti", "title": "Variational Inference with Vine Copulas: An efficient Approach for\n  Bayesian Computer Model Calibration", "comments": "Submitted to the Statistics and Computing Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancements of computer architectures, the use of computational\nmodels proliferates to solve complex problems in many scientific applications\nsuch as nuclear physics and climate research. However, the potential of such\nmodels is often hindered because they tend to be computationally expensive and\nconsequently ill-fitting for uncertainty quantification. Furthermore, they are\nusually not calibrated with real-time observations. We develop a\ncomputationally efficient algorithm based on variational Bayes inference (VBI)\nfor calibration of computer models with Gaussian processes. Unfortunately, the\nspeed and scalability of VBI diminishes when applied to the calibration\nframework with dependent data. To preserve the efficiency of VBI, we adopt a\npairwise decomposition of the data likelihood using vine copulas that separate\nthe information on dependence structure in data from their marginal\ndistributions. We provide both theoretical and empirical evidence for the\ncomputational scalability of our methodology and describe all the necessary\ndetails for an efficient implementation of the proposed algorithm. We also\ndemonstrate the opportunities given by our method for practitioners on a real\ndata example through calibration of the Liquid Drop Model of nuclear binding\nenergies.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2020 21:05:16 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 02:49:21 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2003.13111", "submitter": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez", "authors": "Maria Xose Rodriguez-Alvarez and Vanda Inacio", "title": "ROCnReg: An R Package for Receiver Operating Characteristic Curve\n  Inference with and without Covariate Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The receiver operating characteristic (ROC) curve is the most popular tool\nused to evaluate the discriminatory capability of diagnostic tests/biomarkers\nmeasured on a continuous scale when distinguishing between two alternative\ndisease states (e.g, diseased and nondiseased). In some circumstances, the\ntest's performance and its discriminatory ability may vary according to\nsubject-specific characteristics or different test settings. In such cases,\ninformation-specific accuracy measures, such as the covariate-specific and the\ncovariate-adjusted ROC curve are needed, as ignoring covariate information may\nlead to biased or erroneous results. This paper introduces the R package\nROCnReg that allows estimating the pooled (unadjusted) ROC curve, the\ncovariate-specific ROC curve, and the covariate-adjusted ROC curve by different\nmethods, both from (semi) parametric and nonparametric perspectives and within\nBayesian and frequentist paradigms. From the estimated ROC curve (pooled,\ncovariate-specific or covariate-adjusted), several summary measures of\naccuracy, such as the (partial) area under the ROC curve and the Youden index,\ncan be obtained. The package also provides functions to obtain ROC-based\noptimal threshold values using several criteria, namely, the Youden Index\ncriterion and the criterion that sets a target value for the false positive\nfraction. For the Bayesian methods, we provide tools for assessing model fit\nvia posterior predictive checks, while model choice can be carried out via\nseveral information criteria. Numerical and graphical outputs are provided for\nall methods. The package is illustrated through the analyses of data from an\nendocrine study where the aim is to assess the capability of the body mass\nindex to detect the presence or absence of cardiovascular disease risk factors.\nThe package is available from CRAN at\nhttps://CRAN.R-project.org/package=ROCnReg.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2020 19:04:42 GMT"}, {"version": "v2", "created": "Fri, 24 Apr 2020 17:55:33 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 09:53:14 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2021 18:00:15 GMT"}, {"version": "v5", "created": "Fri, 19 Mar 2021 17:04:27 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Rodriguez-Alvarez", "Maria Xose", ""], ["Inacio", "Vanda", ""]]}, {"id": "2003.13849", "submitter": "Ad Ridder", "authors": "Shaul K. Bar-Lev and Ad Ridder", "title": "New exponential dispersion models for count data -- the ABM and LM\n  classes", "comments": "27 pages, 4 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their fundamental paper on cubic variance functions, Letac and Mora (The\nAnnals of Statistics,1990) presented a systematic, rigorous and comprehensive\nstudy of natural exponential families on the real line, their characterization\nthrough their variance functions and mean value parameterization. They\npresented a section that for some reason has been left unnoticed. This section\ndeals with the construction of variance functions associated with natural\nexponential families of counting distributions on the set of nonnegative\nintegers and allows to find the corresponding generating measures. As\nexponential dispersion models are based on natural exponential families, we\nintroduce in this paper two new classes of exponential dispersion models based\non their results. For these classes, which are associated with simple variance\nfunctions, we derive their mean value parameterization and their associated\ngenerating measures. We also prove that they have some desirable properties.\nBoth classes are shown to be overdispersed and zero-inflated in ascending\norder, making them as competitive statistical models for those in use in both,\nstatistical and actuarial modeling. To our best knowledge, the classes of\ncounting distributions we present in this paper, have not been introduced or\ndiscussed before in the literature. To show that our classes can serve as\ncompetitive statistical models for those in use (e.g., Poisson, Negative\nbinomial), we include a numerical example of real data. In this example, we\ncompare the performance of our classes with relevant competitive models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:38:35 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 21:45:14 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Bar-Lev", "Shaul K.", ""], ["Ridder", "Ad", ""]]}, {"id": "2003.13854", "submitter": "Ad Ridder", "authors": "Shaul K. Bar-Lev and Ad Ridder", "title": "Exponential Dispersion Models for Overdispersed Zero-Inflated Count Data", "comments": "22 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three new classes of exponential dispersion models of discrete\nprobability distributions which are defined by specifying their variance\nfunctions in their mean value parameterization. In a previous paper (Bar-Lev\nand Ridder, 2020a), we have developed the framework of these classes and proved\nthat they have some desirable properties. Each of these classes was shown to be\noverdispersed and zero inflated in ascending order, making them as competitive\nstatistical models for those in use in statistical modeling. In this paper we\nelaborate on the computational aspects of their probability mass functions.\nFurthermore, we apply these classes for fitting real data sets having\noverdispersed and zero-inflated statistics. Classic models based on Poisson or\nnegative binomial distributions show poor fits, and therefore many alternatives\nhave already proposed in recent years. We execute an extensive comparison with\nthese other proposals, from which we may conclude that our framework is a\nflexible tool that gives excellent results in all cases. Moreover, in most\ncases our model gives the best fit.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 22:51:50 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Bar-Lev", "Shaul K.", ""], ["Ridder", "Ad", ""]]}, {"id": "2003.13936", "submitter": "Hanyu Song", "authors": "Hanyu Song and Yingjian Wang and David B. Dunson", "title": "Distributed Bayesian clustering using finite mixture of mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many modern applications, there is interest in analyzing enormous data\nsets that cannot be easily moved across computers or loaded into memory on a\nsingle computer. In such settings, it is very common to be interested in\nclustering. Existing distributed clustering algorithms are mostly distance or\ndensity based without a likelihood specification, precluding the possibility of\nformal statistical inference. Model-based clustering allows statistical\ninference, yet research on distributed inference has emphasized nonparametric\nBayesian mixture models over finite mixture models. To fill this gap, we\nintroduce a nearly embarrassingly parallel algorithm for clustering under a\nBayesian overfitted finite mixture of Gaussian mixtures, which we term\ndistributed Bayesian clustering (DIB-C). DIB-C can flexibly accommodate data\nsets with various shapes (e.g. skewed or multi-modal). With data randomly\npartitioned and distributed, we first run Markov chain Monte Carlo in an\nembarrassingly parallel manner to obtain local clustering draws and then refine\nacross workers for a final clustering estimate based on any loss function on\nthe space of partitions. DIB-C can also estimate cluster densities, quickly\nclassify new subjects and provide a posterior predictive distribution. Both\nsimulation studies and real data applications show superior performance of\nDIB-C in terms of robustness and computational efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 03:28:36 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 04:42:43 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Song", "Hanyu", ""], ["Wang", "Yingjian", ""], ["Dunson", "David B.", ""]]}, {"id": "2003.14118", "submitter": "Andreas Groll", "authors": "Maike Hohberg and Andreas Groll", "title": "A flexible adaptive lasso Cox frailty model based on the full likelihood", "comments": "Keywords: Cox Proportional Hazards Model, Lasso, Regularization,\n  Variable Selection, B-Splines", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work a method to regularize Cox frailty models is proposed that\naccommodates time-varying covariates and time-varying coefficients and is based\non the full instead of the partial likelihood. A particular advantage in this\nframework is that the baseline hazard can be explicitly modeled in a smooth,\nsemi-parametric way, e.g. via P-splines. Regularization for variable selection\nis performed via a lasso penalty and via group lasso for categorical variables\nwhile a second penalty regularizes wiggliness of smooth estimates of\ntime-varying coefficients and the baseline hazard. Additionally, adaptive\nweights are included to stabilize the estimation. The method is implemented in\nR as coxlasso and will be compared to other packages for regularized Cox\nregression. Existing packages, however, do not allow for the combination of\ndifferent effects that are accommodated in coxlasso.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 11:49:30 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Hohberg", "Maike", ""], ["Groll", "Andreas", ""]]}, {"id": "2003.14331", "submitter": "Vladimir Temlyakov", "authors": "Vladimir Temlyakov", "title": "Numerical integration without smoothness assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider numerical integration in classes, for which we do not impose any\nsmoothness assumptions. We illustrate how nonlinear approximation, in\nparticular greedy approximation, allows us to guarantee some rate of decay of\nerrors of numerical integration even in such a general setting with no\nsmoothness assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2020 06:10:22 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Temlyakov", "Vladimir", ""]]}]