[{"id": "1108.0020", "submitter": "Thomas Loredo", "authors": "Thomas J. Loredo, James O. Berger, David F. Chernoff, Merlise A.\n  Clyde, Bin Liu", "title": "Bayesian Methods for Analysis and Adaptive Scheduling of Exoplanet\n  Observations", "comments": "29 pages, 11 figures. An abridged version is accepted for publication\n  in Statistical Methodology for a special issue on astrostatistics, with\n  selected (refereed) papers presented at the Astronomical Data Analysis\n  Conference (ADA VI) held in Monastir, Tunisia, in May 2010. Update corrects\n  equation (3)", "journal-ref": "Statistical Methodology 9 (2012) 101-114", "doi": "10.1016/j.stamet.2011.07.005", "report-no": null, "categories": "astro-ph.IM astro-ph.EP stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe work in progress by a collaboration of astronomers and\nstatisticians developing a suite of Bayesian data analysis tools for extrasolar\nplanet (exoplanet) detection, planetary orbit estimation, and adaptive\nscheduling of observations. Our work addresses analysis of stellar reflex\nmotion data, where a planet is detected by observing the \"wobble\" of its host\nstar as it responds to the gravitational tug of the orbiting planet. Newtonian\nmechanics specifies an analytical model for the resulting time series, but it\nis strongly nonlinear, yielding complex, multimodal likelihood functions; it is\neven more complex when multiple planets are present. The parameter spaces range\nin size from few-dimensional to dozens of dimensions, depending on the number\nof planets in the system, and the type of motion measured (line-of-sight\nvelocity, or position on the sky). Since orbits are periodic, Bayesian\ngeneralizations of periodogram methods facilitate the analysis. This relies on\nthe model being linearly separable, enabling partial analytical\nmarginalization, reducing the dimension of the parameter space. Subsequent\nanalysis uses adaptive Markov chain Monte Carlo methods and adaptive importance\nsampling to perform the integrals required for both inference (planet detection\nand orbit measurement), and information-maximizing sequential design (for\nadaptive scheduling of observations). We present an overview of our current\ntechniques and highlight directions being explored by ongoing research.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 21:24:29 GMT"}, {"version": "v2", "created": "Thu, 10 May 2018 05:05:44 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Loredo", "Thomas J.", ""], ["Berger", "James O.", ""], ["Chernoff", "David F.", ""], ["Clyde", "Merlise A.", ""], ["Liu", "Bin", ""]]}, {"id": "1108.0185", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong, Bin Dai, and Peter Z. G. Qian", "title": "OEM for least squares problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We propose an algorithm, called OEM (a.k.a. orthogonalizing EM), intended for\nvar- ious least squares problems. The first step, named active orthogonization,\northogonalizes an arbi- trary regression matrix by elaborately adding more\nrows. The second step imputes the responses of the new rows. The third step\nsolves the least squares problem of interest for the complete orthog- onal\ndesign. The second and third steps have simple closed forms, and iterate until\nconvergence. The algorithm works for ordinary least squares and regularized\nleast squares with the lasso, SCAD, MCP and other penalties. It has several\nattractive theoretical properties. For the ordinary least squares with a\nsingular regression matrix, an OEM sequence converges to the Moore-Penrose gen-\neralized inverse-based least squares estimator. For the SCAD and MCP, an OEM\nsequence can achieve the oracle property after sufficient iterations for a\nfixed or diverging number of variables. For ordinary and regularized least\nsquares with various penalties, an OEM sequence converges to a point having\ngrouping coherence for fully aliased regression matrices. Convergence and\nconvergence rate of the algorithm are examined. These convergence rate results\nshow that for the same data set, OEM converges faster for regularized least\nsquares than ordinary least squares. This provides a new theoretical comparison\nbetween these methods. Numerical examples are provided to illustrate the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jul 2011 15:50:02 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 01:22:34 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Xiong", "Shifeng", ""], ["Dai", "Bin", ""], ["Qian", "Peter Z. G.", ""]]}, {"id": "1108.0445", "submitter": "Surya Tokdar Surya Tokdar", "authors": "Surya T Tokdar", "title": "Adaptive Gaussian Predictive Process Approximation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of knots selection for Gaussian predictive process\nmethodology. Predictive process approximation provides an effective solution to\nthe cubic order computational complexity of Gaussian process models. This\napproximation crucially depends on a set of points, called knots, at which the\noriginal process is retained, while the rest is approximated via a\ndeterministic extrapolation. Knots should be few in number to keep the\ncomputational complexity low, but provide a good coverage of the process domain\nto limit approximation error. We present theoretical calculations to show that\ncoverage must be judged by the canonical metric of the Gaussian process. This\nnecessitates having in place a knots selection algorithm that automatically\nadapts to the changes in the canonical metric affected by changes in the\nparameter values controlling the Gaussian process covariance function. We\npresent an algorithm toward this by employing an incomplete Cholesky\nfactorization with pivoting and dynamic stopping. Although these concepts\nalready exist in the literature, our contribution lies in unifying them into a\nfast algorithm and in using computable error bounds to finesse implementation\nof the predictive process approximation. The resulting adaptive predictive\nprocess offers a substantial automatization of Guassian process model fitting,\nespecially for Bayesian applications where thousands of values of the\ncovariance parameters are to be explored.\n", "versions": [{"version": "v1", "created": "Mon, 1 Aug 2011 22:23:13 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Tokdar", "Surya T", ""]]}, {"id": "1108.0486", "submitter": "Richard Brent", "authors": "Nimalan Nandapalan, Richard P. Brent, Lawrence M. Murray and Alistair\n  Rendell", "title": "High-Performance Pseudo-Random Number Generation on Graphics Processing\n  Units", "comments": "10 pages, submitted to PPAM 2011 (Torun, Poland, 11-14 Sept. 2011).\n  For further information, see http://maths.anu.edu.au/~brent/pub/pub241.html", "journal-ref": "Lecture Notes in Computer Science, Vol. 7203 (2012), 609-618", "doi": null, "report-no": null, "categories": "cs.DC math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the deployment of pseudo-random number generators (PRNGs)\non graphics processing units (GPUs), developing an approach based on the\nxorgens generator to rapidly produce pseudo-random numbers of high statistical\nquality. The chosen algorithm has configurable state size and period, making it\nideal for tuning to the GPU architecture. We present a comparison of both speed\nand statistical quality with other common parallel, GPU-based PRNGs,\ndemonstrating favourable performance of the xorgens-based approach.\n", "versions": [{"version": "v1", "created": "Tue, 2 Aug 2011 05:41:44 GMT"}], "update_date": "2013-03-13", "authors_parsed": [["Nandapalan", "Nimalan", ""], ["Brent", "Richard P.", ""], ["Murray", "Lawrence M.", ""], ["Rendell", "Alistair", ""]]}, {"id": "1108.1079", "submitter": "Sylvie Tchumtchoua", "authors": "Sylvie Tchumtchoua, David B. Dunson, and Jeffrey S. Morris", "title": "Online Variational Bayes Inference for High-Dimensional Correlated Data", "comments": "36 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data with hundreds of thousands of observations are becoming\ncommonplace in many disciplines. The analysis of such data poses many\ncomputational challenges, especially when the observations are correlated over\ntime and/or across space. In this paper we propose flexible hierarchical\nregression models for analyzing such data that accommodate serial and/or\nspatial correlation. We address the computational challenges involved in\nfitting these models by adopting an approximate inference framework. We develop\nan online variational Bayes algorithm that works by incrementally reading the\ndata into memory one portion at a time. The performance of the method is\nassessed through simulation studies. We applied the methodology to analyze\nsignal intensity in MRI images of subjects with knee osteoarthritis, using data\nfrom the Osteoarthritis Initiative.\n", "versions": [{"version": "v1", "created": "Thu, 4 Aug 2011 13:16:11 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["Tchumtchoua", "Sylvie", ""], ["Dunson", "David B.", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "1108.1320", "submitter": "Rasmus Pagh", "authors": "Rasmus Pagh", "title": "Compressed Matrix Multiplication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problems of computing sample covariance matrices, and of\ntransforming a collection of vectors to a basis where they are sparse, we\npresent a simple algorithm that computes an approximation of the product of two\nn-by-n real matrices A and B. Let ||AB||_F denote the Frobenius norm of AB, and\nb be a parameter determining the time/accuracy trade-off. Given 2-wise\nindependent hash functions $_1,h_2: [n] -> [b], and s_1,s_2: [n] -> {-1,+1} the\nalgorithm works by first \"compressing\" the matrix product into the polynomial\np(x) = sum_{k=1}^n (sum_{i=1}^n A_{ik} s_1(i) x^{h_1(i)}) (sum_{j=1}^n B_{kj}\ns_2(j) x^{h_2(j)})\n  Using FFT for polynomial multiplication, we can compute c_0,...,c_{b-1} such\nthat sum_i c_i x^i = (p(x) mod x^b) + (p(x) div x^b) in time \\~O(n^2+ n b).\n  An unbiased estimator of (AB)_{ij} with variance at most ||AB||_F^2 / b can\nthen be computed as:\n  C_{ij} = s_1(i) s_2(j) c_{(h_1(i)+h_2(j)) mod b.\n  Our approach also leads to an algorithm for computing AB exactly, whp., in\ntime \\~O(N + nb) in the case where A and B have at most N nonzero entries, and\nAB has at most b nonzero entries.\n  Also, we use error-correcting codes in a novel way to recover significant\nentries of AB in near-linear time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Aug 2011 12:29:06 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2011 18:02:58 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Pagh", "Rasmus", ""]]}, {"id": "1108.1494", "submitter": "Alexandre Thiery", "authors": "Natesh S. Pillai, Andrew M. Stuart, Alexandre H. Thiery", "title": "Gradient Flow from a Random Walk in Hilbert Space", "comments": "Major revision of the original version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a probability measure on a Hilbert space defined via its density\nwith respect to a Gaussian. The purpose of this paper is to demonstrate that an\nappropriately defined Markov chain, which is reversible with respect to the\nmeasure in question, exhibits a diffusion limit to a noisy gradient flow, also\nreversible with respect to the same measure. The Markov chain is defined by\napplying a Metropolis-Hastings accept-reject mechanism to an Ornstein-Uhlenbeck\nproposal which is itself reversible with respect to the underlying Gaussian\nmeasure. The resulting noisy gradient flow is a stochastic partial differential\nequation driven by a Wiener process with spatial correlation given by the\nunderlying Gaussian structure.\n", "versions": [{"version": "v1", "created": "Sat, 6 Aug 2011 16:33:02 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2012 19:51:20 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2013 16:16:05 GMT"}, {"version": "v4", "created": "Fri, 18 Apr 2014 16:34:13 GMT"}], "update_date": "2014-04-21", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Stuart", "Andrew M.", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1108.2245", "submitter": "Michael Braun", "authors": "Michael Braun and Paul Damien", "title": "Generalized Direct Sampling for Hierarchical Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to sample from posterior distributions in\nhierarchical models without using Markov chain Monte Carlo. This method, which\nis a variant of importance sampling ideas, is generally applicable to\nhigh-dimensional models involving large data sets. Samples are independent, so\nthey can be collected in parallel, and we do not need to be concerned with\nissues like chain convergence and autocorrelation. Additionally, the method can\nbe used to compute marginal likelihoods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Aug 2011 18:54:53 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2011 17:25:48 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2012 15:19:26 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Braun", "Michael", ""], ["Damien", "Paul", ""]]}, {"id": "1108.2836", "submitter": "Julien Cornebise", "authors": "J. Cornebise, E. Moulines, J. Olsson", "title": "Adaptive sequential Monte Carlo by means of mixture of experts", "comments": "24 pages, 9 figures, under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriately designing the proposal kernel of particle filters is an issue\nof significant importance, since a bad choice may lead to deterioration of the\nparticle sample and, consequently, waste of computational power. In this paper\nwe introduce a novel algorithm adaptively approximating the so-called optimal\nproposal kernel by a mixture of integrated curved exponential distributions\nwith logistic weights. This family of distributions, referred to as mixtures of\nexperts, is broad enough to be used in the presence of multi-modality or\nstrongly skewed distributions. The mixtures are fitted, via online-EM methods,\nto the optimal kernel through minimisation of the Kullback-Leibler divergence\nbetween the auxiliary target and instrumental distributions of the particle\nfilter. At each iteration of the particle filter, the algorithm is required to\nsolve only a single optimisation problem for the whole particle sample,\nyielding an algorithm with only linear complexity. In addition, we illustrate\nin a simulation study how the method can be successfully applied to optimal\nfiltering in nonlinear state-space models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 03:17:37 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 14:12:05 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Cornebise", "J.", ""], ["Moulines", "E.", ""], ["Olsson", "J.", ""]]}, {"id": "1108.2883", "submitter": "Surya Tokdar", "authors": "Surya T. Tokdar and Ryan Martin", "title": "Bayesian test of normality versus a Dirichlet process mixture\n  alternative", "comments": "24 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian test of normality for univariate or multivariate data\nagainst alternative nonparametric models characterized by Dirichlet process\nmixture distributions. The alternative models are based on the principles of\nembedding and predictive matching. They can be interpreted to offer random\ngranulation of a normal distribution into a mixture of normals with mixture\ncomponents occupying a smaller volume the farther they are from the\ndistribution center. A scalar parametrization based on latent clustering is\nused to cover an entire spectrum of separation between the normal distributions\nand the alternative models. An efficient sequential importance sampler is\ndeveloped to calculate Bayes factors. Simulations indicate the proposed test\ncan detect non-normality without favoring the nonparametric alternative when\nnormality holds.\n", "versions": [{"version": "v1", "created": "Sun, 14 Aug 2011 15:51:13 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2013 19:56:22 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 05:08:00 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2019 08:37:27 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Tokdar", "Surya T.", ""], ["Martin", "Ryan", ""]]}, {"id": "1108.2999", "submitter": "Mohamed Cherfi", "authors": "Mohamed Cherfi", "title": "Dual $\\phi$-divergences estimation in normal models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of robust estimators which are obtained from dual representation of\n$\\phi$-divergences, are studied empirically for the normal location model.\nMembers of this class of estimators are compared, and it is found that they are\nefficient at the true model and offer an attractive alternative to the maximum\nlikelihood, in term of robustness .\n", "versions": [{"version": "v1", "created": "Mon, 15 Aug 2011 14:07:59 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Cherfi", "Mohamed", ""]]}, {"id": "1108.3137", "submitter": "Gareth Peters Dr", "authors": "Igor A. Korostil, Gareth W. Peters, Julien Cornebise, David G. Regan", "title": "Adaptive Markov Chain Monte Carlo Forward Simulation for Statistical\n  Analysis in Epidemic Modelling of Human Papillomavirus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian statistical model and estimation methodology based on\nForward Projection Adaptive Markov chain Monte Carlo in order to perform the\ncalibration of a high-dimensional non-linear system of Ordinary Differential\nEquations representing an epidemic model for Human Papillomavirus types 6 and\n11 (HPV-6, HPV-11). The model is compartmental and involves stratification by\nage, gender and sexual activity-group. Developing this model and a means to\ncalibrate it efficiently is relevant since HPV is a very multi-typed and common\nsexually transmitted infection with more than 100 types currently known. The\ntwo types studied in this paper, types 6 and 11, are causing about 90% of\nanogenital warts.\n  We extend the development of a sexual mixing matrix for the population, based\non a formulation first suggested by Garnett and Anderson. In particular we\nconsider a stochastic mixing matrix framework which allows us to jointly\nestimate unknown attributes and parameters of the mixing matrix along with the\nparameters involved in the calibration of the HPV epidemic model. This matrix\ndescribes the sexual interactions between members of the population under study\nand relies on several quantities which are a-priori unknown. The Bayesian model\ndeveloped allows one to estimate jointly the HPV-6 and HPV-11 epidemic model\nparameters such as the probability of transmission, HPV incubation period,\nduration of infection, duration of genital warts treatment, duration of\nimmunity, the probability of seroconversion, per gender, age-group and sexual\nactivity-group, as well as unknown sexual mixing matrix parameters related to\nassortativity. We conclude with simulation studies on synthetic and actual data\nfrom studies undertaken recently in Australia.\n", "versions": [{"version": "v1", "created": "Tue, 16 Aug 2011 01:41:07 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Korostil", "Igor A.", ""], ["Peters", "Gareth W.", ""], ["Cornebise", "Julien", ""], ["Regan", "David G.", ""]]}, {"id": "1108.3829", "submitter": "Rahul Mazumder", "authors": "Rahul Mazumder and Trevor Hastie", "title": "Exact covariance thresholding into connected components for large-scale\n  Graphical Lasso", "comments": "Report Version 2 (adding more experiments and correcting minor typos)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the sparse inverse covariance regularization problem or graphical\nlasso with regularization parameter $\\rho$. Suppose the co- variance graph\nformed by thresholding the entries of the sample covariance matrix at $\\rho$ is\ndecomposed into connected components. We show that the vertex-partition induced\nby the thresholded covariance graph is exactly equal to that induced by the\nestimated concentration graph. This simple rule, when used as a wrapper around\nexisting algorithms, leads to enormous performance gains. For large values of\n$\\rho$, our proposal splits a large graphical lasso problem into smaller\ntractable problems, making it possible to solve an otherwise infeasible large\nscale graphical lasso problem.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2011 19:52:38 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 03:06:36 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Mazumder", "Rahul", ""], ["Hastie", "Trevor", ""]]}, {"id": "1108.3988", "submitter": "Nikolas Kantas", "authors": "Nick Whiteley, Nikolas Kantas and Ajay Jasra", "title": "Linear Variance Bounds for Particle Approximations of Time-Homogeneous\n  Feynman-Kac Formulae", "comments": "to appear in Stochastic Processes and their Applications, 29 pages, 2\n  figures", "journal-ref": null, "doi": "10.1016/j.spa.2012.02.002", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article establishes sufficient conditions for a linear-in-time bound on\nthe non-asymptotic variance of particle approximations of time-homogeneous\nFeynman-Kac formulae. These formulae appear in a wide variety of applications\nincluding option pricing in finance and risk sensitive control in engineering.\nIn direct Monte Carlo approximation of these formulae, the non-asymptotic\nvariance typically increases at an exponential rate in the time parameter. It\nis shown that a linear bound holds when a non-negative kernel, defined by the\nlogarithmic potential function and Markov kernel which specify the Feynman-Kac\nmodel, satisfies a type of multiplicative drift condition and other regularity\nassumptions. Examples illustrate that these conditions are general and flexible\nenough to accommodate two rather extreme cases, which can occur in the context\nof a non-compact state space: 1) when the potential function is bounded above,\nnot bounded below and the Markov kernel is not ergodic; and 2) when the\npotential function is not bounded above, but the Markov kernel itself satisfies\na multiplicative drift condition.\n", "versions": [{"version": "v1", "created": "Fri, 19 Aug 2011 16:00:49 GMT"}, {"version": "v2", "created": "Mon, 13 Feb 2012 11:57:11 GMT"}], "update_date": "2012-02-14", "authors_parsed": [["Whiteley", "Nick", ""], ["Kantas", "Nikolas", ""], ["Jasra", "Ajay", ""]]}, {"id": "1108.4126", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Chi-square and classical exact tests often wildly misreport\n  significance; the remedy lies in computers", "comments": "63 pages, 51 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If a discrete probability distribution in a model being tested for\ngoodness-of-fit is not close to uniform, then forming the Pearson chi-square\nstatistic can involve division by nearly zero. This often leads to serious\ntrouble in practice -- even in the absence of round-off errors -- as the\npresent article illustrates via numerous examples. Fortunately, with the now\nwidespread availability of computers, avoiding all the trouble is simple and\neasy: without the problematic division by nearly zero, the actual values taken\nby goodness-of-fit statistics are not humanly interpretable, but black-box\ncomputer programs can rapidly calculate their precise significance.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 16:23:12 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 19:10:14 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1108.4146", "submitter": "Xun Huan", "authors": "Xun Huan and Youssef M. Marzouk", "title": "Simulation-based optimal Bayesian experimental design for nonlinear\n  systems", "comments": "Preprint 53 pages, 17 figures (54 small figures). v1 submitted to the\n  Journal of Computational Physics on August 4, 2011; v2 submitted on August\n  12, 2012. v2 changes: (a) addition of Appendix B and Figure 17 to address the\n  bias in the expected utility estimator; (b) minor language edits; v3\n  submitted on November 30, 2012. v3 changes: minor edits", "journal-ref": "Journal of Computational Physics 232 (2013) 288-317", "doi": "10.1016/j.jcp.2012.08.013", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal selection of experimental conditions is essential to maximizing\nthe value of data for inference and prediction, particularly in situations\nwhere experiments are time-consuming and expensive to conduct. We propose a\ngeneral mathematical framework and an algorithmic approach for optimal\nexperimental design with nonlinear simulation-based models; in particular, we\nfocus on finding sets of experiments that provide the most information about\ntargeted sets of parameters.\n  Our framework employs a Bayesian statistical setting, which provides a\nfoundation for inference from noisy, indirect, and incomplete data, and a\nnatural mechanism for incorporating heterogeneous sources of information. An\nobjective function is constructed from information theoretic measures,\nreflecting expected information gain from proposed combinations of experiments.\nPolynomial chaos approximations and a two-stage Monte Carlo sampling method are\nused to evaluate the expected information gain. Stochastic approximation\nalgorithms are then used to make optimization feasible in computationally\nintensive and high-dimensional settings. These algorithms are demonstrated on\nmodel problems and on nonlinear parameter estimation problems arising in\ndetailed combustion kinetics.\n", "versions": [{"version": "v1", "created": "Sat, 20 Aug 2011 22:49:15 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2012 18:46:49 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2012 23:34:15 GMT"}], "update_date": "2012-12-04", "authors_parsed": [["Huan", "Xun", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1108.4215", "submitter": "Juan Pablo Boyero", "authors": "Juan Pablo Boyero Garrido", "title": "Precise Computation of Position Accuracy in GNSS Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy and Availability computations for a GNSS System - or combination of\nSystems - through Service Volume Simulations take considerable time. Therefore,\nthe computation of the accuracy in 2D and 3D are often simplified by an\napproximate solution. The drawback is that such simplifications can lead to\naccuracy results that are too conservative (up to 25% in the 2D case and up to\n43% in the 3D case, for a 95% confidence level), which in turn translates into\npessimistic System Availability. This article presents a way to compute the\nexact accuracy, for any confidence level, for the one, two or three dimensional\ncases, through the derivation of corresponding factors. Using the factors\nintroduced here, allows getting accurate results swiftly.\n  The generic mathematical solution to compute the accuracy is presented. The\napproximate and precise computations are described. Then, the exact factors\nthat should be applied to obtain the accuracy at a typical confidence level\n(95%) are derived for the three cases.\n", "versions": [{"version": "v1", "created": "Sun, 21 Aug 2011 21:47:28 GMT"}], "update_date": "2011-08-23", "authors_parsed": [["Garrido", "Juan Pablo Boyero", ""]]}, {"id": "1108.4879", "submitter": "Brendan Tracey", "authors": "Brendan Tracey, David Wolpert and Juan J. Alonso", "title": "Using Supervised Learning to Improve Monte Carlo Integral Estimation", "comments": "18 pages, 10 figures, originally published by AIAA at the 13th\n  Non-Deterministic Approaches Conference", "journal-ref": "13th AIAA Non-Deterministic Approaches Conference, Denver, CO,\n  April 2011, AIAA Paper 2011-1843", "doi": null, "report-no": null, "categories": "stat.ML cs.CE cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) techniques are often used to estimate integrals of a\nmultivariate function using randomly generated samples of the function. In\nlight of the increasing interest in uncertainty quantification and robust\ndesign applications in aerospace engineering, the calculation of expected\nvalues of such functions (e.g. performance measures) becomes important.\nHowever, MC techniques often suffer from high variance and slow convergence as\nthe number of samples increases. In this paper we present Stacked Monte Carlo\n(StackMC), a new method for post-processing an existing set of MC samples to\nimprove the associated integral estimate. StackMC is based on the supervised\nlearning techniques of fitting functions and cross validation. It should reduce\nthe variance of any type of Monte Carlo integral estimate (simple sampling,\nimportance sampling, quasi-Monte Carlo, MCMC, etc.) without adding bias. We\nreport on an extensive set of experiments confirming that the StackMC estimate\nof an integral is more accurate than both the associated unprocessed Monte\nCarlo estimate and an estimate based on a functional fit to the MC samples.\nThese experiments run over a wide variety of integration spaces, numbers of\nsample points, dimensions, and fitting functions. In particular, we apply\nStackMC in estimating the expected value of the fuel burn metric of future\ncommercial aircraft and in estimating sonic boom loudness measures. We compare\nthe efficiency of StackMC with that of more standard methods and show that for\nnegligible additional computational cost significant increases in accuracy are\ngained.\n", "versions": [{"version": "v1", "created": "Wed, 24 Aug 2011 16:22:55 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Tracey", "Brendan", ""], ["Wolpert", "David", ""], ["Alonso", "Juan J.", ""]]}, {"id": "1108.4973", "submitter": "Alexandre Levada", "authors": "Alexandre L. M. Levada", "title": "Learning from Complex Systems: On the Roles of Entropy and Fisher\n  Information in Pairwise Isotropic Gaussian Markov Random Fields", "comments": "46 pages, 16 Figures", "journal-ref": "Entropy, v. 16, n. 2, Special Issue on Information Geometry, 2014", "doi": "10.3390/e16021002", "report-no": null, "categories": "cs.IT cs.AI cs.CV math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Random Field models are powerful tools for the study of complex\nsystems. However, little is known about how the interactions between the\nelements of such systems are encoded, especially from an information-theoretic\nperspective. In this paper, our goal is to enlight the connection between\nFisher information, Shannon entropy, information geometry and the behavior of\ncomplex systems modeled by isotropic pairwise Gaussian Markov random fields. We\npropose analytical expressions to compute local and global versions of these\nmeasures using Besag's pseudo-likelihood function, characterizing the system's\nbehavior through its \\emph{Fisher curve}, a parametric trajectory accross the\ninformation space that provides a geometric representation for the study of\ncomplex systems. Computational experiments show how the proposed tools can be\nuseful in extrating relevant information from complex patterns. The obtained\nresults quantify and support our main conclusion, which is: in terms of\ninformation, moving towards higher entropy states (A --> B) is different from\nmoving towards lower entropy states (B --> A), since the \\emph{Fisher curves}\nare not the same given a natural orientation (the direction of time).\n", "versions": [{"version": "v1", "created": "Thu, 25 Aug 2011 00:50:42 GMT"}, {"version": "v10", "created": "Mon, 27 May 2013 19:24:52 GMT"}, {"version": "v11", "created": "Tue, 11 Jun 2013 17:15:42 GMT"}, {"version": "v12", "created": "Wed, 16 Oct 2013 20:35:39 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2011 14:36:19 GMT"}, {"version": "v3", "created": "Tue, 4 Oct 2011 17:04:07 GMT"}, {"version": "v4", "created": "Fri, 25 Nov 2011 01:50:30 GMT"}, {"version": "v5", "created": "Thu, 24 May 2012 18:59:01 GMT"}, {"version": "v6", "created": "Mon, 26 Nov 2012 23:37:41 GMT"}, {"version": "v7", "created": "Sun, 2 Dec 2012 01:26:00 GMT"}, {"version": "v8", "created": "Sat, 8 Dec 2012 12:40:00 GMT"}, {"version": "v9", "created": "Thu, 23 May 2013 23:45:35 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Levada", "Alexandre L. M.", ""]]}, {"id": "1108.5364", "submitter": "Vasileios Maroulas", "authors": "Dwueng-Chwuan Jhwueng and Vasileios Maroulas", "title": "Phylogenetic Ornstein-Uhlenbeck regression curves", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression curves for studying trait relationships are developed herein. The\nadaptive evolution model is considered an Ornstein-Uhlenbeck system whose\nparameters are estimated by a novel engagement of generalized least-squares and\noptimization. Our algorithm is implemented to ecological data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Aug 2011 18:19:23 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 14:42:00 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Jhwueng", "Dwueng-Chwuan", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1108.6094", "submitter": "David Bailey", "authors": "Orianna DeMasi, Juan Meza, David H. Bailey", "title": "Dimension Reduction Using Rule Ensemble Machine Learning Methods: A\n  Numerical Study of Three Ensemble Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Ensemble methods for supervised machine learning have become popular due to\ntheir ability to accurately predict class labels with groups of simple,\nlightweight \"base learners.\" While ensembles offer computationally efficient\nmodels that have good predictive capability they tend to be large and offer\nlittle insight into the patterns or structure in a dataset. We consider an\nensemble technique that returns a model of ranked rules. The model accurately\npredicts class labels and has the advantage of indicating which parameter\nconstraints are most useful for predicting those labels. An example of the rule\nensemble method successfully ranking rules and selecting attributes is given\nwith a dataset containing images of potential supernovas where the number of\nnecessary features is reduced from 39 to 21. We also compare the rule ensemble\nmethod on a set of multi-class problems with boosting and bagging, which are\ntwo well known ensemble techniques that use decision trees as base learners,\nbut do not have a rule ranking scheme.\n", "versions": [{"version": "v1", "created": "Tue, 30 Aug 2011 22:36:15 GMT"}], "update_date": "2011-09-01", "authors_parsed": [["DeMasi", "Orianna", ""], ["Meza", "Juan", ""], ["Bailey", "David H.", ""]]}]