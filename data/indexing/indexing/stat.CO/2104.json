[{"id": "2104.00108", "submitter": "Jamie Yap", "authors": "Jamie Yap, John Dziak, Raju Maiti, Kevin Lynch, James R. McKay, Bibhas\n  Chakraborty, Inbal Nahum-Shani", "title": "Planning SMARTs: Sample size estimation for comparing dynamic treatment\n  regimens using longitudinal count outcomes with excess zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many health domains such as substance-use, outcomes are often counts with\nan excessive number of zeros (EZ) - count data having zero counts at a rate\nsignificantly higher than that expected of a standard count distribution (e.g.,\nPoisson). However, an important gap exists in sample size estimation\nmethodology for planning sequential multiple assignment randomized trials\n(SMARTs) for comparing dynamic treatment regimens (DTRs) using longitudinal\ncount data. DTRs, also known as treatment algorithms or adaptive interventions,\nmimic the individualized and evolving nature of patient care through the\nspecification of decision rules guiding the type, timing and modality of\ndelivery, and dosage of treatments to address the unique and changing needs of\nindividuals. To close this gap, we develop a Monte Carlo-based approach to\nsample size estimation. A SMART for engaging alcohol and cocaine-dependent\npatients in treatment is used as motivation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yap", "Jamie", ""], ["Dziak", "John", ""], ["Maiti", "Raju", ""], ["Lynch", "Kevin", ""], ["McKay", "James R.", ""], ["Chakraborty", "Bibhas", ""], ["Nahum-Shani", "Inbal", ""]]}, {"id": "2104.00673", "submitter": "Stephen Bates", "authors": "Stephen Bates and Trevor Hastie and Robert Tibshirani", "title": "Cross-validation: what does it estimate and how well does it do it?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a widely-used technique to estimate prediction error, but\nits behavior is complex and not fully understood. Ideally, one would like to\nthink that cross-validation estimates the prediction error for the model at\nhand, fit to the training data. We prove that this is not the case for the\nlinear model fit by ordinary least squares; rather it estimates the average\nprediction error of models fit on other unseen training sets drawn from the\nsame population. We further show that this phenomenon occurs for most popular\nestimates of prediction error, including data splitting, bootstrapping, and\nMallow's Cp. Next, the standard confidence intervals for prediction error\nderived from cross-validation may have coverage far below the desired level.\nBecause each data point is used for both training and testing, there are\ncorrelations among the measured accuracies for each fold, and so the usual\nestimate of variance is too small. We introduce a nested cross-validation\nscheme to estimate this variance more accurately, and show empirically that\nthis modification leads to intervals with approximately correct coverage in\nmany examples where traditional cross-validation intervals fail. Lastly, our\nanalysis also shows that when producing confidence intervals for prediction\naccuracy with simple data splitting, one should not re-fit the model on the\ncombined data, since this invalidates the confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:54 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:51:23 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2104.01360", "submitter": "Md Tahmid Rashid", "authors": "Md Tahmid Rashid, Na Wei, Dong Wang", "title": "A Survey on Social-Physical Sensing", "comments": "Submitted to IEEE Communication Surveys and Tutorials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propelled by versatile data capture, communication, and computing\ntechnologies, physical sensing has revolutionized the avenue for spontaneously\ncapturing and interpreting real-world phenomenon. Despite its virtues, various\nlimitations (e.g., high application specificity, partial autonomy, and sparse\ncoverage) hinder physical sensing's effectiveness in critical scenarios such as\ndisaster response. Meanwhile, social sensing is contriving as a pervasive\nsensing paradigm that leverages the observations from human participants\nequipped with portable devices and ubiquitous Internet connectivity (i.e.,\nthrough social media or crowdsensing apps) to perceive the environment. While\nsocial sensing possesses a plethora of benefits, it also inherently suffers\nfrom a few drawbacks (e.g., inconsistent reliability, uncertain data\nprovenance, and limited sensing availability). Motivated by the complementary\nvirtues of both physical and social sensing, social-physical sensing (SPS) is\nprotruding as an emerging sensing paradigm that tightly integrates social and\nphysical sensors at an unprecedented scale. The vision of SPS centers on\nmitigating the individual weaknesses of physical and social sensing while\nexploiting their collective strengths in reconstructing the \"state of the\nworld\", both physically and socially. While a good amount of interesting SPS\napplications has been explored, several important unsolved challenges and open\nresearch questions prevail in the way of developing dependable SPS systems,\nwhich require careful study to address. In this paper, we provide a\ncomprehensive survey of SPS, with an emphasis on its definition and key\nenablers, state-of-the-art applications, potential research challenges, and\nroad-map for future work. This paper intends to bridge the knowledge gap in\ncurrent literature by thoroughly examining the various aspects of SPS crucial\nfor building potent SPS systems.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:36:00 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Rashid", "Md Tahmid", ""], ["Wei", "Na", ""], ["Wang", "Dong", ""]]}, {"id": "2104.01707", "submitter": "Aaron Molstad", "authors": "Piotr M. Suder and Aaron J. Molstad", "title": "Scalable algorithms for semiparametric accelerated failure time models\n  in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semiparametric accelerated failure time (AFT) models are a useful alternative\nto Cox proportional hazards models, especially when the assumption of constant\nhazard ratios is untenable. However, rank-based criteria for fitting AFT models\nare often non-differentiable, which poses a computational challenge in\nhigh-dimensional settings. In this article, we propose a new alternating\ndirection method of multipliers algorithm for fitting semiparametric AFT models\nby minimizing a penalized rank-based loss function. Our algorithm scales well\nin both the number of subjects and number of predictors; and can easily\naccommodate a wide range of popular penalties. To improve the selection of\ntuning parameters, we propose a new criterion which avoids some common problems\nin cross-validation with censored responses. Through extensive simulation\nstudies, we show that our algorithm and software is much faster than existing\nmethods (which can only be applied to special cases), and we show that\nestimators which minimize a penalized rank-based criterion often outperform\nalternative estimators which minimize penalized weighted least squares\ncriteria. Application to nine cancer datasets further demonstrates that\nrank-based estimators of semiparametric AFT models are competitive with\nestimators assuming proportional hazards model in high-dimensional settings,\nwhereas weighted least squares estimators are often not. A software package\nimplementing the algorithm, along with a set of auxiliary functions, is\navailable for download at github.com/ajmolstad/penAFT.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 22:22:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Suder", "Piotr M.", ""], ["Molstad", "Aaron J.", ""]]}, {"id": "2104.02020", "submitter": "Dootika Vats", "authors": "Sanket Agrawal, Dootika Vats, Krzysztof {\\L}atuszy\\'nski, Gareth O.\n  Roberts", "title": "Optimal Scaling of MCMC Beyond Metropolis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimally scaling the proposal distribution in a Markov chain\nMonte Carlo algorithm is critical to the quality of the generated samples. Much\nwork has gone into obtaining such results for various Metropolis-Hastings (MH)\nalgorithms. Recently, acceptance probabilities other than MH are being employed\nin problems with intractable target distributions. There is little resource\navailable on tuning the Gaussian proposal distributions for this situation. We\nobtain optimal scaling results for a general class of acceptance functions,\nwhich includes Barker's and Lazy-MH acceptance functions. In particular,\noptimal values for Barker's algorithm are derived and are found to be\nsignificantly different from that obtained for MH algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:19:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Agrawal", "Sanket", ""], ["Vats", "Dootika", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2104.02134", "submitter": "Mattias Villani", "authors": "Mattias Villani, Matias Quiroz, Robert Kohn and Robert Salomone", "title": "Spectral Subsampling MCMC for Stationary Multivariate Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral subsampling MCMC was recently proposed to speed up Markov chain\nMonte Carlo (MCMC) for long stationary univariate time series by subsampling\nperiodogram observations in the frequency domain. This article extends the\napproach to stationary multivariate time series. It also proposes a\nmultivariate generalisation of the autoregressive tempered fractionally\ndifferentiated moving average model (ARTFIMA) and establishes some of its\nproperties. The new model is shown to provide a better fit compared to\nmultivariate autoregressive moving average models for three real world\nexamples. We demonstrate that spectral subsampling may provide up to two orders\nof magnitude faster estimation, while retaining MCMC sampling efficiency and\naccuracy, compared to spectral methods using the full dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:20:59 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Villani", "Mattias", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Salomone", "Robert", ""]]}, {"id": "2104.02444", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Lampros Bouranis and Robert Krause and Nial Friel", "title": "Statistical Network Analysis with Bergm", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in computational methods for intractable models have made\nnetwork data increasingly amenable to statistical analysis. Exponential random\ngraph models (ERGMs) emerged as one of the main families of models capable of\ncapturing the complex dependence structure of network data in a wide range of\napplied contexts. The Bergm package for R has become a popular package to carry\nout Bayesian parameter inference, missing data imputation, model selection and\ngoodness-of-fit diagnostics for ERGMs. Over the last few years, the package has\nbeen considerably improved in terms of efficiency by adopting some of the\nstate-of-the-art Bayesian computational methods for doubly-intractable\ndistributions. Recently, version 5 of the package has been made available on\nCRAN having undergone a substantial makeover, which has made it more accessible\nand easy to use for practitioners. New functions include data augmentation\nprocedures based on the approximate exchange algorithm for dealing with missing\ndata, adjusted pseudo-likelihood and pseudo-posterior procedures, which allow\nfor fast approximate inference of the ERGM parameter posterior and model\nevidence for networks on several thousands nodes.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:59:53 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Caimo", "Alberto", ""], ["Bouranis", "Lampros", ""], ["Krause", "Robert", ""], ["Friel", "Nial", ""]]}, {"id": "2104.02456", "submitter": "Tomoya Wakayama", "authors": "Tomoya Wakayama, Shonosuke Sugasawa", "title": "Locally Adaptive Smoothing for Functional Data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite increasing accessibility to function data, effective methods for\nflexibly estimating underlying trend structures are still scarce. We thereby\ndevelop locally adaptive smoothing methods for both functional time series and\nspatial data by extending trend filtering, a powerful nonparametric trend\nestimation technique for scalar data. We formulate the functional version of\ntrend filtering by introducing $L_2$-norm of the differences of adjacent trend\nfunctions. Through orthonormal basis expansion, we simplify the objective\nfunction to squared loss for coefficient vectors with grouped fused lasso\npenalty, and develop an efficient iteration algorithm for optimization. The\ntuning parameter in the proposed method is selected via cross validation. We\nalso consider an extension of the proposed algorithm to spatial functional\ndata. The proposed methods are demonstrated by simulation studies and an\napplication to two real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:23:13 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 13:40:21 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Wakayama", "Tomoya", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2104.02705", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Ruolin Shen, Christina Bukas, Lisa Barros de Andrade\n  e Sousa, Dominik Thalmeier, Nadja Klein, Chris Kolb, Florian Pfisterer,\n  Philipp Kopper, Bernd Bischl, Christian L. M\\\"uller", "title": "deepregression: a Flexible Neural Network Framework for Semi-Structured\n  Deep Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the implementation of semi-structured deep\ndistributional regression, a flexible framework to learn distributions based on\na combination of additive regression models and deep neural networks.\ndeepregression is implemented in both R and Python, using the deep learning\nlibraries TensorFlow and PyTorch, respectively. The implementation consists of\n(1) a modular neural network building system for the combination of various\nstatistical and deep learning approaches, (2) an orthogonalization cell to\nallow for an interpretable combination of different subnetworks as well as (3)\npre-processing steps necessary to initialize such models. The software package\nallows to define models in a user-friendly manner using distribution\ndefinitions via a formula environment that is inspired by classical statistical\nmodel frameworks such as mgcv. The packages' modular design and functionality\nprovides a unique resource for rapid and reproducible prototyping of complex\nstatistical and deep learning models while simultaneously retaining the\nindispensable interpretability of classical statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:56:31 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Shen", "Ruolin", ""], ["Bukas", "Christina", ""], ["Sousa", "Lisa Barros de Andrade e", ""], ["Thalmeier", "Dominik", ""], ["Klein", "Nadja", ""], ["Kolb", "Chris", ""], ["Pfisterer", "Florian", ""], ["Kopper", "Philipp", ""], ["Bischl", "Bernd", ""], ["M\u00fcller", "Christian L.", ""]]}, {"id": "2104.02717", "submitter": "Jonathan P Williams", "authors": "Jonathan P Williams", "title": "Discussion of \"A Gibbs sampler for a class of random convex polytopes\"", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An exciting new algorithmic breakthrough has been advanced for how to carry\nout inferences in a Dempster-Shafer (DS) formulation of a categorical data\ngenerating model. The developed sampling mechanism, which draws on theory for\ndirected graphs, is a clever and remarkable achievement, as this has been an\nopen problem for many decades. In this discussion, I comment on important\ncontributions, central questions, and prevailing matters of the article.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:27:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Williams", "Jonathan P", ""]]}, {"id": "2104.02764", "submitter": "Panpan Zhang", "authors": "Panpan Zhang, Tiandong Wang, Jun Yan", "title": "PageRank centrality and algorithms for weighted, directed networks with\n  applications to World Input-Output Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PageRank (PR) is a fundamental tool for assessing the relative importance of\nthe nodes in a network. In this paper, we propose a measure, weighted PageRank\n(WPR), extended from the classical PR for weighted, directed networks with\npossible non-uniform node-specific information that is dependent or independent\nof network structure. A tuning parameter leveraging node degree and strength is\nintroduced. An efficient algorithm based on R program has been developed for\ncomputing WPR in large-scale networks. We have tested the proposed WPR on\nwidely used simulated network models, and found it outperformed other competing\nmeasures in the literature. By applying the proposed WPR to the real network\ndata generated from World Input-Output Tables, we have seen the results that\nare consistent with the global economic trends, which renders it a preferred\nmeasure in the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:08:58 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 03:34:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhang", "Panpan", ""], ["Wang", "Tiandong", ""], ["Yan", "Jun", ""]]}, {"id": "2104.02949", "submitter": "Hyunjoo Yang", "authors": "Hyunjoo Yang and Jaeyong Lee", "title": "Laplace-aided variational inference for differential equation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equation (ODE) model whose regression curves are a set\nof solution curves for some ODEs, poses a challenge in parameter estimation.\nThe challenge due to the frequent absence of analytic solutions and the\ncomplicated likelihood surface, tends to be more severe especially for larger\nmodels with many parameters and variables. Yang and Lee (2020) proposed\nstate-space model with variational Bayes (SSVB) for ODE, capable of fast and\nstable estimation in somewhat large ODE models. The method has shown excellent\nperformance in parameter estimation but has a weakness of underestimation of\nthe posterior covariance, which originates from the mean-field variational\nmethod. This paper proposes a way to overcome the weakness, by using the\nLaplace approximation. In numerical experiments, the covariance modified by the\nLaplace approximation showed a high degree of improvement when checked against\nthe covariances obtained by a standard Markov chain Monte Carlo method. With\nthe improved covariance estimation, the SSVB renders fairly accurate posterior\napproximations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:42:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yang", "Hyunjoo", ""], ["Lee", "Jaeyong", ""]]}, {"id": "2104.03193", "submitter": "Mai Ngoc Bui Miss", "authors": "Mai Ngoc Bui, Yvo Pokern and Petros Dellaportas", "title": "Inference for partially observed Riemannian Ornstein--Uhlenbeck\n  diffusions of covariance matrices", "comments": "35 pages, 13 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a generalization of the Ornstein--Uhlenbeck processes on the\ncone of covariance matrices endowed with the Log-Euclidean and the\nAffine-Invariant metrics. Our development exploits the Riemannian geometric\nstructure of symmetric positive definite matrices viewed as a differential\nmanifold. We then provide Bayesian inference for discretely observed diffusion\nprocesses of covariance matrices based on an MCMC algorithm built with the help\nof a novel diffusion bridge sampler accounting for the geometric structure. Our\nproposed algorithm is illustrated with a real data financial application.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:34:26 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Bui", "Mai Ngoc", ""], ["Pokern", "Yvo", ""], ["Dellaportas", "Petros", ""]]}, {"id": "2104.03307", "submitter": "Richard Clancy", "authors": "Richard J Clancy and Stephen Becker", "title": "Approximate maximum likelihood estimators for linear regression with\n  design matrix uncertainty", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider regression problems subject to arbitrary noise in\nthe operator or design matrix. This characterization appropriately models many\nphysical phenomena with uncertainty in the regressors. Although the problem has\nbeen studied extensively for ordinary/total least squares, and via models that\nimplicitly or explicitly assume Gaussianity, less attention has been paid to\nimproving estimation for regression problems under general uncertainty in the\ndesign matrix. To address difficulties encountered when dealing with\ndistributions of sums of random variables, we rely on the saddle point method\nto estimate densities and form an approximate log-likelihood to maximize. We\nshow that the proposed method performs favorably against other classical\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:58:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Clancy", "Richard J", ""], ["Becker", "Stephen", ""]]}, {"id": "2104.03446", "submitter": "Jiguo Cao", "authors": "Hua Liu, Jinhong You and Jiguo Cao", "title": "Functional L-Optimality Subsampling for Massive Data", "comments": "37 pages and 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive data bring the big challenges of memory and computation for analysis.\nThese challenges can be tackled by taking subsamples from the full data as a\nsurrogate. For functional data, it is common to collect multiple measurements\nover their domains, which require even more memory and computation time when\nthe sample size is large. The computation would be much more intensive when\nstatistical inference is required through bootstrap samples. To the best of our\nknowledge, this article is the first attempt to study the subsampling method\nfor the functional linear model. We propose an optimal subsampling method based\non the functional L-optimality criterion. When the response is a discrete or\ncategorical variable, we further extend our proposed functional L-optimality\nsubsampling (FLoS) method to the functional generalized linear model. We\nestablish the asymptotic properties of the estimators by the FLoS method. The\nfinite sample performance of our proposed FLoS method is investigated by\nextensive simulation studies. The FLoS method is further demonstrated by\nanalyzing two large-scale datasets: the global climate data and the kidney\ntransplant data. The analysis results on these data show that the FLoS method\nis much better than the uniform subsampling approach and can well approximate\nthe results based on the full data while dramatically reducing the computation\ntime and memory.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:41:35 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 04:20:48 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Hua", ""], ["You", "Jinhong", ""], ["Cao", "Jiguo", ""]]}, {"id": "2104.03448", "submitter": "H.Sherry Zhang", "authors": "H.Sherry Zhang, Dianne Cook, Ursula Laa, Nicolas Langren\\'e, and\n  Patricia Men\\'endez", "title": "Visual Diagnostics for Constrained Optimisation with Application to\n  Guided Tours", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A guided tour helps to visualise high-dimensional data by showing\nlow-dimensional projections along a projection pursuit optimisation path.\nProjection pursuit is a generalisation of principal component analysis, in the\nsense that different indexes are used to define the interestingness of the\nprojected data. While much work has been done in developing new indexes in the\nliterature, less has been done on understanding the optimisation. Index\nfunctions can be noisy, might have multiple local maxima as well as an optimal\nmaximum, and are constrained to generate orthonormal projection frames, which\ncomplicates the optimization. In addition, projection pursuit is primarily used\nfor exploratory data analysis, and finding the local maxima is also useful. The\nguided tour is especially useful for exploration, because it conducts geodesic\ninterpolation connecting steps in the optimisation and shows how the projected\ndata changes as a maxima is approached. This work provides new visual\ndiagnostics for examining a choice of optimisation procedure, based on the\nprovision of a new data object which collects information throughout the\noptimisation. It has helped to diagnose and fix several problems with\nprojection pursuit guided tour. This work might be useful more broadly for\ndiagnosing optimisers, and comparing their performance. The diagnostics are\nimplemented in the R package, ferrn.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:52:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zhang", "H. Sherry", ""], ["Cook", "Dianne", ""], ["Laa", "Ursula", ""], ["Langren\u00e9", "Nicolas", ""], ["Men\u00e9ndez", "Patricia", ""]]}, {"id": "2104.03889", "submitter": "Lorenzo Pacchiardi", "authors": "Lorenzo Pacchiardi, Ritabrata Dutta", "title": "Generalized Bayesian Likelihood-Free Inference Using Scoring Rules\n  Estimators", "comments": "23 pages, plus references and appendices. 14 figures. Code available\n  at https://github.com/LoryPack/GenBayes_LikelihoodFree_ScoringRules. v3:\n  typos fixes and small additions. v2: added more theoretical and experimental\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for Bayesian Likelihood-Free Inference (LFI) based on\nGeneralized Bayesian Inference using scoring rules (SRs). SRs are used to\nevaluate probabilistic models given an observation; a proper SR is minimised in\nexpectation when the model corresponds to the data generating process for the\nobservations. Using a strictly proper SR, for which the above minimum is\nunique, ensures posterior consistency of our method. Further, we prove finite\nsample posterior consistency and outlier robustness of our posterior for the\nKernel and Energy Scores. As the likelihood function is intractable for LFI, we\nemploy consistent estimators of SRs using model simulations in a\npseudo-marginal MCMC; we show the target of such chain converges to the exact\nSR posterior by increasing the number of simulations. Furthermore, we note\npopular LFI techniques such as Bayesian Synthetic Likelihood (BSL) can be seen\nas special cases of our framework using only proper (but not strictly so) SR.\nWe empirically validate our consistency and outlier robustness results and show\nhow related approaches do not enjoy these properties. Practically, we use the\nEnergy and Kernel Scores, but our general framework sets the stage for\nextensions with other scoring rules.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:00:41 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 17:14:26 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 15:17:56 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Pacchiardi", "Lorenzo", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2104.03942", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Jukka Corander", "title": "Approximate Bayesian inference from noisy likelihoods with Gaussian\n  process emulated MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient approach for doing approximate Bayesian inference\nwhen only a limited number of noisy likelihood evaluations can be obtained due\nto computational constraints, which is becoming increasingly common for\napplications of complex models. Our main methodological innovation is to model\nthe log-likelihood function using a Gaussian process (GP) in a local fashion\nand apply this model to emulate the progression that an exact\nMetropolis-Hastings (MH) algorithm would take if it was applicable. New\nlog-likelihood evaluation locations are selected using sequential experimental\ndesign strategies such that each MH accept/reject decision is done within a\npre-specified error tolerance. The resulting approach is conceptually simple\nand sample-efficient as it takes full advantage of the GP model. It is also\nmore robust to violations of GP modelling assumptions and better suited for the\ntypical situation where the posterior is substantially more concentrated than\nthe prior, compared with various existing inference methods based on global GP\nsurrogate modelling. We discuss the probabilistic interpretations and central\ntheoretical aspects of our approach, and we then demonstrate the benefits of\nthe resulting algorithm in the context of likelihood-free inference for\nsimulator-based statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:38:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Corander", "Jukka", ""]]}, {"id": "2104.03995", "submitter": "Radoslav Harman", "authors": "Radoslav Harman, Lenka Filov\\'a, Samuel Rosa", "title": "Optimal Design of Multifactor Experiments via Grid Exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For computing efficient approximate designs of multifactor experiments, we\npropose a simple algorithm based on adaptive exploration of the grid of all\ncombinations of factor levels. We demonstrate that the algorithm significantly\noutperforms several state-of-the-art competitors for problems with discrete,\ncontinuous, as well as mixed factors. Importantly, we provide a free R code\nthat permits direct verification of the numerical results and allows the\nresearchers to easily compute optimal or nearly-optimal experimental designs\nfor their own statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:15:58 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Harman", "Radoslav", ""], ["Filov\u00e1", "Lenka", ""], ["Rosa", "Samuel", ""]]}, {"id": "2104.04514", "submitter": "Tin Nguyen", "authors": "Brian L. Trippe, Tin D. Nguyen, Tamara Broderick", "title": "Optimal transport couplings of Gibbs samplers on partitions for unbiased\n  estimation", "comments": "Brian Trippe and Tin Nguyen contributed equally. First presented at\n  3rd Symposium on Advances in Approximate Bayesian Inference, 2020. This\n  version fixes authoring order of first version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational couplings of Markov chains provide a practical route to\nunbiased Monte Carlo estimation that can utilize parallel computation. However,\nthese approaches depend crucially on chains meeting after a small number of\ntransitions. For models that assign data into groups, e.g. mixture models, the\nobvious approaches to couple Gibbs samplers fail to meet quickly. This failure\nowes to the so-called \"label-switching\" problem; semantically equivalent\nrelabelings of the groups contribute well-separated posterior modes that impede\nfast mixing and cause large meeting times. We here demonstrate how to avoid\nlabel switching by considering chains as exploring the space of partitions\nrather than labelings. Using a metric on this space, we employ an optimal\ntransport coupling of the Gibbs conditionals. This coupling outperforms\nalternative couplings that rely on labelings and, on a real dataset, provides\nestimates more precise than usual ergodic averages in the limited time regime.\nCode is available at github.com/tinnguyen96/coupling-Gibbs-partition.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 17:55:12 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:11:14 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Trippe", "Brian L.", ""], ["Nguyen", "Tin D.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2104.05076", "submitter": "Daoji Li", "authors": "Ruipeng Dong, Daoji Li, Zemin Zheng", "title": "Parallel integrative learning for large-scale multi-response regression\n  with incomplete outcomes", "comments": "32 pages", "journal-ref": "Computational Statistics and Data Analysis, 2021", "doi": "10.1016/j.csda.2021.107243", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multi-task learning is increasingly used to investigate the association\nstructure between multiple responses and a single set of predictor variables in\nmany applications. In the era of big data, the coexistence of incomplete\noutcomes, large number of responses, and high dimensionality in predictors\nposes unprecedented challenges in estimation, prediction, and computation. In\nthis paper, we propose a scalable and computationally efficient procedure,\ncalled PEER, for large-scale multi-response regression with incomplete\noutcomes, where both the numbers of responses and predictors can be\nhigh-dimensional. Motivated by sparse factor regression, we convert the\nmulti-response regression into a set of univariate-response regressions, which\ncan be efficiently implemented in parallel. Under some mild regularity\nconditions, we show that PEER enjoys nice sampling properties including\nconsistency in estimation, prediction, and variable selection. Extensive\nsimulation studies show that our proposal compares favorably with several\nexisting methods in estimation accuracy, variable selection, and computation\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:01:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Dong", "Ruipeng", ""], ["Li", "Daoji", ""], ["Zheng", "Zemin", ""]]}, {"id": "2104.05091", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Simple, Optimal Algorithms for Random Sampling Without Replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the fundamental problem of drawing a simple random sample of size k\nwithout replacement from [n] := {1, . . . , n}. Although a number of classical\nalgorithms exist for this problem, we construct algorithms that are even\nsimpler, easier to implement, and have optimal space and time complexity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:06:13 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "2104.05110", "submitter": "Brieuc Lehmann", "authors": "Brieuc Lehmann and Simon White", "title": "Bayesian exponential random graph models for populations of networks", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The collection of data on populations of networks is becoming increasingly\ncommon, where each data point can be seen as a realisation of a network-valued\nrandom variable. A canonical example is that of brain networks: a typical\nneuroimaging study collects one or more brain scans across multiple\nindividuals, each of which can be modelled as a network with nodes\ncorresponding to distinct brain regions and edges corresponding to structural\nor functional connections between these regions. Most statistical network\nmodels, however, were originally proposed to describe a single underlying\nrelational structure, although recent years have seen a drive to extend these\nmodels to populations of networks. Here, we propose one such extension: a\nmultilevel framework for populations of networks based on exponential random\ngraph models. By pooling information across the individual networks, this\nframework provides a principled approach to characterise the relational\nstructure for an entire population. To perform inference, we devise a novel\nexchange-within-Gibbs MCMC algorithm that generates samples from the\ndoubly-intractable posterior. To illustrate our framework, we use it to assess\ngroup-level variations in networks derived from fMRI scans, enabling the\ninference of age-related differences in the topological structure of the\nbrain's functional connectivity.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 21:15:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lehmann", "Brieuc", ""], ["White", "Simon", ""]]}, {"id": "2104.05134", "submitter": "Kai Xu", "authors": "Kai Xu, Tor Erlend Fjelde, Charles Sutton, Hong Ge", "title": "Couplings for Multinomial Hamiltonian Monte Carlo", "comments": "Published in AISTATS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a popular sampling method in Bayesian\ninference. Recently, Heng & Jacob (2019) studied Metropolis HMC with couplings\nfor unbiased Monte Carlo estimation, establishing a generic parallelizable\nscheme for HMC. However, in practice a different HMC method, multinomial HMC,\nis considered as the go-to method, e.g. as part of the no-U-turn sampler. In\nmultinomial HMC, proposed states are not limited to end-points as in Metropolis\nHMC; instead points along the entire trajectory can be proposed. In this paper,\nwe establish couplings for multinomial HMC, based on optimal transport for\nmultinomial sampling in its transition. We prove an upper bound for the meeting\ntime - the time it takes for the coupled chains to meet - based on the notion\nof local contractivity. We evaluate our methods using three targets: 1,000\ndimensional Gaussians, logistic regression and log-Gaussian Cox point\nprocesses. Compared to Heng & Jacob (2019), coupled multinomial HMC generally\nattains a smaller meeting time, and is more robust to choices of step sizes and\ntrajectory lengths, which allows re-use of existing adaptation methods for HMC.\nThese improvements together paves the way for a wider and more practical use of\ncoupled HMC methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 23:06:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Kai", ""], ["Fjelde", "Tor Erlend", ""], ["Sutton", "Charles", ""], ["Ge", "Hong", ""]]}, {"id": "2104.05886", "submitter": "Zuheng Xu", "authors": "Zuheng Xu, Trevor Campbell", "title": "The computational asymptotics of Gaussian variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular alternative to Markov chain Monte Carlo\nmethods that constructs a Bayesian posterior approximation by minimizing a\ndiscrepancy to the true posterior within a pre-specified family. This converts\nBayesian inference into an optimization problem, enabling the use of simple and\nscalable stochastic optimization algorithms. However, a key limitation of\nvariational inference is that the optimal approximation is typically not\ntractable to compute; even in simple settings the problem is nonconvex. Thus,\nrecently developed statistical guarantees -- which all involve the (data)\nasymptotic properties of the optimal variational distribution -- are not\nreliably obtained in practice. In this work, we provide two major\ncontributions: a theoretical analysis of the asymptotic convexity properties of\nvariational inference in the popular setting with a Gaussian family; and\nconsistent stochastic variational inference (CSVI), an algorithm that exploits\nthese properties to find the optimal approximation in the asymptotic regime.\nCSVI consists of a tractable initialization procedure that finds the local\nbasin of the optimal solution, and a scaled gradient descent algorithm that\nstays locally confined to that basin. Experiments on nonconvex synthetic and\nreal-data examples show that compared with standard stochastic gradient\ndescent, CSVI improves the likelihood of obtaining the globally optimal\nposterior approximation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:23:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Zuheng", ""], ["Campbell", "Trevor", ""]]}, {"id": "2104.06384", "submitter": "Philippe Gagnon", "authors": "Sebastian M Schmon and Philippe Gagnon", "title": "Optimal scaling of random walk Metropolis algorithms using Bayesian\n  large-sample asymptotics", "comments": "Both authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional limit theorems have been shown to be useful to derive tuning\nrules for finding the optimal scaling in random walk Metropolis algorithms. The\nassumptions under which weak convergence results are proved are however\nrestrictive; the target density is typically assumed to be of a product form.\nUsers may thus doubt the validity of such tuning rules in practical\napplications. In this paper, we shed some light on optimal scaling problems\nfrom a different perspective, namely a large-sample one. This allows to prove\nweak convergence results under realistic assumptions and to propose novel\nparameter-dimension-dependent tuning guidelines. The proposed guidelines are\nconsistent with previous ones when the target density is close to having a\nproduct form, but significantly different otherwise.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:39:50 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 23:27:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schmon", "Sebastian M", ""], ["Gagnon", "Philippe", ""]]}, {"id": "2104.06648", "submitter": "Eugene Ndiaye", "authors": "Eugene Ndiaye and Ichiro Takeuchi", "title": "Root-finding Approaches for Computing Conformal Prediction Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conformal prediction constructs a confidence set for an unobserved response\nof a feature vector based on previous identically distributed and exchangeable\nobservations of responses and features. It has a coverage guarantee at any\nnominal level without additional assumptions on their distribution. Its\ncomputation deplorably requires a refitting procedure for all replacement\ncandidates of the target response. In regression settings, this corresponds to\nan infinite number of model fit. Apart from relatively simple estimators that\ncan be written as pieces of linear function of the response, efficiently\ncomputing such sets is difficult and is still considered as an open problem. We\nexploit the fact that, \\emph{often}, conformal prediction sets are intervals\nwhose boundaries can be efficiently approximated by classical root-finding\nalgorithm. We investigate how this approach can overcome many limitations of\nformerly used strategies and we discuss its complexity and drawbacks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 06:41:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:05:26 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ndiaye", "Eugene", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2104.06771", "submitter": "Aur\\'elien Enfroy", "authors": "Alain Durmus, Andreas Eberle, Aur\\'elien Enfroy, Arnaud Guillin and\n  Pierre Monmarch\\'e", "title": "Discrete sticky couplings of functional autoregressive processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide bounds in Wasserstein and total variation distances\nbetween the distributions of the successive iterates of two functional\nautoregressive processes with isotropic Gaussian noise of the form $Y_{k+1} =\n\\mathrm{T}_\\gamma(Y_k) + \\sqrt{\\gamma\\sigma^2} Z_{k+1}$ and $\\tilde{Y}_{k+1} =\n\\tilde{\\mathrm{T}}_\\gamma(\\tilde{Y}_k) + \\sqrt{\\gamma\\sigma^2}\n\\tilde{Z}_{k+1}$. More precisely, we give non-asymptotic bounds on\n$\\rho(\\mathcal{L}(Y_{k}),\\mathcal{L}(\\tilde{Y}_k))$, where $\\rho$ is an\nappropriate weighted Wasserstein distance or a $V$-distance, uniformly in the\nparameter $\\gamma$, and on $\\rho(\\pi_{\\gamma},\\tilde{\\pi}_{\\gamma})$, where\n$\\pi_{\\gamma}$ and $\\tilde{\\pi}_{\\gamma}$ are the respective stationary\nmeasures of the two processes. The class of considered processes encompasses\nthe Euler-Maruyama discretization of Langevin diffusions and its variants. The\nbounds we derive are of order $\\gamma$ as $\\gamma \\to 0$. To obtain our\nresults, we rely on the construction of a discrete sticky Markov chain\n$(W_k^{(\\gamma)})_{k \\in \\mathbb{N}}$ which bounds the distance between an\nappropriate coupling of the two processes. We then establish stability and\nquantitative convergence results for this process uniformly on $\\gamma$. In\naddition, we show that it converges in distribution to the continuous sticky\nprocess studied in previous work. Finally, we apply our result to Bayesian\ninference of ODE parameters and numerically illustrate them on two particular\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:03:01 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Durmus", "Alain", ""], ["Eberle", "Andreas", ""], ["Enfroy", "Aur\u00e9lien", ""], ["Guillin", "Arnaud", ""], ["Monmarch\u00e9", "Pierre", ""]]}, {"id": "2104.06919", "submitter": "Felipe Uribe", "authors": "Felipe Uribe and Johnathan M. Bardsley and Yiqiu Dong and Per\n  Christian Hansen and Nicolai A. B. Riis", "title": "A hybrid Gibbs sampler for edge-preserving tomographic reconstruction\n  with uncertain view angles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computed tomography, data consist of measurements of the attenuation of\nX-rays passing through an object. The goal is to reconstruct the linear\nattenuation coefficient of the object's interior. For each position of the\nX-ray source, characterized by its angle with respect to a fixed coordinate\nsystem, one measures a set of data referred to as a view. A common assumption\nis that these view angles are known, but in some applications they are known\nwith imprecision. We propose a framework to solve a Bayesian inverse problem\nthat jointly estimates the view angles and an image of the object's attenuation\ncoefficient. We also include a few hyperparameters that characterize the\nlikelihood and the priors. Our approach is based on a Gibbs sampler where the\nassociated conditional densities are simulated using different sampling schemes\n- hence the term hybrid. In particular, the conditional distribution associated\nwith the reconstruction is nonlinear in the image pixels, non-Gaussian and\nhigh-dimensional. We approach this distribution by constructing a Laplace\napproximation that represents the target conditional locally at each Gibbs\niteration. This enables sampling of the attenuation coefficients in an\nefficient manner using iterative reconstruction algorithms. The numerical\nresults show that our algorithm is able to jointly identify the image and the\nview angles, while also providing uncertainty estimates of both. We demonstrate\nour method with 2D X-ray computed tomography problems using fan beam\nconfigurations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:07:19 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Uribe", "Felipe", ""], ["Bardsley", "Johnathan M.", ""], ["Dong", "Yiqiu", ""], ["Hansen", "Per Christian", ""], ["Riis", "Nicolai A. B.", ""]]}, {"id": "2104.07084", "submitter": "Hussein Hazimeh", "authors": "Hussein Hazimeh, Rahul Mazumder, Peter Radchenko", "title": "Grouped Variable Selection with Discrete Optimization: Computational and\n  Statistical Perspectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithmic framework for grouped variable selection that is\nbased on discrete mathematical optimization. While there exist several\nappealing approaches based on convex relaxations and nonconvex heuristics, we\nfocus on optimal solutions for the $\\ell_0$-regularized formulation, a problem\nthat is relatively unexplored due to computational challenges. Our methodology\ncovers both high-dimensional linear regression and nonparametric sparse\nadditive modeling with smooth components. Our algorithmic framework consists of\napproximate and exact algorithms. The approximate algorithms are based on\ncoordinate descent and local search, with runtimes comparable to popular sparse\nlearning algorithms. Our exact algorithm is based on a standalone\nbranch-and-bound (BnB) framework, which can solve the associated mixed integer\nprogramming (MIP) problem to certified optimality. By exploiting the problem\nstructure, our custom BnB algorithm can solve to optimality problem instances\nwith $5 \\times 10^6$ features in minutes to hours -- over $1000$ times larger\nthan what is currently possible using state-of-the-art commercial MIP solvers.\nWe also explore statistical properties of the $\\ell_0$-based estimators. We\ndemonstrate, theoretically and empirically, that our proposed estimators have\nan edge over popular group-sparse estimators in terms of statistical\nperformance in various regimes.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:21:59 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""]]}, {"id": "2104.07146", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko, Ronald Kriemann, Vladimir Berikov", "title": "Identification of unknown parameters and prediction with hierarchical\n  matrices", "comments": "16 pages, 5 tables, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of massive datasets very often implies expensive linear\nalgebra operations with large dense matrices. Typical tasks are an estimation\nof unknown parameters of the underlying statistical model and prediction of\nmissing values. We developed the H-MLE procedure, which solves these tasks. The\nunknown parameters can be estimated by maximizing the joint Gaussian\nlog-likelihood function, which depends on a covariance matrix. To decrease high\ncomputational cost, we approximate the covariance matrix in the hierarchical\n(H-) matrix format. The H-matrix technique allows us to work with inhomogeneous\ncovariance matrices and almost arbitrary locations. Especially, H-matrices can\nbe applied in cases when the matrices under consideration are dense and\nunstructured.\n  For validation purposes, we implemented three machine learning methods: the\nk-nearest neighbors (kNN), random forest, and deep neural network. The best\nresults (for the given datasets) were obtained by the kNN method with three or\nseven neighbors depending on the dataset. The results computed with the H-MLE\nmethod were compared with the results obtained by the kNN method.\n  The developed H-matrix code and all datasets are freely available online.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:13:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Litvinenko", "Alexander", ""], ["Kriemann", "Ronald", ""], ["Berikov", "Vladimir", ""]]}, {"id": "2104.07180", "submitter": "Xuan Wu", "authors": "Xuan Wu", "title": "Enhanced Monte Carlo Estimation of the Fisher Information Matrix with\n  Independent Perturbations for Complex Problems", "comments": "14 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Fisher information matrix provides a way to measure the amount of\ninformation given observed data based on parameters of interest. Many\napplications of the FIM exist in statistical modeling, system identification,\nand parameter estimation. We sometimes use the Monte Carlo-based method to\nestimate the FIM because its analytical form is often impossible or difficult\nto be computed in real-world models. In this paper, we review the basic method\nbased on simultaneous perturbations and present an enhanced resampling-based\nmethod with independent simultaneous perturbations to estimate the Fisher\ninformation matrix. We conduct theoretical and numerical analysis to show its\naccuracy via variance reduction from $O(1/N)$ to $O(1/(nN))$, where $n$ is the\nsample size of the data and $N$ is a measure of the Monte Carlo averaging. We\nalso consider the trade-off between accuracy and computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:34:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Wu", "Xuan", ""]]}, {"id": "2104.07212", "submitter": "Guanyang Wang", "authors": "Persi Diaconis, Guanyang Wang", "title": "Discussion of `A Gibbs sampler for a class of random convex polytopes'", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This is a contribution for the discussion on \"A Gibbs sampler for a class of\nrandom convex polytopes\" by Pierre E. Jacob, Ruobin Gong, Paul T. Edlefsen and\nArthur P. Dempster to appear in the Journal of American Statistical\nAssociation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:09:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Diaconis", "Persi", ""], ["Wang", "Guanyang", ""]]}, {"id": "2104.07359", "submitter": "Takuo Matsubara", "authors": "Takuo Matsubara, Jeremias Knoblauch, Fran\\c{c}ois-Xavier Briol, Chris.\n  J. Oates", "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised Bayesian inference updates prior beliefs using a loss function,\nrather than a likelihood, and can therefore be used to confer robustness\nagainst possible misspecification of the likelihood. Here we consider\ngeneralised Bayesian inference with a Stein discrepancy as a loss function,\nmotivated by applications in which the likelihood contains an intractable\nnormalisation constant. In this context, the Stein discrepancy circumvents\nevaluation of the normalisation constant and produces generalised posteriors\nthat are either closed form or accessible using standard Markov chain Monte\nCarlo. On a theoretical level, we show consistency, asymptotic normality, and\nbias-robustness of the generalised posterior, highlighting how these properties\nare impacted by the choice of Stein discrepancy. Then, we provide numerical\nexperiments on a range of intractable distributions, including applications to\nkernel-based exponential family models and non-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:31:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Matsubara", "Takuo", ""], ["Knoblauch", "Jeremias", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.07386", "submitter": "Michael Von Maltitz", "authors": "A. J. van der Merwe, M. J. von Maltitz, J. H. Meyer", "title": "Reference and Probability-Matching Priors for the Parameters of a\n  Univariate Student $t$-Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper reference and probability-matching priors are derived for the\nunivariate Student $t$-distribution. These priors generally lead to procedures\nwith properties frequentists can relate to while still retaining Bayes\nvalidity. The priors are tested by performing simulation studies. The focus is\non the relative mean squared error from the posterior median ($MSE(\\nu)/\\nu$)\nand on the frequentist coverage of the 95\\% credibility intervals for a sample\nsize of $n=30$. Average interval lengths of the credibility intervals as well\nas the modes of the interval lengths based on 2000 simulations are also\nconsidered. The performance of the priors are also tested on real data, namely\ndaily logarithmic returns of IBM stocks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:31:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["van der Merwe", "A. J.", ""], ["von Maltitz", "M. J.", ""], ["Meyer", "J. H.", ""]]}, {"id": "2104.07462", "submitter": "Felix Newberry", "authors": "Felix Newberry, Jerrad Hampton, Kenneth Jansen, Alireza Doostan", "title": "Bi-fidelity Reduced Polynomial Chaos Expansion for Uncertainty\n  Quantification", "comments": "22 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A ubiquitous challenge in design space exploration or uncertainty\nquantification of complex engineering problems is the minimization of\ncomputational cost. A useful tool to ease the burden of solving such systems is\nmodel reduction. This work considers a stochastic model reduction method (SMR),\nin the context of polynomial chaos (PC) expansions, where low-fidelity (LF)\nsamples are leveraged to form a stochastic reduced basis. The reduced basis\nenables the construction of a bi-fidelity (BF) estimate of a quantity of\ninterest from a small number of high-fidelity (HF) samples. A successful BF\nestimate approximates the quantity of interest with accuracy comparable to the\nHF model and computational expense close to the LF model. We develop new error\nbounds for the SMR approach and present a procedure to practically utilize\nthese bounds in order to assess the appropriateness of a given pair of LF and\nHF models for BF estimation. The effectiveness of the SMR approach, and the\nutility of the error bound are presented in three numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:48:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Newberry", "Felix", ""], ["Hampton", "Jerrad", ""], ["Jansen", "Kenneth", ""], ["Doostan", "Alireza", ""]]}, {"id": "2104.07537", "submitter": "Augusto Fasano", "authors": "Augusto Fasano and Giovanni Rebaudo", "title": "Variational Inference for the Smoothing Distribution in Dynamic Probit\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Fasano, Rebaudo, Durante and Petrone (2019) provided closed-form\nexpressions for the filtering, predictive and smoothing distributions of\nmultivariate dynamic probit models, leveraging on unified skew-normal\ndistribution properties. This allows to develop algorithms to draw independent\nand identically distributed samples from such distributions, as well as\nsequential Monte Carlo procedures for the filtering and predictive\ndistributions, allowing to overcome computational bottlenecks that may arise\nfor large sample sizes. In this paper, we briefly review the above-mentioned\nclosed-form expressions, mainly focusing on the smoothing distribution of the\nunivariate dynamic probit. We develop a variational Bayes approach, extending\nthe partially factorized mean-field variational approximation introduced by\nFasano, Durante and Zanella (2019) for the static binary probit model to the\ndynamic setting. Results are shown for a financial application.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:46:00 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 13:44:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Fasano", "Augusto", ""], ["Rebaudo", "Giovanni", ""]]}, {"id": "2104.07694", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura, Zhenyu Zhang, and Marc A. Suchard", "title": "Hamiltonian zigzag sampler got more momentum than its Markovian\n  counterpart: Equivalence of two zigzags under a momentum refreshment limit", "comments": "29 pages, 7 figures (+ 8 pages of Supplement); Data available at\n  http://doi.org/10.5281/zenodo.4679720", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zigzag and other piecewise deterministic Markov process samplers have\nattracted significant interest for their non-reversibility and other appealing\nproperties for Bayesian posterior computation. Hamiltonian Monte Carlo is\nanother state-of-the-art sampler, exploiting fictitious momentum to guide\nMarkov chains through complex target distributions. In this article, we uncover\na remarkable connection between the zigzag sampler and a variant of Hamiltonian\nMonte Carlo exploiting Laplace-distributed momentum. The position and velocity\ncomponent of the corresponding Hamiltonian dynamics travels along a zigzag path\nparalleling the Markovian zigzag process; however, the dynamics is\nnon-Markovian as the momentum component encodes non-immediate pasts. This\ninformation is partially lost during a momentum refreshment step, in which we\npreserve its direction but re-sample magnitude. In the limit of increasingly\nfrequent momentum refreshments, we prove that this Hamiltonian zigzag converges\nto its Markovian counterpart. This theoretical insight suggests that, by\nretaining full momentum information, Hamiltonian zigzag can better explore\ntarget distributions with highly correlated parameters. We corroborate this\nintuition by comparing performance of the two zigzag cousins on\nhigh-dimensional truncated multivariate Gaussians, including a\n11,235-dimensional target arising from a Bayesian phylogenetic multivariate\nprobit model applied to HIV virus data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:05:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Zhang", "Zhenyu", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2104.08198", "submitter": "Kari Heine", "authors": "Kari Heine, Daniel Burrows", "title": "Multilevel Bootstrap Particle Filter", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider situations where the applicability of sequential Monte Carlo\nparticle filters is compromised due to the expensive evaluation of the particle\nweights. To alleviate this problem, we propose a new particle filter algorithm\nbased on the multilevel approach. We show that the resulting multilevel\nbootstrap particle filter (MLBPF) retains the strong law of large numbers as\nwell as the central limit theorem of classical particle filters under mild\nconditions. Our numerical experiments demonstrate up to 85\\% reduction in\ncomputation time compared to the classical bootstrap particle filter, in\ncertain settings. While it should be acknowledged that this reduction is highly\napplication dependent, and a similar gain should not be expected for all\napplications across the board, we believe that this substantial improvement in\ncertain settings makes MLBPF an important addition to the family of sequential\nMonte Carlo methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:13:25 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Heine", "Kari", ""], ["Burrows", "Daniel", ""]]}, {"id": "2104.09863", "submitter": "Ivan Jericevich", "authors": "Ivan Jericevich and Murray McKechnie and Tim Gebbie", "title": "Calibrating an adaptive Farmer-Joshi agent-based model for financial\n  markets", "comments": "9 pages, 13 figures", "journal-ref": null, "doi": "10.25375/uct.11636922.v1", "report-no": null, "categories": "q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We replicate the contested calibration of the Farmer and Joshi agent based\nmodel of financial markets using a genetic algorithm and a Nelder-Mead with\nthreshold accepting algorithm following Fabretti. The novelty of the\nFarmer-Joshi model is that the dynamics are driven by trade entry and exit\nthresholds alone. We recover the known claim that some important stylized facts\nobserved in financial markets cannot be easily found under calibration -- in\nparticular those relating to the auto-correlations in the absolute values of\nthe price fluctuations, and sufficient kurtosis. However, rather than concerns\nrelating to the calibration method, what is novel here is that we extended the\nFarmer-Joshi model to include agent adaptation using an Brock and Hommes\napproach to strategy fitness based on trading strategy profitability. We call\nthis an adaptive Farmer-Joshi model: the model allows trading agents to switch\nbetween strategies by favouring strategies that have been more profitable over\nsome period of time determined by a free-parameter fixing the profit monitoring\ntime-horizon. In the adaptive model we are able to calibrate and recover\nadditional stylized facts, despite apparent degeneracy's. This is achieved by\ncombining the interactions of trade entry levels with trade strategy switching.\nWe use this to argue that for low-frequency trading across days, as calibrated\nto daily sampled data, feed-backs can be accounted for by strategy die-out\nbased on intermediate term profitability; we find that the average trade\nmonitoring horizon is approximately two to three months (or 40 to 60 days) of\ntrading.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:53:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Jericevich", "Ivan", ""], ["McKechnie", "Murray", ""], ["Gebbie", "Tim", ""]]}, {"id": "2104.10009", "submitter": "Efstathia Bura", "authors": "Daniel Kapla, Lukas Fertl, Efstathia Bura", "title": "Fusing Sufficient Dimension Reduction with Neural Networks", "comments": "19 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the regression problem where the dependence of the response Y on\na set of predictors X is fully captured by the regression function E(Y |\nX)=g(B'X), for an unknown function g and low rank parameter B matrix. We\ncombine neural networks with sufficient dimension reduction in order to remove\nthe limitation of small p and n of the latter. We show in simulations that the\nproposed estimator is on par with competing sufficient dimension reduction\nmethods in small p and n settings, such as minimum average variance estimation\nand conditional variance estimation. Among those, it is the only\ncomputationally applicable in large p and n problems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:40:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kapla", "Daniel", ""], ["Fertl", "Lukas", ""], ["Bura", "Efstathia", ""]]}, {"id": "2104.10041", "submitter": "Elvis Cui", "authors": "Elvis Cui, Dongyuan Song, Weng Kee Wong", "title": "Particle swarm optimization in constrained maximum likelihood estimation\n  a case study", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of paper is to apply two types of particle swarm optimization, global\nbest andlocal best PSO to a constrained maximum likelihood estimation problem\nin pseudotime anal-ysis, a sub-field in bioinformatics. The results have shown\nthat particle swarm optimizationis extremely useful and efficient when the\noptimization problem is non-differentiable and non-convex so that analytical\nsolution can not be derived and gradient-based methods can not beapplied.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 07:32:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cui", "Elvis", ""], ["Song", "Dongyuan", ""], ["Wong", "Weng Kee", ""]]}, {"id": "2104.10103", "submitter": "Wanli Qiao", "authors": "Wanli Qiao and Amarda Shehu", "title": "Space Partitioning and Regression Mode Seeking via a Mean-Shift-Inspired\n  Algorithm", "comments": "44 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean shift (MS) algorithm is a nonparametric method used to cluster\nsample points and find the local modes of kernel density estimates, using an\nidea based on iterative gradient ascent. In this paper we develop a\nmean-shift-inspired algorithm to estimate the modes of regression functions and\npartition the sample points in the input space. We prove convergence of the\nsequences generated by the algorithm and derive the non-asymptotic rates of\nconvergence of the estimated local modes for the underlying regression model.\nWe also demonstrate the utility of the algorithm for data-enabled discovery\nthrough an application on biomolecular structure data. An extension to subspace\nconstrained mean shift (SCMS) algorithm used to extract ridges of regression\nfunctions is briefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:35:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Qiao", "Wanli", ""], ["Shehu", "Amarda", ""]]}, {"id": "2104.10150", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Bayesian subset selection and variable importance for interpretable\n  prediction and classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subset selection is a valuable tool for interpretable learning, scientific\ndiscovery, and data compression. However, classical subset selection is often\neschewed due to selection instability, computational bottlenecks, and lack of\npost-selection inference. We address these challenges from a Bayesian\nperspective. Given any Bayesian predictive model $\\mathcal{M}$, we elicit\npredictively-competitive subsets using linear decision analysis. The approach\nis customizable for (local) prediction or classification and provides\ninterpretable summaries of $\\mathcal{M}$. A key quantity is the acceptable\nfamily of subsets, which leverages the predictive distribution from\n$\\mathcal{M}$ to identify subsets that offer nearly-optimal prediction. The\nacceptable family spawns new (co-) variable importance metrics based on whether\nvariables (co-) appear in all, some, or no acceptable subsets. Crucially, the\nlinear coefficients for any subset inherit regularization and predictive\nuncertainty quantification via $\\mathcal{M}$. The proposed approach exhibits\nexcellent prediction, interval estimation, and variable selection for simulated\ndata, including $p=400 > n$. These tools are applied to a large education\ndataset with highly correlated covariates, where the acceptable family is\nespecially useful. Our analysis provides unique insights into the combination\nof environmental, socioeconomic, and demographic factors that predict\neducational outcomes, and features highly competitive prediction with\nremarkable stability.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:48:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2104.10544", "submitter": "James Townsend", "authors": "James Townsend", "title": "Lossless Compression with Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We develop a simple and elegant method for lossless compression using latent\nvariable models, which we call 'bits back with asymmetric numeral systems'\n(BB-ANS). The method involves interleaving encode and decode steps, and\nachieves an optimal rate when compressing batches of data. We demonstrate it\nfirstly on the MNIST test set, showing that state-of-the-art lossless\ncompression is possible using a small variational autoencoder (VAE) model. We\nthen make use of a novel empirical insight, that fully convolutional generative\nmodels, trained on small images, are able to generalize to images of arbitrary\nsize, and extend BB-ANS to hierarchical latent variable models, enabling\nstate-of-the-art lossless compression of full-size colour images from the\nImageNet dataset. We describe 'Craystack', a modular software framework which\nwe have developed for rapid prototyping of compression using deep generative\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:03:05 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:28:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Townsend", "James", ""]]}, {"id": "2104.10906", "submitter": "Francisco Javier Rubio", "authors": "Danilo Alvares and Francisco Javier Rubio", "title": "A tractable Bayesian joint model for longitudinal and survival data", "comments": "To appear in Statistics in Medicine. Software available at\n  https://github.com/daniloalvares/Tractable-BJM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a numerically tractable formulation of Bayesian joint models for\nlongitudinal and survival data. The longitudinal process is modelled using\ngeneralised linear mixed models, while the survival process is modelled using a\nparametric general hazard structure. The two processes are linked by sharing\nfixed and random effects, separating the effects that play a role at the time\nscale from those that affect the hazard scale. This strategy allows for the\ninclusion of non-linear and time-dependent effects while avoiding the need for\nnumerical integration, which facilitates the implementation of the proposed\njoint model. We explore the use of flexible parametric distributions for\nmodelling the baseline hazard function which can capture the basic shapes of\ninterest in practice. We discuss prior elicitation based on the interpretation\nof the parameters. We present an extensive simulation study, where we analyse\nthe inferential properties of the proposed models, and illustrate the trade-off\nbetween flexibility, sample size, and censoring. We also apply our proposal to\ntwo real data applications in order to demonstrate the adaptability of our\nformulation both in univariate time-to-event data and in a competing risks\nframework. The methodology is implemented in rstan.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:36:40 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Alvares", "Danilo", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "2104.10975", "submitter": "Motonori Oka", "authors": "Motonori Oka, Kensuke Okada", "title": "Assessing the Performance of Diagnostic Classification Models in Small\n  Sample Contexts with Different Estimation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fueled by the call for formative assessments, diagnostic classification\nmodels (DCMs) have recently gained popularity in psychometrics. Despite their\npotential for providing diagnostic information that aids in classroom\ninstruction and students' learning, empirical applications of DCMs to classroom\nassessments have been highly limited. This is partly because how DCMs with\ndifferent estimation methods perform in small sample contexts is not yet\nwell-explored. Hence, this study aims to investigate the performance of\nrespondent classification and item parameter estimation with a comprehensive\nsimulation design that resembles classroom assessments using different\nestimation methods. The key findings are the following: (1) although the marked\ndifference in respondent classification accuracy was not observed among the\nmaximum likelihood (ML), Bayesian, and nonparametric methods, the Bayesian\nmethod provided slightly more accurate respondent classification in\nparsimonious DCMs than the ML method, and in complex DCMs, the ML method\nyielded the slightly better result than the Bayesian method; (2) while item\nparameter recovery was poor in both Bayesian and ML methods, the Bayesian\nmethod exhibited unstable slip values owing to the multimodality of their\nposteriors under complex DCMs, and the ML method produced irregular estimates\nthat appear to be well-estimated due to a boundary problem under parsimonious\nDCMs.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:19:40 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Oka", "Motonori", ""], ["Okada", "Kensuke", ""]]}, {"id": "2104.11594", "submitter": "Mohsen Rezapour", "authors": "Bahareh Afhami, Mohsen Rezapour, Mohsen Madadi, Vahed Maroufy", "title": "Dynamic investment portfolio optimization using a Multivariate Merton\n  Model with Correlated Jump Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are concerned with the optimization of a dynamic investment\nportfolio when the securities which follow a multivariate Merton model with\ndependent jumps are periodically invested and proceed by approximating the\nCondition-Value-at-Risk (CVaR) by comonotonic bounds and maximize the expected\nterminal wealth. Numerical studies as well as applications of our results to\nreal datasets are also provided.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:08:31 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Afhami", "Bahareh", ""], ["Rezapour", "Mohsen", ""], ["Madadi", "Mohsen", ""], ["Maroufy", "Vahed", ""]]}, {"id": "2104.11708", "submitter": "Sy Han Chiou", "authors": "Sy Han Chiou, Gongjun Xu, Jun Yan, Chiung-Yu Huang", "title": "Regression Modeling for Recurrent Events Using R Package reReg", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent event analyses have found a wide range of applications in\nbiomedicine, public health, and engineering, among others, where study subjects\nmay experience a sequence of event of interest during follow-up. The R package\nreReg (Chiou and Huang 2021) offers a comprehensive collection of practical and\neasy-to-use tools for regression analysis of recurrent events, possibly with\nthe presence of an informative terminal event. The regression framework is a\ngeneral scale-change model which encompasses the popular Cox-type model, the\naccelerated rate model, and the accelerated mean model as special cases.\nInformative censoring is accommodated through a subject-specific frailty\nwithout no need for parametric specification. Different regression models are\nallowed for the recurrent event process and the terminal event. Also included\nare visualization and simulation tools.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:53:49 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Chiou", "Sy Han", ""], ["Xu", "Gongjun", ""], ["Yan", "Jun", ""], ["Huang", "Chiung-Yu", ""]]}, {"id": "2104.11870", "submitter": "Guang Zhang", "authors": "Li Chen and Guang Zhang", "title": "Hermite Polynomial-based Valuation of American Options with General\n  Jump-Diffusion Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP econ.EM q-fin.MF q-fin.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approximation scheme for the price and exercise policy of\nAmerican options. The scheme is based on Hermite polynomial expansions of the\ntransition density of the underlying asset dynamics and the early exercise\npremium representation of the American option price. The advantages of the\nproposed approach are threefold. First, our approach does not require the\ntransition density and characteristic functions of the underlying asset\ndynamics to be attainable in closed form. Second, our approach is fast and\naccurate, while the prices and exercise policy can be jointly produced. Third,\nour approach has a wide range of applications. We show that the proposed\napproximations of the price and optimal exercise boundary converge to the true\nones. We also provide a numerical method based on a step function to implement\nour proposed approach. Applications to nonlinear mean-reverting models, double\nmean-reverting models, Merton's and Kou's jump-diffusion models are presented\nand discussed.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 03:16:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Li", ""], ["Zhang", "Guang", ""]]}, {"id": "2104.11963", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, V. Roshan Joseph, Douglas M. Ray", "title": "Constrained Minimum Energy Designs", "comments": "Submitted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-filling designs are important in computer experiments, which are\ncritical for building a cheap surrogate model that adequately approximates an\nexpensive computer code. Many design construction techniques in the existing\nliterature are only applicable for rectangular bounded space, but in real world\napplications, the input space can often be non-rectangular because of\nconstraints on the input variables. One solution to generate designs in a\nconstrained space is to first generate uniformly distributed samples in the\nfeasible region, and then use them as the candidate set to construct the\ndesigns. Sequentially Constrained Monte Carlo (SCMC) is the state-of-the-art\ntechnique for candidate generation, but it still requires large number of\nconstraint evaluations, which is problematic especially when the constraints\nare expensive to evaluate. Thus, to reduce constraint evaluations and improve\nefficiency, we propose the Constrained Minimum Energy Design (CoMinED) that\nutilizes recent advances in deterministic sampling methods. Extensive\nsimulation results on 15 benchmark problems with dimensions ranging from 2 to\n13 are provided for demonstrating the improved performance of CoMinED over the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 14:28:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Chaofan", ""], ["Joseph", "V. Roshan", ""], ["Ray", "Douglas M.", ""]]}, {"id": "2104.12208", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e and Angelos Vouldis", "title": "Robust selection of predictors and conditional outlier detection in a\n  perturbed large-dimensional regression context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a fast methodology, called ROBOUT, to identify outliers\nin a response variable conditional on a set of linearly related predictors,\nretrieved from a large granular dataset. ROBOUT is shown to be effective and\nparticularly versatile compared to existing methods in the presence of a number\nof data idiosyncratic features. ROBOUT is able to identify observations with\noutlying conditional variance when the dataset contains element-wise sparse\nvariables, and the set of predictors contains multivariate outliers. Existing\nintegrated methodologies like SPARSE-LTS and RLARS are systematically\nsub-optimal under those conditions. ROBOUT entails a robust selection stage of\nthe statistically relevant predictors (by using a Huber or a quantile loss),\nthe estimation of a robust regression model based on the selected predictors\n(by LTS, GS or MM), and a criterion to identify conditional outliers based on a\nrobust measure of the residuals' dispersion. We conduct a comprehensive\nsimulation study in which the different variants of the proposed algorithm are\ntested under an exhaustive set of different perturbation scenarios. The\nmethodology is also applied to a granular supervisory banking dataset collected\nby the European Central Bank.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 17:08:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Vouldis", "Angelos", ""]]}, {"id": "2104.12587", "submitter": "Chris Oates", "authors": "Junyang Wang, Jon Cockayne, Oksana Chkrebtii, T. J. Sullivan, Chris.\n  J. Oates", "title": "Bayesian Numerical Methods for Nonlinear Partial Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The numerical solution of differential equations can be formulated as an\ninference problem to which formal statistical approaches can be applied.\nHowever, nonlinear partial differential equations (PDEs) pose substantial\nchallenges from an inferential perspective, most notably the absence of\nexplicit conditioning formula. This paper extends earlier work on linear PDEs\nto a general class of initial value problems specified by nonlinear PDEs,\nmotivated by problems for which evaluations of the right-hand-side, initial\nconditions, or boundary conditions of the PDE have a high computational cost.\nThe proposed method can be viewed as exact Bayesian inference under an\napproximate likelihood, which is based on discretisation of the nonlinear\ndifferential operator. Proof-of-concept experimental results demonstrate that\nmeaningful probabilistic uncertainty quantification for the unknown solution of\nthe PDE can be performed, while controlling the number of times the\nright-hand-side, initial and boundary conditions are evaluated. A suitable\nprior model for the solution of the PDE is identified using novel theoretical\nanalysis of the sample path properties of Mat\\'{e}rn processes, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:02:10 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:26:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Chkrebtii", "Oksana", ""], ["Sullivan", "T. J.", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.12588", "submitter": "Dursun Bulutoglu A", "authors": "Dursun A. Bulutoglu, Kenneth J. Ryan", "title": "Algorithms for finding generalized minimum aberration designs", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1501.02281", "journal-ref": "Journal of Complexity 31 (4) (2015) 577-589", "doi": "10.1016/j.jco.2014.12.001", "report-no": null, "categories": "stat.CO math.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical design of experiments is widely used in scientific and industrial\ninvestigations. A generalized minimum aberration (GMA) orthogonal array is\noptimum under the well-established, so-called GMA criterion, and such an array\ncan extract as much information as possible at a fixed cost. Finding GMA arrays\nis an open (yet fundamental) problem in design of experiments because\nconstructing such arrays becomes intractable as the number of runs and factors\nincrease. We develop two directed enumeration algorithms that call the integer\nprogramming with isomorphism pruning algorithm of Margot (2007) for the purpose\nof finding GMA arrays. Our results include 16 GMA arrays that were not\npreviously in the literature, along with documentation of the efficiencies that\nmade the required calculations possible within a reasonable budget of computer\ntime. We also validate heuristic algorithms against a GMA array catalog, by\nshowing that they quickly output near GMA arrays, and then use the heuristics\nto find near GMA arrays when enumeration is computationally burdensome.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:42:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bulutoglu", "Dursun A.", ""], ["Ryan", "Kenneth J.", ""]]}, {"id": "2104.12597", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Valid Heteroskedasticity Robust Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tests based on heteroskedasticity robust standard errors are an important\ntechnique in econometric practice. Choosing the right critical value, however,\nis not all that simple: Conventional critical values based on asymptotics often\nlead to severe size distortions; and so do existing adjustments including the\nbootstrap. To avoid these issues, we suggest to use smallest size-controlling\ncritical values, the generic existence of which we prove in this article.\nFurthermore, sufficient and often also necessary conditions for their existence\nare given that are easy to check. Granted their existence, these critical\nvalues are the canonical choice: larger critical values result in unnecessary\npower loss, whereas smaller critical values lead to over-rejections under the\nnull hypothesis, make spurious discoveries more likely, and thus are invalid.\nWe suggest algorithms to numerically determine the proposed critical values and\nprovide implementations in accompanying software. Finally, we numerically study\nthe behavior of the proposed testing procedures, including their power\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:01:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2104.12657", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski, Jens Kley-Holsteg, Florian Ziel", "title": "tsrobprep -- an R package for robust preprocessing of time series data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.MS stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Data cleaning is a crucial part of every data analysis exercise. Yet, the\ncurrently available R packages do not provide fast and robust methods for\ncleaning and preparation of time series data. The open source package tsrobprep\nintroduces efficient methods for handling missing values and outliers using\nmodel based approaches. For data imputation a probabilistic replacement model\nis proposed, which may consist of autoregressive components and external\ninputs. For outlier detection a clustering algorithm based on finite mixture\nmodelling is introduced, which considers typical time series related properties\nas features. By assigning to each observation a probability of being an\noutlying data point, the degree of outlyingness can be determined. The methods\nwork robust and are fully tunable. Moreover, by providing the\nauto_data_cleaning function the data preprocessing can be carried out in one\ncast, without manual tuning and providing suitable results. The primary\nmotivation of the package is the preprocessing of energy system data, however,\nthe package is also suited for other moderate and large sized time series data\nset. We present application for electricity load, wind and solar power data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:35:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Kley-Holsteg", "Jens", ""], ["Ziel", "Florian", ""]]}, {"id": "2104.13026", "submitter": "Johan Larsson", "authors": "Johan Larsson, Jonas Wallin", "title": "The Hessian Screening Rule", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictor screening rules, which discard predictors from the design matrix\nbefore fitting a model, have had sizable impacts on the speed with which\n$\\ell_1$-regularized regression problems, such as the lasso, can be solved.\nCurrent state-of-the-art screening rules, however, have difficulties in dealing\nwith highly-correlated predictors, often becoming too conservative. In this\npaper, we present a new screening rule to deal with this issue: the Hessian\nScreening Rule. The rule uses second-order information from the model in order\nto provide more accurate screening as well as higher-quality warm starts. In\nour experiments on $\\ell_1$-regularized least-squares (the lasso) and logistic\nregression, we show that the rule outperforms all other alternatives in\nsimulated experiments with high correlation, as well as in the majority of real\ndatasets that we study.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 07:55:29 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Larsson", "Johan", ""], ["Wallin", "Jonas", ""]]}, {"id": "2104.13520", "submitter": "Erniel Barrios", "authors": "Paolo Victor T. Redondo, Joseph Ryan G. Lansangan and Erniel B.\n  Barrios", "title": "Estimation of Poisson Autoregressive Model for Multiple Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A Poisson autoregressive (PAR) model accounting for discreteness and\nautocorrelation of count time series data is typically estimated in the\nstate-space modelling framework through extended Kalman filter. However,\nbecause of the complex dependencies in count time series, estimation becomes\nmore challenging. PAR is viewed as an additive model and estimated using a\nhybrid of cubic smoothing splines and maximum likelihood estimation (MLE) in\nthe backfitting framework. Simulation studies show that this estimation method\nis comparable or better than PAR estimated in the state-space context,\nespecially with larger count values. However, as [2] formulated PAR for\nstationary counts, both estimation procedures underestimate parameters in\nnearly nonstationary models. The flexibility of the additive model has two\nbenefits though: robust estimation in the presence of temporary structural\nchange, and; viability to integrate PAR model into a more complex model\nstructure. We further generalized the PAR(p) model into multiple time series of\ncounts and illustrated with indicators in the financial markets.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:27:52 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Redondo", "Paolo Victor T.", ""], ["Lansangan", "Joseph Ryan G.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2104.14008", "submitter": "Zhi Zhao", "authors": "Zhi Zhao, Marco Banterle, Leonardo Bottolo, Sylvia Richardson, Alex\n  Lewin, Manuela Zucknick", "title": "BayesSUR: An R package for high-dimensional multivariate Bayesian\n  variable and covariance selection in linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In molecular biology, advances in high-throughput technologies have made it\npossible to study complex multivariate phenotypes and their simultaneous\nassociations with high-dimensional genomic and other omics data, a problem that\ncan be studied with high-dimensional multi-response regression, where the\nresponse variables are potentially highly correlated. To this purpose, we\nrecently introduced several multivariate Bayesian variable and covariance\nselection models, e.g., Bayesian estimation methods for sparse seemingly\nunrelated regression for variable and covariance selection. Several variable\nselection priors have been implemented in this context, in particular the\nhotspot detection prior for latent variable inclusion indicators, which results\nin sparse variable selection for associations between predictors and multiple\nphenotypes. We also propose an alternative, which uses a Markov random field\n(MRF) prior for incorporating prior knowledge about the dependence structure of\nthe inclusion indicators. Inference of Bayesian seemingly unrelated regression\n(SUR) by Markov chain Monte Carlo methods is made computationally feasible by\nfactorisation of the covariance matrix amongst the response variables. In this\npaper we present BayesSUR, an R package, which allows the user to easily\nspecify and run a range of different Bayesian SUR models, which have been\nimplemented in C++ for computational efficiency. The R package allows the\nspecification of the models in a modular way, where the user chooses the priors\nfor variable selection and for covariance selection separately. We demonstrate\nthe performance of sparse SUR models with the hotspot prior and spike-and-slab\nMRF prior on synthetic and real data sets representing eQTL or mQTL studies and\nin vitro anti-cancer drug screening studies as examples for typical\napplications.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:29:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zhao", "Zhi", ""], ["Banterle", "Marco", ""], ["Bottolo", "Leonardo", ""], ["Richardson", "Sylvia", ""], ["Lewin", "Alex", ""], ["Zucknick", "Manuela", ""]]}, {"id": "2104.14581", "submitter": "Amanda Muyskens", "authors": "Amanda Muyskens, Benjamin Priest, Im\\`ene Goumiri, and Michael\n  Schneider", "title": "MuyGPs: Scalable Gaussian Process Hyperparameter Estimation Using Local\n  Cross-Validation", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are non-linear probabilistic models popular in many\napplications. However, na\\\"ive GP realizations require quadratic memory to\nstore the covariance matrix and cubic computation to perform inference or\nevaluate the likelihood function. These bottlenecks have driven much investment\nin the development of approximate GP alternatives that scale to the large data\nsizes common in modern data-driven applications. We present in this manuscript\nMuyGPs, a novel efficient GP hyperparameter estimation method. MuyGPs builds\nupon prior methods that take advantage of the nearest neighbors structure of\nthe data, and uses leave-one-out cross-validation to optimize covariance\n(kernel) hyperparameters without realizing a possibly expensive likelihood. We\ndescribe our model and methods in detail, and compare our implementations\nagainst the state-of-the-art competitors in a benchmark spatial statistics\nproblem. We show that our method outperforms all known competitors both in\nterms of time-to-solution and the root mean squared error of the predictions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:10:21 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Muyskens", "Amanda", ""], ["Priest", "Benjamin", ""], ["Goumiri", "Im\u00e8ne", ""], ["Schneider", "Michael", ""]]}, {"id": "2104.14957", "submitter": "Lena Sembach", "authors": "Lena Sembach, Jan Pablo Burgard, Volker H. Schulz", "title": "A Riemannian Newton Trust-Region Method for Fitting Gaussian Mixture\n  Models", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Mixture Models are a powerful tool in Data Science and Statistics\nthat are mainly used for clustering and density approximation. The task of\nestimating the model parameters is in practice often solved by the Expectation\nMaximization (EM) algorithm which has its benefits in its simplicity and low\nper-iteration costs. However, the EM converges slowly if there is a large share\nof hidden information or overlapping clusters. Recent advances in Manifold\nOptimization for Gaussian Mixture Models have gained increasing interest. We\nintroduce a formula for the Riemannian Hessian for Gaussian Mixture Models. On\ntop, we propose a new Riemannian Newton Trust-Region method which outperforms\ncurrent approaches both in terms of runtime and number of iterations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:48:32 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sembach", "Lena", ""], ["Burgard", "Jan Pablo", ""], ["Schulz", "Volker H.", ""]]}, {"id": "2104.15090", "submitter": "Sebastian H\\\"onel", "authors": "Sebastian H\\\"onel", "title": "Technical Reports Compilation: Detecting the Fire Drill anti-pattern\n  using Source Code and issue-tracking data", "comments": "208 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.35805.33766/2", "report-no": null, "categories": "cs.SE stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Detecting the presence of project management anti-patterns (AP) currently\nrequires experts on the matter and is an expensive endeavor. Worse, experts may\nintroduce their individual subjectivity or bias. Using the Fire Drill AP, we\nfirst introduce a novel way to translate descriptions into detectable AP that\nare comprised of arbitrary metrics and events such as logged time or\nmaintenance activities, which are mined from the underlying source code or\nissue-tracking data, thus making the description objective as it becomes\ndata-based. Secondly, we demonstrate a novel method to quantify and score the\ndeviations of real-world projects to data-based AP descriptions. Using nine\nreal-world projects that exhibit a Fire Drill to some degree, we show how to\nfurther enhance the translated AP. The ground truth in these projects was\nextracted from two individual experts and consensus was found between them. Our\nevaluation spans three kinds of pattern, where the first is purely derived from\ndescription, the second type is enhanced by data, and the third kind is derived\nfrom data only. The Fire Drill AP as translated from description only for\neither, source code- or issue-tracking-based detection, shows weak potential of\nconfidently detecting the presence of the anti-pattern in a project. Enriching\nthe AP with data from real-world projects significantly improves detection.\nUsing patterns derived from data only leads to almost perfect correlations of\nthe scores with the ground truth. Some APs share symptoms with the Fire Drill\nAP, and we conclude that the presence of similar patterns is most certainly\ndetectable. Furthermore, any pattern that can be characteristically modeled\nusing the proposed approach is potentially well detectable.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:16:32 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:52:43 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 12:25:41 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 16:22:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["H\u00f6nel", "Sebastian", ""]]}]