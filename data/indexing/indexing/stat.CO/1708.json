[{"id": "1708.00145", "submitter": "Rohit Patra", "authors": "Arun K. Kuchibhotla, Rohit K. Patra, and Bodhisattva Sen", "title": "Semiparametric Efficiency in Convexity Constrained Single Index Model", "comments": "Removed the density bounded away from zero assumption in assumption\n  (A5). Weakened assumption (B2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation and inference in a single index regression model with\nan unknown convex link function. We introduce a convex and Lipschitz\nconstrained least squares estimator (CLSE) for both the parametric and the\nnonparametric components given independent and identically distributed\nobservations. We prove the consistency and find the rates of convergence of the\nCLSE when the errors are assumed to have only $q \\ge 2$ moments and are allowed\nto depend on the covariates. When $q\\ge 5$, we establish $n^{-1/2}$-rate of\nconvergence and asymptotic normality of the estimator of the parametric\ncomponent. Moreover, the CLSE is proved to be semiparametrically efficient if\nthe errors happen to be homoscedastic. {We develop and implement a numerically\nstable and computationally fast algorithm to compute our proposed estimator in\nthe R package~\\texttt{simest}}. We illustrate our methodology through extensive\nsimulations and data analysis. Finally, our proof of efficiency is geometric\nand provides a general framework that can be used to prove efficiency of\nestimators in a wide variety of semiparametric models even when they do not\nsatisfy the efficient score equation directly.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 03:21:11 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 20:14:16 GMT"}, {"version": "v3", "created": "Fri, 31 Aug 2018 19:15:13 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 19:39:24 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Kuchibhotla", "Arun K.", ""], ["Patra", "Rohit K.", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1708.00257", "submitter": "Teng Zhang", "authors": "Teng Zhang and Yi Yang", "title": "Robust PCA by Manifold Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA is a widely used statistical procedure to recover a underlying\nlow-rank matrix with grossly corrupted observations. This work considers the\nproblem of robust PCA as a nonconvex optimization problem on the manifold of\nlow-rank matrices, and proposes two algorithms (for two versions of\nretractions) based on manifold optimization. It is shown that, with a proper\ndesigned initialization, the proposed algorithms are guaranteed to converge to\nthe underlying low-rank matrix linearly. Compared with a previous work based on\nthe Burer-Monterio decomposition of low-rank matrices, the proposed algorithms\nreduce the dependence on the conditional number of the underlying low-rank\nmatrix theoretically. Simulations and real data examples confirm the\ncompetitive performance of our method.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 11:39:21 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 05:00:54 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 07:00:36 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Zhang", "Teng", ""], ["Yang", "Yi", ""]]}, {"id": "1708.00347", "submitter": "Ben Derrick Mr", "authors": "Ben Derrick, Paul White and Deirdre Toher", "title": "An Inverse Normal Transformation Solution for the comparison of two\n  samples that contain both paired observations and independent observations", "comments": "Inverse Normal Transformations Partially overlapping samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse normal transformations applied to the partially overlapping samples\nt-tests by Derrick et.al. (2017) are considered for their Type I error\nrobustness and power. The inverse normal transformation solutions proposed in\nthis paper are shown to maintain Type I error robustness. For increasing\ndegrees of skewness they also offer improved power relative to the parametric\npartially overlapping samples t-tests. The power when using inverse normal\ntransformation solutions are comparable to rank based non-parametric solutions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:14:10 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Derrick", "Ben", ""], ["White", "Paul", ""], ["Toher", "Deirdre", ""]]}, {"id": "1708.00427", "submitter": "Jing Lei", "authors": "Jing Lei", "title": "Fast Exact Conformalization of Lasso using Piecewise Linear Homotopy", "comments": "24 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction is a general method that converts almost any point\npredictor to a prediction set. The resulting set keeps good statistical\nproperties of the original estimator under standard assumptions, and guarantees\nvalid average coverage even when the model is misspecified. A main challenge in\napplying conformal prediction in modern applications is efficient computation,\nas it generally requires an exhaustive search over the entire output space. In\nthis paper we develop an exact and computationally efficient conformalization\nof the Lasso and elastic net. The method makes use of a novel piecewise linear\nhomotopy of the Lasso solution under perturbation of a single input sample\npoint. As a by-product, we provide a simpler and better justified online Lasso\nalgorithm, which may be of independent interest. Our derivation also reveals an\ninteresting accuracy-stability trade-off in conformal inference, which is\nanalogous to the bias-variance trade-off in traditional parameter estimation.\nThe practical performance of the new algorithm is demonstrated using both\nsynthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 17:29:56 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lei", "Jing", ""]]}, {"id": "1708.00707", "submitter": "Henri Vuollekoski", "authors": "Jarno Lintusaari, Henri Vuollekoski, Antti Kangasr\\\"a\\\"asi\\\"o, Kusti\n  Skyt\\'en, Marko J\\\"arvenp\\\"a\\\"a, Pekka Marttinen, Michael U. Gutmann, Aki\n  Vehtari, Jukka Corander, and Samuel Kaski", "title": "ELFI: Engine for Likelihood-Free Inference", "comments": null, "journal-ref": "Journal of Machine Learning Research, 19(16):1-7, 2018.\n  http://jmlr.org/papers/v19/17-374.html", "doi": null, "report-no": null, "categories": "stat.ML cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engine for Likelihood-Free Inference (ELFI) is a Python software library for\nperforming likelihood-free inference (LFI). ELFI provides a convenient syntax\nfor arranging components in LFI, such as priors, simulators, summaries or\ndistances, to a network called ELFI graph. The components can be implemented in\na wide variety of languages. The stand-alone ELFI graph can be used with any of\nthe available inference methods without modifications. A central method\nimplemented in ELFI is Bayesian Optimization for Likelihood-Free Inference\n(BOLFI), which has recently been shown to accelerate likelihood-free inference\nup to several orders of magnitude by surrogate-modelling the distance. ELFI\nalso has an inbuilt support for output data storing for reuse and analysis, and\nsupports parallelization of computation from multiple cores up to a cluster\nenvironment. ELFI is designed to be extensible and provides interfaces for\nwidening its functionality. This makes the adding of new inference methods to\nELFI straightforward and automatically compatible with the inbuilt features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 11:39:11 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 13:04:57 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 08:34:27 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Lintusaari", "Jarno", ""], ["Vuollekoski", "Henri", ""], ["Kangasr\u00e4\u00e4si\u00f6", "Antti", ""], ["Skyt\u00e9n", "Kusti", ""], ["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Marttinen", "Pekka", ""], ["Gutmann", "Michael U.", ""], ["Vehtari", "Aki", ""], ["Corander", "Jukka", ""], ["Kaski", "Samuel", ""]]}, {"id": "1708.00829", "submitter": "Jun Yang", "authors": "Jun Yang and Jeffrey S. Rosenthal", "title": "Complexity Results for MCMC derived from Quantitative Bounds", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers how to obtain MCMC quantitative convergence bounds which\ncan be translated into tight complexity bounds in high-dimensional settings. We\npropose a modified drift-and-minorization approach, which establishes a\ngeneralized drift condition defined in a subset of the state space. The subset\nis called the ``large set'' and is chosen to rule out some ``bad'' states which\nhave poor drift property when the dimension gets large. Using the ``large set''\ntogether with a ``centered'' drift function, a quantitative bound can be\nobtained which can be translated into a tight complexity bound. As a\ndemonstration, we analyze a certain realistic Gibbs sampler algorithm and\nobtain a complexity upper bound for the mixing time, which shows that the\nnumber of iterations required for the Gibbs sampler to converge is constant\nunder certain conditions on the observed data and the initial state. It is our\nhope that this modified drift-and-minorization approach can be employed in many\nother specific examples to obtain complexity bounds for high-dimensional Markov\nchains.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:06:28 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 03:49:01 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 20:33:54 GMT"}, {"version": "v4", "created": "Mon, 30 Sep 2019 22:09:34 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Yang", "Jun", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1708.00842", "submitter": "Murat Uney Dr", "authors": "Murat Uney, Bernard Mulgrew, Daniel E Clark", "title": "Latent Parameter Estimation in Fusion Networks Using Separable\n  Likelihoods", "comments": "accepted with minor revisions, IEEE Transactions on Signal and\n  Information Processing Over Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.IT cs.MA math.IT stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multi-sensor state space models underpin fusion applications in networks of\nsensors. Estimation of latent parameters in these models has the potential to\nprovide highly desirable capabilities such as network self-calibration.\nConventional solutions to the problem pose difficulties in scaling with the\nnumber of sensors due to the joint multi-sensor filtering involved when\nevaluating the parameter likelihood. In this article, we propose a separable\npseudo-likelihood which is a more accurate approximation compared to a\npreviously proposed alternative under typical operating conditions. In\naddition, we consider using separable likelihoods in the presence of many\nobjects and ambiguity in associating measurements with objects that originated\nthem. To this end, we use a state space model with a hypothesis based\nparameterisation, and, develop an empirical Bayesian perspective in order to\nevaluate separable likelihoods on this model using local filtering. Bayesian\ninference with this likelihood is carried out using belief propagation on the\nassociated pairwise Markov random field. We specify a particle algorithm for\nlatent parameter estimation in a linear Gaussian state space model and\ndemonstrate its efficacy for network self-calibration using measurements from\nnon-cooperative targets in comparison with alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 17:34:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 22:45:52 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Uney", "Murat", ""], ["Mulgrew", "Bernard", ""], ["Clark", "Daniel E", ""]]}, {"id": "1708.00886", "submitter": "Atiye Alaeddini", "authors": "Atiye Alaeddini and Daniel J. Klein", "title": "Application of a Second-order Stochastic Optimization Algorithm for\n  Fitting Stochastic Epidemiological Models", "comments": "Proceedings of the 2017 Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemiological models have tremendous potential to forecast disease burden\nand quantify the impact of interventions. Detailed models are increasingly\npopular, however these models tend to be stochastic and very costly to\nevaluate. Fortunately, readily available high-performance cloud computing now\nmeans that these models can be evaluated many times in parallel. Here, we\nbriefly describe PSPO, an extension to Spall's second-order stochastic\noptimization algorithm, Simultaneous Perturbation Stochastic Approximation\n(SPSA), that takes full advantage of parallel computing environments. The main\nfocus of this work is on the use of PSPO to maximize the pseudo-likelihood of a\nstochastic epidemiological model to data from a 1861 measles outbreak in\nHagelloch, Germany. Results indicate that PSPO far outperforms gradient ascent\nand SPSA on this challenging likelihood maximization problem.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 18:36:06 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Alaeddini", "Atiye", ""], ["Klein", "Daniel J.", ""]]}, {"id": "1708.00955", "submitter": "Matias Quiroz", "authors": "Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc Tran, Mattias\n  Villani", "title": "Hamiltonian Monte Carlo with Energy Conserving Subsampling", "comments": "Includes an experiment on the scalability of the method. Text has\n  been revised too", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) samples efficiently from high-dimensional\nposterior distributions with proposed parameter draws obtained by iterating on\na discretized version of the Hamiltonian dynamics. The iterations make HMC\ncomputationally costly, especially in problems with large datasets, since it is\nnecessary to compute posterior densities and their derivatives with respect to\nthe parameters. Naively computing the Hamiltonian dynamics on a subset of the\ndata causes HMC to lose its key ability to generate distant parameter proposals\nwith high acceptance probability. The key insight in our article is that\nefficient subsampling HMC for the parameters is possible if both the dynamics\nand the acceptance probability are computed from the same data subsample in\neach complete HMC iteration. We show that this is possible to do in a\nprincipled way in a HMC-within-Gibbs framework where the subsample is updated\nusing a pseudo marginal MH step and the parameters are then updated using an\nHMC step, based on the current subsample. We show that our subsampling methods\nare fast and compare favorably to two popular sampling algorithms that utilize\ngradient estimates from data subsampling. We also explore the current\nlimitations of subsampling HMC algorithms by varying the quality of the\nvariance reducing control variates used in the estimators of the posterior\ndensity and its gradients.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 23:19:07 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 01:33:52 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 00:02:16 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Villani", "Mattias", ""]]}, {"id": "1708.01198", "submitter": "Jithin George", "authors": "Jithin Donny George, Ronan Keane and Conor Zellmer", "title": "Estimating speech from lip dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to develop a limited lip reading algorithm for a\nsubset of the English language. We consider a scenario in which no audio\ninformation is available. The raw video is processed and the position of the\nlips in each frame is extracted. We then prepare the lip data for processing\nand classify the lips into visemes and phonemes. Hidden Markov Models are used\nto predict the words the speaker is saying based on the sequences of classified\nphonemes and visemes. The GRID audiovisual sentence corpus [10][11] database is\nused for our study.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 16:23:13 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["George", "Jithin Donny", ""], ["Keane", "Ronan", ""], ["Zellmer", "Conor", ""]]}, {"id": "1708.02079", "submitter": "Christopher T Ryan", "authors": "Xi Chen, Simai He, Bo Jiang, Christopher Thomas Ryan, and Teng Zhang", "title": "The discrete moment problem with nonconvex shape constraints", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrete moment problem is a foundational problem in distribution-free\nrobust optimization, where the goal is to find a worst-case distribution that\nsatisfies a given set of moments. This paper studies the discrete moment\nproblems with additional \"shape constraints\" that guarantee the worst case\ndistribution is either log-concave or has an increasing failure rate. These\nclasses of shape constraints have not previously been studied in the\nliterature, in part due to their inherent nonconvexities. Nonetheless, these\nclasses of distributions are useful in practice. We characterize the structure\nof optimal extreme point distributions by developing new results in reverse\nconvex optimization, a lesser-known tool previously employed in designing\nglobal optimization algorithms. We are able to show, for example, that an\noptimal extreme point solution to a moment problem with $m$ moments and\nlog-concave shape constraints is piecewise geometric with at most $m$ pieces.\nMoreover, this structure allows us to design an exact algorithm for computing\noptimal solutions in a low-dimensional space of parameters. Moreover, We\ndescribe a computational approach to solving these low-dimensional problems,\nincluding numerical results for a representative set of instances.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:47:24 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Chen", "Xi", ""], ["He", "Simai", ""], ["Jiang", "Bo", ""], ["Ryan", "Christopher Thomas", ""], ["Zhang", "Teng", ""]]}, {"id": "1708.02102", "submitter": "Kari Lock Morgan", "authors": "Kari Lock Morgan", "title": "Reallocating and Resampling: A Comparison for Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation-based inference plays a major role in modern statistics, and often\nemploys either reallocating (as in a randomization test) or resampling (as in\nbootstrapping). Reallocating mimics random allocation to treatment groups,\nwhile resampling mimics random sampling from a larger population; does it\nmatter whether the simulation method matches the data collection method?\nMoreover, do the results differ for testing versus estimation? Here we answer\nthese questions in a simple setting by exploring the distribution of a sample\ndifference in means under a basic two group design and four different\nscenarios: true random allocation, true random sampling, reallocating, and\nresampling. For testing a sharp null hypothesis, reallocating is superior in\nsmall samples, but reallocating and resampling are asymptotically equivalent.\nFor estimation, resampling is generally superior, unless the effect is truly\nadditive. Moreover, these results hold regardless of whether the data were\ncollected by random sampling or random allocation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 13:03:43 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Morgan", "Kari Lock", ""]]}, {"id": "1708.02230", "submitter": "Richard Everitt", "authors": "Richard G. Everitt and Paulina A. Rowi\\'nska", "title": "Delayed acceptance ABC-SMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is now an established technique for\nstatistical inference used in cases where the likelihood function is\ncomputationally expensive or not available. It relies on the use of a~model\nthat is specified in the form of a~simulator, and approximates the likelihood\nat a~parameter value $\\theta$ by simulating auxiliary data sets $x$ and\nevaluating the distance of $x$ from the true data $y$. However, ABC is not\ncomputationally feasible in cases where using the simulator for each $\\theta$\nis very expensive. This paper investigates this situation in cases where\na~cheap, but approximate, simulator is available. The approach is to employ\ndelayed acceptance Markov chain Monte Carlo (MCMC) within an ABC sequential\nMonte Carlo (SMC) sampler in order to, in a~first stage of the kernel, use the\ncheap simulator to rule out parts of the parameter space that are not worth\nexploring, so that the ``true'' simulator is only run (in the second stage of\nthe kernel) where there is a~reasonable chance of accepting proposed values of\n$\\theta$. We show that this approach can be used quite automatically, with few\ntuning parameters. Applications to stochastic differential equation models and\nlatent doubly intractable distributions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 17:52:04 GMT"}, {"version": "v2", "created": "Sun, 31 May 2020 14:56:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Everitt", "Richard G.", ""], ["Rowi\u0144ska", "Paulina A.", ""]]}, {"id": "1708.02234", "submitter": "Gael Martin Prof", "authors": "David Harris, Gael M. Martin, Indeewara Perera and D.S. Poskitt", "title": "Construction and Visualization of Optimal Confidence Sets for\n  Frequentist Distributional Forecasts", "comments": "This paper contains animated figures that can be viewed using Adobe\n  Reader/Acrobat. The animations are not supported within a web browser or in\n  non-Acrobat document viewers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is on the quantification of sampling variation in\nfrequentist probabilistic forecasts. We propose a method of constructing\nconfidence sets that respects the functional nature of the forecast\ndistribution, and use animated graphics to visualize the impact of parameter\nuncertainty on the location, dispersion and shape of the distribution. The\nconfidence sets are derived via the inversion of a Wald test and are\nasymptotically uniformly most accurate and, hence, optimal in this sense. A\nwide range of linear and non-linear time series models - encompassing long\nmemory, state space and mixture specifications - is used to demonstrate the\nprocedure, based on artificially generated data. An empirical example in which\ndistributional forecasts of both financial returns and its stochastic\nvolatility are produced is then used to illustrate the practical importance of\naccommodating sampling variation in the manner proposed.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 02:11:43 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Harris", "David", ""], ["Martin", "Gael M.", ""], ["Perera", "Indeewara", ""], ["Poskitt", "D. S.", ""]]}, {"id": "1708.02340", "submitter": "Xiao Lin", "authors": "Xiao Lin, Gabriel Terejanu", "title": "EnLLVM: Ensemble Based Nonlinear Bayesian Filtering Using Linear Latent\n  Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time nonlinear Bayesian filtering algorithms are overwhelmed by data\nvolume, velocity and increasing complexity of computational models. In this\npaper, we propose a novel ensemble based nonlinear Bayesian filtering approach\nwhich only requires a small number of simulations and can be applied to\nhigh-dimensional systems in the presence of intractable likelihood functions.\nThe proposed approach uses linear latent projections to estimate the joint\nprobability distribution between states, parameters, and observables using a\nmixture of Gaussian components generated by the reconstruction error for each\nensemble member. Since it leverages the computational machinery behind linear\nlatent variable models, it can achieve fast implementations without the need to\ncompute high-dimensional sample covariance matrices. The performance of the\nproposed approach is compared with the performance of ensemble Kalman filter on\na high-dimensional Lorenz nonlinear dynamical system.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 00:45:18 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 03:25:54 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Lin", "Xiao", ""], ["Terejanu", "Gabriel", ""]]}, {"id": "1708.02365", "submitter": "David Frazier", "authors": "David T. Frazier, Tatsushi Oka and Dan Zhu", "title": "Indirect Inference with a Non-Smooth Criterion Function", "comments": "This paper is a revision of arXiv:1708.02365 and supersedes the\n  earlier arXiv paper \"Derivative-Based Optimization with a Non-Smooth\n  Simulated Criterion\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect inference requires simulating realisations of endogenous variables\nfrom the model under study. When the endogenous variables are discontinuous\nfunctions of the model parameters, the resulting indirect inference criterion\nfunction is discontinuous and does not permit the use of derivative-based\noptimisation routines. Using a change of variables technique, we propose a\nnovel simulation algorithm that alleviates the discontinuities inherent in such\nindirect inference criterion functions, and permits the application of\nderivative-based optimisation routines to estimate the unknown model\nparameters. Unlike competing approaches, this approach does not rely on kernel\nsmoothing or bandwidth parameters. Several Monte Carlo examples that have\nfeatured in the literature on indirect inference with discontinuous outcomes\nillustrate the approach, and demonstrate the superior performance of this\napproach over existing alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 04:04:02 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 01:16:17 GMT"}, {"version": "v3", "created": "Tue, 9 Jul 2019 21:57:34 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Frazier", "David T.", ""], ["Oka", "Tatsushi", ""], ["Zhu", "Dan", ""]]}, {"id": "1708.02598", "submitter": "Christian Schmid", "authors": "Christian S. Schmid and Bruce A. Desmarais", "title": "Exponential Random Graph Models with Big Networks: Maximum\n  Pseudolikelihood Estimation and the Parametric Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growth of interest in network data across fields, the Exponential\nRandom Graph Model (ERGM) has emerged as the leading approach to the\nstatistical analysis of network data. ERGM parameter estimation requires the\napproximation of an intractable normalizing constant. Simulation methods\nrepresent the state-of-the-art approach to approximating the normalizing\nconstant, leading to estimation by Monte Carlo maximum likelihood (MCMLE).\nMCMLE is accurate when a large sample of networks is used to approximate the\nnormalizing constant. However, MCMLE is computationally expensive, and may be\nprohibitively so if the size of the network is on the order of 1,000 nodes\n(i.e., one million potential ties) or greater. When the network is large, one\noption is maximum pseudolikelihood estimation (MPLE). The standard MPLE is\nsimple and fast, but generally underestimates standard errors. We show that a\nresampling method---the parametric bootstrap---results in accurate coverage\nprobabilities for confidence intervals. We find that bootstrapped MPLE can be\nrun in 1/5th the time of MCMLE. We study the relative performance of MCMLE and\nMPLE with simulation studies, and illustrate the two different approaches by\napplying them to a network of bills introduced in the United State Senate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 18:21:42 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Schmid", "Christian S.", ""], ["Desmarais", "Bruce A.", ""]]}, {"id": "1708.02632", "submitter": "Peida Zhan", "authors": "Peida Zhan, Hong Jiao, Kaiwen Man", "title": "Using JAGS for Bayesian Cognitive Diagnosis Modeling: A Tutorial", "comments": "38 pages,14 tables", "journal-ref": "Journal of Educational and Behavioral Statistics-2019", "doi": "10.3102/1076998619826040", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, the JAGS software program is systematically introduced to\nfit common Bayesian cognitive diagnosis models (CDMs), including the\ndeterministic inputs, noisy \"and\" gate (DINA) model, the deterministic inputs,\nnoisy \"or\" gate (DINO) model, the linear logistic model, the reduced\nreparameterized unified model (rRUM), and the log-linear CDM (LCDM). The\nunstructured latent structural model and the higher-order latent structural\nmodel are both introduced. We also show how to extend those models to consider\nthe polytomous attributes, the testlet effect, and the longitudinal diagnosis.\nFinally, an empirical example is presented as a tutorial to illustrate how to\nuse the JAGS codes in R.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 19:59:53 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 13:17:53 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Zhan", "Peida", ""], ["Jiao", "Hong", ""], ["Man", "Kaiwen", ""]]}, {"id": "1708.03027", "submitter": "Rongrong Zhang", "authors": "Rongrong Zhang, Wei Deng, Michael Yu Zhu", "title": "Using Deep Neural Networks to Automate Large Scale Statistical Analysis\n  for Big Data Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis (SA) is a complex process to deduce population\nproperties from analysis of data. It usually takes a well-trained analyst to\nsuccessfully perform SA, and it becomes extremely challenging to apply SA to\nbig data applications. We propose to use deep neural networks to automate the\nSA process. In particular, we propose to construct convolutional neural\nnetworks (CNNs) to perform automatic model selection and parameter estimation,\ntwo most important SA tasks. We refer to the resulting CNNs as the neural model\nselector and the neural model estimator, respectively, which can be properly\ntrained using labeled data systematically generated from candidate models.\nSimulation study shows that both the selector and estimator demonstrate\nexcellent performances. The idea and proposed framework can be further extended\nto automate the entire SA process and have the potential to revolutionize how\nSA is performed in big data analytics.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 22:34:36 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Zhang", "Rongrong", ""], ["Deng", "Wei", ""], ["Zhu", "Michael Yu", ""]]}, {"id": "1708.03264", "submitter": "Jason Morton", "authors": "Jason Morton", "title": "Contextuality from missing and versioned data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally categorical data analysis (e.g. generalized linear models)\nworks with simple, flat datasets akin to a single table in a database with no\nnotion of missing data or conflicting versions. In contrast, modern data\nanalysis must deal with distributed databases with many partial local tables\nthat need not always agree. The computational agents tabulating these tables\nare spatially separated, with binding speed-of-light constraints and data\narriving too rapidly for these distributed views ever to be fully informed and\nglobally consistent. Contextuality is a mathematical property which describes a\nkind of inconsistency arising in quantum mechanics (e.g. in Bell's theorem). In\nthis paper we show how contextuality can arise in common data collection\nscenarios, including missing data and versioning (as in low-latency distributed\ndatabases employing snapshot isolation). In the companion paper, we develop\nstatistical models adapted to this regime.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:37:53 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Morton", "Jason", ""]]}, {"id": "1708.03272", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Leonhard Held and H{\\aa}vard Rue", "title": "Fast and accurate Bayesian model criticism and conflict diagnostics\n  using R-INLA", "comments": null, "journal-ref": "Stat 6(1):331-344, 2017", "doi": "10.1002/sta4.163", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian hierarchical models are increasingly popular for realistic modelling\nand analysis of complex data. This trend is accompanied by the need for\nflexible, general, and computationally efficient methods for model criticism\nand conflict detection. Usually, a Bayesian hierarchical model incorporates a\ngrouping of the individual data points, for example individuals in repeated\nmeasurement data. In such cases, the following question arises: Are any of the\ngroups \"outliers\", or in conflict with the remaining groups? Existing general\napproaches aiming to answer such questions tend to be extremely computationally\ndemanding when model fitting is based on MCMC. We show how group-level model\ncriticism and conflict detection can be done quickly and accurately through\nintegrated nested Laplace approximations (INLA). The new method is implemented\nas a part of the open source R-INLA package for Bayesian computing\n(http://r-inla.org).\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:49:25 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 16:50:55 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 20:26:35 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 17:15:55 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Held", "Leonhard", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1708.03288", "submitter": "Peter Radchenko", "authors": "Rahul Mazumder and Peter Radchenko and Antoine Dedieu", "title": "Subset Selection with Shrinkage: Sparse Linear Modeling when the SNR is\n  low", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a seemingly unexpected and relatively less understood overfitting\naspect of a fundamental tool in sparse linear modeling - best subset selection,\nwhich minimizes the residual sum of squares subject to a constraint on the\nnumber of nonzero coefficients. While the best subset selection procedure is\noften perceived as the \"gold standard\" in sparse learning when the signal to\nnoise ratio (SNR) is high, its predictive performance deteriorates when the SNR\nis low. In particular, it is outperformed by continuous shrinkage methods, such\nas ridge regression and the Lasso. We investigate the behavior of best subset\nselection in the high-noise regimes and propose an alternative approach based\non a regularized version of the least-squares criterion. Our proposed\nestimators (a) mitigate, to a large extent, the poor predictive performance of\nbest subset selection in the high-noise regimes; and (b) perform favorably,\nwhile generally delivering substantially sparser models, relative to the best\npredictive models available via ridge regression and the Lasso. We conduct an\nextensive theoretical analysis of the predictive properties of the proposed\napproach and provide justification for its superior predictive performance\nrelative to best subset selection when the noise-level is high. Our estimators\ncan be expressed as solutions to mixed integer second order conic optimization\nproblems and, hence, are amenable to modern computational tools from\nmathematical optimization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 16:28:39 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 18:30:37 GMT"}, {"version": "v3", "created": "Wed, 2 Jun 2021 04:23:46 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Mazumder", "Rahul", ""], ["Radchenko", "Peter", ""], ["Dedieu", "Antoine", ""]]}, {"id": "1708.03625", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, John O'Leary and Yves F. Atchad\\'e", "title": "Unbiased Markov chain Monte Carlo with couplings", "comments": "Final version, accepted as a JRSS discussion paper; includes\n  supplementary material as appendices; 12 figures, 48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods provide consistent of integrals as\nthe number of iterations goes to infinity. MCMC estimators are generally biased\nafter any fixed number of iterations. We propose to remove this bias by using\ncouplings of Markov chains together with a telescopic sum argument of Glynn and\nRhee (2014). The resulting unbiased estimators can be computed independently in\nparallel. We discuss practical couplings for popular MCMC algorithms. We\nestablish the theoretical validity of the proposed estimators and study their\nefficiency relative to the underlying MCMC algorithms. Finally, we illustrate\nthe performance and limitations of the method on toy examples, on an Ising\nmodel around its critical temperature, on a high-dimensional variable selection\nproblem, and on an approximation of the cut distribution arising in Bayesian\ninference for models made of multiple modules.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 17:42:28 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 20:28:31 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 13:07:19 GMT"}, {"version": "v4", "created": "Wed, 31 Oct 2018 16:20:07 GMT"}, {"version": "v5", "created": "Wed, 17 Jul 2019 11:42:09 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Jacob", "Pierre E.", ""], ["O'Leary", "John", ""], ["Atchad\u00e9", "Yves F.", ""]]}, {"id": "1708.03730", "submitter": "Joaqu\\'in M\\'iguez", "authors": "Sara P\\'erez-Vieites, In\\'es P. Mari\\~no and Joaqu\\'in M\\'iguez", "title": "A probabilistic scheme for joint parameter estimation and state\n  prediction in complex dynamical systems", "comments": "In this version we have tried to clarify the presentation of the\n  methodology and the analysis. The body of the manuscript has been simplified\n  (in terms of the notation as well) by moving some algorithms and the proofs\n  to appendices. We have added a new variant of the algorithm based on a\n  sequential quasi-Monte Carlo scheme. All numerical results have been updated", "journal-ref": "Phys. Rev. E 98, 063305 (2018)", "doi": "10.1103/PhysRevE.98.063305", "report-no": null, "categories": "stat.CO math.PR nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in the geophysical sciences demand the ability to calibrate the\nparameters and predict the time evolution of complex dynamical models using\nsequentially-collected data. Here we introduce a general methodology for the\njoint estimation of the static parameters and the forecasting of the state\nvariables of nonlinear, and possibly chaotic, dynamical models. The proposed\nscheme is essentially probabilistic. It aims at recursively computing the\nsequence of joint posterior probability distributions of the unknown model\nparameters and its (time varying) state variables conditional on the available\nobservations. The latter are possibly partial and contaminated by noise. The\nnew framework combines a Monte Carlo scheme to approximate the posterior\ndistribution of the fixed parameters with filtering (or {\\em data\nassimilation}) techniques to track and predict the distribution of the state\nvariables. For this reason, we refer to the proposed methodology as {\\em nested\nfiltering}. In this paper we specifically explore the use of Gaussian filtering\nmethods, but other approaches fit naturally within the new framework. As an\nillustrative example, we apply three different implementations of the\nmethodology to the tracking of the state, and the estimation of the fixed\nparameters, of a stochastic two-scale Lorenz 96 system. This model is commonly\nused to assess data assimilation procedures in meteorology. For this example,\nwe compare different nested filters and show estimation and forecasting results\nfor a 4,000-dimensional system.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 23:20:59 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 09:23:40 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["P\u00e9rez-Vieites", "Sara", ""], ["Mari\u00f1o", "In\u00e9s P.", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1708.04490", "submitter": "Giles Hooker", "authors": "David Sinclair and Giles Hooker", "title": "Sparse Inverse Covariance Estimation for High-throughput microRNA\n  Sequencing Data in the Poisson Log-Normal Graphical Model", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Poisson Log-Normal Graphical Model for count data, and\npresent a normality transformation for data arising from this distribution. The\nmodel and transformation are feasible for high-throughput microRNA (miRNA)\nsequencing data and directly account for known overdispersion relationships\npresent in this data set. The model allows for network dependencies to be\nmodeled, and we provide an algorithm which utilizes a one-step EM based result\nin order to allow for a provable increase in performance in determining the\nnetwork structure. The model is shown to provide an increase in performance in\nsimulation settings over a range of network structures. The model is applied to\nhigh-throughput miRNA sequencing data from patients with breast cancer from The\nCancer Genome Atlas (TCGA). By selecting the most highly connected miRNA\nmolecules in the fitted network we find that nearly all of them are known to be\ninvolved in the regulation of breast cancer.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 13:32:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sinclair", "David", ""], ["Hooker", "Giles", ""]]}, {"id": "1708.04527", "submitter": "Martin Copenhaver", "authors": "Dimitris Bertsimas, Martin S. Copenhaver, and Rahul Mazumder", "title": "The Trimmed Lasso: Sparsity and Robustness", "comments": "32 pages (excluding appendix); 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonconvex penalty methods for sparse modeling in linear regression have been\na topic of fervent interest in recent years. Herein, we study a family of\nnonconvex penalty functions that we call the trimmed Lasso and that offers\nexact control over the desired level of sparsity of estimators. We analyze its\nstructural properties and in doing so show the following:\n  1) Drawing parallels between robust statistics and robust optimization, we\nshow that the trimmed-Lasso-regularized least squares problem can be viewed as\na generalized form of total least squares under a specific model of\nuncertainty. In contrast, this same model of uncertainty, viewed instead\nthrough a robust optimization lens, leads to the convex SLOPE (or OWL) penalty.\n  2) Further, in relating the trimmed Lasso to commonly used sparsity-inducing\npenalty functions, we provide a succinct characterization of the connection\nbetween trimmed-Lasso- like approaches and penalty functions that are\ncoordinate-wise separable, showing that the trimmed penalties subsume existing\ncoordinate-wise separable penalties, with strict containment in general.\n  3) Finally, we describe a variety of exact and heuristic algorithms, both\nexisting and new, for trimmed Lasso regularized estimation problems. We include\na comparison between the different approaches and an accompanying\nimplementation of the algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 14:56:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Copenhaver", "Martin S.", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1708.04753", "submitter": "Yun Yang", "authors": "Yun Yang, Anirban Bhattacharya, Debdeep Pati", "title": "Frequentist coverage and sup-norm convergence rate in Gaussian process\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) regression is a powerful interpolation technique due to\nits flexibility in capturing non-linearity. In this paper, we provide a general\nframework for understanding the frequentist coverage of point-wise and\nsimultaneous Bayesian credible sets in GP regression. As an intermediate\nresult, we develop a Bernstein von-Mises type result under supremum norm in\nrandom design GP regression. Identifying both the mean and covariance function\nof the posterior distribution of the Gaussian process as regularized\n$M$-estimators, we show that the sampling distribution of the posterior mean\nfunction and the centered posterior distribution can be respectively\napproximated by two population level GPs. By developing a comparison inequality\nbetween two GPs, we provide exact characterization of frequentist coverage\nprobabilities of Bayesian point-wise credible intervals and simultaneous\ncredible bands of the regression function. Our results show that inference\nbased on GP regression tends to be conservative; when the prior is\nunder-smoothed, the resulting credible intervals and bands have minimax-optimal\nsizes, with their frequentist coverage converging to a non-degenerate value\nbetween their nominal level and one. As a byproduct of our theory, we show that\nthe GP regression also yields minimax-optimal posterior contraction rate\nrelative to the supremum norm, which provides a positive evidence to the long\nstanding problem on optimal supremum norm contraction rate in GP regression.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:07:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Yang", "Yun", ""], ["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "1708.04870", "submitter": "Moritz Schauer", "authors": "Frank van der Meulen and Moritz Schauer", "title": "On residual and guided proposals for diffusion bridge simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Whitaker et al. (2017) considered Bayesian estimation of diffusion\ndriven mixed effects models using data-augmentation. The missing data,\ndiffusion bridges connecting discrete time observations, are drawn using a\n\"residual bridge construct\". In this paper we compare this construct (which we\ncall residual proposal) with the guided proposals introduced in Schauer et al.\n2017. It is shown that both approaches are related, but use a different\napproximation to the intractable stochastic differential equation of the true\ndiffusion bridge. It reveals that the computational complexity of both\napproaches is similar. Some examples are included to compare the ability of\nboth proposals to capture local nonlinearities in the dynamics of the true\nbridge.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 13:12:54 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "1708.05136", "submitter": "Robert Hannah", "authors": "Robert Hannah, Wotao Yin", "title": "More Iterations per Second, Same Quality -- Why Asynchronous Algorithms\n  may Drastically Outperform Traditional Ones", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DC math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the convergence of a very general\nasynchronous-parallel algorithm called ARock, that takes many well-known\nasynchronous algorithms as special cases (gradient descent, proximal gradient,\nDouglas Rachford, ADMM, etc.). In asynchronous-parallel algorithms, the\ncomputing nodes simply use the most recent information that they have access\nto, instead of waiting for a full update from all nodes in the system. This\nmeans that nodes do not have to waste time waiting for information, which can\nbe a major bottleneck, especially in distributed systems. When the system has\n$p$ nodes, asynchronous algorithms may complete $\\Theta(\\ln(p))$ more\niterations than synchronous algorithms in a given time period (\"more iterations\nper second\").\n  Although asynchronous algorithms may compute more iterations per second,\nthere is error associated with using outdated information. How many more\niterations in total are needed to compensate for this error is still an open\nquestion. The main results of this paper aim to answer this question. We prove,\nloosely, that as the size of the problem becomes large, the number of\nadditional iterations that asynchronous algorithms need becomes negligible\ncompared to the total number (\"same quality\" of the iterations). Taking these\nfacts together, our results provide solid evidence of the potential of\nasynchronous algorithms to vastly speed up certain distributed computations.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 05:01:25 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Hannah", "Robert", ""], ["Yin", "Wotao", ""]]}, {"id": "1708.05239", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Fredrik Lindsten, Maurizio Filippone and James\n  Hensman", "title": "Pseudo-extended Markov chain Monte Carlo", "comments": "Advances in Neural Information Processing Systems 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from posterior distributions using Markov chain Monte Carlo (MCMC)\nmethods can require an exhaustive number of iterations, particularly when the\nposterior is multi-modal as the MCMC sampler can become trapped in a local mode\nfor a large number of iterations. In this paper, we introduce the\npseudo-extended MCMC method as a simple approach for improving the mixing of\nthe MCMC sampler for multi-modal posterior distributions. The pseudo-extended\nmethod augments the state-space of the posterior using pseudo-samples as\nauxiliary variables. On the extended space, the modes of the posterior are\nconnected, which allows the MCMC sampler to easily move between well-separated\nposterior modes. We demonstrate that the pseudo-extended approach delivers\nimproved MCMC sampling over the Hamiltonian Monte Carlo algorithm on\nmulti-modal posteriors, including Boltzmann machines and models with\nsparsity-inducing priors.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:45:07 GMT"}, {"version": "v2", "created": "Thu, 14 Feb 2019 12:03:37 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 17:13:57 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Nemeth", "Christopher", ""], ["Lindsten", "Fredrik", ""], ["Filippone", "Maurizio", ""], ["Hensman", "James", ""]]}, {"id": "1708.05573", "submitter": "Soumendu Sundar Mukherjee", "authors": "Soumendu Sundar Mukherjee, Purnamrita Sarkar, and Peter J. Bickel", "title": "Two provably consistent divide and conquer clustering algorithms for\n  large networks", "comments": "41 pages, comments are most welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advance divide-and-conquer strategies for solving the\ncommunity detection problem in networks. We propose two algorithms which\nperform clustering on a number of small subgraphs and finally patches the\nresults into a single clustering. The main advantage of these algorithms is\nthat they bring down significantly the computational cost of traditional\nalgorithms, including spectral clustering, semi-definite programs, modularity\nbased methods, likelihood based methods etc., without losing on accuracy and\neven improving accuracy at times. These algorithms are also, by nature,\nparallelizable. Thus, exploiting the facts that most traditional algorithms are\naccurate and the corresponding optimization problems are much simpler in small\nproblems, our divide-and-conquer methods provide an omnibus recipe for scaling\ntraditional algorithms up to large networks. We prove consistency of these\nalgorithms under various subgraph selection procedures and perform extensive\nsimulations and real-data analysis to understand the advantages of the\ndivide-and-conquer approach in various settings.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 12:09:10 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Mukherjee", "Soumendu Sundar", ""], ["Sarkar", "Purnamrita", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1708.05653", "submitter": "Luca Weihs", "authors": "Luca Weihs, Mathias Drton, Nicolai Meinshausen", "title": "Symmetric Rank Covariances: a Generalised Framework for Nonparametric\n  Measures of Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to test whether two random vectors are independent has spawned a\nlarge number of competing measures of dependence. We are interested in\nnonparametric measures that are invariant under strictly increasing\ntransformations, such as Kendall's tau, Hoeffding's D, and the more recently\ndiscovered Bergsma--Dassios sign covariance. Each of these measures exhibits\nsymmetries that are not readily apparent from their definitions. Making these\nsymmetries explicit, we define a new class of multivariate nonparametric\nmeasures of dependence that we refer to as Symmetric Rank Covariances. This new\nclass generalises all of the above measures and leads naturally to multivariate\nextensions of the Bergsma--Dassios sign covariance. Symmetric Rank Covariances\nmay be estimated unbiasedly using U-statistics for which we prove results on\ncomputational efficiency and large-sample behavior. The algorithms we develop\nfor their computation include, to the best of our knowledge, the first\nefficient algorithms for the well-known Hoeffding's D statistic in the\nmultivariate setting.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 15:40:02 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Weihs", "Luca", ""], ["Drton", "Mathias", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1708.05678", "submitter": "Jim Griffin", "authors": "Jim Griffin, Krys Latuszynski and Mark Steel", "title": "In Search of Lost (Mixing) Time: Adaptive Markov chain Monte Carlo\n  schemes for Bayesian variable selection with very large p", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The availability of data sets with large numbers of variables is rapidly\nincreasing. The effective application of Bayesian variable selection methods\nfor regression with these data sets has proved difficult since available Markov\nchain Monte Carlo methods do not perform well in typical problem sizes of\ninterest. The current paper proposes new adaptive Markov chain Monte Carlo\nalgorithms to address this shortcoming. The adaptive design of these algorithms\nexploits the observation that in large $p$ small $n$ settings, the majority of\nthe $p$ variables will be approximately uncorrelated a posteriori. The\nalgorithms adaptively build suitable non-local proposals that result in moves\nwith squared jumping distance significantly larger than standard methods. Their\nperformance is studied empirically in high-dimensional problems (with both\nsimulated and actual data) and speedups of up to 4 orders of magnitude are\nobserved. The proposed algorithms are easily implementable on multi-core\narchitectures and are well suited for parallel tempering or sequential Monte\nCarlo implementations.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 16:23:04 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 13:25:16 GMT"}, {"version": "v3", "created": "Tue, 7 May 2019 07:39:05 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Griffin", "Jim", ""], ["Latuszynski", "Krys", ""], ["Steel", "Mark", ""]]}, {"id": "1708.06018", "submitter": "Shin Harase", "authors": "Shin Harase", "title": "Conversion of Mersenne Twister to double-precision floating-point\n  numbers", "comments": null, "journal-ref": "Mathematics and Computers in Simulation, Volume 161, July 2019,\n  Pages 76-83", "doi": "10.1016/j.matcom.2018.08.006", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 32-bit Mersenne Twister generator MT19937 is a widely used random number\ngenerator. To generate numbers with more than 32 bits in bit length, and\nparticularly when converting into 53-bit double-precision floating-point\nnumbers in $[0,1)$ in the IEEE 754 format, the typical implementation\nconcatenates two successive 32-bit integers and divides them by a power of $2$.\nIn this case, the 32-bit MT19937 is optimized in terms of its equidistribution\nproperties (the so-called dimension of equidistribution with $v$-bit accuracy)\nunder the assumption that one will mainly be using 32-bit output values, and\nhence the concatenation sometimes degrades the dimension of equidistribution\ncompared with the simple use of 32-bit outputs. In this paper, we analyze such\nphenomena by investigating hidden $\\mathbb{F}_2$-linear relations among the\nbits of high-dimensional outputs. Accordingly, we report that MT19937 with a\nspecific lag set fails several statistical tests, such as the overlapping\ncollision test, matrix rank test, and Hamming independence test.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 20:55:19 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 13:42:43 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 13:53:13 GMT"}, {"version": "v4", "created": "Sun, 2 Sep 2018 16:59:58 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Harase", "Shin", ""]]}, {"id": "1708.06250", "submitter": "Biswa Sengupta", "authors": "Biswa Sengupta and Yu Qian", "title": "Pillar Networks++: Distributed non-parametric deep and wide networks", "comments": "arXiv admin note: substantial text overlap with arXiv:1707.06923", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent work, it was shown that combining multi-kernel based support vector\nmachines (SVMs) can lead to near state-of-the-art performance on an action\nrecognition dataset (HMDB-51 dataset). This was 0.4\\% lower than frameworks\nthat used hand-crafted features in addition to the deep convolutional feature\nextractors. In the present work, we show that combining distributed Gaussian\nProcesses with multi-stream deep convolutional neural networks (CNN) alleviate\nthe need to augment a neural network with hand-crafted features. In contrast to\nprior work, we treat each deep neural convolutional network as an expert\nwherein the individual predictions (and their respective uncertainties) are\ncombined into a Product of Experts (PoE) framework.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:51:43 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Sengupta", "Biswa", ""], ["Qian", "Yu", ""]]}, {"id": "1708.06302", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss and Joseph Guinness", "title": "A general framework for Vecchia approximations of Gaussian processes", "comments": null, "journal-ref": "Statistical Science, 36(1), 124-141 (2021)", "doi": "10.1214/19-STS755", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are commonly used as models for functions, time\nseries, and spatial fields, but they are computationally infeasible for large\ndatasets. Focusing on the typical setting of modeling data as a GP plus an\nadditive noise term, we propose a generalization of the Vecchia (1988) approach\nas a framework for GP approximations. We show that our general Vecchia approach\ncontains many popular existing GP approximations as special cases, allowing for\ncomparisons among the different methods within a unified framework.\nRepresenting the models by directed acyclic graphs, we determine the sparsity\nof the matrices necessary for inference, which leads to new insights regarding\nthe computational properties. Based on these results, we propose a novel sparse\ngeneral Vecchia approximation, which ensures computational feasibility for\nlarge spatial datasets but can lead to considerable improvements in\napproximation accuracy over Vecchia's original approach. We provide several\ntheoretical results and conduct numerical comparisons. We conclude with\nguidelines for the use of Vecchia approximations in spatial statistics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 16:03:04 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 16:07:51 GMT"}, {"version": "v3", "created": "Sun, 27 May 2018 20:45:48 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 20:52:12 GMT"}, {"version": "v5", "created": "Sat, 17 Aug 2019 15:25:13 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""]]}, {"id": "1708.06448", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "The p-convolution forest: a method for solving graphical models with\n  additive probabilistic equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolution trees, loopy belief propagation, and fast numerical p-convolution\nare combined for the first time to efficiently solve networks with several\nadditive constraints between random variables. An implementation of this\n\"convolution forest\" approach is constructed from scratch, including an\nimproved trimmed convolution tree algorithm and engineering details that permit\nfast inference in practice, and improve the ability of scientists to prototype\nmodels with additive relationships between discrete variables. The utility of\nthis approach is demonstrated using several examples: these include\nillustrations on special cases of some classic NP-complete problems (subset sum\nand knapsack), identification of GC-rich genomic regions with a large hidden\nMarkov model, inference of molecular composition from summary statistics of the\nintact molecule, and estimation of elemental abundance in the presence of\noverlapping isotope peaks.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 22:48:57 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "1708.07073", "submitter": "Benjamin Baumer", "authors": "Benjamin S. Baumer", "title": "A Grammar for Reproducible and Painless Extract-Transform-Load\n  Operations on Medium Data", "comments": "30 pages, plus supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many interesting data sets available on the Internet are of a medium\nsize---too big to fit into a personal computer's memory, but not so large that\nthey won't fit comfortably on its hard disk. In the coming years, data sets of\nthis magnitude will inform vital research in a wide array of application\ndomains. However, due to a variety of constraints they are cumbersome to\ningest, wrangle, analyze, and share in a reproducible fashion. These\nobstructions hamper thorough peer-review and thus disrupt the forward progress\nof science. We propose a predictable and pipeable framework for R (the\nstate-of-the-art statistical computing environment) that leverages SQL (the\nvenerable database architecture and query language) to make reproducible\nresearch on medium data a painless reality.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 16:05:32 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 20:38:07 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 15:00:33 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Baumer", "Benjamin S.", ""]]}, {"id": "1708.07114", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Aaron Smith", "title": "Rapid Mixing of Hamiltonian Monte Carlo on Strongly Log-Concave\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain several quantitative bounds on the mixing properties of the\nHamiltonian Monte Carlo (HMC) algorithm for a strongly log-concave target\ndistribution $\\pi$ on $\\mathbb{R}^{d}$, showing that HMC mixes quickly in this\nsetting. One of our main results is a dimension-free bound on the mixing of an\n\"ideal\" HMC chain, which is used to show that the usual leapfrog implementation\nof HMC can sample from $\\pi$ using only $\\mathcal{O}(d^{\\frac{1}{4}})$ gradient\nevaluations. This dependence on dimension is sharp, and our results\nsignificantly extend and improve previous quantitative bounds on the mixing of\nHMC.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 17:29:10 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Mangoubi", "Oren", ""], ["Smith", "Aaron", ""]]}, {"id": "1708.07441", "submitter": "Joseph Hart", "authors": "Joseph Hart, Julie Bessac, and Emil Constantinescu", "title": "Global sensitivity analysis for statistical model parameters", "comments": "revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis (GSA) is frequently used to analyze the influence\nof uncertain parameters in mathematical models and simulations. In principle,\ntools from GSA may be extended to analyze the influence of parameters in\nstatistical models. Such analyses may enable reduced or parsimonious modeling\nand greater predictive capability. However, difficulties such as parameter\ncorrelation, model stochasticity, multivariate model output, and unknown\nparameter distributions prohibit a direct application of GSA tools to\nstatistical models. By leveraging a loss function associated with the\nstatistical model, we introduce a novel framework to address these difficulties\nand enable efficient GSA for statistical model parameters. Theoretical and\ncomputational properties are considered and illustrated on a synthetic example.\nThe framework is applied to a Gaussian process model from the literature, which\ndepends on 95 parameters. Non-influential parameters are discovered through GSA\nand a reduced model with equal or stronger predictive capability is constructed\nby using only 79 parameters.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 14:42:42 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 18:05:47 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 17:47:21 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Hart", "Joseph", ""], ["Bessac", "Julie", ""], ["Constantinescu", "Emil", ""]]}, {"id": "1708.07466", "submitter": "Nabil Kahale", "authors": "Nabil Kahale", "title": "Randomized Dimension Reduction for Monte Carlo Simulations", "comments": "42 pages, 1 figure. A preliminary version has appeared at the 9th\n  NIPS Workshop on Optimization for Machine Learning, Dec. 2016, Barcelona", "journal-ref": "Management Science 66(3): 1421-1439 (2020)", "doi": "10.1287/mnsc.2018.3250", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new unbiased algorithm that estimates the expected value of f(U)\nvia Monte Carlo simulation, where U is a vector of d independent random\nvariables, and f is a function of d variables. We assume that f does not depend\nequally on all its arguments. Under certain conditions we prove that, for the\nsame computational cost, the variance of our estimator is lower than the\nvariance of the standard Monte Carlo estimator by a factor of order d. Our\nmethod can be used to obtain a low-variance unbiased estimator for the\nexpectation of a function of the state of a Markov chain at a given time-step.\nWe study applications to volatility forecasting and time-varying queues.\nNumerical experiments show that our algorithm dramatically improves upon the\nstandard Monte Carlo method for large values of d, and is highly resilient to\ndiscontinuities.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 15:47:50 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 13:02:01 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 15:54:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Kahale", "Nabil", ""]]}, {"id": "1708.07481", "submitter": "Andrew Knyazev", "authors": "David Zhuzhunashvili and Andrew Knyazev", "title": "Preconditioned Spectral Clustering for Stochastic Block Partition\n  Streaming Graph Challenge", "comments": "6 pages. To appear in Proceedings of the 2017 IEEE High Performance\n  Extreme Computing Conference. Student Innovation Award Streaming Graph\n  Challenge: Stochastic Block Partition, see\n  http://graphchallenge.mit.edu/champions", "journal-ref": "2017 IEEE High Performance Extreme Computing Conference (HPEC),\n  Waltham, MA, USA, 2017, pp. 1-6", "doi": "10.1109/HPEC.2017.8091045", "report-no": null, "categories": "cs.MS cs.DC cs.DS stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Locally Optimal Block Preconditioned Conjugate Gradient (LOBPCG) is\ndemonstrated to efficiently solve eigenvalue problems for graph Laplacians that\nappear in spectral clustering. For static graph partitioning, 10-20 iterations\nof LOBPCG without preconditioning result in ~10x error reduction, enough to\nachieve 100% correctness for all Challenge datasets with known truth\npartitions, e.g., for graphs with 5K/.1M (50K/1M) Vertices/Edges in 2 (7)\nseconds, compared to over 5,000 (30,000) seconds needed by the baseline Python\ncode. Our Python code 100% correctly determines 98 (160) clusters from the\nChallenge static graphs with 0.5M (2M) vertices in 270 (1,700) seconds using\n10GB (50GB) of memory. Our single-precision MATLAB code calculates the same\nclusters at half time and memory. For streaming graph partitioning, LOBPCG is\ninitiated with approximate eigenvectors of the graph Laplacian already computed\nfor the previous graph, in many cases reducing 2-3 times the number of required\nLOBPCG iterations, compared to the static case. Our spectral clustering is\ngeneric, i.e. assuming nothing specific of the block model or streaming, used\nto generate the graphs for the Challenge, in contrast to the base code.\nNevertheless, in 10-stage streaming comparison with the base code for the 5K\ngraph, the quality of our clusters is similar or better starting at stage 4 (7)\nfor emerging edging (snowballing) streaming, while the computations are over\n100-1000 faster.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:09:05 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Zhuzhunashvili", "David", ""], ["Knyazev", "Andrew", ""]]}, {"id": "1708.07787", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray, Daniel Lund\\'en, Jan Kudlicka, David Broman,\n  Thomas B. Sch\\\"on", "title": "Delayed Sampling and Automatic Rao-Blackwellization of Probabilistic\n  Programs", "comments": "13 pages, 4 figures", "journal-ref": "Proceedings of the 21st International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2018", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a dynamic mechanism for the solution of analytically-tractable\nsubstructure in probabilistic programs, using conjugate priors and affine\ntransformations to reduce variance in Monte Carlo estimators. For inference\nwith Sequential Monte Carlo, this automatically yields improvements such as\nlocally-optimal proposals and Rao-Blackwellization. The mechanism maintains a\ndirected graph alongside the running program that evolves dynamically as\noperations are triggered upon it. Nodes of the graph represent random\nvariables, edges the analytically-tractable relationships between them. Random\nvariables remain in the graph for as long as possible, to be sampled only when\nthey are used by the program in a way that cannot be resolved analytically. In\nthe meantime, they are conditioned on as many observations as possible. We\ndemonstrate the mechanism with a few pedagogical examples, as well as a\nlinear-nonlinear state-space model with simulated data, and an epidemiological\nmodel with real data of a dengue outbreak in Micronesia. In all cases one or\nmore variables are automatically marginalized out to significantly reduce\nvariance in estimates of the marginal likelihood, in the final case\nfacilitating a random-weight or pseudo-marginal-type importance sampler for\nparameter estimation. We have implemented the approach in Anglican and a new\nprobabilistic programming language called Birch.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:48:20 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 15:30:49 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Lund\u00e9n", "Daniel", ""], ["Kudlicka", "Jan", ""], ["Broman", "David", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1708.07801", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Aky{\\i}ld{\\i}z, Joaqu\\'in M\\'iguez", "title": "Nudging the particle filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a new sampling scheme aimed at improving the performance of\nparticle filters whenever (a) there is a significant mismatch between the\nassumed model dynamics and the actual system, or (b) the posterior probability\ntends to concentrate in relatively small regions of the state space. The\nproposed scheme pushes some particles towards specific regions where the\nlikelihood is expected to be high, an operation known as nudging in the\ngeophysics literature. We re-interpret nudging in a form applicable to any\nparticle filtering scheme, as it does not involve any changes in the rest of\nthe algorithm. Since the particles are modified, but the importance weights do\nnot account for this modification, the use of nudging leads to additional bias\nin the resulting estimators. However, we prove analytically that nudged\nparticle filters can still attain asymptotic convergence with the same error\nrates as conventional particle methods. Simple analysis also yields an\nalternative interpretation of the nudging operation that explains its\nrobustness to model errors. Finally, we show numerical results that illustrate\nthe improvements that can be attained using the proposed scheme. In particular,\nwe present nonlinear tracking examples with synthetic data and a model\ninference example using real-world financial data.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 16:31:49 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 10:06:37 GMT"}, {"version": "v3", "created": "Sat, 10 Mar 2018 23:25:53 GMT"}, {"version": "v4", "created": "Tue, 19 Mar 2019 11:44:01 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Aky\u0131ld\u0131z", "\u00d6mer Deniz", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1708.07804", "submitter": "Saptarshi Chakraborty", "authors": "Saptarshi Chakraborty and Samuel W. K. Wong", "title": "BAMBI: An R package for Fitting Bivariate Angular Mixture Models", "comments": "63 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analyses of directional or angular data have applications in a\nvariety of fields, such as geology, meteorology and bioinformatics. There is\nsubstantial literature on descriptive and inferential techniques for univariate\nangular data, with the bivariate (or more generally, multivariate) cases\nreceiving more attention in recent years. More specifically, the bivariate\nwrapped normal, von Mises sine and von Mises cosine distributions, and mixtures\nthereof, have been proposed for practical use. However, there is a lack of\nsoftware implementing these distributions and the associated inferential\ntechniques. In this article, we introduce BAMBI, an R package for analyzing\nbivariate (and univariate) angular data. We implement random data generation,\ndensity evaluation, and computation of theoretical summary measures (variances\nand correlation coefficients) for the three aforementioned bivariate angular\ndistributions, as well as two univariate angular distributions: the univariate\nwrapped normal and the univariate von Mises distribution. The major\ncontribution of BAMBI to statistical computing is in providing Bayesian methods\nfor modeling angular data using finite mixtures of these distributions. We also\nprovide functions for visual and numerical diagnostics and Bayesian inference\nfor the fitted models. In this article, we first provide a brief review of the\ndistributions and techniques used in BAMBI, then describe the capabilities of\nthe package, and finally conclude with demonstrations of mixture model fitting\nusing BAMBI on the two real datasets included in the package, one univariate\nand one bivariate.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 16:37:46 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 01:10:05 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 00:53:05 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chakraborty", "Saptarshi", ""], ["Wong", "Samuel W. K.", ""]]}, {"id": "1708.08268", "submitter": "Peter Rousseeuw", "authors": "Peter J. Rousseeuw, Domenico Perrotta, Marco Riani, Mia Hubert", "title": "Robust Monitoring of Time Series with Application to Fraud Detection", "comments": null, "journal-ref": "Econometrics and Statistics, 2019, Vol. 9, 108-121", "doi": "10.1016/j.ecosta.2018.05.001", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series often contain outliers and level shifts or structural changes.\nThese unexpected events are of the utmost importance in fraud detection, as\nthey may pinpoint suspicious transactions. The presence of such unusual events\ncan easily mislead conventional time series analysis and yield erroneous\nconclusions. In this paper we provide a unified framework for detecting\noutliers and level shifts in short time series that may have a seasonal\npattern. The approach combines ideas from the FastLTS algorithm for robust\nregression with alternating least squares. The double wedge plot is proposed, a\ngraphical display which indicates outliers and potential level shifts. The\nmethodology was developed to detect potential fraud cases in time series of\nimports into the European Union, and is illustrated on two such series.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 10:57:26 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 13:32:11 GMT"}, {"version": "v3", "created": "Sat, 7 Apr 2018 06:11:31 GMT"}, {"version": "v4", "created": "Sun, 20 May 2018 12:52:00 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Rousseeuw", "Peter J.", ""], ["Perrotta", "Domenico", ""], ["Riani", "Marco", ""], ["Hubert", "Mia", ""]]}, {"id": "1708.08354", "submitter": "Andrew Knyazev", "authors": "Andrew Knyazev", "title": "Recent implementations, applications, and extensions of the Locally\n  Optimal Block Preconditioned Conjugate Gradient method (LOBPCG)", "comments": "4 pages. Householder Symposium on Numerical Linear Algebra, June 2017", "journal-ref": null, "doi": null, "report-no": "MERL TR2017-078", "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since introduction [A. Knyazev, Toward the optimal preconditioned\neigensolver: Locally optimal block preconditioned conjugate gradient method,\nSISC (2001) DOI:10.1137/S1064827500366124] and efficient parallel\nimplementation [A. Knyazev et al., Block locally optimal preconditioned\neigenvalue xolvers (BLOPEX) in HYPRE and PETSc, SISC (2007)\nDOI:10.1137/060661624], LOBPCG has been used is a wide range of applications in\nmechanics, material sciences, and data sciences. We review its recent\nimplementations and applications, as well as extensions of the local optimality\nidea beyond standard eigenvalue problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 14:53:30 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Knyazev", "Andrew", ""]]}, {"id": "1708.08396", "submitter": "Jeremy Heng", "authors": "Jeremy Heng, Adrian N. Bishop, George Deligiannidis, Arnaud Doucet", "title": "Controlled Sequential Monte Carlo", "comments": null, "journal-ref": "The Annals of Statistics, Volume 48, Number 5, pages: 2904-2929,\n  October 2020", "doi": "10.1214/19-AOS1914", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo methods, also known as particle methods, are a popular\nset of techniques for approximating high-dimensional probability distributions\nand their normalizing constants. These methods have found numerous applications\nin statistics and related fields; e.g. for inference in non-linear non-Gaussian\nstate space models, and in complex static models. Like many Monte Carlo\nsampling schemes, they rely on proposal distributions which crucially impact\ntheir performance. We introduce here a class of controlled sequential Monte\nCarlo algorithms, where the proposal distributions are determined by\napproximating the solution to an associated optimal control problem using an\niterative scheme. This method builds upon a number of existing algorithms in\neconometrics, physics, and statistics for inference in state space models, and\ngeneralizes these methods so as to accommodate complex static models. We\nprovide a theoretical analysis concerning the fluctuation and stability of this\nmethodology that also provides insight into the properties of related\nalgorithms. We demonstrate significant gains over state-of-the-art methods at a\nfixed computational complexity on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 16:07:15 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 14:15:41 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 15:36:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Heng", "Jeremy", ""], ["Bishop", "Adrian N.", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1708.09028", "submitter": "Etienne Marceau", "authors": "H\\'el\\`ene Cossette, Etienne Marceau, Quang Huy Nguyen, Christian\n  Robert", "title": "Tail approximations for sums of dependent regularly varying random\n  variables under Archimedean copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare two numerical methods for approximating the\nprobability that the sum of dependent regularly varying random variables\nexceeds a high threshold under Archimedean copula models. The first method is\nbased on conditional Monte Carlo. We present four estimators and show that most\nof them have bounded relative errors. The second method is based on analytical\nexpressions of the multivariate survival or cumulative distribution functions\nof the regularly varying random variables and provides sharp and deterministic\nbounds of the probability of exceedance. We discuss implementation issues and\nillustrate the accuracy of both procedures through numerical studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 21:15:12 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Cossette", "H\u00e9l\u00e8ne", ""], ["Marceau", "Etienne", ""], ["Nguyen", "Quang Huy", ""], ["Robert", "Christian", ""]]}, {"id": "1708.09461", "submitter": "Sayan Nag", "authors": "Sayan Nag", "title": "A Type II Fuzzy Entropy Based Multi-Level Image Thresholding Using\n  Adaptive Plant Propagation Algorithm", "comments": "12 Pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1708.07040", "journal-ref": null, "doi": "10.17605/OSF.IO/5KQZD", "report-no": null, "categories": "cs.CV math.OC physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most straightforward, direct and efficient approaches to Image\nSegmentation is Image Thresholding. Multi-level Image Thresholding is an\nessential viewpoint in many image processing and Pattern Recognition based\nreal-time applications which can effectively and efficiently classify the\npixels into various groups denoting multiple regions in an Image. Thresholding\nbased Image Segmentation using fuzzy entropy combined with intelligent\noptimization approaches are commonly used direct methods to properly identify\nthe thresholds so that they can be used to segment an Image accurately. In this\npaper a novel approach for multi-level image thresholding is proposed using\nType II Fuzzy sets combined with Adaptive Plant Propagation Algorithm (APPA).\nObtaining the optimal thresholds for an image by maximizing the entropy is\nextremely tedious and time consuming with increase in the number of thresholds.\nHence, Adaptive Plant Propagation Algorithm (APPA), a memetic algorithm based\non plant intelligence, is used for fast and efficient selection of optimal\nthresholds. This fact is reasonably justified by comparing the accuracy of the\noutcomes and computational time consumed by other modern state-of-the-art\nalgorithms such as Particle Swarm Optimization (PSO), Gravitational Search\nAlgorithm (GSA) and Genetic Algorithm (GA).\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 09:43:00 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Nag", "Sayan", ""]]}]