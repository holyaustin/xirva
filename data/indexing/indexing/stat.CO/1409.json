[{"id": "1409.0074", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Benjamin Haaland", "title": "Speeding up neighborhood search in local Gaussian process prediction", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent implementations of local approximate Gaussian process models have\npushed computational boundaries for non-linear, non-parametric prediction\nproblems, particularly when deployed as emulators for computer experiments.\nTheir flavor of spatially independent computation accommodates massive\nparallelization, meaning that they can handle designs two or more orders of\nmagnitude larger than previously. However, accomplishing that feat can still\nrequire massive supercomputing resources. Here we aim to ease that burden. We\nstudy how predictive variance is reduced as local designs are built up for\nprediction. We then observe how the exhaustive and discrete nature of an\nimportant search subroutine involved in building such local designs may be\noverly conservative. Rather, we suggest that searching the space radially,\ni.e., continuously along rays emanating from the predictive location of\ninterest, is a far thriftier alternative. Our empirical work demonstrates that\nray-based search yields predictors with accuracy comparable to exhaustive\nsearch, but in a fraction of the time - bringing a supercomputer implementation\nback onto the desktop.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 01:02:47 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 18:26:27 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1409.1842", "submitter": "Robert Maidstone", "authors": "Robert Maidstone, Toby Hocking, Guillem Rigaill and Paul Fearnhead", "title": "On Optimal Multiple Changepoint Algorithms for Large Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need for algorithms that can accurately detect\nchangepoints in long time-series, or equivalent, data. Many common approaches\nto detecting changepoints, for example based on penalised likelihood or minimum\ndescription length, can be formulated in terms of minimising a cost over\nsegmentations. Dynamic programming methods exist to solve this minimisation\nproblem exactly, but these tend to scale at least quadratically in the length\nof the time-series. Algorithms, such as Binary Segmentation, exist that have a\ncomputational cost that is close to linear in the length of the time-series,\nbut these are not guaranteed to find the optimal segmentation. Recently pruning\nideas have been suggested that can speed up the dynamic programming algorithms,\nwhilst still being guaranteed to find true minimum of the cost function. Here\nwe extend these pruning methods, and introduce two new algorithms for\nsegmenting data, FPOP and SNIP. Empirical results show that FPOP is\nsubstantially faster than existing dynamic programming methods, and unlike the\nexisting methods its computational efficiency is robust to the number of\nchangepoints in the data. We evaluate the method at detecting Copy Number\nVariations and observe that FPOP has a computational cost that is competitive\nwith that of Binary Segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 15:44:34 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Maidstone", "Robert", ""], ["Hocking", "Toby", ""], ["Rigaill", "Guillem", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1409.3027", "submitter": "Lorenzo Mercuri", "authors": "Stefano M. Iacus and Lorenzo Mercuri", "title": "Implementation of L\\'evy CARMA model in Yuima package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper shows how to use the R package yuima available on CRAN for the\nsimulation and the estimation of a general L\\'evy Continuous Autoregressive\nMoving Average (CARMA) model. The flexibility of the package is due to the fact\nthat the user is allowed to choose several parametric L\\'evy distribution for\nthe increments. Some numerical examples are given in order to explain the main\nclasses and the corresponding methods implemented in yuima package for the\nCARMA model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 11:27:15 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Iacus", "Stefano M.", ""], ["Mercuri", "Lorenzo", ""]]}, {"id": "1409.3144", "submitter": "Duncan Temple Lang", "authors": "Duncan Temple Lang", "title": "Enhancing R with Advanced Compilation Tools and Methods", "comments": "Published in at http://dx.doi.org/10.1214/13-STS462 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 181-200", "doi": "10.1214/13-STS462", "report-no": "IMS-STS-STS462", "categories": "stat.CO cs.MS cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I describe an approach to compiling common idioms in R code directly to\nnative machine code and illustrate it with several examples. Not only can this\nyield significant performance gains, but it allows us to use new approaches to\ncomputing in R. Importantly, the compilation requires no changes to R itself,\nbut is done entirely via R packages. This allows others to experiment with\ndifferent compilation strategies and even to define new domain-specific\nlanguages within R. We use the Low-Level Virtual Machine (LLVM) compiler\ntoolkit to create the native code and perform sophisticated optimizations on\nthe code. By adopting this widely used software within R, we leverage its\nability to generate code for different platforms such as CPUs and GPUs, and\nwill continue to benefit from its ongoing development. This approach\npotentially allows us to develop high-level R code that is also fast, that can\nbe compiled to work with different data representations and sources, and that\ncould even be run outside of R. The approach aims to both provide a compiler\nfor a limited subset of the R language and also to enable R programmers to\nwrite other compilers. This is another approach to help us write high-level\ndescriptions of what we want to compute, not how.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:37:20 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Lang", "Duncan Temple", ""]]}, {"id": "1409.3531", "submitter": "John M. Chambers", "authors": "John M. Chambers", "title": "Object-Oriented Programming, Functional Programming and R", "comments": "Published in at http://dx.doi.org/10.1214/13-STS452 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 167-180", "doi": "10.1214/13-STS452", "report-no": "IMS-STS-STS452", "categories": "stat.ME cs.PL cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews some programming techniques in R that have proved useful,\nparticularly for substantial projects. These include several versions of\nobject-oriented programming, used in a large number of R packages. The review\ntries to clarify the origins and ideas behind the various versions, each of\nwhich is valuable in the appropriate context. R has also been strongly\ninfluenced by the ideas of functional programming and, in particular, by the\ndesire to combine functional with object oriented programming. To clarify how\nthis particular mix of ideas has turned out in the current R language and\nsupporting software, the paper will first review the basic ideas behind\nobject-oriented and functional programming, and then examine the evolution of R\nwith these ideas providing context. Functional programming supports\nwell-defined, defensible software giving reproducible results. Object-oriented\nprogramming is the mechanism par excellence for managing complexity while\nkeeping things simple for the user. The two paradigms have been valuable in\nsupporting major software for fitting models to data and numerous other\nstatistical applications. The paradigms have been adopted, and adapted,\ndistinctively in R. Functional programming motivates much of R but R does not\nenforce the paradigm. Object-oriented programming from a functional perspective\ndiffers from that used in non-functional languages, a distinction that needs to\nbe emphasized to avoid confusion. R initially replicated the S language from\nBell Labs, which in turn was strongly influenced by earlier program libraries.\nAt each stage, new ideas have been added, but the previous software continues\nto show its influence in the design as well. Outlining the evolution will\nfurther clarify why we currently have this somewhat unusual combination of\nideas.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:34:09 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Chambers", "John M.", ""]]}, {"id": "1409.3601", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Vertical-likelihood Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review, we address the use of Monte Carlo methods for approximating\ndefinite integrals of the form $Z = \\int L(x) d P(x)$, where $L$ is a target\nfunction (often a likelihood) and $P$ a finite measure. We present\nvertical-likelihood Monte Carlo, which is an approach for designing the\nimportance function $g(x)$ used in importance sampling. Our approach exploits a\nduality between two random variables: the random draw $X \\sim g$, and the\ncorresponding random likelihood ordinate $Y\\equiv L(X)$ of the draw. It is\nnatural to specify $g(x)$ and ask: what is the the implied distribution of $Y$?\nIn this paper, we take up the opposite question: what should the distribution\nof $Y$ be so that the implied importance function $g(x)$ is good for\napproximating $Z$? Our answer turns out to unite seven seemingly disparate\nclasses of algorithms under the vertical-likelihood perspective: importance\nsampling, slice sampling, simulated annealing/tempering, the harmonic-mean\nestimator, the vertical-density sampler, nested sampling, and energy-level\nsampling (a suite of related methods from statistical physics). In particular,\nwe give an alterate presentation of nested sampling, paying special attention\nto the connection between this method and the vertical-likelihood perspective\narticulated here. As an alternative to nested sampling, we describe an MCMC\nmethod based on re-weighted slice sampling. This method's convergence\nproperties are studied, and two examples demonstrate the promise of the overall\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 21:24:46 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 15:02:15 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1409.3768", "submitter": "Sang-Yun Oh", "authors": "Sang-Yun Oh, Onkar Dalal, Kshitij Khare, Bala Rajaratnam", "title": "Optimization Methods for Sparse Pseudo-Likelihood Graphical Model\n  Selection", "comments": "NIPS accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse high dimensional graphical model selection is a popular topic in\ncontemporary machine learning. To this end, various useful approaches have been\nproposed in the context of $\\ell_1$-penalized estimation in the Gaussian\nframework. Though many of these inverse covariance estimation approaches are\ndemonstrably scalable and have leveraged recent advances in convex\noptimization, they still depend on the Gaussian functional form. To address\nthis gap, a convex pseudo-likelihood based partial correlation graph estimation\nmethod (CONCORD) has been recently proposed. This method uses coordinate-wise\nminimization of a regression based pseudo-likelihood, and has been shown to\nhave robust model selection properties in comparison with the Gaussian\napproach. In direct contrast to the parallel work in the Gaussian setting\nhowever, this new convex pseudo-likelihood framework has not leveraged the\nextensive array of methods that have been proposed in the machine learning\nliterature for convex optimization. In this paper, we address this crucial gap\nby proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for\nperforming $\\ell_1$-regularized inverse covariance matrix estimation in the\npseudo-likelihood framework. We present timing comparisons with coordinate-wise\nminimization and demonstrate that our approach yields tremendous payoffs for\n$\\ell_1$-penalized partial correlation graph estimation outside the Gaussian\nsetting, thus yielding the fastest and most scalable approach for such\nproblems. We undertake a theoretical analysis of our approach and rigorously\ndemonstrate convergence, and also derive rates thereof.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 15:25:07 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Oh", "Sang-Yun", ""], ["Dalal", "Onkar", ""], ["Khare", "Kshitij", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1409.3821", "submitter": "Andrea Montanari", "authors": "Andrea Montanari", "title": "Computational Implications of Reducing Data to Sufficient Statistics", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large dataset and an estimation task, it is common to pre-process the\ndata by reducing them to a set of sufficient statistics. This step is often\nregarded as straightforward and advantageous (in that it simplifies statistical\nanalysis). I show that -on the contrary- reducing data to sufficient statistics\ncan change a computationally tractable estimation problem into an intractable\none. I discuss connections with recent work in theoretical computer science,\nand implications for some techniques to estimate graphical models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 18:57:01 GMT"}, {"version": "v2", "created": "Mon, 15 Sep 2014 16:39:26 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 19:35:44 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Montanari", "Andrea", ""]]}, {"id": "1409.3836", "submitter": "Guy Bresler", "authors": "Guy Bresler, David Gamarnik, and Devavrat Shah", "title": "Hardness of parameter estimation in graphical models", "comments": "15 pages. To appear in NIPS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the canonical parameters specifying an\nundirected graphical model (Markov random field) from the mean parameters. For\ngraphical models representing a minimal exponential family, the canonical\nparameters are uniquely determined by the mean parameters, so the problem is\nfeasible in principle. The goal of this paper is to investigate the\ncomputational feasibility of this statistical task. Our main result shows that\nparameter estimation is in general intractable: no algorithm can learn the\ncanonical parameters of a generic pair-wise binary graphical model from the\nmean parameters in time bounded by a polynomial in the number of variables\n(unless RP = NP). Indeed, such a result has been believed to be true (see the\nmonograph by Wainwright and Jordan (2008)) but no proof was known.\n  Our proof gives a polynomial time reduction from approximating the partition\nfunction of the hard-core model, known to be hard, to learning approximate\nparameters. Our reduction entails showing that the marginal polytope boundary\nhas an inherent repulsive property, which validates an optimization procedure\nover the polytope that does not use any knowledge of its structure (as required\nby the ellipsoid method and others).\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 19:57:59 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 19:57:51 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Bresler", "Guy", ""], ["Gamarnik", "David", ""], ["Shah", "Devavrat", ""]]}, {"id": "1409.3901", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu", "title": "Fast implementation of the Tukey depth", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tukey depth function is one of the most famous multivariate tools serving\nrobust purposes. It is also very well known for its computability problems in\ndimensions $p \\ge 3$. In this paper, we address this computing issue by\npresenting two combinatorial algorithms. The first is naive and calculates the\nTukey depth of a single point with complexity $O\\left(n^{p-1}\\log(n)\\right)$,\nwhile the second further utilizes the quasiconcave of the Tukey depth function\nand hence is more efficient than the first. Both require very minimal memory\nand run much faster than the existing ones. All experiments indicate that they\ncompute the exact Tukey depth.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 02:18:47 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 11:56:38 GMT"}, {"version": "v3", "created": "Wed, 5 Oct 2016 22:14:54 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Liu", "Xiaohui", ""]]}, {"id": "1409.3934", "submitter": "Alex Selby", "authors": "Alex Selby", "title": "Efficient subgraph-based sampling of Ising-type models with frustration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here is proposed a general subgraph-based method for efficiently sampling\ncertain graphical models, typically using subgraphs of a fixed treewidth, and\nalso a related method for finding minimum energy (ground) states. In the case\nof models with frustration, such as the spin glass, evidence is presented that\nthis method can be more efficient than traditional single-site update methods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 11:13:16 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Selby", "Alex", ""]]}, {"id": "1409.4302", "submitter": "Chang-han Rhee", "authors": "Peter W. Glynn and Chang-han Rhee", "title": "Exact Estimation for Markov Chain Equilibrium Expectations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of Monte Carlo methods, which we call exact\nestimation algorithms. Such algorithms provide unbiased estimators for\nequilibrium expectations associated with real- valued functionals defined on a\nMarkov chain. We provide easily implemented algorithms for the class of\npositive Harris recurrent Markov chains, and for chains that are contracting on\naverage. We further argue that exact estimation in the Markov chain setting\nprovides a significant theoretical relaxation relative to exact simulation\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 15:50:26 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Glynn", "Peter W.", ""], ["Rhee", "Chang-han", ""]]}, {"id": "1409.4362", "submitter": "Andrew Golightly", "authors": "Andrew Golightly and Darren J. Wilkinson", "title": "Bayesian inference for Markov jump processes with informative\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of parameter inference for Markov jump\nprocess (MJP) representations of stochastic kinetic models. Since transition\nprobabilities are intractable for most processes of interest yet forward\nsimulation is straightforward, Bayesian inference typically proceeds through\ncomputationally intensive methods such as (particle) MCMC. Such methods\nostensibly require the ability to simulate trajectories from the conditioned\njump process. When observations are highly informative, use of the forward\nsimulator is likely to be inefficient and may even preclude an exact\n(simulation based) analysis. We therefore propose three methods for improving\nthe efficiency of simulating conditioned jump processes. A conditioned hazard\nis derived based on an approximation to the jump process, and used to generate\nend-point conditioned trajectories for use inside an importance sampling\nalgorithm. We also adapt a recently proposed sequential Monte Carlo scheme to\nour problem. Essentially, trajectories are reweighted at a set of intermediate\ntime points, with more weight assigned to trajectories that are consistent with\nthe next observation. We consider two implementations of this approach, based\non two continuous approximations of the MJP. We compare these constructs for a\nsimple tractable jump process before using them to perform inference for a\nLotka-Volterra system. The best performing construct is used to infer the\nparameters governing a simple model of motility regulation in Bacillus\nsubtilis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 18:16:45 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Golightly", "Andrew", ""], ["Wilkinson", "Darren J.", ""]]}, {"id": "1409.5191", "submitter": "Jascha Sohl-Dickstein", "authors": "Jascha Sohl-Dickstein, Mayur Mudigonda, Michael R. DeWeese", "title": "Hamiltonian Monte Carlo Without Detailed Balance", "comments": "Accepted conference submission to ICML 2014 and also featured in a\n  special edition of JMLR. Since updated to include additional literature\n  citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for performing Hamiltonian Monte Carlo that largely\neliminates sample rejection for typical hyperparameters. In situations that\nwould normally lead to rejection, instead a longer trajectory is computed until\na new state is reached that can be accepted. This is achieved using Markov\nchain transitions that satisfy the fixed point equation, but do not satisfy\ndetailed balance. The resulting algorithm significantly suppresses the random\nwalk behavior and wasted function evaluations that are typically the\nconsequence of update rejection. We demonstrate a greater than factor of two\nimprovement in mixing time on three test problems. We release the source code\nas Python and MATLAB packages.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 05:01:07 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 07:36:42 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2015 04:31:19 GMT"}, {"version": "v4", "created": "Wed, 24 Jun 2015 06:43:37 GMT"}, {"version": "v5", "created": "Fri, 25 Mar 2016 21:42:22 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Sohl-Dickstein", "Jascha", ""], ["Mudigonda", "Mayur", ""], ["DeWeese", "Michael R.", ""]]}, {"id": "1409.5676", "submitter": "Gustavo H. Esteves", "authors": "Gustavo H. Esteves and Roberto Hirata Jr", "title": "maigesPack: A Computational Environment for Microarray Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microarray technology is still an important way to assess gene expression in\nmolecular biology, mainly because it measures expression profiles for thousands\nof genes simultaneously, what makes this technology a good option for some\nstudies focused on systems biology. One of its main problem is complexity of\nexperimental procedure, presenting several sources of variability, hindering\nstatistical modeling. So far, there is no standard protocol for generation and\nevaluation of microarray data. To mitigate the analysis process this paper\npresents an R package, named maigesPack, that helps with data organization.\nBesides that, it makes data analysis process more robust, reliable and\nreproducible. Also, maigesPack aggregates several data analysis procedures\nreported in literature, for instance: cluster analysis, differential\nexpression, supervised classifiers, relevance networks and functional\nclassification of gene groups or gene networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 14:27:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 18:07:29 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Esteves", "Gustavo H.", ""], ["Hirata", "Roberto", "Jr"]]}, {"id": "1409.5827", "submitter": "Norm Matloff PhD", "authors": "Norman Matloff", "title": "Software Alchemy: Turning Complex Statistical Computations into\n  Embarrassingly-Parallel Ones", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growth in the use of computationally intensive statistical procedures,\nespecially with Big Data, has necessitated the usage of parallel computation on\ndiverse platforms such as multicore, GPU, clusters and clouds. However,\nslowdown due to interprocess communication costs typically limits such methods\nto \"embarrassingly parallel\" (EP) algorithms, especially on non-shared memory\nplatforms. This paper develops a broadly-applicable method for converting many\nnon-EP algorithms into statistically equivalent EP ones. The method is shown to\nyield excellent levels of speedup for a variety of statistical computations. It\nalso overcomes certain problems of memory limitations.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 22:34:29 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Matloff", "Norman", ""]]}, {"id": "1409.6019", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Paul D. McNicholas", "title": "Robust Clustering in Regression Analysis via the Contaminated Gaussian\n  Cluster-Weighted Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian cluster-weighted model (CWM) is a mixture of regression models\nwith random covariates that allows for flexible clustering of a random vector\ncomposed of response variables and covariates. In each mixture component, it\nadopts a Gaussian distribution for both the covariates and the responses given\nthe covariates. To robustify the approach with respect to possible elliptical\nheavy tailed departures from normality, due to the presence of atypical\nobservations, the contaminated Gaussian CWM is here introduced. In addition to\nthe parameters of the Gaussian CWM, each mixture component of our contaminated\nCWM has a parameter controlling the proportion of outliers, one controlling the\nproportion of leverage points, one specifying the degree of contamination with\nrespect to the response variables, and one specifying the degree of\ncontamination with respect to the covariates. Crucially, these parameters do\nnot have to be specified a priori, adding flexibility to our approach.\nFurthermore, once the model is estimated and the observations are assigned to\nthe groups, a finer intra-group classification in typical points, outliers,\ngood leverage points, and bad leverage points - concepts of primary importance\nin robust regression analysis - can be directly obtained. Relations with other\nmixture-based contaminated models are analyzed, identifiability conditions are\nprovided, an expectation-conditional maximization algorithm is outlined for\nparameter estimation, and various implementation and operational issues are\ndiscussed. Properties of the estimators of the regression coefficients are\nevaluated through Monte Carlo experiments and compared to the estimators from\nthe Gaussian CWM. A sensitivity study is also conducted based on a real data\nset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 17:31:35 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1409.6994", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella", "title": "Bayesian complementary clustering, MCMC and Anglo-Saxon placenames", "comments": "33 pages, 13 figures. Version 4: minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common cluster models for multi-type point processes model the aggregation of\npoints of the same type. In complete contrast, in the study of Anglo-Saxon\nsettlements it is hypothesized that administrative clusters involving\ncomplementary names tend to appear. We investigate the evidence for such an\nhypothesis by developing a Bayesian Random Partition Model based on clusters\nformed by points of different types (complementary clustering).\n  As a result we obtain an intractable posterior distribution on the space of\nmatchings contained in a k-partite hypergraph. We apply the Metropolis-Hastings\n(MH) algorithm to sample from this posterior. We consider the problem of\nchoosing an efficient MH proposal distribution and we obtain consistent mixing\nimprovements compared to the choices found in the literature. Simulated\nTempering techniques can be used to overcome multimodality and a multiple\nproposal scheme is developed to allow for parallel programming. Finally, we\ndiscuss results arising from the careful use of convergence diagnostic\ntechniques.\n  This allows us to study a dataset including locations and placenames of 1316\nAnglo-Saxon settlements dated approximately around 750-850 AD. Without strong\nprior knowledge, the model allows for explicit estimation of the number of\nclusters, the average intra-cluster dispersion and the level of interaction\namong placenames. The results support the hypothesis of organization of\nsettlements into administrative clusters based on complementary names.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 15:36:28 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 18:53:41 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 11:27:58 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 14:47:23 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Zanella", "Giacomo", ""]]}, {"id": "1409.7089", "submitter": "Pranay Seshadri", "authors": "Pranay Seshadri, Paul Constantine, Gianluca Iaccarino, Geoffrey Parks", "title": "A density-matching approach for optimization under uncertainty", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern computers enable methods for design optimization that account for\nuncertainty in the system---so-called optimization under uncertainty. We\npropose a metric for OUU that measures the distance between a\ndesigner-specified probability density function of the system response the\ntarget and system response's density function at a given design. We study an\nOUU formulation that minimizes this distance metric over all designs. We\ndiscretize the objective function with numerical quadrature and approximate the\nresponse density function with a Gaussian kernel density estimate. We offer\nheuristics for addressing issues that arise in this formulation, and we apply\nthe approach to a CFD-based airfoil shape optimization problem. We\nqualitatively compare the density-matching approach to a multi-objective robust\ndesign optimization to gain insight into the method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:35:16 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 20:47:37 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Seshadri", "Pranay", ""], ["Constantine", "Paul", ""], ["Iaccarino", "Gianluca", ""], ["Parks", "Geoffrey", ""]]}, {"id": "1409.7287", "submitter": "Andreas Svensson", "authors": "Andreas Svensson, Thomas B. Sch\\\"on, Fredrik Lindsten", "title": "Identification of jump Markov linear models using particle filters", "comments": "Accepted to 53rd IEEE International Conference on Decision and\n  Control (CDC), 2014 (Los Angeles, CA, USA)", "journal-ref": "Proc. of IEEE 53rd Conference on Decision and Control (CDC),\n  pp.6504,6509, 15-17 Dec. 2014 (Los Angeles, CA, USA)", "doi": "10.1109/CDC.2014.7040409", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Jump Markov linear models consists of a finite number of linear state space\nmodels and a discrete variable encoding the jumps (or switches) between the\ndifferent linear models. Identifying jump Markov linear models makes for a\nchallenging problem lacking an analytical solution. We derive a new expectation\nmaximization (EM) type algorithm that produce maximum likelihood estimates of\nthe model parameters. Our development hinges upon recent progress in combining\nparticle filters with Markov chain Monte Carlo methods in solving the nonlinear\nstate smoothing problem inherent in the EM formulation. Key to our development\nis that we exploit a conditionally linear Gaussian substructure in the model,\nallowing for an efficient algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 15:18:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Svensson", "Andreas", ""], ["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1409.7715", "submitter": "Libo Sun", "authors": "Libo Sun, Chihoon Lee, Jennifer A. Hoeting", "title": "Parameter inference and model selection in deterministic and stochastic\n  dynamical models via approximate Bayesian computation: modeling a wildlife\n  epidemic", "comments": "24 pages, 4 figures, submitted", "journal-ref": "Environmetrics 2015, 26: 451-462", "doi": "10.1002/env.2353", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting deterministic or stochastic models for a\nbiological, ecological, or environmental dynamical process. In most cases, one\nprefers either deterministic or stochastic models as candidate models based on\nexperience or subjective judgment. Due to the complex or intractable likelihood\nin most dynamical models, likelihood-based approaches for model selection are\nnot suitable. We use approximate Bayesian computation for parameter estimation\nand model selection to gain further understanding of the dynamics of two\nepidemics of chronic wasting disease in mule deer. The main novel contribution\nof this work is that under a hierarchical model framework we compare three\ntypes of dynamical models: ordinary differential equation, continuous time\nMarkov chain, and stochastic differential equation models. To our knowledge\nmodel selection between these types of models has not appeared previously.\nSince the practice of incorporating dynamical models into data models is\nbecoming more common, the proposed approach may be very useful in a variety of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 20:33:25 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 16:39:41 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Sun", "Libo", ""], ["Lee", "Chihoon", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1409.7852", "submitter": "Sivaram Ambikasaran", "authors": "Sivaram Ambikasaran", "title": "Generalized Rybicki Press algorithm", "comments": "13 pages, 11 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.DS cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article discusses a more general and numerically stable Rybicki Press\nalgorithm, which enables inverting and computing determinants of covariance\nmatrices, whose elements are sums of exponentials. The algorithm is true in\nexact arithmetic and relies on introducing new variables and corresponding\nequations, thereby converting the matrix into a banded matrix of larger size.\nLinear complexity banded algorithms for solving linear systems and computing\ndeterminants on the larger matrix enable linear complexity algorithms for the\ninitial semi-separable matrix as well. Benchmarks provided illustrate the\nlinear scaling of the algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 22:24:03 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 01:10:27 GMT"}, {"version": "v3", "created": "Wed, 8 Oct 2014 20:31:33 GMT"}, {"version": "v4", "created": "Sat, 2 May 2015 02:35:51 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Ambikasaran", "Sivaram", ""]]}, {"id": "1409.8047", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Ian A. Wood", "title": "Asymptotic Normality of the Maximum Pseudolikelihood Estimator for Fully\n  Visible Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boltzmann machines (BMs) are a class of binary neural networks for which\nthere have been numerous proposed methods of estimation. Recently, it has been\nshown that in the fully visible case of the BM, the method of maximum\npseudolikelihood estimation (MPLE) results in parameter estimates which are\nconsistent in the probabilistic sense. In this article, we investigate the\nproperties of MPLE for the fully visible BMs further, and prove that MPLE also\nyields an asymptotically normal parameter estimator. These results can be used\nto construct confidence intervals and to test statistical hypotheses. We\nsupport our theoretical results by showing that the estimator behaves as\nexpected in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 09:35:46 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Wood", "Ian A.", ""]]}, {"id": "1409.8083", "submitter": "Beyza Ermis Ms", "authors": "Beyza Ermis, Y. Kenan Y{\\i}lmaz, A. Taylan Cemgil, Evrim Acar", "title": "Variational Inference For Probabilistic Latent Tensor Factorization with\n  KL Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Latent Tensor Factorization (PLTF) is a recently proposed\nprobabilistic framework for modelling multi-way data. Not only the common\ntensor factorization models but also any arbitrary tensor factorization\nstructure can be realized by the PLTF framework. This paper presents full\nBayesian inference via variational Bayes that facilitates more powerful\nmodelling and allows more sophisticated inference on the PLTF framework. We\nillustrate our approach on model order selection and link prediction.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 11:45:36 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Ermis", "Beyza", ""], ["Y\u0131lmaz", "Y. Kenan", ""], ["Cemgil", "A. Taylan", ""], ["Acar", "Evrim", ""]]}, {"id": "1409.8129", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Marcelo Pereyra and Jose Bioucas-Dias", "title": "Collaborative sparse regression using spatially correlated supports -\n  Application to hyperspectral unmixing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Bayesian collaborative sparse regression method for\nlinear unmixing of hyperspectral images. Our contribution is twofold; first, we\npropose a new Bayesian model for structured sparse regression in which the\nsupports of the sparse abundance vectors are a priori spatially correlated\nacross pixels (i.e., materials are spatially organised rather than randomly\ndistributed at a pixel level). This prior information is encoded in the model\nthrough a truncated multivariate Ising Markov random field, which also takes\ninto consideration the facts that pixels cannot be empty (i.e, there is at\nleast one material present in each pixel), and that different materials may\nexhibit different degrees of spatial regularity. Secondly, we propose an\nadvanced Markov chain Monte Carlo algorithm to estimate the posterior\nprobabilities that materials are present or absent in each pixel, and,\nconditionally to the maximum marginal a posteriori configuration of the\nsupport, compute the MMSE estimates of the abundance vectors. A remarkable\nproperty of this algorithm is that it self-adjusts the values of the parameters\nof the Markov random field, thus relieving practitioners from setting\nregularisation parameters by cross-validation. The performance of the proposed\nmethodology is finally demonstrated through a series of experiments with\nsynthetic and real data and comparisons with other algorithms from the\nliterature.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 14:10:16 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2015 10:57:40 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Altmann", "Yoann", ""], ["Pereyra", "Marcelo", ""], ["Bioucas-Dias", "Jose", ""]]}, {"id": "1409.8363", "submitter": "Gael Martin Prof", "authors": "Gael M. Martin, Brendan P.M. McCabe, Worapree Maneesoonthorn and\n  Christian P. Robert", "title": "Approximate Bayesian Computation in State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to inference in state space models is proposed, based on\napproximate Bayesian computation (ABC). ABC avoids evaluation of the likelihood\nfunction by matching observed summary statistics with statistics computed from\ndata simulated from the true process; exact inference being feasible only if\nthe statistics are sufficient. With finite sample sufficiency unattainable in\nthe state space setting, we seek asymptotic sufficiency via the maximum\nlikelihood estimator (MLE) of the parameters of an auxiliary model. We prove\nthat this auxiliary model-based approach achieves Bayesian consistency, and\nthat - in a precise limiting sense - the proximity to (asymptotic) sufficiency\nyielded by the MLE is replicated by the score. In multiple parameter settings a\nseparate treatment of scalar parameters, based on integrated likelihood\ntechniques, is advocated as a way of avoiding the curse of dimensionality. Some\nattention is given to a structure in which the state variable is driven by a\ncontinuous time process, with exact inference typically infeasible in this case\nas a result of intractable transitions. The ABC method is demonstrated using\nthe unscented Kalman filter as a fast and simple way of producing an\napproximation in this setting, with a stochastic volatility model for financial\nreturns used for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 02:11:25 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Martin", "Gael M.", ""], ["McCabe", "Brendan P. M.", ""], ["Maneesoonthorn", "Worapree", ""], ["Robert", "Christian P.", ""]]}, {"id": "1409.8502", "submitter": "Juho Kokkala", "authors": "Juho Kokkala and Simo S\\\"arkk\\\"a", "title": "Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking", "comments": "Revised version. 43 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.dsp.2015.04.004", "report-no": null, "categories": "stat.ME math.DS math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:59:41 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 21:03:46 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kokkala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1409.8542", "submitter": "Gerko Vink", "authors": "Gerko Vink and Stef van Buuren", "title": "Pooling multiple imputations when the sample happens to be the\n  population", "comments": "6 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current pooling rules for multiply imputed data assume infinite populations.\nIn some situations this assumption is not feasible as every unit in the\npopulation has been observed, potentially leading to over-covered population\nestimates. We simplify the existing pooling rules for situations where the\nsampling variance is not of interest. We compare these rules to the\nconventional pooling rules and demonstrate their use in a situation where there\nis no sampling variance. Using the standard pooling rules in situations where\nsampling variance should not be considered, leads to overestimation of the\nvariance of the estimates of interest, especially when the amount of\nmissingness is not very large. As a result, populations estimates are\nover-covered, which may lead to a loss of statistical power. We conclude that\nthe theory of multiple imputation can be extended to the situation where the\nsample happens to be the population. The simplified pooling rules can be easily\nimplemented to obtain valid inference in cases where we have observed\nessentially all units and in simulation studies addressing the missingness\nmechanism only.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 13:43:14 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Vink", "Gerko", ""], ["van Buuren", "Stef", ""]]}]