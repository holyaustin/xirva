[{"id": "1101.0387", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "MCMC Using Ensembles of States for Problems with Fast and Slow Variables\n  such as Gaussian Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I introduce a Markov chain Monte Carlo (MCMC) scheme in which sampling from a\ndistribution with density pi(x) is done using updates operating on an\n\"ensemble\" of states. The current state x is first stochastically mapped to an\nensemble, x^{(1)},...,x^{(K)}. This ensemble is then updated using MCMC updates\nthat leave invariant a suitable ensemble density, rho(x^{(1)},...,x^{(K)}),\ndefined in terms of pi(x^{(i)}) for i=1,...,K. Finally a single state is\nstochastically selected from the ensemble after these updates. Such ensemble\nMCMC updates can be useful when characteristics of pi and the ensemble permit\npi(x^{(i)}) for all i in {1,...,K}, to be computed in less than K times the\namount of computation time needed to compute pi(x) for a single x. One common\nsituation of this type is when changes to some \"fast\" variables allow for quick\nre-computation of the density, whereas changes to other \"slow\" variables do\nnot. Gaussian process regression models are an example of this sort of problem,\nwith an overall scaling factor for covariances and the noise variance being\nfast variables. I show that ensemble MCMC for Gaussian process regression\nmodels can indeed substantially improve sampling performance. Finally, I\ndiscuss other possible applications of ensemble MCMC, and its relationship to\nthe \"multiple-try Metropolis\" method of Liu, Liang, and Wong and the \"multiset\nsampler\" of Leman, Chen, and Lavine.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 05:27:11 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Neal", "Radford M.", ""]]}, {"id": "1101.0955", "submitter": "Jean-Michel Marin", "authors": "Jean-Michel Marin (I3M), Pierre Pudlo (I3M), Christian P. Robert\n  (University Paris-Dauphine and CREST) and Robin Ryder (CREST)", "title": "Approximate Bayesian Computational methods", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Also known as likelihood-free methods, approximate Bayesian computational\n(ABC) methods have appeared in the past ten years as the most satisfactory\napproach to untractable likelihood problems, first in genetics then in a\nbroader spectrum of applications. However, these methods suffer to some degree\nfrom calibration difficulties that make them rather volatile in their\nimplementation and thus render them suspicious to the users of more traditional\nMonte Carlo methods. In this survey, we study the various improvements and\nextensions made to the original ABC algorithm over the recent years.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 12:52:12 GMT"}, {"version": "v2", "created": "Fri, 27 May 2011 12:56:57 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Marin", "Jean-Michel", "", "I3M"], ["Pudlo", "Pierre", "", "I3M"], ["Robert", "Christian P.", "", "University Paris-Dauphine and CREST"], ["Ryder", "Robin", "", "CREST"]]}, {"id": "1101.1136", "submitter": "Benedict Escoto", "authors": "Benedict Escoto", "title": "Marginal Likelihood Computation via Arrogance Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper describes a method for estimating the marginal likelihood or Bayes\nfactors of Bayesian models using non-parametric importance sampling (\"arrogance\nsampling\"). This method can also be used to compute the normalizing constant of\nprobability distributions. Because the required inputs are samples from the\ndistribution to be normalized and the scaled density at those samples, this\nmethod may be a convenient replacement for the harmonic mean estimator. The\nmethod has been implemented in the open source R package margLikArrogance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jan 2011 04:30:27 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Escoto", "Benedict", ""]]}, {"id": "1101.1264", "submitter": "Garfield Brown", "authors": "Garfield Brown and Steve Brooks", "title": "Bayesian Analysis of Loss Ratios Using the Reversible Jump Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of model choice for a set of insurance\nloss ratios. We use a reversible jump algorithm for our model discrimination\nand show how the vanilla reversible jump algorithm can be improved on using\nrecent methodological advances in reversible jump computation.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jan 2011 17:40:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Brown", "Garfield", ""], ["Brooks", "Steve", ""]]}, {"id": "1101.1528", "submitter": "Pierre E. Jacob", "authors": "Nicolas Chopin, Pierre E. Jacob and Omiros Papaspiliopoulos", "title": "SMC^2: an efficient algorithm for sequential analysis of state-space\n  models", "comments": "27 pages, 4 figures; supplementary material available on the second\n  author's web page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the generic problem of performing sequential Bayesian inference\nin a state-space model with observation process y, state process x and fixed\nparameter theta. An idealized approach would be to apply the iterated batch\nimportance sampling (IBIS) algorithm of Chopin (2002). This is a sequential\nMonte Carlo algorithm in the theta-dimension, that samples values of theta,\nreweights iteratively these values using the likelihood increments\np(y_t|y_1:t-1, theta), and rejuvenates the theta-particles through a resampling\nstep and a MCMC update step. In state-space models these likelihood increments\nare intractable in most cases, but they may be unbiasedly estimated by a\nparticle filter in the x-dimension, for any fixed theta. This motivates the\nSMC^2 algorithm proposed in this article: a sequential Monte Carlo algorithm,\ndefined in the theta-dimension, which propagates and resamples many particle\nfilters in the x-dimension. The filters in the x-dimension are an example of\nthe random weight particle filter as in Fearnhead et al. (2010). On the other\nhand, the particle Markov chain Monte Carlo (PMCMC) framework developed in\nAndrieu et al. (2010) allows us to design appropriate MCMC rejuvenation steps.\nThus, the theta-particles target the correct posterior distribution at each\niteration t, despite the intractability of the likelihood increments. We\nexplore the applicability of our algorithm in both sequential and\nnon-sequential applications and consider various degrees of freedom, as for\nexample increasing dynamically the number of x-particles. We contrast our\napproach to various competing methods, both conceptually and empirically\nthrough a detailed simulation study, included here and in a supplement, and\nbased on particularly challenging examples.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jan 2011 21:18:22 GMT"}, {"version": "v2", "created": "Thu, 24 Feb 2011 22:06:52 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2012 18:55:03 GMT"}], "update_date": "2012-01-30", "authors_parsed": [["Chopin", "Nicolas", ""], ["Jacob", "Pierre E.", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1101.2374", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and Camille Brunet", "title": "Simultaneous model-based clustering and visualization in the Fisher\n  discriminative subspace", "comments": null, "journal-ref": "Statistics and Computing, 2011", "doi": "10.1007/s11222-011-9249-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering in high-dimensional spaces is nowadays a recurrent problem in many\nscientific domains but remains a difficult task from both the clustering\naccuracy and the result understanding points of view. This paper presents a\ndiscriminative latent mixture (DLM) model which fits the data in a latent\northonormal discriminative subspace with an intrinsic dimension lower than the\ndimension of the original space. By constraining model parameters within and\nbetween groups, a family of 12 parsimonious DLM models is exhibited which\nallows to fit onto various situations. An estimation algorithm, called the\nFisher-EM algorithm, is also proposed for estimating both the mixture\nparameters and the discriminative subspace. Experiments on simulated and real\ndatasets show that the proposed approach performs better than existing\nclustering methods while providing a useful representation of the clustered\ndata. The method is as well applied to the clustering of mass spectrometry\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 14:40:06 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 08:21:44 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Bouveyron", "Charles", ""], ["Brunet", "Camille", ""]]}, {"id": "1101.3214", "submitter": "Mehdi Molkaraie", "authors": "Giovanni Sabato and Mehdi Molkaraie", "title": "Generalized Belief Propagation for the Noiseless Capacity and\n  Information Rates of Run-Length Limited Constraints", "comments": "8 pages, 11 figures", "journal-ref": "IEEE Transactions on Communications, Volume 60, March 2012, pages\n  669 - 675", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of the generalized belief propagation algorithm for computing\nthe noiseless capacity and mutual information rates of finite-size\ntwo-dimensional and three-dimensional run-length limited constraints is\ninvestigated. For each constraint, a method is proposed to choose the basic\nregions and to construct the region graph. Simulation results for the capacity\nof different constraints as a function of the size of the channel and mutual\ninformation rates of different constraints as a function of signal-to-noise\nratio are reported. Convergence to the Shannon capacity is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 14:01:13 GMT"}, {"version": "v2", "created": "Fri, 27 May 2011 12:26:19 GMT"}], "update_date": "2012-05-29", "authors_parsed": [["Sabato", "Giovanni", ""], ["Molkaraie", "Mehdi", ""]]}, {"id": "1101.4179", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot, Peggy C\\'enac, Jean-Marie Monnez", "title": "A fast and recursive algorithm for clustering large datasets with\n  $k$-medians", "comments": "Under revision for Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering with fast algorithms large samples of high dimensional data is an\nimportant challenge in computational statistics. Borrowing ideas from MacQueen\n(1967) who introduced a sequential version of the $k$-means algorithm, a new\nclass of recursive stochastic gradient algorithms designed for the $k$-medians\nloss criterion is proposed. By their recursive nature, these algorithms are\nvery fast and are well adapted to deal with large samples of data that are\nallowed to arrive sequentially. It is proved that the stochastic gradient\nalgorithm converges almost surely to the set of stationary points of the\nunderlying loss criterion. A particular attention is paid to the averaged\nversions, which are known to have better performances, and a data-driven\nprocedure that allows automatic selection of the value of the descent step is\nproposed.\n  The performance of the averaged sequential estimator is compared on a\nsimulation study, both in terms of computation speed and accuracy of the\nestimations, with more classical partitioning techniques such as $k$-means,\ntrimmed $k$-means and PAM (partitioning around medoids). Finally, this new\nonline clustering technique is illustrated on determining television audience\nprofiles with a sample of more than 5000 individual television audiences\nmeasured every minute over a period of 24 hours.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 16:48:51 GMT"}, {"version": "v2", "created": "Mon, 18 Jul 2011 10:54:46 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 12:19:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["C\u00e9nac", "Peggy", ""], ["Monnez", "Jean-Marie", ""]]}, {"id": "1101.4242", "submitter": "Jarad Niemi", "authors": "Jarad Niemi and Matthew Wheeler", "title": "Efficient Bayesian inference in stochastic chemical kinetic models using\n  graphical processing units", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal of systems biology is to understand the dynamics of intracellular\nsystems. Stochastic chemical kinetic models are often utilized to accurately\ncapture the stochastic nature of these systems due to low numbers of molecules.\nCollecting system data allows for estimation of stochastic chemical kinetic\nrate parameters. We describe a well-known, but typically impractical data\naugmentation Markov chain Monte Carlo algorithm for estimating these\nparameters. The impracticality is due to the use of rejection sampling for\nlatent trajectories with fixed initial and final endpoints which can have\ndiminutive acceptance probability. We show how graphical processing units can\nbe efficiently utilized for parameter estimation in systems that hitherto were\ninestimable. For more complex systems, we show the efficiency gain over\ntraditional CPU computing is on the order of 200. Finally, we show a Bayesian\nanalysis of a system based on Michaelis-Menton kinetics.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 22:57:52 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Niemi", "Jarad", ""], ["Wheeler", "Matthew", ""]]}, {"id": "1101.4373", "submitter": "Klaus Frick", "authors": "Klaus Frick, Philipp Marnitz, Axel Munk", "title": "Statistical Multiresolution Dantzig Estimation in Imaging: Fundamental\n  Concepts and Algorithmic Framework", "comments": null, "journal-ref": "Electron. J. Stat. 6 (2012) 231-268", "doi": "10.1214/12-EJS671", "report-no": null, "categories": "stat.AP cs.CV cs.SY math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are concerned with fully automatic and locally adaptive\nestimation of functions in a \"signal + noise\"-model where the regression\nfunction may additionally be blurred by a linear operator, e.g. by a\nconvolution. To this end, we introduce a general class of statistical\nmultiresolution estimators and develop an algorithmic framework for computing\nthose. By this we mean estimators that are defined as solutions of convex\noptimization problems with supremum-type constraints. We employ a combination\nof the alternating direction method of multipliers with Dykstra's algorithm for\ncomputing orthogonal projections onto intersections of convex sets and prove\nnumerical convergence. The capability of the proposed method is illustrated by\nvarious examples from imaging and signal detection.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jan 2011 13:46:43 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2011 00:24:44 GMT"}, {"version": "v3", "created": "Wed, 1 Feb 2012 09:25:47 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Frick", "Klaus", ""], ["Marnitz", "Philipp", ""], ["Munk", "Axel", ""]]}, {"id": "1101.5091", "submitter": "Christian P. Robert", "authors": "Christian Robert (Universite Paris Dauphine), Jean-Michel Marin\n  (Universite de Montpellier 2) and Natesh S. Pillai (Harvard University)", "title": "Why approximate Bayesian computational (ABC) methods cannot handle model\n  choice problems", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC), also known as likelihood-free\nmethods, have become a favourite tool for the analysis of complex stochastic\nmodels, primarily in population genetics but also in financial analyses. We\nadvocated in Grelaud et al. (2009) the use of ABC for Bayesian model choice in\nthe specific case of Gibbs random fields (GRF), relying on a sufficiency\nproperty mainly enjoyed by GRFs to show that the approach was legitimate.\nDespite having previously suggested the use of ABC for model choice in a wider\nrange of models in the DIY ABC software (Cornuet et al., 2008), we present\ntheoretical evidence that the general use of ABC for model choice is fraught\nwith danger in the sense that no amount of computation, however large, can\nguarantee a proper approximation of the posterior probabilities of the models\nunder comparison.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jan 2011 15:51:55 GMT"}, {"version": "v2", "created": "Thu, 27 Jan 2011 06:32:36 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Robert", "Christian", "", "Universite Paris Dauphine"], ["Marin", "Jean-Michel", "", "Universite de Montpellier 2"], ["Pillai", "Natesh S.", "", "Harvard University"]]}, {"id": "1101.5837", "submitter": "Krzysztof Latuszynski", "authors": "Krzysztof Latuszynski, Blazej Miasojedow, Wojciech Niemiro", "title": "Nonasymptotic bounds on the mean square error for MCMC estimates via\n  renewal techniques", "comments": null, "journal-ref": "for MCQMC 2010 Conference Proceeding", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nummellin's split chain construction allows to decompose a Markov chain\nMonte Carlo (MCMC) trajectory into i.i.d. \"excursions\". RegenerativeMCMC\nalgorithms based on this technique use a random number of samples. They have\nbeen proposed as a promising alternative to usual fixed length simulation [25,\n33, 14]. In this note we derive nonasymptotic bounds on the mean square error\n(MSE) of regenerative MCMC estimates via techniques of renewal theory and\nsequential statistics. These results are applied to costruct confidence\nintervals. We then focus on two cases of particular interest: chains satisfying\nthe Doeblin condition and a geometric drift condition. Available explicit\nnonasymptotic results are compared for different schemes of MCMC simulation.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 00:27:45 GMT"}, {"version": "v2", "created": "Thu, 12 May 2011 18:19:59 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Latuszynski", "Krzysztof", ""], ["Miasojedow", "Blazej", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "1101.5838", "submitter": "Krzysztof {\\L}atuszy\\'{n}ski", "authors": "Krzysztof {\\L}atuszy\\'nski, Gareth O. Roberts, Jeffrey S. Rosenthal", "title": "Adaptive Gibbs samplers and related MCMC methods", "comments": "Published in at http://dx.doi.org/10.1214/11-AAP806 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org). arXiv admin note:\n  substantial text overlap with arXiv:1001.2797", "journal-ref": "Annals of Applied Probability 2013, Vol. 23, No. 1, 66-98", "doi": "10.1214/11-AAP806", "report-no": "IMS-AAP-AAP806", "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider various versions of adaptive Gibbs and Metropolis-within-Gibbs\nsamplers, which update their selection probabilities (and perhaps also their\nproposal distributions) on the fly during a run by learning as they go in an\nattempt to optimize the algorithm. We present a cautionary example of how even\na simple-seeming adaptive Gibbs sampler may fail to converge. We then present\nvarious positive results guaranteeing convergence of adaptive Gibbs samplers\nunder certain conditions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 00:44:45 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2013 13:02:27 GMT"}], "update_date": "2013-02-28", "authors_parsed": [["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1101.6037", "submitter": "Christian Schafer", "authors": "Christian Sch\\\"afer (CREST, CEREMADE), Nicolas Chopin (CREST, ENSAE)", "title": "Sequential Monte Carlo on large binary sampling spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Monte Carlo algorithm is said to be adaptive if it automatically calibrates\nits current proposal distribution using past simulations. The choice of the\nparametric family that defines the set of proposal distributions is critical\nfor good performance. In this paper, we present such a parametric family for\nadaptive sampling on high-dimensional binary spaces. A practical motivation for\nthis problem is variable selection in a linear regression context. We want to\nsample from a Bayesian posterior distribution on the model space using an\nappropriate version of Sequential Monte Carlo. Raw versions of Sequential Monte\nCarlo are easily implemented using binary vectors with independent components.\nFor high-dimensional problems, however, these simple proposals do not yield\nsatisfactory results. The key to an efficient adaptive algorithm are binary\nparametric families which take correlations into account, analogously to the\nmultivariate normal distribution on continuous spaces. We provide a review of\nmodels for binary data and make one of them work in the context of Sequential\nMonte Carlo sampling. Computational studies on real life data with about a\nhundred covariates suggest that, on difficult instances, our Sequential Monte\nCarlo approach clearly outperforms standard techniques based on Markov chain\nexploration.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 17:47:32 GMT"}, {"version": "v2", "created": "Thu, 3 Feb 2011 16:47:02 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2011 17:23:41 GMT"}, {"version": "v4", "created": "Wed, 9 Nov 2011 09:04:40 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Sch\u00e4fer", "Christian", "", "CREST, CEREMADE"], ["Chopin", "Nicolas", "", "CREST, ENSAE"]]}]