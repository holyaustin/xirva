[{"id": "2001.00084", "submitter": "Ravi Goyal", "authors": "Ravi Goyal and Victor De Gruttola", "title": "Recursive Formula for Labeled Graph Enumeration", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This manuscript presents a general recursive formula to estimate the size of\nfibers associated with algebraic maps from graphs to summary statistics of\nimportance for social network analysis, such as number of edges (graph\ndensity), degree sequence, degree distribution, mixing by nodal covariates, and\ndegree mixing. That is, the formula estimates the number of labeled graphs that\nhave given values for network properties. The proposed approach can be extended\nto additional network properties (e.g., clustering) as well as properties of\nbipartite networks. For special settings in which alternative formulas exist,\nsimulation studies demonstrate the validity of the proposed approach. We\nillustrate the approach for estimating the size of fibers associated with the\nBarab\\'{a}si--Albert model for the properties of degree distribution and degree\nmixing. In addition, we demonstrate how the approach can be used to assess the\ndiversity of graphs within a fiber.\n", "versions": [{"version": "v1", "created": "Tue, 31 Dec 2019 21:15:54 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Goyal", "Ravi", ""], ["De Gruttola", "Victor", ""]]}, {"id": "2001.00996", "submitter": "Athanasios Rakitzis", "authors": "P. H. Tran and A. C. Rakitzis and H. D. Nguyen and Q. T. Nguyen and K.\n  P. Tran and C. Heuchenne", "title": "Monitoring the Multivariate Coefficient of Variation using Run Rules\n  Type Control Charts", "comments": "27 pages, 4 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In practice, there are processes where the in-control mean and standard\ndeviation of a quality characteristic is not stable. In such cases, the\ncoefficient of variation (CV) is a more appropriate measure for assessing\nprocess stability. In this paper, we consider the statistical design of Run\nRules based control charts for monitoring the CV of multivariate data. A Markov\nchain approach is used to evaluate the statistical performance of the proposed\ncharts. The computational results show that the Run Rules based charts\noutperform significantly the standard Shewhart control chart. Moreover, by\nchoosing an appropriate scheme, the Run Rules based charts perform better than\nthe Rum Sum control chart for monitoring the multivariate CV. An example in a\nspring manufacturing process is given to illustrate the implementation of the\nproposed charts.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 21:51:43 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2020 13:25:58 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "P. H.", ""], ["Rakitzis", "A. C.", ""], ["Nguyen", "H. D.", ""], ["Nguyen", "Q. T.", ""], ["Tran", "K. P.", ""], ["Heuchenne", "C.", ""]]}, {"id": "2001.01373", "submitter": "Thomas Catanach", "authors": "Thomas A. Catanach, Huy D. Vo, Brian Munsky", "title": "Bayesian inference of Stochastic reaction networks using Multifidelity\n  Sequential Tempered Markov Chain Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic reaction network models are often used to explain and predict the\ndynamics of gene regulation in single cells. These models usually involve\nseveral parameters, such as the kinetic rates of chemical reactions, that are\nnot directly measurable and must be inferred from experimental data. Bayesian\ninference provides a rigorous probabilistic framework for identifying these\nparameters by finding a posterior parameter distribution that captures their\nuncertainty. Traditional computational methods for solving inference problems\nsuch as Markov Chain Monte Carlo methods based on classical Metropolis-Hastings\nalgorithm involve numerous serial evaluations of the likelihood function, which\nin turn requires expensive forward solutions of the chemical master equation\n(CME). We propose an alternative approach based on a multifidelity extension of\nthe Sequential Tempered Markov Chain Monte Carlo (ST-MCMC) sampler. This\nalgorithm is built upon Sequential Monte Carlo and solves the Bayesian\ninference problem by decomposing it into a sequence of efficiently solved\nsubproblems that gradually increase model fidelity and the influence of the\nobserved data. We reformulate the finite state projection (FSP) algorithm, a\nwell-known method for solving the CME, to produce a hierarchy of surrogate\nmaster equations to be used in this multifidelity scheme. To determine the\nappropriate fidelity, we introduce a novel information-theoretic criteria that\nseeks to extract the most information about the ultimate Bayesian posterior\nfrom each model in the hierarchy without inducing significant bias. This novel\nsampling scheme is tested with high performance computing resources using\nbiologically relevant problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 02:57:58 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Catanach", "Thomas A.", ""], ["Vo", "Huy D.", ""], ["Munsky", "Brian", ""]]}, {"id": "2001.01532", "submitter": "Miryam Merk", "authors": "Miryam S. Merk and Philipp Otto", "title": "Estimation of the spatial weighting matrix for regular lattice data --\n  An adaptive lasso approach with cross-sectional resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial econometric research typically relies on the assumption that the\nspatial dependence structure is known in advance and is represented by a\ndeterministic spatial weights matrix. Contrary to classical approaches, we\ninvestigate the estimation of sparse spatial dependence structures for regular\nlattice data. In particular, an adaptive least absolute shrinkage and selection\noperator (lasso) is used to select and estimate the individual connections of\nthe spatial weights matrix. To recover the spatial dependence structure, we\npropose cross-sectional resampling, assuming that the random process is\nexchangeable. The estimation procedure is based on a two-step approach to\ncircumvent simultaneity issues that typically arise from endogenous spatial\nautoregressive dependencies. The two-step adaptive lasso approach with\ncross-sectional resampling is verified using Monte Carlo simulations.\nEventually, we apply the procedure to model nitrogen dioxide ($\\mathrm{NO_2}$)\nconcentrations and show that estimating the spatial dependence structure\ncontrary to using prespecified weights matrices improves the prediction\naccuracy considerably.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 12:51:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Merk", "Miryam S.", ""], ["Otto", "Philipp", ""]]}, {"id": "2001.01702", "submitter": "Cyrille Mascart", "authors": "Cyrille Mascart, Alexandre Muzy, Patricia Reynaud-bouret", "title": "Efficient Simulation of Sparse Graphs of Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We derive new discrete event simulation algorithms for marked time point\nprocesses. The main idea is to couple a special structure, namely the\nassociated local independence graph, as defined by Didelez arXiv:0710.5874,\nwith the activity tracking algorithm [muzy, 2019] for achieving high\nperformance asynchronous simulations. With respect to classical algorithm, this\nallows reducing drastically the computational complexity, especially when the\ngraph is sparse.\n  [muzy, 2019] A. Muzy. 2019. Exploiting activity for the modeling and\nsimulation of dynamics and learning processes in hierarchical (neurocognitive)\nsystems. (Submitted to) Magazine of Computing in Science & Engineering (2019)\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 18:31:01 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 11:20:20 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 11:51:33 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 07:52:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Mascart", "Cyrille", ""], ["Muzy", "Alexandre", ""], ["Reynaud-bouret", "Patricia", ""]]}, {"id": "2001.01805", "submitter": "Antoni Musolas", "authors": "Antoni Musolas, Steven T. Smith, Youssef Marzouk", "title": "Geodesically parameterized covariance estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling of spatiotemporal phenomena often requires selecting a\ncovariance matrix from a covariance class. Yet standard parametric covariance\nfamilies can be insufficiently flexible for practical applications, while\nnon-parametric approaches may not easily allow certain kinds of prior knowledge\nto be incorporated. We propose instead to build covariance families out of\ngeodesic curves. These covariances offer more flexibility for problem-specific\ntailoring than classical parametric families, and are preferable to simple\nconvex combinations. Once the covariance family has been chosen, one typically\nneeds to select a representative member by solving an optimization problem,\ne.g., by maximizing the likelihood of a data set. We consider instead a\ndifferential geometric interpretation of this problem: minimizing the geodesic\ndistance to a sample covariance matrix (\"natural projection\"). Our approach is\nconsistent with the notion of distance employed to build the covariance family\nand does not require assuming a particular probability distribution for the\ndata. We show that natural projection and maximum likelihood estimation within\nthe covariance family are locally equivalent up to second order. We also\ndemonstrate that natural projection may yield more accurate estimates with\nnoise-corrupted data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 22:49:23 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 18:40:31 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Musolas", "Antoni", ""], ["Smith", "Steven T.", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2001.01821", "submitter": "Phuong Hanh Tran", "authors": "P. H. Tran and C. Heuchenne and H. D. Nguyen", "title": "Monitoring Coefficient of Variation using One-Sided Run Rules control\n  charts in the presence of Measurement Errors", "comments": "23 pages, 8 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate in this paper the effect of the measurement error on the\nperformance of Run Rules control charts monitoring the coefficient of variation\n(CV) squared. The previous Run Rules CV chart in the literature is improved\nslightly by monitoring the CV squared using two one-sided Run Rules charts\ninstead of monitoring the CV itself using a two-sided chart. The numerical\nresults show that this improvement gives better performance in detecting\nprocess shifts. Moreover, we will show through simulation that the\n\\textit{precision} and \\textit{accuracy} errors do have negative effect on the\nperformance of the proposed Run Rules charts. We also find out that taking\nmultiple measurements per item is not an effective way to reduce these negative\neffects.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 00:23:40 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Tran", "P. H.", ""], ["Heuchenne", "C.", ""], ["Nguyen", "H. D.", ""]]}, {"id": "2001.01916", "submitter": "Seyoon Ko", "authors": "Seyoon Ko, Hua Zhou, Jin J. Zhou, Joong-Ho Won", "title": "High-Performance Statistical Computing in the Computing Environments of\n  the 2020s", "comments": "Accepted for publication in Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advances in the past decade, hardware and software alike, have\nmade access to high-performance computing (HPC) easier than ever. We review\nthese advances from a statistical computing perspective. Cloud computing makes\naccess to supercomputers affordable. Deep learning software libraries make\nprogramming statistical algorithms easy and enable users to write code once and\nrun it anywhere -- from a laptop to a workstation with multiple graphics\nprocessing units (GPUs) or a supercomputer in a cloud. Highlighting how these\ndevelopments benefit statisticians, we review recent optimization algorithms\nthat are useful for high-dimensional models and can harness the power of HPC.\nCode snippets are provided to demonstrate the ease of programming. We also\nprovide an easy-to-use distributed matrix data structure suitable for HPC.\nEmploying this data structure, we illustrate various statistical applications\nincluding large-scale positron emission tomography and $\\ell_1$-regularized Cox\nregression. Our examples easily scale up to an 8-GPU workstation and a\n720-CPU-core cluster in a cloud. As a case in point, we analyze the onset of\ntype-2 diabetes from the UK Biobank with 200,000 subjects and about 500,000\nsingle nucleotide polymorphisms using the HPC $\\ell_1$-regularized Cox\nregression. Fitting this half-million-variate model takes less than 45 minutes\nand reconfirms known associations. To our knowledge, this is the first\ndemonstration of the feasibility of penalized regression of survival outcomes\nat this scale.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 07:24:19 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 04:13:09 GMT"}, {"version": "v3", "created": "Fri, 16 Jul 2021 05:57:49 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ko", "Seyoon", ""], ["Zhou", "Hua", ""], ["Zhou", "Jin J.", ""], ["Won", "Joong-Ho", ""]]}, {"id": "2001.01988", "submitter": "Nicholas Horton", "authors": "Matthew D. Beckman, Mine \\c{C}etinkaya-Rundel, Nicholas J. Horton,\n  Colin W. Rundel, Adam J. Sullivan and Maria Tackett", "title": "Implementing version control with Git and GitHub as a learning objective\n  in statistics and data science courses", "comments": "In press, Journal of Statistics and Data Science Education", "journal-ref": null, "doi": "10.1080/10691898.2020.1848485", "report-no": null, "categories": "stat.CO stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A version control system records changes to a file or set of files over time\nso that changes can be tracked and specific versions of a file can be recalled\nlater. As such, it is an essential element of a reproducible workflow that\ndeserves due consideration among the learning objectives of statistics and data\nscience courses. This paper describes experiences and implementation decisions\nof four contributing faculty who are teaching different courses at a variety of\ninstitutions. Each of these faculty have set version control as a learning\nobjective and successfully integrated one such system (Git) into one or more\nstatistics courses. The various approaches described in the paper span\ndifferent implementation strategies to suit student background, course type,\nsoftware choices, and assessment practices. By presenting a wide range of\napproaches to teaching Git, the paper aims to serve as a resource for\nstatistics and data science instructors teaching courses at any level within an\nundergraduate or graduate curriculum.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 11:49:38 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2020 11:06:21 GMT"}, {"version": "v3", "created": "Wed, 4 Nov 2020 15:34:29 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Beckman", "Matthew D.", ""], ["\u00c7etinkaya-Rundel", "Mine", ""], ["Horton", "Nicholas J.", ""], ["Rundel", "Colin W.", ""], ["Sullivan", "Adam J.", ""], ["Tackett", "Maria", ""]]}, {"id": "2001.02013", "submitter": "Yvo Pokern", "authors": "Jeremie Coullon, Yvo Pokern", "title": "MCMC for a hyperbolic Bayesian inverse problem in traffic flow modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As work on hyperbolic Bayesian inverse problems remains rare in the\nliterature, we explore empirically the sampling challenges these offer which\nhave to do with shock formation in the solution of the PDE. Furthermore, we\nprovide a unified statistical model to estimate using motorway data both\nboundary conditions and fundamental diagram parameters in LWR, a well known\nmotorway traffic flow model. This allows us to provide a traffic flow density\nestimation method that is shown to be superior to two methods found in the\ntraffic flow literature. Finally, we highlight how \\emph{Population Parallel\nTempering} - a modification of Parallel Tempering - is a scalable method that\ncan increase the mixing speed of the sampler by a factor of 10.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 13:13:46 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Coullon", "Jeremie", ""], ["Pokern", "Yvo", ""]]}, {"id": "2001.02168", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock and Andrew Golightly", "title": "Exact Bayesian inference for discretely observed Markov Jump Processes\n  using finite rate matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methodologies for Bayesian inference on the rate parameters of\na discretely observed continuous-time Markov jump processes with a countably\ninfinite state space. The usual method of choice for inference, particle Markov\nchain Monte Carlo (particle MCMC), struggles when the observation noise is\nsmall. We consider the most challenging regime of exact observations and\nprovide two new methodologies for inference in this case: the minimal extended\nstate space algorithm (MESA) and the nearly minimal extended state space\nalgorithm (nMESA). By extending the Markov chain Monte Carlo state space, both\nMESA and nMESA use the exponentiation of finite rate matrices to perform exact\nBayesian inference on the Markov jump process even though its state space is\ncountably infinite. Numerical experiments show improvements over particle MCMC\nof between a factor of three and several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:58:52 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sherlock", "Chris", ""], ["Golightly", "Andrew", ""]]}, {"id": "2001.02225", "submitter": "David Hofmeyr", "authors": "David P. Hofmeyr", "title": "Fast Kernel Smoothing in R with Applications to Projection Pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package FKSUM, which offers fast and exact\nevaluation of univariate kernel smoothers. The main kernel computations are\nimplemented in C++, and are wrapped in simple, intuitive and versatile R\nfunctions. The fast kernel computations are based on recursive expressions\ninvolving the order statistics, which allows for exact evaluation of kernel\nsmoothers at all sample points in log-linear time. In addition to general\npurpose kernel smoothing functions, the package offers purpose built and\nready-to-use implementations of popular kernel-type estimators. On top of these\nbasic smoothing problems, this paper focuses on projection pursuit problems in\nwhich the projection index is based on kernel-type estimators of functionals of\nthe projected density.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 18:56:07 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 08:45:51 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Hofmeyr", "David P.", ""]]}, {"id": "2001.02775", "submitter": "Rachael C Aikens", "authors": "Rachael C. Aikens, Joseph Rigdon, Justin Lee, Michael Baiocchi, Andrew\n  B. Goldstone, Peter Chiu, Y. Joseph Woo, Jonathan H. Chen", "title": "stratamatch: Prognostic ScoreStratification using a Pilot Design", "comments": "15 pages, 5 figures, submitted to The R Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal propensity score matching has emerged as one of the most ubiquitous\napproaches for causal inference studies on observational data; However,\noutstanding critiques of the statistical properties of propensity score\nmatching have cast doubt on the statistical efficiency of this technique, and\nthe poor scalability of optimal matching to large data sets makes this approach\ninconvenient if not infeasible for sample sizes that are increasingly\ncommonplace in modern observational data. The stratamatch package provides\nimplementation support and diagnostics for `stratified matching designs,' an\napproach which addresses both of these issues with optimal propensity score\nmatching for large-sample observational studies. First, stratifying the data\nenables more computationally efficient matching of large data sets. Second,\nstratamatch implements a `pilot design' approach in order to stratify by a\nprognostic score, which may increase the precision of the effect estimate and\nincrease power in sensitivity analyses of unmeasured confounding.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 22:43:13 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 22:20:46 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Aikens", "Rachael C.", ""], ["Rigdon", "Joseph", ""], ["Lee", "Justin", ""], ["Baiocchi", "Michael", ""], ["Goldstone", "Andrew B.", ""], ["Chiu", "Peter", ""], ["Woo", "Y. Joseph", ""], ["Chen", "Jonathan H.", ""]]}, {"id": "2001.03090", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Luca Martino, and Pau Closas", "title": "Importance Gaussian Quadrature", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3045526", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling (IS) and numerical integration methods are usually\nemployed for approximating moments of complicated target distributions. In its\nbasic procedure, the IS methodology randomly draws samples from a proposal\ndistribution and weights them accordingly, accounting for the mismatch between\nthe target and proposal. In this work, we present a general framework of\nnumerical integration techniques inspired by the IS methodology. The framework\ncan also be seen as an incorporation of deterministic rules into IS methods,\nreducing the error of the estimators by several orders of magnitude in several\nproblems of interest. The proposed approach extends the range of applicability\nof the Gaussian quadrature rules. For instance, the IS perspective allows us to\nuse Gauss-Hermite rules in problems where the integrand is not involving a\nGaussian distribution, and even more, when the integrand can only be evaluated\nup to a normalizing constant, as it is usually the case in Bayesian inference.\nThe novel perspective makes use of recent advances on the multiple IS (MIS) and\nadaptive (AIS) literatures, and incorporates it to a wider numerical\nintegration framework that combines several numerical integration rules that\ncan be iteratively adapted. We analyze the convergence of the algorithms and\nprovide some representative examples showing the superiority of the proposed\napproach in terms of performance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 16:45:53 GMT"}, {"version": "v2", "created": "Sun, 31 Jan 2021 12:39:45 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""], ["Closas", "Pau", ""]]}, {"id": "2001.03140", "submitter": "Peter Simonson", "authors": "Peter Simonson (1) and Douglas Nychka (1) and Soutir Bandyopadhyay (1)\n  ((1) Colorado School of Mines)", "title": "Rapid Numerical Approximation Method for Integrated Covariance Functions\n  Over Irregular Data Regions", "comments": "14 pages, 7 figures, 7 tables. Submitted to Stat (Wiley Online\n  Library) in December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many practical applications, spatial data are often collected at areal\nlevels (i.e., block data) and the inferences and predictions about the variable\nat points or blocks different from those at which it has been observed\ntypically depend on integrals of the underlying continuous spatial process. In\nthis paper we describe a method based on Fourier transform by which multiple\nintegrals of covariance functions over irregular data regions may be\nnumerically approximated with the same level of accuracy to traditional\nmethods, but at a greatly reduced computational expense.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 18:06:49 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Simonson", "Peter", "", "Colorado School of Mines"], ["Nychka", "Douglas", "", "Colorado School of Mines"], ["Bandyopadhyay", "Soutir", "", "Colorado School of Mines"]]}, {"id": "2001.03192", "submitter": "Mark Tygert", "authors": "Chuan Guo, Awni Hannun, Brian Knott, Laurens van der Maaten, Mark\n  Tygert, and Ruiyu Zhu", "title": "Secure multiparty computations in floating-point arithmetic", "comments": "31 pages, 13 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.IT cs.LG cs.NA math.IT math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Secure multiparty computations enable the distribution of so-called shares of\nsensitive data to multiple parties such that the multiple parties can\neffectively process the data while being unable to glean much information about\nthe data (at least not without collusion among all parties to put back together\nall the shares). Thus, the parties may conspire to send all their processed\nresults to a trusted third party (perhaps the data provider) at the conclusion\nof the computations, with only the trusted third party being able to view the\nfinal results. Secure multiparty computations for privacy-preserving\nmachine-learning turn out to be possible using solely standard floating-point\narithmetic, at least with a carefully controlled leakage of information less\nthan the loss of accuracy due to roundoff, all backed by rigorous mathematical\nproofs of worst-case bounds on information loss and numerical stability in\nfinite-precision arithmetic. Numerical examples illustrate the high performance\nattained on commodity off-the-shelf hardware for generalized linear models,\nincluding ordinary linear least-squares regression, binary and multinomial\nlogistic regression, probit regression, and Poisson regression.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:30:14 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Guo", "Chuan", ""], ["Hannun", "Awni", ""], ["Knott", "Brian", ""], ["van der Maaten", "Laurens", ""], ["Tygert", "Mark", ""], ["Zhu", "Ruiyu", ""]]}, {"id": "2001.03195", "submitter": "V\\'ictor Elvira", "authors": "\\'Emilie Chouzenoux and V\\'ictor Elvira", "title": "GraphEM: EM algorithm for blind Kalman filtering under graphical\n  sparsity constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and inference with multivariate sequences is central in a number of\nsignal processing applications such as acoustics, social network analysis,\nbiomedical, and finance, to name a few. The linear-Gaussian state-space model\nis a common way to describe a time series through the evolution of a hidden\nstate, with the advantage of presenting a simple inference procedure due to the\ncelebrated Kalman filter. A fundamental question when analyzing multivariate\nsequences is the search for relationships between their entries (or the modeled\nhidden states), especially when the inherent structure is a non-fully connected\ngraph. In such context, graphical modeling combined with parsimony constraints\nallows to limit the proliferation of parameters and enables a compact data\nrepresentation which is easier to interpret by the experts. In this work, we\npropose a novel expectation-minimization algorithm for estimating the linear\nmatrix operator in the state equation of a linear-Gaussian state-space model.\nLasso regularization is included in the M-step, that we solved using a proximal\nsplitting Douglas-Rachford algorithm. Numerical experiments illustrate the\nbenefits of the proposed model and inference technique, named GraphEM, over\ncompetitors relying on Granger causality.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 19:32:30 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Chouzenoux", "\u00c9milie", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "2001.03322", "submitter": "Sam Davanloo", "authors": "Dewei Zhang, Yin Liu, and Sam Davanloo Tajbakhsh", "title": "A first-order optimization algorithm for statistical learning with\n  hierarchical sparsity structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many statistical learning problems, it is desired that the optimal\nsolution conforms to an a priori known sparsity structure represented by a\ndirected acyclic graph. Inducing such structures by means of convex\nregularizers requires nonsmooth penalty functions that exploit group\noverlapping. Our study focuses on evaluating the proximal operator of the\nLatent Overlapping Group lasso developed by Jacob et al. (2009). We implemented\nan Alternating Direction Method of Multiplier with a sharing scheme to solve\nlarge-scale instances of the underlying optimization problem efficiently. In\nthe absence of strong convexity, global linear convergence of the algorithm is\nestablished using the error bound theory. More specifically, the paper\ncontributes to establishing primal and dual error bounds when the nonsmooth\ncomponent in the objective function does not have a polyhedral epigraph. We\nalso investigate the effect of the graph structure on the speed of convergence\nof the algorithm. Detailed numerical simulation studies over different graph\nstructures supporting the proposed algorithm and two applications in learning\nare provided.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 06:23:13 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 03:55:07 GMT"}, {"version": "v3", "created": "Sun, 18 Oct 2020 04:51:24 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zhang", "Dewei", ""], ["Liu", "Yin", ""], ["Tajbakhsh", "Sam Davanloo", ""]]}, {"id": "2001.03396", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig, Jordi Cort\\'es Mart\\'inez and Guadalupe G\\'omez\n  Melis", "title": "Decision tool and Sample Size Calculator for Composite Endpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary points:\n  - This article considers the combination of two binary or two time-to-event\nendpoints to form the primary composite endpoint for leading a trial.\n  - It discusses the relative efficiency of choosing a composite endpoint over\none of its components in terms of: the frequencies of observing each component;\nthe relative treatment effect of the tested therapy; and the association\nbetween both components.\n  - We highlight the very important role of the association between components\nin choosing the most efficient endpoint to use as primary.\n  - For better grounded future trials, we recommend trialists to always\nreporting the association between components of the composite endpoint.\n  - Common fallacies to note when using composite endpoints: i) composite\nendpoints always imply higher power; ii) treatment effect on the composite\nendpoint is similar to the average effects of its components; and iii) the\nprobability of observing the primary endpoint increases significantly.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 11:40:06 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Mart\u00ednez", "Jordi Cort\u00e9s", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2001.03530", "submitter": "Mehmet Ugurbil", "authors": "Mehmet Ugurbil", "title": "Dynamic Gauss Newton Metropolis Algorithm", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.EP astro-ph.SR physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GNM: The MCMC Jagger. A rocking awesome sampler. This python package is an\naffine invariant Markov chain Monte Carlo (MCMC) sampler based on the dynamic\nGauss-Newton-Metropolis (GNM) algorithm. The GNM algorithm is specialized in\nsampling highly non-linear posterior probability distribution functions of the\nform $e^{-||f(x)||^2/2}$, and the package is an implementation of this\nalgorithm. On top of the back-off strategy in the original GNM algorithm, there\nis the dynamic hyper-parameter optimization feature added to the algorithm and\nincluded in the package to help increase performance of the back-off and\ntherefore the sampling. Also, there are the Jacobian tester, error bars creator\nand many more features for the ease of use included in the code. The problem is\nintroduced and a guide to installation is given in the introduction. Then how\nto use the python package is explained. The algorithm is given and finally\nthere are some examples using exponential time series to show the performance\nof the algorithm and the back-off strategy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2019 10:05:05 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Ugurbil", "Mehmet", ""]]}, {"id": "2001.03689", "submitter": "Alfredo Garbuno-Inigo", "authors": "Emmet Cleary, Alfredo Garbuno-Inigo, Shiwei Lan, Tapio Schneider and\n  Andrew M Stuart", "title": "Calibrate, Emulate, Sample", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2020.109716", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many parameter estimation problems arising in applications are best cast in\nthe framework of Bayesian inversion. This allows not only for an estimate of\nthe parameters, but also for the quantification of uncertainties in the\nestimates. Often in such problems the parameter-to-data map is very expensive\nto evaluate, and computing derivatives of the map, or derivative-adjoints, may\nnot be feasible. Additionally, in many applications only noisy evaluations of\nthe map may be available. We propose an approach to Bayesian inversion in such\nsettings that builds on the derivative-free optimization capabilities of\nensemble Kalman inversion methods. The overarching approach is to first use\nensemble Kalman sampling (EKS) to calibrate the unknown parameters to fit the\ndata; second, to use the output of the EKS to emulate the parameter-to-data\nmap; third, to sample from an approximate Bayesian posterior distribution in\nwhich the parameter-to-data map is replaced by its emulator. This results in a\nprincipled approach to approximate Bayesian inference that requires only a\nsmall number of evaluations of the (possibly noisy approximation of the)\nparameter-to-data map. It does not require derivatives of this map, but instead\nleverages the documented power of ensemble Kalman methods. Furthermore, the EKS\nhas the desirable property that it evolves the parameter ensembles towards the\nregions in which the bulk of the parameter posterior mass is located, thereby\nlocating them well for the emulation phase of the methodology. In essence, the\nEKS methodology provides a cheap solution to the design problem of where to\nplace points in parameter space to efficiently train an emulator of the\nparameter-to-data map for the purposes of Bayesian inversion.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 23:19:24 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Cleary", "Emmet", ""], ["Garbuno-Inigo", "Alfredo", ""], ["Lan", "Shiwei", ""], ["Schneider", "Tapio", ""], ["Stuart", "Andrew M", ""]]}, {"id": "2001.03984", "submitter": "Kjartan Kloster Osmundsen", "authors": "Kjartan Kloster Osmundsen, Tore Selland Kleppe, Roman Liesenfeld, Atle\n  Oglend", "title": "Estimating the Competitive Storage Model with Stochastic Trends in\n  Commodity Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a state-space model (SSM) for commodity prices that combines the\ncompetitive storage model with a stochastic trend. This approach fits into the\neconomic rationality of storage decisions, and adds to previous deterministic\ntrend specifications of the storage model. Parameters are estimated using a\nparticle Markov chain Monte Carlo procedure. Empirical application to four\ncommodity markets shows that the stochastic trend SSM is favored over\ndeterministic trend specifications. The stochastic trend SSM identifies\nstructural parameters that differ from those for deterministic trend\nspecifications. In particular, the estimated price elasticities of demand are\nsignificantly larger under the stochastic trend SSM.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 19:33:49 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Osmundsen", "Kjartan Kloster", ""], ["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""], ["Oglend", "Atle", ""]]}, {"id": "2001.03985", "submitter": "Luigi Acerbi", "authors": "Bas van Opheusden, Luigi Acerbi and Wei Ji Ma", "title": "Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial\n  Sampling", "comments": "Bas van Opheusden and Luigi Acerbi contributed equally to this work", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008483", "report-no": null, "categories": "cs.LG q-bio.NC q-bio.QM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fate of scientific hypotheses often relies on the ability of a\ncomputational model to explain the data, quantified in modern statistical\napproaches by the likelihood function. The log-likelihood is the key element\nfor parameter estimation and model evaluation. However, the log-likelihood of\ncomplex models in fields such as computational biology and neuroscience is\noften intractable to compute analytically or numerically. In those cases,\nresearchers can often only estimate the log-likelihood by comparing observed\ndata with synthetic observations generated by model simulations. Standard\ntechniques to approximate the likelihood via simulation either use summary\nstatistics of the data or are at risk of producing severe biases in the\nestimate. Here, we explore another method, inverse binomial sampling (IBS),\nwhich can estimate the log-likelihood of an entire data set efficiently and\nwithout bias. For each observation, IBS draws samples from the simulator model\nuntil one matches the observation. The log-likelihood estimate is then a\nfunction of the number of samples drawn. The variance of this estimator is\nuniformly bounded, achieves the minimum variance for an unbiased estimator, and\nwe can compute calibrated estimates of the variance. We provide theoretical\narguments in favor of IBS and an empirical assessment of the method for\nmaximum-likelihood estimation with simulation-based models. As case studies, we\ntake three model-fitting problems of increasing complexity from computational\nand cognitive neuroscience. In all problems, IBS generally produces lower error\nin the estimated parameters and maximum log-likelihood values than alternative\nsampling methods with the same average number of samples. Our results\ndemonstrate the potential of IBS as a practical, robust, and easy to implement\nmethod for log-likelihood evaluation when exact techniques are not available.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 19:51:35 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 19:24:28 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 20:08:25 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["van Opheusden", "Bas", ""], ["Acerbi", "Luigi", ""], ["Ma", "Wei Ji", ""]]}, {"id": "2001.04230", "submitter": "Chon Lok Lei", "authors": "Chon Lok Lei, Sanmitra Ghosh, Dominic G. Whittaker, Yasser\n  Aboelkassem, Kylie A. Beattie, Chris D. Cantwell, Tammo Delhaas, Charles\n  Houston, Gustavo Montes Novaes, Alexander V. Panfilov, Pras Pathmanathan,\n  Marina Riabiz, Rodrigo Weber dos Santos, John Walmsley, Keith Worden, Gary R.\n  Mirams and Richard D. Wilkinson", "title": "Considering discrepancy when calibrating a mechanistic electrophysiology\n  model", "comments": "This version is published in Philosophical Transactions of the Royal\n  Society A; Updated in response to reviewer comments, including: added details\n  to the introduction, fixed mathematical notations for clarity, and moved the\n  original Table 3 to the supplement to avoid confusion", "journal-ref": "Phil. Trans. R. Soc. A. 378 (2020): 20190349", "doi": "10.1098/rsta.2019.0349", "report-no": null, "categories": "stat.CO q-bio.QM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification (UQ) is a vital step in using mathematical models\nand simulations to take decisions. The field of cardiac simulation has begun to\nexplore and adopt UQ methods to characterise uncertainty in model inputs and\nhow that propagates through to outputs or predictions. In this perspective\npiece we draw attention to an important and under-addressed source of\nuncertainty in our predictions -- that of uncertainty in the model structure or\nthe equations themselves. The difference between imperfect models and reality\nis termed model discrepancy, and we are often uncertain as to the size and\nconsequences of this discrepancy. Here we provide two examples of the\nconsequences of discrepancy when calibrating models at the ion channel and\naction potential scales. Furthermore, we attempt to account for this\ndiscrepancy when calibrating and validating an ion channel model using\ndifferent methods, based on modelling the discrepancy using Gaussian processes\n(GPs) and autoregressive-moving-average (ARMA) models, then highlight the\nadvantages and shortcomings of each approach. Finally, suggestions and lines of\nenquiry for future work are provided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:26:13 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 13:50:13 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Lei", "Chon Lok", ""], ["Ghosh", "Sanmitra", ""], ["Whittaker", "Dominic G.", ""], ["Aboelkassem", "Yasser", ""], ["Beattie", "Kylie A.", ""], ["Cantwell", "Chris D.", ""], ["Delhaas", "Tammo", ""], ["Houston", "Charles", ""], ["Novaes", "Gustavo Montes", ""], ["Panfilov", "Alexander V.", ""], ["Pathmanathan", "Pras", ""], ["Riabiz", "Marina", ""], ["Santos", "Rodrigo Weber dos", ""], ["Walmsley", "John", ""], ["Worden", "Keith", ""], ["Mirams", "Gary R.", ""], ["Wilkinson", "Richard D.", ""]]}, {"id": "2001.04555", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Martin C. Rinard, Vikash K.\n  Mansinghka", "title": "Optimal Approximate Sampling from Discrete Probability Distributions", "comments": null, "journal-ref": "Proc. ACM Program. Lang. 4, POPL, Article 36 (January 2020)", "doi": "10.1145/3371104", "report-no": null, "categories": "cs.DS cs.DM cs.IT math.IT math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a fundamental problem in random variate generation:\ngiven access to a random source that emits a stream of independent fair bits,\nwhat is the most accurate and entropy-efficient algorithm for sampling from a\ndiscrete probability distribution $(p_1, \\dots, p_n)$, where the probabilities\nof the output distribution $(\\hat{p}_1, \\dots, \\hat{p}_n)$ of the sampling\nalgorithm must be specified using at most $k$ bits of precision? We present a\ntheoretical framework for formulating this problem and provide new techniques\nfor finding sampling algorithms that are optimal both statistically (in the\nsense of sampling accuracy) and information-theoretically (in the sense of\nentropy consumption). We leverage these results to build a system that, for a\nbroad family of measures of statistical accuracy, delivers a sampling algorithm\nwhose expected entropy usage is minimal among those that induce the same\ndistribution (i.e., is \"entropy-optimal\") and whose output distribution\n$(\\hat{p}_1, \\dots, \\hat{p}_n)$ is a closest approximation to the target\ndistribution $(p_1, \\dots, p_n)$ among all entropy-optimal sampling algorithms\nthat operate within the specified $k$-bit precision. This optimal approximate\nsampler is also a closer approximation than any (possibly entropy-suboptimal)\nsampler that consumes a bounded amount of entropy with the specified precision,\na class which includes floating-point implementations of inversion sampling and\nrelated methods found in many software libraries. We evaluate the accuracy,\nentropy consumption, precision requirements, and wall-clock runtime of our\noptimal approximate sampling algorithms on a broad set of distributions,\ndemonstrating the ways that they are superior to existing approximate samplers\nand establishing that they often consume significantly fewer resources than are\nneeded by exact samplers.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 22:48:07 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Rinard", "Martin C.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "2001.04657", "submitter": "Sakae Oya", "authors": "Sakae Oya and Teruo Nakatsuma", "title": "A positive-definiteness-assured block Gibbs sampler for Bayesian\n  graphical models with shrinkage priors", "comments": "In line with a revised version submitted to a journal in October\n  2020, we also modified the preprint version in arxiv. Although we reflected\n  the title change, the content of English proofreading by native English\n  speakers and changes of variable name ,and added a graph, there are no\n  changes to the model content, simulations and application results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the block Gibbs sampler for the Bayesian graphical LASSO proposed by\nWang (2012) has been widely applied and extended to various shrinkage priors in\nrecent years, it has a less noticeable but possibly severe disadvantage that\nthe positive definiteness of a precision matrix in the Gaussian graphical model\nis not guaranteed in each cycle of the Gibbs sampler. Specifically, if the\ndimension of the precision matrix exceeds the sample size, the positive\ndefiniteness of the precision matrix will be barely satisfied and the Gibbs\nsampler will almost surely fail. In this paper, we propose modifying the\noriginal block Gibbs sampler so that the precision matrix never fails to be\npositive definite by sampling it exactly from the domain of the positive\ndefiniteness. As we have shown in the Monte Carlo experiments, this\nmodification not only stabilizes the sampling procedure but also significantly\nimproves the performance of the parameter estimation and graphical structure\nlearning. We also apply our proposed algorithm to a graphical model of the\nmonthly return data in which the number of stocks exceeds the sample period,\ndemonstrating its stability and scalability.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 08:13:58 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 01:31:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Oya", "Sakae", ""], ["Nakatsuma", "Teruo", ""]]}, {"id": "2001.04676", "submitter": "Takashi Goda", "authors": "Kei Ishikawa, Takashi Goda", "title": "Efficient Debiased Evidence Estimation by Multilevel Monte Carlo\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new stochastic optimization algorithm for\nBayesian inference based on multilevel Monte Carlo (MLMC) methods. In Bayesian\nstatistics, biased estimators of the model evidence have been often used as\nstochastic objectives because the existing debiasing techniques are\ncomputationally costly to apply. To overcome this issue, we apply an MLMC\nsampling technique to construct low-variance unbiased estimators both for the\nmodel evidence and its gradient. In the theoretical analysis, we show that the\ncomputational cost required for our proposed MLMC estimator to estimate the\nmodel evidence or its gradient with a given accuracy is an order of magnitude\nsmaller than those of the previously known estimators. Our numerical\nexperiments confirm considerable computational savings compared to the\nconventional estimators. Combining our MLMC estimator with gradient-based\nstochastic optimization results in a new scalable, efficient, debiased\ninference algorithm for Bayesian statistical models.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 09:14:24 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2021 23:48:39 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Ishikawa", "Kei", ""], ["Goda", "Takashi", ""]]}, {"id": "2001.04802", "submitter": "Amir Mosavi Prof", "authors": "Amin Kazemian-Kale-Kale, Azadeh Gholami, Mohammad Rezaie-Balf, Amir\n  Mosavi, Ahmed A Sattar, Bahram Gharabaghi, Hossein Bonakdari", "title": "A Bayesian Monte-Carlo Uncertainty Model for Assessment of Shear Stress\n  Entropy", "comments": "48 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The entropy models have been recently adopted in many studies to evaluate the\ndistribution of the shear stress in circular channels. However, the uncertainty\nin their predictions and their reliability remains an open question. We present\na novel method to evaluate the uncertainty of four popular entropy models,\nincluding Shannon, Shannon-Power Low (PL), Tsallis, and Renyi, in shear stress\nestimation in circular channels. The Bayesian Monte-Carlo (BMC) uncertainty\nmethod is simplified considering a 95% Confidence Bound (CB). We developed a\nnew statistic index called as FREEopt-based OCB (FOCB) using the statistical\nindices Forecasting Range of Error Estimation (FREE) and the percentage of\nobserved data in the CB (Nin), which integrates their combined effect. The\nShannon and Shannon PL entropies had close values of the FOCB equal to 8.781\nand 9.808, respectively, had the highest certainty in the calculation of shear\nstress values in circular channels followed by traditional uniform flow shear\nstress and Tsallis models with close values of 14.491 and 14.895, respectively.\nHowever, Renyi entropy with much higher values of FOCB equal to 57.726 has less\ncertainty in the estimation of shear stress than other models. Using the\npresented results in this study, the amount of confidence in entropy methods in\nthe calculation of shear stress to design and implement different types of open\nchannels and their stability is determined.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 22:46:59 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Kazemian-Kale-Kale", "Amin", ""], ["Gholami", "Azadeh", ""], ["Rezaie-Balf", "Mohammad", ""], ["Mosavi", "Amir", ""], ["Sattar", "Ahmed A", ""], ["Gharabaghi", "Bahram", ""], ["Bonakdari", "Hossein", ""]]}, {"id": "2001.04847", "submitter": "Anita Nandi", "authors": "Anita K. Nandi, Tim C. D. Lucas, Rohan Arambepola, Peter Gething,\n  Daniel J. Weiss", "title": "disaggregation: An R Package for Bayesian Spatial Disaggregation\n  Modelling", "comments": "16 pages, 5 figures, submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disaggregation modelling, or downscaling, has become an important discipline\nin epidemiology. Surveillance data, aggregated over large regions, is becoming\nmore common, leading to an increasing demand for modelling frameworks that can\ndeal with this data to understand spatial patterns. Disaggregation regression\nmodels use response data aggregated over large heterogenous regions to make\npredictions at fine-scale over the region by using fine-scale covariates to\ninform the heterogeneity. This paper presents the R package disaggregation,\nwhich provides functionality to streamline the process of running a\ndisaggregation model for fine-scale predictions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 16:00:16 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Nandi", "Anita K.", ""], ["Lucas", "Tim C. D.", ""], ["Arambepola", "Rohan", ""], ["Gething", "Peter", ""], ["Weiss", "Daniel J.", ""]]}, {"id": "2001.05033", "submitter": "Pavel Sountsov", "authors": "Dan Piponi and Matthew D. Hoffman and Pavel Sountsov", "title": "Hamiltonian Monte Carlo Swindles", "comments": "To be published in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a powerful Markov chain Monte Carlo (MCMC)\nalgorithm for estimating expectations with respect to continuous un-normalized\nprobability distributions. MCMC estimators typically have higher variance than\nclassical Monte Carlo with i.i.d. samples due to autocorrelations; most MCMC\nresearch tries to reduce these autocorrelations. In this work, we explore a\ncomplementary approach to variance reduction based on two classical Monte Carlo\n\"swindles\": first, running an auxiliary coupled chain targeting a tractable\napproximation to the target distribution, and using the auxiliary samples as\ncontrol variates; and second, generating anti-correlated (\"antithetic\") samples\nby running two chains with flipped randomness. Both ideas have been explored\npreviously in the context of Gibbs samplers and random-walk Metropolis\nalgorithms, but we argue that they are ripe for adaptation to HMC in light of\nrecent coupling results from the HMC theory literature. For many posterior\ndistributions, we find that these swindles generate effective sample sizes\norders of magnitude larger than plain HMC, as well as being more efficient than\nanalogous swindles for Metropolis-adjusted Langevin algorithm and random-walk\nMetropolis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 20:08:20 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 20:34:53 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Piponi", "Dan", ""], ["Hoffman", "Matthew D.", ""], ["Sountsov", "Pavel", ""]]}, {"id": "2001.05035", "submitter": "Pavel Sountsov", "authors": "Pavel Sountsov and Alexey Radul and Srinivas Vasudevan", "title": "FunMC: A functional API for building Markov Chains", "comments": "Updated source code to reflect API; updated link to point to new\n  location", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Constant-memory algorithms, also loosely called Markov chains, power the vast\nmajority of probabilistic inference and machine learning applications today. A\nlot of progress has been made in constructing user-friendly APIs around these\nalgorithms. Such APIs, however, rarely make it easy to research new algorithms\nof this type. In this work we present FunMC, a minimal Python library for doing\nmethodological research into algorithms based on Markov chains. FunMC is not\ntargeted toward data scientists or others who wish to use MCMC or optimization\nas a black box, but rather towards researchers implementing new Markovian\nalgorithms from scratch.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 20:20:06 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 23:21:12 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 17:52:16 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Sountsov", "Pavel", ""], ["Radul", "Alexey", ""], ["Vasudevan", "Srinivas", ""]]}, {"id": "2001.05729", "submitter": "Marco Stefanucci", "authors": "Marco Stefanucci, Antonio Canale", "title": "Multiscale stick-breaking mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of multiscale stick-breaking mixture models for\nBayesian nonparametric density estimation. The Bayesian nonparametric\nliterature is dominated by single scale methods, exception made for P\\`olya\ntrees and allied approaches. Our proposal is based on a mixture specification\nexploiting an infinitely-deep binary tree of random weights that grows\naccording to a multiscale generalization of a large class of stick-breaking\nprocesses; this multiscale stick-breaking is paired with specific stochastic\nprocesses generating sequences of parameters that induce stochastically ordered\nkernel functions. Properties of this family of multiscale stick-breaking\nmixtures are described. Focusing on a Gaussian specification, a Markov Chain\nMontecarlo algorithm for posterior computation is introduced. The performance\nof the method is illustrated analyzing both synthetic and real data sets. The\nmethod is well-suited for data living in $\\mathbb{R}$ and is able to detect\ndensities with varying degree of smoothness and local features.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 10:21:45 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Stefanucci", "Marco", ""], ["Canale", "Antonio", ""]]}, {"id": "2001.05819", "submitter": "Yuling Jiao", "authors": "Jian Huang, Yuling Jiao, Lican Kang, Jin Liu, Yanyan Liu, Xiliang Lu", "title": "A Support Detection and Root Finding Approach for Learning\n  High-dimensional Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is important for modeling high-dimensional data, where the\nnumber of variables can be much larger than the sample size. In this paper, we\ndevelop a support detection and root finding procedure to learn the high\ndimensional sparse generalized linear models and denote this method by GSDAR.\nBased on the KKT condition for $\\ell_0$-penalized maximum likelihood\nestimations, GSDAR generates a sequence of estimators iteratively.\n  Under some restricted invertibility conditions on the maximum likelihood\nfunction and sparsity assumption on the target coefficients, the errors of the\nproposed estimate decays exponentially to the optimal order. Moreover, the\noracle estimator can be recovered if the target signal is stronger than the\ndetectable level.\n  We conduct simulations and real data analysis to illustrate the advantages of\nour proposed method over several existing methods, including Lasso and MCP.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 14:35:17 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Kang", "Lican", ""], ["Liu", "Jin", ""], ["Liu", "Yanyan", ""], ["Lu", "Xiliang", ""]]}, {"id": "2001.06249", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (U Paris Dauphine and U Warwick) and Wu Changye (U\n  Paris Dauphine)", "title": "Markov Chain Monte Carlo Methods, a survey with some frequent\n  misunderstandings", "comments": "To appear in the Handbook of Computational Statistics, John Wiley &\n  Sons", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this chapter, we review some of the most standard MCMC tools used in\nBayesian computation, along with vignettes on standard misunderstandings of\nthese approaches taken from Q \\&~A's on the forum Cross-validated answered by\nthe first author.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 11:48:52 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Robert", "Christian P.", "", "U Paris Dauphine and U Warwick"], ["Changye", "Wu", "", "U\n  Paris Dauphine"]]}, {"id": "2001.06256", "submitter": "Thomas Prescott", "authors": "Thomas P. Prescott and Ruth E. Baker", "title": "Multifidelity Approximate Bayesian Computation with Sequential Monte\n  Carlo Parameter Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifidelity approximate Bayesian computation (MF-ABC) is a likelihood-free\ntechnique for parameter inference that exploits model approximations to\nsignificantly increase the speed of ABC algorithms (Prescott and Baker, 2020).\nPrevious work has considered MF-ABC only in the context of rejection sampling,\nwhich does not explore parameter space particularly efficiently. In this work,\nwe integrate the multifidelity approach with the ABC sequential Monte Carlo\n(ABC-SMC) algorithm into a new MF-ABC-SMC algorithm. We show that the\nimprovements generated by each of ABC-SMC and MF-ABC to the efficiency of\ngenerating Monte Carlo samples and estimates from the ABC posterior are\namplified when the two techniques are used together.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:04:00 GMT"}, {"version": "v2", "created": "Mon, 12 Oct 2020 16:05:18 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Prescott", "Thomas P.", ""], ["Baker", "Ruth E.", ""]]}, {"id": "2001.06465", "submitter": "James Scott Mr", "authors": "Axel Gandy and James Scott", "title": "Unit Testing for MCMC and other Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose approaches for testing implementations of Markov Chain Monte Carlo\nmethods as well as of general Monte Carlo methods. Based on statistical\nhypothesis tests, these approaches can be used in a unit testing framework to,\nfor example, check if individual steps in a Gibbs sampler or a reversible jump\nMCMC have the desired invariant distribution. Two exact tests for assessing\nwhether a given Markov chain has a specified invariant distribution are\ndiscussed. These and other tests of Monte Carlo methods can be embedded into a\nsequential method that allows low expected effort if the simulation shows the\ndesired behavior and high power if it does not. Moreover, the false rejection\nprobability can be kept arbitrarily low. For general Monte Carlo methods, this\nallows testing, for example, if a sampler has a specified distribution or if a\nsampler produces samples with the desired mean. The methods have been\nimplemented in the R-package MCUnit.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:29:10 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Gandy", "Axel", ""], ["Scott", "James", ""]]}, {"id": "2001.06471", "submitter": "Hussein Hazimeh", "authors": "Antoine Dedieu, Hussein Hazimeh, Rahul Mazumder", "title": "Learning Sparse Classifiers: Continuous and Mixed Integer Optimization\n  Perspectives", "comments": "To appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discrete optimization formulation for learning sparse\nclassifiers, where the outcome depends upon a linear combination of a small\nsubset of features. Recent work has shown that mixed integer programming (MIP)\ncan be used to solve (to optimality) $\\ell_0$-regularized regression problems\nat scales much larger than what was conventionally considered possible. Despite\ntheir usefulness, MIP-based global optimization approaches are significantly\nslower compared to the relatively mature algorithms for $\\ell_1$-regularization\nand heuristics for nonconvex regularized problems. We aim to bridge this gap in\ncomputation times by developing new MIP-based algorithms for\n$\\ell_0$-regularized classification. We propose two classes of scalable\nalgorithms: an exact algorithm that can handle $p\\approx 50,000$ features in a\nfew minutes, and approximate algorithms that can address instances with\n$p\\approx 10^6$ in times comparable to the fast $\\ell_1$-based algorithms. Our\nexact algorithm is based on the novel idea of \\textsl{integrality generation},\nwhich solves the original problem (with $p$ binary variables) via a sequence of\nmixed integer programs that involve a small number of binary variables. Our\napproximate algorithms are based on coordinate descent and local combinatorial\nsearch. In addition, we present new estimation error bounds for a class of\n$\\ell_0$-regularized estimators. Experiments on real and synthetic data\ndemonstrate that our approach leads to models with considerably improved\nstatistical performance (especially, variable selection) when compared to\ncompeting methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:47:02 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 17:38:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dedieu", "Antoine", ""], ["Hazimeh", "Hussein", ""], ["Mazumder", "Rahul", ""]]}, {"id": "2001.06505", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Urban Street Network Analysis in a Computational Notebook", "comments": null, "journal-ref": "Region 6 (3), 39-51 (2020)", "doi": "10.18335/region.v6i3.278", "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational notebooks offer researchers, practitioners, students, and\neducators the ability to interactively conduct analytics and disseminate\nreproducible workflows that weave together code, visuals, and narratives. This\narticle explores the potential of computational notebooks in urban analytics\nand planning, demonstrating their utility through a case study of OSMnx and its\ntutorials repository. OSMnx is a Python package for working with OpenStreetMap\ndata and modeling, analyzing, and visualizing street networks anywhere in the\nworld. Its official demos and tutorials are distributed as open-source Jupyter\nnotebooks on GitHub. This article showcases this resource by documenting the\nrepository and demonstrating OSMnx interactively through a synoptic tutorial\nadapted from the repository. It illustrates how to download urban data and\nmodel street networks for various study sites, compute network indicators,\nvisualize street centrality, calculate routes, and work with other spatial data\nsuch as building footprints and points of interest. Computational notebooks\nhelp introduce methods to new users and help researchers reach broader\naudiences interested in learning from, adapting, and remixing their work. Due\nto their utility and versatility, the ongoing adoption of computational\nnotebooks in urban planning, analytics, and related geocomputation disciplines\nshould continue into the future.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 19:19:53 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "2001.07625", "submitter": "Fredrik Bagge Carlson", "authors": "Fredrik Bagge Carlson", "title": "MonteCarloMeasurements.jl: Nonlinear Propagation of Arbitrary\n  Multivariate Distributions by means of Method Overloading", "comments": "5 pages, 4 figure, 5 code blocks, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscript outlines a software package that facilitates working with\nprobability distributions by means of Monte-Carlo methods, in a way that allows\nfor propagation of multivariate probability distributions through arbitrary\nfunctions. We provide a \\emph{type} that represents probability distributions\nby an internal vector of unweighted samples, \\texttt{Particles}, which is a\nsubtype of a \\texttt{Real} number and behaves just like a regular real number\nin calculations by means of method overloading. This makes the software easy to\nwork with and presents minimal friction for the user. We highlight how this\ndesign facilitates optimal usage of SIMD instructions and showcase the package\nfor uncertainty propagation through an off-the-shelf ODE solver, as well as for\nrobust probabilistic optimization with automatic differentiation.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 16:03:57 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Carlson", "Fredrik Bagge", ""]]}, {"id": "2001.07778", "submitter": "Hugo Maruri-Aguilar", "authors": "Hugo Maruri-Aguilar and Simon Lunagomez", "title": "Lasso for hierarchical polynomial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a polynomial regression model, the divisibility conditions implicit in\npolynomial hierarchy give way to a natural construction of constraints for the\nmodel parameters. We use this principle to derive versions of strong and weak\nhierarchy and to extend existing work in the literature, which at the moment is\nonly concerned with models of degree two. We discuss how to estimate parameters\nin lasso using standard quadratic programming techniques and apply our proposal\nto both simulated data and examples from the literature. The proposed\nmethodology compares favorably with existing techniques in terms of low\nvalidation error and model size.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:26:24 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Maruri-Aguilar", "Hugo", ""], ["Lunagomez", "Simon", ""]]}, {"id": "2001.07824", "submitter": "Fernando Alarid-Escudero", "authors": "Fernando Alarid-Escudero, Eline M. Krijkamp, Eva A. Enns, Alan Yang,\n  M.G. Myriam Hunink, Petros Pechlivanoglou, Hawre Jalal", "title": "Cohort State-Transition Models in R: A Tutorial", "comments": "Tutorial with 48 pages and 12 figures. For R code, see\n  https://github.com/DARTH-git/Cohort-modeling-tutorial", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision models can synthesize evidence from different sources to simulate\nlong-term consequences of different strategies in the presence of uncertainty.\nCohort state-transition models (cSTM) are decision models commonly used in\nmedical decision making to simulate hypothetical cohorts' transitions across\nvarious health states over time. This tutorial shows how to implement cSTMs in\nR, an open-source mathematical and statistical programming language. As an\nexample, we use a previously published cSTM-based cost-effectiveness analysis.\nWith this example, we illustrate both time-independent cSTMs, where transition\nprobabilities are constant over time, and time-dependent cSTMs, where\ntransition probabilities vary by age and are dependent on time spent in a\nhealth state (state residence). We also illustrate how to compute various\nepidemiological outcomes of interest, such as survival and prevalence. We\ndemonstrate how to calculate economic outcomes and conducting a\ncost-effectiveness analysis of multiple strategies using the example model, and\nprovide additional resources to conduct probabilistic sensitivity analyses. We\nprovide a link to a public repository with all the R code described in this\ntutorial that can be used to replicate the example or be adapted for various\ndecision modeling applications.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 00:14:28 GMT"}, {"version": "v2", "created": "Wed, 14 Oct 2020 04:40:27 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Alarid-Escudero", "Fernando", ""], ["Krijkamp", "Eline M.", ""], ["Enns", "Eva A.", ""], ["Yang", "Alan", ""], ["Hunink", "M. G. Myriam", ""], ["Pechlivanoglou", "Petros", ""], ["Jalal", "Hawre", ""]]}, {"id": "2001.08038", "submitter": "Andrew Manderson", "authors": "Andrew A. Manderson and Robert J. B. Goudie", "title": "A numerically stable algorithm for integrating Bayesian models using\n  Markov melding", "comments": "18 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When statistical analyses consider multiple data sources, Markov melding\nprovides a method for combining the source-specific Bayesian models. Models\noften contain different quantities of information due to variation in the\nrichness of model-specific data, or availability of model-specific prior\ninformation. We show that this can make the multi-stage Markov chain Monte\nCarlo sampler employed by Markov melding unstable and unreliable. We propose a\nrobust multi-stage algorithm that estimates the required prior marginal\nself-density ratios using weighted samples, dramatically improving accuracy in\nthe tails of the distribution, thus stabilising the algorithm and providing\nreliable inference. We demonstrate our approach using an evidence synthesis for\ninferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:45:22 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Manderson", "Andrew A.", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2001.08308", "submitter": "Jagath Senarathne", "authors": "S. G. Jagath Senarathne, James M. McGree, Werner G. M\\\"uller", "title": "Bayesian design for minimising uncertainty in spatial processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based geostatistical design involves the selection of locations to\ncollect data to minimise an expected loss function over a set of all possible\nlocations. The loss function is specified to reflect the aim of data\ncollection, which, for geostatistical studies, would typically be to minimise\nthe uncertainty in a spatial process. In this paper, we propose a new approach\nto design such studies via a loss function derived through considering the\nentropy of model predictions, and we show that this simultaneously addresses\nthe goal of precise parameter estimation. One drawback of this loss function is\nthat is it computationally expensive to evaluate, so we provide an efficient\napproximation such that it can be used within realistically sized\ngeostatistical studies. To demonstrate our approach, we apply the proposed\napproach to design the collection of spatially dependent multiple responses,\nand compare this with either designing for estimation or prediction only. The\nresults show that our designs remain highly efficient in achieving each\nexperimental objective individually, and provide an ideal compromise between\nthe two objectives. Accordingly, we advocate that our design approach should be\nused more generally in model-based geostatistical studies.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 23:37:17 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 03:00:23 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Senarathne", "S. G. Jagath", ""], ["McGree", "James M.", ""], ["M\u00fcller", "Werner G.", ""]]}, {"id": "2001.08327", "submitter": "Himel Mallick", "authors": "Himel Mallick, Rahim Alhamzawi, Erina Paul, Vladimir Svetnik", "title": "The Reciprocal Bayesian LASSO", "comments": "35 pages, 2 figures, 4 tables; includes revised simulation and real\n  data analysis results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reciprocal LASSO (rLASSO) regularization employs a decreasing penalty\nfunction as opposed to conventional penalization approaches that use increasing\npenalties on the coefficients, leading to stronger parsimony and superior model\nselection relative to traditional shrinkage methods. Here we consider a fully\nBayesian formulation of the rLASSO problem, which is based on the observation\nthat the rLASSO estimate for linear regression parameters can be interpreted as\na Bayesian posterior mode estimate when the regression parameters are assigned\nindependent inverse Laplace priors. Bayesian inference from this posterior is\npossible using an expanded hierarchy motivated by a scale mixture of double\nPareto or truncated normal distributions. On simulated and real datasets, we\nshow that the Bayesian formulation outperforms its classical cousin in\nestimation, prediction, and variable selection across a wide range of scenarios\nwhile offering the advantage of posterior inference. Finally, we discuss other\nvariants of this new approach and provide a unified framework for variable\nselection using flexible reciprocal penalties. All methods described in this\npaper are publicly available as an R package at:\nhttps://github.com/himelmallick/BayesRecipe.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:21:59 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:33:20 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 11:06:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mallick", "Himel", ""], ["Alhamzawi", "Rahim", ""], ["Paul", "Erina", ""], ["Svetnik", "Vladimir", ""]]}, {"id": "2001.08431", "submitter": "Thomas Yee", "authors": "Thomas William Yee", "title": "On the Hauck-Donner Effect in Wald Tests: Detection, Tipping Points, and\n  Parameter Space Characterization", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wald test remains ubiquitous in statistical practice despite shortcomings\nsuch as its inaccuracy in small samples and lack of invariance under\nreparameterization. This paper develops on another but lesser-known shortcoming\ncalled the Hauck--Donner effect (HDE) whereby a Wald test statistic is not\nmonotonely increasing as a function of increasing distance between the\nparameter estimate and the null value. Resulting in an upward biased $p$-value\nand loss of power, the aberration can lead to very damaging consequences such\nas in variable selection. The HDE afflicts many types of regression models and\ncorresponds to estimates near the boundary of the parameter space. This article\npresents several new results, and its main contributions are to (i) propose a\nvery general test for detecting the HDE, regardless of its underlying cause;\n(ii) fundamentally characterize the HDE by pairwise ratios of Wald and Rao\nscore and likelihood ratio test statistics for 1-parameter distributions; (iii)\nshow that the parameter space may be partitioned into an interior encased by 5\nHDE severity measures (faint, weak, moderate, strong, extreme); (iv) prove that\na necessary condition for the HDE in a 2 by 2 table is a log odds ratio of at\nleast 2; (v) give some practical guidelines about HDE-free hypothesis testing.\nOverall, practical post-fit tests can now be conducted potentially to any model\nestimated by iteratively reweighted least squares, such as the generalized\nlinear model (GLM) and Vector GLM (VGLM) classes, the latter which encompasses\nmany popular regression models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:15:50 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yee", "Thomas William", ""]]}, {"id": "2001.08535", "submitter": "Muhammad Adnan Dr.", "authors": "Muhammad Adnan, Fatma Outay, Shiraz Ahmed, Erika Brattich, Silvana di\n  Sabatino and Davy Janssens", "title": "Integrated Agent-based Microsimulation Framework for Examining Impacts\n  of Mobility-oriented Policies", "comments": "Post-print", "journal-ref": "Personal and Ubiquitous Computing, 2020", "doi": "10.1007/s00779-020-01363-w", "report-no": null, "categories": "physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Travel demand management measures/policies are important to sustain positive\nchanges among individuals' travel behaviour. An integrated agent-based\nmicrosimulation platform provides a rich framework for examining such\ninterventions to assess their impacts using indicators about demand as well as\nsupply side. This paper presents an approach, where individual schedules,\nderived from a lighter version of an activity-based model, are fed into a\nMATSIM simulation framework. Simulations are performed for two European cities\ni.e. Hasselt (Belgium), Bologna (Italy). After calibrating the modelling\nframework against aggregate traffic counts for the base case, the impacts of a\nfew traffic management policies (restricting car access, increase in bus\nfrequency) are examined. The results indicate that restricting car access is\nmore effective in terms of reducing traffic from the network and also shifting\ncar drivers/passengers to other modes of travel. The enhancement of bus\ninfrastructure in relation to increase in frequency caused shifting of\nbicyclist towards public transport, which is an undesirable result of the\npolicy if the objective is to improve sustainability and environment. In future\nresearch, the framework will be enhanced to integrate emission and air\ndispersion models to ascertain effects on air quality as a result of such\ninterventions.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 12:29:57 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 10:32:28 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Adnan", "Muhammad", ""], ["Outay", "Fatma", ""], ["Ahmed", "Shiraz", ""], ["Brattich", "Erika", ""], ["di Sabatino", "Silvana", ""], ["Janssens", "Davy", ""]]}, {"id": "2001.09111", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Abhirup Datta, Sudipto Banerjee", "title": "spNNGP R package for Nearest Neighbor Gaussian Process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and illustrates functionality of the spNNGP R package.\nThe package provides a suite of spatial regression models for Gaussian and\nnon-Gaussian point-referenced outcomes that are spatially indexed. The package\nimplements several Markov chain Monte Carlo (MCMC) and MCMC-free Nearest\nNeighbor Gaussian Process (NNGP) models for inference about large spatial data.\nNon-Gaussian outcomes are modeled using a NNGP Polya-Gamma latent variable.\nOpenMP parallelization options are provided to take advantage of multiprocessor\nsystems. Package features are illustrated using simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 17:30:24 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 16:10:55 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Finley", "Andrew O.", ""], ["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2001.09186", "submitter": "James Townsend", "authors": "James Townsend", "title": "A tutorial on the range variant of asymmetric numeral systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is intended to be a brief and accessible introduction to the range\nvariant of asymmetric numeral systems (ANS), a system for lossless compression\nof sequences which can be used as a drop in replacement for arithmetic coding\n(AC). Because of the relative simplicity of ANS, we are able to provide enough\nmathematical detail to rigorously prove that ANS attains a compression rate\nclose to the Shannon limit. Pseudo-code, intuitive interpretation and diagrams\nare given alongside the mathematical derivations. A working Python demo which\naccompanies this tutorial is available at\nhttps://raw.githubusercontent.com/j-towns/ans-notes/master/rans.py.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 20:18:54 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 10:14:22 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 08:58:47 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Townsend", "James", ""]]}, {"id": "2001.09187", "submitter": "Jonas Latz", "authors": "Daniel Kressner, Jonas Latz, Stefano Massei, Elisabeth Ullmann", "title": "Certified and fast computations with shallow covariance kernels", "comments": null, "journal-ref": "Foundations of Data Science 2(4): 487-512, 2020", "doi": "10.3934/fods.2020022", "report-no": null, "categories": "math.NA cs.LG cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many techniques for data science and uncertainty quantification demand\nefficient tools to handle Gaussian random fields, which are defined in terms of\ntheir mean functions and covariance operators. Recently, parameterized Gaussian\nrandom fields have gained increased attention, due to their higher degree of\nflexibility. However, especially if the random field is parameterized through\nits covariance operator, classical random field discretization techniques fail\nor become inefficient. In this work we introduce and analyze a new and\ncertified algorithm for the low-rank approximation of a parameterized family of\ncovariance operators which represents an extension of the adaptive cross\napproximation method for symmetric positive definite matrices. The algorithm\nrelies on an affine linear expansion of the covariance operator with respect to\nthe parameters, which needs to be computed in a preprocessing step using, e.g.,\nthe empirical interpolation method. We discuss and test our new approach for\nisotropic covariance kernels, such as Mat\\'ern kernels. The numerical results\ndemonstrate the advantages of our approach in terms of computational time and\nconfirm that the proposed algorithm provides the basis of a fast sampling\nprocedure for parameter dependent Gaussian random fields.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 20:28:05 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 10:01:10 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 11:16:20 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 18:48:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kressner", "Daniel", ""], ["Latz", "Jonas", ""], ["Massei", "Stefano", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "2001.09188", "submitter": "Arnaud  Doucet", "authors": "George Deligiannidis, Arnaud Doucet and Sylvain Rubenthaler", "title": "Ensemble Rejection Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Ensemble Rejection Sampling, a scheme for exact simulation from\nthe posterior distribution of the latent states of a class of non-linear\nnon-Gaussian state-space models. Ensemble Rejection Sampling relies on a\nproposal for the high-dimensional state sequence built using ensembles of state\nsamples. Although this algorithm can be interpreted as a rejection sampling\nscheme acting on an extended space, we show under regularity conditions that\nthe expected computational cost to obtain an exact sample increases cubically\nwith the length of the state sequence instead of exponentially for standard\nrejection sampling. We demonstrate this methodology by sampling exactly state\nsequences according to the posterior distribution of a stochastic volatility\nmodel and a non-linear autoregressive process. We also present an application\nto rare event simulation.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 20:34:43 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""], ["Rubenthaler", "Sylvain", ""]]}, {"id": "2001.09367", "submitter": "Andrew Roth", "authors": "Alexandre Bouchard-C\\^ot\\'e and Andrew Roth", "title": "Particle-Gibbs Sampling For Bayesian Feature Allocation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian feature allocation models are a popular tool for modelling data with\na combinatorial latent structure. Exact inference in these models is generally\nintractable and so practitioners typically apply Markov Chain Monte Carlo\n(MCMC) methods for posterior inference. The most widely used MCMC strategies\nrely on an element wise Gibbs update of the feature allocation matrix. These\nelement wise updates can be inefficient as features are typically strongly\ncorrelated. To overcome this problem we have developed a Gibbs sampler that can\nupdate an entire row of the feature allocation matrix in a single move.\nHowever, this sampler is impractical for models with a large number of features\nas the computational complexity scales exponentially in the number of features.\nWe develop a Particle Gibbs sampler that targets the same distribution as the\nrow wise Gibbs updates, but has computational complexity that only grows\nlinearly in the number of features. We compare the performance of our proposed\nmethods to the standard Gibbs sampler using synthetic data from a range of\nfeature allocation models. Our results suggest that row wise updates using the\nPG methodology can significantly improve the performance of samplers for\nfeature allocation models.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 22:11:51 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Roth", "Andrew", ""]]}, {"id": "2001.09593", "submitter": "Daniel Fryer Mr", "authors": "Daniel Fryer and Inga Strumke and Hien Nguyen", "title": "Shapley value confidence intervals for attributing variance explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient of determination, the $R^2$, is often used to measure the\nvariance explained by an affine combination of multiple explanatory covariates.\nAn attribution of this explanatory contribution to each of the individual\ncovariates is often sought in order to draw inference regarding the importance\nof each covariate with respect to the response phenomenon. A recent method for\nascertaining such an attribution is via the game theoretic Shapley value\ndecomposition of the coefficient of determination. Such a decomposition has the\ndesirable efficiency, monotonicity, and equal treatment properties. Under a\nweak assumption that the joint distribution is pseudo-elliptical, we obtain the\nasymptotic normality of the Shapley values. We then utilize this result in\norder to construct confidence intervals and hypothesis tests regarding such\nquantities. Monte Carlo studies regarding our results are provided. We found\nthat our asymptotic confidence intervals are computationally superior to\ncompeting bootstrap methods and are able to improve upon the performance of\nsuch intervals. In an expository application to Australian real estate price\nmodelling, we employ Shapley value confidence intervals to identify significant\ndifferences between the explanatory contributions of covariates, between\nmodels, which otherwise share approximately the same $R^2$ value. These\ndifferent models are based on real estate data from the same periods in 2019\nand 2020, the latter covering the early stages of the arrival of the novel\ncoronavirus, COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 05:53:12 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 05:45:13 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fryer", "Daniel", ""], ["Strumke", "Inga", ""], ["Nguyen", "Hien", ""]]}, {"id": "2001.10168", "submitter": "HaiYing Wang", "authors": "HaiYing Wang and Yanyuan Ma", "title": "Optimal subsampling for quantile regression in big data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate optimal subsampling for quantile regression. We derive the\nasymptotic distribution of a general subsampling estimator and then derive two\nversions of optimal subsampling probabilities. One version minimizes the trace\nof the asymptotic variance-covariance matrix for a linearly transformed\nparameter estimator and the other minimizes that of the original parameter\nestimator. The former does not depend on the densities of the responses given\ncovariates and is easy to implement. Algorithms based on optimal subsampling\nprobabilities are proposed and asymptotic distributions and asymptotic\noptimality of the resulting estimators are established. Furthermore, we propose\nan iterative subsampling procedure based on the optimal subsampling\nprobabilities in the linearly transformed parameter estimation which has great\nscalability to utilize available computational resources. In addition, this\nprocedure yields standard errors for parameter estimators without estimating\nthe densities of the responses given the covariates. We provide numerical\nexamples based on both simulated and real data to illustrate the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 05:03:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "HaiYing", ""], ["Ma", "Yanyuan", ""]]}, {"id": "2001.10451", "submitter": "Taylor Brown", "authors": "Taylor R. Brown", "title": "PF: A C++ Library for Fast Particle Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filters are a class of algorithms that are used for \"tracking\" or\n\"filtering\" in real-time for a wide array of time series models. Despite their\ncomprehensive applicability, particle filters are not always the tool of choice\nfor many practitioners, due to how difficult they are to implement. This short\narticle presents PF, a C++ header-only template library that provides fast\nimplementations of many different particle filters. A tutorial along with an\nextensive fully-worked example is provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 16:30:11 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Brown", "Taylor R.", ""]]}, {"id": "2001.10954", "submitter": "Charles Houston", "authors": "C. Houston, B. Marchand, L. Engelbert, C. D. Cantwell", "title": "Reducing complexity and unidentifiability when modelling human atrial\n  cells", "comments": "15 pages, 6 figures, submitted to Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2019.0339", "report-no": null, "categories": "q-bio.QM stat.AP stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical models of a cellular action potential in cardiac modelling have\nbecome increasingly complex, particularly in gating kinetics which control the\nopening and closing of individual ion channel currents. As cardiac models\nadvance towards use in personalised medicine to inform clinical\ndecision-making, it is critical to understand the uncertainty hidden in\nparameter estimates from their calibration to experimental data. This study\napplies approximate Bayesian computation to re-calibrate the gating kinetics of\nfour ion channels in two existing human atrial cell models to their original\ndatasets, providing a measure of uncertainty and indication of potential issues\nwith selecting a single unique value given the available experimental data. Two\napproaches are investigated to reduce the uncertainty present: re-calibrating\nthe models to a more complete dataset and using a less complex formulation with\nfewer parameters to constrain. The re-calibrated models are inserted back into\nthe full cell model to study the overall effect on the action potential. The\nuse of more complete datasets does not eliminate uncertainty present in\nparameter estimates. The less complex model, particularly for the fast sodium\ncurrent, gave a better fit to experimental data alongside lower parameter\nuncertainty and improved computational speed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 16:57:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Houston", "C.", ""], ["Marchand", "B.", ""], ["Engelbert", "L.", ""], ["Cantwell", "C. D.", ""]]}, {"id": "2001.11168", "submitter": "Ryoya Yamasaki", "authors": "Ryoya Yamasaki, Toshiyuki Tanaka", "title": "Kernel Selection for Modal Linear Regression: Optimal Kernel and IRLS\n  Algorithm", "comments": "7 pages, 4 figures, published in the proceedings of the 18th IEEE\n  International Conference on Machine Learning and Applications - ICMLA 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modal linear regression (MLR) is a method for obtaining a conditional mode\npredictor as a linear model. We study kernel selection for MLR from two\nperspectives: \"which kernel achieves smaller error?\" and \"which kernel is\ncomputationally efficient?\". First, we show that a Biweight kernel is optimal\nin the sense of minimizing an asymptotic mean squared error of a resulting MLR\nparameter. This result is derived from our refined analysis of an asymptotic\nstatistical behavior of MLR. Secondly, we provide a kernel class for which\niteratively reweighted least-squares algorithm (IRLS) is guaranteed to\nconverge, and especially prove that IRLS with an Epanechnikov kernel terminates\nin a finite number of iterations. Simulation studies empirically verified that\nusing a Biweight kernel provides good estimation accuracy and that using an\nEpanechnikov kernel is computationally efficient. Our results improve MLR of\nwhich existing studies often stick to a Gaussian kernel and modal EM algorithm\nspecialized for it, by providing guidelines of kernel selection.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 03:57:07 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Yamasaki", "Ryoya", ""], ["Tanaka", "Toshiyuki", ""]]}, {"id": "2001.11256", "submitter": "Tsuyoshi Ishizone", "authors": "Tsuyoshi Ishizone and Kazuyuki Nakamura", "title": "Real-time Linear Operator Construction and State Estimation with the\n  Kalman Filter", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kalman filter is the most powerful tool for estimation of the states of a\nlinear Gaussian system. In addition, using this method, an expectation\nmaximization algorithm can be used to estimate the parameters of the model.\nHowever, this algorithm cannot function in real time. Thus, we propose a new\nmethod that can be used to estimate the transition matrices and the states of\nthe system in real time. The proposed method uses three ideas: estimation in an\nobservation space, a time-invariant interval, and an online learning framework.\nApplied to damped oscillation model, we have obtained extraordinary performance\nto estimate the matrices. In addition, by introducing localization and spatial\nuniformity to the proposed method, we have demonstrated that noise can be\nreduced in high-dimensional spatio-temporal data. Moreover, the proposed method\nhas potential for use in areas such as weather forecasting and vector field\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 11:16:16 GMT"}, {"version": "v2", "created": "Sat, 15 Feb 2020 06:44:45 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 03:04:27 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Ishizone", "Tsuyoshi", ""], ["Nakamura", "Kazuyuki", ""]]}, {"id": "2001.11425", "submitter": "Fei Ding", "authors": "Fei Ding, Shiyuan He, David E. Jones, Jianhua Z. Huang", "title": "Supervised Functional PCA with Covariate Dependent Mean and Covariance\n  Structure", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating covariate information into functional data analysis methods can\nsubstantially improve modeling and prediction performance. However, many\nfunctional data analysis methods do not make use of covariate or supervision\ninformation, and those that do often have high computational cost or assume\nthat only the scores are related to covariates, an assumption that is usually\nviolated in practice. In this article, we propose a functional data analysis\nframework that relates both the mean and covariance function to covariate\ninformation. To facilitate modeling and ensure the covariance function is\npositive semi-definite, we represent it using splines and design a map from\nEuclidean space to the symmetric positive semi-definite matrix manifold. Our\nmodel is combined with a roughness penalty to encourage smoothness of the\nestimated functions in both the temporal and covariate domains. We also develop\nan efficient method for fast evaluation of the objective and gradient\nfunctions. Cross-validation is used to choose the tuning parameters. We\ndemonstrate the advantages of our approach through a simulation study and an\nastronomical data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:08:35 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ding", "Fei", ""], ["He", "Shiyuan", ""], ["Jones", "David E.", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2001.11548", "submitter": "Damian Brzyski", "authors": "Damian Brzyski, Xixi Hu, Joaquin Goni, Beau Ances, Timothy W.\n  Randolph, Jaroslaw Harezlak", "title": "A Sparsity Inducing Nuclear-Norm Estimator (SpINNEr) for Matrix-Variate\n  Regression in Brain Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical scalar-response regression methods treat covariates as a vector and\nestimate a corresponding vector of regression coefficients. In medical\napplications, however, regressors are often in a form of multi-dimensional\narrays. For example, one may be interested in using MRI imaging to identify\nwhich brain regions are associated with a health outcome. Vectorizing the\ntwo-dimensional image arrays is an unsatisfactory approach since it destroys\nthe inherent spatial structure of the images and can be computationally\nchallenging. We present an alternative approach - regularized matrix regression\n- where the matrix of regression coefficients is defined as a solution to the\nspecific optimization problem. The method, called SParsity Inducing Nuclear\nNorm EstimatoR (SpINNEr), simultaneously imposes two penalty types on the\nregression coefficient matrix---the nuclear norm and the lasso norm---to\nencourage a low rank matrix solution that also has entry-wise sparsity. A\nspecific implementation of the alternating direction method of multipliers\n(ADMM) is used to build a fast and efficient numerical solver. Our simulations\nshow that SpINNEr outperforms other methods in estimation accuracy when the\nresponse-related entries (representing the brain's functional connectivity) are\narranged in well-connected communities. SpINNEr is applied to investigate\nassociations between HIV-related outcomes and functional connectivity in the\nhuman brain.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 20:10:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Brzyski", "Damian", ""], ["Hu", "Xixi", ""], ["Goni", "Joaquin", ""], ["Ances", "Beau", ""], ["Randolph", "Timothy W.", ""], ["Harezlak", "Jaroslaw", ""]]}, {"id": "2001.11819", "submitter": "Dan Piponi", "authors": "Dan Piponi, Dave Moore, Joshua V. Dillon", "title": "Joint Distributions for TensorFlow Probability", "comments": "Based on extended abstract submitted to PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central tenet of probabilistic programming is that a model is specified\nexactly once in a canonical representation which is usable by inference\nalgorithms. We describe JointDistributions, a family of declarative\nrepresentations of directed graphical models in TensorFlow Probability.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 01:00:35 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Piponi", "Dan", ""], ["Moore", "Dave", ""], ["Dillon", "Joshua V.", ""]]}, {"id": "2001.11860", "submitter": "Sibo Cheng", "authors": "Sibo Cheng (EDF R&D PERICLES, LIMSI), Jean-Philippe Argaud (EDF R&D\n  PERICLES), Bertrand Iooss (EDF R&D PRISME, IMT), Ang\\'elique Pon\\c{c}ot (EDF\n  R&D PERICLES), Didier Lucor (LIMSI)", "title": "A graph clustering approach to localization for adaptive covariance\n  tuning in data assimilation based on state-observation mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An original graph clustering approach to efficient localization of error\ncovariances is proposed within an ensemble-variational data assimilation\nframework. Here the localization term is very generic and refers to the idea of\nbreaking up a global assimilation into subproblems. This unsupervised\nlocalization technique based on a linearizedstate-observation measure is\ngeneral and does not rely on any prior information such as relevant spatial\nscales, empirical cut-off radius or homogeneity assumptions. It automatically\nsegregates the state and observation variables in an optimal number of clusters\n(otherwise named as subspaces or communities), more amenable to scalable data\nassimilation.The application of this method does not require underlying\nblock-diagonal structures of prior covariance matrices. In order to deal with\ninter-cluster connectivity, two alternative data adaptations are proposed. Once\nthe localization is completed, an adaptive covariance diagnosis and tuning is\nperformed within each cluster. Numerical tests show that this approach is less\ncostly and more flexible than a global covariance tuning, and most often\nresults in more accurate background and observations error covariances.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 14:15:31 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Cheng", "Sibo", "", "EDF R&D PERICLES, LIMSI"], ["Argaud", "Jean-Philippe", "", "EDF R&D\n  PERICLES"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT"], ["Pon\u00e7ot", "Ang\u00e9lique", "", "EDF\n  R&D PERICLES"], ["Lucor", "Didier", "", "LIMSI"]]}, {"id": "2001.11950", "submitter": "Radford M. Neal", "authors": "Radford M. Neal", "title": "Non-reversibly updating a uniform [0,1] value for Metropolis\n  accept/reject decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I show how it can be beneficial to express Metropolis accept/reject decisions\nin terms of comparison with a uniform [0,1] value, u, and to then update u\nnon-reversibly, as part of the Markov chain state, rather than sampling it\nindependently each iteration. This provides a small improvement for random walk\nMetropolis and Langevin updates in high dimensions. It produces a larger\nimprovement when using Langevin updates with persistent momentum, giving\nperformance comparable to that of Hamiltonian Monte Carlo (HMC) with long\ntrajectories. This is of significance when some variables are updated by other\nmethods, since if HMC is used, these updates can be done only between\ntrajectories, whereas they can be done more often with Langevin updates. I\ndemonstrate that for a problem with some continuous variables, updated by HMC\nor Langevin updates, and also discrete variables, updated by Gibbs sampling\nbetween updates of the continuous variables, Langevin with persistent momentum\nand non-reversible updates to u samples nearly a factor of two more efficiently\nthan HMC. Benefits are also seen for a Bayesian neural network model in which\nhyperparameters are updated by Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:57:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Neal", "Radford M.", ""]]}]