[{"id": "1404.0042", "submitter": "Diego Salmer\\'on", "authors": "Diego Salmer\\'on and Juan Antonio Cano", "title": "Monte Carlo error in the Bayesian estimation of risk ratios using\n  log-binomial regression models: an efficient MCMC method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cohort studies binary outcomes are very often analyzed by logistic\nregression. However, it is well-known that when the goal is to estimate a risk\nratio, the logistic regression is inappropriate if the outcome is common. In\nthese cases, a log-binomial regression model is preferable. On the other hand,\nthe estimation of the regression coefficients of the log-binomial model is\ndifficult due to the constraints that must be imposed on these coefficients.\nBayesian methods allow a straightforward approach for log-binomial regression\nmodels, produce smaller mean squared errors and the posterior inferences can be\nobtained using the software WinBUGS. However, the Markov chain Monte Carlo\n(MCMC) methods implemented in WinBUGS can lead to a high Monte Carlo error. To\navoid this drawback we propose an MCMC algorithm that uses a reparameterization\nbased on a Poisson approximation and has been designed to efficiently explore\nthe constrained parameter space.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 21:14:29 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Salmer\u00f3n", "Diego", ""], ["Cano", "Juan Antonio", ""]]}, {"id": "1404.0099", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Daniel Selsam and Yura Perov", "title": "Venture: a higher-order probabilistic programming platform with\n  programmable inference", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Venture, an interactive virtual machine for probabilistic\nprogramming that aims to be sufficiently expressive, extensible, and efficient\nfor general-purpose use. Like Church, probabilistic models and inference\nproblems in Venture are specified via a Turing-complete, higher-order\nprobabilistic language descended from Lisp. Unlike Church, Venture also\nprovides a compositional language for custom inference strategies built out of\nscalable exact and approximate techniques. We also describe four key aspects of\nVenture's implementation that build on ideas from probabilistic graphical\nmodels. First, we describe the stochastic procedure interface (SPI) that\nspecifies and encapsulates primitive random variables. The SPI supports custom\ncontrol flow, higher-order probabilistic procedures, partially exchangeable\nsequences and ``likelihood-free'' stochastic simulators. It also supports\nexternal models that do inference over latent variables hidden from Venture.\nSecond, we describe probabilistic execution traces (PETs), which represent\nexecution histories of Venture programs. PETs capture conditional dependencies,\nexistential dependencies and exchangeable coupling. Third, we describe\npartitions of execution histories called scaffolds that factor global inference\nproblems into coherent sub-problems. Finally, we describe a family of\nstochastic regeneration algorithms for efficiently modifying PET fragments\ncontained within scaffolds. Stochastic regeneration linear runtime scaling in\ncases where many previous approaches scaled quadratically. We show how to use\nstochastic regeneration and the SPI to implement general-purpose inference\nstrategies such as Metropolis-Hastings, Gibbs sampling, and blocked proposals\nbased on particle Markov chain Monte Carlo and mean-field variational inference\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 01:44:05 GMT"}], "update_date": "2014-04-02", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Selsam", "Daniel", ""], ["Perov", "Yura", ""]]}, {"id": "1404.0221", "submitter": "Arthur White Mr.", "authors": "Arthur White and Thomas Brendan Murphy", "title": "Mixed-Membership of Experts Stochastic Blockmodel", "comments": "32 pages, 8 figures", "journal-ref": "Network Science, 4, pp 48-80 (2016)", "doi": "10.1017/nws.2015.29", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social network analysis is the study of how links between a set of actors are\nformed. Typically, it is believed that links are formed in a structured manner,\nwhich may be due to, for example, political or material incentives, and which\noften may not be directly observable. The stochastic blockmodel represents this\nstructure using latent groups which exhibit different connective properties, so\nthat conditional on the group membership of two actors, the probability of a\nlink being formed between them is represented by a connectivity matrix. The\nmixed membership stochastic blockmodel (MMSBM) extends this model to allow\nactors membership to different groups, depending on the interaction in\nquestion, providing further flexibility.\n  Attribute information can also play an important role in explaining network\nformation. Network models which do not explicitly incorporate covariate\ninformation require the analyst to compare fitted network models to additional\nattributes in a post-hoc manner. We introduce the mixed membership of experts\nstochastic blockmodel, an extension to the MMSBM which incorporates covariate\nactor information into the existing model. The method is illustrated with\napplication to the Lazega Lawyers dataset. Model and variable selection methods\nare also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Apr 2014 12:51:23 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["White", "Arthur", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1404.0651", "submitter": "Fabian Dunker", "authors": "Fabian Dunker and Thorsten Hohage", "title": "On parameter identification in stochastic differential equations by\n  penalized maximum likelihood", "comments": null, "journal-ref": "Inverse Problems, 2014, 30, 095001", "doi": "10.1088/0266-5611/30/9/095001", "report-no": null, "categories": "stat.CO math.NA q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present nonparametric estimators for coefficients in\nstochastic differential equation if the data are described by independent,\nidentically distributed random variables. The problem is formulated as a\nnonlinear ill-posed operator equation with a deterministic forward operator\ndescribed by the Fokker-Planck equation. We derive convergence rates of the\nrisk for penalized maximum likelihood estimators with convex penalty terms and\nfor Newton-type methods. The assumptions of our general convergence results are\nverified for estimation of the drift coefficient. The advantages of\nlog-likelihood compared to quadratic data fidelity terms are demonstrated in\nMonte-Carlo simulations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 18:26:02 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Dunker", "Fabian", ""], ["Hohage", "Thorsten", ""]]}, {"id": "1404.0734", "submitter": "Aaron Fisher", "authors": "Aaron Fisher, Harris Jaffee, Michael Rosenblum", "title": "interAdapt -- An Interactive Tool for Designing and Evaluating\n  Randomized Trials with Adaptive Enrollment Criteria", "comments": "14 pages, 2 figures (software screenshots); v2 includes command line\n  function description", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interAdapt R package is designed to be used by statisticians and clinical\ninvestigators to plan randomized trials. It can be used to determine if certain\nadaptive designs offer tangible benefits compared to standard designs, in the\ncontext of investigators' specific trial goals and constraints. Specifically,\ninterAdapt compares the performance of trial designs with adaptive enrollment\ncriteria versus standard (non-adaptive) group sequential trial designs.\nPerformance is compared in terms of power, expected trial duration, and\nexpected sample size. Users can either work directly in the R console, or with\na user-friendly shiny application that requires no programming experience.\nSeveral added features are available when using the shiny application. For\nexample, the application allows users to immediately download the results of\nthe performance comparison as a csv-table, or as a printable, html-based\nreport.\n", "versions": [{"version": "v1", "created": "Wed, 2 Apr 2014 23:16:18 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 16:24:56 GMT"}], "update_date": "2014-06-19", "authors_parsed": [["Fisher", "Aaron", ""], ["Jaffee", "Harris", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1404.0880", "submitter": "Jimmy Olsson Dr", "authors": "Randal Douc, Florian Maire, Jimmy Olsson", "title": "On the use of Markov chain Monte Carlo methods for the sampling of\n  mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study asymptotic properties of different\ndata-augmentation-type Markov chain Monte Carlo algorithms sampling from\nmixture models comprising discrete as well as continuous random variables. Of\nparticular interest to us is the situation where sampling from the conditional\ndistribution of the continuous component given the discrete component is\ninfeasible. In this context, we cast Carlin & Chib's pseudo-prior method into\nthe framework of mixture models and discuss and compare different variants of\nthis scheme. We propose a novel algorithm, the FCC sampler, which is less\ncomputationally demanding than any Metropolised Carlin & Chib-type algorithm.\nThe significant gain of computational efficiency is however obtained at the\ncost of some asymptotic variance. The performance of the algorithm vis-\\`a-vis\nalternative schemes is investigated theoretically, using some recent results\nobtained in [3] for inhomogeneous Markov chains evolving alternatingly\naccording to two different reversible Markov transition kernels, as well as\nnumerically.\n", "versions": [{"version": "v1", "created": "Thu, 3 Apr 2014 12:36:09 GMT"}], "update_date": "2014-04-04", "authors_parsed": [["Douc", "Randal", ""], ["Maire", "Florian", ""], ["Olsson", "Jimmy", ""]]}, {"id": "1404.1147", "submitter": "Karthik Gurumoorthy", "authors": "Karthik S. Gurumoorthy and Anand Rangarajan", "title": "Error bounds for gradient density estimation computed from a finite\n  sample set using the method of stationary phase", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a twice continuously differentiable function $S$, we define the density\nfunction of its gradient (derivative in one dimension) $s = S^{\\prime}$ as a\nrandom variable transformation of a uniformly distributed random variable using\n$s$ as the transformation function. Given $N$ values of $S$ sampled at equally\nspaced locations, we demonstrate using the method of stationary phase that the\napproximation error between the integral of the scaled, discrete power spectrum\nof the wave function\n$\\phi^{D}_{\\tau}=\\frac{1}{\\sqrt{L}}\\exp\\left(\\frac{iS}{\\tau}\\right)$ and the\nintegral of the true density function of $s$ over an arbitrarily small interval\nis bounded above by $O(1/N)$ as $N \\rightarrow \\infty$ ($\\tau \\rightarrow 0$).\nIn addition to its easy implementation and fast computability in $O(N \\log N)$\nthat only requires computing the discrete Fourier transform, our framework for\nobtaining the derivative density does not involve any parameter selection like\nthe number of histogram bins, width of the histogram bins, width of the kernel\nparameter, number of mixture components etc. as required by other widely\napplied methods like histograms and Parzen windows.\n", "versions": [{"version": "v1", "created": "Fri, 4 Apr 2014 03:43:28 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 08:36:03 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2016 05:42:10 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Rangarajan", "Anand", ""]]}, {"id": "1404.1556", "submitter": "Christopher Fallaize", "authors": "Christopher Fallaize, Peter Green, Kanti Mardia, Stuart Barber", "title": "Bayesian Protein Sequence and Structure Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure of a protein is crucial in determining its functionality, and\nis much more conserved than sequence during evolution. A key task in structural\nbiology is to compare protein structures in order to determine evolutionary\nrelationships, estimate the function of newly-discovered structures, and\npredict unknown structures. We propose a Bayesian method for protein structure\nalignment, with the prior on alignments based on functions which penalise\n``gaps'' in the aligned sequences. We show how a broad class of penalty\nfunctions fits into this framework, and how the resulting posterior\ndistribution can be efficiently sampled. A commonly-used gap penalty function\nis shown to be a special case, and we propose a new penalty function which\nalleviates an undesirable feature of the commonly-used penalty. We illustrate\nour method on benchmark data sets, and find it competes well with popular tools\nfrom computational biology. Our method has the benefit of being able to\npotentially explore multiple competing alignments and quantify their merits\nprobabilistically. The framework naturally allows for further information such\nas amino acid sequence to be included, and could be adapted to other situations\nsuch as flexible proteins or domain swaps.\n", "versions": [{"version": "v1", "created": "Sun, 6 Apr 2014 08:56:15 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 11:30:46 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 18:22:33 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Fallaize", "Christopher", ""], ["Green", "Peter", ""], ["Mardia", "Kanti", ""], ["Barber", "Stuart", ""]]}, {"id": "1404.2063", "submitter": "Pedro Lind", "authors": "Pedro G. Lind, Matthias W\\\"achter, Joachim Peinke", "title": "Reconstructing the intermittent dynamics of the torque in wind turbines", "comments": "8 pages, 6 figures, for Conference paper of TORQUE 2014 proceedings", "journal-ref": null, "doi": "10.1088/1742-6596/524/1/012179", "report-no": null, "categories": "physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply a framework introduced in the late nineties to analyze load\nmeasurements in off-shore wind energy converters (WEC). The framework is\nborrowed from statistical physics and properly adapted to the analysis of\nmultivariate data comprising wind velocity, power production and torque\nmeasurements, taken at one single WEC. In particular, we assume that wind\nstatistics drives the fluctuations of the torque produced in the wind turbine\nand show how to extract an evolution equation of the Langevin type for the\ntorque driven by the wind velocity. It is known that the intermittent nature of\nthe atmosphere, i.e. of the wind field, is transferred to the power production\nof a wind energy converter and consequently to the shaft torque. We show that\nthe derived stochastic differential equation quantifies the dynamical coupling\nof the measured fluctuating properties as well as it reproduces the\nintermittency observed in the data. Finally, we discuss our approach in the\nlight of turbine monitoring, a particular important issue in off-shore wind\nfarms.\n", "versions": [{"version": "v1", "created": "Tue, 8 Apr 2014 09:47:39 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Lind", "Pedro G.", ""], ["W\u00e4chter", "Matthias", ""], ["Peinke", "Joachim", ""]]}, {"id": "1404.2911", "submitter": "Jason Wyse", "authors": "Jason Wyse and Nial Friel and Pierre Latouche", "title": "Inferring structure in bipartite networks using the latent block model\n  and exact ICL", "comments": "23 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of simultaneous clustering of the two node sets involved\nin a bipartite network. The approach we adopt is based on use of the exact\nintegrated complete likelihood for the latent block model. Using this allows\none to infer the number of clusters as well as cluster memberships using a\ngreedy search. This gives a model-based clustering of the node sets.\nExperiments on simulated bipartite network data show that the greedy search\napproach is vastly more scalable than competing Markov chain Monte Carlo based\nmethods. Application to a number of real observed bipartite networks\ndemonstrate the algorithms discussed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Apr 2014 19:20:20 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 13:39:46 GMT"}, {"version": "v3", "created": "Sat, 16 May 2015 10:32:10 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Wyse", "Jason", ""], ["Friel", "Nial", ""], ["Latouche", "Pierre", ""]]}, {"id": "1404.3174", "submitter": "Yang Tang", "authors": "Yang Tang, Ryan P. Browne and Paul D. McNicholas", "title": "Model Based Clustering of High-Dimensional Binary Data", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2014.12.009", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a mixture of latent trait models with common slope parameters\n(MCLT) for model-based clustering of high-dimensional binary data, a data type\nfor which few established methods exist. Recent work on clustering of binary\ndata, based on a $d$-dimensional Gaussian latent variable, is extended by\nincorporating common factor analyzers. Accordingly, our approach facilitates a\nlow-dimensional visual representation of the clusters. We extend the model\nfurther by the incorporation of random block effects. The dependencies in each\nblock are taken into account through block-specific parameters that are\nconsidered to be random variables. A variational approximation to the\nlikelihood is exploited to derive a fast algorithm for determining the model\nparameters. Our approach is demonstrated on real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:15:30 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 02:56:17 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Tang", "Yang", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1404.3177", "submitter": "Asad Hasan", "authors": "Asad Hasan, Wang Zhiyu, Alireza S. Mahani", "title": "Fast Estimation of Multinomial Logit Models: R Package mnlogit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present R package mnlogit for training multinomial logistic regression\nmodels, particularly those involving a large number of classes and features.\nCompared to existing software, mnlogit offers speedups of 10x-50x for modestly\nsized problems and more than 100x for larger problems. Running mnlogit in\nparallel mode on a multicore machine gives an additional 2x-4x speedup on up to\n8 processor cores. Computational efficiency is achieved by drastically speeding\nup calculation of the log-likelihood function's Hessian matrix by exploiting\nstructure in matrices that arise in intermediate calculations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Apr 2014 18:29:57 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 12:48:04 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Hasan", "Asad", ""], ["Zhiyu", "Wang", ""], ["Mahani", "Alireza S.", ""]]}, {"id": "1404.3816", "submitter": "Sivaram Ambikasaran", "authors": "Judith Y. Li, Sivaram Ambikasaran, Eric F. Darve, Peter K. Kitanidis", "title": "A Kalman filter powered by $\\mathcal{H}^2$-matrices for quasi-continuous\n  data assimilation problems", "comments": "18 pages, 7 figures. Water Resources Research, 2014", "journal-ref": null, "doi": "10.1002/2013WR014607", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuously tracking the movement of a fluid or a plume in the subsurface is\na challenge that is often encountered in applications, such as tracking a plume\nof injected CO$_2$ or of a hazardous substance. Advances in monitoring\ntechniques have made it possible to collect measurements at a high frequency\nwhile the plume moves, which has the potential advantage of providing\ncontinuous high-resolution images of fluid flow with the aid of data\nprocessing. However, the applicability of this approach is limited by the high\ncomputational cost associated with having to analyze large data sets within the\ntime constraints imposed by real-time monitoring. Existing data assimilation\nmethods have computational requirements that increase super-linearly with the\nsize of the unknowns $m$. In this paper, we present the HiKF, a new Kalman\nfilter (KF) variant powered by the hierarchical matrix approach that\ndramatically reduces the computational and storage cost of the standard KF from\n$\\mathcal{O}(m^2)$ to $\\mathcal{O}(m)$, while producing practically the same\nresults. The version of HiKF that is presented here takes advantage of the\nso-called random walk dynamical model, which is tailored to a class of data\nassimilation problems in which measurements are collected quasi-continuously.\nThe proposed method has been applied to a realistic CO$_2$ injection model and\ncompared with the ensemble Kalman filter (EnKF). Numerical results show that\nHiKF can provide estimates that are more accurate than EnKF, and also\ndemonstrate the usefulness of modeling the system dynamics as a random walk in\nthis context.\n", "versions": [{"version": "v1", "created": "Tue, 15 Apr 2014 04:43:03 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Li", "Judith Y.", ""], ["Ambikasaran", "Sivaram", ""], ["Darve", "Eric F.", ""], ["Kitanidis", "Peter K.", ""]]}, {"id": "1404.4178", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Robert Kohn, Mattias Villani, Minh-Ngoc Tran", "title": "Speeding Up MCMC by Efficient Data Subsampling", "comments": "Main changes: The theory has been significantly revised", "journal-ref": null, "doi": "10.1080/01621459.2018.1448827", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Subsampling MCMC, a Markov Chain Monte Carlo (MCMC) framework\nwhere the likelihood function for $n$ observations is estimated from a random\nsubset of $m$ observations. We introduce a highly efficient unbiased estimator\nof the log-likelihood based on control variates, such that the computing cost\nis much smaller than that of the full log-likelihood in standard MCMC. The\nlikelihood estimate is bias-corrected and used in two dependent pseudo-marginal\nalgorithms to sample from a perturbed posterior, for which we derive the\nasymptotic error with respect to $n$ and $m$, respectively. We propose a\npractical estimator of the error and show that the error is negligible even for\na very small $m$ in our applications. We demonstrate that Subsampling MCMC is\nsubstantially more efficient than standard MCMC in terms of sampling efficiency\nfor a given computational budget, and that it outperforms other subsampling\nmethods for MCMC proposed in the literature.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 09:33:36 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 19:45:08 GMT"}, {"version": "v3", "created": "Tue, 2 Feb 2016 07:05:04 GMT"}, {"version": "v4", "created": "Mon, 12 Dec 2016 15:39:30 GMT"}, {"version": "v5", "created": "Wed, 2 Aug 2017 00:29:59 GMT"}, {"version": "v6", "created": "Mon, 1 Jan 2018 05:19:34 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Villani", "Mattias", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1404.4185", "submitter": "Peter Neal", "authors": "Peter Neal", "title": "Simulation based sequential Monte Carlo methods for discretely observed\n  Markov processes", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter estimation for discretely observed Markov processes is a\nchallenging problem. However, simulation of Markov processes is straightforward\nusing the Gillespie algorithm. We exploit this ease of simulation to develop an\neffective sequential Monte Carlo (SMC) algorithm for obtaining samples from the\nposterior distribution of the parameters. In particular, we introduce two key\ninnovations, coupled simulations, which allow us to study multiple parameter\nvalues on the basis of a single simulation, and a simple, yet effective,\nimportance sampling scheme for steering simulations towards the observed data.\nThese innovations substantially improve the efficiency of the SMC algorithm\nwith minimal effect on the speed of the simulation process. The SMC algorithm\nis successfully applied to two examples, a Lotka-Volterra model and a\nRepressilator model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 10:13:57 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Neal", "Peter", ""]]}, {"id": "1404.4272", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu", "title": "Fast and exact implementation of 3-dimensional Tukey depth regions", "comments": "This manuscript is still under a revision. Any comment would be\n  deeply appreciated!", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tukey depth regions are important notions in nonparametric multivariate data\nanalysis. A $\\tau$-th Tukey depth region $\\mathcal{D}_{\\tau}$ is the set of all\npoints that have at least depth $\\tau$. While the Tukey depth regions are\neasily defined and interpreted as $p$-variate quantiles, their practical\napplications is impeded by the lack of efficient computational procedures in\ndimensions with $p > 2$. Feasible algorithms are available, but practically\nvery slow. In this paper we present a new exact algorithm for 3-dimensional\ndata. An efficient implementation is also provided. Data examples indicate that\nthe proposed algorithm runs much faster than the existing ones.\n", "versions": [{"version": "v1", "created": "Wed, 16 Apr 2014 14:44:53 GMT"}], "update_date": "2014-04-17", "authors_parsed": [["Liu", "Xiaohui", ""]]}, {"id": "1404.5100", "submitter": "Kshitij Khare", "authors": "Kshitij Khare and Bala Rajaratnam", "title": "Convergence of cyclic coordinatewise l1 minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the general problem of minimizing an objective function which is\nthe sum of a convex function (not strictly convex) and absolute values of a\nsubset of variables (or equivalently the l1-norm of the variables). This\nproblem appears exten- sively in modern statistical applications associated\nwith high-dimensional data or \"big data\", and corresponds to optimizing\nl1-regularized likelihoods in the context of model selection. In such\napplications, cyclic coordinatewise minimization (CCM), where the objective\nfunction is sequentially minimized with respect to each individual coordi-\nnate, is often employed as it offers a computationally cheap and effective\noptimization method. Consequently, it is crucial to obtain theoretical\nguarantees of convergence for the sequence of iterates produced by the cyclic\ncoordinatewise minimization in this setting. Moreover, as the objective\ncorresponds to at l1-regularized likelihoods of many variables, it is important\nto obtain convergence of the iterates themselves, and not just the function\nvalues. Previous results in the literature only establish either, (i) that\nevery limit point of the sequence of iterates is a stationary point of the\nobjective function, or (ii) establish convergence under special assumptions, or\n(iii) establish con- vergence for a different minimization approach (which uses\nquadratic approximation based gradient descent followed by an inexact line\nsearch), (iv) establish convergence of only the function values of the sequence\nof iterates produced by random coordinatewise minimization (a variant of CCM).\nIn this paper, a rigorous general proof of convergence for the cyclic\ncoordinatewise minimization algorithm is provided. We demonstrate the\nusefulness of our general results in contemporary applications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Apr 2014 03:33:16 GMT"}, {"version": "v2", "created": "Sun, 14 Sep 2014 00:08:41 GMT"}, {"version": "v3", "created": "Thu, 29 Jan 2015 15:39:02 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Khare", "Kshitij", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1404.5666", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie", "title": "An Importance Sampling Algorithm for the Ising Model with Strong\n  Couplings", "comments": "Proc. 2016 Int. Zurich Seminar on Communications (IZS), Zurich,\n  Switzerland, March 2-4, 2016, pp. 180-184", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the partition function of the\nferromagnetic Ising model in a consistent external magnetic field. The\nestimation is done via importance sampling in the dual of the Forney factor\ngraph representing the model. Emphasis is on models at low temperature\n(corresponding to models with strong couplings) and on models with a mixture of\nstrong and weak coupling parameters.\n", "versions": [{"version": "v1", "created": "Tue, 22 Apr 2014 23:08:27 GMT"}, {"version": "v2", "created": "Mon, 16 Jun 2014 17:46:01 GMT"}, {"version": "v3", "created": "Tue, 26 Aug 2014 03:41:40 GMT"}, {"version": "v4", "created": "Thu, 18 Dec 2014 16:28:56 GMT"}, {"version": "v5", "created": "Fri, 13 Feb 2015 15:48:29 GMT"}, {"version": "v6", "created": "Fri, 22 Jul 2016 10:10:45 GMT"}, {"version": "v7", "created": "Thu, 26 Jan 2017 18:23:41 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Molkaraie", "Mehdi", ""]]}, {"id": "1404.5745", "submitter": "Paolo Lella", "authors": "Daniele Agostini, Davide Alberelli, Francesco Grande and Paolo Lella", "title": "The maximum likelihood degree of Fermat hypersurfaces", "comments": "Final version. Accepted for publication on Journal of Algebraic\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.AC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the critical points of the likelihood function over the Fermat\nhypersurface. This problem is related to one of the main problems in\nstatistical optimization: maximum likelihood estimation. The number of critical\npoints over a projective variety is a topological invariant of the variety and\nis called maximum likelihood degree. We provide closed formulas for the maximum\nlikelihood degree of any Fermat curve in the projective plane and of Fermat\nhypersurfaces of degree 2 in any projective space. Algorithmic methods to\ncompute the ML degree of a generic Fermat hypersurface are developed throughout\nthe paper. Such algorithms heavily exploit the symmetries of the varieties we\nare considering. A computational comparison of the different methods and a list\nof the maximum likelihood degrees of several Fermat hypersurfaces are available\nin the last section.\n", "versions": [{"version": "v1", "created": "Wed, 23 Apr 2014 08:49:02 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 12:53:29 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Agostini", "Daniele", ""], ["Alberelli", "Davide", ""], ["Grande", "Francesco", ""], ["Lella", "Paolo", ""]]}, {"id": "1404.6225", "submitter": "Manuel Chiach\\'io-Ruano", "authors": "Manuel Chiachio and James L. Beck and Juan Chiachio and Guillermo Rus", "title": "Approximate Bayesian Computation by Subset Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new Approximate Bayesian Computation (ABC) algorithm for Bayesian updating\nof model parameters is proposed in this paper, which combines the ABC\nprinciples with the technique of Subset Simulation for efficient rare-event\nsimulation, first developed in S.K. Au and J.L. Beck [1]. It has been named\nABC- SubSim. The idea is to choose the nested decreasing sequence of regions in\nSubset Simulation as the regions that correspond to increasingly closer\napproximations of the actual data vector in observation space. The efficiency\nof the algorithm is demonstrated in two examples that illustrate some of the\nchallenges faced in real-world applications of ABC. We show that the proposed\nalgorithm outperforms other recent sequential ABC algorithms in terms of\ncomputational efficiency while achieving the same, or better, measure of ac-\ncuracy in the posterior distribution. We also show that ABC-SubSim readily\nprovides an estimate of the evidence (marginal likelihood) for posterior model\nclass assessment, as a by-product.\n", "versions": [{"version": "v1", "created": "Thu, 24 Apr 2014 18:55:55 GMT"}], "update_date": "2014-04-25", "authors_parsed": [["Chiachio", "Manuel", ""], ["Beck", "James L.", ""], ["Chiachio", "Juan", ""], ["Rus", "Guillermo", ""]]}, {"id": "1404.6298", "submitter": "Luke Bornn", "authors": "Luke Bornn, Natesh Pillai, Aaron Smith, Dawn Woodard", "title": "The Use of a Single Pseudo-Sample in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the computational efficiency of approximate Bayesian computation\n(ABC), which approximates a likelihood function by drawing pseudo-samples from\nthe associated model. For the rejection sampling version of ABC, it is known\nthat multiple pseudo-samples cannot substantially increase (and can\nsubstantially decrease) the efficiency of the algorithm as compared to\nemploying a high-variance estimate based on a single pseudo-sample. We show\nthat this conclusion also holds for a Markov chain Monte Carlo version of ABC,\nimplying that it is unnecessary to tune the number of pseudo-samples used in\nABC-MCMC. This conclusion is in contrast to particle MCMC methods, for which\nincreasing the number of particles can provide large gains in computational\nefficiency.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 01:13:02 GMT"}, {"version": "v2", "created": "Fri, 18 Jul 2014 03:30:20 GMT"}, {"version": "v3", "created": "Mon, 6 Oct 2014 17:51:48 GMT"}, {"version": "v4", "created": "Thu, 2 Apr 2015 21:08:42 GMT"}, {"version": "v5", "created": "Tue, 16 Feb 2016 22:07:01 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Bornn", "Luke", ""], ["Pillai", "Natesh", ""], ["Smith", "Aaron", ""], ["Woodard", "Dawn", ""]]}, {"id": "1404.6473", "submitter": "Lucas Mentch", "authors": "Lucas Mentch, Giles Hooker", "title": "Quantifying Uncertainty in Random Forests via Confidence Intervals and\n  Hypothesis Tests", "comments": "To appear in The Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work develops formal statistical inference procedures for machine\nlearning ensemble methods. Ensemble methods based on bootstrapping, such as\nbagging and random forests, have improved the predictive accuracy of individual\ntrees, but fail to provide a framework in which distributional results can be\neasily determined. Instead of aggregating full bootstrap samples, we consider\npredicting by averaging over trees built on subsamples of the training set and\ndemonstrate that the resulting estimator takes the form of a U-statistic. As\nsuch, predictions for individual feature vectors are asymptotically normal,\nallowing for confidence intervals to accompany predictions. In practice, a\nsubset of subsamples is used for computational speed; here our estimators take\nthe form of incomplete U-statistics and equivalent results are derived. We\nfurther demonstrate that this setup provides a framework for testing the\nsignificance of features. Moreover, the internal estimation method we develop\nallows us to estimate the variance parameters and perform these inference\nprocedures at no additional computational cost. Simulations and illustrations\non a real dataset are provided.\n", "versions": [{"version": "v1", "created": "Fri, 25 Apr 2014 16:15:59 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 18:52:49 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Mentch", "Lucas", ""], ["Hooker", "Giles", ""]]}, {"id": "1404.6635", "submitter": "Gugan Thoppe", "authors": "Gugan Thoppe, Vivek S. Borkar, Dinesh Garg", "title": "Greedy Block Coordinate Descent (GBCD) Method for High Dimensional\n  Quadratic Programs", "comments": "29 pages, 3 figures, New references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional unconstrained quadratic programs (UQPs) involving massive\ndatasets are now common in application areas such as web, social networks, etc.\nUnless computational resources that match up to these datasets are available,\nsolving such problems using classical UQP methods is very difficult. This paper\ndiscusses alternatives. We first define high dimensional compliant (HDC)\nmethods for UQPs---methods that can solve high dimensional UQPs by adapting to\navailable computational resources. We then show that the class of block\nKaczmarz and block coordinate descent (BCD) are the only existing methods that\ncan be made HDC. As a possible answer to the question of the `best' amongst BCD\nmethods for UQP, we propose a novel greedy BCD (GBCD) method with serial,\nparallel and distributed variants. Convergence rates and numerical tests\nconfirm that the GBCD is indeed an effective method to solve high dimensional\nUQPs. In fact, it sometimes beats even the conjugate gradient.\n", "versions": [{"version": "v1", "created": "Sat, 26 Apr 2014 11:36:46 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 12:05:55 GMT"}, {"version": "v3", "created": "Sat, 12 Jul 2014 08:04:36 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Thoppe", "Gugan", ""], ["Borkar", "Vivek S.", ""], ["Garg", "Dinesh", ""]]}, {"id": "1404.6909", "submitter": "Matti Vihola", "authors": "Christophe Andrieu and Matti Vihola", "title": "Establishing some order amongst exact approximations of MCMCs", "comments": "32 pages, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact approximations of Markov chain Monte Carlo (MCMC) algorithms are a\ngeneral emerging class of sampling algorithms. One of the main ideas behind\nexact approximations consists of replacing intractable quantities required to\nrun standard MCMC algorithms, such as the target probability density in a\nMetropolis-Hastings algorithm, with estimators. Perhaps surprisingly, such\napproximations lead to powerful algorithms which are exact in the sense that\nthey are guaranteed to have correct limiting distributions. In this paper we\ndiscover a general framework which allows one to compare, or order, performance\nmeasures of two implementations of such algorithms. In particular, we establish\nan order with respect to the mean acceptance probability, the first\nautocorrelation coefficient, the asymptotic variance and the right spectral\ngap. The key notion to guarantee the ordering is that of the convex order\nbetween estimators used to implement the algorithms. We believe that our convex\norder condition is close to optimal, and this is supported by a counter-example\nwhich shows that a weaker variance order is not sufficient. The convex order\nplays a central role by allowing us to construct a martingale coupling which\nenables the comparison of performance measures of Markov chain with differing\ninvariant distributions, contrary to existing results. We detail applications\nof our result by identifying extremal distributions within given classes of\napproximations, by showing that averaging replicas improves performance in a\nmonotonic fashion and that stratification is guaranteed to improve performance\nfor the standard implementation of the Approximate Bayesian Computation (ABC)\nMCMC method.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 09:45:11 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 09:40:42 GMT"}], "update_date": "2015-10-30", "authors_parsed": [["Andrieu", "Christophe", ""], ["Vihola", "Matti", ""]]}, {"id": "1404.7188", "submitter": "Matthias Morzfeld", "authors": "Fei Lu, Matthias Morzfeld, Xuemin Tu, Alexandre J. Chorin", "title": "Limitations of polynomial chaos expansions in the Bayesian solution of\n  inverse problems", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2014.11.010", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions are used to reduce the computational cost in the\nBayesian solutions of inverse problems by creating a surrogate posterior that\ncan be evaluated inexpensively. We show, by analysis and example, that when the\ndata contain significant information beyond what is assumed in the prior, the\nsurrogate posterior can be very different from the posterior, and the resulting\nestimates become inaccurate. One can improve the accuracy by adaptively\nincreasing the order of the polynomial chaos, but the cost may increase too\nfast for this to be cost effective compared to Monte Carlo sampling without a\nsurrogate posterior.\n", "versions": [{"version": "v1", "created": "Mon, 28 Apr 2014 22:33:33 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2014 19:07:08 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Lu", "Fei", ""], ["Morzfeld", "Matthias", ""], ["Tu", "Xuemin", ""], ["Chorin", "Alexandre J.", ""]]}, {"id": "1404.7397", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "A fully data-driven method for estimating the shape of a point cloud", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support $S$. Under the\nmild assumption that $S$ is $r$-convex, the smallest $r$-convex set which\ncontains the sample points is the natural estimator. The main problem for using\nthis estimator in practice is that $r$ is an unknown geometric characteristic\nof the set $S$. A stochastic algorithm is proposed for selecting it from the\ndata under the hypothesis that the sample is uniformly generated. The new\ndata-driven reconstruction of $S$ is able to achieve the same convergence rates\nas the convex hull for estimating convex sets, but under a much more flexible\nsmoothness shape condition. The practical performance of the estimator is\nillustrated through a real data example and a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 Apr 2014 15:34:04 GMT"}, {"version": "v2", "created": "Thu, 27 Nov 2014 21:30:31 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1404.7625", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos", "title": "The R Package JMbayes for Fitting Joint Models for Longitudinal and\n  Time-to-Event Data using MCMC", "comments": "42 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models for longitudinal and time-to-event data constitute an attractive\nmodeling framework that has received a lot of interest in the recent years.\nThis paper presents the capabilities of the R package JMbayes for fitting these\nmodels under a Bayesian approach using Markon chain Monte Carlo algorithms.\nJMbayes can fit a wide range of joint models, including among others joint\nmodels for continuous and categorical longitudinal responses, and provides\nseveral options for modeling the association structure between the two\noutcomes. In addition, this package can be used to derive dynamic predictions\nfor both outcomes, and offers several tools to validate these predictions in\nterms of discrimination and calibration. All these features are illustrated\nusing a real data example on patients with primary biliary cirrhosis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 08:19:50 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Rizopoulos", "Dimitris", ""]]}, {"id": "1404.7710", "submitter": "Soo-Heang Eo", "authors": "Soo-Heang Eo and Seung-Mo Hong and HyungJun Cho", "title": "Identification of Outlying Observations with Quantile Regression for\n  Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Outlying observations, which significantly deviate from other measurements,\nmay distort the conclusions of data analysis. Therefore, identifying outliers\nis one of the important problems that should be solved to obtain reliable\nresults. While there are many statistical outlier detection algorithms and\nsoftware programs for uncensored data, few are available for censored data. In\nthis article, we propose three outlier detection algorithms based on censored\nquantile regression, two of which are modified versions of existing algorithms\nfor uncensored or censored data, while the third is a newly developed algorithm\nto overcome the demerits of previous approaches. The performance of the three\nalgorithms was investigated in simulation studies. In addition, real data from\nSEER database, which contains a variety of data sets related to various\ncancers, is illustrated to show the usefulness of our methodology. The\nalgorithms are implemented into an R package OutlierDC which can be\nconveniently employed in the \\proglang{R} environment and freely obtained from\nCRAN.\n", "versions": [{"version": "v1", "created": "Wed, 30 Apr 2014 13:06:53 GMT"}], "update_date": "2014-05-01", "authors_parsed": [["Eo", "Soo-Heang", ""], ["Hong", "Seung-Mo", ""], ["Cho", "HyungJun", ""]]}]