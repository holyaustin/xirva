[{"id": "1306.0040", "submitter": "James Scott", "authors": "James G. Scott and Liang Sun", "title": "Expectation-maximization for logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of expectation-maximization (EM) algorithms for binary\nand negative-binomial logistic regression, drawing a sharp connection with the\nvariational-Bayes algorithm of Jaakkola and Jordan (2000). Indeed, our results\nallow a version of this variational-Bayes approach to be re-interpreted as a\ntrue EM algorithm. We study several interesting features of the algorithm, and\nof this previously unrecognized connection with variational Bayes. We also\ngeneralize the approach to sparsity-promoting priors, and to an online method\nwhose convergence properties are easily established. This latter method\ncompares favorably with stochastic-gradient descent in situations with marked\ncollinearity.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 21:57:12 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Scott", "James G.", ""], ["Sun", "Liang", ""]]}, {"id": "1306.0063", "submitter": "Shiwei Lan", "authors": "Shiwei Lan, Jeffrey Streets and Babak Shahbaba", "title": "Wormhole Hamiltonian Monte Carlo", "comments": null, "journal-ref": "Proc Conf AAAI Artif Intell. 2014 Jul 31, 2014, 1953-1959", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning and statistics, probabilistic inference involving\nmultimodal distributions is quite difficult. This is especially true in high\ndimensional problems, where most existing algorithms cannot easily move from\none mode to another. To address this issue, we propose a novel Bayesian\ninference approach based on Markov Chain Monte Carlo. Our method can\neffectively sample from multimodal distributions, especially when the dimension\nis high and the modes are isolated. To this end, it exploits and modifies the\nRiemannian geometric properties of the target distribution to create\n\\emph{wormholes} connecting modes in order to facilitate moving between them.\nFurther, our proposed method uses the regeneration technique in order to adapt\nthe algorithm by identifying new modes and updating the network of wormholes\nwithout affecting the stationary distribution. To find new modes, as opposed to\nrediscovering those previously identified, we employ a novel mode searching\nalgorithm that explores a \\emph{residual energy} function obtained by\nsubtracting an approximate Gaussian mixture density (based on previously\ndiscovered modes) from the target density function.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 02:16:46 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 03:09:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Lan", "Shiwei", ""], ["Streets", "Jeffrey", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1306.0187", "submitter": "Marcelo Pereyra", "authors": "Marcelo Pereyra", "title": "Proximal Markov chain Monte Carlo algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Metropolis-adjusted Langevin algorithm (MALA) that\nuses convex analysis to simulate efficiently from high-dimensional densities\nthat are log-concave, a class of probability distributions that is widely used\nin modern high-dimensional statistics and data analysis. The method is based on\na new first-order approximation for Langevin diffusions that exploits\nlog-concavity to construct Markov chains with favourable convergence\nproperties. This approximation is closely related to Moreau-Yoshida\nregularisations for convex functions and uses proximity mappings instead of\ngradient mappings to approximate the continuous-time process. The proposed\nmethod complements existing MALA methods in two ways. First, the method is\nshown to have very robust stability properties and to converge geometrically\nfor many target densities for which other MALA are not geometric, or only if\nthe step size is sufficiently small. Second, the method can be applied to\nhigh-dimensional target densities that are not continuously differentiable, a\nclass of distributions that is increasingly used in image processing and\nmachine learning and that is beyond the scope of existing MALA and HMC\nalgorithms. To use this method it is necessary to compute or to approximate\nefficiently the proximity mappings of the logarithm of the target density. For\nseveral popular models, including many Bayesian models used in modern signal\nand image processing and machine learning, this can be achieved with convex\noptimisation algorithms and with approximations based on proximal splitting\ntechniques, which can be implemented in parallel. The proposed method is\ndemonstrated on two challenging high-dimensional and non-differentiable models\nrelated to image resolution enhancement and low-rank matrix estimation that are\nnot well addressed by existing MCMC methodology.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2013 09:41:33 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 12:56:44 GMT"}, {"version": "v3", "created": "Thu, 3 Jul 2014 15:00:44 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2015 11:50:36 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Pereyra", "Marcelo", ""]]}, {"id": "1306.0735", "submitter": "Christopher Nemeth", "authors": "Christopher Nemeth, Paul Fearnhead, Lyudmila Mihaylova", "title": "Particle approximations of the score and observed information matrix for\n  parameter estimation in state space models with linear computational cost", "comments": "Accepted to Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poyiadjis et al. (2011) show how particle methods can be used to estimate\nboth the score and the observed information matrix for state space models.\nThese methods either suffer from a computational cost that is quadratic in the\nnumber of particles, or produce estimates whose variance increases\nquadratically with the amount of data. This paper introduces an alternative\napproach for estimating these terms at a computational cost that is linear in\nthe number of particles. The method is derived using a combination of kernel\ndensity estimation, to avoid the particle degeneracy that causes the\nquadratically increasing variance, and Rao-Blackwellisation. Crucially, we show\nthe method is robust to the choice of bandwidth within the kernel density\nestimation, as it has good asymptotic properties regardless of this choice. Our\nestimates of the score and observed information matrix can be used within both\nonline and batch procedures for estimating parameters for state space models.\nEmpirical results show improved parameter estimates compared to existing\nmethods at a significantly reduced computational cost. Supplementary materials\nincluding code are available.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 11:37:05 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 14:30:52 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2015 16:09:23 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Nemeth", "Christopher", ""], ["Fearnhead", "Paul", ""], ["Mihaylova", "Lyudmila", ""]]}, {"id": "1306.0818", "submitter": "Ulf Schepsmeier", "authors": "Ulf Schepsmeier", "title": "A goodness-of-fit test for regular vine copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new goodness-of-fit test for regular vine (R-vine) copula\nmodels. R-vine copulas are a very flexible class of multivariate copulas based\non a pair-copula construction (PCC). The test arises from the information\nmatrix equality and specification test proposed by White (1982) and extends the\ngoodness-of-fit test for copulas introduced by Huang and Prokhorov (2011). The\ncorresponding critical value can be approximated by asymptotic theory or\nsimulation. The simulation based test shows excellent performance with regard\nto observed size and power in an extensive simulation study, while the\nasymptotic theory based test is inaccurate for n<10000 for a 5-dimensional\nmodel (in d=8 even 20000 are not enough). The simulation based test is applied\nto select among different R-vine specifications to model the dependency among\nexchange rates.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 14:35:51 GMT"}], "update_date": "2013-06-05", "authors_parsed": [["Schepsmeier", "Ulf", ""]]}, {"id": "1306.1052", "submitter": "Aleksandr Aravkin", "authors": "Mohammad Emtiyaz Khan, Aleksandr Y. Aravkin, Michael P. Friedlander,\n  Matthias Seeger", "title": "Fast Dual Variational Inference for Non-Conjugate LGMs", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models (LGMs) are widely used in statistics and machine\nlearning. Bayesian inference in non-conjugate LGMs is difficult due to\nintractable integrals involving the Gaussian prior and non-conjugate\nlikelihoods. Algorithms based on variational Gaussian (VG) approximations are\nwidely employed since they strike a favorable balance between accuracy,\ngenerality, speed, and ease of use. However, the structure of the optimization\nproblems associated with these approximations remains poorly understood, and\nstandard solvers take too long to converge. We derive a novel dual variational\ninference approach that exploits the convexity property of the VG\napproximations. We obtain an algorithm that solves a convex optimization\nproblem, reduces the number of variational parameters, and converges much\nfaster than previous methods. Using real-world data, we demonstrate these\nadvantages on a variety of LGMs, including Gaussian process classification, and\nlatent Gaussian Markov random fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 10:45:59 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Aravkin", "Aleksandr Y.", ""], ["Friedlander", "Michael P.", ""], ["Seeger", "Matthias", ""]]}, {"id": "1306.1182", "submitter": "Agust\\'in G. Nogales", "authors": "Agust\\'in G. Nogales, P. P\\'erez, P. Monfort", "title": "A Monte Carlo Method to Approximate Conditional Expectations based on a\n  Theorem of Besicovitch: Application to Equivariant Estimation of the\n  Parameters of the General Half-Normal Distribution", "comments": "Research paper. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural Monte Carlo method to approximate conditional expectations in a\nprobabilistic framework is justified by a general result inspired on the\nBesicovitch covering theorem on differentiation of measures. The method is\nspecially useful when densities are not available or are not easy to compute.\nThe method is illustrated by means of some examples and can also be used in a\nstatistical setting to approximate the conditional expectation given a\nsufficient statistic, for instance. In fact, it is applied to evaluate the\nminimum risk equivariant estimator (MRE) of the location parameter of a general\nhalf-normal distribution since this estimator is described in terms of a\nconditional expectation for known values of the location and scale parameters.\nFor the sake of completeness, an explicit expression of the the minimum risk\nequivariant estimator of the scale parameter is given. For all we know, these\nestimators have not been given before in the literature. Simulation studies are\nrealized to compare the behavior of these estimators with that of maximum\nlikelihood and unbiased estimators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 17:37:01 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Nogales", "Agust\u00edn G.", ""], ["P\u00e9rez", "P.", ""], ["Monfort", "P.", ""]]}, {"id": "1306.1265", "submitter": "Constantinos Daskalakis", "authors": "Constantinos Daskalakis and Christos Papadimitriou", "title": "Sparse Covers for Sums of Indicators", "comments": "PTRF, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For all $n, \\epsilon >0$, we show that the set of Poisson Binomial\ndistributions on $n$ variables admits a proper $\\epsilon$-cover in total\nvariation distance of size $n^2+n \\cdot (1/\\epsilon)^{O(\\log^2 (1/\\epsilon))}$,\nwhich can also be computed in polynomial time. We discuss the implications of\nour construction for approximation algorithms and the computation of\napproximate Nash equilibria in anonymous games.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 23:16:31 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 18:18:18 GMT"}, {"version": "v3", "created": "Wed, 1 Oct 2014 22:16:02 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Papadimitriou", "Christos", ""]]}, {"id": "1306.1718", "submitter": "Ana Arribas-Gil", "authors": "Ana Arribas-Gil and Juan Romo", "title": "Shape Outlier Detection and Visualization for Functional Data: the\n  Outliergram", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to visualize and detect shape outliers in samples of\ncurves. In functional data analysis we observe curves defined over a given real\ninterval and shape outliers are those curves that exhibit a different shape\nfrom the rest of the sample. Whereas magnitude outliers, that is, curves that\nexhibit atypically high or low values at some points or across the whole\ninterval, are in general easy to identify, shape outliers are often masked\namong the rest of the curves and thus difficult to detect. In this article we\nexploit the relation between two depths for functional data to help visualizing\ncurves in terms of shape and to develop an algorithm for shape outlier\ndetection. We illustrate the use of the visualization tool, the outliergram,\nthrough several examples and asses the performance of the algorithm on a\nsimulation study. We apply them to the detection of outliers in a children\ngrowth dataset in which the girls sample is contaminated with boys curves and\nviceversa.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 13:19:46 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2013 16:04:27 GMT"}], "update_date": "2013-10-01", "authors_parsed": [["Arribas-Gil", "Ana", ""], ["Romo", "Juan", ""]]}, {"id": "1306.1999", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan, Victor M. H. Ong, David J. Nott and Ajay Jasra", "title": "Variational inference for sparse spectrum Gaussian process regression", "comments": "20 pages, 11 figures, 1 table", "journal-ref": "Statistics and Computing (2016) 26 pp 1243-1261", "doi": "10.1007/s11222-015-9600-7", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fast variational approximation scheme for Gaussian process (GP)\nregression, where the spectrum of the covariance function is subjected to a\nsparse approximation. Our approach enables uncertainty in covariance function\nhyperparameters to be treated without using Monte Carlo methods and is robust\nto overfitting. Our article makes three contributions. First, we present a\nvariational Bayes algorithm for fitting sparse spectrum GP regression models\nthat uses nonconjugate variational message passing to derive fast and efficient\nupdates. Second, we propose a novel adaptive neighbourhood technique for\nobtaining predictive inference that is effective in dealing with\nnonstationarity. Regression is performed locally at each point to be predicted\nand the neighbourhood is determined using a measure defined based on\nlengthscales estimated from an initial fit. Weighting dimensions according to\nlengthscales, this downweights variables of little relevance, leading to\nautomatic variable selection and improved prediction. Third, we introduce a\ntechnique for accelerating convergence in nonconjugate variational message\npassing by adapting step sizes in the direction of the natural gradient of the\nlower bound. Our adaptive strategy can be easily implemented and empirical\nresults indicate significant speedups.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 09:31:24 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2013 02:14:19 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2015 20:30:40 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Tan", "Linda S. L.", ""], ["Ong", "Victor M. H.", ""], ["Nott", "David J.", ""], ["Jasra", "Ajay", ""]]}, {"id": "1306.2144", "submitter": "Farhan Feroz", "authors": "F. Feroz, M.P. Hobson, E. Cameron, A.N. Pettitt", "title": "Importance Nested Sampling and the MultiNest Algorithm", "comments": "28 pages, 6 figures, 2 tables. Accepted for publication in The Open\n  Journal of Astrophysics. Code available from\n  https://github.com/farhanferoz/MultiNest/", "journal-ref": null, "doi": "10.21105/astro.1306.2144", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference involves two main computational challenges. First, in\nestimating the parameters of some model for the data, the posterior\ndistribution may well be highly multi-modal: a regime in which the convergence\nto stationarity of traditional Markov Chain Monte Carlo (MCMC) techniques\nbecomes incredibly slow. Second, in selecting between a set of competing models\nthe necessary estimation of the Bayesian evidence for each is, by definition, a\n(possibly high-dimensional) integration over the entire parameter space; again\nthis can be a daunting computational task, although new Monte Carlo (MC)\nintegration algorithms offer solutions of ever increasing efficiency. Nested\nsampling (NS) is one such contemporary MC strategy targeted at calculation of\nthe Bayesian evidence, but which also enables posterior inference as a\nby-product, thereby allowing simultaneous parameter estimation and model\nselection. The widely-used MultiNest algorithm presents a particularly\nefficient implementation of the NS technique for multi-modal posteriors. In\nthis paper we discuss importance nested sampling (INS), an alternative\nsummation of the MultiNest draws, which can calculate the Bayesian evidence at\nup to an order of magnitude higher accuracy than `vanilla' NS with no change in\nthe way MultiNest explores the parameter space. This is accomplished by\ntreating as a (pseudo-)importance sample the totality of points collected by\nMultiNest, including those previously discarded under the constrained\nlikelihood sampling of the NS algorithm. We apply this technique to several\nchallenging test problems and compare the accuracy of Bayesian evidences\nobtained with INS against those from vanilla NS.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 09:22:32 GMT"}, {"version": "v2", "created": "Sun, 19 Jan 2014 14:21:25 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2019 18:54:07 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Feroz", "F.", ""], ["Hobson", "M. P.", ""], ["Cameron", "E.", ""], ["Pettitt", "A. N.", ""]]}, {"id": "1306.2685", "submitter": "Alfredo Kalaitzis", "authors": "Alfredo Kalaitzis and Ricardo Silva", "title": "Flexible sampling of discrete data correlations without the marginal\n  distributions", "comments": "An overhauled version of the experimental section moved to the main\n  paper. Old experimental section moved to supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the joint dependence of discrete variables is a fundamental problem\nin machine learning, with many applications including prediction, clustering\nand dimensionality reduction. More recently, the framework of copula modeling\nhas gained popularity due to its modular parametrization of joint\ndistributions. Among other properties, copulas provide a recipe for combining\nflexible models for univariate marginal distributions with parametric families\nsuitable for potentially high dimensional dependence structures. More\nradically, the extended rank likelihood approach of Hoff (2007) bypasses\nlearning marginal models completely when such information is ancillary to the\nlearning task at hand as in, e.g., standard dimensionality reduction problems\nor copula parameter estimation. The main idea is to represent data by their\nobservable rank statistics, ignoring any other information from the marginals.\nInference is typically done in a Bayesian framework with Gaussian copulas, and\nit is complicated by the fact this implies sampling within a space where the\nnumber of constraints increases quadratically with the number of data points.\nThe result is slow mixing when using off-the-shelf Gibbs sampling. We present\nan efficient algorithm based on recent advances on constrained Hamiltonian\nMarkov chain Monte Carlo that is simple to implement and does not require\npaying for a quadratic cost in sample size.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2013 01:13:46 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 18:23:45 GMT"}, {"version": "v3", "created": "Thu, 14 Nov 2013 15:31:46 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Kalaitzis", "Alfredo", ""], ["Silva", "Ricardo", ""]]}, {"id": "1306.3014", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Geoffrey J. McLachlan, and Ian A. Wood", "title": "Mixtures of Spatial Spline Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an extension of the functional data analysis framework for\nunivariate functions to the analysis of surfaces: functions of two variables.\nThe spatial spline regression (SSR) approach developed can be used to model\nsurfaces that are sampled over a rectangular domain. Furthermore, combining SSR\nwith linear mixed effects models (LMM) allows for the analysis of populations\nof surfaces, and combining the joint SSR-LMM method with finite mixture models\nallows for the analysis of populations of surfaces with sub-family structures.\nThrough the mixtures of spatial splines regressions (MSSR) approach developed,\nwe present methodologies for clustering surfaces into sub-families, and for\nperforming surface-based discriminant analysis. The effectiveness of our\nmethodologies, as well as the modeling capabilities of the SSR model are\nassessed through an application to handwritten character recognition.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 03:45:47 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 02:17:23 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""], ["Wood", "Ian A.", ""]]}, {"id": "1306.3185", "submitter": "Ryan Martin", "authors": "Ryan Martin and Zhen Han", "title": "A semiparametric scale-mixture regression model and predictive recursion\n  maximum likelihood", "comments": "17 pages, 4 figures, 2 tables", "journal-ref": "Computational Statistics and Data Analysis, volume 94, pages\n  75--85, 2016", "doi": "10.1016/j.csda.2015.08.005", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To avoid specification of the error distribution in a regression model, we\npropose a general nonparametric scale mixture model for the error distribution.\nFor fitting such mixtures, the predictive recursion method is a simple and\ncomputationally efficient alternative to existing methods. We define a\npredictive recursion-based marginal likelihood function, and estimation of the\nregression parameters proceeds by maximizing this function. A hybrid predictive\nrecursion--EM algorithm is proposed for this purpose. The method's performance\nis compared with that of existing methods in simulations and real data\nanalyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 18:28:46 GMT"}, {"version": "v2", "created": "Sat, 24 May 2014 11:59:54 GMT"}, {"version": "v3", "created": "Tue, 1 Sep 2015 21:22:51 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Martin", "Ryan", ""], ["Han", "Zhen", ""]]}, {"id": "1306.3277", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray", "title": "Bayesian State-Space Modelling on High-Performance Hardware Using LibBi", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LibBi is a software package for state-space modelling and Bayesian inference\non modern computer hardware, including multi-core central processing units\n(CPUs), many-core graphics processing units (GPUs) and distributed-memory\nclusters of such devices. The software parses a domain-specific language for\nmodel specification, then optimises, generates, compiles and runs code for the\ngiven model, inference method and hardware platform. In presenting the\nsoftware, this work serves as an introduction to state-space models and the\nspecialised methods developed for Bayesian inference with them. The focus is on\nsequential Monte Carlo (SMC) methods such as the particle filter for state\nestimation, and the particle Markov chain Monte Carlo (PMCMC) and SMC^2 methods\nfor parameter estimation. All are well-suited to current computer hardware. Two\nexamples are given and developed throughout, one a linear three-element\nwindkessel model of the human arterial system, the other a nonlinear Lorenz '96\nmodel. These are specified in the prescribed modelling language, and LibBi\ndemonstrated by performing inference with them. Empirical results are\npresented, including a performance comparison of the software with different\nhardware configurations.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 01:22:16 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Murray", "Lawrence M.", ""]]}, {"id": "1306.3494", "submitter": "Jelena Bradic", "authors": "Jelena Bradic", "title": "Randomized maximum-contrast selection: subagging for large-scale\n  regression", "comments": null, "journal-ref": "Electron. J. Statist. Volume 10, Number 1 (2016), 121-170", "doi": "10.1214/15-EJS1085", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a very general method for sparse and large-scale variable\nselection. The large-scale regression settings is such that both the number of\nparameters and the number of samples are extremely large. The proposed method\nis based on careful combination of penalized estimators, each applied to a\nrandom projection of the sample space into a low-dimensional space. In one\nspecial case that we study in detail, the random projections are divided into\nnon-overlapping blocks; each consisting of only a small portion of the original\ndata. Within each block we select the projection yielding the smallest\nout-of-sample error. Our random ensemble estimator then aggregates the results\naccording to new maximal-contrast voting scheme to determine the final selected\nset. Our theoretical results illuminate the effect on performance of increasing\nthe number of non-overlapping blocks. Moreover, we demonstrate that statistical\noptimality is retained along with the computational speedup. The proposed\nmethod achieves minimax rates for approximate recovery over all estimators\nusing the full set of samples. Furthermore, our theoretical results allow the\nnumber of subsamples to grow with the subsample size and do not require\nirrepresentable condition. The estimator is also compared empirically with\nseveral other popular high-dimensional estimators via an extensive simulation\nstudy, which reveals its excellent finite-sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 19:39:54 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 07:01:09 GMT"}, {"version": "v3", "created": "Fri, 19 Sep 2014 00:43:48 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2015 15:25:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Bradic", "Jelena", ""]]}, {"id": "1306.3627", "submitter": "Pablo Andrade", "authors": "Pablo de Morais Andrade, Julio Michael Stern, Carlos Alberto de\n  Bragan\\c{c}a Pereira", "title": "Bayesian test of significance for conditional independence: The\n  multinomial model", "comments": "24 pages, 33 figures", "journal-ref": null, "doi": "10.3390/e16031376", "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence tests (CI tests) have received special attention\nlately in Machine Learning and Computational Intelligence related literature as\nan important indicator of the relationship among the variables used by their\nmodels. In the field of Probabilistic Graphical Models (PGM)--which includes\nBayesian Networks (BN) models--CI tests are especially important for the task\nof learning the PGM structure from data. In this paper, we propose the Full\nBayesian Significance Test (FBST) for tests of conditional independence for\ndiscrete datasets. FBST is a powerful Bayesian test for precise hypothesis, as\nan alternative to frequentist's significance tests (characterized by the\ncalculation of the \\emph{p-value}).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 05:21:19 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Andrade", "Pablo de Morais", ""], ["Stern", "Julio Michael", ""], ["Pereira", "Carlos Alberto de Bragan\u00e7a", ""]]}, {"id": "1306.3706", "submitter": "William Fithian", "authors": "William Fithian, Trevor Hastie", "title": "Local case-control sampling: Efficient subsampling in imbalanced data\n  sets", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1220 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 1693-1724", "doi": "10.1214/14-AOS1220", "report-no": "IMS-AOS-AOS1220", "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For classification problems with significant class imbalance, subsampling can\nreduce computational costs at the price of inflated variance in estimating\nmodel parameters. We propose a method for subsampling efficiently for logistic\nregression by adjusting the class balance locally in feature space via an\naccept-reject scheme. Our method generalizes standard case-control sampling,\nusing a pilot estimate to preferentially select examples whose responses are\nconditionally rare given their features. The biased subsampling is corrected by\na post-hoc analytic adjustment to the parameters. The method is simple and\nrequires one parallelizable scan over the full data set. Standard case-control\nsampling is inconsistent under model misspecification for the population\nrisk-minimizing coefficients $\\theta^*$. By contrast, our estimator is\nconsistent for $\\theta^*$ provided that the pilot estimate is. Moreover, under\ncorrect specification and with a consistent, independent pilot estimate, our\nestimator has exactly twice the asymptotic variance of the full-sample MLE -\neven if the selected subsample comprises a miniscule fraction of the full data\nset, as happens when the original data are severely imbalanced. The factor of\ntwo improves to $1+\\frac{1}{c}$ if we multiply the baseline acceptance\nprobabilities by $c>1$ (and weight points with acceptance probability greater\nthan 1), taking roughly $\\frac{1+c}{2}$ times as many data points into the\nsubsample. Experiments on simulated and real data show that our method can\nsubstantially outperform standard case-control subsampling.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 21:18:12 GMT"}, {"version": "v2", "created": "Tue, 23 Sep 2014 08:53:42 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Fithian", "William", ""], ["Hastie", "Trevor", ""]]}, {"id": "1306.4032", "submitter": "Anne-Marie Lyne", "authors": "Anne-Marie Lyne, Mark Girolami, Yves Atchad\\'e, Heiko Strathmann,\n  Daniel Simpson", "title": "On Russian Roulette Estimates for Bayesian Inference with\n  Doubly-Intractable Likelihoods", "comments": "Published at http://dx.doi.org/10.1214/15-STS523 in the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2015, Vol. 30, No. 4, 443-467", "doi": "10.1214/15-STS523", "report-no": "IMS-STS-STS523", "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of statistical models are \"doubly-intractable\": the likelihood\nnormalising term, which is a function of the model parameters, is intractable,\nas well as the marginal likelihood (model evidence). This means that standard\ninference techniques to sample from the posterior, such as Markov chain Monte\nCarlo (MCMC), cannot be used. Examples include, but are not confined to,\nmassive Gaussian Markov random fields, autologistic models and Exponential\nrandom graph models. A number of approximate schemes based on MCMC techniques,\nApproximate Bayesian computation (ABC) or analytic approximations to the\nposterior have been suggested, and these are reviewed here. Exact MCMC schemes,\nwhich can be applied to a subset of doubly-intractable distributions, have also\nbeen developed and are described in this paper. As yet, no general method\nexists which can be applied to all classes of models with doubly-intractable\nposteriors. In addition, taking inspiration from the Physics literature, we\nstudy an alternative method based on representing the intractable likelihood as\nan infinite series. Unbiased estimates of the likelihood can then be obtained\nby finite time stochastic truncation of the series via Russian Roulette\nsampling, although the estimates are not necessarily positive. Results from the\nQuantum Chromodynamics literature are exploited to allow the use of possibly\nnegative estimates in a pseudo-marginal MCMC scheme such that expectations with\nrespect to the posterior distribution are preserved. The methodology is\nreviewed on well-known examples such as the parameters in Ising models, the\nposterior for Fisher-Bingham distributions on the $d$-Sphere and a large-scale\nGaussian Markov Random Field model describing the Ozone Column data. This leads\nto a critical assessment of the strengths and weaknesses of the methodology\nwith pointers to ongoing research.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 22:04:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 11:07:23 GMT"}, {"version": "v3", "created": "Wed, 4 Feb 2015 18:29:05 GMT"}, {"version": "v4", "created": "Thu, 10 Dec 2015 10:47:34 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Lyne", "Anne-Marie", ""], ["Girolami", "Mark", ""], ["Atchad\u00e9", "Yves", ""], ["Strathmann", "Heiko", ""], ["Simpson", "Daniel", ""]]}, {"id": "1306.4469", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "An empirical study to check the accuracy of approximating averages of\n  ratios using ratios of averages", "comments": "3 tables, 3 figures", "journal-ref": "Journal of Informetrics, 2013, 7(4), pp. 909-913", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a number of researchers a number of publications for each author is\nsimulated using the zeta distribution and then for each publication a number of\ncitations per publication simulated. Bootstrap confidence intervals indicate\nthat the difference between the average of ratios and the ratio of averages are\nnot significant, and there are no significant differences in the distributions\nin realistic problems when using the two-sample Kolmogorov-Smirnov test to\ncompare distributions. It was found that the log-logistic distribution which is\na general form for the ratio of two correlated Pareto random variables, give a\ngood fit to the estimated ratios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 09:40:34 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 11:20:18 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1306.4508", "submitter": "Ajay Jasra", "authors": "Junshan Wang, Ajay Jasra, Maria De Iorio", "title": "Computational Methods for a Class of Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we provide an exposition of exact computational\nmethods to perform parameter inference from partially observed network models.\nIn particular, we consider the duplication attachment (DA) model which has a\nlikelihood function that typically cannot be evaluated in any reasonable\ncomputational time. We consider a number of importance sampling (IS) and\nsequential Monte Carlo (SMC) methods for approximating the likelihood of the\nnetwork model for a fixed parameter value. It is well-known that for IS, the\nrelative variance of the likelihood estimate typically grows at an exponential\nrate in the time parameter (here this is associated to the size of the\nnetwork): we prove that, under assumptions, the SMC method will have relative\nvariance which can grow only polynomially. In order to perform parameter\nestimation, we develop particle Markov chain Monte Carlo (PMCMC) algorithms to\nperform Bayesian inference. Such algorithms use the afore-mentioned SMC\nalgorithms within the transition dynamics. The approaches are illustrated\nnumerically.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 12:04:02 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Wang", "Junshan", ""], ["Jasra", "Ajay", ""], ["De Iorio", "Maria", ""]]}, {"id": "1306.5006", "submitter": "Antonio Punzo", "authors": "Luca Bagnato and Lucio De Capitani and Antonio Punzo", "title": "Improving the autodependogram using the Kulback-Leibler divergence", "comments": "We have decided to withdraw the paper due to a crucial error in\n  equation (9), that is in the definition of the p-value. This invalidates the\n  results reported into the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autodependogram is a graphical device recently proposed in the literature\nto analyze autodependencies. It is defined computing the classical Pearson\nchi-square statistics of independence at various lags in order to point out the\npresence lag-depedencies. This paper proposes an improvement of this diagram\nobtained by substituting the chi-square statistics with an estimator of the\nKulback-Leibler divergence between the bivariate density of two delayed\nvariables and the product of their marginal distributions. A simulation study,\non well-established time series models, shows that this new autodependogram is\nmore powerful than the previous one. An application to financial data is also\nshown.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 21:43:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 13:48:13 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 10:33:14 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Bagnato", "Luca", ""], ["De Capitani", "Lucio", ""], ["Punzo", "Antonio", ""]]}, {"id": "1306.5289", "submitter": "Jie Yang", "authors": "Liping Tong, Hans W. Volkmer and Jie Yang", "title": "Analytic Solutions for D-optimal Factorial Designs under Generalized\n  Linear Models", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two analytic approaches to solve D-optimal approximate designs\nunder generalized linear models. The first approach provides analytic D-optimal\nallocations for generalized linear models with two factors, which include as a\nspecial case the $2^2$ main-effects model considered by Yang, Mandal and\nMajumdar (2012). The second approach leads to explicit solutions for a class of\ngeneralized linear models with more than two factors. With the aid of the\nanalytic solutions, we provide a necessary and sufficient condition under which\na D-optimal design with two quantitative factors could be constructed on the\nboundary points only. It bridges the gap between D-optimal factorial designs\nand D-optimal designs with continuous factors.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 05:06:53 GMT"}, {"version": "v2", "created": "Sun, 21 Jul 2013 05:19:06 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 05:26:10 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Tong", "Liping", ""], ["Volkmer", "Hans W.", ""], ["Yang", "Jie", ""]]}, {"id": "1306.5294", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsky", "title": "A Note on Computing Extreme Tail Probabilities of the Noncentral T\n  Distribution with Large Noncentrality Parameter", "comments": "Preprint submitted to Acta Universitatis Palackianae Olomucensis,\n  Facultas rerum naturalium, Mathematica, submitted June 21, 2013, revised\n  September 4, 2013", "journal-ref": "Acta Universitatis Palackianae Olomucensis Facultas Rerum\n  Naturalium Mathematica (52) 2013 131-143", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noncentral $t$-distribution is a generalization of the Student's\n$t$-distribution. In this paper we suggest an alternative approach for\ncomputing the cumulative distribution function (CDF) of the noncentral\n$t$-distribution which is based on a direct numerical integration of a well\nbehaved function. With a double-precision arithmetic, the algorithm provides\nhighly precise and fast evaluation of the extreme tail probabilities of the\nnoncentral $t$-distribution, even for large values of the noncentrality\nparameter $\\delta$ and the degrees of freedom $\\nu$. The implementation of the\nalgorithm is available at the MATLAB Central, File Exchange:\nhttp://www.mathworks.com/matlabcentral/fileexchange/41790-nctcdfvw.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2013 06:26:46 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2013 15:47:20 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Witkovsky", "Viktor", ""]]}, {"id": "1306.5368", "submitter": "Sanjeena Subedi", "authors": "Sanjeena Subedi and Paul D. McNicholas", "title": "A Variational Approximations-DIC Rubric for Parameter Estimation and\n  Mixture Model Selection Within a Family Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture model-based clustering has become an increasingly popular data\nanalysis technique since its introduction over fifty years ago, and is now\ncommonly utilized within a family setting. Families of mixture models arise\nwhen the component parameters, usually the component covariance (or scale)\nmatrices, are decomposed and a number of constraints are imposed. Within the\nfamily setting, model selection involves choosing the member of the family,\ni.e., the appropriate covariance structure, in addition to the number of\nmixture components. To date, the Bayesian information criterion (BIC) has\nproved most effective for model selection, and the expectation-maximization\n(EM) algorithm is usually used for parameter estimation. In fact, this EM-BIC\nrubric has virtually monopolized the literature on families of mixture models.\nDeviating from this rubric, variational Bayes approximations are developed for\nparameter estimation and the deviance information criterion for model\nselection. The variational Bayes approach provides an alternate framework for\nparameter estimation by constructing a tight lower bound on the complex\nmarginal likelihood and maximizing this lower bound by minimizing the\nassociated Kullback-Leibler divergence. This approach is taken on the most\ncommonly used family of Gaussian mixture models, and real and simulated data\nare used to compare the new approach to the EM-BIC rubric.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 02:30:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jun 2016 21:56:32 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:16:30 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Subedi", "Sanjeena", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1306.5417", "submitter": "Eyal Neuman", "authors": "Ilya Gertsbakh, Eyal Neuman, Radislav Vaisman", "title": "Monte Carlo for estimating exponential convolution", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we study the numerical stability problem that may take place\nwhen calculating the cumulative distribution function of the {\\it\nHypoexponential} random variable. This computation is extensively used during\nthe execution of Monte Carlo network reliability estimation algorithms. In\nspite of the fact that analytical formulas are available, they can be unstable\nin practice. This instability occurs frequently when estimating very small\nfailure probabilities $(10^{-30}-10^{-40})$ that can happen for example while\nestimating the unreliability of telecommunication systems. In order to address\nthis problem, we propose a simple unbiased estimation algorithm that is capable\nof handling a large number of variables. We show that the proposed estimator\nhas a bounded relative error and that it compares favorably with other existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 14:29:37 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 05:18:05 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Gertsbakh", "Ilya", ""], ["Neuman", "Eyal", ""], ["Vaisman", "Radislav", ""]]}, {"id": "1306.5583", "submitter": "Yan Zhou Yan Zhou", "authors": "Yan Zhou", "title": "vSMC: Parallel Sequential Monte Carlo in C++", "comments": "44 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Sequential Monte Carlo is a family of algorithms for sampling from a sequence\nof distributions. Some of these algorithms, such as particle filters, are\nwidely used in the physics and signal processing researches. More recent\ndevelopments have established their application in more general inference\nproblems such as Bayesian modeling.\n  These algorithms have attracted considerable attentions in recent years as\nthey admit natural and scalable parallelizations. However, these algorithms are\nperceived to be difficult to implement. In addition, parallel programming is\noften unfamiliar to many researchers though conceptually appealing, especially\nfor sequential Monte Carlo related fields.\n  A C++ template library is presented for the purpose of implementing general\nsequential Monte Carlo algorithms on parallel hardware. Two examples are\npresented: a simple particle filter and a classic Bayesian modeling problem.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 11:44:30 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Zhou", "Yan", ""]]}, {"id": "1306.5715", "submitter": "Dajiang Liu", "authors": "Xiaowei Zhan, Dajiang J. Liu", "title": "TaSer (TabAnno and SeqMiner): a toolset for annotating and querying\n  next-generation sequence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.GN", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Summary: We develop TaSer (TabAnno and SeqMiner), a toolkit for annotating\nand querying next generation sequence (NGS) dataset in tab-delimited files.\nTabAnno is a powerful and efficient command-line tool designed to pre-process\nsequence data, annotate variations and generate an indexed feature-enriched\nproject file that can integrate multiple sources of information. Using the\nproject file generated by TabAnno, complex queries to the sequence dataset can\nbe performed using SeqMiner, an R-package designed to efficiently access large\ndatasets. Extracted information can be conveniently viewed and analyzed by\ntools in R. TaSer is optimized and computationally more efficient than software\nusing database systems. It enables annotating and querying NGS dataset using\nmoderate computing resource. Availability and implementation: TabAnno can be\ndownloaded from github (zhanxw.github.io/anno/). SeqMiner is distributed on\nCRAN (cran.r-project.org/web/packages/seqminer). Contact: X.Z.\n(zhanxw@umich.edu) D.J.L (dajiang@umich.edu)\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 19:23:38 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Zhan", "Xiaowei", ""], ["Liu", "Dajiang J.", ""]]}, {"id": "1306.5824", "submitter": "Paul McNicholas", "authors": "Ryan P. Browne, Sanjeena Subedi and Paul McNicholas", "title": "Constrained Optimization for a Subset of the Gaussian Parsimonious\n  Clustering Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm is an iterative method for\nfinding maximum likelihood estimates when data are incomplete or are treated as\nbeing incomplete. The EM algorithm and its variants are commonly used for\nparameter estimation in applications of mixture models for clustering and\nclassification. This despite the fact that even the Gaussian mixture model\nlikelihood surface contains many local maxima and is singularity riddled.\nPrevious work has focused on circumventing this problem by constraining the\nsmallest eigenvalue of the component covariance matrices. In this paper, we\nconsider constraining the smallest eigenvalue, the largest eigenvalue, and both\nthe smallest and largest within the family setting. Specifically, a subset of\nthe GPCM family is considered for model-based clustering, where we use a\nre-parameterized version of the famous eigenvalue decomposition of the\ncomponent covariance matrices. Our approach is illustrated using various\nexperiments with simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 01:27:09 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Browne", "Ryan P.", ""], ["Subedi", "Sanjeena", ""], ["McNicholas", "Paul", ""]]}, {"id": "1306.5993", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early", "title": "Frequency-Domain Stochastic Modeling of Stationary Bivariate or\n  Complex-Valued Signals", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three equivalent ways of representing two jointly observed\nreal-valued signals: as a bivariate vector signal, as a single complex-valued\nsignal, or as two analytic signals known as the rotary components. Each\nrepresentation has unique advantages depending on the system of interest and\nthe application goals. In this paper we provide a joint framework for all three\nrepresentations in the context of frequency-domain stochastic modeling. This\nframework allows us to extend many established statistical procedures for\nbivariate vector time series to complex-valued and rotary representations.\nThese include procedures for parametrically modeling signal coherence,\nestimating model parameters using the Whittle likelihood, performing\nsemi-parametric modeling, and choosing between classes of nested models using\nmodel choice. We also provide a new method of testing for impropriety in\ncomplex-valued signals, which tests for noncircular or anisotropic second-order\nstatistical structure when the signal is represented in the complex plane.\nFinally, we demonstrate the usefulness of our methodology in capturing the\nanisotropic structure of signals observed from fluid dynamic simulations of\nturbulence.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 15:15:06 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 06:59:25 GMT"}, {"version": "v3", "created": "Sun, 22 May 2016 00:48:38 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 13:04:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1306.6028", "submitter": "Demetris Lamnisos", "authors": "Demetris Lamnisos, Jim E. Griffin and Mark F.J. Steel", "title": "Adaptive MC^3 and Gibbs algorithms for Bayesian Model Averaging in\n  Linear Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MC$^3$ (Madigan and York, 1995) and Gibbs (George and McCulloch, 1997)\nsamplers are the most widely implemented algorithms for Bayesian Model\nAveraging (BMA) in linear regression models. These samplers draw a variable at\nrandom in each iteration using uniform selection probabilities and then propose\nto update that variable. This may be computationally inefficient if the number\nof variables is large and many variables are redundant. In this work, we\nintroduce adaptive versions of these samplers that retain their simplicity in\nimplementation and reduce the selection probabilities of the many redundant\nvariables. The improvements in efficiency for the adaptive samplers are\nillustrated in real and simulated datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 16:49:11 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Lamnisos", "Demetris", ""], ["Griffin", "Jim E.", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1306.6408", "submitter": "Geoff Robinson", "authors": "G.K. Robinson and L.M. Ryan", "title": "Using interpolation to reduce computing time for analysis of large but\n  simple data sets with application to design of epidemiological studies", "comments": "6 pages, no figures or tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way to investigate the precision of estimates likely to result from\nplanned experiments and planned epidemiological studies is to simulate a large\nnumber of possible outcomes and analyse the sets of possible results. This\nappears to be computationally expensive for some multi-stage designs, so choice\nof designs is instead based on theoretical derivation of expected information.\nThis paper shows that for some types of studies the analysis of large numbers\nof simulated outcomes can be achieved more rapidly by making use of\ninterpolation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 05:19:38 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Robinson", "G. K.", ""], ["Ryan", "L. M.", ""]]}, {"id": "1306.6462", "submitter": "Ajay Jasra", "authors": "Alexandros Beskos, Ajay Jasra, Nikolas Kantas, Alexandre Thiery", "title": "On the Convergence of Adaptive Sequential Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several implementations of Sequential Monte Carlo (SMC) methods it is\nnatural, and important in terms of algorithmic efficiency, to exploit the\ninformation of the history of the samples to optimally tune their subsequent\npropagations. In this article we provide a carefully formulated asymptotic\ntheory for a class of such \\emph{adaptive} SMC methods. The theoretical\nframework developed here will cover, under assumptions, several commonly used\nSMC algorithms. There are only limited results about the theoretical\nunderpinning of such adaptive methods: we will bridge this gap by providing a\nweak law of large numbers (WLLN) and a central limit theorem (CLT) for some of\nthese algorithms. The latter seems to be the first result of its kind in the\nliterature and provides a formal justification of algorithms used in many real\ndata context. We establish that for a general class of adaptive SMC algorithms\nthe asymptotic variance of the estimators from the adaptive SMC method is\n\\emph{identical} to a so-called `perfect' SMC algorithm which uses ideal\nproposal kernels. Our results are supported by application on a complex\nhigh-dimensional posterior distribution associated with the Navier-Stokes\nmodel, where adapting high-dimensional parameters of the proposal kernels is\ncritical for the efficiency of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 10:43:15 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2013 02:39:07 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2014 00:01:07 GMT"}], "update_date": "2014-02-07", "authors_parsed": [["Beskos", "Alexandros", ""], ["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""], ["Thiery", "Alexandre", ""]]}, {"id": "1306.6684", "submitter": "Somak Dutta", "authors": "Somak Dutta and Sourabh Bhattacharya", "title": "Supplement to \"Markov Chain Monte Carlo Based on Deterministic\n  Transformations\"", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a supplement to the article \"Markov Chain Monte Carlo Based on\nDeterministic Transformations\" available at http://arxiv.org/abs/1106.5850\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 00:10:19 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Dutta", "Somak", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1306.6928", "submitter": "Christian P. Robert", "authors": "Diego Salmeron (CIBERESP), Juan Antonio Cano (Universidad de Murcia),\n  and C.P. Robert (Universite Paris-Dauphine)", "title": "Objective Bayesian hypothesis testing in binomial regression models with\n  integral prior distributions", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we apply the methodology of integral priors to handle Bayesian\nmodel selection in binomial regression models with a general link function.\nThese models are very often used to investigate associations and risks in\nepidemiological studies where one goal is to exhibit whether or not an exposure\nis a risk factor for developing a certain disease; the purpose of the current\npaper is to test the effect of specific exposure factors. We formulate the\nproblem as a Bayesian model selection case and solve it using objective Bayes\nfactors. To construct the reference prior distributions on the regression\ncoefficients of the binomial regression models, we rely on the methodology of\nintegral priors that is nearly automatic as it only requires the specification\nof estimation reference priors and it does not depend on tuning parameters or\non hyperparameters within these priors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 18:58:35 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Salmeron", "Diego", "", "CIBERESP"], ["Cano", "Juan Antonio", "", "Universidad de Murcia"], ["Robert", "C. P.", "", "Universite Paris-Dauphine"]]}]