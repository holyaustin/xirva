[{"id": "1202.0078", "submitter": "Wenjin Mao", "authors": "Wenjin Mao and Jem Corcoran", "title": "A Class Coupler for Perfect Sampling from Continuous Distributions With\n  and Without Atoms", "comments": "21 pages, 9 figures; Journal of Statistical Theory and Applications\n  Volume 10, Number 3, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the simulation of distributions that are a mixture of discrete\nand continuous components. We extend a Metropolis-Hastings-based perfect\nsampling algorithm of Corcoran and Tweedie to allow for a broader class of\ntransition candidate densities. The resulting algorithm, know as a \"class\ncoupler\", is fast to implement and is applicable to purely discrete or purely\ncontinuous densities as well. Our work is motivated by the study of a composite\nhypothesis test in a Bayesian setting via posterior simulation and we give\nsimulation results for some problems in this area.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 01:55:55 GMT"}], "update_date": "2012-02-02", "authors_parsed": [["Mao", "Wenjin", ""], ["Corcoran", "Jem", ""]]}, {"id": "1202.0193", "submitter": "Mihail-Ioan Pop", "authors": "Mihail-Ioan Pop", "title": "Maximum entropy estimation of probability distributions with Gaussian\n  conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method to computationally estimate the probability density\nfunction of a univariate random variable by applying the maximum entropy\nprinciple with some local conditions given by Gaussian functions. The\nestimation errors and optimal values of parameters are determined. Experimental\nresults are presented. The method estimates the distribution well if a large\nenough selection is used, typically at least 1 000 values. Compared to the\nclassical approach of entropy maximisation, local conditions allow improving\nestimation locally. The method is well suited for a heuristic optimisation\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2012 15:38:44 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2012 21:59:45 GMT"}], "update_date": "2012-06-21", "authors_parsed": [["Pop", "Mihail-Ioan", ""]]}, {"id": "1202.0709", "submitter": "S. L. Cotter", "authors": "S. L. Cotter, G. O. Roberts, A. M. Stuart, D. White", "title": "MCMC Methods for Functions: Modifying Old Algorithms to Make Them Faster", "comments": "Published in at http://dx.doi.org/10.1214/13-STS421 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 3, 424-446", "doi": "10.1214/13-STS421", "report-no": "IMS-STS-STS421", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems arising in applications result in the need to probe a\nprobability distribution for functions. Examples include Bayesian nonparametric\nstatistics and conditioned diffusion processes. Standard MCMC algorithms\ntypically become arbitrarily slow under the mesh refinement dictated by\nnonparametric description of the unknown function. We describe an approach to\nmodifying a whole range of MCMC methods, applicable whenever the target measure\nhas density with respect to a Gaussian process or Gaussian random field\nreference measure, which ensures that their speed of convergence is robust\nunder mesh refinement. Gaussian processes or random fields are fields whose\nmarginal distributions, when evaluated at any finite set of $N$ points, are\n$\\mathbb{R}^N$-valued Gaussians. The algorithmic approach that we describe is\napplicable not only when the desired probability measure has density with\nrespect to a Gaussian process or Gaussian random field reference measure, but\nalso to some useful non-Gaussian reference measures constructed through random\ntruncation. In the applications of interest the data is often sparse and the\nprior specification is an essential part of the overall modelling strategy.\nThese Gaussian-based reference measures are a very flexible modelling tool,\nfinding wide-ranging application. Examples are shown in density estimation,\ndata assimilation in fluid mechanics, subsurface geophysics and image\nregistration. The key design principle is to formulate the MCMC method so that\nit is, in principle, applicable for functions; this may be achieved by use of\nproposals based on carefully chosen time-discretizations of stochastic\ndynamical systems which exactly preserve the Gaussian reference measure. Taking\nthis approach leads to many new algorithms which can be implemented via minor\nmodification of existing algorithms, yet which show enormous speed-up on a wide\nrange of applied problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 14:10:03 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 15:38:50 GMT"}, {"version": "v3", "created": "Thu, 10 Oct 2013 07:44:15 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Cotter", "S. L.", ""], ["Roberts", "G. O.", ""], ["Stuart", "A. M.", ""], ["White", "D.", ""]]}, {"id": "1202.0753", "submitter": "Lorenzo Fagiano", "authors": "Lorenzo Fagiano and Mustafa Khammash", "title": "Simulation of stochastic systems via polynomial chaos expansions and\n  convex optimization", "comments": "This manuscript is a preprint of a paper published on Physical\n  Reviews E and is subject to American Physical Society copyright. The copy of\n  record is available at http://pre.aps.org.\n  http://link.aps.org/doi/10.1103/PhysRevE.86.036702", "journal-ref": "Physical Reviews E, Volume 86, Issue 3, 036702, 2012", "doi": "10.1103/PhysRevE.86.036702", "report-no": null, "categories": "stat.CO cs.SY math-ph math.DS math.MP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial Chaos Expansions represent a powerful tool to simulate stochastic\nmodels of dynamical systems. Yet, deriving the expansion's coefficients for\ncomplex systems might require a significant and non-trivial manipulation of the\nmodel, or the computation of large numbers of simulation runs, rendering the\napproach too time consuming and impracticable for applications with more than a\nhandful of random variables. We introduce a novel computationally tractable\ntechnique for computing the coefficients of polynomial chaos expansions. The\napproach exploits a regularization technique with a particular choice of\nweighting matrices, which allow to take into account the specific features of\nPolynomial Chaos expansions. The method, completely based on convex\noptimization, can be applied to problems with a large number of random\nvariables and uses a modest number of Monte Carlo simulations, while avoiding\nmodel manipulations. Additional information on the stochastic process, when\navailable, can be also incorporated in the approach by means of convex\nconstraints. We show the effectiveness of the proposed technique in three\napplications in diverse fields, including the analysis of a nonlinear electric\ncircuit, a chaotic model of organizational behavior, finally a chemical\noscillator.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2012 16:24:06 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2012 01:14:12 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2012 11:33:32 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Fagiano", "Lorenzo", ""], ["Khammash", "Mustafa", ""]]}, {"id": "1202.1330", "submitter": "Wang Ru", "authors": "Ru Wang and Qiuping Alexandre Wang", "title": "A dual modelling of evolving political opinion networks", "comments": null, "journal-ref": "Physics Review E 84, 036108 (2011)", "doi": "10.1103/PhysRevE.84.036108", "report-no": null, "categories": "physics.soc-ph cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the result of a dual modeling of opinion network. The model\ncomplements the agent-based opinion models by attaching to the social agent\n(voters) network a political opinion (party) network having its own intrinsic\nmechanisms of evolution. These two sub-networks form a global network which can\nbe either isolated from or dependent on the external influence. Basically, the\nevolution of the agent network includes link adding and deleting, the opinion\nchanges influenced by social validation, the political climate, the\nattractivity of the parties and the interaction between them. The opinion\nnetwork is initially composed of numerous nodes representing opinions or\nparties which are located on a one dimensional axis according to their\npolitical positions. The mechanism of evolution includes union, splitting,\nchange of position and of attractivity, taken into account the pairwise node\ninteraction decaying with node distance in power law. The global evolution ends\nin a stable distribution of the social agents over a quasi-stable and\nfluctuating stationary number of remaining parties. Empirical study on the\nlifetime distribution of numerous parties and vote results is carried out to\nverify numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2012 01:54:56 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Wang", "Ru", ""], ["Wang", "Qiuping Alexandre", ""]]}, {"id": "1202.1738", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor and Peter J. Diggle", "title": "INLA or MCMC? A Tutorial and Comparative Evaluation for Spatial\n  Prediction in log-Gaussian Cox Processes", "comments": "This replaces the previous version of the report. The new version\n  includes results from an additional simulation study, and corrects an error\n  in the implementation of the INLA-based methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two options for performing Bayesian inference on spatial\nlog-Gaussian Cox processes assuming a spatially continuous latent field: Markov\nchain Monte Carlo (MCMC) and the integrated nested Laplace approximation\n(INLA). We first describe the device of approximating a spatially continuous\nGaussian field by a Gaussian Markov random field on a discrete lattice, and\npresent a simulation study showing that, with careful choice of parameter\nvalues, small neighbourhood sizes can give excellent approximations. We then\nintroduce the spatial log-Gaussian Cox process and describe MCMC and INLA\nmethods for spatial prediction within this model class. We report the results\nof a simulation study in which we compare MALA and the technique of\napproximating the continuous latent field by a discrete one, followed by\napproximate Bayesian inference via INLA over a selection of 18 simulated\nscenarios. The results question the notion that the latter technique is both\nsignificantly faster and more robust than MCMC in this setting; 100,000\niterations of the MALA algorithm running in 20 minutes on a desktop PC\ndelivered greater predictive accuracy than the default \\verb=INLA= strategy,\nwhich ran in 4 minutes and gave comparative performance to the full Laplace\napproximation which ran in 39 minutes.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2012 15:29:01 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 09:09:26 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Taylor", "Benjamin M.", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1202.1928", "submitter": "Tim Sullivan", "authors": "T. J. Sullivan, M. McKerns, D. Meyer, F. Theil, H. Owhadi, and M.\n  Ortiz", "title": "Optimal uncertainty quantification for legacy data observations of\n  Lipschitz functions", "comments": "38 pages", "journal-ref": "ESAIM Math. Model. Numer. Anal. 47(6):1657--1689, 2013", "doi": "10.1051/m2an/2013083", "report-no": null, "categories": "math.PR cs.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of providing optimal uncertainty quantification (UQ)\n--- and hence rigorous certification --- for partially-observed functions. We\npresent a UQ framework within which the observations may be small or large in\nnumber, and need not carry information about the probability distribution of\nthe system in operation. The UQ objectives are posed as optimization problems,\nthe solutions of which are optimal bounds on the quantities of interest; we\nconsider two typical settings, namely parameter sensitivities (McDiarmid\ndiameters) and output deviation (or failure) probabilities. The solutions of\nthese optimization problems depend non-trivially (even non-monotonically and\ndiscontinuously) upon the specified legacy data. Furthermore, the extreme\nvalues are often determined by only a few members of the data set; in our\nprincipal physically-motivated example, the bounds are determined by just 2 out\nof 32 data points, and the remainder carry no information and could be\nneglected without changing the final answer. We propose an analogue of the\nsimplex algorithm from linear programming that uses these observations to offer\nefficient and rigorous UQ for high-dimensional systems with high-cardinality\nlegacy data. These findings suggest natural methods for selecting optimal\n(maximally informative) next experiments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 09:43:49 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2013 23:39:46 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2013 04:25:43 GMT"}], "update_date": "2016-05-20", "authors_parsed": [["Sullivan", "T. J.", ""], ["McKerns", "M.", ""], ["Meyer", "D.", ""], ["Theil", "F.", ""], ["Owhadi", "H.", ""], ["Ortiz", "M.", ""]]}, {"id": "1202.2008", "submitter": "Jakob Stoeber", "authors": "Carlos Almeida, Claudia Czado, Hans Manner", "title": "Modeling high dimensional time-varying dependence using D-vine SCAR\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modeling the dependence among many time series. We\nbuild high dimensional time-varying copula models by combining pair-copula\nconstructions (PCC) with stochastic autoregressive copula (SCAR) models to\ncapture dependence that changes over time. We show how the estimation of this\nhighly complex model can be broken down into the estimation of a sequence of\nbivariate SCAR models, which can be achieved by using the method of simulated\nmaximum likelihood. Further, by restricting the conditional dependence\nparameter on higher cascades of the PCC to be constant, we can greatly reduce\nthe number of parameters to be estimated without losing much flexibility. We\nstudy the performance of our estimation method by a large scale Monte Carlo\nsimulation. An application to a large dataset of stock returns of all\nconstituents of the Dax 30 illustrates the usefulness of the proposed model\nclass.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2012 14:52:56 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Almeida", "Carlos", ""], ["Czado", "Claudia", ""], ["Manner", "Hans", ""]]}, {"id": "1202.3665", "submitter": "Daniel Foreman-Mackey", "authors": "Daniel Foreman-Mackey, David W. Hogg, Dustin Lang, Jonathan Goodman", "title": "emcee: The MCMC Hammer", "comments": "Code re-licensed under MIT", "journal-ref": null, "doi": "10.1086/670067", "report-no": null, "categories": "astro-ph.IM physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stable, well tested Python implementation of the\naffine-invariant ensemble sampler for Markov chain Monte Carlo (MCMC) proposed\nby Goodman & Weare (2010). The code is open source and has already been used in\nseveral published projects in the astrophysics literature. The algorithm behind\nemcee has several advantages over traditional MCMC sampling methods and it has\nexcellent performance as measured by the autocorrelation time (or function\ncalls per independent sample). One major advantage of the algorithm is that it\nrequires hand-tuning of only 1 or 2 parameters compared to $\\sim N^2$ for a\ntraditional algorithm in an N-dimensional parameter space. In this document, we\ndescribe the algorithm and the details of our implementation and API.\nExploiting the parallelism of the ensemble method, emcee permits any user to\ntake advantage of multiple CPU cores without extra effort. The code is\navailable online at http://dan.iel.fm/emcee under the MIT License.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 20:41:19 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 03:52:41 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2013 15:48:37 GMT"}, {"version": "v4", "created": "Mon, 25 Nov 2013 15:56:48 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Foreman-Mackey", "Daniel", ""], ["Hogg", "David W.", ""], ["Lang", "Dustin", ""], ["Goodman", "Jonathan", ""]]}, {"id": "1202.3739", "submitter": "Akshat Kumar", "authors": "Akshat Kumar, Shlomo Zilberstein", "title": "Message-Passing Algorithms for Quadratic Programming Formulations of MAP\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "UAI-P-2011-PG-428-435", "categories": "cs.AI cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing maximum a posteriori (MAP) estimation in graphical models is an\nimportant inference problem with many applications. We present message-passing\nalgorithms for quadratic programming (QP) formulations of MAP estimation for\npairwise Markov random fields. In particular, we use the concave-convex\nprocedure (CCCP) to obtain a locally optimal algorithm for the non-convex QP\nformulation. A similar technique is used to derive a globally convergent\nalgorithm for the convex QP relaxation of MAP. We also show that a recently\ndeveloped expectation-maximization (EM) algorithm for the QP formulation of MAP\ncan be derived from the CCCP perspective. Experiments on synthetic and\nreal-world problems confirm that our new approach is competitive with\nmax-product and its variations. Compared with CPLEX, we achieve more than an\norder-of-magnitude speedup in solving optimally the convex QP relaxation.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2012 16:41:17 GMT"}], "update_date": "2012-02-20", "authors_parsed": [["Kumar", "Akshat", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1202.3819", "submitter": "M. G. B. Blum", "authors": "M. G. B. Blum, M. A. Nunes, D. Prangle, S. A. Sisson", "title": "A Comparative Review of Dimension Reduction Methods in Approximate\n  Bayesian Computation", "comments": "Published in at http://dx.doi.org/10.1214/12-STS406 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 2, 189-208", "doi": "10.1214/12-STS406", "report-no": "IMS-STS-STS406", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) methods make use of comparisons\nbetween simulated and observed summary statistics to overcome the problem of\ncomputationally intractable likelihood functions. As the practical\nimplementation of ABC requires computations based on vectors of summary\nstatistics, rather than full data sets, a central question is how to derive\nlow-dimensional summary statistics from the observed data with minimal loss of\ninformation. In this article we provide a comprehensive review and comparison\nof the performance of the principal methods of dimension reduction proposed in\nthe ABC literature. The methods are split into three nonmutually exclusive\nclasses consisting of best subset selection methods, projection techniques and\nregularization. In addition, we introduce two new methods of dimension\nreduction. The first is a best subset selection method based on Akaike and\nBayesian information criteria, and the second uses ridge regression as a\nregularization procedure. We illustrate the performance of these dimension\nreduction techniques through the analysis of three challenging models and data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2012 23:47:00 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2012 02:05:09 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2013 11:18:18 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Blum", "M. G. B.", ""], ["Nunes", "M. A.", ""], ["Prangle", "D.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1202.4044", "submitter": "Michael McCoy", "authors": "Gilad Lerman, Michael McCoy, Joel A. Tropp, and Teng Zhang", "title": "Robust computation of linear models by convex relaxation", "comments": "Formerly titled \"Robust computation of linear models, or How to find\n  a needle in a haystack\"", "journal-ref": "Foundations of Computational Mathematics, April 2015, Volume 15,\n  Issue 2, pp 363-410", "doi": "10.1007/s10208-014-9221-0", "report-no": null, "categories": "cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a dataset of vector-valued observations that consists of noisy\ninliers, which are explained well by a low-dimensional subspace, along with\nsome number of outliers. This work describes a convex optimization problem,\ncalled REAPER, that can reliably fit a low-dimensional model to this type of\ndata. This approach parameterizes linear subspaces using orthogonal projectors,\nand it uses a relaxation of the set of orthogonal projectors to reach the\nconvex formulation. The paper provides an efficient algorithm for solving the\nREAPER problem, and it documents numerical experiments which confirm that\nREAPER can dependably find linear structure in synthetic and natural data. In\naddition, when the inliers lie near a low-dimensional subspace, there is a\nrigorous theory that describes when REAPER can approximate this subspace.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 00:47:22 GMT"}, {"version": "v2", "created": "Mon, 11 Aug 2014 19:19:28 GMT"}], "update_date": "2015-07-24", "authors_parsed": [["Lerman", "Gilad", ""], ["McCoy", "Michael", ""], ["Tropp", "Joel A.", ""], ["Zhang", "Teng", ""]]}, {"id": "1202.4094", "submitter": "Shane Jensen", "authors": "Dean Foster and Shane T. Jensen", "title": "A Level-Set Hit-and-Run Sampler for Quasi-Concave Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new sampling strategy that uses the hit-and-run algorithm within\nlevel sets of the target density. Our method can be applied to any\nquasi-concave density, which covers a broad class of models. Our sampler\nperforms well in high-dimensional settings, which we illustrate with a\ncomparison to Gibbs sampling on a spike-and-slab mixture model. We also extend\nour method to exponentially-tilted quasi-concave densities, which arise often\nin Bayesian models consisting of a log-concave likelihood and quasi-concave\nprior density. Within this class of models, our method is effective at sampling\nfrom posterior distributions with high dependence between parameters, which we\nillustrate with a simple multivariate normal example. We also implement our\nlevel-set sampler on a Cauchy-normal model where we demonstrate the ability of\nour level set sampler to handle multi-modal posterior distributions.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2012 19:01:56 GMT"}], "update_date": "2012-02-21", "authors_parsed": [["Foster", "Dean", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1202.5093", "submitter": "Nakagawa Shigekazu", "authors": "Shigekazu Nakagawa, Hiroki Hashiguchi and Naoto Niki", "title": "A measure of skewness for testing departures from normality", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new skewness test statistic for normality based on the Pearson\nmeasure of skewness. We obtain asymptotic first four moments of the null\ndistribution for this statistic by using a computer algebra system and its\nnormalizing transformation based on the Johnson $S_{U}$ system. Finally the\nperformance of the proposed statistic is shown by comparing the powers of\nseveral skewness test statistics against some alternative hypotheses.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2012 06:17:39 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Nakagawa", "Shigekazu", ""], ["Hashiguchi", "Hiroki", ""], ["Niki", "Naoto", ""]]}, {"id": "1202.5682", "submitter": "Ivan Kojadinovic", "authors": "Ivan Kojadinovic and Jun Yan", "title": "Goodness-of-fit testing based on a weighted bootstrap: A fast\n  large-sample alternative to the parametric bootstrap", "comments": "26 pages, 5 tables, 1 figure", "journal-ref": "The Canadian Journal of Statistics 40:3, pages 480-501, 2012", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process comparing the empirical cumulative distribution function of the\nsample with a parametric estimate of the cumulative distribution function is\nknown as the empirical process with estimated parameters and has been\nextensively employed in the literature for goodness-of-fit testing. The\nsimplest way to carry out such goodness-of-fit tests, especially in a\nmultivariate setting, is to use a parametric bootstrap. Although very easy to\nimplement, the parametric bootstrap can become very computationally expensive\nas the sample size, the number of parameters, or the dimension of the data\nincrease. An alternative resampling technique based on a fast weighted\nbootstrap is proposed in this paper, and is studied both theoretically and\nempirically. The outcome of this work is a generic and computationally\nefficient multiplier goodness-of-fit procedure that can be used as a\nlarge-sample alternative to the parametric bootstrap. In order to approximately\ndetermine how large the sample size needs to be for the parametric and weighted\nbootstraps to have roughly equivalent powers, extensive Monte Carlo experiments\nare carried out in dimension one, two and three, and for models containing up\nto nine parameters. The computational gains resulting from the use of the\nproposed multiplier goodness-of-fit procedure are illustrated on trivariate\nfinancial data. A by-product of this work is a fast large-sample\ngoodness-of-fit procedure for the bivariate and trivariate t distribution whose\ndegrees of freedom are fixed.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2012 18:32:01 GMT"}], "update_date": "2012-10-08", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Yan", "Jun", ""]]}, {"id": "1202.5957", "submitter": "Olivia Saierli", "authors": "Prashant Kumar, Anchala Kumari and Soubhik Chakraborty", "title": "Parameterized Complexity on a New Sorting Algorithm: A Study in\n  Simulation", "comments": "14 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 9-22", "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sundararajan and Chakraborty (2007) introduced a new sorting algorithm by\nmodifying the fast and popular Quick sort and removing the interchanges. In a\nsubsequent empirical study, Sourabh, Sundararajan and Chakraborty (2007)\ndemonstrated that this algorithm sorts inputs from certain probability\ndistributions faster than others and the authors made a list of some standard\nprobability distributions in decreasing order of speed, namely, Continuous\nuniform < Discrete uniform < Binomial < Negative Binomial < Poisson < Geometric\n< Exponential < Standard Normal. It is clear from this interesting second study\nthat the algorithm is sensitive to input probability distribution. Based on\nthese pervious findings, in the present paper we are motivated to do some\nfurther study on this sorting algorithm through simulation and determine the\nappropriate empirical model which explains its average sorting time with\nspecial emphasis on parameterized complexity.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 14:45:46 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Kumar", "Prashant", ""], ["Kumari", "Anchala", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "1202.5968", "submitter": "Olivia Saierli", "authors": "Debasish Sahani and Soubhik Chakraborty", "title": "How many interchanges does the selection sort make for iid geometric(p)\n  input?", "comments": "10 pages", "journal-ref": "Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 67-76", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The note derives an expression for the number of interchanges made by\nselection sort when the sorting elements are iid variates from geometric\ndistribution. Empirical results reveal we can work with a simpler model\ncompared to what is suggestive in theory. The morale is that statistical\nanalysis of an algorithm's complexity has something to offer in its own right\nand should be therefore ventured not with a predetermined mindset to verify\nwhat we already know in theory. Herein also lies the concept of an empirical O,\na novel although subjective bound estimate over a finite input range obtained\nby running computer experiments. For an arbitrary algorithm, where theoretical\nresults could be tedious, this could be of greater use.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 15:11:21 GMT"}], "update_date": "2012-02-28", "authors_parsed": [["Sahani", "Debasish", ""], ["Chakraborty", "Soubhik", ""]]}, {"id": "1202.5983", "submitter": "Mathias Trabs", "authors": "Jakob S\\\"ohl and Mathias Trabs", "title": "Option calibration of exponential L\\'evy models: Confidence intervals\n  and empirical results", "comments": "to appear in Journal of Computational Finance", "journal-ref": "J. Comput. Finance 18(2) (2014) 91-119", "doi": "10.21314/JCF.2014.275", "report-no": null, "categories": "q-fin.PR math.ST q-fin.CP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observing prices of European put and call options, we calibrate exponential\nL\\'evy models nonparametrically. We discuss the efficient implementation of the\nspectral estimation procedures for L\\'evy models of finite jump activity as\nwell as for self-decomposable L\\'evy models. Based on finite sample variances,\nconfidence intervals are constructed for the volatility, for the drift and,\npointwise, for the jump density. As demonstrated by simulations, these\nintervals perform well in terms of size and coverage probabilities. We compare\nthe performance of the procedures for finite and infinite jump activity based\non options on the German DAX index and find that both methods achieve good\ncalibration results. The stability of the finite activity model is studied when\nthe option prices are observed in a sequence of trading days.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 15:59:52 GMT"}, {"version": "v2", "created": "Mon, 3 Sep 2012 14:11:21 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2012 11:53:11 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["S\u00f6hl", "Jakob", ""], ["Trabs", "Mathias", ""]]}, {"id": "1202.6042", "submitter": "Kevin Xu", "authors": "Kevin S. Xu, Mark Kliger, and Alfred O. Hero III", "title": "A Regularized Graph Layout Framework for Dynamic Network Visualization", "comments": "To appear in Data Mining and Knowledge Discovery, supporting material\n  (animations and MATLAB toolbox) available at\n  http://tbayes.eecs.umich.edu/xukevin/visualization_dmkd_2012", "journal-ref": "Data Mining and Knowledge Discovery 27 (2013) 84-116", "doi": "10.1007/s10618-012-0286-6", "report-no": null, "categories": "cs.SI cs.DM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks, including social and information networks, are\ndynamic structures that evolve over time. Such dynamic networks are typically\nvisualized using a sequence of static graph layouts. In addition to providing a\nvisual representation of the network structure at each time step, the sequence\nshould preserve the mental map between layouts of consecutive time steps to\nallow a human to interpret the temporal evolution of the network. In this\npaper, we propose a framework for dynamic network visualization in the on-line\nsetting where only present and past graph snapshots are available to create the\npresent layout. The proposed framework creates regularized graph layouts by\naugmenting the cost function of a static graph layout algorithm with a grouping\npenalty, which discourages nodes from deviating too far from other nodes\nbelonging to the same group, and a temporal penalty, which discourages large\nnode movements between consecutive time steps. The penalties increase the\nstability of the layout sequence, thus preserving the mental map. We introduce\ntwo dynamic layout algorithms within the proposed framework, namely dynamic\nmultidimensional scaling (DMDS) and dynamic graph Laplacian layout (DGLL). We\napply these algorithms on several data sets to illustrate the importance of\nboth grouping and temporal regularization for producing interpretable\nvisualizations of dynamic networks.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2012 20:05:53 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 20:54:01 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 16:24:26 GMT"}], "update_date": "2014-01-21", "authors_parsed": [["Xu", "Kevin S.", ""], ["Kliger", "Mark", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1202.6159", "submitter": "Lawrence Murray", "authors": "Lawrence M. Murray and Emlyn M. Jones and John Parslow", "title": "On Disturbance State-Space Models and the Particle Marginal\n  Metropolis-Hastings Sampler", "comments": null, "journal-ref": "SIAM/ASA Journal of Uncertainty Quantification, 2013, 1, 494-521", "doi": "10.1137/130915376", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate nonlinear state-space models without a closed-form transition\ndensity, and propose reformulating such models over their latent noise\nvariables rather than their latent state variables. In doing so the tractable\nnoise density emerges in place of the intractable transition density. For\nimportance sampling methods such as the auxiliary particle filter, this enables\nimportance weights to be computed where they could not be otherwise. As case\nstudies we take two multivariate marine biogeochemical models and perform state\nand parameter estimation using the particle marginal Metropolis-Hastings\nsampler. For the particle filter within this sampler, we compare several\nproposal strategies over noise variables, all based on lookaheads with the\nunscented Kalman filter. These strategies are compared using conventional means\nfor assessing Metropolis-Hastings efficiency, as well as with a novel metric\ncalled the conditional acceptance rate for assessing the consequences of using\nan estimated, and not exact, likelihood. Results indicate the utility of\nreformulating the model over noise variables, particularly for fast-mixing\nprocess models.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 09:58:35 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2013 06:02:54 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 09:59:49 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Murray", "Lawrence M.", ""], ["Jones", "Emlyn M.", ""], ["Parslow", "John", ""]]}, {"id": "1202.6163", "submitter": "Lawrence Murray", "authors": "Lawrence Murray", "title": "GPU acceleration of the particle filter: the Metropolis resampler", "comments": "Originally presented at Distributed Machine Learning and Sparse\n  Representation with Massive Data Sets (DMMD 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider deployment of the particle filter on modern massively parallel\nhardware architectures, such as Graphics Processing Units (GPUs), with a focus\non the resampling stage. While standard multinomial and stratified resamplers\nrequire a sum of importance weights computed collectively between threads, a\nMetropolis resampler favourably requires only pair-wise ratios between weights,\ncomputed independently by threads, and can be further tuned for performance by\nadjusting its number of iterations. While achieving respectable results for the\nstratified and multinomial resamplers, we demonstrate that a Metropolis\nresampler can be faster where the variance in importance weights is modest, and\nso is worth considering in a performance-critical context, such as particle\nMarkov chain Monte Carlo and real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2012 10:13:50 GMT"}], "update_date": "2012-02-29", "authors_parsed": [["Murray", "Lawrence", ""]]}, {"id": "1202.6590", "submitter": "Giusi Moffa", "authors": "Jack Kuipers and Giusi Moffa", "title": "Uniform random generation of large acyclic digraphs", "comments": "15 pages, 2 figures. To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-013-9428-y", "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs are the basic representation of the structure\nunderlying Bayesian networks, which represent multivariate probability\ndistributions. In many practical applications, such as the reverse engineering\nof gene regulatory networks, not only the estimation of model parameters but\nthe reconstruction of the structure itself is of great interest. As well as for\nthe assessment of different structure learning algorithms in simulation\nstudies, a uniform sample from the space of directed acyclic graphs is required\nto evaluate the prevalence of certain structural features. Here we analyse how\nto sample acyclic digraphs uniformly at random through recursive enumeration,\nan approach previously thought too computationally involved. Based on\ncomplexity considerations, we discuss in particular how the enumeration\ndirectly provides an exact method, which avoids the convergence issues of the\nalternative Markov chain methods and is actually computationally much faster.\nThe limiting behaviour of the distribution of acyclic digraphs then allows us\nto sample arbitrarily large graphs. Building on the ideas of recursive\nenumeration based sampling we also introduce a novel hybrid Markov chain with\nmuch faster convergence than current alternatives while still being easy to\nadapt to various restrictions. Finally we discuss how to include such\nrestrictions in the combinatorial enumeration and the new hybrid Markov chain\nmethod for efficient uniform sampling of the corresponding graphs.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 16:24:13 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2012 12:50:10 GMT"}, {"version": "v3", "created": "Fri, 3 May 2013 13:51:07 GMT"}, {"version": "v4", "created": "Thu, 14 Nov 2013 20:56:06 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""]]}, {"id": "1202.6678", "submitter": "Nikolas Kantas", "authors": "Nick Whiteley and Nikolas Kantas", "title": "Calculating principal eigen-functions of non-negative integral kernels:\n  particle approximations and applications", "comments": "38 pages, 4 figures, 1 table; to appear in Mathematics of Operations\n  Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in applications such as rare events estimation or optimal control it is\nrequired that one calculates the principal eigen-function and eigen-value of a\nnon-negative integral kernel. Except in the finite-dimensional case, usually\nneither the principal eigen-function nor the eigen-value can be computed\nexactly. In this paper, we develop numerical approximations for these\nquantities. We show how a generic interacting particle algorithm can be used to\ndeliver numerical approximations of the eigen-quantities and the associated\nso-called \"twisted\" Markov kernel as well as how these approximations are\nrelevant to the aforementioned applications. In addition, we study a collection\nof random integral operators underlying the algorithm, address some of their\nmean and path-wise properties, and obtain $L_{r}$ error estimates. Finally,\nnumerical examples are provided in the context of importance sampling for\ncomputing tail probabilities of Markov chains and computing value functions for\na class of stochastic optimal control problems.\n", "versions": [{"version": "v1", "created": "Wed, 29 Feb 2012 20:46:02 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 18:15:28 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 14:57:25 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Whiteley", "Nick", ""], ["Kantas", "Nikolas", ""]]}]