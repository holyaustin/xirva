[{"id": "1709.00151", "submitter": "Yu Wang", "authors": "Yu Wang, Nhu D. Le, James V. Zidek", "title": "Approximately Optimal Subset Selection for Statistical Design and\n  Modelling", "comments": "14 pages, 3 figures, 1 table; Added examples in statistical design", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of optimal subset selection from a set of correlated\nrandom variables. In particular, we consider the associated combinatorial\noptimization problem of maximizing the determinant of a symmetric positive\ndefinite matrix that characterizes the chosen subset. This problem arises in\nmany domains, such as experimental designs, regression modeling, and\nenvironmental statistics. We establish an efficient polynomial-time algorithm\nusing Determinantal Point Process for approximating the optimal solution to the\nproblem. We demonstrate the advantages of our methods by presenting\ncomputational results for both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 04:34:37 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 22:53:00 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 22:43:44 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Wang", "Yu", ""], ["Le", "Nhu D.", ""], ["Zidek", "James V.", ""]]}, {"id": "1709.00404", "submitter": "Jeremy Heng", "authors": "Jeremy Heng and Pierre E. Jacob", "title": "Unbiased Hamiltonian Monte Carlo with couplings", "comments": "Final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a methodology to parallelize Hamiltonian Monte Carlo estimators.\nOur approach constructs a pair of Hamiltonian Monte Carlo chains that are\ncoupled in such a way that they meet exactly after some random number of\niterations. These chains can then be combined so that resulting estimators are\nunbiased. This allows us to produce independent replicates in parallel and\naverage them to obtain estimators that are consistent in the limit of the\nnumber of replicates, instead of the usual limit of the number of Markov chain\niterations. We investigate the scalability of our coupling in high dimensions\non a toy example. The choice of algorithmic parameters and the efficiency of\nour proposed methodology are then illustrated on a logistic regression with 300\ncovariates, and a log-Gaussian Cox point processes model with low to fine\ngrained discretizations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Sep 2017 17:54:56 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 22:53:33 GMT"}, {"version": "v3", "created": "Mon, 27 Aug 2018 16:45:13 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""]]}, {"id": "1709.00872", "submitter": "Michail Papathomas Dr", "authors": "Michail Papathomas", "title": "On synthetic data with predetermined subject partitioning and cluster\n  profiling, and pre-specified categorical variable marginal dependence\n  structure", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard approach for assessing the performance of partition or mixture\nmodels is to create synthetic data sets with a pre-specified clustering\nstructure, and assess how well the model reveals this structure. A common\nformat is that subjects are assigned to different clusters, with variable\nobservations simulated so that subjects within the same cluster have similar\nprofiles, allowing for some variability. In this manuscript, we consider\nobservations from nominal, ordinal and interval categorical variables.\nTheoretical and empirical results are utilized to explore the dependence\nstructure between the variables, in relation to the clustering structure for\nthe subjects. A novel approach is proposed that allows to control the marginal\nassociation or correlation structure of the variables, and to specify exact\ncorrelation values. Practical examples are shown and additional theoretical\nresults are derived for interval data, commonly observed in cohort studies,\nincluding observations that emulate Single Nucleotide Polymorphisms. We compare\na synthetic dataset to a real one, to demonstrate similarities and differences.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 09:14:09 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 11:41:06 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Papathomas", "Michail", ""]]}, {"id": "1709.01002", "submitter": "Anthony Lee", "authors": "Anthony Lee, Simone Tiberi, Giacomo Zanella", "title": "Unbiased approximations of products of expectations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of approximating the product of $n$ expectations with\nrespect to a common probability distribution $\\mu$. Such products routinely\narise in statistics as values of the likelihood in latent variable models.\nMotivated by pseudo-marginal Markov chain Monte Carlo schemes, we focus on\nunbiased estimators of such products. The standard approach is to sample $N$\nparticles from $\\mu$ and assign each particle to one of the expectations. This\nis wasteful and typically requires the number of particles to grow\nquadratically with the number of expectations. We propose an alternative\nestimator that approximates each expectation using most of the particles while\npreserving unbiasedness. We carefully study its properties, showing that in\nlatent variable contexts the proposed estimator needs only $\\mathcal{O}(n)$\nparticles to match the performance of the standard approach with\n$\\mathcal{O}(n^{2})$ particles. We demonstrate the procedure on two latent\nvariable examples from approximate Bayesian computation and single-cell gene\nexpression analysis, observing computational gains of the order of the number\nof expectations, i.e. data points, $n$.\n", "versions": [{"version": "v1", "created": "Mon, 4 Sep 2017 15:19:49 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Lee", "Anthony", ""], ["Tiberi", "Simone", ""], ["Zanella", "Giacomo", ""]]}, {"id": "1709.01195", "submitter": "George Ostrouchov", "authors": "George Ostrouchov and Wei-Chen Chen and Drew Schmidt", "title": "Parallel Statistical Computing with R: An Illustration on Two\n  Architectures", "comments": "Presented at: International Statistical Institute 61st World\n  Statistics Congress, Marrakech, Morocco, July 16-21, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To harness the full benefit of new computing platforms, it is necessary to\ndevelop software with parallel computing capabilities. This is no less true for\nstatisticians than for astrophysicists. The R programming language, which is\nperhaps the most popular software environment for statisticians today, has many\npackages available for parallel computing. Their diversity in approach can be\ndifficult to navigate. Some have attempted to alleviate this problem by\ndesigning common interfaces. However, these approaches offer limited\nflexibility to the user; additionally, they often serve as poor abstractions to\nthe reality of modern hardware, leading to poor performance. We give a short\nintroduction to two basic parallel computing approaches that closely align with\nhardware reality, allow the user to understand its performance, and provide\nsufficient capability to fully utilize multicore and multinode environments.\n  We illustrate both approaches by working through a simple example fitting a\nrandom forest model. Beginning with a serial algorithm, we derive two parallel\nversions. Our objective is to illustrate the use of multiple cores on a single\nprocessor and the use of multiple processors in a cluster computer. We discuss\nthe differences between the two versions and how the underlying hardware is\nused in each case.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 00:02:01 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 02:54:50 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Ostrouchov", "George", ""], ["Chen", "Wei-Chen", ""], ["Schmidt", "Drew", ""]]}, {"id": "1709.01303", "submitter": "Shohruh Miryusupov", "authors": "Raphael Douady, Shohruh Miryusupov", "title": "Hamiltonian Flow Simulation of Rare Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Flow Monte Carlo(HFMC) methods have been implemented in\nengineering, biology and chemistry. HFMC makes large gradient based steps to\nrapidly explore the state space. The application of the Hamiltonian dynamics\nallows to estimate rare events and sample from target distributions defined as\nthe change of measures. The estimates demonstrated a variance reduction of the\npresented algorithm and its efficiency with respect to a standard Monte Carlo\nand interacting particle based system(IPS). We tested the algorithm on the case\nof the barrier option pricing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:37:48 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Douady", "Raphael", ""], ["Miryusupov", "Shohruh", ""]]}, {"id": "1709.01310", "submitter": "Claudio Heinrich", "authors": "Claudio Heinrich, Mikko S. Pakkanen, Almut E.D. Veraart", "title": "Hybrid simulation scheme for volatility modulated moving average fields", "comments": null, "journal-ref": "Mathematics and Computers in Simulation 2019, Vol. 166, 224-244", "doi": "10.1016/j.matcom.2019.04.006", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a simulation scheme for a class of spatial stochastic processes\ncalled volatility modulated moving averages. A characteristic feature of this\nmodel is that the behaviour of the moving average kernel at zero governs the\nroughness of realisations, whereas its behaviour away from zero determines the\nglobal properties of the process, such as long range dependence. Our simulation\nscheme takes this into account and approximates the moving average kernel by a\npower function around zero and by a step function elsewhere. For this type of\napproach the authors of [8], who considered an analogous model in one\ndimension, coined the expression hybrid simulation scheme. We derive the\nasymptotic mean square error of the simulation scheme and compare it in a\nsimulation study with several other simulation techniques and exemplify its\nfavourable performance in a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 09:52:49 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Heinrich", "Claudio", ""], ["Pakkanen", "Mikko S.", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1709.01589", "submitter": "Bruno Sudret", "authors": "S. Marelli and B. Sudret", "title": "An active-learning algorithm that combines sparse polynomial chaos\n  expansions and bootstrap for structural reliability analysis", "comments": null, "journal-ref": "Structural Safety, 75, pp. 67-74 (2018)", "doi": "10.1016/j.strusafe.2018.06.003", "report-no": "RSUQ-2017-009", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCE) have seen widespread use in the context of\nuncertainty quantification. However, their application to structural\nreliability problems has been hindered by the limited performance of PCE in the\ntails of the model response and due to the lack of local metamodel error\nestimates. We propose a new method to provide local metamodel error estimates\nbased on bootstrap resampling and sparse PCE. An initial experimental design is\niteratively updated based on the current estimation of the limit-state surface\nin an active learning algorithm. The greedy algorithm uses the bootstrap-based\nlocal error estimates for the polynomial chaos predictor to identify the best\ncandidate set of points to enrich the experimental design. We demonstrate the\neffectiveness of this approach on a well-known analytical benchmark\nrepresenting a series system, on a truss structure and on a complex realistic\nframe structure problem.\n", "versions": [{"version": "v1", "created": "Tue, 5 Sep 2017 20:44:32 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 16:52:07 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.02069", "submitter": "Dengdeng Yu", "authors": "Dengdeng Yu, Linglong Kong and Ivan Mizera", "title": "An Alternative Approach to Functional Linear Partial Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have previously proposed the partial quantile regression (PQR) prediction\nprocedure for functional linear model by using partial quantile covariance\ntechniques and developed the simple partial quantile regression (SIMPQR)\nalgorithm to efficiently extract PQR basis for estimating functional\ncoefficients. However, although the PQR approach is considered as an attractive\nalternative to projections onto the principal component basis, there are\ncertain limitations to uncovering the corresponding asymptotic properties\nmainly because of its iterative nature and the non-differentiability of the\nquantile loss function. In this article, we propose and implement an\nalternative formulation of partial quantile regression (APQR) for functional\nlinear model by using block relaxation method and finite smoothing techniques.\nThe proposed reformulation leads to insightful results and motivates new\ntheory, demonstrating consistency and establishing convergence rates by\napplying advanced techniques from empirical process theory. Two simulations and\ntwo real data from ADHD-200 sample and ADNI are investigated to show the\nsuperiority of our proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 05:03:24 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Yu", "Dengdeng", ""], ["Kong", "Linglong", ""], ["Mizera", "Ivan", ""]]}, {"id": "1709.02317", "submitter": "Radoslav Harman", "authors": "Radoslav Harman and Maryna Prus", "title": "Computing optimal experimental designs with respect to a compound Bayes\n  risk criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing optimal experimental design on a finite\ndesign space with respect to a compound Bayes risk criterion, which includes\nthe linear criterion for prediction in a random coefficient regression model.\nWe show that the problem can be restated as constrained A-optimality in an\nartificial model. This permits using recently developed computational tools,\nfor instance the algorithms based on the second-order cone programming for\noptimal approximate design, and mixed-integer second-order cone programming for\noptimal exact designs. We demonstrate the use of the proposed method for the\nproblem of computing optimal designs of a random coefficient regression model\nwith respect to an integrated mean squared error criterion.\n", "versions": [{"version": "v1", "created": "Thu, 7 Sep 2017 15:37:50 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Harman", "Radoslav", ""], ["Prus", "Maryna", ""]]}, {"id": "1709.02532", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Generalizing Distance Covariance to Measure and Test Multivariate Mutual\n  Dependence", "comments": "34 pages, 10 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose three measures of mutual dependence between multiple random\nvectors. All the measures are zero if and only if the random vectors are\nmutually independent. The first measure generalizes distance covariance from\npairwise dependence to mutual dependence, while the other two measures are sums\nof squared distance covariance. All the measures share similar properties and\nasymptotic distributions to distance covariance, and capture non-linear and\nnon-monotone mutual dependence between the random vectors. Inspired by complete\nand incomplete V-statistics, we define the empirical measures and simplified\nempirical measures as a trade-off between the complexity and power when testing\nmutual independence. Implementation of the tests is demonstrated by both\nsimulation results and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 04:36:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 05:59:44 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 00:56:40 GMT"}, {"version": "v4", "created": "Sun, 24 Dec 2017 01:21:46 GMT"}, {"version": "v5", "created": "Sun, 25 Feb 2018 22:58:23 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1709.02611", "submitter": "Marko Laine", "authors": "Otto Lamminp\\\"a\\\"a, Marko Laine, Simo Tukiainen and Johanna Tamminen", "title": "Likelihood informed dimension reduction for inverse problems in remote\n  sensing of atmospheric constituent profiles", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-04161-8_6", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use likelihood informed dimension reduction (LIS) (T. Cui et al. 2014) for\ninverting vertical profile information of atmospheric methane from ground based\nFourier transform infrared (FTIR) measurements at Sodankyl\\\"a, Northern\nFinland. The measurements belong to the word wide TCCON network for greenhouse\ngas measurements and, in addition to providing accurate greenhouse gas\nmeasurements, they are important for validating satellite observations. LIS\nallows construction of an efficient Markov chain Monte Carlo sampling algorithm\nthat explores only a reduced dimensional space but still produces a good\napproximation of the original full dimensional Bayesian posterior distribution.\nThis in effect makes the statistical estimation problem independent of the\ndiscretization of the inverse problem. In addition, we compare LIS to a\ndimension reduction method based on prior covariance matrix truncation used\nearlier (S. Tukiainen et al. 2016).\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 09:37:46 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 14:26:40 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Lamminp\u00e4\u00e4", "Otto", ""], ["Laine", "Marko", ""], ["Tukiainen", "Simo", ""], ["Tamminen", "Johanna", ""]]}, {"id": "1709.02695", "submitter": "Ryan Martin", "authors": "Minwoo Chae, Ryan Martin, Stephen G. Walker", "title": "Applications of an algorithm for solving Fredholm equations of the first\n  kind", "comments": "22 pages, 7 figures", "journal-ref": "Statistics and Computing, 2019, volume 29, number 4, pages\n  645--654", "doi": "10.1007/s11222-018-9829-z", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use an iterative algorithm for solving Fredholm equations of\nthe first kind. The basic algorithm is known and is based on an EM algorithm\nwhen involved functions are non-negative and integrable. With this algorithm we\ndemonstrate two examples involving the estimation of a mixing density and a\nfirst passage time density function involving Brownian motion. We also develop\nthe basic algorithm to include functions which are not necessarily non-negative\nand again present illustrations under this scenario. A self contained proof of\nconvergence of all the algorithms employed is presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 13:28:17 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Chae", "Minwoo", ""], ["Martin", "Ryan", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1709.02888", "submitter": "Ricky Fok", "authors": "Ricky Fok, Aijun An, Xiaogang Wang", "title": "Optimization assisted MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) sampling methods are widely used but often\nencounter either slow convergence or biased sampling when applied to multimodal\nhigh dimensional distributions. In this paper, we present a general framework\nof improving classical MCMC samplers by employing a global optimization method.\nThe global optimization method first reduces a high dimensional search to an\none dimensional geodesic to find a starting point close to a local mode. The\nsearch is accelerated and completed by using a local search method such as\nBFGS. We modify the target distribution by extracting a local Gaussian\ndistribution aound the found mode. The process is repeated to find all the\nmodes during sampling on the fly. We integrate the optimization algorithm into\nthe Wormhole Hamiltonian Monte Carlo (WHMC) method. Experimental results show\nthat, when applied to high dimensional, multimodal Gaussian mixture models and\nthe network sensor localization problem, the proposed method achieves much\nfaster convergence, with relative error from the mean improved by about an\norder of magnitude than WHMC in some cases.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 01:03:08 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Fok", "Ricky", ""], ["An", "Aijun", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1709.02907", "submitter": "Pritam Ranjan", "authors": "Natalia V. Bhattacharjee and Pritam Ranjan and Abhyuday Mandal and\n  Ernest W. Tollner", "title": "A History Matching Approach for Calibrating Hydrological Models", "comments": null, "journal-ref": "Environmental and Ecological Statistics, 26(1), 87-105, 2019", "doi": "10.1007/s10651-019-00420-9", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration of hydrological time-series models is a challenging task since\nthese models give a wide spectrum of output series and calibration procedures\nrequire significant amount of time. From a statistical standpoint, this model\nparameter estimation problem simplifies to finding an inverse solution of a\ncomputer model that generates pre-specified time-series output (i.e., realistic\noutput series). In this paper, we propose a modified history matching approach\nfor calibrating the time-series rainfall-runoff models with respect to the real\ndata collected from the state of Georgia, USA. We present the methodology and\nillustrate the application of the algorithm by carrying a simulation study and\nthe two case studies. Several goodness-of-fit statistics were calculated to\nassess the model performance. The results showed that the proposed history\nmatching algorithm led to a significant improvement, of 30% and 14% (in terms\nof root mean squared error) and 26% and 118% (in terms of peak percent\nthreshold statistics), for the two case-studies with Matlab-Simulink and SWAT\nmodels, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 9 Sep 2017 04:31:17 GMT"}, {"version": "v2", "created": "Mon, 9 Oct 2017 10:26:20 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 05:43:40 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bhattacharjee", "Natalia V.", ""], ["Ranjan", "Pritam", ""], ["Mandal", "Abhyuday", ""], ["Tollner", "Ernest W.", ""]]}, {"id": "1709.03162", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Bayesian bandits: balancing the exploration-exploitation tradeoff via\n  double sampling", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning studies how to balance exploration and exploitation in\nreal-world systems, optimizing interactions with the world while simultaneously\nlearning how the world operates. One general class of algorithms for such\nlearning is the multi-armed bandit setting. Randomized probability matching,\nbased upon the Thompson sampling approach introduced in the 1930s, has recently\nbeen shown to perform well and to enjoy provable optimality properties. It\npermits generative, interpretable modeling in a Bayesian setting, where prior\nknowledge is incorporated, and the computed posteriors naturally capture the\nfull state of knowledge. In this work, we harness the information contained in\nthe Bayesian posterior and estimate its sufficient statistics via sampling. In\nseveral application domains, for example in health and medicine, each\ninteraction with the world can be expensive and invasive, whereas drawing\nsamples from the model is relatively inexpensive. Exploiting this viewpoint, we\ndevelop a double sampling technique driven by the uncertainty in the learning\nprocess: it favors exploitation when certain about the properties of each arm,\nexploring otherwise. The proposed algorithm does not make any distributional\nassumption and it is applicable to complex reward distributions, as long as\nBayesian posterior updates are computable. Utilizing the estimated posterior\nsufficient statistics, double sampling autonomously balances the\nexploration-exploitation tradeoff to make better informed decisions. We\nempirically show its reduced cumulative regret when compared to\nstate-of-the-art alternatives in representative bandit settings.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 19:58:34 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 20:20:27 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1709.03163", "submitter": "I\\~nigo Urteaga", "authors": "I\\~nigo Urteaga and Chris H. Wiggins", "title": "Variational inference for the multi-armed contextual bandit", "comments": "The software used for this study is publicly available at\n  https://github.com/iurteaga/bandits", "journal-ref": "Proceedings of the Twenty-First International Conference on\n  Artificial Intelligence and Statistics, PMLR 84:698-706, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many biomedical, science, and engineering problems, one must sequentially\ndecide which action to take next so as to maximize rewards. One general class\nof algorithms for optimizing interactions with the world, while simultaneously\nlearning how the world operates, is the multi-armed bandit setting and, in\nparticular, the contextual bandit case. In this setting, for each executed\naction, one observes rewards that are dependent on a given 'context', available\nat each interaction with the world. The Thompson sampling algorithm has\nrecently been shown to enjoy provable optimality properties for this set of\nproblems, and to perform well in real-world settings. It facilitates generative\nand interpretable modeling of the problem at hand. Nevertheless, the design and\ncomplexity of the model limit its application, since one must both sample from\nthe distributions modeled and calculate their expected rewards. We here show\nhow these limitations can be overcome using variational inference to\napproximate complex models, applying to the reinforcement learning case\nadvances developed for the inference case in the machine learning community\nover the past two decades. We consider contextual multi-armed bandit\napplications where the true reward distribution is unknown and complex, which\nwe approximate with a mixture model whose parameters are inferred via\nvariational inference. We show how the proposed variational Thompson sampling\napproach is accurate in approximating the true distribution, and attains\nreduced regrets even with complex reward distributions. The proposed algorithm\nis valuable for practical scenarios where restrictive modeling assumptions are\nundesirable.\n", "versions": [{"version": "v1", "created": "Sun, 10 Sep 2017 19:58:44 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 20:16:40 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 20:40:32 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Urteaga", "I\u00f1igo", ""], ["Wiggins", "Chris H.", ""]]}, {"id": "1709.03283", "submitter": "Bruno Sudret", "authors": "Joseph B. Nagel and J\\\"org Rieckermann and Bruno Sudret", "title": "Principal component analysis and sparse polynomial chaos expansions for\n  global sensitivity analysis and model calibration: application to urban\n  drainage simulation", "comments": null, "journal-ref": null, "doi": "10.1016/j.ress.2019.106737", "report-no": "RSUQ-2017-010B", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient surrogate modeling strategy for the\nuncertainty quantification and Bayesian calibration of a hydrological model. In\nparticular, a process-based dynamical urban drainage simulator that predicts\nthe discharge from a catchment area during a precipitation event is considered.\nThe goal of the case study is to perform a global sensitivity analysis and to\nidentify the unknown model parameters as well as the measurement and prediction\nerrors. These objectives can only be achieved by cheapening the incurred\ncomputational costs, that is, lowering the number of necessary model runs. With\nthis in mind, a regularity-exploiting metamodeling technique is proposed that\nenables fast uncertainty quantification. Principal component analysis is used\nfor output dimensionality reduction and sparse polynomial chaos expansions are\nused for the emulation of the reduced outputs. Sobol' sensitivity indices are\nobtained directly from the expansion coefficients by a mere post-processing.\nBayesian inference via Markov chain Monte Carlo posterior sampling is\ndrastically accelerated.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 07:56:09 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 08:39:15 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Nagel", "Joseph B.", ""], ["Rieckermann", "J\u00f6rg", ""], ["Sudret", "Bruno", ""]]}, {"id": "1709.03312", "submitter": "Louis Ellam", "authors": "Louis Ellam, Heiko Strathmann, Mark Girolami and Iain Murray", "title": "A determinant-free method to simulate the parameters of large Gaussian\n  fields", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": "10.1002/sta4.153", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a determinant-free approach for simulation-based Bayesian\ninference in high-dimensional Gaussian models. We introduce auxiliary variables\nwith covariance equal to the inverse covariance of the model. The joint\nprobability of the auxiliary model can be computed without evaluating\ndeterminants, which are often hard to compute in high dimensions. We develop a\nMarkov chain Monte Carlo sampling scheme for the auxiliary model that requires\nno more than the application of inverse-matrix-square-roots and the solution of\nlinear systems. These operations can be performed at large scales with rational\napproximations. We provide an empirical study on both synthetic and real-world\ndata for sparse Gaussian processes and for large-scale Gaussian Markov random\nfields.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 09:56:31 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Ellam", "Louis", ""], ["Strathmann", "Heiko", ""], ["Girolami", "Mark", ""], ["Murray", "Iain", ""]]}, {"id": "1709.03471", "submitter": "Alan Benson", "authors": "Alan Benson and Nial Friel", "title": "Bayesian inference, model selection and likelihood estimation using fast\n  rejection sampling: the Conway-Maxwell-Poisson distribution", "comments": "To appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for models with intractable likelihood functions\nrepresents a challenging suite of problems in modern statistics. In this work\nwe analyse the Conway-Maxwell-Poisson (COM-Poisson) distribution, a two\nparameter generalisation of the Poisson distribution. COM-Poisson regression\nmodelling allows the flexibility to model dispersed count data as part of a\ngeneralised linear model (GLM) with a COM-Poisson response, where exogenous\ncovariates control the mean and dispersion level of the response. The major\ndifficulty with COM-Poisson regression is that the likelihood function contains\nmultiple intractable normalising constants and is not amenable to standard\ninference and MCMC techniques. Recent work by Chanialidis et al. (2017) has\nseen the development of a sampler to draw random variates from the COM-Poisson\nlikelihood using a rejection sampling algorithm. We provide a new rejection\nsampler for the COM-Poisson distribution which significantly reduces the CPU\ntime required to perform inference for COM-Poisson regression models. A novel\nextension of this work shows that for any intractable likelihood function with\nan associated rejection sampler it is possible to construct unbiased estimators\nof the intractable likelihood which proves useful for model selection or for\nuse within pseudo-marginal MCMC algorithms (Andrieu and Roberts, 2009). We\ndemonstrate all of these methods on a real-world dataset of takeover bids.\n", "versions": [{"version": "v1", "created": "Mon, 11 Sep 2017 16:42:08 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 15:16:41 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Benson", "Alan", ""], ["Friel", "Nial", ""]]}, {"id": "1709.04048", "submitter": "Daniel Ting", "authors": "Daniel Ting", "title": "Data Sketches for Disaggregated Subset Sum and Frequent Item Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a new data sketch for processing massive datasets. It\naddresses two common problems: 1) computing a sum given arbitrary filter\nconditions and 2) identifying the frequent items or heavy hitters in a data\nset. For the former, the sketch provides unbiased estimates with state of the\nart accuracy. It handles the challenging scenario when the data is\ndisaggregated so that computing the per unit metric of interest requires an\nexpensive aggregation. For example, the metric of interest may be total clicks\nper user while the raw data is a click stream with multiple rows per user. Thus\nthe sketch is suitable for use in a wide range of applications including\ncomputing historical click through rates for ad prediction, reporting user\nmetrics from event streams, and measuring network traffic for IP flows.\n  We prove and empirically show the sketch has good properties for both the\ndisaggregated subset sum estimation and frequent item problems. On i.i.d. data,\nit not only picks out the frequent items but gives strongly consistent\nestimates for the proportion of each frequent item. The resulting sketch\nasymptotically draws a probability proportional to size sample that is optimal\nfor estimating sums over the data. For non i.i.d. data, we show that it\ntypically does much better than random sampling for the frequent item problem\nand never does worse. For subset sum estimation, we show that even for\npathological sequences, the variance is close to that of an optimal sampling\ndesign. Empirically, despite the disadvantage of operating on disaggregated\ndata, our method matches or bests priority sampling, a state of the art method\nfor pre-aggregated data and performs orders of magnitude better on skewed data\ncompared to uniform sampling. We propose extensions to the sketch that allow it\nto be used in combining multiple data sets, in distributed systems, and for\ntime decayed aggregation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Sep 2017 20:19:25 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Ting", "Daniel", ""]]}, {"id": "1709.04126", "submitter": "Matthew Pietrosanu", "authors": "Matthew Pietrosanu, Jueyu Gao, Linglong Kong, Bei Jiang, Di Niu", "title": "Advanced Algorithms for Penalized Quantile and Composite Quantile\n  Regression", "comments": "24 pages (including 2 appendices), 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss a family of robust, high-dimensional regression\nmodels for quantile and composite quantile regression, both with and without an\nadaptive lasso penalty for variable selection. We reformulate these quantile\nregression problems and obtain estimators by applying the alternating direction\nmethod of multipliers (ADMM), majorize-minimization (MM), and coordinate\ndescent (CD) algorithms. Our new approaches address the lack of publicly\navailable methods for (composite) quantile regression, especially for\nhigh-dimensional data, both with and without regularization. Through simulation\nstudies, we demonstrate the need for different algorithms applicable to a\nvariety of data settings, which we implement in the cqrReg package for R. For\ncomparison, we also introduce the widely used interior point (IP) formulation\nand test our methods against the IP algorithms in the existing quantreg\npackage. Our simulation studies show that each of our methods, particularly MM\nand CD, excel in different settings such as with large or high-dimensional data\nsets, respectively, and outperform the methods currently implemented in\nquantreg. The ADMM approach offers specific promise for future developments in\nits amenability to parallelization and scalability.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 03:38:33 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 15:39:30 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Pietrosanu", "Matthew", ""], ["Gao", "Jueyu", ""], ["Kong", "Linglong", ""], ["Jiang", "Bei", ""], ["Niu", "Di", ""]]}, {"id": "1709.04196", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Hans K\\\"unsch", "title": "Particle Filters and Data Assimilation", "comments": "To appear in `Annual Review of Statistics and Its Application'", "journal-ref": null, "doi": "10.1146/annurev-statistics-031017-100232", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models can be used to incorporate subject knowledge on the\nunderlying dynamics of a time series by the introduction of a latent Markov\nstate-process. A user can specify the dynamics of this process together with\nhow the state relates to partial and noisy observations that have been made.\nInference and prediction then involves solving a challenging inverse problem:\ncalculating the conditional distribution of quantities of interest given the\nobservations. This article reviews Monte Carlo algorithms for solving this\ninverse problem, covering methods based on the particle filter and the ensemble\nKalman filter. We discuss the challenges posed by models with high-dimensional\nstates, joint estimation of parameters and the state, and inference for the\nhistory of the state process. We also point out some potential new developments\nwhich will be important for tackling cutting-edge filtering applications.\n", "versions": [{"version": "v1", "created": "Wed, 13 Sep 2017 08:59:35 GMT"}], "update_date": "2017-09-14", "authors_parsed": [["Fearnhead", "Paul", ""], ["K\u00fcnsch", "Hans", ""]]}, {"id": "1709.04419", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko, Ying Sun, Marc G. Genton and David Keyes", "title": "Likelihood Approximation With Hierarchical Matrices For Large Spatial\n  Datasets", "comments": "23 pages, 24 figures, 3 tables, version after the second major\n  revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use available measurements to estimate the unknown parameters (variance,\nsmoothness parameter, and covariance length) of a covariance function by\nmaximizing the joint Gaussian log-likelihood function. To overcome cubic\ncomplexity in the linear algebra, we approximate the discretized covariance\nfunction in the hierarchical (H-) matrix format. The H-matrix format has a\nlog-linear computational cost and storage $O(kn \\log n)$, where the rank $k$ is\na small integer and $n$ is the number of locations. The H-matrix technique\nallows us to work with general covariance matrices in an efficient way, since\nH-matrices can approximate inhomogeneous covariance functions, with a fairly\ngeneral mesh that is not necessarily axes-parallel, and neither the covariance\nmatrix itself nor its inverse have to be sparse. We demonstrate our method with\nMonte Carlo simulations and an application to soil moisture data. The C, C++\ncodes and data are freely available.\n", "versions": [{"version": "v1", "created": "Fri, 8 Sep 2017 18:03:12 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 09:59:58 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Litvinenko", "Alexander", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""], ["Keyes", "David", ""]]}, {"id": "1709.04611", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen", "title": "A Novel Algorithm for Clustering of Data on the Unit Sphere via Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new maximum approximate likelihood (ML) estimation algorithm for the\nmixture of Kent distribution is proposed. The new algorithm is constructed via\nthe BSLM (block successive lower-bound maximization) framework and incorporates\nmanifold optimization procedures within it. The BSLM algorithm is iterative and\nmonotonically increases the approximate log-likelihood function in each step.\nUnder mild regularity conditions, the BSLM algorithm is proved to be convergent\nand the approximate ML estimator is proved to be consistent. A Bayesian\ninformation criterion-like (BIC-like) model selection criterion is also derive,\nfor the task of choosing the number of components in the mixture distribution.\nThe approximate ML estimator and the BIC-like criterion are both demonstrated\nto be successful via simulation studies. A model-based clustering rule is\nproposed and also assessed favorably via simulations. Example applications of\nthe developed methodology are provided via an image segmentation task and a\nneural imaging clustering problem.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 04:39:20 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Nguyen", "Hien D.", ""]]}, {"id": "1709.04718", "submitter": "Vivak Patel", "authors": "Vivak Patel", "title": "The Impact of Local Geometry and Batch Size on Stochastic Gradient\n  Descent for Nonconvex Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several experimental reports on nonconvex optimization problems in machine\nlearning, stochastic gradient descent (SGD) was observed to prefer minimizers\nwith flat basins in comparison to more deterministic methods, yet there is very\nlittle rigorous understanding of this phenomenon. In fact, the lack of such\nwork has led to an unverified, but widely-accepted stochastic mechanism\ndescribing why SGD prefers flatter minimizers to sharper minimizers. However,\nas we demonstrate, the stochastic mechanism fails to explain this phenomenon.\nHere, we propose an alternative deterministic mechanism that can accurately\nexplain why SGD prefers flatter minimizers to sharper minimizers. We derive\nthis mechanism based on a detailed analysis of a generic stochastic quadratic\nproblem, which generalizes known results for classical gradient descent.\nFinally, we verify the predictions of our deterministic mechanism on two\nnonconvex problems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 11:59:10 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 18:47:53 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Patel", "Vivak", ""]]}, {"id": "1709.04743", "submitter": "Sebastian Lerch", "authors": "Alexander Jordan, Fabian Kr\\\"uger, Sebastian Lerch", "title": "Evaluating probabilistic forecasts with scoringRules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic forecasts in the form of probability distributions over future\nevents have become popular in several fields including meteorology, hydrology,\neconomics, and demography. In typical applications, many alternative\nstatistical models and data sources can be used to produce probabilistic\nforecasts. Hence, evaluating and selecting among competing methods is an\nimportant task. The scoringRules package for R provides functionality for\ncomparative evaluation of probabilistic models based on proper scoring rules,\ncovering a wide range of situations in applied work. This paper discusses\nimplementation and usage details, presents case studies from meteorology and\neconomics, and points to the relevant background literature.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 12:55:24 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:24:06 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Jordan", "Alexander", ""], ["Kr\u00fcger", "Fabian", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1709.04835", "submitter": "James Fry", "authors": "J.T. Fry, Matt Slifko, Scotland Leman", "title": "Generalized Biplots for Multidimensional Scaled Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction and visualization is a staple of data analytics. Methods\nsuch as Principal Component Analysis (PCA) and Multidimensional Scaling (MDS)\nprovide low dimensional (LD) projections of high dimensional (HD) data while\npreserving an HD relationship between observations. Traditional biplots assign\nmeaning to the LD space of a PCA projection by displaying LD axes for the\nattributes. These axes, however, are specific to the linear projection used in\nPCA. MDS projections, which allow for arbitrary stress and dissimilarity\nfunctions, require special care when labeling the LD space. We propose an\niterative scheme to plot an LD axis for each attribute based on the\nuser-specified stress and dissimilarity metrics. We discuss the details of our\ngeneral biplot methodology, its relationship with PCA-derived biplots, and\nprovide examples using real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 15:03:33 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 16:30:36 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Fry", "J. T.", ""], ["Slifko", "Matt", ""], ["Leman", "Scotland", ""]]}, {"id": "1709.05006", "submitter": "Xiuyuan Cheng", "authors": "Xiuyuan Cheng, Alexander Cloninger and Ronald R. Coifman", "title": "Two-sample Statistics Based on Anisotropic Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a new kernel-based Maximum Mean Discrepancy (MMD)\nstatistic for measuring the distance between two distributions given\nfinitely-many multivariate samples. When the distributions are locally\nlow-dimensional, the proposed test can be made more powerful to distinguish\ncertain alternatives by incorporating local covariance matrices and\nconstructing an anisotropic kernel. The kernel matrix is asymmetric; it\ncomputes the affinity between $n$ data points and a set of $n_R$ reference\npoints, where $n_R$ can be drastically smaller than $n$. While the proposed\nstatistic can be viewed as a special class of Reproducing Kernel Hilbert Space\nMMD, the consistency of the test is proved, under mild assumptions of the\nkernel, as long as $\\|p-q\\| \\sqrt{n} \\to \\infty $, and a finite-sample lower\nbound of the testing power is obtained. Applications to flow cytometry and\ndiffusion MRI datasets are demonstrated, which motivate the proposed approach\nto compare distributions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Sep 2017 23:06:19 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 15:39:36 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 21:56:28 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Cheng", "Xiuyuan", ""], ["Cloninger", "Alexander", ""], ["Coifman", "Ronald R.", ""]]}, {"id": "1709.05515", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Suhas N., Talasila Sai Teja and Anshul Juneja", "title": "Some variations on Ensembled Random Survival Forest with application to\n  Cancer Research", "comments": "16 pages; 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a novel implementation of adaboost for prediction\nof survival function. We take different variations of the algorithm and compare\nthe algorithms based on system run time and root mean square error. Our\nconstruction includes right censoring data and competing risk data too. We take\ndifferent data set to illustrate the performance of the algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 14:12:20 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 19:24:01 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["N.", "Suhas", ""], ["Teja", "Talasila Sai", ""], ["Juneja", "Anshul", ""]]}, {"id": "1709.05534", "submitter": "Camilo Jose Torres-Jimenez", "authors": "Camilo Jose Torres-Jimenez and Alvaro Mauricio Montenegro-Diaz", "title": "An alternative to continuous univariate distributions supported on a\n  bounded interval: The BMT distribution", "comments": "30 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the BMT distribution as an unimodal alternative\nto continuous univariate distributions supported on a bounded interval. The\nideas behind the mathematical formulation of this new distribution come from\ncomputer aid geometric design, specifically from Bezier curves. First, we\nreview general properties of a distribution given by parametric equations and\nextend the definition of a Bezier distribution. Then, after proposing the BMT\ncumulative distribution function, we derive its probability density function\nand a closed-form expression for quantile function, median, interquartile\nrange, mode, and moments. The domain change from [0,1] to [c,d] is mentioned.\nEstimation of parameters is approached by the methods of maximum likelihood and\nmaximum product of spacing. We test the numerical estimation procedures using\nsome simulated data. Usefulness and flexibility of the new distribution are\nillustrated in three real data sets. The BMT distribution has a significant\npotential to estimate domain parameters and to model data outside the scope of\nthe beta or similar distributions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Sep 2017 15:48:19 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Torres-Jimenez", "Camilo Jose", ""], ["Montenegro-Diaz", "Alvaro Mauricio", ""]]}, {"id": "1709.05870", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Jianfei Chen, Jun Zhu, Shengyang Sun, Yucen Luo, Yihong\n  Gu, Yuhao Zhou", "title": "ZhuSuan: A Library for Bayesian Deep Learning", "comments": "The GitHub page is at https://github.com/thu-ml/zhusuan", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce ZhuSuan, a python probabilistic programming\nlibrary for Bayesian deep learning, which conjoins the complimentary advantages\nof Bayesian methods and deep learning. ZhuSuan is built upon Tensorflow. Unlike\nexisting deep learning libraries, which are mainly designed for deterministic\nneural networks and supervised tasks, ZhuSuan is featured for its deep root\ninto Bayesian inference, thus supporting various kinds of probabilistic models,\nincluding both the traditional hierarchical Bayesian models and recent deep\ngenerative models. We use running examples to illustrate the probabilistic\nprogramming on ZhuSuan, including Bayesian logistic regression, variational\nauto-encoders, deep sigmoid belief networks and Bayesian recurrent neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 11:30:08 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Shi", "Jiaxin", ""], ["Chen", "Jianfei", ""], ["Zhu", "Jun", ""], ["Sun", "Shengyang", ""], ["Luo", "Yucen", ""], ["Gu", "Yihong", ""], ["Zhou", "Yuhao", ""]]}, {"id": "1709.05885", "submitter": "Bangti Jin", "authors": "Simon Arridge, Kazufumi Ito, Bangti Jin, Chen Zhang", "title": "Variational Gaussian Approximation for Poisson Data", "comments": "26 pages", "journal-ref": null, "doi": "10.1088/1361-6420/aaa0ab", "report-no": null, "categories": "math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Poisson model is frequently employed to describe count data, but in a\nBayesian context it leads to an analytically intractable posterior probability\ndistribution. In this work, we analyze a variational Gaussian approximation to\nthe posterior distribution arising from the Poisson model with a Gaussian\nprior. This is achieved by seeking an optimal Gaussian distribution minimizing\nthe Kullback-Leibler divergence from the posterior distribution to the\napproximation, or equivalently maximizing the lower bound for the model\nevidence. We derive an explicit expression for the lower bound, and show the\nexistence and uniqueness of the optimal Gaussian approximation. The lower bound\nfunctional can be viewed as a variant of classical Tikhonov regularization that\npenalizes also the covariance. Then we develop an efficient alternating\ndirection maximization algorithm for solving the optimization problem, and\nanalyze its convergence. We discuss strategies for reducing the computational\ncomplexity via low rank structure of the forward operator and the sparsity of\nthe covariance. Further, as an application of the lower bound, we discuss\nhierarchical Bayesian modeling for selecting the hyperparameter in the prior\ndistribution, and propose a monotonically convergent algorithm for determining\nthe hyperparameter. We present extensive numerical experiments to illustrate\nthe Gaussian approximation and the algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 12:12:12 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Arridge", "Simon", ""], ["Ito", "Kazufumi", ""], ["Jin", "Bangti", ""], ["Zhang", "Chen", ""]]}, {"id": "1709.05906", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Sanku Dey and Debasis Kundu", "title": "Bayesian analysis of three parameter singular Marshall-Olkin bivariate\n  Pareto distribution", "comments": "23 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides bayesian analysis of singular Marshall-Olkin bivariate\nPareto distribution. We consider three parameter singular Marshall-Olkin\nbivariate Pareto distribution. We consider two types of prior - reference prior\nand gamma prior. Bayes estimate of the parameters are calculated based on slice\ncum gibbs sampler and Lindley approximation. Credible interval is also provided\nfor all methods and all prior distributions. A data analysis is kept for\nillustrative purpose.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 13:13:12 GMT"}, {"version": "v2", "created": "Sat, 30 Sep 2017 02:06:48 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""], ["Kundu", "Debasis", ""]]}, {"id": "1709.06181", "submitter": "Tom Rainforth", "authors": "Tom Rainforth, Robert Cornish, Hongseok Yang, Andrew Warrington, Frank\n  Wood", "title": "On Nesting Monte Carlo Estimators", "comments": "To appear at International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in machine learning and statistics involve nested expectations\nand thus do not permit conventional Monte Carlo (MC) estimation. For such\nproblems, one must nest estimators, such that terms in an outer estimator\nthemselves involve calculation of a separate, nested, estimation. We\ninvestigate the statistical implications of nesting MC estimators, including\ncases of multiple levels of nesting, and establish the conditions under which\nthey converge. We derive corresponding rates of convergence and provide\nempirical evidence that these rates are observed in practice. We further\nestablish a number of pitfalls that can arise from naive nesting of MC\nestimators, provide guidelines about how these can be avoided, and lay out\nnovel methods for reformulating certain classes of nested expectation problems\ninto single expectations, leading to improved convergence rates. We demonstrate\nthe applicability of our work by using our results to develop a new estimator\nfor discrete Bayesian experimental design problems and derive error bounds for\na class of variational objectives.\n", "versions": [{"version": "v1", "created": "Mon, 18 Sep 2017 22:01:05 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 20:36:06 GMT"}, {"version": "v3", "created": "Tue, 28 Nov 2017 16:04:11 GMT"}, {"version": "v4", "created": "Wed, 23 May 2018 17:11:26 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rainforth", "Tom", ""], ["Cornish", "Robert", ""], ["Yang", "Hongseok", ""], ["Warrington", "Andrew", ""], ["Wood", "Frank", ""]]}, {"id": "1709.06254", "submitter": "Aijun Zhang", "authors": "Canhong Wen, Aijun Zhang, Shijie Quan, Xueqin Wang", "title": "BeSS: An R Package for Best Subset Selection in Linear, Logistic and\n  CoxPH Models", "comments": "To appear in Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new R package, BeSS, for solving the best subset selection\nproblem in linear, logistic and Cox's proportional hazard (CoxPH) models. It\nutilizes a highly efficient active set algorithm based on primal and dual\nvariables, and supports sequential and golden search strategies for best subset\nselection. We provide a C++ implementation of the algorithm using Rcpp\ninterface. We demonstrate through numerical experiments based on enormous\nsimulation and real datasets that the new BeSS package has competitive\nperformance compared to other R packages for best subset selection purpose.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 04:55:10 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 05:30:57 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wen", "Canhong", ""], ["Zhang", "Aijun", ""], ["Quan", "Shijie", ""], ["Wang", "Xueqin", ""]]}, {"id": "1709.06272", "submitter": "Udaysinh T. Bhosale", "authors": "Udaysinh T. Bhosale", "title": "Entanglement transitions induced by large deviations", "comments": "12 pages, 4 figures. Comments are welcome", "journal-ref": "Phys. Rev. E 96, 062149 (2017)", "doi": "10.1103/PhysRevE.96.062149", "report-no": null, "categories": "quant-ph math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability of large deviations of the smallest Schmidt eigenvalue for\nrandom pure states of bipartite systems, denoted as $A$ and $B$, is computed\nanalytically using a Coulomb gas method. It is shown that this probability, for\nlarge $N$, goes as $\\exp[-\\beta N^2\\Phi(\\zeta)]$, where the parameter $\\beta$\nis the Dyson index of the ensemble, $\\zeta$ is the large deviation parameter\nwhile the rate function $\\Phi(\\zeta)$ is calculated exactly. Corresponding\nequilibrium Coulomb charge density is derived for its large deviations. Effects\nof the large deviations of the extreme (largest and smallest) Schmidt\neigenvalues on the bipartite entanglement are studied using the von Neumann\nentropy. Effect of these deviations is also studied on the entanglement between\nsubsystems $1$ and $2$, obtained by further partitioning the subsystem $A$,\nusing the properties of the density matrix's partial transpose\n$\\rho_{12}^\\Gamma$. The density of states of $\\rho_{12}^\\Gamma$ is found to be\nclose to the Wigner's semicircle law with these large deviations. The\nentanglement properties are captured very well by a simple random matrix model\nfor the partial transpose. The model predicts the entanglement transition\nacross a critical large deviation parameter $\\zeta$. Log negativity is used to\nquantify the entanglement between subsystems $1$ and $2$. Analytical formulas\nfor it are derived using the simple model. Numerical simulations are in\nexcellent agreement with the analytical results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 07:04:59 GMT"}, {"version": "v2", "created": "Sat, 16 Dec 2017 11:40:51 GMT"}, {"version": "v3", "created": "Fri, 29 Dec 2017 06:15:19 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Bhosale", "Udaysinh T.", ""]]}, {"id": "1709.06597", "submitter": "Peter Carbonetto", "authors": "Peter Carbonetto, Xiang Zhou and Matthew Stephens", "title": "varbvs: Fast Variable Selection for Large-scale Regression", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce varbvs, a suite of functions written in R and MATLAB for\nregression analysis of large-scale data sets using Bayesian variable selection\nmethods. We have developed numerical optimization algorithms based on\nvariational approximation methods that make it feasible to apply Bayesian\nvariable selection to very large data sets. With a focus on examples from\ngenome-wide association studies, we demonstrate that varbvs scales well to data\nsets with hundreds of thousands of variables and thousands of samples, and has\nfeatures that facilitate rapid data analyses. Moreover, varbvs allows for\nextensive model customization, which can be used to incorporate external\ninformation into the analysis. We expect that the combination of an easy-to-use\ninterface and robust, scalable algorithms for posterior computation will\nencourage broader use of Bayesian variable selection in areas of applied\nstatistics and computational biology. The most recent R and MATLAB source code\nis available for download at Github (https://github.com/pcarbo/varbvs), and the\nR package can be installed from CRAN\n(https://cran.r-project.org/package=varbvs).\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 18:29:35 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Carbonetto", "Peter", ""], ["Zhou", "Xiang", ""], ["Stephens", "Matthew", ""]]}, {"id": "1709.06633", "submitter": "Michael Crowther", "authors": "Michael J. Crowther", "title": "Multilevel mixed effects parametric survival analysis: Estimation,\n  simulation and application", "comments": null, "journal-ref": null, "doi": "10.1177/1536867X19893639", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, I present the user written stmixed command for the fitting\nof multilevel survival models, which serves as both an alternative to Stata's\nofficial mestreg, and a complimentary program with substantial extensions.\nstmixed can fit multilevel survival models with any number of levels and random\neffects at each level, including flexible spline-based approaches (such as\nRoyston-Parmar and the log hazard equivalent) or user-defined hazard models.\nSimple or complex time-dependent effects can be included, as well as the\naddition of expected mortality for a relative survival model.\nLeft-truncation/delayed entry can be used and t-distributed random effects are\nprovided as an alternative to Gaussian random effects. The methods are\nillustrated with a commonly used dataset of patients with kidney disease\nsuffering recurrent infections, and a simulated example, illustrating a simple\napproach to simulating clustered survival data using survsim (Crowther and\nLambert 2012, 2013). stmixed is part of the merlin family (Crowther 2017,\n2018).\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:25:00 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 08:46:29 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Crowther", "Michael J.", ""]]}, {"id": "1709.06635", "submitter": "Alberto Carrassi", "authors": "Sammy Metref, Alexis Hannart, Juan Ruiz, Marc Bocquet, Alberto\n  Carrassi and Michael Ghil", "title": "Estimating model evidence using ensemble-based data assimilation with\n  localization - The model selection problem", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IIn recent years, there has been a growing interest in applying data\nassimilation (DA) methods, originally designed for state estimation, to the\nmodel selection problem. In this setting, Carrassi et al. (2017) introduced the\ncontextual formulation of model evidence (CME) and showed that CME can be\nefficiently computed using a hierarchy of ensemble-based DA procedures.\nAlthough Carrassi et al. (2017) analyzed the DA methods most commonly used for\noperational atmospheric and oceanic prediction worldwide, they did not study\nthese methods in conjunction with localization to a specific domain. Yet any\napplication of ensemble DA methods to realistic geophysical models requires the\nimplementation of some form of localization. The present study extends the\ntheory for estimating CME to ensemble DA methods with domain localization. The\ndomain-localized CME (DL-CME) developed herein is tested for model selection\nwith two models: (i) the Lorenz 40-variable mid-latitude atmospheric dynamics\nmodel (L95); and (ii) the simplified global atmospheric SPEEDY model. The CME\nis compared to the root-mean-square-error (RMSE) as a metric for model\nselection. The experiments show that CME improves systematically over the RMSE,\nand that this skill improvement is further enhanced by applying localization in\nthe estimate of the CME, using the DL-CME. The potential use and range of\napplications of the CME and DL-CME as a model selection metric are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 20:29:56 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 12:59:14 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Metref", "Sammy", ""], ["Hannart", "Alexis", ""], ["Ruiz", "Juan", ""], ["Bocquet", "Marc", ""], ["Carrassi", "Alberto", ""], ["Ghil", "Michael", ""]]}, {"id": "1709.06849", "submitter": "Saeid Amiri", "authors": "Saeid Amiri", "title": "Rbox: an integrated R package for ATOM Editor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  R is a programming language and environment that is a central tool in the\napplied sciences for writing program. Its impact on the development of modern\nstatistics is inevitable. Current research, especially for big data may not be\ndone solely using R and will likely use different programming languages; hence,\nhaving a modern integrated development environment (IDE) is very important.\nAtom editor is modern IDE that is developed by GitHub, it is described as \"A\nhackable text editor for the 21st Century\". This report is intended to present\na package deployed entitled Rbox that allows Atom Editor to write and run codes\nprofessionally in R.\n", "versions": [{"version": "v1", "created": "Tue, 19 Sep 2017 14:38:20 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Amiri", "Saeid", ""]]}, {"id": "1709.06896", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (2, 3, 1), Julien Bect (3, 1), S\\'everine Demeyer (2),\n  Nicolas Fischer (2), Emmanuel Vazquez (3, 1) ((1) GdR MASCOT-NUM, (2) LNE,\n  (3) L2S)", "title": "Integrating hyper-parameter uncertainties in a multi-fidelity Bayesian\n  model for the estimation of a probability of failure", "comments": null, "journal-ref": null, "doi": "10.1142/9789813274303_0035", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-fidelity simulator is a numerical model, in which one of the inputs\ncontrols a trade-off between the realism and the computational cost of the\nsimulation. Our goal is to estimate the probability of exceeding a given\nthreshold on a multi-fidelity stochastic simulator. We propose a fully Bayesian\napproach based on Gaussian processes to compute the posterior probability\ndistribution of this probability. We pay special attention to the\nhyper-parameters of the model. Our methodology is illustrated on an academic\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 20 Sep 2017 14:22:17 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Stroh", "R\u00e9mi", ""], ["Bect", "Julien", ""], ["Demeyer", "S\u00e9verine", ""], ["Fischer", "Nicolas", ""], ["Vazquez", "Emmanuel", ""]]}, {"id": "1709.07557", "submitter": "Hadi Meidani", "authors": "Negin Alemazkoor, Hadi Meidani", "title": "A preconditioning approach for improved estimation of sparse polynomial\n  chaos expansions", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2018.08.005", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sampling has been widely used for sparse polynomial chaos (PC)\napproximation of stochastic functions. The recovery accuracy of compressive\nsampling highly depends on the incoherence properties of the measurement\nmatrix. In this paper, we consider preconditioning the underdetermined system\nof equations that is to be solved. Premultiplying a linear equation system by a\nnon-singular matrix results in an equivalent equation system, but it can\npotentially improve the incoherence properties of the resulting preconditioned\nmeasurement matrix and lead to a better recovery accuracy. When measurements\nare noisy, however, preconditioning can also potentially result in a worse\nsignal-to-noise ratio, thereby deteriorating recovery accuracy. In this work,\nwe propose a preconditioning scheme that improves the incoherence properties of\nmeasurement matrix and at the same time prevents undesirable deterioration of\nsignal-to-noise ratio. We provide theoretical motivations and numerical\nexamples that demonstrate the promise of the proposed approach in improving the\naccuracy of estimated polynomial chaos expansions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 01:05:44 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 20:31:15 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Alemazkoor", "Negin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1709.07637", "submitter": "Bruno Sudret", "authors": "I. Abdallah, C. Lataniotis and B. Sudret", "title": "Hierarchical Kriging for multi-fidelity aero-servo-elastic simulators -\n  Application to extreme loads on wind turbines", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-011", "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present work, we consider multi-fidelity surrogate modelling to fuse\nthe output of multiple aero-servo-elastic computer simulators of varying\ncomplexity. In many instances, predictions from multiple simulators for the\nsame quantity of interest on a wind turbine are available. In this type of\nsituation, there is strong evidence that fusing the output from multiple\naero-servo-elastic simulators yields better predictive ability and lower model\nuncertainty than using any single simulator. Hierarchical Kriging is a\nmulti-fidelity surrogate modelling method in which the Kriging surrogate model\nof the cheap (low-fidelity) simulator is used as a trend of the Kriging\nsurrogate model of the higher fidelity simulator. We propose a parametric\napproach to Hierarchical Kriging where the best surrogate models are selected\nbased on evaluating all possible combinations of the available Kriging\nparameters candidates. The parametric Hierarchical Kriging approach is\nillustrated by fusing the extreme flapwise bending moment at the blade root of\na large multi-megawatt wind turbine as a function of wind velocity, turbulence\nand wind shear exponent in the presence of model uncertainty and\nheterogeneously noisy output. The extreme responses are obtained by two widely\naccepted wind turbine specific aero-servo-elastic computer simulators, FAST and\nBladed. With limited high-fidelity simulations, Hierarchical Kriging produces\nmore accurate predictions of validation data compared to conventional Kriging.\nIn addition, contrary to conventional Kriging, Hierarchical Kriging is shown to\nbe a robust surrogate modelling technique because it is less sensitive to the\nchoice of the Kriging parameters and the choice of the estimation error.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 08:45:15 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Abdallah", "I.", ""], ["Lataniotis", "C.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.07710", "submitter": "Fl\\'avio Gon\\c{c}alves", "authors": "Flavio B. Gon\\c{c}alves, Krzysztof {\\L}atuszy\\'nski, Gareth O. Roberts", "title": "Barker's algorithm for Bayesian inference with intractable likelihoods", "comments": "To appear in the Brazilian Journal of Probability and Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this expository paper we abstract and describe a simple MCMC scheme for\nsampling from intractable target densities. The approach has been introduced in\nGon\\c{c}alves et al. (2017a) in the specific context of jump-diffusions, and is\nbased on the Barker's algorithm paired with a simple Bernoulli factory type\nscheme, the so called 2-coin algorithm. In many settings it is an alternative\nto standard Metropolis-Hastings pseudo-marginal method for simulating from\nintractable target densities. Although Barker's is well-known to be slightly\nless efficient than Metropolis-Hastings, the key advantage of our approach is\nthat it allows to implement the \"marginal Barker's\" instead of the extended\nstate space pseudo-marginal Metropolis-Hastings, owing to the special form of\nthe accept/reject probability. We shall illustrate our methodology in the\ncontext of Bayesian inference for discretely observed Wright-Fisher family of\ndiffusions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 12:27:22 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Gon\u00e7alves", "Flavio B.", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1709.07915", "submitter": "Amir Karami", "authors": "George Shaw Jr., and Amir Karami", "title": "Computational Content Analysis of Negative Tweets for Obesity, Diet,\n  Diabetes, and Exercise", "comments": "The 2017 Annual Meeting of the Association for Information Science\n  and Technology (ASIST)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media based digital epidemiology has the potential to support faster\nresponse and deeper understanding of public health related threats. This study\nproposes a new framework to analyze unstructured health related textual data\nvia Twitter users' post (tweets) to characterize the negative health sentiments\nand non-health related concerns in relations to the corpus of negative\nsentiments, regarding Diet Diabetes Exercise, and Obesity (DDEO). Through the\ncollection of 6 million Tweets for one month, this study identified the\nprominent topics of users as it relates to the negative sentiments. Our\nproposed framework uses two text mining methods, sentiment analysis and topic\nmodeling, to discover negative topics. The negative sentiments of Twitter users\nsupport the literature narratives and the many morbidity issues that are\nassociated with DDEO and the linkage between obesity and diabetes. The\nframework offers a potential method to understand the publics' opinions and\nsentiments regarding DDEO. More importantly, this research provides new\nopportunities for computational social scientists, medical experts, and public\nhealth professionals to collectively address DDEO-related issues.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:18:42 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Shaw", "George", "Jr."], ["Karami", "Amir", ""]]}, {"id": "1709.07916", "submitter": "Amir Karami", "authors": "Amir Karami, Alicia A. Dahl, Gabrielle Turner-McGrievy, Hadi Kharrazi,\n  Jr., George Shaw", "title": "Characterizing Diabetes, Diet, Exercise, and Obesity Comments on Twitter", "comments": "International Journal of Information Management (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CL stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media provide a platform for users to express their opinions and share\ninformation. Understanding public health opinions on social media, such as\nTwitter, offers a unique approach to characterizing common health issues such\nas diabetes, diet, exercise, and obesity (DDEO), however, collecting and\nanalyzing a large scale conversational public health data set is a challenging\nresearch task. The goal of this research is to analyze the characteristics of\nthe general public's opinions in regard to diabetes, diet, exercise and obesity\n(DDEO) as expressed on Twitter. A multi-component semantic and linguistic\nframework was developed to collect Twitter data, discover topics of interest\nabout DDEO, and analyze the topics. From the extracted 4.5 million tweets, 8%\nof tweets discussed diabetes, 23.7% diet, 16.6% exercise, and 51.7% obesity.\nThe strongest correlation among the topics was determined between exercise and\nobesity. Other notable correlations were: diabetes and obesity, and diet and\nobesity DDEO terms were also identified as subtopics of each of the DDEO\ntopics. The frequent subtopics discussed along with Diabetes, excluding the\nDDEO terms themselves, were blood pressure, heart attack, yoga, and Alzheimer.\nThe non-DDEO subtopics for Diet included vegetarian, pregnancy, celebrities,\nweight loss, religious, and mental health, while subtopics for Exercise\nincluded computer games, brain, fitness, and daily plan. Non-DDEO subtopics for\nObesity included Alzheimer, cancer, and children. With 2.67 billion social\nmedia users in 2016, publicly available data such as Twitter posts can be\nutilized to support clinical providers, public health experts, and social\nscientists in better understanding common public opinions in regard to\ndiabetes, diet, exercise, and obesity.\n", "versions": [{"version": "v1", "created": "Fri, 22 Sep 2017 19:19:49 GMT"}], "update_date": "2019-08-17", "authors_parsed": [["Karami", "Amir", ""], ["Dahl", "Alicia A.", ""], ["Turner-McGrievy", "Gabrielle", ""], ["Kharrazi,", "Hadi", "Jr."], ["Shaw", "George", ""]]}, {"id": "1709.08221", "submitter": "Mark Steel", "authors": "Mark F.J. Steel", "title": "Model Averaging and its Use in Economics", "comments": "forthcoming; accepted version", "journal-ref": "Journal of Economic Literature, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of model averaging has become an important tool to deal with model\nuncertainty, for example in situations where a large amount of different\ntheories exist, as are common in economics. Model averaging is a natural and\nformal response to model uncertainty in a Bayesian framework, and most of the\npaper deals with Bayesian model averaging. The important role of the prior\nassumptions in these Bayesian procedures is highlighted. In addition,\nfrequentist model averaging methods are also discussed. Numerical methods to\nimplement these methods are explained, and I point the reader to some freely\navailable computational resources. The main focus is on uncertainty regarding\nthe choice of covariates in normal linear regression models, but the paper also\ncovers other, more challenging, settings, with particular emphasis on sampling\nmodels commonly used in economics. Applications of model averaging in economics\nare reviewed and discussed in a wide range of areas, among which growth\neconomics, production modelling, finance and forecasting macroeconomic\nquantities.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 16:49:05 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 09:29:53 GMT"}, {"version": "v3", "created": "Mon, 4 Feb 2019 17:33:20 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Steel", "Mark F. J.", ""]]}, {"id": "1709.08238", "submitter": "Martin Gould", "authors": "Martin D. Gould, Nikolaus Hautsch, Sam D. Howison, and Mason A. Porter", "title": "Counterparty Credit Limits: The Impact of a Risk-Mitigation Measure on\n  Everyday Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR econ.EM math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A counterparty credit limit (CCL) is a limit that is imposed by a financial\ninstitution to cap its maximum possible exposure to a specified counterparty.\nCCLs help institutions to mitigate counterparty credit risk via selective\ndiversification of their exposures. In this paper, we analyze how CCLs impact\nthe prices that institutions pay for their trades during everyday trading. We\nstudy a high-quality data set from a large electronic trading platform in the\nforeign exchange spot market, which enables institutions to apply CCLs. We find\nempirically that CCLs had little impact on the vast majority of trades in this\ndata. We also study the impact of CCLs using a new model of trading. By\nsimulating our model with different underlying CCL networks, we highlight that\nCCLs can have a major impact in some situations.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 18:53:41 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 20:45:13 GMT"}, {"version": "v3", "created": "Fri, 22 Jan 2021 19:35:56 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Gould", "Martin D.", ""], ["Hautsch", "Nikolaus", ""], ["Howison", "Sam D.", ""], ["Porter", "Mason A.", ""]]}, {"id": "1709.08258", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "On Fractionally-Supervised Classification: Weight Selection and\n  Extension to the Multivariate t-Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on fractionally-supervised classification (FSC), an approach that\nallows classification to be carried out with a fractional amount of weight\ngiven to the unlabelled points, is further developed in two respects. The\nprimary development addresses a question of fundamental importance over how to\nchoose the amount of weight given to the unlabelled points. The resolution of\nthis matter is essential because it makes FSC more readily applicable to real\nproblems. Interestingly, the resolution of the weight selection problem opens\nup the possibility of a different approach to model selection in model-based\nclustering and classification. A secondary development demonstrates that the\nFSC approach can be effective beyond Gaussian mixture models. To this end, an\nFSC approach is illustrated using mixtures of multivariate t-distributions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 21:19:03 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1709.08625", "submitter": "Alexander Litvinenko", "authors": "Alexander Litvinenko", "title": "HLIBCov: Parallel Hierarchical Matrix Approximation of Large Covariance\n  Matrices and Likelihoods with Applications in Parameter Identification", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide more technical details about the HLIBCov package, which is using\nparallel hierarchical ($\\H$-) matrices to identify unknown parameters of the\ncovariance function (variance, smoothness, and covariance length). These\nparameters are estimated by maximizing the joint Gaussian log-likelihood\nfunction. The HLIBCov package approximates large dense inhomogeneous covariance\nmatrices with a log-linear computational cost and storage requirement. We\nexplain how to compute the Cholesky factorization, determinant, inverse and\nquadratic form in the H-matrix format. To demonstrate the numerical\nperformance, we identify three unknown parameters in an example with 2,000,000\nlocations on a PC-desktop.\n", "versions": [{"version": "v1", "created": "Sun, 24 Sep 2017 15:52:02 GMT"}, {"version": "v2", "created": "Sun, 18 Feb 2018 16:40:47 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 12:52:17 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Litvinenko", "Alexander", ""]]}, {"id": "1709.08626", "submitter": "Bruno Sudret", "authors": "E. Torre, S. Marelli, P. Embrechts, B. Sudret", "title": "A general framework for data-driven uncertainty quantification under\n  complex input dependencies using vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-012", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systems subject to uncertain inputs produce uncertain responses. Uncertainty\nquantification (UQ) deals with the estimation of statistics of the system\nresponse, given a computational model of the system and a probabilistic model\nof its inputs. In engineering applications it is common to assume that the\ninputs are mutually independent or coupled by a Gaussian or elliptical\ndependence structure (copula). In this paper we overcome such limitations by\nmodelling the dependence structure of multivariate inputs as vine copulas. Vine\ncopulas are models of multivariate dependence built from simpler pair-copulas.\nThe vine representation is flexible enough to capture complex dependencies.\nThis paper formalises the framework needed to build vine copula models of\nmultivariate inputs and to combine them with virtually any UQ method. The\nframework allows for a fully automated, data-driven inference of the\nprobabilistic input model on available input data. The procedure is exemplified\non two finite element models of truss structures, both subject to inputs with\nnon-Gaussian dependence structures. For each case, we analyse the moments of\nthe model response (using polynomial chaos expansions), and perform a\nstructural reliability analysis to calculate the probability of failure of the\nsystem (using the first order reliability method and importance sampling).\nReference solutions are obtained by Monte Carlo simulation. The results show\nthat, while the Gaussian assumption yields biased statistics, the vine copula\nrepresentation achieves significantly more precise estimates, even when its\nstructure needs to be fully inferred from a limited amount of observations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 08:17:21 GMT"}, {"version": "v2", "created": "Fri, 10 Aug 2018 12:38:20 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Torre", "E.", ""], ["Marelli", "S.", ""], ["Embrechts", "P.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.08720", "submitter": "Richard Minkah", "authors": "Richard Minkah and Tertius de Wet and Kwabena Doku-Amponsah", "title": "On Extreme Value Index Estimation under Random Censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme value analysis in the presence of censoring is receiving much\nattention as it has applications in many disciplines, including survival and\nreliability studies. Estimation of extreme value index (EVI) is of primary\nimportance as it is a critical parameter needed in estimating extreme events\nsuch as quantiles and exceedance probabilities. In this paper, we review\nseveral estimators of the extreme value index when data is subject to random\ncensoring. In addition, four estimators are proposed, one based on the\nexponential regression approximation of log spacings, one based on a Zipf\nestimator and two based on variants of the moment estimator. The proposed\nestimators and the existing ones are compared under the same simulation\nconditions. The performance measures for the estimators include confidence\ninterval length and coverage probability. The simulation results show that no\nestimator is universally the best as the estimators depend on the size of the\nEVI parameter, percentage of censoring in the right tail and the underlying\ndistribution. However, certain estimators such as the proposed reduced-bias\nestimator and the adapted moment estimator are found to perform well across\nmost scenarios. Moreover, we present a bootstrap algorithm for obtaining\nsamples for extreme value analysis in the context of censoring. Some of the\nestimators that performed well in the simulation study are illustrated using a\npractical dataset from medical research\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 20:52:42 GMT"}, {"version": "v2", "created": "Sun, 1 Oct 2017 08:49:56 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Minkah", "Richard", ""], ["de Wet", "Tertius", ""], ["Doku-Amponsah", "Kwabena", ""]]}, {"id": "1709.08723", "submitter": "Richard Minkah", "authors": "Richard Minkah, Tertius de Wet, Ezekiel Nii Noi Nortey", "title": "A Simulation Comparison of Estimators of Conditional Extreme Value Index\n  under Right Random Censoring", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In extreme value analysis, the extreme value index plays a vital role as it\ndetermines the tail heaviness of the underlying distribution and is the primary\nparameter required for the estimation of other extreme events. In this paper,\nwe review the estimation of the extreme value index when observations are\nsubject to right random censoring and the presence of covariate information. In\naddition, we propose some estimators of the extreme value index, including a\nmaximum likelihood estimator from a perturbed Pareto distribution. The existing\nestimators and the proposed ones are compared through a simulation study under\nidentical conditions. The results show that the performance of the estimators\ndepend on the percentage of censoring, the underlying distribution, the size of\nextreme value index and the number of top order statistics. Overall, we found\nthe proposed estimator from the perturbed Pareto distribution to be robust to\ncensoring, size of the extreme value index and the number of top order\nstatistics.\n", "versions": [{"version": "v1", "created": "Mon, 25 Sep 2017 20:54:35 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Minkah", "Richard", ""], ["de Wet", "Tertius", ""], ["Nortey", "Ezekiel Nii Noi", ""]]}, {"id": "1709.08862", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Antonietta Mira, Jukka-Pekka Onnela", "title": "Bayesian Inference of Spreading Processes on Networks", "comments": null, "journal-ref": null, "doi": "10.1098/rspa.2018.0129", "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infectious diseases are studied to understand their spreading mechanisms, to\nevaluate control strategies and to predict the risk and course of future\noutbreaks. Because people only interact with a small number of individuals, and\nbecause the structure of these interactions matters for spreading processes,\nthe pairwise relationships between individuals in a population can be usefully\nrepresented by a network. Although the underlying processes of transmission are\ndifferent, the network approach can be used to study the spread of pathogens in\na contact network or the spread of rumors in an online social network. We study\nsimulated simple and complex epidemics on synthetic networks and on two\nempirical networks, a social / contact network in an Indian village and an\nonline social network in the U.S. Our goal is to learn simultaneously about the\nspreading process parameters and the source node (first infected node) of the\nepidemic, given a fixed and known network structure, and observations about\nstate of nodes at several points in time. Our inference scheme is based on\napproximate Bayesian computation (ABC), an inference technique for complex\nmodels with likelihood functions that are either expensive to evaluate or\nanalytically intractable. ABC enables us to adopt a Bayesian approach to the\nproblem despite the posterior distribution being very complex. Our method is\nagnostic about the topology of the network and the nature of the spreading\nprocess. It generally performs well and, somewhat counter-intuitively, the\ninference problem appears to be easier on more heterogeneous network\ntopologies, which enhances its future applicability to real-world settings\nwhere few networks have homogeneous topologies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 07:00:46 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 21:35:41 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 13:55:08 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Mira", "Antonietta", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1709.09216", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Ryan P. Adams, and Tamara Broderick", "title": "PASS-GLM: polynomial approximate sufficient statistics for scalable\n  Bayesian GLM inference", "comments": "In Proceedings of the 31st Annual Conference on Neural Information\n  Processing Systems (NIPS 2017). v3: corrected typos in Appendix A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) -- such as logistic regression, Poisson\nregression, and robust regression -- provide interpretable models for diverse\ndata types. Probabilistic approaches, particularly Bayesian ones, allow\ncoherent estimates of uncertainty, incorporation of prior information, and\nsharing of power across experiments via hierarchical models. In practice,\nhowever, the approximate Bayesian methods necessary for inference have either\nfailed to scale to large data sets or failed to provide theoretical guarantees\non the quality of inference. We propose a new approach based on constructing\npolynomial approximate sufficient statistics for GLMs (PASS-GLM). We\ndemonstrate that our method admits a simple algorithm as well as trivial\nstreaming and distributed extensions that do not compound error across\ncomputations. We provide theoretical guarantees on the quality of point (MAP)\nestimates, the approximate posterior, and posterior mean and uncertainty\nestimates. We validate our approach empirically in the case of logistic\nregression using a quadratic approximation and show competitive performance\nwith stochastic gradient descent, MCMC, and the Laplace approximation in terms\nof speed and multiple measures of accuracy -- including on an advertising data\nset with 40 million data points and 20,000 covariates.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 18:48:13 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 15:11:11 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 21:09:06 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Adams", "Ryan P.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1709.09280", "submitter": "Yasuhiro Omori", "authors": "Naoki Awaya and Yasuhiro Omori", "title": "Particle rolling MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient simulation-based methodology is proposed for the rolling window\nestimation of state space models, called particle rolling Markov chain Monte\nCarlo (MCMC) with double block sampling. In our method, which is based on\nSequential Monte Carlo (SMC), particles are sequentially updated to approximate\nthe posterior distribution for each window by learning new information and\ndiscarding old information from observations. Th particles are refreshed with\nan MCMC algorithm when the importance weights degenerate. To avoid degeneracy,\nwhich is crucial for reducing the computation time, we introduce a block\nsampling scheme and generate multiple candidates by the algorithm based on the\nconditional SMC. The theoretical discussion shows that the proposed methodology\nwith a nested structure is expressed as SMC sampling for the augmented space to\nprovide the justification. The computational performance is evaluated in\nillustrative examples, showing that the posterior distributions of the model\nparameters are accurately estimated. The proofs and additional discussions\n(algorithms and experimental results) are provided in the Supplementary\nMaterial.\n", "versions": [{"version": "v1", "created": "Tue, 26 Sep 2017 22:57:40 GMT"}, {"version": "v2", "created": "Thu, 15 Mar 2018 07:14:05 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 07:02:55 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2019 00:26:52 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Awaya", "Naoki", ""], ["Omori", "Yasuhiro", ""]]}, {"id": "1709.09382", "submitter": "Bruno Sudret", "authors": "C. Lataniotis, S. Marelli and B. Sudret", "title": "The Gaussian process modelling module in UQLab", "comments": null, "journal-ref": "Soft Comput. Civil Eng., 2, 91-116 (2018)", "doi": null, "report-no": "RSUQ-2017-013", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Gaussian process (GP) modelling module developed within the\nUQLab software framework. The novel design of the GP-module aims at providing\nseamless integration of GP modelling into any uncertainty quantification\nworkflow, as well as a stand-alone surrogate modelling tool. We first briefly\npresent the key mathematical tools at the basis of GP modelling (a.k.a.\nKriging), as well as the associated theoretical and computational framework. We\nthen provide an extensive overview of the available features of the software\nand demonstrate its flexibility and user-friendliness. Finally, we showcase the\nusage and the performance of the software on several applications borrowed from\ndifferent fields of engineering. These include a basic surrogate of a\nwell-known analytical benchmark function, a hierarchical Kriging example\napplied to wind turbine aero-servo-elastic simulations and a more complex\ngeotechnical example that requires a non-stationary, user-defined correlation\nfunction. The GP-module, like the rest of the scientific code that is shipped\nwith UQLab, is open source (BSD license).\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 08:21:13 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 15:01:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Lataniotis", "C.", ""], ["Marelli", "S.", ""], ["Sudret", "B.", ""]]}, {"id": "1709.09763", "submitter": "Jonas Latz", "authors": "Jonas Latz, Iason Papaioannou, Elisabeth Ullmann", "title": "Multilevel Sequential${}^2$ Monte Carlo for Bayesian Inverse Problems", "comments": null, "journal-ref": "J. Comput. Phys. 368 (2018) 154-178", "doi": "10.1016/j.jcp.2018.04.014", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of parameters in mathematical models using noisy\nobservations is a common task in uncertainty quantification. We employ the\nframework of Bayesian inversion: we combine monitoring and observational data\nwith prior information to estimate the posterior distribution of a parameter.\nSpecifically, we are interested in the distribution of a diffusion coefficient\nof an elliptic PDE. In this setting, the sample space is high-dimensional, and\neach sample of the PDE solution is expensive. To address these issues we\npropose and analyse a novel Sequential Monte Carlo (SMC) sampler for the\napproximation of the posterior distribution. Classical, single-level SMC\nconstructs a sequence of measures, starting with the prior distribution, and\nfinishing with the posterior distribution. The intermediate measures arise from\na tempering of the likelihood, or, equivalently, a rescaling of the noise. The\nresolution of the PDE discretisation is fixed. In contrast, our estimator\nemploys a hierarchy of PDE discretisations to decrease the computational cost.\nWe construct a sequence of intermediate measures by decreasing the temperature\nor by increasing the discretisation level at the same time. This idea builds on\nand generalises the multi-resolution sampler proposed in [P.S. Koutsourelakis,\nJ. Comput. Phys., 228 (2009), pp. 6184-6211] where a bridging scheme is used to\ntransfer samples from coarse to fine discretisation levels. Importantly, our\nchoice between tempering and bridging is fully adaptive. We present numerical\nexperiments in 2D space, comparing our estimator to single-level SMC and the\nmulti-resolution sampler.\n", "versions": [{"version": "v1", "created": "Wed, 27 Sep 2017 23:50:08 GMT"}, {"version": "v2", "created": "Sun, 6 May 2018 08:57:04 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Latz", "Jonas", ""], ["Papaioannou", "Iason", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "1709.09952", "submitter": "Nicholas Clark", "authors": "Nicholas J. Clark and Philip M. Dixon", "title": "An Extended Laplace Approximation Method for Bayesian Inference of\n  Self-Exciting Spatial-Temporal Models of Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-Exciting models are statistical models of count data where the\nprobability of an event occurring is influenced by the history of the process.\nIn particular, self-exciting spatio-temporal models allow for spatial\ndependence as well as temporal self-excitation. For large spatial or temporal\nregions, however, the model leads to an intractable likelihood. An increasingly\ncommon method for dealing with large spatio-temporal models is by using Laplace\napproximations (LA). This method is convenient as it can easily be applied and\nis quickly implemented. However, as we will demonstrate in this manuscript,\nwhen applied to self-exciting Poisson spatial-temporal models, Laplace\nApproximations result in a significant bias in estimating some parameters. Due\nto this bias, we propose using up to sixth-order corrections to the LA for\nfitting these models. We will demonstrate how to do this in a Bayesian setting\nfor Self-Exciting Spatio-Temporal models. We will further show there is a\nlimited parameter space where the extended LA method still has bias. In these\nuncommon instances we will demonstrate how a more computationally intensive\nfully Bayesian approach using the Stan software program is possible in those\nrare instances. The performance of the extended LA method is illustrated with\nboth simulation and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 28 Sep 2017 13:39:17 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Clark", "Nicholas J.", ""], ["Dixon", "Philip M.", ""]]}, {"id": "1709.10261", "submitter": "Claudio Agostinelli", "authors": "Marina Valdora, Claudio Agostinelli and Victor J. Yohai", "title": "Robust Estimation in High Dimensional Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models are routinely used in data analysis. The classical\nprocedures for estimation are based on Maximum Likelihood and it is well known\nthat the presence of outliers can have a large impact on this estimator. Robust\nprocedures are presented in the literature but they need a robust initial\nestimate in order to be computed. This is especially important for robust\nprocedures with non convex loss function such as redescending M-estimators.\nSubsampling techniques are often used to determine a robust initial estimate;\nhowever when the number of unknown parameters is large the number of subsamples\nneeded in order to have a high probability of having one subsample free of\noutliers become infeasible. Furthermore the subsampling procedure provides a\nnon deterministic starting point. Based on ideas in Pena and Yohai (1999), we\nintroduce a deterministic robust initial estimate for M-estimators based on\ntransformations Valdora and Yohai (2014) for which we also develop an\niteratively reweighted least squares algorithm. The new methods are studied by\nMonte Carlo experiments.\n", "versions": [{"version": "v1", "created": "Fri, 29 Sep 2017 07:27:21 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Valdora", "Marina", ""], ["Agostinelli", "Claudio", ""], ["Yohai", "Victor J.", ""]]}]