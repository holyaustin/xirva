[{"id": "1312.0424", "submitter": "Rodrigo S. Targino", "authors": "Rodrigo S. Targino, Gareth W. Peters, Georgy Sofronov, Pavel V.\n  Shevchenko", "title": "Optimal insurance purchase strategies via optimal multiple stopping\n  times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a class of insurance products where the policy holder\nhas the option to insure $k$ of its annual Operational Risk losses in a horizon\nof $T$ years. This involves a choice of $k$ out of $T$ years in which to apply\nthe insurance policy coverage by making claims against losses in the given\nyear. The insurance product structure presented can accommodate any kind of\nannual mitigation, but we present three basic generic insurance policy\nstructures that can be combined to create more complex types of coverage.\nFollowing the Loss Distributional Approach (LDA) with Poisson distributed\nannual loss frequencies and Inverse-Gaussian loss severities we are able to\ncharacterize in closed form analytical expressions for the multiple optimal\ndecision strategy that minimizes the expected Operational Risk loss over the\nnext $T$ years. For the cases where the combination of insurance policies and\nLDA model does not lead to closed form expressions for the multiple optimal\ndecision rules, we also develop a principled class of closed form\napproximations to the optimal decision rule. These approximations are developed\nbased on a class of orthogonal Askey polynomial series basis expansion\nrepresentations of the annual loss compound process distribution and functions\nof this annual loss.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 11:32:55 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Targino", "Rodrigo S.", ""], ["Peters", "Gareth W.", ""], ["Sofronov", "Georgy", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1312.0518", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang and Paul D. McNicholas", "title": "Families of Parsimonious Finite Mixtures of Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models offer a flexible framework for\ninvestigating heterogeneity in data with functional dependencies. These models\ncan be conveniently used for unsupervised learning on data with clear\nregression relationships. We extend such models by imposing an\neigen-decomposition on the multivariate error covariance matrix. By\nconstraining parts of this decomposition, we obtain families of parsimonious\nmixtures of regressions and mixtures of regressions with concomitant variables.\nThese families of models account for correlations between multiple responses.\nAn expectation-maximization algorithm is presented for parameter estimation and\nperformance is illustrated on simulated and real data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 17:08:03 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1312.0538", "submitter": "Pietro Coretto", "authors": "Pietro Coretto and Francesco Giordano", "title": "Nonparametric estimation of the dynamic range of music signals", "comments": null, "journal-ref": "2017, Australian & New Zealand Journal of Statistics, Vol. 59(4),\n  pp. 389-412", "doi": "10.1111/anzs.12217", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic range is an important parameter which measures the spread of\nsound power, and for music signals it is a measure of recording quality. There\nare various descriptive measures of sound power, none of which has strong\nstatistical foundations. We start from a nonparametric model for sound waves\nwhere an additive stochastic term has the role to catch transient energy. This\ncomponent is recovered by a simple rate-optimal kernel estimator that requires\na single data-driven tuning. The distribution of its variance is approximated\nby a consistent random subsampling method that is able to cope with the massive\nsize of the typical dataset. Based on the latter, we propose a statistic, and\nan estimation method that is able to represent the dynamic range concept\nconsistently. The behavior of the statistic is assessed based on a large\nnumerical experiment where we simulate dynamic compression on a selection of\nreal music signals. Application of the method to real data also shows how the\nproposed method can predict subjective experts' opinions about the hifi quality\nof a recording.\n", "versions": [{"version": "v1", "created": "Mon, 2 Dec 2013 18:24:28 GMT"}, {"version": "v2", "created": "Mon, 5 May 2014 15:37:58 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 17:10:17 GMT"}, {"version": "v4", "created": "Wed, 5 Nov 2014 09:57:57 GMT"}, {"version": "v5", "created": "Sat, 4 Apr 2015 12:47:31 GMT"}, {"version": "v6", "created": "Wed, 2 Sep 2015 08:22:14 GMT"}, {"version": "v7", "created": "Thu, 6 Oct 2016 15:48:58 GMT"}, {"version": "v8", "created": "Wed, 14 Feb 2018 07:53:40 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Coretto", "Pietro", ""], ["Giordano", "Francesco", ""]]}, {"id": "1312.0781", "submitter": "Fredrik Lindsten", "authors": "Emre \\\"Ozkan, Fredrik Lindsten, Carsten Fritsche, Fredrik Gustafsson", "title": "Recursive maximum likelihood identification of jump Markov nonlinear\n  systems", "comments": "Submitted to the IEEE Transactions on Signal Processing on October\n  14, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we present an online method for joint state and\nparameter estimation in jump Markov non-linear systems (JMNLS). State inference\nis enabled via the use of particle filters which makes the method applicable to\na wide range of non-linear models. To exploit the inherent structure of JMNLS,\nwe design a Rao-Blackwellized particle filter (RBPF) where the discrete mode is\nmarginalized out analytically. This results in an efficient implementation of\nthe algorithm and reduces the estimation error variance. The proposed RBPF is\nthen used to compute, recursively in time, smoothed estimates of complete data\nsufficient statistics. Together with the online expectation maximization\nalgorithm, this enables recursive identification of unknown model parameters.\nThe performance of the method is illustrated in simulations and on a\nlocalization problem in wireless networks using real data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 11:41:25 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["\u00d6zkan", "Emre", ""], ["Lindsten", "Fredrik", ""], ["Fritsche", "Carsten", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1312.1254", "submitter": "Yangyang Xu", "authors": "Yangyang Xu, Ruru Hao, Wotao Yin, Zhixun Su", "title": "Parallel matrix factorization for low-rank tensor completion", "comments": "25 pages, 12 figures", "journal-ref": "Inverse Problems and Imaging. Volume 9, No.2, 601-624, 2015", "doi": "10.3934/ipi.2015.9.601", "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order low-rank tensors naturally arise in many applications including\nhyperspectral data recovery, video inpainting, seismic data recon- struction,\nand so on. We propose a new model to recover a low-rank tensor by\nsimultaneously performing low-rank matrix factorizations to the all-mode ma-\ntricizations of the underlying tensor. An alternating minimization algorithm is\napplied to solve the model, along with two adaptive rank-adjusting strategies\nwhen the exact rank is not known.\n  Phase transition plots reveal that our algorithm can recover a variety of\nsynthetic low-rank tensors from significantly fewer samples than the compared\nmethods, which include a matrix completion method applied to tensor recovery\nand two state-of-the-art tensor completion methods. Further tests on real-\nworld data show similar advantages. Although our model is non-convex, our\nalgorithm performs consistently throughout the tests and give better results\nthan the compared methods, some of which are based on convex models. In\naddition, the global convergence of our algorithm can be established in the\nsense that the gradient of Lagrangian function converges to zero.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 17:36:49 GMT"}, {"version": "v2", "created": "Tue, 24 Mar 2015 21:10:10 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Xu", "Yangyang", ""], ["Hao", "Ruru", ""], ["Yin", "Wotao", ""], ["Su", "Zhixun", ""]]}, {"id": "1312.1476", "submitter": "Daniel Simpson", "authors": "Daniel P. Simpson, Ian W. Turner, Christopher M. Strickland, and\n  Anthony N. Pettitt", "title": "Scalable iterative methods for sampling from massive Gaussian random\n  vectors", "comments": "17 Pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from Gaussian Markov random fields (GMRFs), that is multivariate\nGaussian ran- dom vectors that are parameterised by the inverse of their\ncovariance matrix, is a fundamental problem in computational statistics. In\nthis paper, we show how we can exploit arbitrarily accu- rate approximations to\na GMRF to speed up Krylov subspace sampling methods. We also show that these\nmethods can be used when computing the normalising constant of a large\nmultivariate Gaussian distribution, which is needed for both any\nlikelihood-based inference method. The method we derive is also applicable to\nother structured Gaussian random vectors and, in particu- lar, we show that\nwhen the precision matrix is a perturbation of a (block) circulant matrix, it\nis still possible to derive O(n log n) sampling schemes.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 09:19:05 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Simpson", "Daniel P.", ""], ["Turner", "Ian W.", ""], ["Strickland", "Christopher M.", ""], ["Pettitt", "Anthony N.", ""]]}, {"id": "1312.1881", "submitter": "Christian Franzke", "authors": "Daniel Peavoy, Christian L. E. Franzke, Gareth O. Roberts", "title": "Systematic Physics Constrained Parameter Estimation of Stochastic\n  Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A systematic Bayesian framework is developed for physics constrained\nparameter inference ofstochastic differential equations (SDE) from partial\nobservations. The physical constraints arederived for stochastic climate models\nbut are applicable for many fluid systems. A condition isderived for global\nstability of stochastic climate models based on energy conservation.\nStochasticclimate models are globally stable when a quadratic form, which is\nrelated to the cubic nonlinearoperator, is negative definite. A new algorithm\nfor the efficient sampling of such negative definite matrices is developed and\nalso for imputing unobserved data which improve the accuracy of theparameter\nestimates. The performance of this framework is evaluated on two conceptual\nclimatemodels.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 15:07:56 GMT"}, {"version": "v2", "created": "Mon, 2 Jun 2014 14:24:19 GMT"}, {"version": "v3", "created": "Mon, 11 Aug 2014 18:41:41 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Peavoy", "Daniel", ""], ["Franzke", "Christian L. E.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1312.1895", "submitter": "Matthew Pratola", "authors": "M.T. Pratola", "title": "Efficient Metropolis-Hastings Proposal Mechanisms for Bayesian\n  Regression Tree Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian regression trees are flexible non-parametric models that are well\nsuited to many modern statistical regression problems. Many such tree models\nhave been proposed, from the simple single- tree model to more complex tree\nensembles. Their non-parametric formulation allows for effective and efficient\nmodeling of datasets exhibiting complex non-linear relationships between the\nmodel pre- dictors and observations. However, the mixing behavior of the Markov\nChain Monte Carlo (MCMC) sampler is sometimes poor. This is because the\nproposals in the sampler are typically local alterations of the tree structure,\nsuch as the birth/death of leaf nodes, which does not allow for efficient\ntraversal of the model space. This poor mixing can lead to inferential\nproblems, such as under-representing uncertainty. In this paper, we develop\nnovel proposal mechanisms for efficient sampling. The first is a rule\nperturbation proposal while the second we call tree rotation. The perturbation\nproposal can be seen as an efficient variation of the change proposal found in\nexisting literature. The novel tree rotation proposal is simple to implement as\nit only requires local changes to the regression tree structure, yet it\nefficiently traverses disparate regions of the model space along contours of\nequal probability. When combined with the classical birth/death proposal, the\nresulting MCMC sampler exhibits good acceptance rates and properly represents\nmodel uncertainty in the posterior samples. We implement this sampling\nalgorithm in the Bayesian Additive Regression Tree (BART) model and demonstrate\nits effectiveness on a prediction problem from computer experiments and a test\nfunction where structural tree variability is needed to fully explore the\nposterior.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 15:38:10 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Pratola", "M. T.", ""]]}, {"id": "1312.1903", "submitter": "Helen Ogden", "authors": "Helen Ogden", "title": "A sequential reduction method for inference in generalized linear mixed\n  models", "comments": "17 pages, 3 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood for the parameters of a generalized linear mixed model\ninvolves an integral which may be of very high dimension. Because of this\nintractability, many approximations to the likelihood have been proposed, but\nall can fail when the model is sparse, in that there is only a small amount of\ninformation available on each random effect. The sequential reduction method\ndescribed in this paper exploits the dependence structure of the posterior\ndistribution of the random effects to reduce substantially the cost of finding\nan accurate approximation to the likelihood in models with sparse structure.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 16:06:38 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 08:38:58 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Ogden", "Helen", ""]]}, {"id": "1312.2372", "submitter": "Ba Tuong Vo Prof", "authors": "B.-N. Vo, B.-T. Vo, D. Phung", "title": "Labeled Random Finite Sets and the Bayes Multi-Target Tracking Filter", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2014.2364014", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an efficient numerical implementation of the $\\delta$-Generalized\nLabeled Multi-Bernoulli multi-target tracking filter. Each iteration of this\nfilter involves an update operation and a prediction operation, both of which\nresult in weighted sums of multi-target exponentials with intractably large\nnumber of terms. To truncate these sums, the ranked assignment and K-th\nshortest path algorithms are used in the update and prediction, respectively,\nto determine the most significant terms without exhaustively computing all of\nthe terms. In addition, using tools derived from the same framework, such as\nprobability hypothesis density filtering, we present inexpensive look-ahead\nstrategies to reduce the number of computations. Characterization of the\n$L_{1}$-error in the multi-target density arising from the truncation is\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 10:31:47 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 09:46:22 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Vo", "B. -N.", ""], ["Vo", "B. -T.", ""], ["Phung", "D.", ""]]}, {"id": "1312.2556", "submitter": "John Arul A", "authors": "A. John Arul and Kannan Iyer", "title": "A method for importance sampling through Markov chain Monte Carlo with\n  post sampling variational estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to efficiently integrate truncated probability densities.\nThe method uses Markov chain Monte Carlo method to sample from a probability\ndensity matching the function being integrated. The required normalisation or\nequivalently the result is obtained by constructing a function with known\nintegral, through non-parametric kernel density estimation and variational\nprocedure. The method is demonstrated with numerical case studies. Possible\nenhancements to the method and limitations are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 11:35:14 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Arul", "A. John", ""], ["Iyer", "Kannan", ""]]}, {"id": "1312.3027", "submitter": "Zdravko Botev", "authors": "A. Huang and Z. I. Botev", "title": "Rare-event Probability Estimation via Empirical Likelihood Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore past and recent developments in rare-event probability estimation\nwith a particular focus on a novel Monte Carlo technique Empirical Likelihood\nMaximization (ELM). This is a versatile method that involves sampling from a\nsequence of densities using MCMC and maximizing an empirical likelihood. The\nquantity of interest, the probability of a given rare-event, is estimated by\nsolving a convex optimization program related to likelihood maximization.\nNumerical experiments are performed using this new technique and benchmarks are\ngiven against existing robust algorithms and estimators.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 03:21:53 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Huang", "A.", ""], ["Botev", "Z. I.", ""]]}, {"id": "1312.3078", "submitter": "Bernhard Klar", "authors": "Christian Goldmann, Bernhard Klar and Simos G. Meintanis", "title": "Data Transformations and Goodness-of-Fit Tests for Type-II Right\n  Censored Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest several goodness-of-fit methods which are appropriate with Type-II\nright censored data. Our strategy is to transform the original observations\nfrom a censored sample into an approximately i.i.d. sample of normal variates\nand then perform a standard goodness-of-fit test for normality on the\ntransformed observations. A simulation study with several well known parametric\ndistributions under testing reveals the sampling properties of the methods. We\nalso provide theoretical analysis of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 08:44:33 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Goldmann", "Christian", ""], ["Klar", "Bernhard", ""], ["Meintanis", "Simos G.", ""]]}, {"id": "1312.4605", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David B. Dunson", "title": "Parallelizing MCMC via Weierstrass Sampler", "comments": "The original Algorithm 1 removed. Provided some theoretical\n  justification for refinement sampling (Theorem 2). Added a new algorithm in\n  addition to the rejection sampling for handling dimensionality curse. New\n  simulations and graphs (with new colors and designs). A real data analysis is\n  also provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing scales of statistical problems, subset based\ncommunication-free parallel MCMC methods are a promising future for large scale\nBayesian analysis. In this article, we propose a new Weierstrass sampler for\nparallel MCMC based on independent subsets. The new sampler approximates the\nfull data posterior samples via combining the posterior draws from independent\nsubset MCMC chains, and thus enjoys a higher computational efficiency. We show\nthat the approximation error for the Weierstrass sampler is bounded by some\ntuning parameters and provide suggestions for choice of the values. Simulation\nstudy shows the Weierstrass sampler is very competitive compared to other\nmethods for combining MCMC chains generated for subsets, including averaging\nand kernel smoothing.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 01:43:39 GMT"}, {"version": "v2", "created": "Sun, 25 May 2014 18:46:47 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David B.", ""]]}, {"id": "1312.5002", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko, Grigory Sokolov and Wenyu Du", "title": "Efficient Performance Evaluation of the Generalized Shiryaev--Roberts\n  Detection Procedure in a Multi-Cyclic Setup", "comments": "33 pages, 2 figures, 4 tables, accepted for publication in Applied\n  Stochastic Models in Business and Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a numerical method to evaluate the performance of the emerging\nGeneralized Shiryaev--Roberts (GSR) change-point detection procedure in a\n\"minimax-ish\" multi-cyclic setup where the procedure of choice is applied\nrepetitively (cyclically) and the change is assumed to take place at an unknown\ntime moment in a distant-future stationary regime. Specifically, the proposed\nmethod is based on the integral-equations approach and uses the collocation\ntechnique with the basis functions chosen so as to exploit a certain\nchange-of-measure identity and the GSR detection statistic's unique martingale\nproperty. As a result, the method's accuracy and robustness improve, as does\nits efficiency since using the change-of-measure ploy the Average Run Length\n(ARL) to false alarm and the Stationary Average Detection Delay (STADD) are\ncomputed simultaneously. We show that the method's rate of convergence is\nquadratic and supply a tight upperbound on its error. We conclude with a case\nstudy and confirm experimentally that the proposed method's accuracy and rate\nof convergence are robust with respect to three factors: (a) partition fineness\n(coarse vs. fine), (b) change magnitude (faint vs. contrast), and (c) the level\nof the ARL to false alarm (low vs. high). Since the method is designed not\nrestricted to a particular data distribution or to a specific value of the GSR\ndetection statistic's headstart, this work may help gain greater insight into\nthe characteristics of the GSR procedure and aid a practitioner to design the\nGSR procedure as needed while fully utilizing its potential.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 23:22:23 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Sokolov", "Grigory", ""], ["Du", "Wenyu", ""]]}, {"id": "1312.5458", "submitter": "Kei Hirose", "authors": "Kei Hirose, Sunyong Kim, Yutaka Kano, Miyuki Imada, Manabu Yoshida and\n  Masato Matsuo", "title": "Full information maximum likelihood estimation in factor analysis with a\n  lot of missing values", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of full information maximum likelihood (FIML)\nestimation in a factor analysis model when a majority of the data values are\nmissing. The expectation-maximization (EM) algorithm is often used to find the\nFIML estimates, in which the missing values on observed variables are included\nin complete data. However, the EM algorithm has an extremely high computational\ncost when the number of observations is large and/or plenty of missing values\nare involved. In this paper, we propose a new algorithm that is based on the EM\nalgorithm but that efficiently computes the FIML estimates. A significant\nimprovement in the computational speed is realized by not treating the missing\nvalues on observed variables as a part of complete data. Our algorithm is\napplied to a real data set collected from a Web questionnaire that asks about\nfirst impressions of human; almost $90\\%$ of the data values are missing. When\nthere are many missing data values, it is not clear if the FIML procedure can\nachieve good estimation accuracy even if the number of observations is large.\nIn order to investigate this, we conduct Monte Carlo simulations under a wide\nvariety of sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 09:49:39 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Hirose", "Kei", ""], ["Kim", "Sunyong", ""], ["Kano", "Yutaka", ""], ["Imada", "Miyuki", ""], ["Yoshida", "Manabu", ""], ["Matsuo", "Masato", ""]]}, {"id": "1312.5496", "submitter": "Carles Breto (Martinez)", "authors": "Carles Bret\\'o", "title": "On idiosyncratic stochasticity of financial leverage effects", "comments": "8 pages, 2 figures", "journal-ref": "Statistics & Probability Letters 91 (2014) 20-26", "doi": "10.1016/j.spl.2014.04.003", "report-no": null, "categories": "q-fin.GN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model leverage as stochastic but independent of return shocks and of\nvolatility and perform likelihood-based inference via the recently developed\niterated filtering algorithm using S&P500 data, contributing new evidence to\nthe still slim empirical support for random leverage variation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 12:01:18 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Bret\u00f3", "Carles", ""]]}, {"id": "1312.5638", "submitter": "Farhan Feroz", "authors": "F. Feroz, J. Skilling", "title": "Exploring Multi-Modal Distributions with Nested Sampling", "comments": "Refereed conference proceeding, presented at 32nd International\n  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and\n  Engineering", "journal-ref": "AIP Conference Proceedings, Volume 1553, pp. 106-113 (2013)", "doi": "10.1063/1.4819989", "report-no": null, "categories": "astro-ph.IM physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In performing a Bayesian analysis, two difficult problems often emerge.\nFirst, in estimating the parameters of some model for the data, the resulting\nposterior distribution may be multi-modal or exhibit pronounced (curving)\ndegeneracies. Secondly, in selecting between a set of competing models,\ncalculation of the Bayesian evidence for each model is computationally\nexpensive using existing methods such as thermodynamic integration. Nested\nSampling is a Monte Carlo method targeted at the efficient calculation of the\nevidence, but also produces posterior inferences as a by-product and therefore\nprovides means to carry out parameter estimation as well as model selection.\nThe main challenge in implementing Nested Sampling is to sample from a\nconstrained probability distribution. One possible solution to this problem is\nprovided by the Galilean Monte Carlo (GMC) algorithm. We show results of\napplying Nested Sampling with GMC to some problems which have proven very\ndifficult for standard Markov Chain Monte Carlo (MCMC) and down-hill methods,\ndue to the presence of large number of local minima and/or pronounced (curving)\ndegeneracies between the parameters. We also discuss the use of Nested Sampling\nwith GMC in Bayesian object detection problems, which are inherently\nmulti-modal and require the evaluation of Bayesian evidence for distinguishing\nbetween true and spurious detections.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 17:03:09 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Feroz", "F.", ""], ["Skilling", "J.", ""]]}, {"id": "1312.6414", "submitter": "Atul Mallik", "authors": "Atul Mallik and Moulinath Banerjee and Michael Woodroofe", "title": "Baseline zone estimation in two dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the region on which a non-parametric\nregression function is at its baseline level in two dimensions. The baseline\nlevel typically corresponds to the minimum/maximum of the function and\nestimating such regions or their complements is pertinent to several problems\narising in edge estimation, environmental statistics, fMRI and related fields.\nWe assume the baseline region to be convex and estimate it via fitting a\n`stump' function to approximate $p$-values obtained from tests for deviation of\nthe regression function from its baseline level. The estimates, obtained using\nan algorithm originally developed for constructing convex contours of a\ndensity, are studied in two different sampling settings, one where several\nresponses can be obtained at a number of different covariate-levels\n(dose-response) and the other involving limited number of response values per\ncovariate (standard regression). The shape of the baseline region and the\nsmoothness of the regression function at its boundary play a critical role in\ndetermining the rate of convergence of our estimate: for a regression function\nwhich is `p-regular' at the boundary of the convex baseline region, our\nestimate converges at a rate $N^{2/(4p+3)}$ in the dose-response setting, $N$\nbeing the total budget, and its analogue in the standard regression setting\nconverges at a rate of $N^{1/(2p+2)}$. Extensions to non-convex baseline\nregions are explored as well.\n", "versions": [{"version": "v1", "created": "Sun, 22 Dec 2013 18:35:45 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Mallik", "Atul", ""], ["Banerjee", "Moulinath", ""], ["Woodroofe", "Michael", ""]]}, {"id": "1312.6489", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Klaus Nordhausen and Heike Schuhmacher", "title": "New Algorithms for $M$-Estimation of Multivariate Scatter and Location", "comments": null, "journal-ref": "Journal of Multivariate Analysis 144 (2016), pp. 200-217", "doi": "10.1016/j.jmva.2015.11.009", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new algorithms for $M$-estimators of multivariate scatter and\nlocation and for symmetrized $M$-estimators of multivariate scatter. The new\nalgorithms are considerably faster than currently used fixed-point and related\nalgorithms. The main idea is to utilize a second order Taylor expansion of the\ntarget functional and to devise a partial Newton-Raphson procedure. In\nconnection with symmetrized $M$-estimators we work with incomplete\n$U$-statistics to accelerate our procedures initially.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 09:13:49 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2014 00:01:13 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 18:03:03 GMT"}, {"version": "v4", "created": "Wed, 20 May 2015 14:08:41 GMT"}, {"version": "v5", "created": "Tue, 3 Nov 2015 09:40:59 GMT"}, {"version": "v6", "created": "Thu, 3 Dec 2015 18:10:54 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Duembgen", "Lutz", ""], ["Nordhausen", "Klaus", ""], ["Schuhmacher", "Heike", ""]]}, {"id": "1312.7366", "submitter": "Stanley Chan", "authors": "Stanley H. Chan, Todd Zickler, and Yue M. Lu", "title": "Monte Carlo non local means: Random sampling for large-scale image\n  filtering", "comments": "submitted for publication", "journal-ref": null, "doi": "10.1109/TIP.2014.2327813", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a randomized version of the non-local means (NLM) algorithm for\nlarge-scale image filtering. The new algorithm, called Monte Carlo non-local\nmeans (MCNLM), speeds up the classical NLM by computing a small subset of image\npatch distances, which are randomly selected according to a designed sampling\npattern. We make two contributions. First, we analyze the performance of the\nMCNLM algorithm and show that, for large images or large external image\ndatabases, the random outcomes of MCNLM are tightly concentrated around the\ndeterministic full NLM result. In particular, our error probability bounds show\nthat, at any given sampling ratio, the probability for MCNLM to have a large\ndeviation from the original NLM solution decays exponentially as the size of\nthe image or database grows. Second, we derive explicit formulas for optimal\nsampling patterns that minimize the error probability bound by exploiting\npartial knowledge of the pairwise similarity weights. Numerical experiments\nshow that MCNLM is competitive with other state-of-the-art fast NLM algorithms\nfor single-image denoising. When applied to denoising images using an external\ndatabase containing ten billion patches, MCNLM returns a randomized solution\nthat is within 0.2 dB of the full NLM solution while reducing the runtime by\nthree orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 27 Dec 2013 23:31:42 GMT"}, {"version": "v2", "created": "Tue, 22 Apr 2014 19:41:16 GMT"}, {"version": "v3", "created": "Wed, 14 May 2014 19:35:11 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Chan", "Stanley H.", ""], ["Zickler", "Todd", ""], ["Lu", "Yue M.", ""]]}, {"id": "1312.7479", "submitter": "Scott Schmidler", "authors": "Douglas N. VanDerwerken and Scott C. Schmidler", "title": "Parallel Markov Chain Monte Carlo", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo is an inherently serial algorithm. Although\nlikelihood calculations for individual steps can sometimes be parallelized, the\nserial evolution of the process is widely viewed as incompatible with\nparallelization, offering no speedup for samplers which require large numbers\nof iterations to converge to equilibrium. We provide a methodology for\nparallelizing Markov chain Monte Carlo across large numbers of independent,\nasynchronous processors. Our approach uses a partitioning and weight estimation\nscheme to combine independent simulations run on separate processors into\nrigorous Monte Carlo estimates. The method is originally motivated by sampling\nmultimodal target distributions, where we see an exponential speedup in running\ntime. However we show that the approach is general-purpose and applicable to\nall Markov chain Monte Carlo simulations, and demonstrate speedups proportional\nto the number of available processors on slowly mixing chains with unimodal\ntarget distributions. The approach is simple and easy to implement, and\nsuggests additional directions for further research.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 23:05:45 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["VanDerwerken", "Douglas N.", ""], ["Schmidler", "Scott C.", ""]]}, {"id": "1312.7827", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Surface response analysis and determination of confidence regions for\n  atmospheric CO2: a global warming study for U.S.A. data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the atmospheric CO2 measurements taken in Hawaii between 1959\nand 2008, a quadratic model with interactions was fitted, using 5 attributable\nvariables. Surface response analysis returned the eigenvalues and eigenvectors\nat the critical point, which turns out to be of mixed type, with two positive\neigenvalues, one null, and the rest negative. From these data, it is derived\nthat the confidence regions in two variables are of various types (elliptic,\nhyperbolic, and degenerate). Based on these results we indicate how to\ndetermine two-dimensional confidence regions for statistically-significant\nvariables which are relevant contributors to the atmospheric CO2 emissions.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 19:09:52 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}, {"id": "1312.7867", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu and Chris Tsokos", "title": "Contributors of carbon dioxide in the atmosphere in Europe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Carbon dioxide, along with atmospheric temperature are interacting to cause\nwhat we have defined as global warming. In the present study we develop a\nstatistical model using real data to identify the attributable variables (risk\nfactors) that cause the CO2 emissions in the atmosphere in Europe. Some\nscientists believe that there are more than nineteen attributable variables\nthat cause the CO2 in our atmosphere. However, our study has identified only\nthree individual risk factors and five interactions among the attributable\nvariables that cause almost all the CO2 emissions in the atmosphere in Europe.\nWe rank the risk factors and interactions according to the amount of CO2 they\ngenerate. In addition, we compare the present findings of the European data\nwith a similar study for the Continental United States [1, 2]. For example, in\nthe US, liquid fuels ranks number one, while in Europe is gas fuels. In fact,\nliquid fuels in Europe is the least contributable variable of CO2 in the\natmosphere, and gas fuels ranks seventh.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 20:51:39 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Teodorescu", "Iuliana", ""], ["Tsokos", "Chris", ""]]}]