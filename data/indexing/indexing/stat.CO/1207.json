[{"id": "1207.0105", "submitter": "Ryan Martin", "authors": "Ryan Martin, Duncan Ermini Leaf, Chuanhai Liu", "title": "Optimal inferential models for a Poisson mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference on the mean of a Poisson distribution is a\nfundamentally important problem with modern applications in, e.g., particle\nphysics. The discreteness of the Poisson distribution makes this problem\nsurprisingly challenging, even in the large-sample case. Here we propose a new\napproach, based on the recently developed framework of inferential models\n(IMs). Specifically, we construct optimal, or at least approximately optimal,\nIMs for two important classes of assertions/hypotheses about the Poisson mean.\nFor point assertions, we develop a novel recursive sorting algorithm to\nconstruct this optimal IM. Numerical comparisons of the proposed method to\nexisting methods are given, for both the mean and the more challenging\nmean-plus-background problem.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2012 15:06:23 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Martin", "Ryan", ""], ["Leaf", "Duncan Ermini", ""], ["Liu", "Chuanhai", ""]]}, {"id": "1207.0188", "submitter": "Duy Q. Vu", "authors": "Duy Q. Vu, David R. Hunter, Michael Schweinberger", "title": "Model-based clustering of large networks", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS617 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 1010-1039", "doi": "10.1214/12-AOAS617", "report-no": "IMS-AOAS-AOAS617", "categories": "stat.CO cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a network clustering framework, based on finite mixture models,\nthat can be applied to discrete-valued networks with hundreds of thousands of\nnodes and billions of edge variables. Relative to other recent model-based\nclustering work for networks, we introduce a more flexible modeling framework,\nimprove the variational-approximation estimation algorithm, discuss and\nimplement standard error estimation via a parametric bootstrap approach, and\napply these methods to much larger data sets than those seen elsewhere in the\nliterature. The more flexible framework is achieved through introducing novel\nparameterizations of the model, giving varying degrees of parsimony, using\nexponential family models whose structure may be exploited in various\ntheoretical and algorithmic ways. The algorithms are based on variational\ngeneralized EM algorithms, where the E-steps are augmented by a\nminorization-maximization (MM) idea. The bootstrapped standard error estimates\nare based on an efficient Monte Carlo network simulation idea. Last, we\ndemonstrate the usefulness of the model-based clustering framework by applying\nit to a discrete-valued network with more than 131,000 nodes and 17 billion\nedge variables.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2012 08:06:40 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2012 21:15:28 GMT"}, {"version": "v3", "created": "Tue, 10 Dec 2013 06:46:57 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Vu", "Duy Q.", ""], ["Hunter", "David R.", ""], ["Schweinberger", "Michael", ""]]}, {"id": "1207.0258", "submitter": "Hidemaro Suwa", "authors": "Hidemaro Suwa and Synge Todo", "title": "General Construction of Irreversible Kernel in Markov Chain Monte Carlo", "comments": "16 pages, 8 figures; submitted to the proceedings of The Tenth\n  International Conference on Monte Carlo and Quasi-Monte Carlo Methods in\n  Scientific Computing (MCQMC 2012), which will be published by\n  Springer-Verlag, in a book entitled Monte Carlo and Quasi-Monte Carlo Methods\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP math.NA physics.data-an stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov chain Monte Carlo update method to construct an irreversible\nkernel has been reviewed and extended to general state spaces. The several\nconvergence conditions of the Markov chain were discussed. The alternative\nmethods to the Gibbs sampler and the Metropolis-Hastings algorithm were\nproposed and assessed in some models. The distribution convergence and the\nsampling efficiency are significantly improved in the Potts model, the\nbivariate Gaussian model, and so on. This approach using the irreversible\nkernel can be applied to any Markov chain Monte Carlo sampling and it is\nexpected to improve the efficiency in general.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 00:21:15 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Suwa", "Hidemaro", ""], ["Todo", "Synge", ""]]}, {"id": "1207.0520", "submitter": "Pengfei Zang", "authors": "Richard A. Davis, Pengfei Zang, Tian Zheng", "title": "Sparse Vector Autoregressive Modeling", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vector autoregressive (VAR) model has been widely used for modeling\ntemporal dependence in a multivariate time series. For large (and even\nmoderate) dimensions, the number of AR coefficients can be prohibitively large,\nresulting in noisy estimates, unstable predictions and difficult-to-interpret\ntemporal dependence. To overcome such drawbacks, we propose a 2-stage approach\nfor fitting sparse VAR (sVAR) models in which many of the AR coefficients are\nzero. The first stage selects non-zero AR coefficients based on an estimate of\nthe partial spectral coherence (PSC) together with the use of BIC. The PSC is\nuseful for quantifying the conditional relationship between marginal series in\na multivariate process. A refinement second stage is then applied to further\nreduce the number of parameters. The performance of this 2-stage approach is\nillustrated with simulation results. The 2-stage approach is also applied to\ntwo real data examples: the first is the Google Flu Trends data and the second\nis a time series of concentration levels of air pollutants.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2012 20:36:13 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Davis", "Richard A.", ""], ["Zang", "Pengfei", ""], ["Zheng", "Tian", ""]]}, {"id": "1207.0939", "submitter": "Antonio Punzo", "authors": "Antonio Punzo", "title": "Flexible Mixture Modeling with the Polynomial Gaussian Cluster-Weighted\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the mixture modeling frame, this paper presents the polynomial Gaussian\ncluster-weighted model (CWM). It extends the linear Gaussian CWM, for bivariate\ndata, in a twofold way. Firstly, it allows for possible nonlinear dependencies\nin the mixture components by considering a polynomial regression. Secondly, it\nis not restricted to be used for model-based clustering only being\ncontextualized in the most general model-based classification framework.\nMaximum likelihood parameter estimates are derived using the EM algorithm and\nmodel selection is carried out using the Bayesian information criterion (BIC)\nand the integrated completed likelihood (ICL). The paper also investigates the\nconditions under which the posterior probabilities of component-membership from\na polynomial Gaussian CWM coincide with those of other well-established\nmixture-models which are related to it. With respect to these models, the\npolynomial Gaussian CWM has shown to give excellent clustering and\nclassification results when applied to the artificial and real data considered\nin the paper.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 10:46:14 GMT"}], "update_date": "2012-07-05", "authors_parsed": [["Punzo", "Antonio", ""]]}, {"id": "1207.1396", "submitter": "Mike Klaas", "authors": "Mike Klaas, Nando de Freitas, Arnaud Doucet", "title": "Toward Practical N2 Monte Carlo: the Marginal Particle Filter", "comments": "Appears in Proceedings of the Twenty-First Conference on Uncertainty\n  in Artificial Intelligence (UAI2005)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2005-PG-308-315", "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo techniques are useful for state estimation in\nnon-linear, non-Gaussian dynamic models. These methods allow us to approximate\nthe joint posterior distribution using sequential importance sampling. In this\nframework, the dimension of the target distribution grows with each time step,\nthus it is necessary to introduce some resampling steps to ensure that the\nestimates provided by the algorithm have a reasonable variance. In many\napplications, we are only interested in the marginal filtering distribution\nwhich is defined on a space of fixed dimension. We present a Sequential Monte\nCarlo algorithm called the Marginal Particle Filter which operates directly on\nthe marginal distribution, hence avoiding having to perform importance sampling\non a space of growing dimension. Using this idea, we also derive an improved\nversion of the auxiliary particle filter. We show theoretic and empirical\nresults which demonstrate a reduction in variance over conventional particle\nfiltering, and present techniques for reducing the cost of the marginal\nparticle filter with N particles from O(N2) to O(N logN).\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2012 16:17:01 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Klaas", "Mike", ""], ["de Freitas", "Nando", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1207.1708", "submitter": "Marius Hofert", "authors": "Marius Hofert, Martin Maechler, Alexander J. McNeil", "title": "Estimators for Archimedean copulas in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of known and new parametric estimators for Archimedean\ncopulas is investigated, with special focus on large dimensions and numerical\ndifficulties. In particular, method-of-moments-like estimators based on\npairwise Kendall's tau, a multivariate extension of Blomqvist's beta, minimum\ndistance estimators, the maximum-likelihood estimator, a simulated\nmaximum-likelihood estimator, and a maximum-likelihood estimator based on the\ncopula diagonal are studied. Their performance is compared in a large-scale\nsimulation study both under known and unknown margins (pseudo-observations), in\nsmall and high dimensions, under small and large dependencies, various\ndifferent Archimedean families and sample sizes. High dimensions up to one\nhundred are considered for the first time and computational problems arising\nfrom such large dimensions are addressed in detail. All methods are implemented\nin the open source \\R{} package \\pkg{copula} and can thus be easily accessed\nand studied.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 19:05:59 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 14:59:15 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Hofert", "Marius", ""], ["Maechler", "Martin", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "1207.1727", "submitter": "Paul McNicholas", "authors": "Brian C. Franczak, Ryan P. Browne, Paul D. McNicholas", "title": "Mixtures of Shifted Asymmetric Laplace Distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2013.216", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of shifted asymmetric Laplace distributions is introduced and used\nfor clustering and classification. A variant of the EM algorithm is developed\nfor parameter estimation by exploiting the relationship with the general\ninverse Gaussian distribution. This approach is mathematically elegant and\nrelatively computationally straightforward. Our novel mixture modelling\napproach is demonstrated on both simulated and real data to illustrate\nclustering and classification applications. In these analyses, our mixture of\nshifted asymmetric Laplace distributions performs favourably when compared to\nthe popular Gaussian approach. This work, which marks an important step in the\nnon-Gaussian model-based clustering and classification direction, concludes\nwith discussion as well as suggestions for future work.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2012 20:06:48 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 18:40:24 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2012 21:03:10 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Franczak", "Brian C.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1207.1888", "submitter": "David  Biagioni", "authors": "David J. Biagioni and Ryan Elmore and Wesley Jones", "title": "Keeping greed good: sparse regression under design uncertainty with\n  application to biomass characterization", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the classic measurement error regression scenario\nin which our independent, or design, variables are observed with several\nsources of additive noise. We will show that our motivating example's\nreplicated measurements on both the design and dependent variables may be\nleveraged to enhance a sparse regression algorithm. Specifically, we estimate\nthe variance and use it to scale our design variables. We demonstrate the\nefficacy of scaling from several points of view and validate it empirically\nwith a biomass characterization data set using two of the most widely used\nsparse algorithms: least angle regression (LARS) and the Dantzig selector (DS).\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 17:15:59 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Biagioni", "David J.", ""], ["Elmore", "Ryan", ""], ["Jones", "Wesley", ""]]}, {"id": "1207.1916", "submitter": "Alejandro Frery", "authors": "Eliana S. de Almeida, Antonio C. Medeiros and Alejandro C. Frery", "title": "How good are MatLab, Octave and Scilab for Computational Modelling?", "comments": "Accepted for publication in the Computational and Applied Mathematics\n  journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we test the accuracy of three platforms used in computational\nmodelling: MatLab, Octave and Scilab, running on i386 architecture and three\noperating systems (Windows, Ubuntu and Mac OS). We submitted them to numerical\ntests using standard data sets and using the functions provided by each\nplatform. A Monte Carlo study was conducted in some of the datasets in order to\nverify the stability of the results with respect to small departures from the\noriginal input. We propose a set of operations which include the computation of\nmatrix determinants and eigenvalues, whose results are known. We also used data\nprovided by NIST (National Institute of Standards and Technology), a protocol\nwhich includes the computation of basic univariate statistics (mean, standard\ndeviation and first-lag correlation), linear regression and extremes of\nprobability distributions. The assessment was made comparing the results\ncomputed by the platforms with certified values, that is, known results,\ncomputing the number of correct significant digits.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2012 21:52:03 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["de Almeida", "Eliana S.", ""], ["Medeiros", "Antonio C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.1963", "submitter": "Julien Bect", "authors": "Ling Li (M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques, E3S), Julien Bect (M\\'ethodes d'Analyse Stochastique des Codes\n  et Traitements Num\\'eriques, E3S), Emmanuel Vazquez (M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, E3S)", "title": "Bayesian Subset Simulation: a kriging-based subset simulation algorithm\n  for the estimation of small probabilities of failure", "comments": "11th International Probabilistic Assessment and Management Conference\n  (PSAM11) and The Annual European Safety and Reliability Conference (ESREL\n  2012), Helsinki : Finland (2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of small probabilities of failure from computer simulations is\na classical problem in engineering, and the Subset Simulation algorithm\nproposed by Au & Beck (Prob. Eng. Mech., 2001) has become one of the most\npopular method to solve it. Subset simulation has been shown to provide\nsignificant savings in the number of simulations to achieve a given accuracy of\nestimation, with respect to many other Monte Carlo approaches. The number of\nsimulations remains still quite high however, and this method can be\nimpractical for applications where an expensive-to-evaluate computer model is\ninvolved. We propose a new algorithm, called Bayesian Subset Simulation, that\ntakes the best from the Subset Simulation algorithm and from sequential\nBayesian methods based on kriging (also known as Gaussian process modeling).\nThe performance of this new algorithm is illustrated using a test case from the\nliterature. We are able to report promising results. In addition, we provide a\nnumerical study of the statistical properties of the estimator.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2012 06:42:21 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Li", "Ling", "", "M\u00e9thodes d'Analyse Stochastique des Codes et Traitements\n  Num\u00e9riques, E3S"], ["Bect", "Julien", "", "M\u00e9thodes d'Analyse Stochastique des Codes\n  et Traitements Num\u00e9riques, E3S"], ["Vazquez", "Emmanuel", "", "M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, E3S"]]}, {"id": "1207.2378", "submitter": "Alejandro Frery", "authors": "Renato J. Cintra, Abra\\~ao D. C. Nascimento, Alejandro C. Frery", "title": "Parametric and Nonparametric Tests for Speckled Imagery", "comments": "Accepted for publication in the Patter Analysis and Applications\n  journal", "journal-ref": null, "doi": "10.1007/s10044-011-0249-3", "report-no": null, "categories": "stat.CO cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic aperture radar (SAR) has a pivotal role as a remote imaging method.\nObtained by means of coherent illumination, SAR images are contaminated with\nspeckle noise. The statistical modeling of such contamination is well described\naccording with the multiplicative model and its implied G0 distribution. The\nunderstanding of SAR imagery and scene element identification is an important\nobjective in the field. In particular, reliable image contrast tools are\nsought. Aiming the proposition of new tools for evaluating SAR image contrast,\nwe investigated new methods based on stochastic divergence. We propose several\ndivergence measures specifically tailored for G0 distributed data. We also\nintroduce a nonparametric approach based on the Kolmogorov-Smirnov distance for\nG0 data. We devised and assessed tests based on such measures, and their\nperformances were quantified according to their test sizes and powers. Using\nMonte Carlo simulation, we present a robustness analysis of test statistics and\nof maximum likelihood estimators for several degrees of innovative\ncontamination. It was identified that the proposed tests based on triangular\nand arithmetic-geometric measures outperformed the Kolmogorov-Smirnov\nmethodology.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2012 14:42:45 GMT"}], "update_date": "2012-07-11", "authors_parsed": [["Cintra", "Renato J.", ""], ["Nascimento", "Abra\u00e3o D. C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1207.2611", "submitter": "Demetris Christopoulos", "authors": "Demetris T. Christopoulos", "title": "A simple and numerical stable algorithm for solving the cone projection\n  problem based on a Gram-Schmidt process", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are presenting a simple and numerical stable algorithm for the solution of\nthe cone projection problem which is suitable for relative small data sets and\nfor simulation purposes needed for convexity tests. Not even one pseudo-inverse\nmatrix is computed because of a proper Gram-Schmidt orthonormalization process\nthat is used.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 12:25:48 GMT"}], "update_date": "2012-07-12", "authors_parsed": [["Christopoulos", "Demetris T.", ""]]}, {"id": "1207.2622", "submitter": "Fabio Rapallo", "authors": "Sonja Kuhnt, Fabio Rapallo and Andr\\'e Rehage", "title": "Outlier Detection in Contingency Tables based on Minimal Patterns", "comments": "24 pages; a simulation study has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new technique for the detection of outliers in contingency tables is\nintroduced. Outliers thereby are unexpected cell counts with respect to\nclassical loglinear Poisson models. Subsets of cell counts called minimal\npatterns are defined, corresponding to non-singular design matrices and leading\nto potentially uncontaminated maximum-likelihood estimates of the model\nparameters and thereby the expected cell counts. A criterion to easily produce\nminimal patterns in the two-way case under independence is derived, based on\nthe analysis of the positions of the chosen cells. A simulation study and a\ncouple of real-data examples are presented to illustrate the performances of\nthe newly developed outlier identification algorithm, and to compare it with\nother existing methods.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 12:55:50 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2012 14:12:00 GMT"}], "update_date": "2012-11-15", "authors_parsed": [["Kuhnt", "Sonja", ""], ["Rapallo", "Fabio", ""], ["Rehage", "Andr\u00e9", ""]]}, {"id": "1207.2961", "submitter": "Alejandro Frery", "authors": "A. C. Frery, L. Rivarola-Duarte, V. Carrilho Le\\~ao Ramos, A. Soares\n  Ramos Junior, W. W. Matos Lira", "title": "Stochastic particle packing with specified granulometry and porosity", "comments": null, "journal-ref": "Granular Matter, v. 14, p. 27--36, 2012", "doi": "10.1007/s10035-011-0300-5", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a technique for particle size generation and placement in\narbitrary closed domains. Its main application is the simulation of granular\nmedia described by disks. Particle size generation is based on the statistical\nanalysis of granulometric curves which are used as empirical cumulative\ndistribution functions to sample from mixtures of uniform distributions. The\ndesired porosity is attained by selecting a certain number of particles, and\ntheir placement is performed by a stochastic point process. We present an\napplication analyzing different types of sand and clay, where we model the\ngrain size with the gamma, lognormal, Weibull and hyperbolic distributions. The\nparameters from the resulting best fit are used to generate samples from the\ntheoretical distribution, which are used for filling a finite-size area with\nnon-overlapping disks deployed by a Simple Sequential Inhibition stochastic\npoint process. Such filled areas are relevant as plausible inputs for assessing\nDiscrete Element Method and similar techniques.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 13:46:29 GMT"}], "update_date": "2012-07-13", "authors_parsed": [["Frery", "A. C.", ""], ["Rivarola-Duarte", "L.", ""], ["Ramos", "V. Carrilho Le\u00e3o", ""], ["Junior", "A. Soares Ramos", ""], ["Lira", "W. W. Matos", ""]]}, {"id": "1207.3106", "submitter": "Simona Caterina Minotti Dr", "authors": "Salvatore Ingrassia and Simona C. Minotti", "title": "Maximum Likelihood Estimation of Gaussian Cluster Weighted Models and\n  Relationships with Mixtures of Regression", "comments": "The paper has been withdrawn because the results contained therein\n  have been revised in a more general framework and moved into a more extended\n  paper about generalized cwm", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-weighted modeling (CWM) is a mixture approach for modeling the joint\nprobability of a response variable and a set of explanatory variables. The\nparameters are estimated by means of the expectation-maximization algorithm\naccording to the maximum likelihood approach. Under Gaussian assumptions, we\nanalyse the complete-data likelihood function of cluster weighted models.\nFurther, under suitable hypotheses we show that the maximization of the\nlikelihood function of Gaussian cluster weighted models leads to the same\nparameter estimates of finite mixtures of regression and finite mixtures of\nregression with concomitant variables. In this sense, the latter ones can be\nconsidered as nested models of Gaussian cluster weighted models.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2012 21:59:58 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 09:33:04 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Ingrassia", "Salvatore", ""], ["Minotti", "Simona C.", ""]]}, {"id": "1207.3195", "submitter": "Takamitsu Araki", "authors": "Takamitsu Araki, Kazushi Ikeda", "title": "Adaptive Markov Chain Monte Carlo for Auxiliary Variable Method and Its\n  Application to Parallel Tempering", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auxiliary variable methods such as the Parallel Tempering and the cluster\nMonte Carlo methods generate samples that follow a target distribution by using\nproposal and auxiliary distributions. In sampling from complex distributions,\nthese algorithms are highly more efficient than the standard Markov chain Monte\nCarlo methods. However, their performance strongly depends on their parameters\nand determining the parameters is critical. In this paper, we proposed an\nalgorithm for adapting the parameters during drawing samples and proved the\nconvergence theorem of the adaptive algorithm. We applied our algorithm to the\nParallel Tempering. That is, we developed adaptive Parallel Tempering that\ntunes the parameters on the fly. We confirmed the effectiveness of our\nalgorithm through the validation of the adaptive Parallel Tempering, comparing\nsamples from the target distribution by the adaptive Parallel Tempering and\nsamples by conventional algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2012 10:45:17 GMT"}], "update_date": "2012-07-16", "authors_parsed": [["Araki", "Takamitsu", ""], ["Ikeda", "Kazushi", ""]]}, {"id": "1207.4085", "submitter": "Xi Luo", "authors": "Xi Luo, Steven Gee, Vikaas S. Sohal, Dylan S. Small", "title": "A Point-process Response Model for Spike Trains from Single Neurons in\n  Neural Circuits under Optogenetic Stimulation", "comments": "24 pages, 7 figures. R package pro implementing the proposed method\n  is available on CRAN at https://CRAN.R-project.org/package=pro . Published by\n  Statistics in Medicine at\n  http://onlinelibrary.wiley.com/doi/10.1002/sim.6742/full", "journal-ref": "Stat Med. 2016; 35(3): 455-74", "doi": "10.1002/sim.6742", "report-no": null, "categories": "stat.ME q-bio.NC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optogenetics is a new tool to study neuronal circuits that have been\ngenetically modified to allow stimulation by flashes of light. We study\nrecordings from single neurons within neural circuits under optogenetic\nstimulation. The data from these experiments present a statistical challenge of\nmodeling a high frequency point process (neuronal spikes) while the input is\nanother high frequency point process (light flashes). We further develop a\ngeneralized linear model approach to model the relationships between two point\nprocesses, employing additive point-process response functions. The resulting\nmodel, Point-process Responses for Optogenetics (PRO), provides explicit\nnonlinear transformations to link the input point process with the output one.\nSuch response functions may provide important and interpretable scientific\ninsights into the properties of the biophysical process that governs neural\nspiking in response to optogenetic stimulation. We validate and compare the PRO\nmodel using a real dataset and simulations, and our model yields a superior\narea-under-the- curve value as high as 93% for predicting every future spike.\nFor our experiment on the recurrent layer V circuit in the prefrontal cortex,\nthe PRO model provides evidence that neurons integrate their inputs in a\nsophisticated manner. Another use of the model is that it enables understanding\nhow neural circuits are altered under various disease conditions and/or\nexperimental conditions by comparing the PRO parameters.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2012 18:51:17 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 03:48:50 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Luo", "Xi", ""], ["Gee", "Steven", ""], ["Sohal", "Vikaas S.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1207.4149", "submitter": "Firas Hamze", "authors": "Firas Hamze, Nando de Freitas", "title": "From Fields to Trees", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-243-250", "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new MCMC algorithms for computing the posterior distributions and\nexpectations of the unknown variables in undirected graphical models with\nregular structure. For demonstration purposes, we focus on Markov Random Fields\n(MRFs). By partitioning the MRFs into non-overlapping trees, it is possible to\ncompute the posterior distribution of a particular tree exactly by conditioning\non the remaining tree. These exact solutions allow us to construct efficient\nblocked and Rao-Blackwellised MCMC algorithms. We show empirically that tree\nsampling is considerably more efficient than other partitioned sampling schemes\nand the naive Gibbs sampler, even in cases where loopy belief propagation fails\nto converge. We prove that tree sampling exhibits lower variance than the naive\nGibbs sampler and other naive partitioning schemes using the theoretical\nmeasure of maximal correlation. We also construct new information theory tools\nfor comparing different MCMC schemes and show that, under these, tree sampling\nis more efficient.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 14:56:43 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Hamze", "Firas", ""], ["de Freitas", "Nando", ""]]}, {"id": "1207.4159", "submitter": "Bo Wang", "authors": "Bo Wang, D. Titterington", "title": "Convergence and asymptotic normality of variational Bayesian\n  approximations for exponential family models with missing values", "comments": "Appears in Proceedings of the Twentieth Conference on Uncertainty in\n  Artificial Intelligence (UAI2004)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2004-PG-577-584", "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of variational Bayes approximations for exponential\nfamily models with missing values. It is shown that the iterative algorithm for\nobtaining the variational Bayesian estimator converges locally to the true\nvalue with probability 1 as the sample size becomes inde nitely large.\nMoreover, the variational posterior distribution is proved to be asymptotically\nnormal.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2012 15:01:58 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Wang", "Bo", ""], ["Titterington", "D.", ""]]}, {"id": "1207.4417", "submitter": "Jingwei  Liu", "authors": "Jingwei Liu, Meizhi Xu", "title": "Penalty Constraints and Kernelization of M-Estimation Based Fuzzy\n  C-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework of M-estimation based fuzzy C-means clustering (MFCM) algorithm\nis proposed with iterative reweighted least squares (IRLS) algorithm, and\npenalty constraint and kernelization extensions of MFCM algorithms are also\ndeveloped. Introducing penalty information to the object functions of MFCM\nalgorithms, the spatially constrained fuzzy C-means (SFCM) is extended to\npenalty constraints MFCM algorithms(abbr. pMFCM).Substituting the Euclidean\ndistance with kernel method, the MFCM and pMFCM algorithms are extended to\nkernelized MFCM (abbr. KMFCM) and kernelized pMFCM (abbr.pKMFCM) algorithms.\nThe performances of MFCM, pMFCM, KMFCM and pKMFCM algorithms are evaluated in\nthree tasks: pattern recognition on 10 standard data sets from UCI Machine\nLearning databases, noise image segmentation performances on a synthetic image,\na magnetic resonance brain image (MRI), and image segmentation of a standard\nimages from Berkeley Segmentation Dataset and Benchmark. The experimental\nresults demonstrate the effectiveness of our proposed algorithms in pattern\nrecognition and image segmentation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2012 17:20:32 GMT"}, {"version": "v2", "created": "Sat, 19 Jan 2013 10:33:02 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Liu", "Jingwei", ""], ["Xu", "Meizhi", ""]]}, {"id": "1207.4814", "submitter": "Hung Bui", "authors": "Hung Hai Bui and Tuyen N. Huynh and Sebastian Riedel", "title": "Automorphism Groups of Graphical Models and Lifted Variational Inference", "comments": "Extended version of the paper to appear in Statistical Relational AI\n  (StaRAI-12) workshop at UAI '12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.CO stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the theory of group action, we first introduce the concept of the\nautomorphism group of an exponential family or a graphical model, thus\nformalizing the general notion of symmetry of a probabilistic model. This\nautomorphism group provides a precise mathematical framework for lifted\ninference in the general exponential family. Its group action partitions the\nset of random variables and feature functions into equivalent classes (called\norbits) having identical marginals and expectations. Then the inference problem\nis effectively reduced to that of computing marginals or expectations for each\nclass, thus avoiding the need to deal with each individual variable or feature.\nWe demonstrate the usefulness of this general framework in lifting two classes\nof variational approximation for MAP inference: local LP relaxation and local\nLP relaxation with cycle constraints; the latter yields the first lifted\ninference that operate on a bound tighter than local constraints. Initial\nexperimental results demonstrate that lifted MAP inference with cycle\nconstraints achieved the state of the art performance, obtaining much better\nobjective function values than local approximation while remaining relatively\nefficient.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jul 2012 21:30:42 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Bui", "Hung Hai", ""], ["Huynh", "Tuyen N.", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1207.5355", "submitter": "Nicolas Dobigeon", "authors": "Marcelo Pereyra and Nicolas Dobigeon and Hadj Batatia and Jean-Yves\n  Tourneret", "title": "Estimating the granularity coefficient of a Potts-Markov random field\n  within an MCMC algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2013.2249076", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating the Potts parameter B jointly\nwith the unknown parameters of a Bayesian model within a Markov chain Monte\nCarlo (MCMC) algorithm. Standard MCMC methods cannot be applied to this problem\nbecause performing inference on B requires computing the intractable\nnormalizing constant of the Potts model. In the proposed MCMC method the\nestimation of B is conducted using a likelihood-free Metropolis-Hastings\nalgorithm. Experimental results obtained for synthetic data show that\nestimating B jointly with the other unknown parameters leads to estimation\nresults that are as good as those obtained with the actual value of B. On the\nother hand, assuming that the value of B is known can degrade estimation\nperformance significantly if this value is incorrect. To illustrate the\ninterest of this method, the proposed algorithm is successfully applied to real\nbidimensional SAR and tridimensional ultrasound images.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2012 11:13:20 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Pereyra", "Marcelo", ""], ["Dobigeon", "Nicolas", ""], ["Batatia", "Hadj", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1207.5758", "submitter": "Nial Friel", "authors": "Nial Friel", "title": "Bayesian inference for Gibbs random fields using composite likelihoods", "comments": "To appear in the proceedings of the 2012 Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields play an important role in statistics, for example the\nautologistic model is commonly used to model the spatial distribution of binary\nvariables defined on a lattice. However they are complicated to work with due\nto an intractability of the likelihood function. It is therefore natural to\nconsider tractable approximations to the likelihood function. Composite\nlikelihoods offer a principled approach to constructing such approximation. The\ncontribution of this paper is to examine the performance of a collection of\ncomposite likelihood approximations in the context of Bayesian inference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2012 18:02:25 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Friel", "Nial", ""]]}, {"id": "1207.6327", "submitter": "Alexandre Bouchard-C\\^ot\\'e", "authors": "Alexandre Bouchard-C\\^ot\\'e and Michael I. Jordan", "title": "Evolutionary Inference via the Poisson Indel Process", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": "10.1073/pnas.1220450110", "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of the joint statistical inference of phylogenetic\ntrees and multiple sequence alignments from unaligned molecular sequences. This\nproblem is generally formulated in terms of string-valued evolutionary\nprocesses along the branches of a phylogenetic tree. The classical evolutionary\nprocess, the TKF91 model, is a continuous-time Markov chain model comprised of\ninsertion, deletion and substitution events. Unfortunately this model gives\nrise to an intractable computational problem---the computation of the marginal\nlikelihood under the TKF91 model is exponential in the number of taxa. In this\nwork, we present a new stochastic process, the Poisson Indel Process (PIP), in\nwhich the complexity of this computation is reduced to linear. The new model is\nclosely related to the TKF91 model, differing only in its treatment of\ninsertions, but the new model has a global characterization as a Poisson\nprocess on the phylogeny. Standard results for Poisson processes allow key\ncomputations to be decoupled, which yields the favorable computational profile\nof inference under the PIP model. We present illustrative experiments in which\nBayesian inference under the PIP model is compared to separate inference of\nphylogenies and alignments.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 16:52:10 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 16:32:03 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1207.6432", "submitter": "James M. Flegal", "authors": "Charles Doss and James M. Flegal and Galin L. Jones and Ronald C.\n  Neath", "title": "Markov Chain Monte Carlo Estimation of Quantiles", "comments": "35 pages, 1 figure", "journal-ref": "Electronic Journal of Statistics, 2014", "doi": "10.1214/14-EJS957", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider quantile estimation using Markov chain Monte Carlo and establish\nconditions under which the sampling distribution of the Monte Carlo error is\napproximately Normal. Further, we investigate techniques to estimate the\nassociated asymptotic variance, which enables construction of an asymptotically\nvalid interval estimator. Finally, we explore the finite sample properties of\nthese methods through examples and provide some recommendations to\npractitioners.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2012 23:54:56 GMT"}, {"version": "v2", "created": "Fri, 12 Jul 2013 20:32:15 GMT"}, {"version": "v3", "created": "Tue, 30 Sep 2014 22:44:36 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Doss", "Charles", ""], ["Flegal", "James M.", ""], ["Jones", "Galin L.", ""], ["Neath", "Ronald C.", ""]]}, {"id": "1207.6779", "submitter": "Yizao Wang", "authors": "Yves Atchad\\'e and Yizao Wang", "title": "On the Convergence Rates of Some Adaptive Markov Chain Monte Carlo\n  Algorithms", "comments": "Major revision. Significantly shortened", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the mixing time of certain adaptive Markov Chain Monte\nCarlo algorithms. Under some regularity conditions, we show that the\nconvergence rate of Importance Resampling MCMC (IRMCMC) algorithm, measured in\nterms of the total variation distance is $O(n^{-1})$, and by means of an\nexample, we establish that in general, this algorithm does not converge at a\nfaster rate. We also study the Equi-Energy sampler and establish that its\nmixing time is of order $O(n^{-1/2})$.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2012 14:11:40 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 12:12:43 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Atchad\u00e9", "Yves", ""], ["Wang", "Yizao", ""]]}, {"id": "1207.7311", "submitter": "Kinjal Basu", "authors": "M. Z. Anis and Kinjal Basu", "title": "Tests for exponentiality against NBUE alternatives: a Monte Carlo\n  comparison", "comments": null, "journal-ref": null, "doi": "10.1080/00949655.2012.704517", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing of various classes of life distributions has been addressed in the\nliterature for more than 45 years. In this paper, we consider the problem of\ntesting exponentiality (which essentially implies no ageing) against positive\nageing which is captured by the fairly large class of new better than used in\nexpectation (NBUE) distributions. These tests of exponentiality against NBUE\nalternatives are discussed and compared. The empirical size of the tests is\nobtained by simulations. Power comparisons for different popular alternatives\nare done using Monte Carlo simulations. These comparisons are made for both\nsmall and large sample sizes. The paper concludes with a discussion in which\nsuggestions are made regarding the choices of the test when a particular\nalternative is suspected.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2012 16:37:26 GMT"}], "update_date": "2012-08-01", "authors_parsed": [["Anis", "M. Z.", ""], ["Basu", "Kinjal", ""]]}]