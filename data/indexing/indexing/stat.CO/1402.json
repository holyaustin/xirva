[{"id": "1402.0361", "submitter": "Paula Saavedra-Nieves", "authors": "Paula Saavedra-Nieves, Wenceslao Gonz\\'alez-Manteiga and Alberto\n  Rodr\\'iguez-Casal", "title": "A comparative simulation study of data-driven methods for estimating\n  density level sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density level sets are mainly estimated using one of three methodologies:\nplug-in, excess mass, or a hybrid approach. The plug-in methods are based on\nreplacing the unknown density by some nonparametric estimator, usually the\nkernel. Thus, the bandwidth selection is a fundamental problem from a practical\npoint of view. Recently, specific selectors for level sets have been proposed.\nHowever, if some a priori information about the geometry of the level set is\navailable, then excess mass algorithms can be useful. In this case, a density\nestimator is not necessary, and the problem of bandwidth selection can be\navoided. The third methodology is a hybrid of the others. As in the excess mass\nmethod, it assumes a mild geometric restriction on the level set and, like the\nplug-in approach, requires a pilot nonparametric estimator of the density. One\ninteresting open question concerns the practical performance of these methods.\nIn this work, existing methods are reviewed, and two new hybrid algorithms are\nproposed. Their practical behaviour is compared through extensive simulations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 12:33:13 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 12:32:41 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Saavedra-Nieves", "Paula", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Rodr\u00edguez-Casal", "Alberto", ""]]}, {"id": "1402.0432", "submitter": "Kaspar Rufibach", "authors": "Stanislas Hubeaux and Kaspar Rufibach", "title": "SurvRegCensCov: Weibull Regression for a Right-Censored Endpoint with a\n  Censored Covariate", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomarker data is often subject to limits of quantification and/or limits of\ndetection. Statistically, this corresponds to left- or interval-censoring. To\nbe able to associate a censored time-to-event endpoint to a biomarker\ncovariate, the R package SurvRegCensCov provides software for Weibull\nregression for a right-censored endpoint, one interval-censored, and an\narbitrary number of non-censored covariates. Furthermore, the package provides\nfunctions to estimate canonical parameters from censored samples based on\nseveral distributional assumptions, and a function to switch between different\nparametrizations used in R for Weibull regression. We illustrate the new\nsoftware by applying it to assess Prentice's criteria for surrogacy in data\nsimulated from a randomized clinical registration trial.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2014 17:02:26 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 19:58:14 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Hubeaux", "Stanislas", ""], ["Rufibach", "Kaspar", ""]]}, {"id": "1402.0686", "submitter": "Emanuele  Giorgi", "authors": "Emanuele Giorgi and Alexander J. McNeil", "title": "On the Computation of Multivariate Scenario Sets for the Skew-t and\n  Generalized Hyperbolic Families", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the problem of computing multivariate scenarios sets for skewed\ndistributions. Our interest is motivated by the potential use of such sets in\nthe \"stress testing\" of insurance companies and banks whose solvency is\ndependent on changes in a set of financial \"risk factors\". We define\nmultivariate scenario sets based on the notion of half-space depth (HD) and\nalso introduce the notion of expectile depth (ED) where half-spaces are defined\nby expectiles rather than quantiles. We then use the HD and ED functions to\ndefine convex scenario sets that generalize the concepts of quantile and\nexpectile to higher dimensions. In the case of elliptical distributions these\nsets coincide with the regions encompassed by the contours of the density\nfunction. In the context of multivariate skewed distributions, the equivalence\nof depth contours and density contours does not hold in general. We consider\ntwo parametric families that account for skewness and heavy tails: the\ngeneralized hyperbolic and the skew-t distributions. By making use of a\ncanonical form representation, where skewness is completely absorbed by one\ncomponent, we show that the HD contours of these distributions are\n\"near-elliptical\" and, in the case of the skew-Cauchy distribution, we prove\nthat the HD contours are exactly elliptical. We propose a measure of\nmultivariate skewness as a deviation from angular symmetry and show that it can\nexplain the quality of the elliptical approximation for the HD contours.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 10:50:29 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Giorgi", "Emanuele", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "1402.0694", "submitter": "Christopher Nemeth", "authors": "Chris Nemeth, Paul Fearnhead", "title": "Particle Metropolis adjusted Langevin algorithms for state space models", "comments": "Replaced with updated article with new title at arXiv:1412.7299", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle MCMC is a class of algorithms that can be used to analyse\nstate-space models. They use MCMC moves to update the parameters of the models,\nand particle filters to propose values for the path of the state-space model.\nCurrently the default is to use random walk Metropolis to update the parameter\nvalues. We show that it is possible to use information from the output of the\nparticle filter to obtain better proposal distributions for the parameters. In\nparticular it is possible to obtain estimates of the gradient of the log\nposterior from each run of the particle filter, and use these estimates within\na Langevin-type proposal. We propose using the recent computationally efficient\napproach of Nemeth et al. (2013) for obtaining such estimates. We show\nempirically that for a variety of state-space models this proposal is more\nefficient than the standard random walk Metropolis proposal in terms of:\nreducing autocorrelation of the posterior samples, reducing the burn-in time of\nthe MCMC sampler and increasing the squared jump distance between posterior\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2014 11:14:17 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 07:58:27 GMT"}], "update_date": "2014-12-25", "authors_parsed": [["Nemeth", "Chris", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1402.1380", "submitter": "Julien Stoehr", "authors": "Julien Stoehr (I3M), Pierre Pudlo (I3M), Lionel Cucala (I3M)", "title": "Adaptive ABC model choice and geometric summary statistics for hidden\n  Gibbs random fields", "comments": null, "journal-ref": "Statistics and Computing (2015) 25:129-141", "doi": "10.1007/s11222-014-9514-9", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting between different dependency structures of hidden Markov random\nfield can be very challenging, due to the intractable normalizing constant in\nthe likelihood. We answer this question with approximate Bayesian computation\n(ABC) which provides a model choice method in the Bayesian paradigm. This comes\nafter the work of Grelaud et al. (2009) who exhibited sufficient statistics on\ndirectly observed Gibbs random fields. But when the random field is latent, the\nsufficiency falls and we complement the set with geometric summary statistics.\nThe general approach to construct these intuitive statistics relies on a\nclustering analysis of the sites based on the observed colors and plausible\nlatent graphs. The efficiency of ABC model choice based on these statistics is\nevaluated via a local error rate which may be of independent interest. As a\nbyproduct we derived an ABC algorithm that adapts the dimension of the summary\nstatistics to the dataset without distorting the model selection.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 15:41:50 GMT"}, {"version": "v2", "created": "Wed, 16 Jul 2014 16:20:13 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Stoehr", "Julien", "", "I3M"], ["Pudlo", "Pierre", "", "I3M"], ["Cucala", "Lionel", "", "I3M"]]}, {"id": "1402.1472", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss and Dorit Hammerling", "title": "Parallel inference for massive distributed spatial data using low-rank\n  models", "comments": "20 pages; published in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-016-9627-4", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid data growth, statistical analysis of massive datasets often has\nto be carried out in a distributed fashion, either because several datasets\nstored in separate physical locations are all relevant to a given problem, or\nsimply to achieve faster (parallel) computation through a divide-and-conquer\nscheme. In both cases, the challenge is to obtain valid inference that does not\nrequire processing all data at a single central computing node. We show that\nfor a very widely used class of spatial low-rank models, which can be written\nas a linear combination of spatial basis functions plus a fine-scale-variation\ncomponent, parallel spatial inference and prediction for massive distributed\ndata can be carried out exactly, meaning that the results are the same as for a\ntraditional, non-distributed analysis. The communication cost of our\ndistributed algorithms does not depend on the number of data points. After\nextending our results to the spatio-temporal case, we illustrate our\nmethodology by carrying out distributed spatio-temporal particle filtering\ninference on total precipitable water measured by three different satellite\nsensor systems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2014 20:16:30 GMT"}, {"version": "v2", "created": "Wed, 18 Jun 2014 23:44:54 GMT"}, {"version": "v3", "created": "Sat, 8 Nov 2014 00:15:42 GMT"}, {"version": "v4", "created": "Fri, 5 Feb 2016 17:10:49 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Hammerling", "Dorit", ""]]}, {"id": "1402.1694", "submitter": "Patrick Conrad", "authors": "Patrick R. Conrad, Youssef M. Marzouk, Natesh S. Pillai, Aaron Smith", "title": "Accelerating Asymptotically Exact MCMC for Computationally Intensive\n  Models via Local Approximations", "comments": "A major update of the theory and examples", "journal-ref": "Journal of the American Statistical Association, volume 111, issue\n  516, 1591--1607 (2016)", "doi": "10.1080/01621459.2015.1096787", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a new framework for accelerating Markov chain Monte Carlo in\nposterior sampling problems where standard methods are limited by the\ncomputational cost of the likelihood, or of numerical models embedded therein.\nOur approach introduces local approximations of these models into the\nMetropolis-Hastings kernel, borrowing ideas from deterministic approximation\ntheory, optimization, and experimental design. Previous efforts at integrating\napproximate models into inference typically sacrifice either the sampler's\nexactness or efficiency; our work seeks to address these limitations by\nexploiting useful convergence characteristics of local approximations. We prove\nthe ergodicity of our approximate Markov chain, showing that it samples\nasymptotically from the \\emph{exact} posterior distribution of interest. We\ndescribe variations of the algorithm that employ either local polynomial\napproximations or local Gaussian process regressors. Our theoretical results\nreinforce the key observation underlying this paper: when the likelihood has\nsome \\emph{local} regularity, the number of model evaluations per MCMC step can\nbe greatly reduced without biasing the Monte Carlo average. Numerical\nexperiments demonstrate multiple order-of-magnitude reductions in the number of\nforward model evaluations used in representative ODE and PDE inference\nproblems, with both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 16:59:31 GMT"}, {"version": "v2", "created": "Mon, 8 Sep 2014 20:53:57 GMT"}, {"version": "v3", "created": "Wed, 5 Nov 2014 15:20:21 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2015 15:45:09 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Conrad", "Patrick R.", ""], ["Marzouk", "Youssef M.", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1402.1782", "submitter": "James M. Flegal", "authors": "Roberto C. Crackel and James M. Flegal", "title": "Bayesian inference for a flexible class of bivariate beta distributions", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several bivariate beta distributions have been proposed in the literature. In\nparticular, Olkin and Liu (2003) proposed a 3 parameter bivariate beta model,\nwhich Arnold and Ng (2011) extend to 5 and 8 parameter models. The 3 parameter\nmodel allows for only positive correlation, while the latter models can\naccommodate both positive and negative correlation. However, these come at the\nexpense of a density that is mathematically intractable. The focus of this\nresearch is on Bayesian estimation for the 5 and 8 parameter models. Since the\nlikelihood does not exist in closed form, we apply approximate Bayesian\ncomputation, a likelihood free approach. Simulation studies have been carried\nout for the 5 and 8 parameter cases under various priors and tolerance levels.\nWe apply the 5 parameter model to a real data set by allowing the model to\nserve as a prior to correlated proportions of a bivariate beta binomial model.\nResults and comparisons are then discussed.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2014 21:58:38 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 20:34:08 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2015 03:31:12 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Crackel", "Roberto C.", ""], ["Flegal", "James M.", ""]]}, {"id": "1402.2468", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Sampling Plans for Control-Inspection Schemes Under Independent and\n  Dependent Sampling Designs With Applications to Photovoltaics", "comments": null, "journal-ref": "Frontiers in Statistical Quality Control 11 - 2015, pp 287-317", "doi": "10.1007/978-3-319-12355-4_18", "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of produced items at the time of delivery is, in practice,\nusually amended by at least one inspection at later time points. We extend the\nmethodology of acceptance sampling for variables for arbitrary unknown\ndistributions when additional sampling infor- mation is available to such\nsettings. Based on appropriate approximations of the operating characteristic,\nwe derive new acceptance sampling plans that control the overall operating\ncharacteristic. The results cover the case of independent sampling as well as\nthe case of dependent sampling. In particular, we study a modified panel\nsampling design and the case of spatial batch sampling. The latter is advisable\nin photovoltaic field monitoring studies, since it allows to detect and analyze\nlocal clusters of degraded or damaged modules. Some finite sample properties\nare examined by a simulation study, focusing on the accuracy of estimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 12:17:50 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1402.2492", "submitter": "Gareth Peters Dr", "authors": "Alice X.D. Dong, Jennifer S.K. Chan, Gareth W. Peters", "title": "Risk Margin Quantile Function Via Parametric and Non-Parametric Bayesian\n  Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop quantile regression models in order to derive risk margin and to\nevaluate capital in non-life insurance applications. By utilizing the entire\nrange of conditional quantile functions, especially higher quantile levels, we\ndetail how quantile regression is capable of providing an accurate estimation\nof risk margin and an overview of implied capital based on the historical\nvolatility of a general insurers loss portfolio. Two modelling frameworks are\nconsidered based around parametric and nonparametric quantile regression models\nwhich we develop specifically in this insurance setting.\n  In the parametric quantile regression framework, several models including the\nflexible generalized beta distribution family, asymmetric Laplace (AL)\ndistribution and power Pareto distribution are considered under a Bayesian\nregression framework. The Bayesian posterior quantile regression models in each\ncase are studied via Markov chain Monte Carlo (MCMC) sampling strategies.\n  In the nonparametric quantile regression framework, that we contrast to the\nparametric Bayesian models, we adopted an AL distribution as a proxy and\ntogether with the parametric AL model, we expressed the solution as a scale\nmixture of uniform distributions to facilitate implementation. The models are\nextended to adopt dynamic mean, variance and skewness and applied to analyze\ntwo real loss reserve data sets to perform inference and discuss interesting\nfeatures of quantile regression for risk margin calculations.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 14:13:17 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Dong", "Alice X. D.", ""], ["Chan", "Jennifer S. K.", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1402.2676", "submitter": "Parameswaran Raman", "authors": "Hyokun Yun, Parameswaran Raman, S.V.N. Vishwanathan", "title": "Ranking via Robust Binary Classification and Parallel Parameter\n  Estimation in Large-Scale Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RoBiRank, a ranking algorithm that is motivated by observing a\nclose connection between evaluation metrics for learning to rank and loss\nfunctions for robust classification. The algorithm shows a very competitive\nperformance on standard benchmark datasets against other representative\nalgorithms in the literature. On the other hand, in large scale problems where\nexplicit feature vectors and scores are not given, our algorithm can be\nefficiently parallelized across a large number of machines; for a task that\nrequires 386,133 x 49,824,519 pairwise interactions between items to be ranked,\nour algorithm finds solutions that are of dramatically higher quality than that\ncan be found by a state-of-the-art competitor algorithm, given the same amount\nof wall-clock time for computation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:39:54 GMT"}, {"version": "v2", "created": "Fri, 4 Apr 2014 21:08:34 GMT"}, {"version": "v3", "created": "Fri, 11 Apr 2014 06:19:04 GMT"}, {"version": "v4", "created": "Thu, 21 Aug 2014 06:00:32 GMT"}], "update_date": "2014-08-22", "authors_parsed": [["Yun", "Hyokun", ""], ["Raman", "Parameswaran", ""], ["Vishwanathan", "S. V. N.", ""]]}, {"id": "1402.2678", "submitter": "Wen-Yu Hua", "authors": "Wen-Yu Hua and Thomas E. Nichols and Debashis Ghosh and the\n  Alzheimer's Disease Neuroimaging Initiative", "title": "Multiple Comparison Procedures for Neuroimaging Genomewide Association\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in neuroimaging has focused on assessing associations between\ngenetic variants that are measured on a genomewide scale and brain imaging\nphenotypes. A large number of works in the area apply massively univariate\nanalyses on a genomewide basis to find single nucleotide polymorphisms that\ninfluence brain structure. In this paper, we propose using various\ndimensionality reduction methods on both brain structural MRI scans and genomic\ndata, motivated by the Alzheimer's Disease Neuroimaging Initiative (ADNI)\nstudy. We also consider a new multiple testing adjustment method and compare it\nwith two existing false discovery rate (FDR) adjustment methods. The simulation\nresults suggest an increase in power for the proposed method. The real data\nanalysis suggests that the proposed procedure is able to find associations\nbetween genetic variants and brain volume differences that offer potentially\nnew biological insights.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2014 21:51:25 GMT"}, {"version": "v2", "created": "Wed, 12 Mar 2014 20:50:44 GMT"}, {"version": "v3", "created": "Sat, 22 Mar 2014 20:01:19 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Hua", "Wen-Yu", ""], ["Nichols", "Thomas E.", ""], ["Ghosh", "Debashis", ""], ["Initiative", "the Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1402.2713", "submitter": "Manuela Zucknick", "authors": "Manuela Zucknick and Sylvia Richardson", "title": "MCMC algorithms for Bayesian variable selection in the logistic\n  regression model for large-scale genomic applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale genomic applications vast numbers of molecular features are\nscanned in order to find a small number of candidates which are linked to a\nparticular disease or phenotype. This is a variable selection problem in the\n\"large p, small n\" paradigm where many more variables than samples are\navailable. Additionally, a complex dependence structure is often observed among\nthe markers/genes due to their joint involvement in biological processes and\npathways. Bayesian variable selection methods that introduce sparseness through\nadditional priors on the model size are well suited to the problem. However,\nthe model space is very large and standard Markov chain Monte Carlo (MCMC)\nalgorithms such as a Gibbs sampler sweeping over all p variables in each\niteration are often computationally infeasible. We propose to employ the\ndependence structure in the data to decide which variables should always be\nupdated together and which are nearly conditionally independent and hence do\nnot need to be considered together. Here, we focus on binary classification\napplications. We follow the implementation of the Bayesian probit regression\nmodel by Albert and Chib (1993) and the Bayesian logistic regression model by\nHolmes and Held (2006) which both lead to marginal Gaussian distributions. We\nin- vestigate several MCMC samplers using the dependence structure in different\nways. The mixing and convergence performances of the resulting Markov chains\nare evaluated and compared to standard samplers in two simulation studies and\nin an application to a real gene expression data set.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 01:52:11 GMT"}], "update_date": "2014-02-13", "authors_parsed": [["Zucknick", "Manuela", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1402.2828", "submitter": "Jonas Hallgren", "authors": "Jonas Hallgren, Timo Koski", "title": "Decomposition Sampling applied to Parallelization of Metropolis-Hastings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for sampling random variables that allows to\nseparation of the sampling process into subproblems by dividing the sample\nspace into overlapping parts. The subproblems can be solved independently of\neach other and are thus well suited for parallelization. Furthermore, on each\nof these subproblems it is possible to use distinct and independent sampling\nmethods. In other words, specific samplers can be designed for specific parts\nof the sample space. The algorithms are demonstrated on a particle marginal\nMetropolis-Hastings sampler applied to calibration of a volatility model and\ntwo toy examples. Significant speedup and decrease of total variation is\nobserved in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2014 14:15:56 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 17:02:42 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Hallgren", "Jonas", ""], ["Koski", "Timo", ""]]}, {"id": "1402.3410", "submitter": "Jean-Benoist Leger", "authors": "Jean-Benoist Leger", "title": "Wmixnet: Software for Clustering the Nodes of Binary and Valued Graphs\n  using the Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering the nodes of a graph allows the analysis of the topology of a\nnetwork.\n  The stochastic block model is a clustering method based on a probabilistic\nmodel. Initially developed for binary networks it has recently been extended to\nvalued networks possibly with covariates on the edges.\n  We present an implementation of a variational EM algorithm. It is written\nusing C++, parallelized, available under a GNU General Public License (version\n3), and can select the optimal number of clusters using the ICL criteria. It\nallows us to analyze networks with ten thousand nodes in a reasonable amount of\ntime.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 09:43:04 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Leger", "Jean-Benoist", ""]]}, {"id": "1402.3466", "submitter": "David Coufal Dr.", "authors": "David Coufal", "title": "Kernel density estimates in particle filter", "comments": "Substantially revised version. The extension of results to partial\n  derivatives has been provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with kernel density estimates of filtering densities in the\nparticle filter. The convergence of the estimates is investigated by means of\nFourier analysis. It is shown that the estimates converge to the theoretical\nfiltering densities in the mean integrated squared error under a certain\nassumption on the Sobolev character of the filtering densities. A sufficient\ncondition is presented for the persistence of this Sobolev character over time.\nBoth results are extended to partial derivatives of the estimates and filtering\ndensities.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 14:06:40 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2014 18:08:28 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Coufal", "David", ""]]}, {"id": "1402.3514", "submitter": "Kaveh Vakili", "authors": "E. Schmitt and K. Vakili", "title": "Robust PCA with FastHCS", "comments": "14 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is widely used to analyze high-dimensional\ndata, but it is very sensitive to outliers. Robust PCA methods seek fits that\nare unaffected by the outliers and can therefore be trusted to reveal them.\nFastHCS (High-dimensional Congruent Subsets) is a robust PCA algorithm suitable\nfor high-dimensional applications, including cases where the number of\nvariables exceeds the number of observations. After detailing the FastHCS\nalgorithm, we carry out an extensive simulation study and three real data\napplications, the results of which show that FastHCS is systematically more\nrobust to outliers than state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 16:13:21 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 09:51:42 GMT"}, {"version": "v3", "created": "Tue, 15 Jul 2014 06:20:07 GMT"}, {"version": "v4", "created": "Tue, 16 Dec 2014 20:28:59 GMT"}, {"version": "v5", "created": "Fri, 19 Dec 2014 04:23:59 GMT"}, {"version": "v6", "created": "Mon, 18 May 2015 09:48:48 GMT"}, {"version": "v7", "created": "Thu, 24 Sep 2015 11:24:08 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Schmitt", "E.", ""], ["Vakili", "K.", ""]]}, {"id": "1402.3569", "submitter": "Peter Forbes", "authors": "Peter G. M. Forbes and Kanti V. Mardia", "title": "A Fast Algorithm for Sampling from the Posterior of a von Mises\n  distribution", "comments": null, "journal-ref": null, "doi": "10.1080/00949655.2014.928711", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by molecular biology, there has been an upsurge of research\nactivities in directional statistics in general and its Bayesian aspect in\nparticular. The central distribution for the circular case is von Mises\ndistribution which has two parameters (mean and concentration) akin to the\nunivariate normal distribution. However, there has been a challenge to sample\nefficiently from the posterior distribution of the concentration parameter. We\ndescribe a novel, highly efficient algorithm to sample from the posterior\ndistribution and fill this long-standing gap.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2014 20:18:22 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 10:22:49 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Forbes", "Peter G. M.", ""], ["Mardia", "Kanti V.", ""]]}, {"id": "1402.3641", "submitter": "Munir Nayak", "authors": "Munir Ahmad Nayak and M C Deo", "title": "Wind speed prediction using different computing techniques", "comments": "BALWOIS 2010 Ohrid Republic of Macedonia 25, 29 May 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind is slated to become one of the most sought after source of energy in\nfuture. Both onshore as well as offshore wind farms are getting deployed\nrapidly over the world. This paper evaluates a neural network based time series\napproach to predict wind speed in real time over shorter duration of up to 12\nhr based on analysis of three hourly wind data collected through a wave rider\nbuoy deployed off Goa in deep water and far away from the shore. The data were\ncollected for 4 years from February 1998 to February 2002. A simple feed\nforward type of network trained using a variety of algorithms was used. The\ninput nodes selected by trial were three in number and belonged to the segment\nof preceding observations while the output node was single and it consisted of\nthe predicted value of the wind speed over the subsequent 3, 6 and 12 hours one\nat a time. The number of hidden nodes was based on trials. The total sample was\ndivided into a training set (first 70 percent) and a testing set (remaining 30\npercent). The outcome of the network was compared with the actual observations\nwith the help of scatter diagrams and time history plots as well as through the\nerror statistics of the correlation coefficient, R, and mean square error, MSE.\nThe testing of the network showed that it predicted the wind speed in a very\nsatisfactory manner with R = 0.99 and MSE = 0.30 (m/s)2 for a 3 hour ahead\nprediction while these values for a 12 hour ahead predictions were 0.96 and\n1.19 (m/s)2, respectively. Such a prediction based on neural network was found\nto be superior to that based on polynomial fittings as well as ARMA models.\nARIMA models were also used but the predicted values showed significant lag.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2014 03:57:56 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Nayak", "Munir Ahmad", ""], ["Deo", "M C", ""]]}, {"id": "1402.3748", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Better Solution Principle: A Facet of Concordance between Optimization\n  and Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical methods require solutions to optimization problems. When the\nglobal solution is hard to attain, statisticians always use the better if there\nare two solutions for chosen, where the word \"better\" is understood in the\nsense of optimization. This seems reasonable in that the better solution is\nmore likely to be the global solution, whose statistical properties of interest\nusually have been well established. From the statistical perspective, we use\nthe better solution because we intuitively believe the principle, called better\nsolution principle (BSP) in this paper, that a better solution to a statistical\noptimization problem also has better statistical properties of interest. BSP\ndisplays some concordance between optimization and statistics, and is expected\nto widely hold. Since theoretical study on BSP seems to be neglected by\nstatisticians, this paper aims to establish a framework for discussing BSP in\nvarious statistical optimization problems. We demonstrate several simple but\neffective comparison theorems as the key results of this paper, and apply them\nto verify BSP in commonly encountered statistical optimization problems,\nincluding maximum likelihood estimation, best subsample selection, and best\nsubset regression. It can be seen that BSP for these problems holds under\nreasonable conditions, i.e., a better solution indeed has better statistical\nproperties of interest. In addition, guided by the BSP theory, we develop a new\nbest subsample selection method that performs well when there are clustered\noutliers.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2014 02:43:51 GMT"}, {"version": "v2", "created": "Sat, 3 May 2014 13:13:13 GMT"}, {"version": "v3", "created": "Thu, 8 May 2014 03:04:39 GMT"}, {"version": "v4", "created": "Fri, 23 May 2014 00:13:37 GMT"}, {"version": "v5", "created": "Wed, 16 Jan 2019 02:11:32 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1402.4039", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Nicolas Chopin", "title": "Sequential Quasi-Monte Carlo", "comments": "55 pages, 10 figures (final version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive and study SQMC (Sequential Quasi-Monte Carlo), a class of\nalgorithms obtained by introducing QMC point sets in particle filtering. SQMC\nis related to, and may be seen as an extension of, the array-RQMC algorithm of\nL'Ecuyer et al. (2006). The complexity of SQMC is $O(N \\log N)$, where $N$ is\nthe number of simulations at each iteration, and its error rate is smaller than\nthe Monte Carlo rate $O_P(N^{-1/2})$. The only requirement to implement SQMC is\nthe ability to write the simulation of particle $x_t^n$ given $x_{t-1}^n$ as a\ndeterministic function of $x_{t-1}^n$ and a fixed number of uniform variates.\nWe show that SQMC is amenable to the same extensions as standard SMC, such as\nforward smoothing, backward smoothing, unbiased likelihood evaluation, and so\non. In particular, SQMC may replace SMC within a PMCMC (particle Markov chain\nMonte Carlo) algorithm. We establish several convergence results. We provide\nnumerical evidence that SQMC may significantly outperform SMC in practical\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 15:47:32 GMT"}, {"version": "v2", "created": "Tue, 4 Mar 2014 13:26:35 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 15:46:47 GMT"}, {"version": "v4", "created": "Mon, 11 Aug 2014 21:13:02 GMT"}, {"version": "v5", "created": "Fri, 28 Nov 2014 17:11:24 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Gerber", "Mathieu", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1402.4089", "submitter": "Andrew Beam", "authors": "Andrew L. Beam, Sujit K. Ghosh, Jon Doyle", "title": "Fast Hamiltonian Monte Carlo Using GPU Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the Hamiltonian Monte Carlo (HMC) algorithm has been found\nto work more efficiently compared to other popular Markov Chain Monte Carlo\n(MCMC) methods (such as random walk Metropolis-Hastings) in generating samples\nfrom a posterior distribution. A general framework for HMC based on the use of\ngraphical processing units (GPUs) is shown to greatly reduce the computing time\nneeded for Bayesian inference. The most expensive computational tasks in HMC\nare the evaluation of the posterior kernel and computing its gradient with\nrespect to the parameters of interest. One of primary goals of this article to\nshow that by expressing each of these tasks in terms of simple matrix or\nelement-wise operations and maintaining persistent objects in GPU memory, the\ncomputational time can be drastically reduced. By using GPU objects to perform\nthe entire HMC simulation, most of the latency penalties associated with\ntransferring data from main to GPU memory can be avoided. Thus, the proposed\ncomputational framework is conceptually very simple, but also is general enough\nto be applied to most problems that use HMC sampling. For clarity of\nexposition, the effectiveness of the proposed approach is demonstrated in the\nhigh-dimensional setting on a standard statistical model - multinomial\nregression. Using GPUs, analyses of data sets that were previously intractable\nfor fully Bayesian approaches due to the prohibitively high computational cost\nare now feasible.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2014 18:08:16 GMT"}], "update_date": "2014-02-18", "authors_parsed": [["Beam", "Andrew L.", ""], ["Ghosh", "Sujit K.", ""], ["Doyle", "Jon", ""]]}, {"id": "1402.4281", "submitter": "Jonathan Stroud", "authors": "Jonathan R. Stroud, Michael L. Stein, Shaun Lysen", "title": "Bayesian and Maximum Likelihood Estimation for Gaussian Processes on an\n  Incomplete Lattice", "comments": "29 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach for Bayesian and maximum likelihood\nparameter estimation for stationary Gaussian processes observed on a large\nlattice with missing values. We propose an MCMC approach for Bayesian\ninference, and a Monte Carlo EM algorithm for maximum likelihood inference. Our\napproach uses data augmentation and circulant embedding of the covariance\nmatrix, and provides exact inference for the parameters and the missing data.\nUsing simulated data and an application to satellite sea surface temperatures\nin the Pacific Ocean, we show that our method provides accurate inference on\nlattices of sizes up to 512 x 512, and outperforms two popular methods:\ncomposite likelihood and spectral approximations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2014 10:38:14 GMT"}], "update_date": "2014-02-19", "authors_parsed": [["Stroud", "Jonathan R.", ""], ["Stein", "Michael L.", ""], ["Lysen", "Shaun", ""]]}, {"id": "1402.4914", "submitter": "Vikash Mansinghka", "authors": "Vikash Mansinghka and Eric Jonas", "title": "Building fast Bayesian computing machines out of intentionally\n  stochastic, digital parts", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain interprets ambiguous sensory information faster and more reliably\nthan modern computers, using neurons that are slower and less reliable than\nlogic gates. But Bayesian inference, which underpins many computational models\nof perception and cognition, appears computationally challenging even given\nmodern transistor speeds and energy budgets. The computational principles and\nstructures needed to narrow this gap are unknown. Here we show how to build\nfast Bayesian computing machines using intentionally stochastic, digital parts,\nnarrowing this efficiency gap by multiple orders of magnitude. We find that by\nconnecting stochastic digital components according to simple mathematical\nrules, one can build massively parallel, low precision circuits that solve\nBayesian inference problems and are compatible with the Poisson firing\nstatistics of cortical neurons. We evaluate circuits for depth and motion\nperception, perceptual learning and causal reasoning, each performing inference\nover 10,000+ latent variables in real time - a 1,000x speed advantage over\ncommodity microprocessors. These results suggest a new role for randomness in\nthe engineering and reverse-engineering of intelligent computation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 07:17:03 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Mansinghka", "Vikash", ""], ["Jonas", "Eric", ""]]}, {"id": "1402.4984", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelme", "title": "Fast matrix computations for functional additive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in functional data analysis to look at a set of related\nfunctions: a set of learning curves, a set of brain signals, a set of spatial\nmaps, etc. One way to express relatedness is through an additive model, whereby\neach individual function $g_{i}\\left(x\\right)$ is assumed to be a variation\naround some shared mean $f(x)$. Gaussian processes provide an elegant way of\nconstructing such additive models, but suffer from computational difficulties\narising from the matrix operations that need to be performed. Recently Heersink\n& Furrer have shown that functional additive model give rise to covariance\nmatrices that have a specific form they called quasi-Kronecker (QK), whose\ninverses are relatively tractable. We show that under additional assumptions\nthe two-level additive model leads to a class of matrices we call restricted\nquasi-Kronecker, which enjoy many interesting properties. In particular, we\nformulate matrix factorisations whose complexity scales only linearly in the\nnumber of functions in latent field, an enormous improvement over the cubic\nscaling of na\\\"ive approaches. We describe how to leverage the properties of\nrQK matrices for inference in Latent Gaussian Models.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 12:34:36 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Barthelme", "Simon", ""]]}, {"id": "1402.5107", "submitter": "Donatello Telesca", "authors": "David Rossell and Donatello Telesca", "title": "Non-Local Priors for High-Dimensional Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneously achieving parsimony and good predictive power in high\ndimensions is a main challenge in statistics. Non-local priors (NLPs) possess\nappealing properties for high-dimensional model choice, but their use for\nestimation has not been studied in detail. We show that, for regular models,\nBayesian model averaging (BMA) estimates based on NLPs shrink spurious\nparameters either at fast polynomial or quasi-exponential rates as the sample\nsize $n$ increases (depending on the chosen prior density). Non-spurious\nparameter estimates only differ from the oracle MLE by a factor of $n^{-1}$. We\nextend some results to linear models with dimension $p$ growing with $n$.\n  Coupled with our theoretical investigations, we outline the constructive\nrepresentation of NLPs as mixtures of truncated distributions. From a\npractitioners' perspective, our work enables simple posterior sampling and\nextending NLPs beyond previous proposals. Our results show notable\nhigh-dimensional estimation for linear models with $p>>n$ at reduced\ncomputational cost. NLPs provided lower estimation error than benchmark and\nhyper-g priors, SCAD and LASSO in simulations, and in gene expression data\nachieved higher cross-validated $R^2$ with an order of magnitude less\npredictors. Remarkably, these results were obtained without the need to\npre-screen predictors. Our findings contribute to the debate of whether\ndifferent priors should be used for estimation and model selection, showing\nthat selection priors may actually be desirable for high-dimensional\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2014 19:20:49 GMT"}, {"version": "v2", "created": "Wed, 16 Apr 2014 18:41:29 GMT"}, {"version": "v3", "created": "Wed, 21 Jan 2015 01:37:19 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Rossell", "David", ""], ["Telesca", "Donatello", ""]]}, {"id": "1402.5257", "submitter": "Minho Park", "authors": "M. Park and K. A. Cliffe", "title": "Conditional Multilevel Monte Carlo Simulation of Groundwater Flow in the\n  Culebra Dolomite at the Waste Isolation Pilot Plant (WIPP) Site", "comments": "16 pages, 13 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extended the multilevel Monte of Carlo (MLMC) approach to simulation of\ngroundwater flow in porous media by incorporating direct measurements of medium\nproperties. Numerical simulations of Waste Isolation Pilot Plant (WIPP)\nrepository in southeastern New Mexico are performed to test the performance of\nthe conditional MLMC technique. The log-transmissivity of WIPP site is modeled\nas the conditional random fields which honor exact field values at a few\nlocations. The conditional random fields are generated through the modified\ncirculant embedding methods in (Dietrich and Newsam, 1996). We also study\neffects of a combination of the conditional MLMC accompanied by antithetic\nvariates. The main quantity of interest is the time of radionuclides travelling\nfrom the center of repository to the site boundary. Numerical examples are\npresented to demonstrate the cost-effectiveness of the multilevel approach in\ncomparison to the standard Monte Carlo (MC) simulation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 11:20:42 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Park", "M.", ""], ["Cliffe", "K. A.", ""]]}, {"id": "1402.5282", "submitter": "Eisa Mahmoudi", "authors": "Eisa Mahmoudi and Ali Akbar Jafari", "title": "The Compound Class of Linear Failure Rate-Power Series Distributions:\n  Model, Properties and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce in this paper a new class of distributions which generalizes the\nlinear failure rate (LFR) distribution and is obtained by compounding the LFR\ndistribution and power series (PS) class of distributions. This new class of\ndistributions is called the linear failure rate-power series (LFRPS)\ndistributions and contains some new distributions such as linear failure rate\ngeometric (LFRG) distribution, linear failure rate Poisson (LFRP) distribution,\nlinear failure rate logarithmic (LFRL) distribution, linear failure rate\nbinomial (LFRB) distribution and Raylight-power series (RPS) class of\ndistributions. Some former works such as exponential-power series (EPS) class\nof distributions, exponential geometric (EG) distribution, exponential Poisson\n(EP) distribution and exponential logarithmic (EL) distribution are special\ncases of the new proposed model.\n  The ability of the LFRPS class of distributions is in covering five possible\nhazard rate function i.e., increasing, decreasing, upside-down bathtub\n(unimodal), bathtub and increasing-decreasing-increasing shaped. Several\nproperties of the LFRPS distributions such as moments, maximum likelihood\nestimation procedure via an EM-algorithm and inference for a large sample, are\ndiscussed in this paper. In order to show the flexibility and potentiality of\nthe new class of distributions, the fitted results of the new class of\ndistributions and some its submodels are compared using a real data set.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 12:44:37 GMT"}], "update_date": "2014-02-24", "authors_parsed": [["Mahmoudi", "Eisa", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1402.5431", "submitter": "Paul McNicholas", "authors": "Adelchi Azzalini, Ryan P. Browne, Marc G. Genton and Paul D.\n  McNicholas", "title": "On nomenclature for, and the relative merits of, two formulations of\n  skew distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine some distributions used extensively within the model-based\nclustering literature in recent years, paying special attention to} claims that\nhave been made about their relative efficacy. Theoretical arguments are\nprovided as well as real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2014 21:40:10 GMT"}, {"version": "v2", "created": "Tue, 1 Jul 2014 17:33:14 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2015 22:53:41 GMT"}], "update_date": "2015-12-07", "authors_parsed": [["Azzalini", "Adelchi", ""], ["Browne", "Ryan P.", ""], ["Genton", "Marc G.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1402.5473", "submitter": "Fritz Obermeyer", "authors": "Fritz Obermeyer and Jonathan Glidden and Eric Jonas", "title": "Scaling Nonparametric Bayesian Inference via Subsample-Annealing", "comments": "To appear in AISTATS 2014", "journal-ref": "PMLR 33:696-705 2014", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an adaptation of the simulated annealing algorithm to\nnonparametric clustering and related probabilistic models. This new algorithm\nlearns nonparametric latent structure over a growing and constantly churning\nsubsample of training data, where the portion of data subsampled can be\ninterpreted as the inverse temperature beta(t) in an annealing schedule. Gibbs\nsampling at high temperature (i.e., with a very small subsample) can more\nquickly explore sketches of the final latent state by (a) making longer jumps\naround latent space (as in block Gibbs) and (b) lowering energy barriers (as in\nsimulated annealing). We prove subsample annealing speeds up mixing time N^2 ->\nN in a simple clustering model and exp(N) -> N in another class of models,\nwhere N is data size. Empirically subsample-annealing outperforms naive Gibbs\nsampling in accuracy-per-wallclock time, and can scale to larger datasets and\ndeeper hierarchical models. We demonstrate improved inference on million-row\nsubsamples of US Census data and network log data and a 307-row hospital rating\ndataset, using a Pitman-Yor generalization of the Cross Categorization model.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2014 03:44:04 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Obermeyer", "Fritz", ""], ["Glidden", "Jonathan", ""], ["Jonas", "Eric", ""]]}, {"id": "1402.6602", "submitter": "Andrew Golightly", "authors": "Chris Sherlock and Andrew Golightly and Colin Gillespie", "title": "Bayesian Inference for Hybrid Discrete-Continuous Stochastic Kinetic\n  Models", "comments": "Submitted", "journal-ref": null, "doi": "10.1088/0266-5611/30/11/114005", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently performing simulation and inference\nfor stochastic kinetic models. Whilst it is possible to work directly with the\nresulting Markov jump process, computational cost can be prohibitive for\nnetworks of realistic size and complexity. In this paper, we consider an\ninference scheme based on a novel hybrid simulator that classifies reactions as\neither \"fast\" or \"slow\" with fast reactions evolving as a continuous Markov\nprocess whilst the remaining slow reaction occurrences are modelled through a\nMarkov jump process with time dependent hazards. A linear noise approximation\n(LNA) of fast reaction dynamics is employed and slow reaction events are\ncaptured by exploiting the ability to solve the stochastic differential\nequation driving the LNA. This simulation procedure is used as a proposal\nmechanism inside a particle MCMC scheme, thus allowing Bayesian inference for\nthe model parameters. We apply the scheme to a simple application and compare\nthe output with an existing hybrid approach and also a scheme for performing\ninference for the underlying discrete stochastic model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 16:38:15 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Sherlock", "Chris", ""], ["Golightly", "Andrew", ""], ["Gillespie", "Colin", ""]]}, {"id": "1402.6744", "submitter": "Paul McNicholas", "authors": "Katherine Morris, Antonio Punzo, Paul D. McNicholas and Ryan P. Browne", "title": "Asymmetric Clusters and Outliers: Mixtures of Multivariate Contaminated\n  Shifted Asymmetric Laplace Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of multivariate contaminated shifted asymmetric Laplace\ndistributions are developed for handling asymmetric clusters in the presence of\noutliers (also referred to as bad points herein). In addition to the parameters\nof the related non-contaminated mixture, for each (asymmetric) cluster, our\nmodel has one parameter controlling the proportion of outliers and one\nspecifying the degree of contamination. Crucially, these parameters do not have\nto be specified a priori, adding a flexibility to our approach that is absent\nfrom other approaches such as trimming. Moreover, each observation is given a\nposterior probability of belonging to a particular cluster, and of being an\noutlier or not; advantageously, this allows for the automatic detection of\noutliers. An expectation-conditional maximization algorithm is outlined for\nparameter estimation and various implementation issues are discussed. The\nbehaviour of the proposed model is investigated, and compared with\nwell-established finite mixtures, on artificial and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2014 23:29:54 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 00:31:37 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Morris", "Katherine", ""], ["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1402.6781", "submitter": "Gael Martin Prof", "authors": "D. S. Poskitt, Gael M. Martin and Simone D. Grose", "title": "Bias Reduction of Long Memory Parameter Estimators via the Pre-filtered\n  Sieve Bootstrap", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the use of bootstrap-based bias correction of\nsemi-parametric estimators of the long memory parameter in fractionally\nintegrated processes. The re-sampling method involves the application of the\nsieve bootstrap to data pre-filtered by a preliminary semi-parametric estimate\nof the long memory parameter. Theoretical justification for using the bootstrap\ntechniques to bias adjust log-periodogram and semi-parametric local Whittle\nestimators of the memory parameter is provided. Simulation evidence comparing\nthe performance of the bootstrap bias correction with analytical bias\ncorrection techniques is also presented. The bootstrap method is shown to\nproduce notable bias reductions, in particular when applied to an estimator for\nwhich analytical adjustments have already been used. The empirical coverage of\nconfidence intervals based on the bias-adjusted estimators is very close to the\nnominal, for a reasonably large sample size, more so than for the comparable\nanalytically adjusted estimators. The precision of inferences (as measured by\ninterval length) is also greater when the bootstrap is used to bias correct\nrather than analytical adjustments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 03:40:15 GMT"}], "update_date": "2014-02-28", "authors_parsed": [["Poskitt", "D. S.", ""], ["Martin", "Gael M.", ""], ["Grose", "Simone D.", ""]]}, {"id": "1402.6928", "submitter": "Arthur White Dr.", "authors": "Arthur White, Jason Wyse and Thomas Brendan Murphy", "title": "Bayesian variable selection for latent class analysis using a collapsed\n  Gibbs sampler", "comments": "(to appear in Statistics and Computing)", "journal-ref": "Statistics and Computing January 2016, Volume 26, Issue 1, pp\n  511-527", "doi": "10.1007/s11222-014-9542-5", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class analysis is used to perform model based clustering for\nmultivariate categorical responses. Selection of the variables most relevant\nfor clustering is an important task which can affect the quality of clustering\nconsiderably. This work considers a Bayesian approach for selecting the number\nof clusters and the best clustering variables. The main idea is to reformulate\nthe problem of group and variable selection as a probabilistically driven\nsearch over a large discrete space using Markov chain Monte Carlo (MCMC)\nmethods. This approach results in estimates of degree of relevance of each\nvariable for clustering along with posterior probability for the number of\nclusters. Bayes factors can then be easily calculated, and a suitable model\nchosen in a principled manner. Both selection tasks are carried out\nsimultaneously using an MCMC approach based on a collapsed Gibbs sampling\nmethod, whereby several model parameters are integrated from the model,\nsubstantially improving computational performance. Approaches for estimating\nposterior marginal probabilities of class membership, variable inclusion and\nnumber of groups are proposed, and post-hoc procedures for parameter and\nuncertainty estimation are outlined. The approach is tested on simulated and\nreal data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2014 15:03:47 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 16:46:50 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["White", "Arthur", ""], ["Wyse", "Jason", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1402.7107", "submitter": "Youhan Fang", "authors": "Youhan Fang, Jesus-Maria Sanz-Serna and Robert D. Skeel", "title": "Compressible Generalized Hybrid Monte Carlo", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": "10.1063/1.4874000", "report-no": null, "categories": "physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most demanding calculations is to generate random samples from a\nspecified probability distribution (usually with an unknown normalizing\nprefactor) in a high-dimensional configuration space. One often has to resort\nto using a Markov chain Monte Carlo method, which converges only in the limit\nto the prescribed distribution. Such methods typically inch through\nconfiguration space step by step, with acceptance of a step based on a\nMetropolis(-Hastings) criterion. An acceptance rate of 100% is possible in\nprinciple by embedding configuration space in a higher-dimensional phase space\nand using ordinary differential equations. In practice, numerical integrators\nmust be used, lowering the acceptance rate. This is the essence of hybrid Monte\nCarlo methods. Presented is a general framework for constructing such methods\nunder relaxed conditions: the only geometric property needed is (weakened)\nreversibility; volume preservation is not needed. The possibilities are\nillustrated by deriving a couple of explicit hybrid Monte Carlo methods, one\nbased on barrier-lowering variable-metric dynamics and another based on\nisokinetic dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 00:01:07 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Fang", "Youhan", ""], ["Sanz-Serna", "Jesus-Maria", ""], ["Skeel", "Robert D.", ""]]}, {"id": "1402.7263", "submitter": "Radoslav Harman", "authors": "Radoslav Harman, Alena Bachrat\\'a, Lenka Filov\\'a", "title": "Heuristic construction of exact experimental designs under multiple\n  resource constraints", "comments": "Compared to the first version, the second version of the manuscript\n  contains a new running example to illustrate basic concepts and a new example\n  of a block design with resource constraints. Some parts of the manuscript\n  have been reformulated with the aim to add more details and clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is twofold. First, we introduce \"resource constraints\"\nas a general concept that covers many practical restrictions on experimental\ndesign. Second, for computing efficient exact designs of experiments under any\ncombination of resource constraints, we propose a tabu search heuristic that\nuses some ideas of the Detmax procedure. To illustrate the scope and\nperformance of our heuristic, we computed D-efficient designs for 1) a block\nmodel with limits on the numbers of blocks and on the availability of\nexperimental material; 2) a quadratic regression model with simultaneous\nmarginal and cost constraints; 3) a non-linear regression model with\nsimultaneous direct and cost constraints. As we show, the proposed heuristic\ngenerates comparable or better results than algorithms specialized for\ncomputing optimal designs under less general constraints.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 14:39:52 GMT"}, {"version": "v2", "created": "Thu, 7 Aug 2014 09:06:54 GMT"}], "update_date": "2014-08-08", "authors_parsed": [["Harman", "Radoslav", ""], ["Bachrat\u00e1", "Alena", ""], ["Filov\u00e1", "Lenka", ""]]}, {"id": "1402.7349", "submitter": "Kean Ming Tan", "authors": "Kean Ming Tan, Palma London, Karthik Mohan, Su-In Lee, Maryam Fazel,\n  and Daniela Witten", "title": "Learning Graphical Models With Hubs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional graphical model in\nwhich certain hub nodes are highly-connected to many other nodes. Many authors\nhave studied the use of an l1 penalty in order to learn a sparse graph in\nhigh-dimensional setting. However, the l1 penalty implicitly assumes that each\nedge is equally likely and independent of all other edges. We propose a general\nframework to accommodate more realistic networks with hub nodes, using a convex\nformulation that involves a row-column overlap norm penalty. We apply this\ngeneral framework to three widely-used probabilistic graphical models: the\nGaussian graphical model, the covariance graph model, and the binary Ising\nmodel. An alternating direction method of multipliers algorithm is used to\nsolve the corresponding convex optimization problems. On synthetic data, we\ndemonstrate that our proposed framework outperforms competitors that do not\nexplicitly model hub nodes. We illustrate our proposal on a webpage data set\nand a gene expression data set.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2014 19:09:11 GMT"}, {"version": "v2", "created": "Sat, 9 Aug 2014 18:33:43 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Tan", "Kean Ming", ""], ["London", "Palma", ""], ["Mohan", "Karthik", ""], ["Lee", "Su-In", ""], ["Fazel", "Maryam", ""], ["Witten", "Daniela", ""]]}]