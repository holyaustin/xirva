[{"id": "1003.0243", "submitter": "Bernard Silverman", "authors": "Graeme K. Ambler and Bernard W. Silverman", "title": "Perfect simulation using dominated coupling from the past with\n  application to area-interaction point processes and wavelet thresholding", "comments": "27 pages, 8 figures. Chapter 3 of \"Probability and Mathematical\n  Genetics: Papers in Honour of Sir John Kingman\" (Editors N.H. Bingham and\n  C.M. Goldie), Cambridge University Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider perfect simulation algorithms for locally stable point processes\nbased on dominated coupling from the past, and apply these methods in two\ndifferent contexts. A new version of the algorithm is developed which is\nfeasible for processes which are neither purely attractive nor purely\nrepulsive. Such processes include multiscale area-interaction processes, which\nare capable of modelling point patterns whose clustering structure varies\nacross scales. The other topic considered is nonparametric regression using\nwavelets, where we use a suitable area-interaction process on the discrete\nspace of indices of wavelet coefficients to model the notion that if one\nwavelet coefficient is non-zero then it is more likely that neighbouring\ncoefficients will be also. A method based on perfect simulation within this\nmodel shows promising results compared to the standard methods which threshold\ncoefficients independently.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 03:35:56 GMT"}], "update_date": "2010-03-02", "authors_parsed": [["Ambler", "Graeme K.", ""], ["Silverman", "Bernard W.", ""]]}, {"id": "1003.0428", "submitter": "Gabriel Stoltz", "authors": "Nicolas Chopin (CREST/Ensae), Tony Lelievre and Gabriel Stoltz\n  (CERMICS/Ecole des Ponts and Micmac, Inria)", "title": "Free Energy Methods for Bayesian Inference: Efficient Exploration of\n  Univariate Gaussian Mixture Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their multimodality, mixture posterior distributions are difficult\nto sample with standard Markov chain Monte Carlo (MCMC) methods. We propose a\nstrategy to enhance the sampling of MCMC in this context, using a biasing\nprocedure which originates from computational Statistical Physics. The\nprinciple is first to choose a \"reaction coordinate\", that is, a \"direction\" in\nwhich the target distribution is multimodal. In a second step, the marginal\nlog-density of the reaction coordinate with respect to the posterior\ndistribution is estimated; minus this quantity is called \"free energy\" in the\ncomputational Statistical Physics literature. To this end, we use adaptive\nbiasing Markov chain algorithms which adapt their targeted invariant\ndistribution on the fly, in order to overcome sampling barriers along the\nchosen reaction coordinate. Finally, we perform an importance sampling step in\norder to remove the bias and recover the true posterior. The efficiency factor\nof the importance sampling step can easily be estimated \\emph{a priori} once\nthe bias is known, and appears to be rather large for the test cases we\nconsidered. A crucial point is the choice of the reaction coordinate. One\nstandard choice (used for example in the classical Wang-Landau algorithm) is\nminus the log-posterior density. We discuss other choices. We show in\nparticular that the hyper-parameter that determines the order of magnitude of\nthe variance of each component is both a convenient and an efficient reaction\ncoordinate. We also show how to adapt the method to compute the evidence\n(marginal likelihood) of a mixture model. We illustrate our approach by\nanalyzing two real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2010 19:24:16 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2010 20:12:08 GMT"}, {"version": "v3", "created": "Thu, 23 Sep 2010 05:21:51 GMT"}, {"version": "v4", "created": "Mon, 18 Apr 2011 13:00:46 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Chopin", "Nicolas", "", "CREST/Ensae"], ["Lelievre", "Tony", "", "CERMICS/Ecole des Ponts and Micmac, Inria"], ["Stoltz", "Gabriel", "", "CERMICS/Ecole des Ponts and Micmac, Inria"]]}, {"id": "1003.0804", "submitter": "Pritam Ranjan", "authors": "Mark Franey, Pritam Ranjan and Hugh Chipman", "title": "Branch and Bound Algorithms for Maximizing Expected Improvement\n  Functions", "comments": "26 pages, 14 figures, preprint submitted to the Journal of\n  Statistical Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deterministic computer simulations are often used as a replacement for\ncomplex physical experiments. Although less expensive than physical\nexperimentation, computer codes can still be time-consuming to run. An\neffective strategy for exploring the response surface of the deterministic\nsimulator is the use of an approximation to the computer code, such as a\nGaussian process (GP) model, coupled with a sequential sampling strategy for\nchoosing design points that can be used to build the GP model. The ultimate\ngoal of such studies is often the estimation of specific features of interest\nof the simulator output, such as the maximum, minimum, or a level set\n(contour). Before approximating such features with the GP model, sufficient\nruns of the computer simulator must be completed.\n  Sequential designs with an expected improvement (EI) function can yield good\nestimates of the features with a minimal number of runs. The challenge is that\nthe expected improvement function itself is often multimodal and difficult to\nmaximize. We develop branch and bound algorithms for efficiently maximizing the\nEI function in specific problems, including the simultaneous estimation of a\nminimum and a maximum, and in the estimation of a contour. These branch and\nbound algorithms outperform other optimization strategies such as genetic\nalgorithms, and over a number of sequential design steps can lead to\ndramatically superior accuracy in estimation of features of interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2010 13:49:48 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Franey", "Mark", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1003.1315", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, Ronald Haynes and Richard Karsten", "title": "A Computationally Stable Approach to Gaussian Process Interpolation of\n  Deterministic Computer Simulation Data", "comments": "26 pages, 12 figures", "journal-ref": "Technometrics, 2011, 53(4), 366-378", "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many expensive deterministic computer simulators, the outputs do not have\nreplication error and the desired metamodel (or statistical emulator) is an\ninterpolator of the observed data. Realizations of Gaussian spatial processes\n(GP) are commonly used to model such simulator outputs. Fitting a GP model to\n$n$ data points requires the computation of the inverse and determinant of $n\n\\times n$ correlation matrices, $R$, that are sometimes computationally\nunstable due to near-singularity of $R$. This happens if any pair of design\npoints are very close together in the input space. The popular approach to\novercome near-singularity is to introduce a small nugget (or jitter) parameter\nin the model that is estimated along with other model parameters. The inclusion\nof a nugget in the model often causes unnecessary over-smoothing of the data.\nIn this paper, we propose a lower bound on the nugget that minimizes the\nover-smoothing and an iterative regularization approach to construct a\npredictor that further improves the interpolation accuracy. We also show that\nthe proposed predictor converges to the GP interpolator.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2010 18:03:27 GMT"}, {"version": "v2", "created": "Wed, 13 Oct 2010 12:29:42 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2012 10:02:45 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Ranjan", "Pritam", ""], ["Haynes", "Ronald", ""], ["Karsten", "Richard", ""]]}, {"id": "1003.1771", "submitter": "Jan Mandel", "authors": "Jan Mandel, Jonathan D. Beezley, Loren Cobb, and Ashok Krishnamurthy", "title": "Data Driven Computing by the Morphing Fast Fourier Transform Ensemble\n  Kalman Filter in Epidemic Spread Simulations", "comments": "11 pages, 3 figures. Submitted to ICCS 2010", "journal-ref": null, "doi": null, "report-no": "UCD CCM Report 286", "categories": "stat.CO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FFT EnKF data assimilation method is proposed and applied to a stochastic\ncell simulation of an epidemic, based on the S-I-R spread model. The FFT EnKF\ncombines spatial statistics and ensemble filtering methodologies into a\nlocalized and computationally inexpensive version of EnKF with a very small\nensemble, and it is further combined with the morphing EnKF to assimilate\nchanges in the position of the epidemic.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 02:38:43 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Mandel", "Jan", ""], ["Beezley", "Jonathan D.", ""], ["Cobb", "Loren", ""], ["Krishnamurthy", "Ashok", ""]]}, {"id": "1003.1950", "submitter": "Ad Ridder", "authors": "Ad Ridder", "title": "Asymptotic optimality of the cross-entropy method for Markov chain\n  problems", "comments": "13 pager; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The correspondence between the cross-entropy method and the zero-variance\napproximation to simulate a rare event problem in Markov chains is shown. This\nleads to a sufficient condition that the cross-entropy estimator is\nasymptotically optimal.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2010 18:55:18 GMT"}], "update_date": "2010-03-10", "authors_parsed": [["Ridder", "Ad", ""]]}, {"id": "1003.3201", "submitter": "Radford M. Neal", "authors": "Madeleine Thompson and Radford M. Neal", "title": "Covariance-Adaptive Slice Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": "Tech. Rep. 1002, Dept. of Statistics, Univ. of Toronto", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe two slice sampling methods for taking multivariate steps using\nthe crumb framework. These methods use the gradients at rejected proposals to\nadapt to the local curvature of the log-density surface, a technique that can\nproduce much better proposals when parameters are highly correlated. We\nevaluate our methods on four distributions and compare their performance to\nthat of a non-adaptive slice sampling method and a Metropolis method. The\nadaptive methods perform favorably on low-dimensional target distributions with\nhighly-correlated parameters.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 17:45:16 GMT"}], "update_date": "2010-03-17", "authors_parsed": [["Thompson", "Madeleine", ""], ["Neal", "Radford M.", ""]]}, {"id": "1003.3272", "submitter": "Hua Zhou", "authors": "Hua Zhou, Kenneth Lange, Marc A. Suchard", "title": "Graphics Processing Units and High-Dimensional Optimization", "comments": null, "journal-ref": null, "doi": "10.1214/10-STS336", "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper discusses the potential of graphics processing units (GPUs) in\nhigh-dimensional optimization problems. A single GPU card with hundreds of\narithmetic cores can be inserted in a personal computer and dramatically\naccelerates many statistical algorithms. To exploit these devices fully,\noptimization algorithms should reduce to multiple parallel tasks, each\naccessing a limited amount of data. These criteria favor EM and MM algorithms\nthat separate parameters and data. To a lesser extent block relaxation and\ncoordinate descent and ascent also qualify. We demonstrate the utility of GPUs\nin nonnegative matrix factorization, PET image reconstruction, and\nmultidimensional scaling. Speedups of 100 fold can easily be attained. Over the\nnext decade, GPUs will fundamentally alter the landscape of computational\nstatistics. It is time for more statisticians to get on-board.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2010 23:06:35 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Zhou", "Hua", ""], ["Lange", "Kenneth", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1003.3357", "submitter": "Alan Tua Mr", "authors": "Alan Tua and Kristian Zarb Adami", "title": "Computational Methods in Bayesian Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on utilizing two different Bayesian methods to deal with a\nvariety of toy problems which occur in data analysis. In particular we\nimplement the Variational Bayesian and Nested Sampling methods to tackle the\nproblems of polynomial selection and Gaussian Mixture Models, comparing the\nalgorithms in terms of processing speed and accuracy. In the problems tackled\nhere it is the Variational Bayesian algorithms which are the faster though both\nresults give similar results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2010 12:26:05 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2010 12:55:08 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Tua", "Alan", ""], ["Adami", "Kristian Zarb", ""]]}, {"id": "1003.3988", "submitter": "Peter Green", "authors": "Peter J. Green", "title": "Colouring and breaking sticks: random distributions and heterogeneous\n  clustering", "comments": "26 pages, 3 figures. Chapter 13 of \"Probability and Mathematical\n  Genetics: Papers in Honour of Sir John Kingman\" (Editors N.H. Bingham and\n  C.M. Goldie), Cambridge University Press, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by reviewing some probabilistic results about the Dirichlet Process\nand its close relatives, focussing on their implications for statistical\nmodelling and analysis. We then introduce a class of simple mixture models in\nwhich clusters are of different `colours', with statistical characteristics\nthat are constant within colours, but different between colours. Thus cluster\nidentities are exchangeable only within colours. The basic form of our model is\na variant on the familiar Dirichlet process, and we find that much of the\nstandard modelling and computational machinery associated with the Dirichlet\nprocess may be readily adapted to our generalisation. The methodology is\nillustrated with an application to the partially-parametric clustering of gene\nexpression profiles.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2010 09:48:06 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Green", "Peter J.", ""]]}, {"id": "1003.4042", "submitter": "Sou-Cheng Choi", "authors": "Sou-Cheng T. Choi, Christopher C. Paige, Michael A. Saunders", "title": "MINRES-QLP: a Krylov subspace method for indefinite or singular\n  symmetric systems", "comments": "26 pages, 6 figures", "journal-ref": "SIAM Journal on Scientific Computing, Volume 33, Issue 4,\n  1810-1836, 2011", "doi": "10.1137/100787921", "report-no": null, "categories": "math.NA cs.CE cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CG, SYMMLQ, and MINRES are Krylov subspace methods for solving symmetric\nsystems of linear equations. When these methods are applied to an incompatible\nsystem (that is, a singular symmetric least-squares problem), CG could break\ndown and SYMMLQ's solution could explode, while MINRES would give a\nleast-squares solution but not necessarily the minimum-length (pseudoinverse)\nsolution. This understanding motivates us to design a MINRES-like algorithm to\ncompute minimum-length solutions to singular symmetric systems.\n  MINRES uses QR factors of the tridiagonal matrix from the Lanczos process\n(where R is upper-tridiagonal). MINRES-QLP uses a QLP decomposition (where\nrotations on the right reduce R to lower-tridiagonal form). On ill-conditioned\nsystems (singular or not), MINRES-QLP can give more accurate solutions than\nMINRES. We derive preconditioned MINRES-QLP, new stopping rules, and better\nestimates of the solution and residual norms, the matrix norm, and the\ncondition number.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 02:02:10 GMT"}, {"version": "v2", "created": "Sun, 3 Apr 2011 20:20:17 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2015 00:50:31 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Choi", "Sou-Cheng T.", ""], ["Paige", "Christopher C.", ""], ["Saunders", "Michael A.", ""]]}, {"id": "1003.4306", "submitter": "Jonathan C. Mattingly", "authors": "Jonathan C. Mattingly, Natesh S. Pillai, Andrew M. Stuart", "title": "Diffusion limits of the random walk Metropolis algorithm in high\n  dimensions", "comments": "Published in at http://dx.doi.org/10.1214/10-AAP754 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2012, Vol. 22, No. 3, 881-930", "doi": "10.1214/10-AAP754", "report-no": "IMS-AAP-AAP754", "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion limits of MCMC methods in high dimensions provide a useful\ntheoretical tool for studying computational complexity. In particular, they\nlead directly to precise estimates of the number of steps required to explore\nthe target measure, in stationarity, as a function of the dimension of the\nstate space. However, to date such results have mainly been proved for target\nmeasures with a product structure, severely limiting their applicability. The\npurpose of this paper is to study diffusion limits for a class of naturally\noccurring high-dimensional measures found from the approximation of measures on\na Hilbert space which are absolutely continuous with respect to a Gaussian\nreference measure. The diffusion limit of a random walk Metropolis algorithm to\nan infinite-dimensional Hilbert space valued SDE (or SPDE) is proved,\nfacilitating understanding of the computational complexity of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2010 23:26:29 GMT"}, {"version": "v2", "created": "Sun, 14 Nov 2010 20:46:04 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2011 12:50:43 GMT"}, {"version": "v4", "created": "Thu, 4 Oct 2012 13:34:23 GMT"}], "update_date": "2012-10-05", "authors_parsed": [["Mattingly", "Jonathan C.", ""], ["Pillai", "Natesh S.", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1003.5338", "submitter": "Shaowei Lin", "authors": "Shaowei Lin", "title": "Ideal-Theoretic Strategies for Asymptotic Approximation of Marginal\n  Likelihood Integrals", "comments": "34 pages. Moved technical parts of the computation for the statistics\n  example to the appendix", "journal-ref": null, "doi": "10.18409/jas.v8i1.47", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate asymptotic evaluation of marginal likelihood integrals is a\nfundamental problem in Bayesian statistics. Following the approach introduced\nby Watanabe, we translate this into a problem of computational algebraic\ngeometry, namely, to determine the real log canonical threshold of a polynomial\nideal, and we present effective methods for solving this problem. Our results\nare based on resolution of singularities. They apply to parametric models where\nthe Kullback-Leibler distance is upper and lower bounded by scalar multiples of\nsome sum of squared real analytic functions. Such models include finite state\ndiscrete models.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2010 04:49:41 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2011 05:05:35 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 06:01:37 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Lin", "Shaowei", ""]]}, {"id": "1003.5851", "submitter": "Jean-Michel Marin", "authors": "Sophie Donnet and Jean-Michel Marin", "title": "An empirical Bayes procedure for the selection of Gaussian graphical\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new methodology for model determination in decomposable graphical Gaussian\nmodels is developed. The Bayesian paradigm is used and, for each given graph, a\nhyper inverse Wishart prior distribution on the covariance matrix is\nconsidered. This prior distribution depends on hyper-parameters. It is\nwell-known that the models's posterior distribution is sensitive to the\nspecification of these hyper-parameters and no completely satisfactory method\nis registered. In order to avoid this problem, we suggest adopting an empirical\nBayes strategy, that is a strategy for which the values of the hyper-parameters\nare determined using the data. Typically, the hyper-parameters are fixed to\ntheir maximum likelihood estimations. In order to calculate these maximum\nlikelihood estimations, we suggest a Markov chain Monte Carlo version of the\nStochastic Approximation EM algorithm. Moreover, we introduce a new sampling\nscheme in the space of graphs that improves the add and delete proposal of\nArmstrong et al. (2009). We illustrate the efficiency of this new scheme on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 15:52:04 GMT"}, {"version": "v2", "created": "Mon, 23 May 2011 10:47:25 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Donnet", "Sophie", ""], ["Marin", "Jean-Michel", ""]]}, {"id": "1003.5930", "submitter": "Mu Zhu", "authors": "Lu Xin, Mu Zhu", "title": "Stochastic Stepwise Ensembles for Variable Selection", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, June 2012, Vol.\n  21, No. 2, Pages 275 - 294", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we advocate the ensemble approach for variable selection. We\npoint out that the stochastic mechanism used to generate the variable-selection\nensemble (VSE) must be picked with care. We construct a VSE using a stochastic\nstepwise algorithm, and compare its performance with numerous state-of-the-art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2010 21:35:26 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2011 21:12:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Xin", "Lu", ""], ["Zhu", "Mu", ""]]}]