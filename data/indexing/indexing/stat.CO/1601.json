[{"id": "1601.00129", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Razvan Stefanescu, and Adrian Sandu", "title": "The Reduced-Order Hybrid Monte Carlo Sampling Smoother", "comments": "32 pages, 2 figures", "journal-ref": null, "doi": "10.1002/fld.4255", "report-no": "CSTR-1/2016", "categories": "cs.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Monte-Carlo (HMC) sampling smoother is a fully non-Gaussian\nfour-dimensional data assimilation algorithm that works by directly sampling\nthe posterior distribution formulated in the Bayesian framework. The smoother\nin its original formulation is computationally expensive due to the intrinsic\nrequirement of running the forward and adjoint models repeatedly. Here we\npresent computationally efficient versions of the HMC sampling smoother based\non reduced-order approximations of the underlying model dynamics. The schemes\ndeveloped herein are tested numerically using the shallow-water equations model\non Cartesian coordinates. The results reveal that the reduced-order versions of\nthe smoother are capable of accurately capturing the posterior probability\ndensity, while being significantly faster than the original full order\nformulation.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 01:55:07 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Attia", "Ahmed", ""], ["Stefanescu", "Razvan", ""], ["Sandu", "Adrian", ""]]}, {"id": "1601.00225", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "Identifying the Optimal Integration Time in Hamiltonian Monte Carlo", "comments": "31 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By leveraging the natural geometry of a smooth probabilistic system,\nHamiltonian Monte Carlo yields computationally efficient Markov Chain Monte\nCarlo estimation. At least provided that the algorithm is sufficiently\nwell-tuned. In this paper I show how the geometric foundations of Hamiltonian\nMonte Carlo implicitly identify the optimal choice of these parameters,\nespecially the integration time. I then consider the practical consequences of\nthese principles in both existing algorithms and a new implementation called\n\\emph{Exhaustive Hamiltonian Monte Carlo} before demonstrating the utility of\nthese ideas in some illustrative examples.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jan 2016 20:59:22 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1601.00630", "submitter": "Jeff M Phillips", "authors": "Kevin Buchin, Jeff M. Phillips, and Pingfan Tang", "title": "Approximating the Distribution of the Median and other Robust Estimators\n  on Uncertain Data", "comments": "Full version of a paper to appear at SoCG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimators, like the median of a point set, are important for data\nanalysis in the presence of outliers. We study robust estimators for\nlocationally uncertain points with discrete distributions. That is, each point\nin a data set has a discrete probability distribution describing its location.\nThe probabilistic nature of uncertain data makes it challenging to compute such\nestimators, since the true value of the estimator is now described by a\ndistribution rather than a single point. We show how to construct and estimate\nthe distribution of the median of a point set. Building the approximate support\nof the distribution takes near-linear time, and assigning probability to that\nsupport takes quadratic time. We also develop a general approximation technique\nfor distributions of robust estimators with respect to ranges with bounded VC\ndimension. This includes the geometric median for high dimensions and the\nSiegel estimator for linear regression.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 20:24:46 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 05:33:00 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Buchin", "Kevin", ""], ["Phillips", "Jeff M.", ""], ["Tang", "Pingfan", ""]]}, {"id": "1601.00670", "submitter": "Alp Kucukelbir", "authors": "David M. Blei, Alp Kucukelbir, Jon D. McAuliffe", "title": "Variational Inference: A Review for Statisticians", "comments": null, "journal-ref": "Journal of the American Statistical Association, Vol. 112 , Iss.\n  518, 2017", "doi": "10.1080/01621459.2017.1285773", "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the core problems of modern statistics is to approximate\ndifficult-to-compute probability densities. This problem is especially\nimportant in Bayesian statistics, which frames all inference about unknown\nquantities as a calculation involving the posterior density. In this paper, we\nreview variational inference (VI), a method from machine learning that\napproximates probability densities through optimization. VI has been used in\nmany applications and tends to be faster than classical methods, such as Markov\nchain Monte Carlo sampling. The idea behind VI is to first posit a family of\ndensities and then to find the member of that family which is close to the\ntarget. Closeness is measured by Kullback-Leibler divergence. We review the\nideas behind mean-field variational inference, discuss the special case of VI\napplied to exponential family models, present a full example with a Bayesian\nmixture of Gaussians, and derive a variant that uses stochastic optimization to\nscale up to massive data. We discuss modern research in VI and highlight\nimportant open problems. VI is powerful, but it is not yet well understood. Our\nhope in writing this paper is to catalyze statistical research on this class of\nalgorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jan 2016 21:28:04 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 20:33:40 GMT"}, {"version": "v3", "created": "Mon, 8 Aug 2016 15:57:26 GMT"}, {"version": "v4", "created": "Wed, 2 Nov 2016 17:58:48 GMT"}, {"version": "v5", "created": "Wed, 14 Jun 2017 13:44:33 GMT"}, {"version": "v6", "created": "Wed, 15 Nov 2017 20:13:02 GMT"}, {"version": "v7", "created": "Mon, 4 Dec 2017 23:07:00 GMT"}, {"version": "v8", "created": "Mon, 26 Mar 2018 01:40:27 GMT"}, {"version": "v9", "created": "Wed, 9 May 2018 20:52:28 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Blei", "David M.", ""], ["Kucukelbir", "Alp", ""], ["McAuliffe", "Jon D.", ""]]}, {"id": "1601.00773", "submitter": "Geoffrey McLachlan", "authors": "Geoffrey J. McLachlan, Sharon X. Lee", "title": "Comment on \"On Nomenclature, and the Relative Merits of Two Formulations\n  of Skew Distributions\" by A. Azzalini, R. Browne, M. Genton, and P.\n  McNicholas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We comment on the recent paper by Azzalini et al. (2015) on two different\ndistributions proposed in the literature for the modelling of data that have\nasymmetric and possibly long-tailed clusters. They are referred to as the\nrestricted and unrestricted skew normal and skew t-distributions by Lee and\nMcLachlan (2013a). We clarify an apparent misunderstanding in Azzalini et\nal.(2015) of this nomenclature to distinguish between these two models. Also,\nwe note that McLachlan and Lee (2014) have obtained improved results for the\nunrestricted model over those reported in Azzalini et al. (2015) for the two\ndatasets that were analysed by them to form the basis of their claimson the\nrelative superiority of the restricted and unrestricted models. On this matter\nof the relative superiority of these two models, Lee and McLachlan (2014b,\n2016) have shown how a distribution belonging to the broader class, the\ncanonical fundamental skew t (CFUST) class, can be fitted with little\nadditional computational effort than for the unrestricted distribution. The\nCFUST class includes the restricted and unrestricted distributions as special\ncases. Thus the user now has the option of letting the data decide as to which\nmodel is appropriate for their particular dataset.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jan 2016 09:10:58 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["McLachlan", "Geoffrey J.", ""], ["Lee", "Sharon X.", ""]]}, {"id": "1601.01125", "submitter": "Tore Selland Kleppe", "authors": "Oliver Grothe, Tore Selland Kleppe and Roman Liesenfeld", "title": "The Gibbs Sampler with Particle Efficient Importance Sampling for\n  State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Particle Gibbs (PG) as a tool for Bayesian analysis of non-linear\nnon-Gaussian state-space models. PG is a Monte Carlo (MC) approximation of the\nstandard Gibbs procedure which uses sequential MC (SMC) importance sampling\ninside the Gibbs procedure to update the latent and potentially\nhigh-dimensional state trajectories. We propose to combine PG with a generic\nand easily implementable SMC approach known as Particle Efficient Importance\nSampling (PEIS). By using SMC importance sampling densities which are\napproximately fully globally adapted to the targeted density of the states,\nPEIS can substantially improve the mixing and the efficiency of the PG draws\nfrom the posterior of the states and the parameters relative to existing PG\nimplementations. The efficiency gains achieved by PEIS are illustrated in PG\napplications to a univariate stochastic volatility model for asset returns, a\nnon-Gaussian nonlinear local-level model for interest rates, and a multivariate\nstochastic volatility model for the realized covariance matrix of asset\nreturns.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 10:19:15 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 10:38:05 GMT"}, {"version": "v3", "created": "Tue, 17 Apr 2018 10:53:35 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Grothe", "Oliver", ""], ["Kleppe", "Tore Selland", ""], ["Liesenfeld", "Roman", ""]]}, {"id": "1601.01178", "submitter": "Kaniav Kamary", "authors": "Kaniav Kamary (Universit\\'e Paris-Dauphine and INRIA), Jeong Eun Lee\n  (Auckland University of Technology), and Christian P. Robert (Universit\\'e\n  Paris-Dauphine and University of Warwick)", "title": "Weakly informative reparameterisations for location-scale mixtures", "comments": "32 pages, 14 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While mixtures of Gaussian distributions have been studied for more than a\ncentury (Pearson, 1894), the construction of a reference Bayesian analysis of\nthose models still remains unsolved, with a general prohibition of the usage of\nimproper priors (Fruwirth-Schnatter, 2006) due to the ill-posed nature of such\nstatistical objects. This difficulty is usually bypassed by an empirical Bayes\nresolution (Richardson and Green, 1997). By creating a new parameterisation\ncantered on the mean and possibly the variance of the mixture distribution\nitself, we manage to develop here a weakly informative prior for a wide class\nof mixtures with an arbitrary number of components. We demonstrate that some\nposterior distributions associated with this prior and a minimal sample size\nare proper. We provide MCMC implementations that exhibit the expected\nexchangeability. We only study here the univariate case, the extension to\nmultivariate location-scale mixtures being currently under study. An R package\ncalled Ultimixt is associated with this paper.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 13:58:56 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 09:31:03 GMT"}, {"version": "v3", "created": "Mon, 13 Mar 2017 08:25:33 GMT"}, {"version": "v4", "created": "Mon, 31 Jul 2017 09:41:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Kamary", "Kaniav", "", "Universit\u00e9 Paris-Dauphine and INRIA"], ["Lee", "Jeong Eun", "", "Auckland University of Technology"], ["Robert", "Christian P.", "", "Universit\u00e9\n  Paris-Dauphine and University of Warwick"]]}, {"id": "1601.01955", "submitter": "Satya Singh P", "authors": "Satya Prakash Singh and Siuli Mukhopadhyay", "title": "Bayesian Crossover Designs for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article discusses D-optimal Bayesian crossover designs for generalized\nlinear models. Crossover trials with t treatments and p periods, for $t <= p$,\nare considered. The designs proposed in this paper minimize the log determinant\nof the variance of the estimated treatment effects over all possible allocation\nof the n subjects to the treatment sequences. It is assumed that the p\nobservations from each subject are mutually correlated while the observations\nfrom different subjects are uncorrelated. Since main interest is in estimating\nthe treatment effects, the subject effect is assumed to be nuisance, and\ngeneralized estimating equations are used to estimate the marginal means. To\naddress the issue of parameter dependence a Bayesian approach is employed.\nPrior distributions are assumed on the model parameters which are then\nincorporated into the D-optimal design criterion by integrating it over the\nprior distribution. Three case studies, one with binary outcomes in a\n4$\\times$4 crossover trial, second one based on count data for a 2$\\times$2\ntrial and a third one with Gamma responses in a 3$times$2 crossover trial are\nused to illustrate the proposed method. The effect of the choice of prior\ndistributions on the designs is also studied.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2016 17:44:26 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 14:57:56 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Singh", "Satya Prakash", ""], ["Mukhopadhyay", "Siuli", ""]]}, {"id": "1601.02387", "submitter": "Guillaume Dehaene", "authors": "Guillaume P Dehaene and Simon Barthelm\\'e", "title": "Bounding errors of Expectation-Propagation", "comments": "Accepted and published at NIPS 2015", "journal-ref": "Advances in Neural Information Processing Systems 28, 244--252,\n  2015", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Propagation is a very popular algorithm for variational\ninference, but comes with few theoretical guarantees. In this article, we prove\nthat the approximation errors made by EP can be bounded. Our bounds have an\nasymptotic interpretation in the number $n$ of datapoints, which allows us to\nstudy EP's convergence with respect to the true posterior. In particular, we\nshow that EP converges at a rate of $\\mathcal{0}(n^{-2})$ for the mean, up to\nan order of magnitude faster than the traditional Gaussian approximation at the\nmode. We also give similar asymptotic expansions for moments of order 2 to 4,\nas well as excess Kullback-Leibler cost (defined as the additional KL cost\nincurred by using EP rather than the ideal Gaussian approximation). All these\nexpansions highlight the superior convergence properties of EP. Our approach\nfor deriving those results is likely applicable to many similar approximate\ninference methods. In addition, we introduce bounds on the moments of\nlog-concave distributions that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 10:34:21 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Dehaene", "Guillaume P", ""], ["Barthelm\u00e9", "Simon", ""]]}, {"id": "1601.02410", "submitter": "Wanchuang Zhu", "authors": "Wanchuang Zhu, Yanan Fan", "title": "A novel approach for Markov Random Field with intractable normalising\n  constant on large lattices", "comments": "24 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pseudo likelihood method of Besag(1974), has remained a popular method\nfor estimating Markov random field on a very large lattice, despite various\ndocumented deficiencies. This is partly because it remains the only\ncomputationally tractable method for large lattices. We introduce a novel\nmethod to estimate Markov random fields defined on a regular lattice. The\nmethod takes advantage of conditional independence structures and recursively\ndecomposes a large lattice into smaller sublattices. An approximation is made\nat each decomposition. Doing so completely avoids the need to compute the\ntroublesome normalising constant. The computational complexity is $O(N)$, where\n$N$ is the the number of pixels in lattice, making it computationally\nattractive for very large lattices. We show through simulation, that the\nproposed method performs well, even when compared to the methods using exact\nlikelihoods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 12:05:01 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Zhu", "Wanchuang", ""], ["Fan", "Yanan", ""]]}, {"id": "1601.02557", "submitter": "Julien Bect", "authors": "Julien Bect (L2S, GdR MASCOT-NUM), Ling Li (L2S, GdR MASCOT-NUM),\n  Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "Bayesian subset simulation", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, 5(1):762-786, 2017", "doi": "10.1137/16M1078276", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a probability of failure $\\alpha$,\ndefined as the volume of the excursion set of a function $f:\\mathbb{X}\n\\subseteq \\mathbb{R}^{d} \\to \\mathbb{R}$ above a given threshold, under a given\nprobability measure on $\\mathbb{X}$. In this article, we combine the popular\nsubset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our\nsequential Bayesian approach for the estimation of a probability of failure\n(Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it\npossible to estimate $\\alpha$ when the number of evaluations of $f$ is very\nlimited and $\\alpha$ is very small. The resulting algorithm is called Bayesian\nsubset simulation (BSS). A key idea, as in the subset simulation algorithm, is\nto estimate the probabilities of a sequence of excursion sets of $f$ above\nintermediate thresholds, using a sequential Monte Carlo (SMC) approach. A\nGaussian process prior on $f$ is used to define the sequence of densities\ntargeted by the SMC algorithm, and drive the selection of evaluation points of\n$f$ to estimate the intermediate probabilities. Adaptive procedures are\nproposed to determine the intermediate thresholds and the number of evaluations\nto be carried out at each stage of the algorithm. Numerical experiments\nillustrate that BSS achieves significant savings in the number of function\nevaluations with respect to other Monte Carlo approaches.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jan 2016 19:07:49 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 15:39:44 GMT"}, {"version": "v3", "created": "Mon, 24 Apr 2017 12:38:59 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Li", "Ling", "", "L2S, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "1601.02698", "submitter": "Daniel Turek", "authors": "Daniel Turek, Perry de Valpine, Christopher J. Paciorek", "title": "Efficient Markov Chain Monte Carlo Sampling for Hierarchical Hidden\n  Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Markov chain Monte Carlo (MCMC) sampling of hidden Markov models\n(HMMs) involves latent states underlying an imperfect observation process, and\ngenerates posterior samples for top-level parameters concurrently with nuisance\nlatent variables. When potentially many HMMs are embedded within a hierarchical\nmodel, this can result in prohibitively long MCMC runtimes. We study\ncombinations of existing methods, which are shown to vastly improve\ncomputational efficiency for these hierarchical models while maintaining the\nmodeling flexibility provided by embedded HMMs. The methods include discrete\nfiltering of the HMM likelihood to remove latent states, reduced data\nrepresentations, and a novel procedure for dynamic block sampling of posterior\ndimensions. The first two methods have been used in isolation in existing\napplication-specific software, but are not generally available for\nincorporation in arbitrary model structures. Using the NIMBLE package for R, we\ndevelop and test combined computational approaches using three examples from\necological capture-recapture, although our methods are generally applicable to\nany embedded discrete HMMs. These combinations provide several orders of\nmagnitude improvement in MCMC sampling efficiency, defined as the rate of\ngenerating effectively independent posterior samples. In addition to being\ncomputationally significant for this class of hierarchical models, this result\nunderscores the potential for vast improvements to MCMC sampling efficiency\nwhich can result from combinations of known algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 00:09:18 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Turek", "Daniel", ""], ["de Valpine", "Perry", ""], ["Paciorek", "Christopher J.", ""]]}, {"id": "1601.02748", "submitter": "Gregory Giecold", "authors": "Gregory Giecold, Eugenio Marco, Lorenzo Trippa and Guo-Cheng Yuan", "title": "Robust Lineage Reconstruction from High-Dimensional Single-Cell Data", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell gene expression data provide invaluable resources for systematic\ncharacterization of cellular hierarchy in multi-cellular organisms. However,\ncell lineage reconstruction is still often associated with significant\nuncertainty due to technological constraints. Such uncertainties have not been\ntaken into account in current methods. We present ECLAIR, a novel computational\nmethod for the statistical inference of cell lineage relationships from\nsingle-cell gene expression data. ECLAIR uses an ensemble approach to improve\nthe robustness of lineage predictions, and provides a quantitative estimate of\nthe uncertainty of lineage branchings. We show that the application of ECLAIR\nto published datasets successfully reconstructs known lineage relationships and\nsignificantly improves the robustness of predictions. In conclusion, ECLAIR is\na powerful bioinformatics tool for single-cell data analysis. It can be used\nfor robust lineage reconstruction with quantitative estimate of prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jan 2016 07:01:55 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Giecold", "Gregory", ""], ["Marco", "Eugenio", ""], ["Trippa", "Lorenzo", ""], ["Yuan", "Guo-Cheng", ""]]}, {"id": "1601.03443", "submitter": "Andrew Gelman", "authors": "Robert L. Grant and Daniel C. Furr and Bob Carpenter and Andrew Gelman", "title": "Fitting Bayesian item response models in Stata and Stan", "comments": "12 pages, 3 figures, 2 tables, some code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stata users have access to two easy-to-use implementations of Bayesian\ninference: Stata's native {\\tt bayesmh} function and StataStan, which calls the\ngeneral Bayesian engine Stan. We compare these on two models that are important\nfor education research: the Rasch model and the hierarchical Rasch model. Stan\n(as called from Stata) fits a more general range of models than can be fit by\n{\\tt bayesmh} and is also more scalable, in that it could easily fit models\nwith at least ten times more parameters than could be fit using Stata's native\nBayesian implementation. In addition, Stan runs between two and ten times\nfaster than {\\tt bayesmh} as measured in effective sample size per second: that\nis, compared to Stan, it takes Stata's built-in Bayesian engine twice to ten\ntimes as long to get inferences with equivalent precision. We attribute Stan's\nadvantage in flexibility to its general modeling language, and its advantages\nin scalability and speed to an efficient sampling algorithm: Hamiltonian Monte\nCarlo using the no-U-turn sampler. In order to further investigate scalability,\nwe also compared to the package Jags, which performed better than Stata's\nnative Bayesian engine but not as well as StataStan.\n  Given its advantages in speed, generality, and scalability, and that Stan is\nopen-source and can be run directly from Stata using StataStan, we recommend\nthat Stata users adopt Stan as their Bayesian inference engine of choice.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 23:14:04 GMT"}, {"version": "v2", "created": "Wed, 14 Dec 2016 04:13:49 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Grant", "Robert L.", ""], ["Furr", "Daniel C.", ""], ["Carpenter", "Bob", ""], ["Gelman", "Andrew", ""]]}, {"id": "1601.03704", "submitter": "Florencia Leonardi", "authors": "Florencia Leonardi and Peter B\\\"uhlmann", "title": "Computationally efficient change point detection for high-dimensional\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale sequential data is often exposed to some degree of inhomogeneity\nin the form of sudden changes in the parameters of the data-generating process.\nWe consider the problem of detecting such structural changes in a\nhigh-dimensional regression setting. We propose a joint estimator of the number\nand the locations of the change points and of the parameters in the\ncorresponding segments. The estimator can be computed using dynamic programming\nor, as we emphasize here, it can be approximated using a binary search\nalgorithm with $O(n \\log(n) \\mathrm{Lasso}(n))$ computational operations while\nstill enjoying essentially the same theoretical properties; here\n$\\mathrm{Lasso}(n)$ denotes the computational cost of computing the Lasso for\nsample size $n$. We establish oracle inequalities for the estimator as well as\nfor its binary search approximation, covering also the case with a large\n(asymptotically growing) number of change points. We evaluate the performance\nof the proposed estimation algorithms on simulated data and apply the\nmethodology to real data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jan 2016 19:30:44 GMT"}], "update_date": "2016-01-15", "authors_parsed": [["Leonardi", "Florencia", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1601.04065", "submitter": "Alexander Gribov", "authors": "Alexander Gribov", "title": "Efficient Kernel Convolution for Smooth Surfaces without Edge Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most efficient ways to produce unconditional simulations is with\nthe kernel convolution using fast Fourier transform (FFT) [1]. However, when\ndata is located on a surface, this approach is not efficient because data needs\nto be processed in a three-dimensional enclosing box. This paper describes a\nnovel approach based on integer transformation to reduce the volume of the\nenclosing box.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jan 2016 21:05:46 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["Gribov", "Alexander", ""]]}, {"id": "1601.04262", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Exact Distribution of the Generalized Shiryaev-Roberts Stopping Time\n  Under the Minimax Brownian Motion Setup", "comments": "39 pages; 13 figures", "journal-ref": "Sequential Analysis, vol. 35, no. 1, pp. 108-143, 2016", "doi": "10.1080/07474946.2016.1132066", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the quickest change-point detection problem where the aim is to\ndetect the onset of a pre-specified drift in \"live\"-monitored standard Brownian\nmotion; the change-point is assumed unknown (nonrandom). The object of interest\nis the distribution of the stopping time associated with the Generalized\nShryaev-Roberts (GSR) detection procedure set up to \"sense\" the presence of the\ndrift in the Brownian motion under surveillance. Specifically, we seek the GSR\nstopping time's survival function (the tail probability that no alarm is\ntriggered by the GSR procedure prior to a given point in time), and distinguish\ntwo scenarios: (a) when the drift never sets in (pre-change regime) and (b)\nwhen the drift is in effect ab initio (post-change regime). Under each\nscenario, we obtain a closed-form formula for the respective survival function,\nwith the GSR statistic's (deterministic) nonnegative headstart assumed\narbitrarily given. The two formulae are found analytically, through direct\nsolution of the respective Kolmogorov forward equation via the Fourier spectral\nmethod to achieve separation of the spacial and temporal variables. We then\nexploit the obtained formulae numerically and characterize the pre- and\npost-change distributions of the GSR stopping time depending on three factors:\n(a) magnitude of the drift, (b) detection threshold, and (c) the GSR\nstatistic's headstart.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jan 2016 08:37:06 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2016 18:27:49 GMT"}], "update_date": "2016-04-19", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1601.05011", "submitter": "Tristan van Leeuwen", "authors": "Tristan van Leeuwen and Aleksandr Aravkin", "title": "Non-smooth Variable Projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable projection solves structured optimization problems by completely\nminimizing over a subset of the variables while iterating over the remaining\nvariables. Over the last 30 years, the technique has been widely used, with\nempirical and theoretical results demonstrating both greater efficacy and\ngreater stability compared to competing approaches. Classic examples have\nexploited closed-form projections and smoothness of the objective function. We\nextend the approach to problems that include non-smooth terms, and where the\nprojection subproblems can only be solved inexactly by iterative methods. We\npropose an inexact adaptive algonrithm for solving such problems and analyze\nits computational complexity. Finally, we show how the theory can be used to\ndesign methods for selected problems occurring frequently in machine-learning\nand inverse problems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jan 2016 17:38:29 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 20:40:06 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2018 14:43:25 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 12:40:01 GMT"}, {"version": "v5", "created": "Tue, 30 Jun 2020 11:41:26 GMT"}, {"version": "v6", "created": "Fri, 20 Nov 2020 14:54:07 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["van Leeuwen", "Tristan", ""], ["Aravkin", "Aleksandr", ""]]}, {"id": "1601.05568", "submitter": "Johan Westerborn", "authors": "Jimmy Olsson and Johan Westerborn", "title": "Efficient parameter inference in general hidden Markov models using the\n  filter derivatives", "comments": "5 pages, changed typo in Algorithm in section 4.1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating online the parameters of general state-space hidden Markov models\nis a topic of importance in many scientific and engineering disciplines. In\nthis paper we present an online parameter estimation algorithm obtained by\ncasting our recently proposed particle-based, rapid incremental smoother\n(PaRIS) into the framework of recursive maximum likelihood estimation for\ngeneral hidden Markov models. Previous such particle implementations suffer\nfrom either quadratic complexity in the number of particles or from the\nwell-known degeneracy of the genealogical particle paths. By using the\ncomputational efficient and numerically stable PaRIS algorithm for estimating\nthe needed prediction filter derivatives we obtain a fast algorithm with a\ncomputational complexity that grows only linearly with the number of particles.\nThe efficiency and stability of the proposed algorithm are illustrated in a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 09:50:27 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 17:13:41 GMT"}], "update_date": "2016-02-25", "authors_parsed": [["Olsson", "Jimmy", ""], ["Westerborn", "Johan", ""]]}, {"id": "1601.05842", "submitter": "Kinjal Basu", "authors": "Kinjal Basu, Rajarshi Mukherjee", "title": "Asymptotic Normality of Scrambled Geometric Net Quadrature", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a very recent work, Basu and Owen (2015) propose the use of scrambled\ngeometric nets in numerical integration when the domain is a product of $s$\narbitrary spaces of dimension $d$ having a certain partitioning constraint. It\nwas shown that for a class of smooth functions, the integral estimate has\nvariance $O( n^{-1 -2/d} (\\log n)^{s-1})$ for scrambled geometric nets,\ncompared to $O(n^{-1})$ for ordinary Monte Carlo.\n  The main idea of this paper is to develop on the work by Loh (2003), to show\nthat the scrambled geometric net estimate has an asymptotic normal distribution\nfor certain smooth functions defined on products of suitable subsets of\n$\\mathbb{R}^d$.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2016 23:19:47 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2016 01:23:10 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 02:59:46 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Basu", "Kinjal", ""], ["Mukherjee", "Rajarshi", ""]]}, {"id": "1601.05870", "submitter": "Olivier Ledoit", "authors": "Olivier Ledoit and Michael Wolf", "title": "Numerical Implementation of the QuEST Function", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": "University of Zurich Department of Economics Working Paper No. 215", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with certain estimation problems involving the covariance\nmatrix in large dimensions. Due to the breakdown of finite-dimensional\nasymptotic theory when the dimension is not negligible with respect to the\nsample size, it is necessary to resort to an alternative framework known as\nlarge-dimensional asymptotics. Recently, Ledoit and Wolf (2015) have proposed\nan estimator of the eigenvalues of the population covariance matrix that is\nconsistent according to a mean-square criterion under large-dimensional\nasymptotics. It requires numerical inversion of a multivariate nonrandom\nfunction which they call the QuEST function. The present paper explains how to\nnumerically implement the QuEST function in practice through a series of six\nsuccessive steps. It also provides an algorithm to compute the Jacobian\nanalytically, which is necessary for numerical inversion by a nonlinear\noptimizer. Monte Carlo simulations document the effectiveness of the code.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jan 2016 03:17:30 GMT"}], "update_date": "2016-01-25", "authors_parsed": [["Ledoit", "Olivier", ""], ["Wolf", "Michael", ""]]}, {"id": "1601.07622", "submitter": "Mahdi Alavi", "authors": "Pierre E. Jacob, S.M.Mahdi Alavi, Adam Mahdi, Stephen J. Payne, and\n  David A. Howey", "title": "Bayesian inference in non-Markovian state-space models with applications\n  to fractional order systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Battery impedance spectroscopy models are given by fractional order (FO)\ndifferential equations. In the discrete-time domain, they give rise to\nstate-space models where the latent process is not Markovian. Parameter\nestimation for these models is therefore challenging, especially for\nnon-commensurate FO models. In this paper, we propose a Bayesian approach to\nidentify the parameters of generic FO systems. The computational challenge is\ntackled with particle Markov chain Monte Carlo methods, with an implementation\nspecifically designed for the non-Markovian setting. The approach is then\napplied to estimate the parameters of a battery non-commensurate FO equivalent\ncircuit model. Extensive simulations are provided to study the practical\nidentifiability of model parameters and their sensitivity to the choice of\nprior distributions, the number of observations, the magnitude of the input\nsignal and the measurement noise.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jan 2016 02:24:53 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Alavi", "S. M. Mahdi", ""], ["Mahdi", "Adam", ""], ["Payne", "Stephen J.", ""], ["Howey", "David A.", ""]]}, {"id": "1601.08057", "submitter": "Samuel Livingstone", "authors": "Samuel Livingstone, Michael Betancourt, Simon Byrne and Mark Girolami", "title": "On the Geometric Ergodicity of Hamiltonian Monte Carlo", "comments": "29 pages + supplement (included in arXival as Appendix), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish general conditions under which Markov chains produced by the\nHamiltonian Monte Carlo method will and will not be geometrically ergodic. We\nconsider implementations with both position-independent and position-dependent\nintegration times. In the former case we find that the conditions for geometric\nergodicity are essentially a gradient of the log-density which asymptotically\npoints towards the centre of the space and grows no faster than linearly. In an\nidealised scenario in which the integration time is allowed to change in\ndifferent regions of the space, we show that geometric ergodicity can be\nrecovered for a much broader class of tail behaviours, leading to some\nguidelines for the choice of this free parameter in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jan 2016 11:13:46 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 13:13:20 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 11:08:32 GMT"}, {"version": "v4", "created": "Fri, 16 Nov 2018 11:32:00 GMT"}], "update_date": "2018-11-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Betancourt", "Michael", ""], ["Byrne", "Simon", ""], ["Girolami", "Mark", ""]]}]