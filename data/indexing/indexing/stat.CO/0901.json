[{"id": "0901.0017", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien, Alfred Hero and Herv\\'e Perdry", "title": "Space Alternating Penalized Kullback Proximal Point Algorithms for\n  Maximizing Likelihood with Nondifferentiable Penalty", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The EM algorithm is a widely used methodology for penalized likelihood\nestimation. Provable monotonicity and convergence are the hallmarks of the EM\nalgorithm and these properties are well established for smooth likelihood and\nsmooth penalty functions. However, many relaxed versions of variable selection\npenalties are not smooth. The goal of this paper is to introduce a new class of\nSpace Alternating Penalized Kullback Proximal extensions of the EM algorithm\nfor nonsmooth likelihood inference. We show that the cluster points of the new\nmethod are stationary points even when on the boundary of the parameter set.\nSpecial attention has been paid to the construction of component-wise version\nof the method in order to ease the implementation for complicated models.\nIllustration for the problems of model selection for finite mixtures of\nregression and to sparse image reconstruction is presented.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2008 22:41:05 GMT"}, {"version": "v2", "created": "Tue, 2 Nov 2010 21:53:08 GMT"}, {"version": "v3", "created": "Wed, 1 Jun 2011 13:22:45 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Hero", "Alfred", ""], ["Perdry", "Herv\u00e9", ""]]}, {"id": "0901.0225", "submitter": "Robert Kohn", "authors": "Paolo Giordani, Xiuyan Mun and Robert Kohn", "title": "Flexible Multivariate Density Estimation with Marginal Adaptation", "comments": "23 pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article addresses the problem of flexibly estimating a multivariate\ndensity while also attempting to estimate its marginals correctly. We do so by\nproposing two new estimators that try to capture the best features of mixture\nof normals and copula estimators while avoiding some of their weaknesses. The\nfirst estimator we propose is a mixture of normals copula model that is a\nflexible alternative to parametric copula models such as the normal and t\ncopula. The second is a marginally adapted mixture of normals estimator that\nimproves on the standard mixture of normals by using information contained in\nunivariate estimates of the marginal densities. We show empirically that copula\nbased approaches can behave much better or much worse than estimators based on\nmixture of normals depending on the properties of the data. We provide fast and\nreliable implementations of the estimators and illustrate the methodology on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2009 09:27:50 GMT"}], "update_date": "2009-01-05", "authors_parsed": [["Giordani", "Paolo", ""], ["Mun", "Xiuyan", ""], ["Kohn", "Robert", ""]]}, {"id": "0901.0401", "submitter": "Adom Giffin", "authors": "Adom Giffin", "title": "From Physics to Economics: An Econometric Example Using Maximum Relative\n  Entropy", "comments": "This paper has been accepted in Physica A. 19 Pages, 3 Figures", "journal-ref": "Physica A 388 (2009), pp. 1610-1620", "doi": "10.1016/j.physa.2008.12.066", "report-no": null, "categories": "q-fin.ST cs.IT math.IT physics.data-an physics.pop-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Econophysics, is based on the premise that some ideas and methods from\nphysics can be applied to economic situations. We intend to show in this paper\nhow a physics concept such as entropy can be applied to an economic problem. In\nso doing, we demonstrate how information in the form of observable data and\nmoment constraints are introduced into the method of Maximum relative Entropy\n(MrE). A general example of updating with data and moments is shown. Two\nspecific econometric examples are solved in detail which can then be used as\ntemplates for real world problems. A numerical example is compared to a large\ndeviation solution which illustrates some of the advantages of the MrE method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2009 21:37:04 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Giffin", "Adom", ""]]}, {"id": "0901.0638", "submitter": "William Shaw", "authors": "William T. Shaw, Thomas Luu, Nick Brickman", "title": "Quantile Mechanics II: Changes of Variables in Monte Carlo methods and\n  GPU-Optimized Normal Quantiles", "comments": "This revision adds substantial discussion of precision and\n  optimization issues, new code for float and double precision operation.\n  Timings for GTX 285, 480, Quadro 4000, Tesla C2050, and comparisons with most\n  major competing approaches", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.RM q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents differential equations and solution methods for the\nfunctions of the form $Q(x) = F^{-1}(G(x))$, where $F$ and $G$ are cumulative\ndistribution functions. Such functions allow the direct recycling of Monte\nCarlo samples from one distribution into samples from another. The method may\nbe developed analytically for certain special cases, and illuminate the idea\nthat it is a more precise form of the traditional Cornish-Fisher expansion. In\nthis manner the model risk of distributional risk may be assessed free of the\nMonte Carlo noise associated with resampling. Examples are given of equations\nfor converting normal samples to Student t, and converting exponential to\nhyperbolic, variance gamma and normal. In the case of the normal distribution,\nthe change of variables employed allows the sampling to take place to good\naccuracy based on a single rational approximation over a very wide range of the\nsample space. The avoidance of any branching statement is of use in optimal GPU\ncomputations as it avoids the effect of {\\it warp divergence}, and we give\nexamples of branch-free normal quantiles that offer performance improvements in\na GPU environment, while retaining the best precision characteristics of\nwell-known methods. We also offer models based on a low-probability of warp\ndivergence. Comparisons of new and old forms are made on the Nvidia Quadro\n4000, GTX 285 and 480, and Tesla C2050 GPUs. We argue that in single-precision\nmode, the change-of-variables approach offers performance competitive with the\nfastest existing scheme while substantially improving precision, and that in\ndouble-precision mode, this approach offers the most GPU-optimal Gaussian\nquantile yet, and without compromise on precision for Monte Carlo applications,\nworking twice as fast as the CUDA 4 library function with increased precision.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2009 12:14:25 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2009 17:51:40 GMT"}, {"version": "v3", "created": "Sun, 15 Feb 2009 10:37:04 GMT"}, {"version": "v4", "created": "Thu, 26 Aug 2010 14:20:22 GMT"}, {"version": "v5", "created": "Wed, 7 Dec 2011 19:16:30 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Shaw", "William T.", ""], ["Luu", "Thomas", ""], ["Brickman", "Nick", ""]]}, {"id": "0901.0876", "submitter": "Leonidas Pitsoulis", "authors": "L. Pitsoulis and G. Zioutas", "title": "A Fast Algorithm for Robust Regression with Penalised Trimmed Squares", "comments": "27 pages", "journal-ref": "Computational Statistics, Volume 25, Number 4, 663-689, 2010", "doi": "10.1007/s00180-010-0196-2", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of groups containing high leverage outliers makes linear\nregression a difficult problem due to the masking effect. The available high\nbreakdown estimators based on Least Trimmed Squares often do not succeed in\ndetecting masked high leverage outliers in finite samples.\n  An alternative to the LTS estimator, called Penalised Trimmed Squares (PTS)\nestimator, was introduced by the authors in \\cite{ZiouAv:05,ZiAvPi:07} and it\nappears to be less sensitive to the masking problem. This estimator is defined\nby a Quadratic Mixed Integer Programming (QMIP) problem, where in the objective\nfunction a penalty cost for each observation is included which serves as an\nupper bound on the residual error for any feasible regression line. Since the\nPTS does not require presetting the number of outliers to delete from the data\nset, it has better efficiency with respect to other estimators. However, due to\nthe high computational complexity of the resulting QMIP problem, exact\nsolutions for moderately large regression problems is infeasible.\n  In this paper we further establish the theoretical properties of the PTS\nestimator, such as high breakdown and efficiency, and propose an approximate\nalgorithm called Fast-PTS to compute the PTS estimator for large data sets\nefficiently. Extensive computational experiments on sets of benchmark instances\nwith varying degrees of outlier contamination, indicate that the proposed\nalgorithm performs well in identifying groups of high leverage outliers in\nreasonable computational time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2009 17:07:52 GMT"}], "update_date": "2011-03-23", "authors_parsed": [["Pitsoulis", "L.", ""], ["Zioutas", "G.", ""]]}, {"id": "0901.1378", "submitter": "Yves F. Atchad\\'{e}", "authors": "Yves F. Atchad\\'e", "title": "A cautionary tale on the efficiency of some adaptive Monte Carlo schemes", "comments": "Published in at http://dx.doi.org/10.1214/09-AAP636 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2010, Vol. 20, No. 3, 841-868", "doi": "10.1214/09-AAP636", "report-no": "IMS-AAP-AAP636", "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the literature for adaptive Markov chain Monte\nCarlo methods based on sequences of random transition kernels $\\{P_n\\}$ where\nthe kernel $P_n$ is allowed to have an invariant distribution $\\pi_n$ not\nnecessarily equal to the distribution of interest $\\pi$ (target distribution).\nThese algorithms are designed such that as $n\\to\\infty$, $P_n$ converges to\n$P$, a kernel that has the correct invariant distribution $\\pi$. Typically, $P$\nis a kernel with good convergence properties, but one that cannot be directly\nimplemented. It is then expected that the algorithm will inherit the good\nconvergence properties of $P$. The equi-energy sampler of [Ann. Statist. 34\n(2006) 1581--1619] is an example of this type of adaptive MCMC. We show in this\npaper that the asymptotic variance of this type of adaptive MCMC is always at\nleast as large as the asymptotic variance of the Markov chain with transition\nkernel $P$. We also show by simulation that the difference can be substantial.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2009 14:16:39 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2009 01:00:29 GMT"}, {"version": "v3", "created": "Fri, 15 Oct 2010 07:07:56 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Atchad\u00e9", "Yves F.", ""]]}, {"id": "0901.1925", "submitter": "Tina Toni", "authors": "Tina Toni, David Welch, Natalja Strelkowa, Andreas Ipsen, Michael P.H.\n  Stumpf", "title": "Approximate Bayesian computation scheme for parameter inference and\n  model selection in dynamical systems", "comments": "26 pages, 9 figures", "journal-ref": "Journal of the Royal Society Interface, Volume 6, Number 31, 2009,\n  pages 187-202", "doi": "10.1098/rsif.2008.0172", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation methods can be used to evaluate posterior\ndistributions without having to calculate likelihoods. In this paper we discuss\nand apply an approximate Bayesian computation (ABC) method based on sequential\nMonte Carlo (SMC) to estimate parameters of dynamical models. We show that ABC\nSMC gives information about the inferability of parameters and model\nsensitivity to changes in parameters, and tends to perform better than other\nABC approaches. The algorithm is applied to several well known biological\nsystems, for which parameters and their credible intervals are inferred.\nMoreover, we develop ABC SMC as a tool for model selection; given a range of\ndifferent mathematical descriptions, ABC SMC is able to choose the best model\nusing the standard Bayesian model selection apparatus.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2009 03:45:46 GMT"}], "update_date": "2009-01-15", "authors_parsed": [["Toni", "Tina", ""], ["Welch", "David", ""], ["Strelkowa", "Natalja", ""], ["Ipsen", "Andreas", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "0901.2231", "submitter": "Christoph Leuenberger", "authors": "Christoph Leuenberger Daniel Wegmann Laurent Excoffier", "title": "Bayesian Computation and Model Selection in Population Genetics", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently, the use of Bayesian inference in population genetics was\nlimited to a few cases because for many realistic population genetic models the\nlikelihood function cannot be calculated analytically . The situation changed\nwith the advent of likelihood-free inference algorithms, often subsumed under\nthe term Approximate Bayesian Computation (ABC). A key innovation was the use\nof a post-sampling regression adjustment, allowing larger tolerance values and\nas such shifting computation time to realistic orders of magnitude (see\nBeaumont et al., 2002). Here we propose a reformulation of the regression\nadjustment in terms of a General Linear Model (GLM). This allows the\nintegration into the framework of Bayesian statistics and the use of its\nmethods, including model selection via Bayes factors. We then apply the\nproposed methodology to the question of population subdivision among western\nchimpanzees Pan troglodytes verus.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2009 11:15:27 GMT"}], "update_date": "2009-01-16", "authors_parsed": [["Excoffier", "Christoph Leuenberger Daniel Wegmann Laurent", ""]]}, {"id": "0901.3749", "submitter": "Alexander Sch\\\"onhuth", "authors": "Alexander Schoenhuth", "title": "Equations for hidden Markov models", "comments": "28 pages; Results presented at the Workshop on Algebraic Statistics,\n  MSRI, UC Berkeley, Dec. 2008. Simplified arguments", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We will outline novel approaches to derive model invariants for hidden Markov\nand related models. These approaches are based on a theoretical framework that\narises from viewing random processes as elements of the vector space of string\nfunctions. Theorems available from that framework then give rise to novel ideas\nto obtain model invariants for hidden Markov and related models.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2009 18:50:54 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2009 01:19:42 GMT"}], "update_date": "2009-02-08", "authors_parsed": [["Schoenhuth", "Alexander", ""]]}, {"id": "0901.4192", "submitter": "Danny Bickson", "authors": "Jason K. Johnson, Danny Bickson and Danny Dolev", "title": "Fixing Convergence of Gaussian Belief Propagation", "comments": "In the IEEE International Symposium on Information Theory (ISIT)\n  2009, Seoul, South Korea, July 2009", "journal-ref": null, "doi": "10.1109/ISIT.2009.5205777", "report-no": null, "categories": "cs.IT cs.LG math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian belief propagation (GaBP) is an iterative message-passing algorithm\nfor inference in Gaussian graphical models. It is known that when GaBP\nconverges it converges to the correct MAP estimate of the Gaussian random\nvector and simple sufficient conditions for its convergence have been\nestablished. In this paper we develop a double-loop algorithm for forcing\nconvergence of GaBP. Our method computes the correct MAP estimate even in cases\nwhere standard GaBP would not have converged. We further extend this\nconstruction to compute least-squares solutions of over-constrained linear\nsystems. We believe that our construction has numerous applications, since the\nGaBP algorithm is linked to solution of linear systems of equations, which is a\nfundamental problem in computer science and engineering. As a case study, we\ndiscuss the linear detection problem. We show that using our new construction,\nwe are able to force convergence of Montanari's linear detection algorithm, in\ncases where it would originally fail. As a consequence, we are able to increase\nsignificantly the number of users that can transmit concurrently.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 08:24:57 GMT"}, {"version": "v2", "created": "Sat, 9 May 2009 07:23:49 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2009 03:25:13 GMT"}], "update_date": "2010-03-23", "authors_parsed": [["Johnson", "Jason K.", ""], ["Bickson", "Danny", ""], ["Dolev", "Danny", ""]]}, {"id": "0901.4203", "submitter": "Thomas Brendan Murphy", "authors": "Isobel Claire Gormley, Thomas Brendan Murphy", "title": "A mixture of experts model for rank data with applications in election\n  studies", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS178 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 4, 1452-1477", "doi": "10.1214/08-AOAS178", "report-no": "IMS-AOAS-AOAS178", "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A voting bloc is defined to be a group of voters who have similar voting\npreferences. The cleavage of the Irish electorate into voting blocs is of\ninterest. Irish elections employ a ``single transferable vote'' electoral\nsystem; under this system voters rank some or all of the electoral candidates\nin order of preference. These rank votes provide a rich source of preference\ninformation from which inferences about the composition of the electorate may\nbe drawn. Additionally, the influence of social factors or covariates on the\nelectorate composition is of interest. A mixture of experts model is a mixture\nmodel in which the model parameters are functions of covariates. A mixture of\nexperts model for rank data is developed to provide a model-based method to\ncluster Irish voters into voting blocs, to examine the influence of social\nfactors on this clustering and to examine the characteristic preferences of the\nvoting blocs. The Benter model for rank data is employed as the family of\ncomponent densities within the mixture of experts model; generalized linear\nmodel theory is employed to model the influence of covariates on the mixing\nproportions. Model fitting is achieved via a hybrid of the EM and MM\nalgorithms. An example of the methodology is illustrated by examining an Irish\npresidential election. The existence of voting blocs in the electorate is\nestablished and it is determined that age and government satisfaction levels\nare important factors in influencing voting in this election.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2009 09:16:05 GMT"}], "update_date": "2014-02-26", "authors_parsed": [["Gormley", "Isobel Claire", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "0901.4752", "submitter": "Stephane Chretien", "authors": "Stephane Chretien", "title": "Estimation of Gaussian mixtures in small sample studies using $l_1$\n  penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments in medicine and ecology can be conveniently modeled by\nfinite Gaussian mixtures but face the problem of dealing with small data sets.\nWe propose a robust version of the estimator based on self-regression and\nsparsity promoting penalization in order to estimate the components of Gaussian\nmixtures in such contexts. A space alternating version of the penalized EM\nalgorithm is obtained and we prove that its cluster points satisfy the\nKarush-Kuhn-Tucker conditions. Monte Carlo experiments are presented in order\nto compare the results obtained by our method and by standard maximum\nlikelihood estimation. In particular, our estimator is seen to perform better\nthan the maximum likelihood estimator.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2009 19:21:33 GMT"}, {"version": "v2", "created": "Wed, 8 Oct 2014 16:55:20 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Chretien", "Stephane", ""]]}]