[{"id": "2107.00007", "submitter": "Walter Schneider", "authors": "Walter Schneider", "title": "On the distribution of the sum of dependent standard normally\n  distributed random variables using copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The distribution function of the sum $Z$ of two standard normally distributed\nrandom variables $X$ and $Y$ is computed with the concept of copulas to model\nthe dependency between $X$ and $Y$. By using implicit copulas such as the\nGauss- or t-copula as well as Archimedean Copulas such as the Clayton-, Gumbel-\nor Frank-copula, a wide variety of different dependencies can be covered. For\neach of these copulas an analytical closed form expression for the\ncorresponding joint probability density function $f_{X,Y}$ is derived. We apply\na numerical approximation algorithm in Matlab to evaluate the resulting double\nintegral for the cumulative distribution function $F_Z$. Our results\ndemonstrate, that there are significant differencies amongst the various\ncopulas concerning $F_Z$. This is particularly true for the higher quantiles\n(e.g. $0.95, 0.99$), where deviations of more than $10\\%$ have been noticed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:57:43 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Schneider", "Walter", ""]]}, {"id": "2107.00153", "submitter": "Min Xu", "authors": "Harry Crane and Min Xu", "title": "Root and community inference on the latent growth process of a network\n  using noisy attachment models", "comments": "52 pages; 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the PAPER (Preferential Attachment Plus Erd\\H{o}s--R\\'{e}nyi)\nmodel for random networks, where we let a random network G be the union of a\npreferential attachment (PA) tree T and additional Erd\\H{o}s--R\\'{e}nyi (ER)\nrandom edges. The PA tree component captures the fact that real world networks\noften have an underlying growth/recruitment process where vertices and edges\nare added sequentially, while the ER component can be regarded as random noise.\nGiven only a single snapshot of the final network G, we study the problem of\nconstructing confidence sets for the early history, in particular the root\nnode, of the unobserved growth process; the root node can be patient zero in a\ndisease infection network or the source of fake news in a social media network.\nWe propose an inference algorithm based on Gibbs sampling that scales to\nnetworks with millions of nodes and provide theoretical analysis showing that\nthe expected size of the confidence set is small so long as the noise level of\nthe ER edges is not too large. We also propose variations of the model in which\nmultiple growth processes occur simultaneously, reflecting the growth of\nmultiple communities, and we use these models to provide a new approach\ncommunity detection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 00:11:04 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 23:30:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Crane", "Harry", ""], ["Xu", "Min", ""]]}, {"id": "2107.00394", "submitter": "Nora L\\\"uthen", "authors": "Nora L\\\"uthen, Olivier Roustant, Fabrice Gamboa, Bertrand Iooss,\n  Stefano Marelli, and Bruno Sudret", "title": "Global sensitivity analysis using derivative-based sparse Poincar\\'e\n  chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-004", "categories": "stat.CO cs.NA math.CA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance-based global sensitivity analysis, in particular Sobol' analysis, is\nwidely used for determining the importance of input variables to a\ncomputational model. Sobol' indices can be computed cheaply based on spectral\nmethods like polynomial chaos expansions (PCE). Another choice are the recently\ndeveloped Poincar\\'e chaos expansions (PoinCE), whose orthonormal\ntensor-product basis is generated from the eigenfunctions of one-dimensional\nPoincar\\'e differential operators. In this paper, we show that the Poincar\\'e\nbasis is the unique orthonormal basis with the property that partial\nderivatives of the basis form again an orthogonal basis with respect to the\nsame measure as the original basis. This special property makes PoinCE ideally\nsuited for incorporating derivative information into the surrogate modelling\nprocess. Assuming that partial derivative evaluations of the computational\nmodel are available, we compute spectral expansions in terms of Poincar\\'e\nbasis functions or basis partial derivatives, respectively, by sparse\nregression. We show on two numerical examples that the derivative-based\nexpansions provide accurate estimates for Sobol' indices, even outperforming\nPCE in terms of bias and variance. In addition, we derive an analytical\nexpression based on the PoinCE coefficients for a second popular sensitivity\nindex, the derivative-based sensitivity measure (DGSM), and explore its\nperformance as upper bound to the corresponding total Sobol' indices.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 12:13:11 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 16:03:26 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["L\u00fcthen", "Nora", ""], ["Roustant", "Olivier", ""], ["Gamboa", "Fabrice", ""], ["Iooss", "Bertrand", ""], ["Marelli", "Stefano", ""], ["Sudret", "Bruno", ""]]}, {"id": "2107.00470", "submitter": "Noemi Corsini", "authors": "Noemi Corsini and Cinzia Viroli", "title": "Dealing with overdispersion in multivariate count data", "comments": "21 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of overdispersion in multivariate count data is a challenging\nissue. Nowadays, it covers a central role mainly due to the relevance of modern\ntechnologies data, such as Next Generation Sequencing and textual data from the\nweb or digital collections. This work presents a comprehensive analysis of the\nlikelihood-based models for extra-variation data proposed in the scientific\nliterature. Particular attention will be paid to the models feasible for\nhigh-dimensional data. A new approach together with its parametric-estimation\nprocedure is proposed. It is a deeper version of the Dirichlet-Multinomial\ndistribution and it leads to important results allowing to get a better\napproximation of the observed variability. A significative comparison of these\nmodels is made through two different simulation studies that both confirm that\nthe new model considered in this work allows to achieve the best results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 14:19:40 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Corsini", "Noemi", ""], ["Viroli", "Cinzia", ""]]}, {"id": "2107.00548", "submitter": "Aseel Mohamed", "authors": "Aseel Sameer Mohamed, Nooriya A. Mohammed", "title": "Comparison of forecasting of the risk of coronavirus (COVID 19) in high\n  quality and low quality healthcare systems, using ANN models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID 19 is a disease that has abnormal over 170 nations worldwide. The\nnumber of infected people (either sick or dead) has been growing at a worrying\nratio in virtually all the affected countries. Forecasting procedures can be\ninstructed so helping in scheming well plans and in captivating creative\nconclusions. These procedures measure the conditions of the previous thus\nallowing well forecasts around the state to arise in the future. These\npredictions strength helps to make contradiction of likely pressures and\nsignificances. Forecasting procedures production a very main character in\nelastic precise predictions. In this case study used two models in order to\ndiagnose optimal approach by compared the outputs. This study was introduced\nforecasting procedures into Artificial Neural Network models compared with\nregression model. Data collected from Al Kindy Teaching Hospital from the\nperiod of 28/5/2019 to 28/7/2019 show an energetic part in forecasting.\nForecasting of a disease can be done founded on several parameters such as the\nage, gender, number of daily infections, number of patient with other disease\nand number of death. Though, forecasting procedures arise with their private\ndata of tests. This study chats these tests and also offers a set of\ncommendations for the persons who are presently hostile the global COVID 19\ndisease.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 15:40:35 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 21:47:02 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Mohamed", "Aseel Sameer", ""], ["Mohammed", "Nooriya A.", ""]]}, {"id": "2107.00731", "submitter": "Andrew Zaharia", "authors": "Andrew D Zaharia, Anish S Potnis, Alexander Walther, Nikolaus\n  Kriegeskorte", "title": "Visualizing the geometry of labeled high-dimensional data with spheres", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data visualizations summarize high-dimensional distributions in two or three\ndimensions. Dimensionality reduction entails a loss of information, and what is\npreserved differs between methods. Existing methods preserve the local or the\nglobal geometry of the points, and most techniques do not consider labels. Here\nwe introduce \"hypersphere2sphere\" (H2S), a new method that aims to visualize\nnot the points, but the relationships between the labeled distributions. H2S\nfits a hypersphere to each labeled set of points in a high-dimensional space\nand visualizes each hypersphere as a sphere in 3D (or circle in 2D). H2S\nperfectly captures the geometry of up to 4 hyperspheres in 3D (or 3 in 2D), and\napproximates the geometry for larger numbers of distributions, matching the\nsizes (radii), and the pairwise separations (between-center distances) and\noverlaps (along the center-connection line). The resulting visualizations are\nrobust to sampling imbalances. Leveraging labels and the sphere as the simplest\ngeometrical primitive, H2S provides an important addition to the toolbox of\nvisualization techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2021 20:15:54 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Zaharia", "Andrew D", ""], ["Potnis", "Anish S", ""], ["Walther", "Alexander", ""], ["Kriegeskorte", "Nikolaus", ""]]}, {"id": "2107.01590", "submitter": "Deyu Ming", "authors": "Deyu Ming and Daniel Williamson and Serge Guillas", "title": "Deep Gaussian Process Emulation using Stochastic Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep Gaussian process (DGP) inference method for computer\nmodel emulation using stochastic imputation. By stochastically imputing the\nlatent layers, the approach transforms the DGP into the linked GP, a\nstate-of-the-art surrogate model formed by linking a system of feed-forward\ncoupled GPs. This transformation renders a simple while efficient DGP training\nprocedure that only involves optimizations of conventional stationary GPs. In\naddition, the analytically tractable mean and variance of the linked GP allows\none to implement predictions from DGP emulators in a fast and accurate manner.\nWe demonstrate the method in a series of synthetic examples and real-world\napplications, and show that it is a competitive candidate for efficient DGP\nsurrogate modeling in comparison to the variational inference and the\nfully-Bayesian approach. A $\\texttt{Python}$ package $\\texttt{dgpsi}$\nimplementing the method is also produced and available at\nhttps://github.com/mingdeyu/DGP.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 10:46:23 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ming", "Deyu", ""], ["Williamson", "Daniel", ""], ["Guillas", "Serge", ""]]}, {"id": "2107.01659", "submitter": "Aramayis Dallakyan", "authors": "Aramayis Dallakyan, Rakheon Kim, Mohsen Pourahmadi", "title": "Time Series Graphical Lasso and Sparse VAR Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We improve upon the two-stage sparse vector autoregression (sVAR) method in\nDavis et al. (2016) by proposing an alternative two-stage modified sVAR method\nwhich relies on time series graphical lasso to estimate sparse inverse spectral\ndensity in the first stage, and the second stage refines non-zero entries of\nthe AR coefficient matrices using a false discovery rate (FDR) procedure. Our\nmethod has the advantage of avoiding the inversion of the spectral density\nmatrix but has to deal with optimization over Hermitian matrices with\ncomplex-valued entries. It significantly improves the computational time with a\nlittle loss in forecasting performance. We study the properties of our proposed\nmethod and compare the performance of the two methods using simulated and a\nreal macro-economic dataset. Our simulation results show that the proposed\nmodification or msVAR is a preferred choice when the goal is to learn the\nstructure of the AR coefficient matrices while sVAR outperforms msVAR when the\nultimate task is forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 15:10:08 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Dallakyan", "Aramayis", ""], ["Kim", "Rakheon", ""], ["Pourahmadi", "Mohsen", ""]]}, {"id": "2107.01688", "submitter": "Ryan Martin", "authors": "Pei-Shien Wu and Ryan Martin", "title": "Calibrating generalized predictive distributions", "comments": "33 pages, 5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In prediction problems, it is common to model the data-generating process and\nthen use a model-based procedure, such as a Bayesian predictive distribution,\nto quantify uncertainty about the next observation. However, if the posited\nmodel is misspecified, then its predictions may not be calibrated -- that is,\nthe predictive distribution's quantiles may not be nominal frequentist\nprediction upper limits, even asymptotically. Rather than abandoning the\ncomfort of a model-based formulation for a more complicated non-model-based\napproach, here we propose a strategy in which the data itself helps determine\nif the assumed model-based solution should be adjusted to account for model\nmisspecification. This is achieved through a generalized Bayes formulation\nwhere a learning rate parameter is tuned, via the proposed generalized\npredictive calibration (GPrC) algorithm, to make the predictive distribution\ncalibrated, even under model misspecification. Extensive numerical experiments\nare presented, under a variety of settings, demonstrating the proposed GPrC\nalgorithm's validity, efficiency, and robustness.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 17:19:24 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wu", "Pei-Shien", ""], ["Martin", "Ryan", ""]]}, {"id": "2107.01742", "submitter": "Gordon J. Ross", "authors": "Gordon J. Ross", "title": "Nonparametric Detection of Multiple Location-Scale Change Points via\n  Wild Binary Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While parametric multiple change point detection has been widely studied,\nless attention has been given to the nonparametric task of detecting multiple\nchange points in a sequence of observations when their distribution is unknown.\nMost existing work on this topic is either based on penalized cost functions\nwhich can suffer from false positive detections, or on binary segmentation\nwhich can fail to detect certain configurations of change points. We introduce\na new approach to change point detection which adapts the recently proposed\nWild Binary Segmentation (WBS) procedure to a nonparametric setting. Our\napproach is based on the use of rank based test statistics which are especially\npowerful at detecting changes in location and/or scale. We show via simulation\nthat the resulting nonparametric WBS procedure has favorable performance\ncompared to existing methods, particularly when it comes to detecting changes\nin scale. We apply our procedure to study a problem in stylometry involving\nchange points in an author's writing style, and provide a full implementation\nof our algorithm in an associated R package.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jul 2021 22:17:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ross", "Gordon J.", ""]]}, {"id": "2107.01865", "submitter": "Motonori Oka", "authors": "Motonori Oka, Shun Saso, Kensuke Okada", "title": "Variational Bayesian Inference for the Polytomous-Attribute Saturated\n  Diagnostic Classification Model with Parallel Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a statistical tool to assist formative assessments in educational\nsettings, diagnostic classification models (DCMs) have been increasingly used\nto provide diagnostic information regarding examinees' attributes. DCMs often\nadopt dichotomous division such as mastery and non-mastery of attributes to\nexpress mastery states of attributes. However, many practical settings involve\ndifferent levels of mastery states rather than a simple dichotomy in a single\nattribute. Although this practical demand can be addressed by\npolytomous-attribute DCMs, their computational cost in a Markov chain Monte\nCarlo estimation impedes their large-scale applications due to the larger\nnumber of polytomous-attribute mastery patterns than that of binary-attribute\nones. This study considers a scalable Bayesian estimation method for\npolytomous-attribute DCMs and developed a variational Bayesian (VB) algorithm\nfor a polytomous-attribute saturated DCM -- a generalization of\npolytomous-attribute DCMs -- by building on the existing literature in VB for\nbinary-attribute DCMs and polytomous-attribute DCMs. Furthermore, we proposed\nthe configuration of parallel computing for the proposed VB algorithm to\nachieve better computational efficiency. Monte Carlo simulations revealed that\nour method exhibited the high performance in parameter recovery under a wide\nrange of conditions. An empirical example is used to demonstrate the utility of\nour method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 08:37:36 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Oka", "Motonori", ""], ["Saso", "Shun", ""], ["Okada", "Kensuke", ""]]}, {"id": "2107.01913", "submitter": "Kody Law", "authors": "Ajay Jasra, Kody J. H. Law, and Fangyuan Yu", "title": "Randomized multilevel Monte Carlo for embarrassingly parallel inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This position paper summarizes a recently developed research program focused\non inference in the context of data centric science and engineering\napplications, and forecasts its trajectory forward over the next decade. Often\none endeavours in this context to learn complex systems in order to make more\ninformed predictions and high stakes decisions under uncertainty. Some key\nchallenges which must be met in this context are robustness, generalizability,\nand interpretability. The Bayesian framework addresses these three challenges,\nwhile bringing with it a fourth, undesirable feature: it is typically far more\nexpensive than its deterministic counterparts. In the 21st century, and\nincreasingly over the past decade, a growing number of methods have emerged\nwhich allow one to leverage cheap low-fidelity models in order to precondition\nalgorithms for performing inference with more expensive models and make\nBayesian inference tractable in the context of high-dimensional and expensive\nmodels. Notable examples are multilevel Monte Carlo (MLMC), multi-index Monte\nCarlo (MIMC), and their randomized counterparts (rMLMC), which are able to\nprovably achieve a dimension-independent (including $\\infty-$dimension)\ncanonical complexity rate with respect to mean squared error (MSE) of $1/$MSE.\nSome parallelizability is typically lost in an inference context, but recently\nthis has been largely recovered via novel double randomization approaches. Such\nan approach delivers i.i.d. samples of quantities of interest which are\nunbiased with respect to the infinite resolution target distribution. Over the\ncoming decade, this family of algorithms has the potential to transform data\ncentric science and engineering, as well as classical machine learning\napplications such as deep learning, by scaling up and scaling out fully\nBayesian inference.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 10:09:41 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Yu", "Fangyuan", ""]]}, {"id": "2107.02070", "submitter": "Wilson Mongwe", "authors": "Wilson Tsakane Mongwe, Rendani Mbuvha, Tshilidzi Marwala", "title": "Antithetic Riemannian Manifold And Quantum-Inspired Hamiltonian Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov Chain Monte Carlo inference of target posterior distributions in\nmachine learning is predominately conducted via Hamiltonian Monte Carlo and its\nvariants. This is due to Hamiltonian Monte Carlo based samplers ability to\nsuppress random-walk behaviour. As with other Markov Chain Monte Carlo methods,\nHamiltonian Monte Carlo produces auto-correlated samples which results in high\nvariance in the estimators, and low effective sample size rates in the\ngenerated samples. Adding antithetic sampling to Hamiltonian Monte Carlo has\nbeen previously shown to produce higher effective sample rates compared to\nvanilla Hamiltonian Monte Carlo. In this paper, we present new algorithms which\nare antithetic versions of Riemannian Manifold Hamiltonian Monte Carlo and\nQuantum-Inspired Hamiltonian Monte Carlo. The Riemannian Manifold Hamiltonian\nMonte Carlo algorithm improves on Hamiltonian Monte Carlo by taking into\naccount the local geometry of the target, which is beneficial for target\ndensities that may exhibit strong correlations in the parameters.\nQuantum-Inspired Hamiltonian Monte Carlo is based on quantum particles that can\nhave random mass. Quantum-Inspired Hamiltonian Monte Carlo uses a random mass\nmatrix which results in better sampling than Hamiltonian Monte Carlo on spiky\nand multi-modal distributions such as jump diffusion processes. The analysis is\nperformed on jump diffusion process using real world financial market data, as\nwell as on real world benchmark classification tasks using Bayesian logistic\nregression.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jul 2021 15:03:07 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Mongwe", "Wilson Tsakane", ""], ["Mbuvha", "Rendani", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "2107.02871", "submitter": "Nicholas Bussberg", "authors": "Nicholas W. Bussberg, Jacob Shields, Chunfeng Huang", "title": "Non-Homogeneity Estimation and Universal Kriging on the Sphere", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kriging is a widely recognized method for making spatial predictions. On the\nsphere, popular methods such as ordinary kriging assume that the spatial\nprocess is intrinsically homogeneous. However, intrinsic homogeneity is too\nstrict in many cases. This research uses intrinsic random function (IRF) theory\nto relax the homogeneity assumption. A key component of modeling IRF processes\nis estimating the degree of non-homogeneity. A graphical approach is proposed\nto accomplish this estimation. With the ability to estimate non-homogeneity, an\nIRF universal kriging procedure can be developed. Results from simulation\nstudies are provided to demonstrate the advantage of using IRF universal\nkriging as opposed to ordinary kriging when the underlying process is not\nintrinsically homogeneous.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2021 20:09:49 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Bussberg", "Nicholas W.", ""], ["Shields", "Jacob", ""], ["Huang", "Chunfeng", ""]]}, {"id": "2107.03077", "submitter": "Rom\\'an Salmer\\'on", "authors": "R. Salmer\\'on, C.B. Garc\\'ia, J. Garc\\'ia", "title": "MultiColl package and other packages to detect multicollinearity in R", "comments": "10 pages, 1 table, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a guide for the use of some of the functions of the\nmultiColl package in R for the detection of near-multicollinearity. The main\ncontribution, in comparison to other existing packages in R or other\neconometric software, is the treatment of qualitative independent variables and\nthe intercept in the simple/multiple linear regression model. The main goal of\nthis paper is to show the advantages of the multiColl package in R, comparing\nits results with the results obtained by other existing packages in R for the\ntreatment of multicollinearity.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 08:49:20 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Salmer\u00f3n", "R.", ""], ["Garc\u00eda", "C. B.", ""], ["Garc\u00eda", "J.", ""]]}, {"id": "2107.03322", "submitter": "Yunzhang Zhu", "authors": "Yunzhang Zhu and Renxiong Liu", "title": "An algorithmic view of $\\ell_2$ regularization and some path-following\n  algorithms", "comments": "62 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish an equivalence between the $\\ell_2$-regularized solution path\nfor a convex loss function, and the solution of an ordinary differentiable\nequation (ODE). Importantly, this equivalence reveals that the solution path\ncan be viewed as the flow of a hybrid of gradient descent and Newton method\napplying to the empirical loss, which is similar to a widely used optimization\ntechnique called trust region method. This provides an interesting algorithmic\nview of $\\ell_2$ regularization, and is in contrast to the conventional view\nthat the $\\ell_2$ regularization solution path is similar to the gradient flow\nof the empirical loss.New path-following algorithms based on homotopy methods\nand numerical ODE solvers are proposed to numerically approximate the solution\npath. In particular, we consider respectively Newton method and gradient\ndescent method as the basis algorithm for the homotopy method, and establish\ntheir approximation error rates over the solution path. Importantly, our theory\nsuggests novel schemes to choose grid points that guarantee an arbitrarily\nsmall suboptimality for the solution path. In terms of computational cost, we\nprove that in order to achieve an $\\epsilon$-suboptimality for the entire\nsolution path, the number of Newton steps required for the Newton method is\n$\\mathcal O(\\epsilon^{-1/2})$, while the number of gradient steps required for\nthe gradient descent method is $\\mathcal O\\left(\\epsilon^{-1}\n\\ln(\\epsilon^{-1})\\right)$. Finally, we use $\\ell_2$-regularized logistic\nregression as an illustrating example to demonstrate the effectiveness of the\nproposed path-following algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2021 16:00:13 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Zhu", "Yunzhang", ""], ["Liu", "Renxiong", ""]]}, {"id": "2107.03584", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Runjing Liu, Michael I. Jordan, Tamara Broderick", "title": "Evaluating Sensitivity to the Stick-Breaking Prior in Bayesian\n  Nonparametrics", "comments": "65 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models based on the Dirichlet process and other stick-breaking\npriors have been proposed as core ingredients for clustering, topic modeling,\nand other unsupervised learning tasks. Prior specification is, however,\nrelatively difficult for such models, given that their flexibility implies that\nthe consequences of prior choices are often relatively opaque. Moreover, these\nchoices can have a substantial effect on posterior inferences. Thus,\nconsiderations of robustness need to go hand in hand with nonparametric\nmodeling. In the current paper, we tackle this challenge by exploiting the fact\nthat variational Bayesian methods, in addition to having computational\nadvantages in fitting complex nonparametric models, also yield sensitivities\nwith respect to parametric and nonparametric aspects of Bayesian models. In\nparticular, we demonstrate how to assess the sensitivity of conclusions to the\nchoice of concentration parameter and stick-breaking distribution for\ninferences under Dirichlet process mixtures and related mixture models. We\nprovide both theoretical and empirical support for our variational approach to\nBayesian sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 03:40:18 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 16:44:03 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Giordano", "Ryan", ""], ["Liu", "Runjing", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "2107.03863", "submitter": "Felix Leopoldo Rios", "authors": "Felix L. Rios, Giusi Moffa, Jack Kuipers", "title": "Benchpress: a scalable and platform-independent workflow for\n  benchmarking structure learning algorithms for graphical models", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Describing the relationship between the variables in a study domain and\nmodelling the data generating mechanism is a fundamental problem in many\nempirical sciences. Probabilistic graphical models are one common approach to\ntackle the problem. Learning the graphical structure is computationally\nchallenging and a fervent area of current research with a plethora of\nalgorithms being developed. To facilitate the benchmarking of different\nmethods, we present a novel automated workflow, called benchpress for producing\nscalable, reproducible, and platform-independent benchmarks of structure\nlearning algorithms for probabilistic graphical models. Benchpress is\ninterfaced via a simple JSON-file, which makes it accessible for all users,\nwhile the code is designed in a fully modular fashion to enable researchers to\ncontribute additional methodologies. Benchpress currently provides an interface\nto a large number of state-of-the-art algorithms from libraries such as BiDAG,\nbnlearn, GOBNILP, pcalg, r.blip, scikit-learn, TETRAD, and trilearn as well as\na variety of methods for data generating models and performance evaluation.\nAlongside user-defined models and randomly generated datasets, the software\ntool also includes a number of standard datasets and graphical models from the\nliterature, which may be included in a benchmarking workflow. We demonstrate\nthe applicability of this workflow for learning Bayesian networks in four\ntypical data scenarios. The source code and documentation is publicly available\nfrom http://github.com/felixleopoldo/benchpress.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jul 2021 14:19:28 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Rios", "Felix L.", ""], ["Moffa", "Giusi", ""], ["Kuipers", "Jack", ""]]}, {"id": "2107.04354", "submitter": "Nicola Donelli", "authors": "Nicola Donelli (1), Stefano Peluso (2) and Antonietta Mira (3) ((1)\n  CGnal srl, (2) Universit\\`a degli Studi di Milano-Bicocca, Department of\n  Statistics and Quantitative Methods, (3) Universit\\`a della Svizzera\n  italiana, Institute of Computational Science and Universit\\`a dell'Insubria,\n  Department of Science and High Technology)", "title": "A Bayesian Semiparametric Vector Multiplicative Error Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interactions among multiple time series of positive random variables are\ncrucial in diverse financial applications, from spillover effects to volatility\ninterdependence. A popular model in this setting is the vector Multiplicative\nError Model (vMEM) which poses a linear iterative structure on the dynamics of\nthe conditional mean, perturbed by a multiplicative innovation term. A main\nlimitation of vMEM is however its restrictive assumption on the distribution of\nthe random innovation term. A Bayesian semiparametric approach that models the\ninnovation vector as an infinite location-scale mixture of multidimensional\nkernels with support on the positive orthant is used to address this major\nshortcoming of vMEM. Computational complications arising from the constraints\nto the positive orthant are avoided through the formulation of a slice sampler\non the parameter-extended unconstrained version of the model. The method is\napplied to simulated and real data and a flexible specification is obtained\nthat outperforms the classical ones in terms of fitting and predictive power.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 10:47:48 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Donelli", "Nicola", ""], ["Peluso", "Stefano", ""], ["Mira", "Antonietta", ""]]}, {"id": "2107.04552", "submitter": "Gabriel Ducrocq", "authors": "Nicolas Chopin and Gabriel Ducrocq", "title": "Fast compression of MCMC output", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose cube thinning, a novel method for compressing the output of a MCMC\n(Markov chain Monte Carlo) algorithm when control variates are available. It\namounts to resampling the initial MCMC sample (according to weights derived\nfrom control variates), while imposing equality constraints on averages of\nthese control variates, using the cube method of \\cite{Deville2004}. Its main\nadvantage is that its CPU cost is linear in $N$, the original sample size, and\nis constant in $M$, the required size for the compressed sample. This compares\nfavourably to Stein thinning \\citep{Riabiz2020}, which has complexity\n$\\mathcal{O}(NM^2)$, and which requires the availability of the gradient of the\ntarget log-density (which automatically implies the availability of control\nvariates). Our numerical experiments suggest that cube thinning is also\ncompetitive in terms of statistical error.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jul 2021 17:11:26 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 21:26:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chopin", "Nicolas", ""], ["Ducrocq", "Gabriel", ""]]}, {"id": "2107.04766", "submitter": "Yuling Jiao", "authors": "Yuling Jiao and Lican Kang and Yanyan Liu and Youzhou Zhou", "title": "Convergence Analysis of Schr{\\\"o}dinger-F{\\\"o}llmer Sampler without\n  Convexity", "comments": "arXiv admin note: text overlap with arXiv:2106.10880", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Schr\\\"{o}dinger-F\\\"{o}llmer sampler (SFS) is a novel and efficient approach\nfor sampling from possibly unnormalized distributions without ergodicity. SFS\nis based on the Euler-Maruyama discretization of Schr\\\"{o}dinger-F\\\"{o}llmer\ndiffusion process $$\\mathrm{d} X_{t}=-\\nabla U\\left(X_t, t\\right) \\mathrm{d}\nt+\\mathrm{d} B_{t}, \\quad t \\in[0,1],\\quad X_0=0$$ on the unit interval, which\ntransports the degenerate distribution at time zero to the target distribution\nat time one. In \\cite{sfs21}, the consistency of SFS is established under a\nrestricted assumption that %the drift term $b(x,t)$ the potential $U(x,t)$ is\nuniformly (on $t$) strongly %concave convex (on $x$). In this paper we provide\na nonasymptotic error bound of SFS in Wasserstein distance under some smooth\nand bounded conditions on the density ratio of the target distribution over the\nstandard normal distribution, but without requiring the strongly convexity of\nthe potential.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jul 2021 05:37:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jiao", "Yuling", ""], ["Kang", "Lican", ""], ["Liu", "Yanyan", ""], ["Zhou", "Youzhou", ""]]}, {"id": "2107.05956", "submitter": "Sourabh Bhattacharya", "authors": "Sourabh Bhattacharya", "title": "IID Sampling from Intractable Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology for drawing iid realizations from any target\ndistribution on the Euclidean space with arbitrary dimension. No assumption of\ncompact support is necessary for the validity of our theory and method. Our\nidea is to construct an appropriate infinite sequence of concentric closed\nellipsoids, represent the target distribution as an infinite mixture on the\ncentral ellipsoid and the ellipsoidal annuli, and to construct efficient\nperfect samplers for the mixture components.\n  In contrast with most of the existing works on perfect sampling, ours is not\nonly a theoretically valid method, it is practically applicable to all target\ndistributions on any dimensional Euclidean space and very much amenable to\nparallel computation. We validate the practicality and usefulness of our\nmethodology by generating 10000 iid realizations from the standard\ndistributions such as normal, Student's t with 5 degrees of freedom and Cauchy,\nfor dimensions d = 1, 5, 10, 50, 100, as well as from a 50-dimensional mixture\nnormal distribution. The implementation time in all the cases are very\nreasonable, and often less than a minute in our parallel implementation. The\nresults turned out to be highly accurate.\n  We also apply our method to draw 10000 iid realizations from the posterior\ndistributions associated with the well-known Challenger data, a Salmonella data\nand the 160-dimensional challenging spatial example of the radionuclide count\ndata on Rongelap Island. Again, we are able to obtain quite encouraging results\nwith very reasonable computing time.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 09:58:02 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bhattacharya", "Sourabh", ""]]}, {"id": "2107.06091", "submitter": "Maxime ElMasri", "authors": "Maxime ElMasri, J\\'er\\^ome Morio, Florian Simatos", "title": "Optimal projection to improve parametric importance sampling in high\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a dimension-reduction strategy in order to improve\nthe performance of importance sampling in high dimension. The idea is to\nestimate variance terms in a small number of suitably chosen directions. We\nfirst prove that the optimal directions, i.e., the ones that minimize the\nKullback--Leibler divergence with the optimal auxiliary density, are the\neigenvectors associated to extreme (small or large) eigenvalues of the optimal\ncovariance matrix. We then perform extensive numerical experiments that show\nthat as dimension increases, these directions give estimations which are very\nclose to optimal. Moreover, we show that the estimation remains accurate even\nwhen a simple empirical estimator of the covariance matrix is used to estimate\nthese directions. These theoretical and numerical results open the way for\ndifferent generalizations, in particular the incorporation of such ideas in\nadaptive importance sampling schemes.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jul 2021 13:50:09 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["ElMasri", "Maxime", ""], ["Morio", "J\u00e9r\u00f4me", ""], ["Simatos", "Florian", ""]]}, {"id": "2107.06659", "submitter": "Marcin W\\k{a}torek", "authors": "Marcin W\\k{a}torek, Jaros{\\l}aw Kwapie\\'n, Stanis{\\l}aw Dro\\.zd\\.z", "title": "Financial Return Distributions: Past, Present, and COVID-19", "comments": null, "journal-ref": "Entropy 2021, 23(7), 884", "doi": "10.3390/e23070884", "report-no": null, "categories": "q-fin.ST econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the price return distributions of currency exchange rates,\ncryptocurrencies, and contracts for differences (CFDs) representing stock\nindices, stock shares, and commodities. Based on recent data from the years\n2017--2020, we model tails of the return distributions at different time scales\nby using power-law, stretched exponential, and $q$-Gaussian functions. We focus\non the fitted function parameters and how they change over the years by\ncomparing our results with those from earlier studies and find that, on the\ntime horizons of up to a few minutes, the so-called \"inverse-cubic power-law\"\nstill constitutes an appropriate global reference. However, we no longer\nobserve the hypothesized universal constant acceleration of the market time\nflow that was manifested before in an ever faster convergence of empirical\nreturn distributions towards the normal distribution. Our results do not\nexclude such a scenario but, rather, suggest that some other short-term\nprocesses related to a current market situation alter market dynamics and may\nmask this scenario. Real market dynamics is associated with a continuous\nalternation of different regimes with different statistical properties. An\nexample is the COVID-19 pandemic outburst, which had an enormous yet short-time\nimpact on financial markets. We also point out that two factors -- speed of the\nmarket time flow and the asset cross-correlation magnitude -- while related\n(the larger the speed, the larger the cross-correlations on a given time\nscale), act in opposite directions with regard to the return distribution\ntails, which can affect the expected distribution convergence to the normal\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2021 12:49:00 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["W\u0105torek", "Marcin", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["Dro\u017cd\u017c", "Stanis\u0142aw", ""]]}, {"id": "2107.07359", "submitter": "Maxime Chaveroche", "authors": "Maxime Chaveroche, Franck Davoine, V\\'eronique Cherfaoui", "title": "Efficient M\\\"obius Transformations and their applications to\n  Dempster-Shafer Theory: Clarification and implementation", "comments": "Extension of an article published in the proceedings of the\n  international conference on Scalable Uncertainty Management (SUM) in 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.DM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dempster-Shafer Theory (DST) generalizes Bayesian probability theory,\noffering useful additional information, but suffers from a high computational\nburden. A lot of work has been done to reduce the complexity of computations\nused in information fusion with Dempster's rule. The main approaches exploit\neither the structure of Boolean lattices or the information contained in belief\nsources. Each has its merits depending on the situation. In this paper, we\npropose sequences of graphs for the computation of the zeta and M\\\"obius\ntransformations that optimally exploit both the structure of distributive\nsemilattices and the information contained in belief sources. We call them the\nEfficient M\\\"obius Transformations (EMT). We show that the complexity of the\nEMT is always inferior to the complexity of algorithms that consider the whole\nlattice, such as the Fast M\\\"obius Transform (FMT) for all DST transformations.\nWe then explain how to use them to fuse two belief sources. More generally, our\nEMTs apply to any function in any finite distributive lattice, focusing on a\nmeet-closed or join-closed subset. This article extends our work published at\nthe international conference on Scalable Uncertainty Management (SUM). It\nclarifies it, brings some minor corrections and provides implementation details\nsuch as data structures and algorithms applied to DST.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 14:35:48 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chaveroche", "Maxime", ""], ["Davoine", "Franck", ""], ["Cherfaoui", "V\u00e9ronique", ""]]}, {"id": "2107.07475", "submitter": "Ian Grooms", "authors": "Ian Grooms", "title": "A comparison of nonlinear extensions to the ensemble Kalman filter:\n  Gaussian Anamorphosis and Two-Step Ensemble Filters", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews two nonlinear, non-Gaussian extensions of the Ensemble\nKalman Filter: Gaussian anamorphosis (GA) methods and two-step updates, of\nwhich the rank histogram filter (RHF) is a prototypical example. GA-EnKF\nmethods apply univariate transforms to the state and observation variables to\nmake their distribution more Gaussian before applying an EnKF. The two-step\nmethods use a scalar Bayesian update for the first step, followed by linear\nregression for the second step. The connection of the two-step framework to the\nfull Bayesian problem is made, which opens the door to more advanced two-step\nmethods in the full Bayesian setting. A new method for the first part of the\ntwo-step framework is proposed, with a similar form to the RHF but a different\nmotivation, called the `improved RHF' (iRHF). A suite of experiments with the\nLorenz-`96 model demonstrate situations where the GA-EnKF methods are similar\nto EnKF, and where they outperform EnKF. The experiments also strongly support\nthe accuracy of the RHF and iRHF filters for nonlinear and non-Gaussian\nobservations; these methods uniformly beat the EnKF and GA-EnKF methods in the\nexperiments reported here. The new iRHF method is only more accurate than RHF\nat small ensemble sizes in the experiments reported here.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 17:23:13 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Grooms", "Ian", ""]]}, {"id": "2107.07561", "submitter": "Luiza Piancastelli", "authors": "Luiza S.C. Piancastelli, Nial Friel, Wagner Barreto-Souza and Hernando\n  Ombao", "title": "Multivariate Conway-Maxwell-Poisson Distribution: Sarmanov Method and\n  Doubly-Intractable Bayesian Inference", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multivariate count distribution with Conway-Maxwell\n(COM)-Poisson marginals is proposed. To do this, we develop a modification of\nthe Sarmanov method for constructing multivariate distributions. Our\nmultivariate COM-Poisson (MultCOMP) model has desirable features such as (i) it\nadmits a flexible covariance matrix allowing for both negative and positive\nnon-diagonal entries; (ii) it overcomes the limitation of the existing\nbivariate COM-Poisson distributions in the literature that do not have\nCOM-Poisson marginals; (iii) it allows for the analysis of multivariate counts\nand is not just limited to bivariate counts. Inferential challenges are\npresented by the likelihood specification as it depends on a number of\nintractable normalizing constants involving the model parameters. These\nobstacles motivate us to propose a Bayesian inferential approach where the\nresulting doubly-intractable posterior is dealt with via the exchange algorithm\nand the Grouped Independence Metropolis-Hastings algorithm. Numerical\nexperiments based on simulations are presented to illustrate the proposed\nBayesian approach. We analyze the potential of the MultCOMP model through a\nreal data application on the numbers of goals scored by the home and away teams\nin the Premier League from 2018 to 2021. Here, our interest is to assess the\neffect of a lack of crowds during the COVID-19 pandemic on the well-known home\nteam advantage. A MultCOMP model fit shows that there is evidence of a\ndecreased number of goals scored by the home team, not accompanied by a reduced\nscore from the opponent. Hence, our analysis suggests a smaller home team\nadvantage in the absence of crowds, which agrees with the opinion of several\nfootball experts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 18:48:42 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Piancastelli", "Luiza S. C.", ""], ["Friel", "Nial", ""], ["Barreto-Souza", "Wagner", ""], ["Ombao", "Hernando", ""]]}, {"id": "2107.07603", "submitter": "Joshua Scurll", "authors": "Joshua M. Scurll", "title": "Measuring inter-cluster similarities with Alpha Shape TRIangulation in\n  loCal Subspaces (ASTRICS) facilitates visualization and clustering of\n  high-dimensional data", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.HC cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clustering and visualizing high-dimensional (HD) data are important tasks in\na variety of fields. For example, in bioinformatics, they are crucial for\nanalyses of single-cell data such as mass cytometry (CyTOF) data. Some of the\nmost effective algorithms for clustering HD data are based on representing the\ndata by nodes in a graph, with edges connecting neighbouring nodes according to\nsome measure of similarity or distance. However, users of graph-based\nalgorithms are typically faced with the critical but challenging task of\nchoosing the value of an input parameter that sets the size of neighbourhoods\nin the graph, e.g. the number of nearest neighbours to which to connect each\nnode or a threshold distance for connecting nodes. The burden on the user could\nbe alleviated by a measure of inter-node similarity that can have value 0 for\ndissimilar nodes without requiring any user-defined parameters or thresholds.\nThis would determine the neighbourhoods automatically while still yielding a\nsparse graph. To this end, I propose a new method called ASTRICS to measure\nsimilarity between clusters of HD data points based on local dimensionality\nreduction and triangulation of critical alpha shapes. I show that my ASTRICS\nsimilarity measure can facilitate both clustering and visualization of HD data\nby using it in Stage 2 of a three-stage pipeline: Stage 1 = perform an initial\nclustering of the data by any method; Stage 2 = let graph nodes represent\ninitial clusters instead of individual data points and use ASTRICS to\nautomatically define edges between nodes; Stage 3 = use the graph for further\nclustering and visualization. This trades the critical task of choosing a graph\nneighbourhood size for the easier task of essentially choosing a resolution at\nwhich to view the data. The graph and consequently downstream clustering and\nvisualization are then automatically adapted to the chosen resolution.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jul 2021 20:51:06 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Scurll", "Joshua M.", ""]]}, {"id": "2107.07687", "submitter": "Yuming Chen", "authors": "Yuming Chen, Daniel Sanz-Alonso, Rebecca Willett", "title": "Auto-differentiable Ensemble Kalman Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data assimilation is concerned with sequentially estimating a\ntemporally-evolving state. This task, which arises in a wide range of\nscientific and engineering applications, is particularly challenging when the\nstate is high-dimensional and the state-space dynamics are unknown. This paper\nintroduces a machine learning framework for learning dynamical systems in data\nassimilation. Our auto-differentiable ensemble Kalman filters (AD-EnKFs) blend\nensemble Kalman filters for state recovery with machine learning tools for\nlearning the dynamics. In doing so, AD-EnKFs leverage the ability of ensemble\nKalman filters to scale to high-dimensional states and the power of automatic\ndifferentiation to train high-dimensional surrogate models for the dynamics.\nNumerical results using the Lorenz-96 model show that AD-EnKFs outperform\nexisting methods that use expectation-maximization or particle filters to merge\ndata assimilation and machine learning. In addition, AD-EnKFs are easy to\nimplement and require minimal tuning.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 03:25:30 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 18:32:48 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Yuming", ""], ["Sanz-Alonso", "Daniel", ""], ["Willett", "Rebecca", ""]]}, {"id": "2107.07703", "submitter": "Otmar Ertl", "authors": "Otmar Ertl", "title": "Estimation from Partially Sampled Distributed Traces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling is often a necessary evil to reduce the processing and storage costs\nof distributed tracing. In this work, we describe a scalable and adaptive\nsampling approach that can preserve events of interest better than the widely\nused head-based sampling approach. Sampling rates can be chosen individually\nand independently for every span, allowing to take span attributes and local\nresource constraints into account. The resulting traces are often only\npartially and not completely sampled which complicates statistical analysis. To\nexploit the given information, an unbiased estimation algorithm is presented.\nEven though it does not need to know whether the traces are complete, it\nreduces the estimation error in many cases compared to considering only\ncomplete traces.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jul 2021 04:41:24 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Ertl", "Otmar", ""]]}, {"id": "2107.08459", "submitter": "Luca Martino", "authors": "Luca Martino, V\\'ictor Elvira", "title": "Compressed Monte Carlo with application in particle filtering", "comments": null, "journal-ref": "Information Sciences, Volume 553, April 2021, Pages 331-352", "doi": "10.1016/j.ins.2020.10.022", "report-no": null, "categories": "stat.CO cs.CE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models have become very popular over the last years in several\nfields such as signal processing, statistics, and machine learning. Bayesian\ninference requires the approximation of complicated integrals involving\nposterior distributions. For this purpose, Monte Carlo (MC) methods, such as\nMarkov Chain Monte Carlo and importance sampling algorithms, are often\nemployed. In this work, we introduce the theory and practice of a Compressed MC\n(C-MC) scheme to compress the statistical information contained in a set of\nrandom samples. In its basic version, C-MC is strictly related to the\nstratification technique, a well-known method used for variance reduction\npurposes. Deterministic C-MC schemes are also presented, which provide very\ngood performance. The compression problem is strictly related to the moment\nmatching approach applied in different filtering techniques, usually called as\nGaussian quadrature rules or sigma-point methods. C-MC can be employed in a\ndistributed Bayesian inference framework when cheap and fast communications\nwith a central processor are required. Furthermore, C-MC is useful within\nparticle filtering and adaptive IS algorithms, as shown by three novel schemes\nintroduced in this work. Six numerical results confirm the benefits of the\nintroduced schemes, outperforming the corresponding benchmark methods. A\nrelated code is also provided.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 14:32:04 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "V\u00edctor", ""]]}, {"id": "2107.08465", "submitter": "Luca Martino", "authors": "Luca Martino, V\\'ictor Elvira, Javier L\\'opez-Santiago, Gustau\n  Camps-Valls", "title": "Compressed particle methods for expensive models with application in\n  Astronomy and Remote Sensing", "comments": "published in IEEE Transactions on Aerospace and Electronic Systems", "journal-ref": null, "doi": "10.1109/TAES.2021.3061791", "report-no": null, "categories": "cs.CE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many inference problems, the evaluation of complex and costly models is\noften required. In this context, Bayesian methods have become very popular in\nseveral fields over the last years, in order to obtain parameter inversion,\nmodel selection or uncertainty quantification. Bayesian inference requires the\napproximation of complicated integrals involving (often costly) posterior\ndistributions. Generally, this approximation is obtained by means of Monte\nCarlo (MC) methods. In order to reduce the computational cost of the\ncorresponding technique, surrogate models (also called emulators) are often\nemployed. Another alternative approach is the so-called Approximate Bayesian\nComputation (ABC) scheme. ABC does not require the evaluation of the costly\nmodel but the ability to simulate artificial data according to that model.\nMoreover, in ABC, the choice of a suitable distance between real and artificial\ndata is also required. In this work, we introduce a novel approach where the\nexpensive model is evaluated only in some well-chosen samples. The selection of\nthese nodes is based on the so-called compressed Monte Carlo (CMC) scheme. We\nprovide theoretical results supporting the novel algorithms and give empirical\nevidence of the performance of the proposed method in several numerical\nexperiments. Two of them are real-world applications in astronomy and satellite\nremote sensing.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 14:45:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "V\u00edctor", ""], ["L\u00f3pez-Santiago", "Javier", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2107.08535", "submitter": "Haoyue Wang", "authors": "Haoyue Wang, Shibal Ibrahim, Rahul Mazumder", "title": "Nonparametric Finite Mixture Models with Possible Shape Constraints: A\n  Cubic Newton Approach", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore computational aspects of maximum likelihood estimation of the\nmixture proportions of a nonparametric finite mixture model -- a convex\noptimization problem with old roots in statistics and a key member of the\nmodern data analysis toolkit. Motivated by problems in shape constrained\ninference, we consider structured variants of this problem with additional\nconvex polyhedral constraints. We propose a new cubic regularized Newton method\nfor this problem and present novel worst-case and local computational\nguarantees for our algorithm. We extend earlier work by Nesterov and Polyak to\nthe case of a self-concordant objective with polyhedral constraints, such as\nthe ones considered herein. We propose a Frank-Wolfe method to solve the cubic\nregularized Newton subproblem; and derive efficient solutions for the linear\noptimization oracles that may be of independent interest. In the particular\ncase of Gaussian mixtures without shape constraints, we derive bounds on how\nwell the finite mixture problem approximates the infinite-dimensional\nKiefer-Wolfowitz maximum likelihood estimator. Experiments on synthetic and\nreal datasets suggest that our proposed algorithms exhibit improved runtimes\nand scalability features over existing benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jul 2021 20:58:24 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wang", "Haoyue", ""], ["Ibrahim", "Shibal", ""], ["Mazumder", "Rahul", ""]]}, {"id": "2107.09357", "submitter": "Mario Beraha", "authors": "Mario Beraha, Daniele Falco and Alessandra Guglielmi", "title": "JAGS, NIMBLE, Stan: a detailed comparison among Bayesian MCMC software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this work is the comparison of the performance of the three\npopular software platforms JAGS, NIMBLE and Stan. These probabilistic\nprogramming languages are able to automatically generate samples from the\nposterior distribution of interest using MCMC algorithms, starting from the\nspecification of a Bayesian model, i.e. the likelihood and the prior. The final\ngoal is to present a detailed analysis of their strengths and weaknesses to\nstatisticians or applied scientists. In this way, we wish to contribute to make\nthem fully aware of the pros and cons of this software. We carry out a\nsystematic comparison of the three platforms on a wide class of models, prior\ndistributions, and data generating mechanisms. Our extensive simulation studies\nevaluate the quality of the MCMC chains produced, the efficiency of the\nsoftware and the goodness of fit of the output. We also consider the efficiency\nof the parallelization made by the three platforms.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 09:24:13 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Beraha", "Mario", ""], ["Falco", "Daniele", ""], ["Guglielmi", "Alessandra", ""]]}, {"id": "2107.09810", "submitter": "Suvra Pal", "authors": "Sandip Barui and Suvra Pal and Nutan Mishra and Katherine Davies", "title": "A Stochastic Version of the EM Algorithm for Mixture Cure Rate Model\n  with Exponentiated Weibull Family of Lifetimes", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handling missing values plays an important role in the analysis of survival\ndata, especially, the ones marked by cure fraction. In this paper, we discuss\nthe properties and implementation of stochastic approximations to the\nexpectation-maximization (EM) algorithm to obtain maximum likelihood (ML) type\nestimates in situations where missing data arise naturally due to right\ncensoring and a proportion of individuals are immune to the event of interest.\nA flexible family of three parameter exponentiated-Weibull (EW) distributions\nis assumed to characterize lifetimes of the non-immune individuals as it\naccommodates both monotone (increasing and decreasing) and non-monotone\n(unimodal and bathtub) hazard functions. To evaluate the performance of the SEM\nalgorithm, an extensive simulation study is carried out under various parameter\nsettings. Using likelihood ratio test we also carry out model discrimination\nwithin the EW family of distributions. Furthermore, we study the robustness of\nthe SEM algorithm with respect to outliers and algorithm starting values. Few\nscenarios where stochastic EM (SEM) algorithm outperforms the well-studied EM\nalgorithm are also examined in the given context. For further demonstration, a\nreal survival data on cutaneous melanoma is analyzed using the proposed cure\nrate model with EW lifetime distribution and the proposed estimation technique.\nThrough this data, we illustrate the applicability of the likelihood ratio test\ntowards rejecting several well-known lifetime distributions that are nested\nwithin the wider class of EW distributions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2021 23:55:14 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Barui", "Sandip", ""], ["Pal", "Suvra", ""], ["Mishra", "Nutan", ""], ["Davies", "Katherine", ""]]}, {"id": "2107.10365", "submitter": "Helton Saulo", "authors": "Alan Dasilva and Helton Saulo", "title": "Scale-mixture Birnbaum-Saunders quantile regression models applied to\n  personal accident insurance data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling of personal accident insurance data has been a topic of extreme\nrelevance in the insurance literature. In general, the data often exhibit\npositive asymmetry and heavy tails and non-quantile Birnbaum-Saunders\nregression models have been used in the modeling strategy. In this work, we\npropose a new quantile regression model based on the scale-mixture\nBirnbaum-Saunders distribution, which is reparametrized by inserting a quantile\nparameter. The maximum likelihood estimates of the model parameters are\nobtained via the EM algorithm. Two Monte Carlo simulation studies were\nperformed using the \\texttt{R} software. The first study aims to analyze the\nperformance of the maximum likelihood estimates, the information criteria AIC,\nAICc, BIC, HIC, the root of the mean square error, and the randomized quantile\nand generalized Cox-Snell residuals. In the second simulation study, the size\nand power of the the Wald, likelihood ratio, score and gradient tests are\nevaluated. The two simulation studies were conducted considering different\nquantiles of interest and sample sizes. Finally, a real insurance data set is\nanalyzed to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2021 21:50:41 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Dasilva", "Alan", ""], ["Saulo", "Helton", ""]]}, {"id": "2107.10572", "submitter": "Ines Wilms", "authors": "Ines Wilms, Rebecca Killick and David S. Matteson", "title": "Graphical Influence Diagnostics for Changepoint Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Changepoint models enjoy a wide appeal in a variety of disciplines to model\nthe heterogeneity of ordered data. Graphical influence diagnostics to\ncharacterize the influence of single observations on changepoint models are,\nhowever, lacking. We address this gap by developing a framework for\ninvestigating instabilities in changepoint segmentations and assessing the\ninfluence of single observations on various outputs of a changepoint analysis.\nWe construct graphical diagnostic plots that allow practitioners to assess\nwhether instabilities occur; how and where they occur; and to detect\ninfluential individual observations triggering instability. We analyze well-log\ndata to illustrate how such influence diagnostic plots can be used in practice\nto reveal features of the data that may otherwise remain hidden.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 10:54:49 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Wilms", "Ines", ""], ["Killick", "Rebecca", ""], ["Matteson", "David S.", ""]]}, {"id": "2107.10587", "submitter": "Simon Bartels", "authors": "Simon Bartels and Wouter Boomsma and Jes Frellsen and Damien Garreau", "title": "Kernel-Matrix Determinant Estimates from stopped Cholesky Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Algorithms involving Gaussian processes or determinantal point processes\ntypically require computing the determinant of a kernel matrix. Frequently, the\nlatter is computed from the Cholesky decomposition, an algorithm of cubic\ncomplexity in the size of the matrix. We show that, under mild assumptions, it\nis possible to estimate the determinant from only a sub-matrix, with\nprobabilistic guarantee on the relative error. We present an augmentation of\nthe Cholesky decomposition that stops under certain conditions before\nprocessing the whole matrix. Experiments demonstrate that this can save a\nconsiderable amount of time while having an overhead of less than $5\\%$ when\nnot stopping early. More generally, we present a probabilistic stopping\nstrategy for the approximation of a sum of known length where addends are\nrevealed sequentially. We do not assume independence between addends, only that\nthey are bounded from below and decrease in conditional expectation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 11:31:02 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bartels", "Simon", ""], ["Boomsma", "Wouter", ""], ["Frellsen", "Jes", ""], ["Garreau", "Damien", ""]]}, {"id": "2107.10731", "submitter": "Lauro Sandor Langosco di Langosco", "authors": "Lauro Langosco di Langosco, Vincent Fortuin, Heiko Strathmann", "title": "Neural Variational Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Particle-based approximate Bayesian inference approaches such as Stein\nVariational Gradient Descent (SVGD) combine the flexibility and convergence\nguarantees of sampling methods with the computational benefits of variational\ninference. In practice, SVGD relies on the choice of an appropriate kernel\nfunction, which impacts its ability to model the target distribution -- a\nchallenging problem with only heuristic solutions. We propose Neural\nVariational Gradient Descent (NVGD), which is based on parameterizing the\nwitness function of the Stein discrepancy by a deep neural network whose\nparameters are learned in parallel to the inference, mitigating the necessity\nto make any kernel choices whatsoever. We empirically evaluate our method on\npopular synthetic inference problems, real-world Bayesian linear regression,\nand Bayesian neural network inference.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 15:10:50 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 10:57:04 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["di Langosco", "Lauro Langosco", ""], ["Fortuin", "Vincent", ""], ["Strathmann", "Heiko", ""]]}, {"id": "2107.10827", "submitter": "Quan Zhou", "authors": "Quan Zhou", "title": "On the Scalability of Informed Importance Tempering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informed MCMC methods have been proposed as scalable solutions to Bayesian\nposterior computation on high-dimensional discrete state spaces. We study a\nclass of MCMC schemes called informed importance tempering (IIT), which combine\nimportance sampling and informed local proposals. Spectral gap bounds for IIT\nestimators are obtained, which demonstrate the remarkable scalability of IIT\nsamplers for unimodal target distributions. The theoretical insights acquired\nin this note provide guidance on the choice of informed proposals in model\nselection and the use of importance sampling in MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 17:39:09 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Zhou", "Quan", ""]]}, {"id": "2107.10880", "submitter": "Andres Fernandez Rodriguez", "authors": "Andres Fernandez, Mark D. Plumbley", "title": "Using UMAP to Inspect Audio Data for Unsupervised Anomaly Detection\n  under Domain-Shift Conditions", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of Unsupervised Anomaly Detection (UAD) is to detect anomalous\nsignals under the condition that only non-anomalous (normal) data is available\nbeforehand. In UAD under Domain-Shift Conditions (UAD-S), data is further\nexposed to contextual changes that are usually unknown beforehand. Motivated by\nthe difficulties encountered in the UAD-S task presented at the 2021 edition of\nthe Detection and Classification of Acoustic Scenes and Events (DCASE)\nchallenge, we visually inspect Uniform Manifold Approximations and Projections\n(UMAPs) for log-STFT, log-mel and pretrained Look, Listen and Learn (L3)\nrepresentations of the DCASE UAD-S dataset. In our exploratory investigation,\nwe look for two qualities, Separability (SEP) and Discriminative Support\n(DSUP), and formulate several hypotheses that could facilitate diagnosis and\ndevelopement of further representation and detection approaches. Particularly,\nwe hypothesize that input length and pretraining may regulate a relevant\ntradeoff between SEP and DSUP. Our code as well as the resulting UMAPs and\nplots are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 18:28:27 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Fernandez", "Andres", ""], ["Plumbley", "Mark D.", ""]]}, {"id": "2107.10947", "submitter": "Nhat Ho", "authors": "Nhat Ho and Stephen G. Walker", "title": "On Integral Theorems: Monte Carlo Estimators and Optimal Functions", "comments": "18 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2106.06608", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.CA stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of integral theorems based on cyclic functions and\nRiemann sums approximating integrals theorem. The Fourier integral theorem,\nderived as a combination of a transform and inverse transform, arises as a\nspecial case. The integral theorems provide natural estimators of density\nfunctions via Monte Carlo integration. Assessments of the quality of the\ndensity estimators can be used to obtain optimal cyclic functions which\nminimize square integrals. Our proof techniques rely on a variational approach\nin ordinary differential equations and the Cauchy residue theorem in complex\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 22:25:21 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2107.11614", "submitter": "Luca Martino", "authors": "L. Martino, F. Llorente, E. Curbelo, J. Lopez-Santiago, J. Miguez", "title": "Automatic tempered posterior distributions for Bayesian inversion\n  problems", "comments": null, "journal-ref": "Mathematics. 2021; 9(7):784", "doi": "10.3390/math9070784", "report-no": null, "categories": "stat.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel adaptive importance sampling scheme for Bayesian inversion\nproblems where the inference of the variables of interest and the power of the\ndata noise is split. More specifically, we consider a Bayesian analysis for the\nvariables of interest (i.e., the parameters of the model to invert), whereas we\nemploy a maximum likelihood approach for the estimation of the noise power. The\nwhole technique is implemented by means of an iterative procedure, alternating\nsampling and optimization steps. Moreover, the noise power is also used as a\ntempered parameter for the posterior distribution of the the variables of\ninterest. Therefore, a sequence of tempered posterior densities is generated,\nwhere the tempered parameter is automatically selected according to the actual\nestimation of the noise power. A complete Bayesian study over the model\nparameters and the scale parameter can be also performed. Numerical experiments\nshow the benefits of the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jul 2021 14:06:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Martino", "L.", ""], ["Llorente", "F.", ""], ["Curbelo", "E.", ""], ["Lopez-Santiago", "J.", ""], ["Miguez", "J.", ""]]}, {"id": "2107.11820", "submitter": "Luca Martino", "authors": "D. Luengo, L. Martino, M. Bugallo, V. Elvira, S. S\u007f\\\"arkk\\\"a", "title": "A Survey of Monte Carlo Methods for Parameter Estimation", "comments": null, "journal-ref": "EURASIP Journal on Advances in Signal Processing, Volume 2020,\n  Article number: 25 (2020)", "doi": "10.1186/s13634-020-00675-6", "report-no": null, "categories": "stat.CO cs.AI cs.NA eess.SP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical signal processing applications usually require the estimation of\nsome parameters of interest given a set of observed data. These estimates are\ntypically obtained either by solving a multi-variate optimization problem, as\nin the maximum likelihood (ML) or maximum a posteriori (MAP) estimators, or by\nperforming a multi-dimensional integration, as in the minimum mean squared\nerror (MMSE) estimators. Unfortunately, analytical expressions for these\nestimators cannot be found in most real-world applications, and the Monte Carlo\n(MC) methodology is one feasible approach. MC methods proceed by drawing random\nsamples, either from the desired distribution or from a simpler one, and using\nthem to compute consistent estimators. The most important families of MC\nalgorithms are Markov chain MC (MCMC) and importance sampling (IS). On the one\nhand, MCMC methods draw samples from a proposal density, building then an\nergodic Markov chain whose stationary distribution is the desired distribution\nby accepting or rejecting those candidate samples as the new state of the\nchain. On the other hand, IS techniques draw samples from a simple proposal\ndensity, and then assign them suitable weights that measure their quality in\nsome appropriate way. In this paper, we perform a thorough review of MC methods\nfor the estimation of static parameters in signal processing applications. A\nhistorical note on the development of MC schemes is also provided, followed by\nthe basic MC method and a brief description of the rejection sampling (RS)\nalgorithm, as well as three sections describing many of the most relevant MCMC\nand IS algorithms, and their combined use.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jul 2021 14:57:58 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Luengo", "D.", ""], ["Martino", "L.", ""], ["Bugallo", "M.", ""], ["Elvira", "V.", ""], ["S\u007f\u00e4rkk\u00e4", "S.", ""]]}, {"id": "2107.12554", "submitter": "Aldo Taranto", "authors": "Aldo Taranto, Ron Addie, Shahjahan Khan", "title": "Bi-Directional Grid Constrained Stochastic Processes' Link to Multi-Skew\n  Brownian Motion", "comments": "Manuscript accepted for publication in the Journal of Applied\n  Probability & Statistics and will appear in issue 1 of volume 17 to be\n  published in April 2022", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bi-Directional Grid Constrained (BGC) stochastic processes (BGCSPs) constrain\nthe random movement toward the origin steadily more and more, the further they\ndeviate from the origin, rather than all at once imposing reflective barriers,\nas does the well-established theory of It^o diffusions with such reflective\nbarriers. We identify that BGCSPs are a variant rather than a special case of\nthe multi-skew Brownian motion (M-SBM). This is because they have their own\ncomplexities, such as the barriers being hidden (not known in advance) and not\nnecessarily constant over time. We provide an M-SBM theoretical framework and\nalso a simulation framework to elaborate deeper properties of BGCSPs. The\nsimulation framework is then applied by generating numerous simulations of the\nconstrained paths and the results are analysed. BGCSPs have applications in\nfinance and indeed many other fields requiring graduated constraining, from\nboth above and below the initial position.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 02:12:51 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Taranto", "Aldo", ""], ["Addie", "Ron", ""], ["Khan", "Shahjahan", ""]]}, {"id": "2107.12890", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Subset selection for linear mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear mixed models (LMMs) are instrumental for regression analysis with\nstructured dependence, such as grouped, clustered, or multilevel data. However,\nselection among the covariates--while accounting for this structured\ndependence--remains a challenge. We introduce a Bayesian decision analysis for\nsubset selection with LMMs. Using a Mahalanobis loss function that incorporates\nthe structured dependence, we derive optimal linear actions for any subset of\ncovariates and under any Bayesian LMM. Crucially, these actions inherit\nshrinkage or regularization and uncertainty quantification from the underlying\nBayesian LMM. Rather than selecting a single \"best\" subset, which is often\nunstable and limited in its information content, we collect the acceptable\nfamily of subsets that nearly match the predictive ability of the \"best\"\nsubset. The acceptable family is summarized by its smallest member and key\nvariable importance metrics. Customized subset search and out-of-sample\napproximation algorithms are provided for more scalable computing. These tools\nare applied to simulated data and a longitudinal physical activity dataset, and\nin both cases demonstrate excellent prediction, estimation, and selection\nability.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 15:47:44 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "2107.13430", "submitter": "Kiheiji Nishida", "authors": "Kiheiji Nishida and Kanta Naito", "title": "Kernel Density Estimation by Stagewise Algorithm with a Simple\n  Dictionary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies kernel density estimation by stagewise minimization\nalgorithm with a simple dictionary on U-divergence. We randomly split an i.i.d.\nsample into the two disjoint sets, one to be used for constructing the kernels\nin the dictionary and the other for evaluating the estimator, and implement the\nalgorithm. The resulting estimator brings us data-adaptive weighting parameters\nand bandwidth matrices, and realizes a sparse representation of kernel density\nestimation. We present the non-asymptotic error bounds of our estimator and\nconfirm its performance by simulations compared with the direct plug-in\nbandwidth matrices and the reduced set density estimator.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jul 2021 17:05:06 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Nishida", "Kiheiji", ""], ["Naito", "Kanta", ""]]}, {"id": "2107.13462", "submitter": "Kasun Bandara", "authors": "Kasun Bandara, Rob J Hyndman, Christoph Bergmeir", "title": "MSTL: A Seasonal-Trend Decomposition Algorithm for Time Series with\n  Multiple Seasonal Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The decomposition of time series into components is an important task that\nhelps to understand time series and can enable better forecasting. Nowadays,\nwith high sampling rates leading to high-frequency data (such as daily, hourly,\nor minutely data), many real-world datasets contain time series data that can\nexhibit multiple seasonal patterns. Although several methods have been proposed\nto decompose time series better under these circumstances, they are often\ncomputationally inefficient or inaccurate. In this study, we propose Multiple\nSeasonal-Trend decomposition using Loess (MSTL), an extension to the\ntraditional Seasonal-Trend decomposition using Loess (STL) procedure, allowing\nthe decomposition of time series with multiple seasonal patterns. In our\nevaluation on synthetic and a perturbed real-world time series dataset,\ncompared to other decomposition benchmarks, MSTL demonstrates competitive\nresults with lower computational cost. The implementation of MSTL is available\nin the R package forecast.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2021 16:14:43 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Bandara", "Kasun", ""], ["Hyndman", "Rob J", ""], ["Bergmeir", "Christoph", ""]]}, {"id": "2107.13783", "submitter": "Federico Ferrari", "authors": "Evan Poworoznek, Federico Ferrari and David Dunson", "title": "Efficiently resolving rotational ambiguity in Bayesian matrix sampling\n  with matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide class of Bayesian models involve unidentifiable random matrices that\ndisplay rotational ambiguity, with the Gaussian factor model being a typical\nexample. A rich variety of Markov chain Monte Carlo (MCMC) algorithms have been\nproposed for sampling the parameters of these models. However, without\nidentifiability constraints, reliable posterior summaries of the parameters\ncannot be obtained directly from the MCMC output. As an alternative, we propose\na computationally efficient post-processing algorithm that allows inference on\nnon-identifiable parameters. We first orthogonalize the posterior samples using\nVarimax and then tackle label and sign switching with a greedy matching\nalgorithm. We compare the performance and computational complexity with other\nmethods using a simulation study and chemical exposures data. The algorithm\nimplementation is available in the infinitefactor R package on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jul 2021 07:25:35 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Poworoznek", "Evan", ""], ["Ferrari", "Federico", ""], ["Dunson", "David", ""]]}, {"id": "2107.14026", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Fearghal Kearney", "title": "Dynamic functional time-series forecasts of foreign exchange implied\n  volatility surfaces", "comments": "52 pages, 5 figures, to appear at the International Journal of\n  Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents static and dynamic versions of univariate, multivariate,\nand multilevel functional time-series methods to forecast implied volatility\nsurfaces in foreign exchange markets. We find that dynamic functional principal\ncomponent analysis generally improves out-of-sample forecast accuracy. More\nspecifically, the dynamic univariate functional time-series method shows the\ngreatest improvement. Our models lead to multiple instances of statistically\nsignificant improvements in forecast accuracy for daily EUR-USD, EUR-GBP, and\nEUR-JPY implied volatility surfaces across various maturities, when benchmarked\nagainst established methods. A stylised trading strategy is also employed to\ndemonstrate the potential economic benefits of our proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jul 2021 20:56:55 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Shang", "Han Lin", ""], ["Kearney", "Fearghal", ""]]}]