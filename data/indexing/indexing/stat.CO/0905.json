[{"id": "0905.0603", "submitter": "Nicole Kraemer", "authors": "Nicole Kraemer, Juliane Schaefer, Anne-Laure Boulesteix", "title": "Regularized estimation of large-scale gene association networks using\n  graphical Gaussian models", "comments": "added additional experiments", "journal-ref": "BMC Bioinformatics, 10:384, 2010", "doi": "10.1186/1471-2105-10-384", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models are popular tools for the estimation of\n(undirected) gene association networks from microarray data. A key issue when\nthe number of variables greatly exceeds the number of samples is the estimation\nof the matrix of partial correlations. Since the (Moore-Penrose) inverse of the\nsample covariance matrix leads to poor estimates in this scenario, standard\nmethods are inappropriate and adequate regularization techniques are needed. In\nthis article, we investigate a general framework for combining regularized\nregression methods with the estimation of Graphical Gaussian models. This\nframework includes various existing methods as well as two new approaches based\non ridge regression and adaptive lasso, respectively. These methods are\nextensively compared both qualitatively and quantitatively within a simulation\nstudy and through an application to six diverse real data sets. In addition,\nall proposed algorithms are implemented in the R package \"parcor\", available\nfrom the R repository CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2009 13:30:23 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2009 21:54:02 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Kraemer", "Nicole", ""], ["Schaefer", "Juliane", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "0905.2042", "submitter": "Lixing Zhu", "authors": "Jane-Ling Wang, Liugen Xue, Lixing Zhu, and Yun Sam Chong", "title": "Estimation for a Partial-Linear Single-Index Model", "comments": "43 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation for a partial-linear single-index\nmodel. A two-stage estimation procedure is proposed to estimate the link\nfunction for the single index and the parameters in the single index, as well\nas the parameters in the linear component of the model. Asymptotic normality is\nestablished for both parametric components. For the index, a constrained\nestimating equation leads to an asymptotically more efficient estimator than\nexisting estimators in the sense that it is of a smaller limiting variance. The\nestimator of the nonparametric link function achieves optimal convergence\nrates; and the structural error variance is obtained. In addition, the results\nfacilitate the construction of confidence regions and hypothesis testing for\nthe unknown parameters. A simulation study is performed and an application to a\nreal dataset is illustrated. The extension to multiple indices is briefly\nsketched.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 09:18:50 GMT"}], "update_date": "2009-05-14", "authors_parsed": [["Wang", "Jane-Ling", ""], ["Xue", "Liugen", ""], ["Zhu", "Lixing", ""], ["Chong", "Yun Sam", ""]]}, {"id": "0905.2181", "submitter": "Xuemin Tu", "authors": "Alexandre J. Chorin and Xuemin Tu", "title": "Non-Bayesian particle filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Particle filters for data assimilation in nonlinear problems use \"particles\"\n(replicas of the underlying system) to generate a sequence of probability\ndensity functions (pdfs) through a Bayesian process. This can be expensive\nbecause a significant number of particles has to be used to maintain accuracy.\nWe offer here an alternative, in which the relevant pdfs are sampled directly\nby an iteration. An example is discussed in detail.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2009 23:54:51 GMT"}], "update_date": "2009-05-15", "authors_parsed": [["Chorin", "Alexandre J.", ""], ["Tu", "Xuemin", ""]]}, {"id": "0905.2441", "submitter": "Anthony Lee", "authors": "Anthony Lee, Christopher Yau, Michael B. Giles, Arnaud Doucet,\n  Christopher C. Holmes", "title": "On the utility of graphics cards to perform massively parallel\n  simulation of advanced Monte Carlo methods", "comments": "Expansion of details; a condensed version has been submitted for\n  publication", "journal-ref": null, "doi": "10.1198/jcgs.2010.10039", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study on the utility of graphics cards to perform massively\nparallel simulation of advanced Monte Carlo methods. Graphics cards, containing\nmultiple Graphics Processing Units (GPUs), are self-contained parallel\ncomputational devices that can be housed in conventional desktop and laptop\ncomputers. For certain classes of Monte Carlo algorithms they offer massively\nparallel simulation, with the added advantage over conventional distributed\nmulti-core processors that they are cheap, easily accessible, easy to maintain,\neasy to code, dedicated local devices with low power consumption. On a\ncanonical set of stochastic simulation examples including population-based\nMarkov chain Monte Carlo methods and Sequential Monte Carlo methods, we find\nspeedups from 35 to 500 fold over conventional single-threaded computer code.\nOur findings suggest that GPUs have the potential to facilitate the growth of\nstatistical modelling into complex data rich domains through the availability\nof cheap and accessible many-core computation. We believe the speedup we\nobserve should motivate wider use of parallelizable simulation methods and\ngreater methodological attention to their design.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2009 22:43:29 GMT"}, {"version": "v2", "created": "Mon, 18 May 2009 17:31:11 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2009 17:45:46 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Lee", "Anthony", ""], ["Yau", "Christopher", ""], ["Giles", "Michael B.", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Christopher C.", ""]]}, {"id": "0905.2646", "submitter": "Yaming Yu", "authors": "Yaming Yu", "title": "Monotonic convergence of a general algorithm for computing optimal\n  designs", "comments": "Published in at http://dx.doi.org/10.1214/09-AOS761 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2010, Vol. 38, No. 3, 1593-1606", "doi": "10.1214/09-AOS761", "report-no": "IMS-AOS-AOS761", "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monotonic convergence is established for a general class of multiplicative\nalgorithms introduced by Silvey, Titterington and Torsney [Comm. Statist.\nTheory Methods 14 (1978) 1379--1389] for computing optimal designs. A\nconjecture of Titterington [Appl. Stat. 27 (1978) 227--234] is confirmed as a\nconsequence. Optimal designs for logistic regression are used as an\nillustration.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2009 18:53:53 GMT"}, {"version": "v2", "created": "Tue, 19 May 2009 06:54:14 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2010 18:13:54 GMT"}, {"version": "v4", "created": "Tue, 5 Oct 2010 07:24:29 GMT"}], "update_date": "2010-10-06", "authors_parsed": [["Yu", "Yaming", ""]]}, {"id": "0905.2979", "submitter": "Jo Bovy", "authors": "Jo Bovy, David W. Hogg, Sam T. Roweis", "title": "Extreme deconvolution: Inferring complete distribution functions from\n  noisy, heterogeneous and incomplete observations", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS439 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1657-1677", "doi": "10.1214/10-AOAS439", "report-no": "IMS-AOAS-AOAS439", "categories": "stat.ME astro-ph.GA physics.data-an stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the well-known mixtures of Gaussians approach to density\nestimation and the accompanying Expectation--Maximization technique for finding\nthe maximum likelihood parameters of the mixture to the case where each data\npoint carries an individual $d$-dimensional uncertainty covariance and has\nunique missing data properties. This algorithm reconstructs the\nerror-deconvolved or \"underlying\" distribution function common to all samples,\neven when the individual data points are samples from different distributions,\nobtained by convolving the underlying distribution with the heteroskedastic\nuncertainty distribution of the data point and projecting out the missing data\ndirections. We show how this basic algorithm can be extended with conjugate\npriors on all of the model parameters and a \"split-and-merge\" procedure\ndesigned to avoid local maxima of the likelihood. We demonstrate the full\nmethod by applying it to the problem of inferring the three-dimensional\nvelocity distribution of stars near the Sun from noisy two-dimensional,\ntransverse velocity measurements from the Hipparcos satellite.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2009 16:26:26 GMT"}, {"version": "v2", "created": "Fri, 29 Jul 2011 10:31:54 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Bovy", "Jo", ""], ["Hogg", "David W.", ""], ["Roweis", "Sam T.", ""]]}, {"id": "0905.3217", "submitter": "Patrick J. Wolfe", "authors": "Keigo Hirakawa and Patrick J. Wolfe", "title": "Skellam shrinkage: Wavelet-based intensity estimation for inhomogeneous\n  Poisson data", "comments": "27 pages, 8 figures, slight formatting changes; submitted for\n  publication", "journal-ref": "IEEE Transactions on Information Theory, vol. 58, pp. 1080-1093,\n  2012", "doi": "10.1109/TIT.2011.2165933", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ubiquity of integrating detectors in imaging and other applications\nimplies that a variety of real-world data are well modeled as Poisson random\nvariables whose means are in turn proportional to an underlying vector-valued\nsignal of interest. In this article, we first show how the so-called Skellam\ndistribution arises from the fact that Haar wavelet and filterbank transform\ncoefficients corresponding to measurements of this type are distributed as sums\nand differences of Poisson counts. We then provide two main theorems on Skellam\nshrinkage, one showing the near-optimality of shrinkage in the Bayesian setting\nand the other providing for unbiased risk estimation in a frequentist context.\nThese results serve to yield new estimators in the Haar transform domain,\nincluding an unbiased risk estimate for shrinkage of Haar-Fisz\nvariance-stabilized data, along with accompanying low-complexity algorithms for\ninference. We conclude with a simulation study demonstrating the efficacy of\nour Skellam shrinkage estimators both for the standard univariate wavelet test\nfunctions as well as a variety of test images taken from the image processing\nliterature, confirming that they offer substantial performance improvements\nover existing alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2009 05:05:14 GMT"}, {"version": "v2", "created": "Fri, 29 May 2009 17:33:23 GMT"}], "update_date": "2012-10-15", "authors_parsed": [["Hirakawa", "Keigo", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0905.4131", "submitter": "Iuliana Teodorescu", "authors": "Iuliana Teodorescu", "title": "Maximum Likelihood Estimation for Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for optimal estimation of Markov chains with sparse transition\nmatrices is presented.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2009 08:20:50 GMT"}], "update_date": "2009-05-27", "authors_parsed": [["Teodorescu", "Iuliana", ""]]}, {"id": "0905.4602", "submitter": "Piero Barone", "authors": "Piero Barone", "title": "A black box method for solving the complex exponentials approximation\n  problem", "comments": "43 pages, 10 figures", "journal-ref": "Digital Signal Processing (2012)", "doi": "10.1016/j.dsp.2012.09.005", "report-no": null, "categories": "stat.CO math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem, arising in many different applied contexts, consists in\nestimating the number of exponentially damped sinusoids whose weighted sum best\nfits a finite set of noisy data and in estimating their parameters. Many\ndifferent methods exist to this purpose. The best of them are based on\napproximate Maximum Likelihood estimators, assuming to know the number of\ndamped sinusoids, which can then be estimated by an order selection procedure.\nAs the problem can be severely ill posed, a stochastic perturbation method is\nproposed which provides better results than Maximum Likelihood based methods\nwhen the signal-to-noise ratio is low. The method depends on some\nhyperparameters which turn out to be essentially independent of the\napplication. Therefore they can be fixed once and for all, giving rise to a\nblack box method.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2009 10:21:19 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 13:04:56 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Barone", "Piero", ""]]}]