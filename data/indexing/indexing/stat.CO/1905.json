[{"id": "1905.00105", "submitter": "Christian Staerk", "authors": "Christian Staerk, Maria Kateri, Ioannis Ntzoufras", "title": "High-dimensional variable selection via low-dimensional adaptive\n  learning", "comments": null, "journal-ref": "Electronic Journal of Statistics, 15(1), 830-879 (2021)", "doi": "10.1214/21-EJS1797", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stochastic search method, the so-called Adaptive Subspace (AdaSub) method,\nis proposed for variable selection in high-dimensional linear regression\nmodels. The method aims at finding the best model with respect to a certain\nmodel selection criterion and is based on the idea of adaptively solving\nlow-dimensional sub-problems in order to provide a solution to the original\nhigh-dimensional problem. Any of the usual $\\ell_0$-type model selection\ncriteria can be used, such as Akaike's Information Criterion (AIC), the\nBayesian Information Criterion (BIC) or the Extended BIC (EBIC), with the last\nbeing particularly suitable for high-dimensional cases. The limiting properties\nof the new algorithm are analysed and it is shown that, under certain\nconditions, AdaSub converges to the best model according to the considered\ncriterion. In a simulation study, the performance of AdaSub is investigated in\ncomparison to alternative methods. The effectiveness of the proposed method is\nillustrated via various simulated datasets and a high-dimensional real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 15:32:23 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 10:03:19 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 12:41:23 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Staerk", "Christian", ""], ["Kateri", "Maria", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "1905.00141", "submitter": "Huang Huang", "authors": "Huang Huang, Lewis R. Blake, Dorit M. Hammerling", "title": "Pushing the Limit: A Hybrid Parallel Implementation of the\n  Multi-resolution Approximation for Massive Data", "comments": null, "journal-ref": null, "doi": "10.5065/nnt6-q689", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-resolution approximation (MRA) of Gaussian processes was recently\nproposed to conduct likelihood-based inference for massive spatial data sets.\nAn advantage of the methodology is that it can be parallelized. We implemented\nthe MRA in C++ for both serial and parallel versions. In the parallel\nimplementation, we use a hybrid parallelism that employs both distributed and\nshared memory computing for communications between and within nodes by using\nthe Message Passing Interface (MPI) and OpenMP, respectively. The performance\nof the serial code is compared between the C++ and MATLAB implementations over\na small data set on a personal laptop. The C++ parallel program is further\ncarefully studied under different configurations by applications to data sets\nfrom around a tenth of a million to 47 million observations. We show the\npracticality of this implementation by demonstrating that we can get quick\ninference for massive real-world data sets. The serial and parallel C++ code\ncan be found at https://github.com/hhuang90.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 23:52:54 GMT"}, {"version": "v2", "created": "Mon, 6 May 2019 03:10:06 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Huang", "Huang", ""], ["Blake", "Lewis R.", ""], ["Hammerling", "Dorit M.", ""]]}, {"id": "1905.00353", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene, Cesar Cristancho, Mariana Ospina, Domingo Morales", "title": "Prevalence of international migration: an alternative for small area\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an alternative procedure for estimating the prevalence\nof international migration at the municipal level in Colombia. The new\nmethodology uses the empirical best linear unbiased predictor based on a\nFay-Herriot model with target and auxiliary variables available from census\nstudies and from the Demographic and Health Survey. The proposed alternative\nproduces prevalence estimates which are consistent with sample sizes and\ndemographic dynamics in Colombia. Additionally, the estimated coefficients of\nvariation are lower than 20% for municipalities and large\ndemographically-relevant capital cities and therefore estimates may be\nconsidered as reliable.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 20:15:26 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Fuquene", "Jairo", ""], ["Cristancho", "Cesar", ""], ["Ospina", "Mariana", ""], ["Morales", "Domingo", ""]]}, {"id": "1905.00515", "submitter": "Jonathan Weare", "authors": "David A. Plotkin, Robert J. Webber, Morgan E O'Neill, Jonathan Weare,\n  Dorian S. Abbot", "title": "Maximizing simulated tropical cyclone intensity with action minimization", "comments": null, "journal-ref": null, "doi": "10.1029/2018MS001419", "report-no": null, "categories": "physics.ao-ph math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct computer simulation of intense tropical cyclones (TCs) in weather\nmodels is limited by computational expense. Intense TCs are rare and have\nsmall-scale structures, making it difficult to produce large ensembles of\nstorms at high resolution. Further, models often fail to capture the process of\nrapid intensification, which is a distinguishing feature of many intense TCs.\nUnderstanding rapid intensification is especially important in the context of\nglobal warming, which may increase the frequency of intense TCs. To better\nleverage computational resources for the study of rapid intensification, we\nintroduce an action minimization algorithm applied to the WRF and WRFPLUS\nmodels. Action minimization nudges the model into forming more intense TCs than\nit otherwise would; it does so via the maximum likelihood path in a stochastic\nformulation of the model, thereby allowing targeted study of intensification\nmechanisms.\n  We apply action minimization to simulations of Hurricanes Danny (2015) and\nFred (2009) at 6 km resolution to demonstrate that the algorithm consistently\nintensifies TCs via physically plausible pathways. We show an approximately\nten-fold computational savings using action minimization to study the tail of\nthe TC intensification distribution. Further, for Hurricanes Danny and Fred,\naction minimization produces perturbations that preferentially reduce low-level\nshear as compared to upper-level shear, at least above a threshold of\napproximately $4 \\mathrm{\\ m \\ s^{-1}}$. We also demonstrate that asymmetric,\ntime-dependent patterns of heating can cause significant TC intensification\nbeyond symmetric, azimuthally-averaged heating and find a regime of non-linear\nresponse to asymmetric heating that has not been extensively studied in\nprevious work.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 21:57:09 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Plotkin", "David A.", ""], ["Webber", "Robert J.", ""], ["O'Neill", "Morgan E", ""], ["Weare", "Jonathan", ""], ["Abbot", "Dorian S.", ""]]}, {"id": "1905.00989", "submitter": "Matthew Parno", "authors": "M. D. Parno, B. A. West, A. J. Song, T. S. Hodgdon, and D. T. O'Connor", "title": "Remote measurement of sea ice dynamics with regularized optimal\n  transport", "comments": null, "journal-ref": null, "doi": "10.1029/2019GL083037", "report-no": null, "categories": "cs.CV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As Arctic conditions rapidly change, human activity in the Arctic will\ncontinue to increase and so will the need for high-resolution observations of\nsea ice. While satellite imagery can provide high spatial resolution, it is\ntemporally sparse and significant ice deformation can occur between\nobservations. This makes it difficult to apply feature tracking or image\ncorrelation techniques that require persistent features to exist between\nimages. With this in mind, we propose a technique based on optimal transport,\nwhich is commonly used to measure differences between probability\ndistributions. When little ice enters or leaves the image scene, we show that\nregularized optimal transport can be used to quantitatively estimate ice\ndeformation. We discuss the motivation for our approach and describe efficient\ncomputational implementations. Results are provided on a combination of\nsynthetic and MODIS imagery to demonstrate the ability of our approach to\nestimate dynamics properties at the original image resolution.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 22:47:22 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Parno", "M. D.", ""], ["West", "B. A.", ""], ["Song", "A. J.", ""], ["Hodgdon", "T. S.", ""], ["O'Connor", "D. T.", ""]]}, {"id": "1905.01252", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael Gutmann, Aki Vehtari, Pekka Marttinen", "title": "Parallel Gaussian process surrogate Bayesian inference with noisy\n  likelihood evaluations", "comments": "Minor changes to the text. 37 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference when only a limited number of noisy\nlog-likelihood evaluations can be obtained. This occurs for example when\ncomplex simulator-based statistical models are fitted to data, and synthetic\nlikelihood (SL) method is used to form the noisy log-likelihood estimates using\ncomputationally costly forward simulations. We frame the inference task as a\nsequential Bayesian experimental design problem, where the log-likelihood\nfunction is modelled with a hierarchical Gaussian process (GP) surrogate model,\nwhich is used to efficiently select additional log-likelihood evaluation\nlocations. Motivated by recent progress in the related problem of batch\nBayesian optimisation, we develop various batch-sequential design strategies\nwhich allow to run some of the potentially costly simulations in parallel. We\nanalyse the properties of the resulting method theoretically and empirically.\nExperiments with several toy problems and simulation models suggest that our\nmethod is robust, highly parallelisable, and sample-efficient.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 16:18:50 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 11:31:22 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 15:19:26 GMT"}, {"version": "v4", "created": "Fri, 6 Mar 2020 13:35:20 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1905.01413", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin, Yuguang Yue, Mingyuan Zhou", "title": "ARSM: Augment-REINFORCE-Swap-Merge Estimator for Gradient\n  Backpropagation Through Categorical Variables", "comments": "Published in ICML 2019. We have updated Section 4.2 and the Appendix\n  to reflect the improvements brought by fixing some bugs hidden in our\n  original code. Please find the Errata in the authors' websites and check the\n  updated code in Github", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the challenge of backpropagating the gradient through categorical\nvariables, we propose the augment-REINFORCE-swap-merge (ARSM) gradient\nestimator that is unbiased and has low variance. ARSM first uses variable\naugmentation, REINFORCE, and Rao-Blackwellization to re-express the gradient as\nan expectation under the Dirichlet distribution, then uses variable swapping to\nconstruct differently expressed but equivalent expectations, and finally shares\ncommon random numbers between these expectations to achieve significant\nvariance reduction. Experimental results show ARSM closely resembles the\nperformance of the true gradient for optimization in univariate settings;\noutperforms existing estimators by a large margin when applied to categorical\nvariational auto-encoders; and provides a \"try-and-see self-critic\" variance\nreduction method for discrete-action policy gradient, which removes the need of\nestimating baselines by generating a random number of pseudo actions and\nestimating their action-value functions.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 02:26:09 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 16:43:00 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Yin", "Mingzhang", ""], ["Yue", "Yuguang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.01455", "submitter": "Achmad Choiruddin", "authors": "Achmad Choiruddin and Francisco Cuevas-Pacheco and Jean-Fran\\c{c}ois\n  Coeurjolly and Rasmus Waagepetersen", "title": "Regularized estimation for highly multivariate log Gaussian Cox\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference for highly multivariate point pattern data is\nchallenging due to complex models with large numbers of parameters. In this\npaper, we develop numerically stable and efficient parameter estimation and\nmodel selection algorithms for a class of multivariate log Gaussian Cox\nprocesses. The methodology is applied to a highly multivariate point pattern\ndata set from tropical rain forest ecology.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 08:18:44 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Choiruddin", "Achmad", ""], ["Cuevas-Pacheco", "Francisco", ""], ["Coeurjolly", "Jean-Fran\u00e7ois", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1905.01707", "submitter": "Philippe Casgrain", "authors": "Philippe Casgrain", "title": "A Latent Variational Framework for Stochastic Optimization", "comments": "8 pages main content, 8 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a unifying theoretical framework for stochastic\noptimization algorithms by means of a latent stochastic variational problem.\nUsing techniques from stochastic control, the solution to the variational\nproblem is shown to be equivalent to that of a Forward Backward Stochastic\nDifferential Equation (FBSDE). By solving these equations, we recover a variety\nof existing adaptive stochastic gradient descent methods. This framework\nestablishes a direct connection between stochastic optimization algorithms and\na secondary Bayesian inference problem on gradients, where a prior measure on\nnoisy gradient observations determines the resulting algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 15:59:51 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 12:22:56 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 02:33:12 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 03:18:02 GMT"}, {"version": "v5", "created": "Sun, 27 Oct 2019 15:36:55 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Casgrain", "Philippe", ""]]}, {"id": "1905.01745", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Faster polytope rounding, sampling, and volume computation via a\n  sublinear \"Ball Walk\"", "comments": "Accepted to IEEE Symposium on Foundations of Computer Science (FOCS)\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of \"isotropically rounding\" a polytope\n$K\\subset\\mathbb{R}^n$, that is, computing a linear transformation which makes\nthe uniform distribution on the polytope have roughly identity covariance\nmatrix. We assume $K$ is defined by $m$ linear inequalities, with guarantee\nthat $rB\\subset K\\subset RB$, where $B$ is the unit ball. We introduce a new\nvariant of the ball walk Markov chain and show that, roughly, the expected\nnumber of arithmetic operations per-step of this Markov chain is $O(m)$ that is\nsublinear in the input size $mn$--the per-step time of all prior Markov chains.\nSubsequently, we give a rounding algorithm that succeeds with probability\n$1-\\varepsilon$ in\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$\narithmetic operations. This gives a factor of $\\sqrt{n}$ improvement on the\nprevious bound of\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$ for\nrounding, which uses the hit-and-run algorithm. Since the rounding\npreprocessing step is in many cases the bottleneck in improving sampling or\nvolume computation, our results imply these tasks can also be achieved in\nroughly\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})+mn^4\\delta^{-2})$\noperations for computing the volume of $K$ up to a factor $1+\\delta$ and\n$\\tilde{O}(mn^{4.5}\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})))$ for\nuniformly sampling on $K$ with TV error $\\varepsilon$. This improves on the\nprevious bounds of\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r})+mn^4\\delta^{-2})$\nfor volume computation when roughly $m\\geq n^{2.5}$, and\n$\\tilde{O}(mn^5\\mbox{polylog}(\\frac{1}{\\varepsilon},\\frac{R}{r}))$ for sampling\nwhen roughly $m\\geq n^{1.5}$. We achieve this improvement by a novel method of\ncomputing polytope membership, where one avoids checking inequalities estimated\nto have a very low probability of being violated.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 20:52:25 GMT"}, {"version": "v2", "created": "Sun, 15 Sep 2019 02:09:14 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1905.01776", "submitter": "Vince Lyzinski", "authors": "Joshua Agterberg, Youngser Park, Jonathan Larson, Christopher White,\n  Carey E. Priebe, and Vince Lyzinski", "title": "Vertex Nomination, Consistent Estimation, and Adversarial Modification", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of graphs $G_1$ and $G_2$ and a vertex set of interest in $G_1$,\nthe vertex nomination (VN) problem seeks to find the corresponding vertices of\ninterest in $G_2$ (if they exist) and produce a rank list of the vertices in\n$G_2$, with the corresponding vertices of interest in $G_2$ concentrating,\nideally, at the top of the rank list. In this paper, we define and derive the\nanalogue of Bayes optimality for VN with multiple vertices of interest, and we\ndefine the notion of maximal consistency classes in vertex nomination. This\ntheory forms the foundation for a novel VN adversarial contamination model, and\nwe demonstrate with real and simulated data that there are VN schemes that\nperform effectively in the uncontaminated setting, and adversarial network\ncontamination adversely impacts the performance of our VN scheme. We further\ndefine a network regularization method for mitigating the impact of the\nadversarial contamination, and we demonstrate the effectiveness of\nregularization in both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 00:55:29 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 03:00:39 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 16:41:46 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Agterberg", "Joshua", ""], ["Park", "Youngser", ""], ["Larson", "Jonathan", ""], ["White", "Christopher", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1905.01805", "submitter": "Eric Bax", "authors": "Eric Bax", "title": "Computing a Data Dividend", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.CY econ.GN q-fin.EC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quality data is a fundamental contributor to success in statistics and\nmachine learning. If a statistical assessment or machine learning leads to\ndecisions that create value, data contributors may want a share of that value.\nThis paper presents methods to assess the value of individual data samples, and\nof sets of samples, to apportion value among different data contributors. We\nuse Shapley values for individual samples and Owen values for combined samples,\nand show that these values can be computed in polynomial time in spite of their\ndefinitions having numbers of terms that are exponential in the number of\nsamples.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 02:51:29 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 16:38:34 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Bax", "Eric", ""]]}, {"id": "1905.02086", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Nicolas Tremblay, Alexandre Gaudilli\\`ere, Luca\n  Avena and Pierre-Olivier Amblard", "title": "Estimating the inverse trace using random forests on graphs", "comments": "Submitted to GRETSI conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some data analysis problems require the computation of (regularised) inverse\ntraces, i.e. quantities of the form $\\Tr (q \\bI + \\bL)^{-1}$. For large\nmatrices, direct methods are unfeasible and one must resort to approximations,\nfor example using a conjugate gradient solver combined with Girard's trace\nestimator (also known as Hutchinson's trace estimator). Here we describe an\nunbiased estimator of the regularized inverse trace, based on Wilson's\nalgorithm, an algorithm that was initially designed to draw uniform spanning\ntrees in graphs. Our method is fast, easy to implement, and scales to very\nlarge matrices. Its main drawback is that it is limited to diagonally dominant\nmatrices $\\bL$.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 15:08:08 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Tremblay", "Nicolas", ""], ["Gaudilli\u00e8re", "Alexandre", ""], ["Avena", "Luca", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "1905.02679", "submitter": "Boris Kramer", "authors": "Boris Kramer, Alexandre Noll Marques, Benjamin Peherstorfer, Umberto\n  Villa, Karen Willcox", "title": "Multifidelity probability estimation via fusion of estimators", "comments": null, "journal-ref": "Journal of Computational Physics 392, 385-402, 2019", "doi": "10.1016/j.jcp.2019.04.071", "report-no": null, "categories": "stat.CO cs.CE cs.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops a multifidelity method that enables estimation of failure\nprobabilities for expensive-to-evaluate models via information fusion and\nimportance sampling. The presented general fusion method combines multiple\nprobability estimators with the goal of variance reduction. We use low-fidelity\nmodels to derive biasing densities for importance sampling and then fuse the\nimportance sampling estimators such that the fused multifidelity estimator is\nunbiased and has mean-squared error lower than or equal to that of any of the\nimportance sampling estimators alone. By fusing all available estimators, the\nmethod circumvents the challenging problem of selecting the best biasing\ndensity and using only that density for sampling. A rigorous analysis shows\nthat the fused estimator is optimal in the sense that it has minimal variance\namongst all possible combinations of the estimators. The asymptotic behavior of\nthe proposed method is demonstrated on a convection-diffusion-reaction partial\ndifferential equation model for which $10^5$ samples can be afforded. To\nillustrate the proposed method at scale, we consider a model of a free plane\njet and quantify how uncertainties at the flow inlet propagate to a quantity of\ninterest related to turbulent mixing. Compared to an importance sampling\nestimator that uses the high-fidelity model alone, our multifidelity estimator\nreduces the required CPU time by 65\\% while achieving a similar coefficient of\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:33:34 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Kramer", "Boris", ""], ["Marques", "Alexandre Noll", ""], ["Peherstorfer", "Benjamin", ""], ["Villa", "Umberto", ""], ["Willcox", "Karen", ""]]}, {"id": "1905.02939", "submitter": "Saifuddin Syed", "authors": "Saifuddin Syed, Alexandre Bouchard-C\\^ot\\'e, George Deligiannidis,\n  Arnaud Doucet", "title": "Non-Reversible Parallel Tempering: a Scalable Highly Parallel MCMC\n  Scheme", "comments": "74 pages, 30 figures. The method is implemented in an open source\n  probabilistic programming available at\n  https://github.com/UBC-Stat-ML/blangSDK", "journal-ref": null, "doi": "10.1111/rssb.12464", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel tempering (PT) methods are a popular class of Markov chain Monte\nCarlo schemes used to sample complex high-dimensional probability\ndistributions. They rely on a collection of $N$ interacting auxiliary chains\ntargeting tempered versions of the target distribution to improve the\nexploration of the state-space. We provide here a new perspective on these\nhighly parallel algorithms and their tuning by identifying and formalizing a\nsharp divide in the behaviour and performance of reversible versus\nnon-reversible PT schemes. We show theoretically and empirically that a class\nof non-reversible PT methods dominates its reversible counterparts and identify\ndistinct scaling limits for the non-reversible and reversible schemes, the\nformer being a piecewise-deterministic Markov process and the latter a\ndiffusion. These results are exploited to identify the optimal annealing\nschedule for non-reversible PT and to develop an iterative scheme approximating\nthis schedule. We provide a wide range of numerical examples supporting our\ntheoretical and methodological contributions. The proposed methodology is\napplicable to sample from a distribution $\\pi$ with a density $L$ with respect\nto a reference distribution $\\pi_0$ and compute the normalizing constant. A\ntypical use case is when $\\pi_0$ is a prior distribution, $L$ a likelihood\nfunction and $\\pi$ the corresponding posterior.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 07:22:30 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 03:21:25 GMT"}, {"version": "v3", "created": "Sun, 22 Nov 2020 23:48:17 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 23:28:08 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Syed", "Saifuddin", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1905.02944", "submitter": "Yoann Altmann", "authors": "Yoann Altmann, Stephen McLaughlin, Michael E. Davies", "title": "Fast online 3D reconstruction of dynamic scenes from individual\n  single-photon detection events", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2019.2952008", "report-no": null, "categories": "eess.IV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an algorithm for online 3D reconstruction of\ndynamic scenes using individual times of arrival (ToA) of photons recorded by\nsingle-photon detector arrays. One of the main challenges in 3D imaging using\nsingle-photon Lidar is the integration time required to build ToA histograms\nand reconstruct reliable 3D profiles in the presence of non-negligible ambient\nillumination. This long integration time also prevents the analysis of rapid\ndynamic scenes using existing techniques. We propose a new method which does\nnot rely on the construction of ToA histograms but allows, for the first time,\nindividual detection events to be processed online, in a parallel manner in\ndifferent pixels, while accounting for the intrinsic spatiotemporal structure\nof dynamic scenes. Adopting a Bayesian approach, a Bayesian model is\nconstructed to capture the dynamics of the 3D profile and an approximate\ninference scheme based on assumed density filtering is proposed, yielding a\nfast and robust reconstruction algorithm able to process efficiently thousands\nto millions of frames, as usually recorded using single-photon detectors. The\nperformance of the proposed method, able to process hundreds of frames per\nsecond, is assessed using a series of experiments conducted with static and\ndynamic 3D scenes and the results obtained pave the way to a new family of\nreal-time 3D reconstruction solutions.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 07:41:33 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Altmann", "Yoann", ""], ["McLaughlin", "Stephen", ""], ["Davies", "Michael E.", ""]]}, {"id": "1905.03138", "submitter": "Ehsan Ebrahimzadeh", "authors": "Ehsan Ebrahimzadeh, Maggie Engler, David Tse, Razvan Cristescu, Aslan\n  Tchamkerten", "title": "Somatic mutations render human exome and pathogen DNA more similar", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0197949", "report-no": null, "categories": "q-bio.GN cs.IT math.IT math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Immunotherapy has recently shown important clinical successes in a\nsubstantial number of oncology indications. Additionally, the tumor somatic\nmutation load has been shown to associate with response to these therapeutic\nagents, and specific mutational signatures are hypothesized to improve this\nassociation, including signatures related to pathogen insults. We sought to\nstudy in silico the validity of these observations and how they relate to each\nother. We first addressed whether somatic mutations typically involved in\ncancer may increase, in a statistically meaningful manner, the similarity\nbetween common pathogens and the human exome. Our study shows that common\nmutagenic processes increase, in the upper range of biologically plausible\nfrequencies, the similarity between cancer exomes and pathogen DNA at a scale\nof 12-16 nucleotide sequences and established that this increased similarity is\ndue to the specific mutation distribution of the considered mutagenic\nprocesses. Next, we studied the impact of mutation rate and showed that\nincreasing mutation rate generally results in an increased similarity between\nthe cancer exome and pathogen DNA, at a scale of 4-5 amino acids. Finally, we\ninvestigated whether the considered mutational processes result in amino-acid\nchanges with functional relevance that are more likely to be immunogenic. We\nshowed that functional tolerance to mutagenic processes across species\ngenerally suggests more resilience to mutagenic processes that are due to\nexposure to elements of nature than to mutagenic processes that are due to\nexposure to cancer-causing artificial substances. These results support the\nidea that recognition of pathogen sequences as well as differential functional\ntolerance to mutagenic processes may play an important role in the immune\nrecognition process involved in tumor infiltration by lymphocytes.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:52:34 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Ebrahimzadeh", "Ehsan", ""], ["Engler", "Maggie", ""], ["Tse", "David", ""], ["Cristescu", "Razvan", ""], ["Tchamkerten", "Aslan", ""]]}, {"id": "1905.03673", "submitter": "Chris Oates", "authors": "Wilson Ye Chen, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Jackson\n  Gorham, Mark Girolami, Lester Mackey, Chris. J. Oates", "title": "Stein Point Markov Chain Monte Carlo", "comments": "Minor bug fixed in Theorem 4 (result unchanged)", "journal-ref": "ICML 2019", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in machine learning and statistics is the approximation of\na probability measure by an empirical measure supported on a discrete point\nset. Stein Points are a class of algorithms for this task, which proceed by\nsequentially minimising a Stein discrepancy between the empirical measure and\nthe target and, hence, require the solution of a non-convex optimisation\nproblem to obtain each new point. This paper removes the need to solve this\noptimisation problem by, instead, selecting each new point based on a Markov\nchain sample path. This significantly reduces the computational cost of Stein\nPoints and leads to a suite of algorithms that are straightforward to\nimplement. The new algorithms are illustrated on a set of challenging Bayesian\ninference problems, and rigorous theoretical guarantees of consistency are\nestablished.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:57:02 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 08:16:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Gorham", "Jackson", ""], ["Girolami", "Mark", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1905.03760", "submitter": "Lifeng Ye", "authors": "Lifeng Ye, Alexandros Beskos, Maria De Iorio, and Jie Hao", "title": "Monte Carlo Co-Ordinate Ascent Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Variational Inference (VI), coordinate-ascent and gradient-based\napproaches are two major types of algorithms for approximating\ndifficult-to-compute probability densities. In real-world implementations of\ncomplex models, Monte Carlo methods are widely used to estimate expectations in\ncoordinate-ascent approaches and gradients in derivative-driven ones. We\ndiscuss a Monte Carlo Co-ordinate Ascent VI (MC-CAVI) algorithm that makes use\nof Markov chain Monte Carlo (MCMC) methods in the calculation of expectations\nrequired within Co-ordinate Ascent VI (CAVI). We show that, under regularity\nconditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of\nthe evidence lower bound (ELBO) with any given high probability. In numerical\nexamples, the performance of MC-CAVI algorithm is compared with that of MCMC\nand -- as a representative of derivative-based VI methods -- of Black Box VI\n(BBVI). We discuss and demonstrate MC-CAVI's suitability for models with hard\nconstraints in simulated and real examples. We compare MC-CAVI's performance\nwith that of MCMC in an important complex model used in Nuclear Magnetic\nResonance (NMR) spectroscopy data analysis -- BBVI is nearly impossible to be\nemployed in this setting due to the hard constraints involved in the model.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 17:18:45 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 16:27:43 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Ye", "Lifeng", ""], ["Beskos", "Alexandros", ""], ["De Iorio", "Maria", ""], ["Hao", "Jie", ""]]}, {"id": "1905.04092", "submitter": "Sean Pinkney", "authors": "Tyler Morrison and Sean Pinkney", "title": "Generating Random Samples from Non-Identical Truncated Order Statistics", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide an efficient algorithm to generate random samples from the bounded\nkth order statistic in a sample of independent, but not necessarily identically\ndistributed, random variables. The bounds can be upper or lower bounds and need\nonly hold on the kth order statistic. Furthermore, we require access to the\ninverse CDF for each statistic in the ordered sample. The algorithm is slightly\nslower than rejection sampling when the density of the bounded statistic is\nlarge, however, it is significantly faster when the bounded density becomes\nsparse. We provide a practical example and a simulation that shows the\nsuperiority of this method for sparse regions arising from tight boundary\nconditions and/or over regions of low probability density.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 11:55:45 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Morrison", "Tyler", ""], ["Pinkney", "Sean", ""]]}, {"id": "1905.04252", "submitter": "Filip Seitl", "authors": "F. Seitl, L. Petrich, J. Stan\\v{e}k, C.E. Krill III, V. Schmidt, V.\n  Bene\\v{s}", "title": "Exploration of Gibbs-Laguerre tessellations for three-dimensional\n  stochastic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random tessellations are well suited for probabilistic modeling of\nthree-dimensional (3D) grain microstructures of polycrystalline materials. The\npresent paper is focused on so-called Gibbs-Laguerre tessellations, in which\nthe generators of the Laguerre tessellation form a Gibbs point process. The\ngoal is to construct an energy function of the Gibbs point process such that\nthe resulting tessellation matches some desired geometrical properties. Since\nthe model is analytically intractable, our main tool of analysis is stochastic\nsimulation based on Markov chain Monte Carlo. Such simulations enable us to\ninvestigate the properties of the models, and, in the next step, to apply the\nknowledge gained to the statistical reconstruction of the 3D microstructure of\nan aluminum alloy extracted from 3D tomographic image data.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 16:39:59 GMT"}, {"version": "v2", "created": "Thu, 21 Nov 2019 18:06:10 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Seitl", "F.", ""], ["Petrich", "L.", ""], ["Stan\u011bk", "J.", ""], ["Krill", "C. E.", "III"], ["Schmidt", "V.", ""], ["Bene\u0161", "V.", ""]]}, {"id": "1905.04492", "submitter": "Erik-Jan van Kesteren", "authors": "Erik-Jan van Kesteren and Daniel L. Oberski", "title": "Structural Equation Models as Computation Graphs", "comments": "R code and package are available online as supplementary material at\n  https://github.com/vankesteren/sem-computationgraphs and\n  https://github.com/vankesteren/tensorsem/tree/computationgraph, respectively", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural equation modeling (SEM) is a popular tool in the social and\nbehavioural sciences, where it is being applied to ever more complex data\ntypes. The high-dimensional data produced by modern sensors, brain images, or\n(epi)genetic measurements require variable selection using parameter\npenalization; experimental models combining disparate data sources benefit from\nregularization to obtain a stable result; and genomic SEM or network models\nlead to alternative objective functions. With each proposed extension,\nresearchers currently have to completely reformulate SEM and its optimization\nalgorithm -- a challenging and time-consuming task.\n  In this paper, we consider each SEM as a computation graph, a flexible method\nof specifying objective functions borrowed from the field of deep learning.\nWhen combined with state-of-the-art optimizers, our computation graph approach\ncan extend SEM without the need for bespoke software development. We show that\nboth existing and novel SEM improvements follow naturally from our approach. To\ndemonstrate, we discuss least absolute deviation estimation and penalized\nregression models. We also introduce spike-and-slab SEM, which may perform\nbetter when shrinkage of large factor loadings is not desired. By applying\ncomputation graphs to SEM, we hope to greatly accelerate the process of\ndeveloping SEM techniques, paving the way for new applications. We provide an\naccompanying R package tensorsem.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 10:23:06 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 12:38:59 GMT"}], "update_date": "2019-10-11", "authors_parsed": [["van Kesteren", "Erik-Jan", ""], ["Oberski", "Daniel L.", ""]]}, {"id": "1905.04582", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Philippe Lemey, Guy Baele, Simon Dellicour, Dirk\n  Brockmann, Andrew Rambaut and Marc Suchard", "title": "Massive parallelization boosts big Bayesian multidimensional scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Bayes is the computationally intensive co-application of big data and\nlarge, expressive Bayesian models for the analysis of complex phenomena in\nscientific inference and statistical learning. Standing as an example, Bayesian\nmultidimensional scaling (MDS) can help scientists learn viral trajectories\nthrough space-time, but its computational burden prevents its wider use.\nCrucial MDS model calculations scale quadratically in the number of\nobservations. We partially mitigate this limitation through massive\nparallelization using multi-core central processing units, instruction-level\nvectorization and graphics processing units (GPUs). Fitting the MDS model using\nHamiltonian Monte Carlo, GPUs can deliver more than 100-fold speedups over\nserial calculations and thus extend Bayesian MDS to a big data setting. To\nillustrate, we employ Bayesian MDS to infer the rate at which different\nseasonal influenza virus subtypes use worldwide air traffic to spread around\nthe globe. We examine 5392 viral sequences and their associated 14 million\npairwise distances arising from the number of commercial airline seats per year\nbetween viral sampling locations. To adjust for shared evolutionary history of\nthe viruses, we implement a phylogenetic extension to the MDS model and learn\nthat subtype H3N2 spreads most effectively, consistent with its epidemic\nsuccess relative to other seasonal influenza subtypes. Finally, we provide\nMassiveMDS, an open-source, stand-alone C++ library and rudimentary R package,\nand discuss program design and high-level implementation with an emphasis on\nimportant aspects of computing architecture that become relevant at scale.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 20:08:08 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 00:35:51 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Holbrook", "Andrew", ""], ["Lemey", "Philippe", ""], ["Baele", "Guy", ""], ["Dellicour", "Simon", ""], ["Brockmann", "Dirk", ""], ["Rambaut", "Andrew", ""], ["Suchard", "Marc", ""]]}, {"id": "1905.04758", "submitter": "Arrigo Coen", "authors": "Arrigo Coen", "title": "The compound product distribution; a solution to the distributional\n  equation X=AX+1", "comments": "10 pages, 5 figures, one appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The solution of $ X=AX+1 $ is analyzed for a discrete variable $ A $ with $\n\\mathbb{P}\\left[A=0\\right]>0 $. Accordingly, a fast algorithm is presented to\ncalculate the obtained heavy tail density. To exemplify, the compound product\ndistribution is studied in detail for some particular families of\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 18:08:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Coen", "Arrigo", ""]]}, {"id": "1905.05255", "submitter": "Alexander Y. Shestopaloff", "authors": "Alexander Y. Shestopaloff and Arnaud Doucet", "title": "Replica Conditional Sequential Monte Carlo", "comments": "To appear in Proceedings of ICML '19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Markov chain Monte Carlo (MCMC) scheme to perform state\ninference in non-linear non-Gaussian state-space models. Current\nstate-of-the-art methods to address this problem rely on particle MCMC\ntechniques and its variants, such as the iterated conditional Sequential Monte\nCarlo (cSMC) scheme, which uses a Sequential Monte Carlo (SMC) type proposal\nwithin MCMC. A deficiency of standard SMC proposals is that they only use\nobservations up to time $t$ to propose states at time $t$ when an entire\nobservation sequence is available. More sophisticated SMC based on lookahead\ntechniques could be used but they can be difficult to put in practice. We\npropose here replica cSMC where we build SMC proposals for one replica using\ninformation from the entire observation sequence by conditioning on the states\nof the other replicas. This approach is easily parallelizable and we\ndemonstrate its excellent empirical performance when compared to the standard\niterated cSMC scheme at fixed computational complexity.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 19:24:18 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Shestopaloff", "Alexander Y.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1905.05284", "submitter": "Ryan Martin", "authors": "Yue Yang and Ryan Martin and Howard Bondell", "title": "Variational approximations using Fisher divergence", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications of Bayesian inference involve models that are\nsufficiently complex that the corresponding posterior distributions are\nintractable and must be approximated. The most common approximation is based on\nMarkov chain Monte Carlo, but these can be expensive when the data set is large\nand/or the model is complex, so more efficient variational approximations have\nrecently received considerable attention. The traditional variational methods,\nthat seek to minimize the Kullback--Leibler divergence between the posterior\nand a relatively simple parametric family, provide accurate and efficient\nestimation of the posterior mean, but often does not capture other moments, and\nhave limitations in terms of the models to which they can be applied. Here we\npropose the construction of variational approximations based on minimizing the\nFisher divergence, and develop an efficient computational algorithm that can be\napplied to a wide range of models without conjugacy or potentially unrealistic\nmean-field assumptions. We demonstrate the superior performance of the proposed\nmethod for the benchmark case of logistic regression.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 20:58:34 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Yang", "Yue", ""], ["Martin", "Ryan", ""], ["Bondell", "Howard", ""]]}, {"id": "1905.05337", "submitter": "Brendan McVeigh", "authors": "Brendan S. McVeigh, Bradley T. Spahn, and Jared S. Murray", "title": "Scaling Bayesian Probabilistic Record Linkage with Post-Hoc Blocking: An\n  Application to the California Great Registers", "comments": "42 pages with appendices, 7 figures, 20 page supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic record linkage (PRL) is the process of determining which\nrecords in two databases correspond to the same underlying entity in the\nabsence of a unique identifier. Bayesian solutions to this problem provide a\npowerful mechanism for propagating uncertainty due to uncertain links between\nrecords (via the posterior distribution). However, computational considerations\nseverely limit the practical applicability of existing Bayesian approaches. We\npropose a new computational approach, providing both a fast algorithm for\nderiving point estimates of the linkage structure that properly account for\none-to-one matching and a restricted MCMC algorithm that samples from an\napproximate posterior distribution. Our advances make it possible to perform\nBayesian PRL for larger problems, and to assess the sensitivity of results to\nvarying prior specifications. We demonstrate the methods on a subset of an\nOCR'd dataset, the California Great Registers, a collection of 57 million voter\nregistrations from 1900 to 1968 that comprise the only panel data set of party\nregistration collected before the advent of scientific surveys.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 01:20:36 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 00:39:15 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["McVeigh", "Brendan S.", ""], ["Spahn", "Bradley T.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1905.05394", "submitter": "Mingyuan Zhou", "authors": "Chaojie Wang, Bo Chen, Sucheng Xiao, Mingyuan Zhou", "title": "Convolutional Poisson Gamma Belief Network", "comments": "ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For text analysis, one often resorts to a lossy representation that either\ncompletely ignores word order or embeds each word as a low-dimensional dense\nfeature vector. In this paper, we propose convolutional Poisson factor analysis\n(CPFA) that directly operates on a lossless representation that processes the\nwords in each document as a sequence of high-dimensional one-hot vectors. To\nboost its performance, we further propose the convolutional Poisson gamma\nbelief network (CPGBN) that couples CPFA with the gamma belief network via a\nnovel probabilistic pooling layer. CPFA forms words into phrases and captures\nvery specific phrase-level topics, and CPGBN further builds a hierarchy of\nincreasingly more general phrase-level topics. For efficient inference, we\ndevelop both a Gibbs sampler and a Weibull distribution based convolutional\nvariational auto-encoder. Experimental results demonstrate that CPGBN can\nextract high-quality text latent representations that capture the word order\ninformation, and hence can be leveraged as a building block to enrich a wide\nvariety of existing latent variable models that ignore word order.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 05:08:47 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Wang", "Chaojie", ""], ["Chen", "Bo", ""], ["Xiao", "Sucheng", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1905.05569", "submitter": "Thomas Faulkenberry", "authors": "Thomas J. Faulkenberry", "title": "Estimating Bayes factors from minimal ANOVA summaries for\n  repeated-measures designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, I develop a formula for estimating Bayes factors directly from\nminimal summary statistics produced in repeated measures analysis of variance\ndesigns. The formula, which requires knowing only the $F$-statistic, the number\nof subjects, and the number of repeated measurements per subject, is based on\nthe BIC approximation of the Bayes factor, a common default method for Bayesian\ncomputation with linear models. In addition to providing computational\nexamples, I report a simulation study in which I demonstrate that the formula\ncompares favorably to a recently developed, more complex method that accounts\nfor correlation between repeated measurements. The minimal BIC method provides\na simple way for researchers to estimate Bayes factors from a minimal set of\nsummary statistics, giving users a powerful index for estimating the evidential\nvalue of not only their own data, but also the data reported in published\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 12:55:08 GMT"}, {"version": "v2", "created": "Sun, 6 Oct 2019 11:44:43 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 10:49:34 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Faulkenberry", "Thomas J.", ""]]}, {"id": "1905.05884", "submitter": "Julyan Arbel", "authors": "Hien D. Nguyen, Julyan Arbel, Hongliang L\\\"u, Florence Forbes", "title": "Approximate Bayesian computation via the energy statistic", "comments": "25 pages, 6 figures, 5 tables", "journal-ref": "IEEE Access (2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has become an essential part of the\nBayesian toolbox for addressing problems in which the likelihood is\nprohibitively expensive or entirely unknown, making it intractable. ABC defines\na pseudo-posterior by comparing observed data with simulated data,\ntraditionally based on some summary statistics, the elicitation of which is\nregarded as a key difficulty. Recently, using data discrepancy measures has\nbeen proposed in order to bypass the construction of summary statistics. Here\nwe propose to use the importance-sampling ABC (IS-ABC) algorithm relying on the\nso-called two-sample energy statistic. We establish a new asymptotic result for\nthe case where both the observed sample size and the simulated data sample size\nincrease to infinity, which highlights to what extent the data discrepancy\nmeasure impacts the asymptotic pseudo-posterior. The result holds in the broad\nsetting of IS-ABC methodologies, thus generalizing previous results that have\nbeen established only for rejection ABC algorithms. Furthermore, we propose a\nconsistent V-statistic estimator of the energy statistic, under which we show\nthat the large sample result holds, and prove that the rejection ABC algorithm,\nbased on the energy statistic, generates pseudo-posterior distributions that\nachieves convergence to the correct limits, when implemented with rejection\nthresholds that converge to zero, in the finite sample setting. Our proposed\nenergy statistic based ABC algorithm is demonstrated on a variety of models,\nincluding a Gaussian mixture, a moving-average model of order two, a bivariate\nbeta and a multivariate $g$-and-$k$ distribution. We find that our proposed\nmethod compares well with alternative discrepancy measures.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 23:41:45 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 10:17:29 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Arbel", "Julyan", ""], ["L\u00fc", "Hongliang", ""], ["Forbes", "Florence", ""]]}, {"id": "1905.05963", "submitter": "Souvik Roy", "authors": "Suvra Pal and Souvik Roy", "title": "A New Estimation Algorithm for Box-Cox Transformation Cure Rate Model\n  and Comparison With EM Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a new estimation procedure based on the non-linear\nconjugate gradient (NCG) algorithm for the Box-Cox transformation cure rate\nmodel. We compare the performance of the NCG algorithm with the well-known\nexpectation maximization (EM) algorithm through a simulation study and show the\nadvantages of the NCG algorithm over the EM algorithm. In particular, we show\nthat the NCG algorithm allows simultaneous maximization of all model parameters\nwhen the likelihood surface is flat with respect to a Box-Cox model parameter.\nThis is a big advantage over the EM algorithm, where a profile likelihood\napproach has been proposed in the literature that may not provide satisfactory\nresults. We finally use the NCG algorithm to analyze a well-known melanoma data\nand show that it results in a better fit.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 06:16:59 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Pal", "Suvra", ""], ["Roy", "Souvik", ""]]}, {"id": "1905.06097", "submitter": "Zhou Fan", "authors": "Sheng Xu and Zhou Fan", "title": "Iterative Alpha Expansion for estimating gradient-sparse signals from\n  linear measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating a piecewise-constant image, or a gradient-sparse\nsignal on a general graph, from noisy linear measurements. We propose and study\nan iterative algorithm to minimize a penalized least-squares objective, with a\npenalty given by the \"l_0-norm\" of the signal's discrete graph gradient. The\nmethod proceeds by approximate proximal descent, applying the alpha-expansion\nprocedure to minimize a proximal gradient in each iteration, and using a\ngeometric decay of the penalty parameter across iterations. Under a\ncut-restricted isometry property for the measurement design, we prove global\nrecovery guarantees for the estimated signal. For standard Gaussian designs,\nthe required number of measurements is independent of the graph structure, and\nimproves upon worst-case guarantees for total-variation (TV) compressed sensing\non the 1-D and 2-D lattice graphs by polynomial and logarithmic factors,\nrespectively. The method empirically yields lower mean-squared recovery error\ncompared with TV regularization in regimes of moderate undersampling and\nmoderate to high signal-to-noise, for several examples of changepoint signals\nand gradient-sparse phantom images.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:40:35 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Xu", "Sheng", ""], ["Fan", "Zhou", ""]]}, {"id": "1905.06501", "submitter": "Raj Agrawal", "authors": "Raj Agrawal, Jonathan H. Huggins, Brian Trippe, Tamara Broderick", "title": "The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise\n  Interactions in High Dimensions", "comments": "Accepted at ICML 2019. 20 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering interaction effects on a response of interest is a fundamental\nproblem faced in biology, medicine, economics, and many other scientific\ndisciplines. In theory, Bayesian methods for discovering pairwise interactions\nenjoy many benefits such as coherent uncertainty quantification, the ability to\nincorporate background knowledge, and desirable shrinkage properties. In\npractice, however, Bayesian methods are often computationally intractable for\neven moderate-dimensional problems. Our key insight is that many hierarchical\nmodels of practical interest admit a particular Gaussian process (GP)\nrepresentation; the GP allows us to capture the posterior with a vector of O(p)\nkernel hyper-parameters rather than O(p^2) interactions and main effects. With\nthe implicit representation, we can run Markov chain Monte Carlo (MCMC) over\nmodel hyper-parameters in time and memory linear in p per iteration. We focus\non sparsity-inducing models and show on datasets with a variety of covariate\nbehaviors that our method: (1) reduces runtime by orders of magnitude over\nnaive applications of MCMC, (2) provides lower Type I and Type II error\nrelative to state-of-the-art LASSO-based approaches, and (3) offers improved\ncomputational scaling in high dimensions relative to existing Bayesian and\nLASSO-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 02:19:10 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 03:46:06 GMT"}], "update_date": "2019-12-04", "authors_parsed": [["Agrawal", "Raj", ""], ["Huggins", "Jonathan H.", ""], ["Trippe", "Brian", ""], ["Broderick", "Tamara", ""]]}, {"id": "1905.06680", "submitter": "Radu V. Craiu", "authors": "Evgeny Levi and Radu V. Craiu", "title": "Finding our Way in the Dark: Approximate MCMC for Approximate Bayesian\n  Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With larger data at their disposal, scientists are emboldened to tackle\ncomplex questions that require sophisticated statistical models. It is not\nunusual for the latter to have likelihood functions that elude analytical\nformulations. Even under such adversity, when one can simulate from the\nsampling distribution, Bayesian analysis can be conducted using approximate\nmethods such as Approximate Bayesian Computation (ABC) or Bayesian Synthetic\nLikelihood (BSL). A significant drawback of these methods is that the number of\nrequired simulations can be prohibitively large, thus severely limiting their\nscope. In this paper we design perturbed MCMC samplers that can be used within\nthe ABC and BSL paradigms to significantly accelerate computation while\nmaintaining control on computational efficiency. The proposed strategy relies\non recycling samples from the chain's past. The algorithmic design is supported\nby a theoretical analysis while practical performance is examined via a series\nof simulation examples and data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:08:50 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Levi", "Evgeny", ""], ["Craiu", "Radu V.", ""]]}, {"id": "1905.06845", "submitter": "Friso H. Kingma", "authors": "Friso H. Kingma, Pieter Abbeel, Jonathan Ho", "title": "Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with\n  Hierarchical Latent Variables", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bits-back argument suggests that latent variable models can be turned\ninto lossless compression schemes. Translating the bits-back argument into\nefficient and practical lossless compression schemes for general latent\nvariable models, however, is still an open problem. Bits-Back with Asymmetric\nNumeral Systems (BB-ANS), recently proposed by Townsend et al. (2019), makes\nbits-back coding practically feasible for latent variable models with one\nlatent layer, but it is inefficient for hierarchical latent variable models. In\nthis paper we propose Bit-Swap, a new compression scheme that generalizes\nBB-ANS and achieves strictly better compression rates for hierarchical latent\nvariable models with Markov chain structure. Through experiments we verify that\nBit-Swap results in lossless compression rates that are empirically superior to\nexisting techniques. Our implementation is available at\nhttps://github.com/fhkingma/bitswap.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 15:32:35 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 17:10:45 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 05:51:17 GMT"}, {"version": "v4", "created": "Wed, 2 Oct 2019 09:57:24 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kingma", "Friso H.", ""], ["Abbeel", "Pieter", ""], ["Ho", "Jonathan", ""]]}, {"id": "1905.07034", "submitter": "Karthik Devarajan", "authors": "Karthik Devarajan", "title": "Non-negative matrix factorization based on generalized dual divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A theoretical framework for non-negative matrix factorization based on\ngeneralized dual Kullback-Leibler divergence, which includes members of the\nexponential family of models, is proposed. A family of algorithms is developed\nusing this framework and its convergence proven using the\nExpectation-Maximization algorithm. The proposed approach generalizes some\nexisting methods for different noise structures and contrasts with the recently\nproposed quasi-likelihood approach, thus providing a useful alternative for\nnon-negative matrix factorizations. A measure to evaluate the goodness-of-fit\nof the resulting factorization is described. This framework can be adapted to\ninclude penalty, kernel and discriminant functions as well as tensors.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 21:10:23 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Devarajan", "Karthik", ""]]}, {"id": "1905.07438", "submitter": "Eric Kawaguchi", "authors": "Eric S Kawaguchi, Jenny I Shen, Gang Li, Marc A Suchard", "title": "A Fast and Scalable Implementation Method for Competing Risks Data with\n  the R Package fastcmprsk", "comments": "21 pages; 5 figures; 3 tables", "journal-ref": null, "doi": "10.32614/RJ-2021-010", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancements in medical informatics tools and high-throughput biological\nexperimentation make large-scale biomedical data routinely accessible to\nresearchers. Competing risks data are typical in biomedical studies where\nindividuals are at risk to more than one cause (type of event) which can\npreclude the others from happening. The Fine-Gray model is a popular and\nwell-appreciated model for competing risks data and is currently implemented in\na number of statistical software packages. However, current implementations are\nnot computationally scalable for large-scale competing risks data. We have\ndeveloped an R package, fastcmprsk, that uses a novel forward-backward scan\nalgorithm to significantly reduce the computational complexity for parameter\nestimation by exploiting the structure of the subject-specific risk sets.\nNumerical studies compare the speed and scalability of our implementation to\ncurrent methods for unpenalized and penalized Fine-Gray regression and show\nimpressive gains in computational efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 18:47:57 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kawaguchi", "Eric S", ""], ["Shen", "Jenny I", ""], ["Li", "Gang", ""], ["Suchard", "Marc A", ""]]}, {"id": "1905.07455", "submitter": "Satish Ramakrishna", "authors": "Satish Ramakrishna, Kamesh Aiyer", "title": "Speeding up the Karatsuba algorithm", "comments": "6 pages", "journal-ref": "J Multidis Res Rev Vol: 2, Issue: 1 (01-03) 2020", "doi": null, "report-no": null, "categories": "cs.DS math.NT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an $\\sim {\\cal O}(n)$ pre-compute technique to speed up\nthe Karatsuba algorithm for multiplying two numbers.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 12:39:10 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 00:07:07 GMT"}, {"version": "v3", "created": "Wed, 23 Oct 2019 14:56:50 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ramakrishna", "Satish", ""], ["Aiyer", "Kamesh", ""]]}, {"id": "1905.07456", "submitter": "Xiao Wu", "authors": "Xiao Wu, Yi Xu, Bradley P. Carlin", "title": "Optimizing Interim Analysis Timing for Bayesian Adaptive Commensurate\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In developing products for rare diseases, statistical challenges arise due to\nthe limited number of patients available for participation in drug trials and\nother clinical research. Bayesian adaptive clinical trial designs offer the\npossibility of increased statistical efficiency, reduced development cost and\nethical hazard prevention via their incorporation of evidence from external\nsources (historical data, expert opinions, and real-world evidence), and\nflexibility in the specification of interim looks. In this paper, we propose a\nnovel Bayesian adaptive commensurate design that borrows adaptively from\nhistorical information and also uses a particular payoff function to optimize\nthe timing of the study's interim analysis. The trial payoff is a function of\nhow many samples can be saved via early stopping and the probability of making\ncorrect early decisions for either futility or efficacy. We calibrate our\nBayesian algorithm to have acceptable long-run frequentist properties (Type I\nerror and power) via simulation at the design stage. We illustrate our approach\nusing a pediatric trial design setting testing the effect of a new drug for a\nrare genetic disease. The optimIA R package available at\nhttps://github.com/wxwx1993/Bayesian_IA_Timing provides an easy-to-use\nimplementation of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 20:05:29 GMT"}, {"version": "v2", "created": "Wed, 18 Sep 2019 04:23:06 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Wu", "Xiao", ""], ["Xu", "Yi", ""], ["Carlin", "Bradley P.", ""]]}, {"id": "1905.07469", "submitter": "Clement Etienam", "authors": "Clement Etienam", "title": "4D Seismic History Matching Incorporating Unsupervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.LG math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The work discussed and presented in this paper focuses on the history\nmatching of reservoirs by integrating 4D seismic data into the inversion\nprocess using machine learning techniques. A new integrated scheme for the\nreconstruction of petrophysical properties with a modified Ensemble Smoother\nwith Multiple Data Assimilation (ES-MDA) in a synthetic reservoir is proposed.\nThe permeability field inside the reservoir is parametrised with an\nunsupervised learning approach, namely K-means with Singular Value\nDecomposition (K-SVD). This is combined with the Orthogonal Matching Pursuit\n(OMP) technique which is very typical for sparsity promoting regularisation\nschemes. Moreover, seismic attributes, in particular, acoustic impedance, are\nparametrised with the Discrete Cosine Transform (DCT). This novel combination\nof techniques from machine learning, sparsity regularisation, seismic imaging\nand history matching aims to address the ill-posedness of the inversion of\nhistorical production data efficiently using ES-MDA. In the numerical\nexperiments provided, I demonstrate that these sparse representations of the\npetrophysical properties and the seismic attributes enables to obtain better\nproduction data matches to the true production data and to quantify the\npropagating waterfront better compared to more traditional methods that do not\nuse comparable parametrisation techniques.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 06:27:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Etienam", "Clement", ""]]}, {"id": "1905.07499", "submitter": "Brian Trippe", "authors": "Brian L. Trippe, Jonathan H. Huggins, Raj Agrawal and Tamara Broderick", "title": "LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data\n  Approximations", "comments": "Accepted at ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the ease of modern data collection, applied statisticians often have\naccess to a large set of covariates that they wish to relate to some observed\noutcome. Generalized linear models (GLMs) offer a particularly interpretable\nframework for such an analysis. In these high-dimensional problems, the number\nof covariates is often large relative to the number of observations, so we face\nnon-trivial inferential uncertainty; a Bayesian approach allows coherent\nquantification of this uncertainty. Unfortunately, existing methods for\nBayesian inference in GLMs require running times roughly cubic in parameter\ndimension, and so are limited to settings with at most tens of thousand\nparameters. We propose to reduce time and memory costs with a low-rank\napproximation of the data in an approach we call LR-GLM. When used with the\nLaplace approximation or Markov chain Monte Carlo, LR-GLM provides a full\nBayesian posterior approximation and admits running times reduced by a full\nfactor of the parameter dimension. We rigorously establish the quality of our\napproximation and show how the choice of rank allows a tunable\ncomputational-statistical trade-off. Experiments support our theory and\ndemonstrate the efficacy of LR-GLM on real large-scale datasets.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 22:59:56 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Trippe", "Brian L.", ""], ["Huggins", "Jonathan H.", ""], ["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1905.07647", "submitter": "Radoslav Harman", "authors": "Radoslav Harman, Samuel Rosa", "title": "On greedy heuristics for computing D-efficient saturated subsets", "comments": "Pre-publication peer review version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathcal{F}$ be a set consisting of $n$ real vectors of dimension $m\n\\leq n$. For any saturated, i.e., $m$-element, subset $\\mathcal{S}$ of\n$\\mathcal{F}$, let $\\mathrm{vol}(\\mathcal{S})$ be the volume of the\nparallelotope formed by the vectors of $\\mathcal{S}$. A set $\\mathcal{S}^*$ is\ncalled a $D$-optimal saturated subset of $\\mathcal{F}$, if it maximizes\n$\\mathrm{vol}(\\mathcal{S})$ among all saturated subsets of $\\mathcal{F}$. In\nthis paper, we propose two greedy heuristics for the construction of saturated\nsubsets performing well with respect to the criterion of $D$-optimality: an\nimprovement of the method suggested by Galil and Kiefer for the initiation of\n$D$-optimal experimental design algorithms, and a modification of the\nKumar-Yildirim method, the original version of which was proposed for the\ninitiation of the minimum-volume enclosing ellipsoid algorithms. We provide\ngeometric and analytic insights into the two methods, and compare them to the\ncommonly used random and regularized greedy heuristics. We also suggest\nvariants of the greedy methods for a large set $\\mathcal{F}$, for the\nconstruction of $D$-efficient non-saturated subsets, and for alternative\noptimality criteria.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 21:46:33 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Harman", "Radoslav", ""], ["Rosa", "Samuel", ""]]}, {"id": "1905.07771", "submitter": "Martina Han\\v{c}ov\\'a", "authors": "Martina Han\\v{c}ov\\'a, Gabriela Voz\\'arikov\\'a, Andrej Gajdo\\v{s},\n  Jozef Han\\v{c}", "title": "Estimating variances in time series linear regression models using\n  empirical BLUPs and convex optimization", "comments": "29 pages, 1 figure, 5 tables", "journal-ref": "Statistical Papers 2020", "doi": "10.1007/s00362-020-01165-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage estimation method of variance components in time\nseries models known as FDSLRMs, whose observations can be described by a linear\nmixed model (LMM). We based estimating variances, fundamental quantities in a\ntime series forecasting approach called kriging, on the empirical (plug-in)\nbest linear unbiased predictions of unobservable random components in FDSLRM.\n  The method, providing invariant non-negative quadratic estimators, can be\nused for any absolutely continuous probability distribution of time series\ndata. As a result of applying the convex optimization and the LMM methodology,\nwe resolved two problems $-$ theoretical existence and equivalence between\nleast squares estimators, non-negative (M)DOOLSE, and maximum likelihood\nestimators, (RE)MLE, as possible starting points of our method and a practical\nlack of computational implementation for FDSLRM. As for computing (RE)MLE in\nthe case of $ n $ observed time series values, we also discovered a new\nalgorithm of order $\\mathcal{O}(n)$, which at the default precision is $10^7$\ntimes more accurate and $n^2$ times faster than the best current Python(or\nR)-based computational packages, namely CVXPY, CVXR, nlme, sommer and mixed.\n  We illustrate our results on three real data sets $-$ electricity\nconsumption, tourism and cyber security $-$ which are easily available,\nreproducible, sharable and modifiable in the form of interactive Jupyter\nnotebooks.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:46:55 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Han\u010dov\u00e1", "Martina", ""], ["Voz\u00e1rikov\u00e1", "Gabriela", ""], ["Gajdo\u0161", "Andrej", ""], ["Han\u010d", "Jozef", ""]]}, {"id": "1905.07976", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Richard G. Everitt", "title": "Stratified sampling and bootstrapping for approximate Bayesian\n  computation", "comments": "35 pages, 10 figures. Major revision: uses stratification with\n  rejection and importance sampling ABC; compares several bootstrap procedures;\n  new supernova case study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approximate Bayesian computation (ABC) is computationally intensive for\ncomplex model simulators. To exploit expensive simulations, data-resampling via\nbootstrapping can be employed to obtain many artificial datasets at little\ncost. However, when using this approach within ABC, the posterior variance is\ninflated, thus resulting in biased posterior inference. Here we use stratified\nMonte Carlo to considerably reduce the bias induced by data resampling. We also\nshow empirically that it is possible to obtain reliable inference using a\nlarger than usual ABC threshold. Finally, we show that with stratified Monte\nCarlo we obtain a less variable ABC likelihood. Ultimately we show how our\napproach improves the computational efficiency of the ABC samplers. We\nconstruct several ABC samplers employing our methodology, such as rejection and\nimportance ABC samplers, and ABC-MCMC samplers. We consider simulation studies\nfor static (Gaussian, g-and-k distribution, Ising model, astronomical model)\nand dynamic models (Lotka-Volterra). We compare against state-of-art sequential\nMonte Carlo ABC samplers, synthetic likelihoods, and likelihood-free Bayesian\noptimization. For a computationally expensive Lotka-Volterra case study, we\nfound that our strategy leads to a more than 10-fold computational saving,\ncompared to a sampler that does not use our novel approach.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 10:28:24 GMT"}, {"version": "v2", "created": "Wed, 1 Jan 2020 13:44:55 GMT"}, {"version": "v3", "created": "Thu, 11 Jun 2020 08:28:11 GMT"}, {"version": "v4", "created": "Fri, 2 Jul 2021 16:15:19 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Picchini", "Umberto", ""], ["Everitt", "Richard G.", ""]]}, {"id": "1905.08227", "submitter": "Balu Nadiga", "authors": "Balasubramanya T. Nadiga, Chiyu Jiang, Daniel Livescu", "title": "Leveraging Bayesian Analysis To Improve Accuracy of Approximate Models", "comments": "22 pages with 7 figures", "journal-ref": null, "doi": "10.1016/j.jcp.2019.05.015", "report-no": "LA-UR-18-29498", "categories": "physics.comp-ph physics.flu-dyn stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on improving the accuracy of an approximate model of a multiscale\ndynamical system that uses a set of parameter-dependent terms to account for\nthe effects of unresolved or neglected dynamics on resolved scales. We start by\nconsidering various methods of calibrating and analyzing such a model given a\nfew well-resolved simulations. After presenting results for various point\nestimates and discussing some of their shortcomings, we demonstrate (a) the\npotential of hierarchical Bayesian analysis to uncover previously unanticipated\nphysical dependencies in the approximate model, and (b) how such insights can\nthen be used to improve the model. In effect parametric dependencies found from\nthe Bayesian analysis are used to improve structural aspects of the model.\nWhile we choose to illustrate the procedure in the context of a closure model\nfor buoyancy-driven, variable-density turbulence, the statistical nature of the\napproach makes it more generally applicable. Towards addressing issues of\nincreased computational cost associated with the procedure, we demonstrate the\nuse of a neural network based surrogate in accelerating the posterior sampling\nprocess and point to recent developments in variational inference as an\nalternative methodology for greatly mitigating such costs. We conclude by\nsuggesting that modern validation and uncertainty quantification techniques\nsuch as the ones we consider have a valuable role to play in the development\nand improvement of approximate models.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 17:27:48 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Nadiga", "Balasubramanya T.", ""], ["Jiang", "Chiyu", ""], ["Livescu", "Daniel", ""]]}, {"id": "1905.08327", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan, Sean Kross, Jeffrey T. Leek", "title": "Tools for analyzing R code the tidy way", "comments": null, "journal-ref": "The R Journal, 12(1), 226 (2020)", "doi": "10.32614/RJ-2020-011", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the current emphasis on reproducibility and replicability, there is an\nincreasing need to examine how data analyses are conducted. In order to analyze\nthe between researcher variability in data analysis choices as well as the\naspects within the data analysis pipeline that contribute to the variability in\nresults, we have created two R packages: matahari and tidycode. These packages\nbuild on methods created for natural language processing; rather than allowing\nfor the processing of natural language, we focus on R code as the substrate of\ninterest. The matahari package facilitates the logging of everything that is\ntyped in the R console or in an R script in a tidy data frame. The tidycode\npackage contains tools to allow for analyzing R calls in a tidy manner. We\ndemonstrate the utility of these packages as well as walk through two examples.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 20:04:44 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Kross", "Sean", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1905.08374", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Gaussian Process Learning via Fisher Scoring of Vecchia's Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a single pass algorithm for computing the gradient and Fisher\ninformation of Vecchia's Gaussian process loglikelihood approximation, which\nprovides a computationally efficient means for applying the Fisher scoring\nalgorithm for maximizing the loglikelihood. The advantages of the optimization\ntechniques are demonstrated in numerical examples and in an application to Argo\nocean temperature data. The new methods are more accurate and much faster than\nan optimization method that uses only function evaluations, especially when the\ncovariance function has many parameters. This allows practitioners to fit\nnonstationary models to large spatial and spatial-temporal datasets.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 23:03:03 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1905.08448", "submitter": "Kirankumar Shiragur", "authors": "Moses Charikar, Kirankumar Shiragur, Aaron Sidford", "title": "Efficient Profile Maximum Likelihood for Universal Symmetric Property\n  Estimation", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating symmetric properties of a distribution, e.g. support size,\ncoverage, entropy, distance to uniformity, are among the most fundamental\nproblems in algorithmic statistics. While each of these properties have been\nstudied extensively and separate optimal estimators are known for each, in\nstriking recent work, Acharya et al. 2016 showed that there is a single\nestimator that is competitive for all symmetric properties. This work proved\nthat computing the distribution that approximately maximizes \\emph{profile\nlikelihood (PML)}, i.e. the probability of observed frequency of frequencies,\nand returning the value of the property on this distribution is sample\ncompetitive with respect to a broad class of estimators of symmetric\nproperties. Further, they showed that even computing an approximation of the\nPML suffices to achieve such a universal plug-in estimator. Unfortunately,\nprior to this work there was no known polynomial time algorithm to compute an\napproximate PML and it was open to obtain a polynomial time universal plug-in\nestimator through the use of approximate PML. In this paper we provide a\nalgorithm (in number of samples) that, given $n$ samples from a distribution,\ncomputes an approximate PML distribution up to a multiplicative error of\n$\\exp(n^{2/3} \\mathrm{poly} \\log(n))$ in time nearly linear in $n$.\nGeneralizing work of Acharya et al. 2016 on the utility of approximate PML we\nshow that our algorithm provides a nearly linear time universal plug-in\nestimator for all symmetric functions up to accuracy $\\epsilon =\n\\Omega(n^{-0.166})$. Further, we show how to extend our work to provide\nefficient polynomial-time algorithms for computing a $d$-dimensional\ngeneralization of PML (for constant $d$) that allows for universal plug-in\nestimation of symmetric relationships between distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 05:39:05 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Charikar", "Moses", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "1905.08552", "submitter": "Jian He", "authors": "Jian He, Asma Khedher, Peter Spreij", "title": "A Kalman particle filter for online parameter estimation with\n  applications to affine models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of estimating the posterior distribution\nof the static parameters of a continuous time state space model with discrete\ntime observations by an algorithm that combines the Kalman filter and a\nparticle filter. The proposed algorithm is semi-recursive and has a two layer\nstructure, in which the outer layer provides the estimation of the posterior\ndistribution of the unknown parameters and the inner layer provides the\nestimation of the posterior distribution of the state variables. This algorithm\nhas a similar structure as the so-called recursive nested particle filter, but\nunlike the latter filter, in which both layers use a particle filter, this\nproposed algorithm introduces a dynamic kernel to sample the parameter\nparticles in the outer layer to obtain a higher convergence speed. Moreover,\nthis algorithm also implements the Kalman filter in the inner layer to reduce\nthe computational time. This algorithm can also be used to estimate the\nparameters that suddenly change value. We prove that, for a state space model\nwith a certain structure, the estimated posterior distribution of the unknown\nparameters and the state variables converge to the actual distribution in $L_p$\nwith rate of order $\\mathcal{O}(N^{-\\frac{1}{2}}+\\delta^{\\frac{1}{2}})$, where\n$N$ is the number of particles for the parameters in the outer layer and\n$\\delta$ is the maximum time step between two consecutive observations. We\npresent numerical results of the implementation of this algorithm, in\nparticularly we implement this algorithm for affine interest models, possibly\nwith stochastic volatility, although the algorithm can be applied to a much\nbroader class of models.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 11:18:02 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["He", "Jian", ""], ["Khedher", "Asma", ""], ["Spreij", "Peter", ""]]}, {"id": "1905.09044", "submitter": "Thomas Galtier", "authors": "H. Chraibi, A. Dutfoy, T. Galtier, J. Garnier", "title": "Application of the interacting particle system method to piecewise\n  deterministic Markov processes used in reliability", "comments": null, "journal-ref": null, "doi": "10.1063/1.5081446", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction methods are often needed for the reliability assessment of\ncomplex industrial systems, we focus on one variance reduction method in a\ngiven context, that is the interacting particle system method (IPS) used on\npiecewise deterministic Markov processes (PDMP) for reliability assessment .\nThe PDMPs are a very large class of processes which benefit from high modeling\ncapacities, they can model almost any Markovian phenomenon that does not\ninclude diffusion. In reliability assessment, the PDMPs modeling industrial\nsystems generally involve low jump rates and jump kernels favoring one safe\narrival, we call such model a \"concentrated PDMP\".\n  Used on such concentrated PDMPs, the IPS is inefficient and does not always\nprovide a variance reduction. Indeed, the efficiency of the IPS method relies\non simulating many different trajectories during its propagation steps, but\nunfortunately concentrated PDMPs are likely to generate the same deterministic\ntrajectories over and over. We propose an adaptation of the IPS method called\nIPS+M that reduces this phenomenon. The IPS+M consists in modifying the\npropagation steps of the IPS, by conditioning the propagation to avoid\ngenerating the same trajectories multiple times. We prove that, compared to the\nIPS, the IPS+M method always provides an estimator with a lower variance. We\nalso carry out a quick simulation study on a two-components system that\nconfirms this result.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:53:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chraibi", "H.", ""], ["Dutfoy", "A.", ""], ["Galtier", "T.", ""], ["Garnier", "J.", ""]]}, {"id": "1905.09110", "submitter": "Kamran Javid Mr", "authors": "Kamran Javid", "title": "Nested sampling on non-trivial geometries", "comments": "13 pages, 11 figures, 28 equations", "journal-ref": null, "doi": "10.5281/zenodo.3653182", "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metropolis nested sampling evolves a Markov chain from a current livepoint\nand accepts new points along the chain according to a version of the Metropolis\nacceptance ratio modified to satisfy the likelihood constraint, characteristic\nof nested sampling algorithms. The geometric nested sampling algorithm we\npresent here is a based on the Metropolis method, but treats parameters as\nthough they represent points on certain geometric objects, namely circles, tori\nand spheres. For parameters which represent points on a circle or torus, the\ntrial distribution is `wrapped' around the domain of the posterior distribution\nsuch that samples cannot be rejected automatically when evaluating the\nMetropolis ratio due to being outside the sampling domain. Furthermore, this\nenhances the mobility of the sampler. For parameters which represent\ncoordinates on the surface of a sphere, the algorithm transforms the parameters\ninto a Cartesian coordinate system before sampling which again makes sure no\nsamples are automatically rejected, and provides a physically intutive way of\nthe sampling the parameter space.\n  We apply the geometric nested sampler to two types of toy model which include\ncircular, toroidal and spherical parameters. We find that the geometric nested\nsampler generally outperforms \\textsc{MultiNest} in both cases. \\\\ %We also\napply the algorithm to a gravitational wave detection model which includes\ncircular and spherical parameters, and find that the geometric nested sampler\nand \\textsc{MultiNest} appear to perform equally well as one another. Our\nimplementation of the algorithm can be found at\n\\url{https://github.com/SuperKam91/nested_sampling}.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 13:00:40 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 10:54:19 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 13:49:34 GMT"}, {"version": "v4", "created": "Sun, 9 Feb 2020 22:16:29 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Javid", "Kamran", ""]]}, {"id": "1905.09149", "submitter": "Julio Castrillon PhD", "authors": "Julio Enrique Castrillon-Candas and Mark Kon", "title": "Analytic regularity and stochastic collocation of high dimensional\n  Newton iterates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce concepts from uncertainty quantification (UQ) and\nnumerical analysis for the efficient evaluation of stochastic high dimensional\nNewton iterates. In particular, we develop complex analytic regularity theory\nof the solution with respect to the random variables. This justifies the\napplication of sparse grids for the computation of stochastic moments.\nConvergence rates are derived and are shown to be subexponential or algebraic\nwith respect to the number of realizations of random perturbations. Due the\naccuracy of the method, sparse grids are well suited for computing low\nprobability events with high confidence. We apply our method to the power flow\nproblem. Numerical experiments on the 39 bus New England power system model\nwith large stochastic loads are consistent with the theoretical convergence\nrates.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 14:02:05 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Castrillon-Candas", "Julio Enrique", ""], ["Kon", "Mark", ""]]}, {"id": "1905.09317", "submitter": "David Woodruff", "authors": "Cristobal Pais, Jaime Carrasco, David L. Martell, Andres Weintraub,\n  David L. Woodruff", "title": "Cell2Fire: A Cell Based Forest Fire Growth Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cell2Fire is a new cell-based forest and wildland landscape fire growth\nsimulator that is open-source and exploits parallelism to support the modelling\nof fire growth cross large spatial and temporal scales in a timely manner. The\nfire environment is characterized by partitioning the landscape into a large\nnumber of cells each of which has specified fuel, weather, fuel moisture and\ntopography attributes. Fire spread within each cell is assumed to be elliptical\nand governed by spread rates predicted by a fire spread model such as the\nCanadian Forest Fire Behavior Prediction (FBP) System. The simulator includes\npowerful statistical and graphical output and spatial analysis features to\nfacilitate the display and analysis of projected fire growth.\n  We validated Cell2Fire by using it to predict the growth of real and\nrealistic hypothetical fires, comparing our fire growth predictions with those\nproduced by the state-of-the-art Prometheus fire growth simulator. Cell2Fire is\nstructured to facilitate its use for predicting the growth of individual fires\nor embedding it in landscape management simulation models. It can be used to\nproduce probabilistic fire scar predictions by allowing for uncertainty\nconcerning the basic spread rate predictions and uncertain weather scenarios\nthat might drive their growth.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 18:20:37 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Pais", "Cristobal", ""], ["Carrasco", "Jaime", ""], ["Martell", "David L.", ""], ["Weintraub", "Andres", ""], ["Woodruff", "David L.", ""]]}, {"id": "1905.09501", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner", "title": "Bayesian Item Response Modeling in R with brms and Stan", "comments": "54 pages, 16 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Item Response Theory (IRT) is widely applied in the human sciences to model\npersons' responses on a set of items measuring one or more latent constructs.\nWhile several R packages have been developed that implement IRT models, they\ntend to be restricted to respective prespecified classes of models. Further,\nmost implementations are frequentist while the availability of Bayesian methods\nremains comparably limited. We demonstrate how to use the R package brms\ntogether with the probabilistic programming language Stan to specify and fit a\nwide range of Bayesian IRT models using flexible and intuitive multilevel\nformula syntax. Further, item and person parameters can be related in both a\nlinear or non-linear manner. Various distributions for categorical, ordinal,\nand continuous responses are supported. Users may even define their own custom\nresponse distribution for use in the presented framework. Common IRT model\nclasses that can be specified natively in the presented framework include 1PL\nand 2PL logistic models optionally also containing guessing parameters, graded\nresponse and partial credit ordinal models, as well as drift diffusion models\nof response times coupled with binary decisions. Posterior distributions of\nitem and person parameters can be conveniently extracted and post-processed.\nModel fit can be evaluated and compared using Bayes factors and efficient\ncross-validation procedures.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 07:11:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 13:35:38 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 11:19:43 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""]]}, {"id": "1905.09800", "submitter": "Ben Moews", "authors": "Ben Moews and Joe Zuntz", "title": "Gaussbock: Fast parallel-iterative cosmological parameter estimation\n  with Bayesian nonparametrics", "comments": "19 pages, 10 figures, accepted for publication in ApJ", "journal-ref": null, "doi": "10.3847/1538-4357/ab93cb", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present and apply Gaussbock, a new embarrassingly parallel iterative\nalgorithm for cosmological parameter estimation designed for an era of cheap\nparallel computing resources. Gaussbock uses Bayesian nonparametrics and\ntruncated importance sampling to accurately draw samples from posterior\ndistributions with an orders-of-magnitude speed-up in wall time over\nalternative methods. Contemporary problems in this area often suffer from both\nincreased computational costs due to high-dimensional parameter spaces and\nconsequent excessive time requirements, as well as the need for fine tuning of\nproposal distributions or sampling parameters. Gaussbock is designed\nspecifically with these issues in mind. We explore and validate the performance\nand convergence of the algorithm on a fast approximation to the Dark Energy\nSurvey Year 1 (DES Y1) posterior, finding reasonable scaling behavior with the\nnumber of parameters. We then test on the full DES Y1 posterior using\nlarge-scale supercomputing facilities, and recover reasonable agreement with\nprevious chains, although the algorithm can underestimate the tails of\npoorly-constrained parameters. Additionally, we discuss and demonstrate how\nGaussbock recovers complex posterior shapes very well at lower dimensions, but\nfaces challenges to perform well on such distributions in higher dimensions. In\naddition, we provide the community with a user-friendly software tool for\naccelerated cosmological parameter estimation based on the methodology\ndescribed in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:50:04 GMT"}, {"version": "v2", "created": "Sat, 16 May 2020 03:29:46 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Moews", "Ben", ""], ["Zuntz", "Joe", ""]]}, {"id": "1905.09813", "submitter": "Ian Langmore", "authors": "Ian Langmore, Michael Dikovsky, Scott Geraedts, Peter Norgaard, Rob\n  Von Behren", "title": "A Condition Number for Hamiltonian Monte Carlo", "comments": "Significant changes: (i) Added connection to inverse Wishart\n  ensemble, (ii) added estimation of kappa, (iii) checked and corrected proofs,\n  (iv) re-wrote everything for clarity, (v) added authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hamiltonian Monte Carlo is a popular sampling technique for smooth target\ndensities. The scale lengths of the target have long been known to influence\nintegration error and sampling efficiency. However, quantitative measures\nintrinsic to the target have been lacking. In this paper, we restrict attention\nto the multivariate Gaussian and the leapfrog integrator, and obtain a\ncondition number corresponding to sampling efficiency. This number, based on\nthe spectral and Schatten norms, quantifies the number of leapfrog steps needed\nto efficiently sample. We demonstrate its utility by using this condition\nnumber to analyze HMC preconditioning techniques. We also find the condition\nnumber of large inverse Wishart matrices, from which we derive burn-in\nheuristics.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:59:31 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 16:00:24 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 00:16:31 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Langmore", "Ian", ""], ["Dikovsky", "Michael", ""], ["Geraedts", "Scott", ""], ["Norgaard", "Peter", ""], ["Von Behren", "Rob", ""]]}, {"id": "1905.09948", "submitter": "HaiYing Wang", "authors": "HaiYing Wang", "title": "Divide-and-Conquer Information-Based Optimal Subdata Selection Algorithm", "comments": "21 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1007/s42519-019-0048-5", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information-based optimal subdata selection (IBOSS) is a computationally\nefficient method to select informative data points from large data sets through\nprocessing full data by columns. However, when the volume of a data set is too\nlarge to be processed in the available memory of a machine, it is infeasible to\nimplement the IBOSS procedure. This paper develops a divide-and-conquer IBOSS\napproach to solving this problem, in which the full data set is divided into\nsmaller partitions to be loaded into the memory and then subsets of data are\nselected from each partitions using the IBOSS algorithm. We derive both finite\nsample properties and asymptotic properties of the resulting estimator.\nAsymptotic results show that if the full data set is partitioned randomly and\nthe number of partitions is not very large, then the resultant estimator has\nthe same estimation efficiency as the original IBOSS estimator. We also carry\nout numerical experiments to evaluate the empirical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:56:38 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""]]}, {"id": "1905.09964", "submitter": "Alessandro Zocca", "authors": "John Moriarty, Jure Vogrinc, Alessandro Zocca", "title": "A Metropolis-class sampler for targets with non-convex support", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to improve upon the exploration of the general-purpose random walk\nMetropolis algorithm when the target has non-convex support $A \\subset\n\\mathbb{R}^d$, by reusing proposals in $A^c$ which would otherwise be rejected.\nThe algorithm is Metropolis-class and under standard conditions the chain\nsatisfies a strong law of large numbers and central limit theorem. Theoretical\nand numerical evidence of improved performance relative to random walk\nMetropolis are provided. Issues of implementation are discussed and numerical\nexamples, including applications to global optimisation and rare event\nsampling, are presented.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 23:12:47 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 10:52:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Moriarty", "John", ""], ["Vogrinc", "Jure", ""], ["Zocca", "Alessandro", ""]]}, {"id": "1905.09971", "submitter": "Niloy Biswas", "authors": "Niloy Biswas, Pierre E. Jacob, Paul Vanetti", "title": "Estimating Convergence of Markov chains with L-Lag Couplings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods generate samples that are\nasymptotically distributed from a target distribution of interest as the number\nof iterations goes to infinity. Various theoretical results provide upper\nbounds on the distance between the target and marginal distribution after a\nfixed number of iterations. These upper bounds are on a case by case basis and\ntypically involve intractable quantities, which limits their use for\npractitioners. We introduce L-lag couplings to generate computable,\nnon-asymptotic upper bound estimates for the total variation or the Wasserstein\ndistance of general Markov chains. We apply L-lag couplings to the tasks of (i)\ndetermining MCMC burn-in, (ii) comparing different MCMC algorithms with the\nsame target, and (iii) comparing exact and approximate MCMC. Lastly, we (iv)\nassess the bias of sequential Monte Carlo and self-normalized importance\nsamplers.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 23:45:38 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 19:40:48 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 19:18:27 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Biswas", "Niloy", ""], ["Jacob", "Pierre E.", ""], ["Vanetti", "Paul", ""]]}, {"id": "1905.10031", "submitter": "Jingbo Liu", "authors": "Vishesh Jain and Frederic Koehler and Jingbo Liu and Elchanan Mossel", "title": "Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation", "comments": "To be presented on COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of Belief Propagation and other algorithms for the {\\em\nreconstruction problem} plays a key role in the analysis of community detection\nin inference on graphs, phylogenetic reconstruction in bioinformatics, and the\ncavity method in statistical physics.\n  We prove a conjecture of Evans, Kenyon, Peres, and Schulman (2000) which\nstates that any bounded memory message passing algorithm is statistically much\nweaker than Belief Propagation for the reconstruction problem. More formally,\nany recursive algorithm with bounded memory for the reconstruction problem on\nthe trees with the binary symmetric channel has a phase transition strictly\nbelow the Belief Propagation threshold, also known as the Kesten-Stigum bound.\nThe proof combines in novel fashion tools from recursive reconstruction,\ninformation theory, and optimal transport, and also establishes an asymptotic\nnormality result for BP and other message-passing algorithms near the critical\nthreshold.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 04:56:12 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Liu", "Jingbo", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1905.10035", "submitter": "Vahid Partovi Nia", "authors": "Shaima Tilouche, Vahid Partovi Nia, Samuel Bassetto", "title": "Parallel Coordinate Order for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of high-dimensional data is counter-intuitive using\nconventional graphs. Parallel coordinates are proposed as an alternative to\nexplore multivariate data more effectively. However, it is difficult to extract\nrelevant information through the parallel coordinates when the data are\nhigh-dimensional with thousands of lines overlapping. The order of the axes\ndetermines the perception of information on parallel coordinates. Thus, the\ninformation between attributes remain hidden if coordinates are improperly\nordered. Here we propose a general framework to reorder the coordinates. This\nframework is general to cover a large range of data visualization objective. It\nis also flexible to contain many conventional ordering measures. Consequently,\nwe present the coordinate ordering binary optimization problem and enhance\ntowards a computationally efficient greedy approach that suites\nhigh-dimensional data. Our approach is applied on wine data and on genetic\ndata. The purpose of dimension reordering of wine data is highlighting\nattributes dependence. Genetic data are reordered to enhance cluster detection.\nThe presented framework shows that it is able to adapt the measures and\ncriteria tested.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 05:08:01 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Tilouche", "Shaima", ""], ["Nia", "Vahid Partovi", ""], ["Bassetto", "Samuel", ""]]}, {"id": "1905.10252", "submitter": "Alessandro Varsi", "authors": "Alessandro Varsi, Lykourgos Kekempanos, Jeyarajan Thiyagalingam, Simon\n  Maskell", "title": "A Single SMC Sampler on MPI that Outperforms a Single MCMC Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a well-established family of algorithms\nwhich are primarily used in Bayesian statistics to sample from a target\ndistribution when direct sampling is challenging. Single instances of MCMC\nmethods are widely considered hard to parallelise in a problem-agnostic fashion\nand hence, unsuitable to meet both constraints of high accuracy and high\nthroughput. Sequential Monte Carlo (SMC) Samplers can address the same problem,\nbut are parallelisable: they share with Particle Filters the same key tasks and\nbottleneck. Although a rich literature already exists on MCMC methods, SMC\nSamplers are relatively underexplored, such that no parallel implementation is\ncurrently available. In this paper, we first propose a parallel MPI version of\nthe SMC Sampler, including an optimised implementation of the bottleneck, and\nthen compare it with single-core Metropolis-Hastings. The goal is to show that\nSMC Samplers may be a promising alternative to MCMC methods with high potential\nfor future improvements. We demonstrate that a basic SMC Sampler with 512 cores\nis up to 85 times faster or up to 8 times more accurate than\nMetropolis-Hastings.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 14:25:37 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Varsi", "Alessandro", ""], ["Kekempanos", "Lykourgos", ""], ["Thiyagalingam", "Jeyarajan", ""], ["Maskell", "Simon", ""]]}, {"id": "1905.10271", "submitter": "Motonobu Kanagawa", "authors": "Motonobu Kanagawa, Philipp Hennig", "title": "Convergence Guarantees for Adaptive Bayesian Quadrature Methods", "comments": "To appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Bayesian quadrature (ABQ) is a powerful approach to numerical\nintegration that empirically compares favorably with Monte Carlo integration on\nproblems of medium dimensionality (where non-adaptive quadrature is not\ncompetitive). Its key ingredient is an acquisition function that changes as a\nfunction of previously collected values of the integrand. While this adaptivity\nappears to be empirically powerful, it complicates analysis. Consequently,\nthere are no theoretical guarantees so far for this class of methods. In this\nwork, for a broad class of adaptive Bayesian quadrature methods, we prove\nconsistency, deriving non-tight but informative convergence rates. To do so we\nintroduce a new concept we call weak adaptivity. Our results identify a large\nand flexible class of adaptive Bayesian quadrature rules as consistent, within\nwhich practitioners can develop empirically efficient methods.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:02:59 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 12:38:04 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kanagawa", "Motonobu", ""], ["Hennig", "Philipp", ""]]}, {"id": "1905.10302", "submitter": "James Wilson", "authors": "Lisha Yu, Inez M. Zwetsloot, Nathaniel T. Stevens, James D. Wilson,\n  Kwok Leung Tsui", "title": "Monitoring dynamic networks: a simulation-based strategy for comparing\n  monitoring methods and a comparative study", "comments": "37 pages, 13 figures, 4 tables. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there has been a lot of interest in monitoring and identifying\nchanges in dynamic networks, which has led to the development of a variety of\nmonitoring methods. Unfortunately, these methods have not been systematically\ncompared; moreover, new methods are often designed for a specialized use case.\nIn light of this, we propose the use of simulation to compare the performance\nof network monitoring methods over a variety of dynamic network changes. Using\nour family of simulated dynamic networks, we compare the performance of several\nstate-of-the-art social network monitoring methods in the literature. We\ncompare their performance over a variety of types of change; we consider both\nincreases in communication levels, node propensity change as well as changes in\ncommunity structure. We show that there does not exist one method that is\nuniformly superior to the others; the best method depends on the context and\nthe type of change one wishes to detect. As such, we conclude that a variety of\nmethods is needed for network monitoring and that it is important to understand\nin which scenarios a given method is appropriate.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 15:57:49 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Yu", "Lisha", ""], ["Zwetsloot", "Inez M.", ""], ["Stevens", "Nathaniel T.", ""], ["Wilson", "James D.", ""], ["Tsui", "Kwok Leung", ""]]}, {"id": "1905.11232", "submitter": "Deborshee Sen", "authors": "Deborshee Sen, Matthias Sachs, Jianfeng Lu, David Dunson", "title": "Efficient posterior sampling for high-dimensional imbalanced logistic\n  regression", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data are routinely collected in many areas. We are\nparticularly interested in Bayesian classification models in which one or more\nvariables are imbalanced. Current Markov chain Monte Carlo algorithms for\nposterior computation are inefficient as $n$ and/or $p$ increase due to\nworsening time per step and mixing rates. One strategy is to use a\ngradient-based sampler to improve mixing while using data sub-samples to reduce\nper-step computational complexity. However, usual sub-sampling breaks down when\napplied to imbalanced data. Instead, we generalize piece-wise deterministic\nMarkov chain Monte Carlo algorithms to include importance-weighted and\nmini-batch sub-sampling. These approaches maintain the correct stationary\ndistribution with arbitrarily small sub-samples, and substantially outperform\ncurrent competitors. We provide theoretical support and illustrate gains in\nsimulated and real data applications.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 13:58:23 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 16:54:26 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Sen", "Deborshee", ""], ["Sachs", "Matthias", ""], ["Lu", "Jianfeng", ""], ["Dunson", "David", ""]]}, {"id": "1905.11351", "submitter": "Mario Collura", "authors": "Mario Collura, Luca Dell'Anna, Timo Felser, and Simone Montangero", "title": "On the descriptive power of Neural-Networks as constrained Tensor\n  Networks with exponentially large bond dimension", "comments": "18 pages, 8 figures", "journal-ref": "SciPost Phys. Core 4, 001 (2021)", "doi": "10.21468/SciPostPhysCore.4.1.001", "report-no": null, "categories": "quant-ph cond-mat.stat-mech physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases, Neural networks can be mapped into tensor networks with an\nexponentially large bond dimension. Here, we compare different sub-classes of\nneural network states, with their mapped tensor network counterpart for\nstudying the ground state of short-range Hamiltonians. We show that when\nmapping a neural network, the resulting tensor network is highly constrained\nand thus the neural network states do in general not deliver the naive expected\ndrastic improvement against the state-of-the-art tensor network methods. We\nexplicitly show this result in two paradigmatic examples, the 1D ferromagnetic\nIsing model and the 2D antiferromagnetic Heisenberg model, addressing the lack\nof a detailed comparison of the expressiveness of these increasingly popular,\nvariational ans\\\"atze.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 17:29:47 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 08:39:48 GMT"}, {"version": "v3", "created": "Fri, 15 May 2020 06:39:57 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 08:44:04 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Collura", "Mario", ""], ["Dell'Anna", "Luca", ""], ["Felser", "Timo", ""], ["Montangero", "Simone", ""]]}, {"id": "1905.11502", "submitter": "Lourens  Waldorp", "authors": "Lourens Waldorp and Maarten Marsman", "title": "Intervention in undirected Ising graphs and the partition function", "comments": "Preprint for original paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Undirected graphical models have many applications in such areas as machine\nlearning, image processing, and, recently, psychology. Psychopathology in\nparticular has received a lot of attention, where symptoms of disorders are\nassumed to influence each other. One of the most relevant questions practically\nis on which symptom (node) to intervene to have the most impact. Interventions\nin undirected graphical models is equal to conditioning, and so we have\navailable the machinery with the Ising model to determine the best strategy to\nintervene. In order to perform such calculations the partition function is\nrequired, which is computationally difficult. Here we use a Curie-Weiss\napproach to approximate the partition function in applications of\ninterventions. We show that when the connection weights in the graph are equal\nwithin each clique then we obtain exactly the correct partition function. And\nif the weights vary according to a sub-Gaussian distribution, then the\napproximation is exponentially close to the correct one. We confirm these\nresults with simulations.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 20:54:36 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Waldorp", "Lourens", ""], ["Marsman", "Maarten", ""]]}, {"id": "1905.11793", "submitter": "Stefano Peluso", "authors": "Stefano Peluso, Antonietta Mira and Pietro Muliere", "title": "Conditionally Gaussian Random Sequences for an Integrated Variance\n  Estimator with Correlation between Noise and Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation between microstructure noise and latent financial logarithmic\nreturns is an empirically relevant phenomenon with sound theoretical\njustification. With few notable exceptions, all integrated variance estimators\nproposed in the financial literature are not designed to explicitly handle such\na dependence, or handle it only in special settings. We provide an integrated\nvariance estimator that is robust to correlated noise and returns. For this\npurpose, a generalization of the Forward Filtering Backward Sampling algorithm\nis proposed, to provide a sampling technique for a latent conditionally\nGaussian random sequence. We apply our methodology to intra-day Microsoft\nprices, and compare it in a simulation study with established alternatives,\nshowing an advantage in terms of root mean square error and dispersion.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 13:17:01 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Peluso", "Stefano", ""], ["Mira", "Antonietta", ""], ["Muliere", "Pietro", ""]]}, {"id": "1905.11846", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Computation of projection regression depth and its induced median", "comments": "33 pages and 6 figures and 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notions of depth in regression have been introduced and studied in the\nliterature. The most famous example is Regression Depth (RD), which is a direct\nextension of location depth to regression. The projection regression depth\n(PRD) is the extension of another prevailing location depth, the projection\ndepth, to regression. The computation issues of the RD have been discussed in\nthe literature. The computation issues of the PRD have never been dealt with\nbefore. The computation issues of the PRD and its induced median (maximum depth\nestimator) in a regression setting are addressed now. For a given\n$\\bs{\\beta}\\in\\R^p$ exact algorithms for the PRD with cost $O(n^2\\log n)$\n($p=2$) and $O(N(n, p)(p^{3}+n\\log n+np^{1.5}+npN_{Iter}))$ ($p>2$) and\napproximate algorithms for the PRD and its induced median with cost\nrespectively $O(N_{\\mb{v}}np)$ and $O(Rp\nN_{\\bs{\\beta}}(p^2+nN_{\\mb{v}}N_{Iter}))$ are proposed. Here $N(n, p)$ is a\nnumber defined based on the total number of $(p-1)$ dimensional hyperplanes\nformed by points induced from sample points and the $\\bs{\\beta}$; $N_{\\mb{v}}$\nis the total number of unit directions $\\mb{v}$ utilized; $N_{\\bs{\\beta}}$ is\nthe total number of candidate regression parameters $\\bs{\\beta}$ employed;\n$N_{Iter}$ is the total number of iterations carried out in an optimization\nalgorithm; $R$ is the total number of replications. Furthermore, as the second\nmajor contribution, three PRD induced estimators, which can be computed up to\n30 times faster than that of the PRD induced median while maintaining a similar\nlevel of accuracy are introduced. Examples and simulation studies reveal that\nthe depth median induced from the PRD is favorable in terms of robustness and\nefficiency, compared to the maximum depth estimator induced from the RD, which\nis the current leading regression median.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 14:27:59 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 22:28:56 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 18:03:07 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 03:20:45 GMT"}, {"version": "v5", "created": "Mon, 18 Jan 2021 15:03:26 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "1905.11916", "submitter": "Ben Bales", "authors": "Ben Bales, Arya Pourzanjani, Aki Vehtari, Linda Petzold", "title": "Selecting the Metric in Hamiltonian Monte Carlo", "comments": "Data/code available at https://github.com/bbbales2/cmdstan-warmup", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a selection criterion for the Euclidean metric adapted during\nwarmup in a Hamiltonian Monte Carlo sampler that makes it possible for a\nsampler to automatically pick the metric based on the model and the\navailability of warmup draws. Additionally, we present a new adaptation\ninspired by the selection criterion that requires significantly fewer warmup\ndraws to be effective. The effectiveness of the selection criterion and\nadaptation are demonstrated on a number of applied problems. An implementation\nfor the Stan probabilistic programming language is provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 16:25:32 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 16:12:17 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 14:16:44 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Bales", "Ben", ""], ["Pourzanjani", "Arya", ""], ["Vehtari", "Aki", ""], ["Petzold", "Linda", ""]]}, {"id": "1905.11937", "submitter": "Maxime Vono", "authors": "Maxime Vono and Daniel Paulin and Arnaud Doucet", "title": "Efficient MCMC Sampling with Dimension-Free Convergence Rate using\n  ADMM-type Splitting", "comments": "68 pages. Revision of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing exact Bayesian inference for complex models is computationally\nintractable. Markov chain Monte Carlo (MCMC) algorithms can provide reliable\napproximations of the posterior distribution but are expensive for large\ndatasets and high-dimensional models. A standard approach to mitigate this\ncomplexity consists in using subsampling techniques or distributing the data\nacross a cluster. However, these approaches are typically unreliable in\nhigh-dimensional scenarios. We focus here on a recent alternative class of MCMC\nschemes exploiting a splitting strategy akin to the one used by the celebrated\nADMM optimization algorithm. These methods appear to provide empirically\nstate-of-the-art performance but their theoretical behavior in high dimension\nis currently unknown. In this paper, we propose a detailed theoretical study of\none of these algorithms known as the split Gibbs sampler. Under regularity\nconditions, we establish explicit convergence rates for this scheme using Ricci\ncurvature and coupling ideas. We support our theory with numerical\nillustrations.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:42:33 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 16:25:23 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 12:50:25 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 15:21:28 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 16:02:51 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Vono", "Maxime", ""], ["Paulin", "Daniel", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1905.12062", "submitter": "Florian Puchhammer", "authors": "Amal Ben Abdellah, Pierre L'Ecuyer, Florian Puchhammer", "title": "Array-RQMC for option pricing under stochastic volatility models", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Array-RQMC has been proposed as a way to effectively apply randomized\nquasi-Monte Carlo (RQMC) when simulating a Markov chain over a large number of\nsteps to estimate an expected cost or reward. The method can be very effective\nwhen the state of the chain has low dimension. For pricing an Asian option\nunder an ordinary geometric Brownian motion model, for example, Array-RQMC\nreduces the variance by huge factors. In this paper, we show how to apply this\nmethod and we study its effectiveness in case the underlying process has\nstochastic volatility. We show that Array-RQMC can also work very well for\nthese models, even if it requires RQMC points in larger dimension. We examine\nin particular the variance-gamma, Heston, and Ornstein-Uhlenbeck stochastic\nvolatility models, and we provide numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 20:05:59 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Abdellah", "Amal Ben", ""], ["L'Ecuyer", "Pierre", ""], ["Puchhammer", "Florian", ""]]}, {"id": "1905.12115", "submitter": "Amelia Henriksen", "authors": "Amelia Henriksen and Rachel Ward", "title": "AdaOja: Adaptive Learning Rates for Streaming PCA", "comments": "15 pages, 8 figures, typos fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Oja's algorithm has been the cornerstone of streaming methods in Principal\nComponent Analysis (PCA) since it was first proposed in 1982. However, Oja's\nalgorithm does not have a standardized choice of learning rate (step size) that\nboth performs well in practice and truly conforms to the online streaming\nsetting. In this paper, we propose a new learning rate scheme for Oja's method\ncalled AdaOja. This new algorithm requires only a single pass over the data and\ndoes not depend on knowing properties of the data set a priori. AdaOja is a\nnovel variation of the Adagrad algorithm to Oja's algorithm in the single\neigenvector case and extended to the multiple eigenvector case. We demonstrate\nfor dense synthetic data, sparse real-world data and dense real-world data that\nAdaOja outperforms common learning rate choices for Oja's method. We also show\nthat AdaOja performs comparably to state-of-the-art algorithms (History PCA and\nStreaming Power Method) in the same streaming PCA setting.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 22:02:24 GMT"}, {"version": "v2", "created": "Thu, 31 Oct 2019 20:46:40 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Henriksen", "Amelia", ""], ["Ward", "Rachel", ""]]}, {"id": "1905.12141", "submitter": "Jingyu He", "authors": "Jingyu He, Nicholas G. Polson, Jianeng Xu", "title": "Bayesian Inference for Gamma Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a data\naugmentation scheme for models that include gamma functions. Our methodology\napplies to many situations in statistics and machine learning, including\nMultinomial-Dirichlet distributions, Negative binomial regression,\nPoisson-Gamma hierarchical models, Extreme value models, to name but a few. All\nof those models include a gamma function which does not admit a natural\nconjugate prior distribution providing a significant challenge to inference and\nprediction. To provide a data augmentation strategy, we construct and develop\nthe theory of the class of Exponential Reciprocal Gamma distributions. This\nallows scalable EM and MCMC algorithms to be developed. We illustrate our\nmethodology on a number of examples, including gamma shape inference, negative\nbinomial regression and Dirichlet allocation. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:15:56 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:44:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["He", "Jingyu", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1905.12146", "submitter": "Xiang Ji", "authors": "Xiang Ji, Zhenyu Zhang, Andrew Holbrook, Akihiko Nishimura, Guy Baele,\n  Andrew Rambaut, Philippe Lemey and Marc A. Suchard", "title": "Gradients do grow on trees: a linear-time ${\\cal O}\\hspace{-0.2em}\\left(\n  N \\right)$-dimensional gradient for statistical phylogenetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculation of the log-likelihood stands as the computational bottleneck for\nmany statistical phylogenetic algorithms. Even worse is its gradient\nevaluation, often used to target regions of high probability. Order ${\\cal\nO}\\hspace{-0.2em}\\left( N \\right)$-dimensional gradient calculations based on\nthe standard pruning algorithm require ${\\cal O}\\hspace{-0.2em}\\left( N^2\n\\right)$ operations where N is the number of sampled molecular sequences. With\nthe advent of high-throughput sequencing, recent phylogenetic studies have\nanalyzed hundreds to thousands of sequences, with an apparent trend towards\neven larger data sets as a result of advancing technology. Such large-scale\nanalyses challenge phylogenetic reconstruction by requiring inference on larger\nsets of process parameters to model the increasing data heterogeneity. To make\nthis tractable, we present a linear-time algorithm for ${\\cal\nO}\\hspace{-0.2em}\\left( N \\right)$-dimensional gradient evaluation and apply it\nto general continuous-time Markov processes of sequence substitution on a\nphylogenetic tree without a need to assume either stationarity or\nreversibility. We apply this approach to learn the branch-specific evolutionary\nrates of three pathogenic viruses: West Nile virus, Dengue virus and Lassa\nvirus. Our proposed algorithm significantly improves inference efficiency with\na 126- to 234-fold increase in maximum-likelihood optimization and a 16- to\n33-fold computational performance increase in a Bayesian framework.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 00:33:42 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Ji", "Xiang", ""], ["Zhang", "Zhenyu", ""], ["Holbrook", "Andrew", ""], ["Nishimura", "Akihiko", ""], ["Baele", "Guy", ""], ["Rambaut", "Andrew", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1905.12247", "submitter": "Yuansi Chen", "authors": "Yuansi Chen, Raaz Dwivedi, Martin J. Wainwright, Bin Yu", "title": "Fast mixing of Metropolized Hamiltonian Monte Carlo: Benefits of\n  multi-step gradients", "comments": "73 pages, 2 figures, fixed a mistake in the proof of Lemma 11,\n  accepted in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a state-of-the-art Markov chain Monte Carlo\nsampling algorithm for drawing samples from smooth probability densities over\ncontinuous spaces. We study the variant most widely used in practice,\nMetropolized HMC with the St\\\"{o}rmer-Verlet or leapfrog integrator, and make\ntwo primary contributions. First, we provide a non-asymptotic upper bound on\nthe mixing time of the Metropolized HMC with explicit choices of step-size and\nnumber of leapfrog steps. This bound gives a precise quantification of the\nfaster convergence of Metropolized HMC relative to simpler MCMC algorithms such\nas the Metropolized random walk, or Metropolized Langevin algorithm. Second, we\nprovide a general framework for sharpening mixing time bounds of Markov chains\ninitialized at a substantial distance from the target distribution over\ncontinuous spaces. We apply this sharpening device to the Metropolized random\nwalk and Langevin algorithms, thereby obtaining improved mixing time bounds\nfrom a non-warm initial distribution.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 07:05:04 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 00:11:41 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 17:16:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Chen", "Yuansi", ""], ["Dwivedi", "Raaz", ""], ["Wainwright", "Martin J.", ""], ["Yu", "Bin", ""]]}, {"id": "1905.12269", "submitter": "Shaoxiong Hu", "authors": "Shaoxiong Hu, Hugo Maruri-Aguliar, Zixiang Ma", "title": "Topological Techniques in Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LASSO is an attractive regularisation method for linear regression that\ncombines variable selection with an efficient computation procedure. This paper\nis concerned with enhancing the performance of LASSO for square-free\nhierarchical polynomial models when combining validation error with a measure\nof model complexity. The measure of the complexity is the sum of Betti numbers\nof the model which is seen as a simplicial complex, and we describe the model\nin terms of components and cycles, borrowing from recent developments in\ncomputational topology. We study and propose an algorithm which combines\nstatistical and topological criteria. This compound criterion would allow us to\ndeal with model selection problems in polynomial regression models containing\nhigher-order interactions. Simulation results demonstrate that the compound\ncriteria produce sparser models with lower prediction errors than the\nestimators of several other statistical methods for higher order interaction\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 08:26:25 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hu", "Shaoxiong", ""], ["Maruri-Aguliar", "Hugo", ""], ["Ma", "Zixiang", ""]]}, {"id": "1905.12934", "submitter": "Muhammad Firmansyah Kasim", "authors": "M. F. Kasim, A. F. A. Bott, P. Tzeferacos, D. Q. Lamb, G. Gregori, and\n  S. M. Vinko", "title": "Retrieving fields from proton radiography without source profiles", "comments": null, "journal-ref": "Phys. Rev. E 100, 033208 (2019)", "doi": "10.1103/PhysRevE.100.033208", "report-no": null, "categories": "physics.plasm-ph stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Proton radiography is a technique in high energy density science to diagnose\nmagnetic and/or electric fields in a plasma by firing a proton beam and\ndetecting its modulated intensity profile on a screen. Current approaches to\nretrieve the integrated field from the modulated intensity profile require the\nunmodulated beam intensity profile before the interaction, which is rarely\navailable experimentally due to shot-to-shot variability. In this paper, we\npresent a statistical method to retrieve the integrated field without needing\nto know the exact source profile. We apply our method to experimental data,\nshowing the robustness of our approach. Our proposed technique allows not only\nfor the retrieval of the path-integrated fields, but also of the statistical\nproperties of the fields.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 09:51:31 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 12:12:25 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Kasim", "M. F.", ""], ["Bott", "A. F. A.", ""], ["Tzeferacos", "P.", ""], ["Lamb", "D. Q.", ""], ["Gregori", "G.", ""], ["Vinko", "S. M.", ""]]}, {"id": "1905.13002", "submitter": "Simo S\\\"arkk\\\"a", "authors": "Simo S\\\"arkk\\\"a and \\'Angel F. Garc\\'ia-Fern\\'andez", "title": "Temporal Parallelization of Bayesian Smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents algorithms for temporal parallelization of Bayesian\nsmoothers. We define the elements and the operators to pose these problems as\nthe solutions to all-prefix-sums operations for which efficient parallel\nscan-algorithms are available. We present the temporal parallelization of the\ngeneral Bayesian filtering and smoothing equations and specialize them to\nlinear/Gaussian models. The advantage of the proposed algorithms is that they\nreduce the linear complexity of standard smoothing algorithms with respect to\ntime to logarithmic.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:42:26 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 07:40:55 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["S\u00e4rkk\u00e4", "Simo", ""], ["Garc\u00eda-Fern\u00e1ndez", "\u00c1ngel F.", ""]]}, {"id": "1905.13120", "submitter": "Tingting Zhao", "authors": "Tingting Zhao and Alexandre Bouchard-C\\^ot\\'e", "title": "Analysis of high-dimensional Continuous Time Markov Chains using the\n  Local Bouncy Particle Sampler", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling the parameters of high-dimensional Continuous Time Markov Chains\n(CTMC) is a challenging problem with important applications in many fields of\napplied statistics. In this work a recently proposed type of non-reversible\nrejection-free Markov Chain Monte Carlo (MCMC) sampler, the Bouncy Particle\nSampler (BPS), is brought to bear to this problem. BPS has demonstrated its\nfavorable computational efficiency compared with state-of-the-art MCMC\nalgorithms, however to date applications to real-data scenario were scarce. An\nimportant aspect of the practical implementation of BPS is the simulation of\nevent times. Default implementations use conservative thinning bounds. Such\nbounds can slow down the algorithm and limit the computational performance. Our\npaper develops an algorithm with an exact analytical solution to the random\nevent times in the context of CTMCs. Our local version of BPS algorithm takes\nadvantage of the sparse structure in the target factor graph and we also\nprovide a framework for assessing the computational complexity of local BPS\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 15:50:57 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 01:56:29 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 14:33:07 GMT"}, {"version": "v4", "created": "Sun, 30 May 2021 03:19:17 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhao", "Tingting", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""]]}, {"id": "1905.13285", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Jelena Diakonikolas, Michael I. Jordan, Peter L.\n  Bartlett", "title": "Langevin Monte Carlo without smoothness", "comments": "Updated to match the AISTATS 2020 camera ready version. Some example\n  applications added and typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Langevin Monte Carlo (LMC) is an iterative algorithm used to generate samples\nfrom a distribution that is known only up to a normalizing constant. The\nnonasymptotic dependence of its mixing time on the dimension and target\naccuracy is understood mainly in the setting of smooth (gradient-Lipschitz)\nlog-densities, a serious limitation for applications in machine learning. In\nthis paper, we remove this limitation, providing polynomial-time convergence\nguarantees for a variant of LMC in the setting of nonsmooth log-concave\ndistributions. At a high level, our results follow by leveraging the implicit\nsmoothing of the log-density that comes from a small Gaussian perturbation that\nwe add to the iterates of the algorithm and controlling the bias and variance\nthat are induced by this perturbation.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:12:22 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 23:00:35 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 19:49:40 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Diakonikolas", "Jelena", ""], ["Jordan", "Michael I.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1905.13362", "submitter": "Biljana Jonoska Stojkova", "authors": "Biljana Jonoska Stojkova and David A. Campbell", "title": "Parallel Tempering via Simulated Tempering Without Normalizing Constants", "comments": "14 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new general Bayesian methodology that\nsimultaneously estimates parameters of interest and the marginal likelihood of\nthe model. The proposed methodology builds on Simulated Tempering, which is a\npowerful algorithm that enables sampling from multi-modal distributions.\nHowever, Simulated Tempering comes with the practical limitation of needing to\nspecify a prior for the temperature along a chosen discretization schedule that\nwill allow calculation of normalizing constants at each temperature. Our\nproposed model defines the prior for the temperature so as to remove the need\nfor calculating normalizing constants at each temperature and thereby enables a\ncontinuous temperature schedule, while preserving the sampling efficiency of\nthe Simulated Tempering algorithm. The resulting algorithm simultaneously\nestimates parameters while estimating marginal likelihoods through\nthermodynamic integration. We illustrate the applicability of the new algorithm\nto different examples involving mixture models of Gaussian distributions and\nordinary differential equation models.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 00:17:38 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Stojkova", "Biljana Jonoska", ""], ["Campbell", "David A.", ""]]}, {"id": "1905.13599", "submitter": "Christian P. Robert", "authors": "Gr\\'egoire Clart\\'e, Christian P. Robert, Robin Ryder, and Julien\n  Stoehr (Universit\\'e Paris-Dauphine, CEREMADE, CNRS)", "title": "Component-wise approximate Bayesian computation via Gibbs-like steps", "comments": "28 pages, 13 figures, third revision (accepted for publication in\n  Biometrika on 17 September, 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation methods are useful for generative models\nwith intractable likelihoods. These methods are however sensitive to the\ndimension of the parameter space, requiring exponentially increasing resources\nas this dimension grows. To tackle this difficulty, we explore a Gibbs version\nof the ABC approach that runs component-wise approximate Bayesian computation\nsteps aimed at the corresponding conditional posterior distributions, and based\non summary statistics of reduced dimensions. While lacking the standard\njustifications for the Gibbs sampler, the resulting Markov chain is shown to\nconverge in distribution under some partial independence conditions. The\nassociated stationary distribution can further be shown to be close to the true\nposterior distribution and some hierarchical versions of the proposed mechanism\nenjoy a closed form limiting distribution. Experiments also demonstrate the\ngain in efficiency brought by the Gibbs version over the standard solution.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 13:11:43 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2020 17:15:44 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 16:36:20 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 11:24:34 GMT"}, {"version": "v5", "created": "Thu, 17 Sep 2020 15:50:04 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Clart\u00e9", "Gr\u00e9goire", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Ryder", "Robin", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"], ["Stoehr", "Julien", "", "Universit\u00e9 Paris-Dauphine, CEREMADE, CNRS"]]}, {"id": "1905.13695", "submitter": "Halaleh Kamari", "authors": "Halaleh Kamari, Sylvie Huet, Marie-Luce Taupin", "title": "RKHSMetaMod: An R package to estimate the Hoeffding decomposition of a\n  complex model by solving RKHS ridge group sparse optimization problem", "comments": "arXiv admin note: text overlap with arXiv:1701.04671", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an R package, called RKHSMetaMod, that implements a procedure for\nestimating a meta-model of a complex model $m$. The meta-model approximates the\nHoeffding decomposition of $m$ and allows to perform sensitivity analysis on\nit. It belongs to a reproducing kernel Hilbert space that is constructed as a\ndirect sum of Hilbert spaces. The estimator of the meta-model is the solution\nof a penalized empirical least-squares minimization with the sum of the Hilbert\nnorm and the empirical $L^2$-norm. This procedure, called RKHS ridge group\nsparse, allows both to select and estimate the terms in the Hoeffding\ndecomposition, and therefore, to select and estimate the Sobol indices that are\nnon-zero. The RKHSMetaMod package provides an interface from R statistical\ncomputing environment to the C++ libraries Eigen and GSL. In order to speed up\nthe execution time and optimize the storage memory, except for a function that\nis written in R, all of the functions of this package are written using the\nefficient C++ libraries through RcppEigen and RcppGSL packages. These functions\nare then interfaced in the R environment in order to propose an user friendly\npackage.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 16:13:07 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2019 12:54:31 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 10:38:08 GMT"}, {"version": "v4", "created": "Sat, 19 Sep 2020 11:14:13 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kamari", "Halaleh", ""], ["Huet", "Sylvie", ""], ["Taupin", "Marie-Luce", ""]]}]