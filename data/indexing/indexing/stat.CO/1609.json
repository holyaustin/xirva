[{"id": "1609.00048", "submitter": "Joel Tropp", "authors": "Joel A. Tropp, Alp Yurtsever, Madeleine Udell, Volkan Cevher", "title": "Practical sketching algorithms for low-rank matrix approximation", "comments": null, "journal-ref": "SIAM J. Matrix Analysis and Applications, Vol. 38, num. 4, pp.\n  1454-1485, Dec. 2017", "doi": "10.1137/17M1111590", "report-no": null, "categories": "cs.NA cs.DS math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a suite of algorithms for constructing low-rank\napproximations of an input matrix from a random linear image of the matrix,\ncalled a sketch. These methods can preserve structural properties of the input\nmatrix, such as positive-semidefiniteness, and they can produce approximations\nwith a user-specified rank. The algorithms are simple, accurate, numerically\nstable, and provably correct. Moreover, each method is accompanied by an\ninformative error bound that allows users to select parameters a priori to\nachieve a given approximation quality. These claims are supported by numerical\nexperiments with real and synthetic data.\n", "versions": [{"version": "v1", "created": "Wed, 31 Aug 2016 21:30:26 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 18:13:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Tropp", "Joel A.", ""], ["Yurtsever", "Alp", ""], ["Udell", "Madeleine", ""], ["Cevher", "Volkan", ""]]}, {"id": "1609.00339", "submitter": "Francisco Cribari-Neto", "authors": "Rodney Fonseca and Francisco Cribari-Neto", "title": "Inference in a bimodal Birnbaum-Saunders model", "comments": null, "journal-ref": null, "doi": "10.1016/j.matcom.2017.11.004", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the issue of performing inference on the parameters that index a\nbimodal extension of the Birnbaum-Saunders distribution (BS). We show that\nmaximum likelihood point estimation can be problematic since the standard\nnonlinear optimization algorithms may fail to converge. To deal with this\nproblem, we penalize the log-likelihood function. The numerical evidence we\npresent show that maximum likelihood estimation based on such penalized\nfunction is made considerably more reliable. We also consider hypothesis\ntesting inference based on the penalized log-likelihood function. In\nparticular, we consider likelihood ratio, signed likelihood ratio, score and\nWald tests. Bootstrap-based testing inference is also considered. We use a\nnonnested hypothesis test to distinguish between two bimodal BS laws. We derive\nanalytical corrections to some tests. Monte Carlo simulation results and\nempirical applications are presented and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 18:29:34 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 14:49:29 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Fonseca", "Rodney", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1609.00691", "submitter": "Tigran Nagapetyan", "authors": "Louis J. M. Aslett and Tigran Nagapetyan and Sebastian J. Vollmer", "title": "Multilevel Monte Carlo for Reliability Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the size of engineered systems grows, problems in reliability theory can\nbecome computationally challenging, often due to the combinatorial growth in\nthe cut sets. In this paper we demonstrate how Multilevel Monte Carlo (MLMC) -\na simulation approach which is typically used for stochastic differential\nequation models - can be applied in reliability problems by carefully\ncontrolling the bias-variance tradeoff in approximating large system behaviour.\nIn this first exposition of MLMC methods in reliability problems we address the\ncanonical problem of estimating the expectation of a functional of system\nlifetime and show the computational advantages compared to classical Monte\nCarlo methods. The difference in computational complexity can be orders of\nmagnitude for very large or complicated system structures.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2016 16:48:27 GMT"}, {"version": "v2", "created": "Sat, 11 Mar 2017 12:11:30 GMT"}], "update_date": "2017-03-14", "authors_parsed": [["Aslett", "Louis J. M.", ""], ["Nagapetyan", "Tigran", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "1609.00770", "submitter": "Ari Pakman", "authors": "Ari Pakman, Dar Gilboa, David Carlson and Liam Paninski", "title": "Stochastic Bouncy Particle Sampler", "comments": "ICML Camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel stochastic version of the non-reversible, rejection-free\nBouncy Particle Sampler (BPS), a Markov process whose sample trajectories are\npiecewise linear. The algorithm is based on simulating first arrival times in a\ndoubly stochastic Poisson process using the thinning method, and allows\nefficient sampling of Bayesian posteriors in big datasets. We prove that in the\nBPS no bias is introduced by noisy evaluations of the log-likelihood gradient.\nOn the other hand, we argue that efficiency considerations favor a small,\ncontrollable bias in the construction of the thinning proposals, in exchange\nfor faster mixing. We introduce a simple regression-based proposal intensity\nfor the thinning method that controls this trade-off. We illustrate the\nalgorithm in several examples in which it outperforms both unbiased, but slowly\nmixing stochastic versions of BPS, as well as biased stochastic gradient-based\nsamplers.\n", "versions": [{"version": "v1", "created": "Sat, 3 Sep 2016 00:13:05 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 17:00:17 GMT"}, {"version": "v3", "created": "Wed, 14 Jun 2017 01:16:51 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Pakman", "Ari", ""], ["Gilboa", "Dar", ""], ["Carlson", "David", ""], ["Paninski", "Liam", ""]]}, {"id": "1609.01022", "submitter": "Jin Zhou", "authors": "Jin Zhou, Kenji Fukumizu", "title": "Local Kernel Dimension Reduction in Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a popular sampling method in\napplications involving intractable likelihood functions. Without evaluating the\nlikelihood function, ABC approximates the posterior distribution by the set of\naccepted samples which are simulated with parameters drawn from the prior\ndistribution, where acceptance is determined by the distance between the\nsummary statistics of the sample and the observation. The sufficiency and\ndimensionality of the summary statistics play a central role in the application\nof ABC. This paper proposes Local Gradient Kernel Dimension Reduction (LGKDR)\nto construct low dimensional summary statistics for ABC. The proposed method\nidentifies a sufficient subspace of the original summary statistics by\nimplicitly considers all nonlinear transforms therein, and a weighting kernel\nis used for the concentration of the projections. No strong assumptions are\nmade on the marginal distributions nor the regression model, permitting usage\nin a wide range of applications. Experiments are done with both simple\nrejection ABC and sequential Monte Carlo ABC methods. Results are reported as\ncompetitive in the former and substantially better in the latter cases in which\nMonte Carlo errors are compressed as much as possible.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 03:20:10 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 06:48:47 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Zhou", "Jin", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1609.01118", "submitter": "David Amar", "authors": "David Amar, Ron Shamir, and Daniel Yekutieli", "title": "Extracting replicable associations across multiple studies: algorithms\n  for controlling the false discovery rate", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1005700", "report-no": null, "categories": "stat.ME q-bio.GN stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting associations that recur across multiple studies while controlling\nthe false discovery rate is a fundamental challenge. Here, we consider an\nextension of Efron's single-study two-groups model to allow joint analysis of\nmultiple studies. We assume that given a set of p-values obtained from each\nstudy, the researcher is interested in associations that recur in at least\n$k>1$ studies. We propose new algorithms that differ in how the study\ndependencies are modeled. We compared our new methods and others using various\nsimulated scenarios. The top performing algorithm, SCREEN (Scalable\nCluster-based REplicability ENhancement), is our new algorithm that is based on\nthree stages: (1) clustering an estimated correlation network of the studies,\n(2) learning replicability (e.g., of genes) within clusters, and (3) merging\nthe results across the clusters using dynamic programming. We applied SCREEN to\ntwo real datasets and demonstrated that it greatly outperforms the results\nobtained via standard meta-analysis. First, on a collection of 29 case-control\nlarge-scale gene expression cancer studies, we detected a large up-regulated\nmodule of genes related to proliferation and cell cycle regulation. These genes\nare both consistently up-regulated across many cancer studies, and are well\nconnected in known gene networks. Second, on a recent pan-cancer study that\nexamined the expression profiles of patients with or without mutations in the\nHLA complex, we detected an active module of up-regulated genes that are\nrelated to immune responses. Thanks to our ability to quantify the false\ndiscovery rate, we detected thrice more genes as compared to the original\nstudy. Our module contains most of the genes reported in the original study,\nand many new ones. Interestingly, the newly discovered genes are needed to\nestablish the connectivity of the module.\n", "versions": [{"version": "v1", "created": "Mon, 5 Sep 2016 12:00:07 GMT"}, {"version": "v2", "created": "Wed, 7 Sep 2016 12:40:26 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Amar", "David", ""], ["Shamir", "Ron", ""], ["Yekutieli", "Daniel", ""]]}, {"id": "1609.01708", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco J. Rubio", "title": "Tractable Bayesian variable selection: beyond normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection often assumes normality, but the effects of model\nmisspecification are not sufficiently understood. There are sound reasons\nbehind this assumption, particularly for large $p$: ease of interpretation,\nanalytical and computational convenience. More flexible frameworks exist,\nincluding semi- or non-parametric models, often at the cost of some\ntractability. We propose a simple extension of the Normal model that allows for\nskewness and thicker-than-normal tails but preserves tractability. It leads to\neasy interpretation and a log-concave likelihood that facilitates optimization\nand integration. We characterize asymptotically parameter estimation and Bayes\nfactor rates, in particular studying the effects of model misspecification.\nUnder suitable conditions misspecified Bayes factors are consistent and induce\nsparsity at the same asymptotic rates than under the correct model. However,\nthe rates to detect signal are altered by an exponential factor, often\nresulting in a loss of sensitivity. These deficiencies can be ameliorated by\ninferring the error distribution from the data, a simple strategy that can\nimprove inference substantially. Our work focuses on the likelihood and can\nthus be combined with any likelihood penalty or prior, but here we focus on\nnon-local priors to induce extra sparsity and ameliorate finite-sample effects\ncaused by misspecification. Our results highlight the practical importance of\nfocusing on the likelihood rather than solely on the prior, when it comes to\nBayesian variable selection. The methodology is available in R package `mombf'.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 19:58:35 GMT"}, {"version": "v2", "created": "Tue, 28 Mar 2017 08:48:32 GMT"}, {"version": "v3", "created": "Fri, 4 Aug 2017 16:57:40 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco J.", ""]]}, {"id": "1609.01807", "submitter": "Karthyek Rajhaa Annaswamy Murthy", "authors": "Henrik Hult, Sandeep Juneja, Karthyek Murthy", "title": "Exact and efficient simulation of tail probabilities of heavy-tailed\n  infinite series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an efficient simulation algorithm for computing the tail\nprobabilities of the infinite series $S = \\sum_{n \\geq 1} a_n X_n$ when random\nvariables $X_n$ are heavy-tailed. As $S$ is the sum of infinitely many random\nvariables, any simulation algorithm that stops after simulating only fixed,\nfinitely many random variables is likely to introduce a bias. We overcome this\nchallenge by rewriting the tail probability of interest as a sum of a random\nnumber of telescoping terms, and subsequently developing conditional Monte\nCarlo based low variance simulation estimators for each telescoping term. The\nresulting algorithm is proved to result in estimators that a) have no bias, and\nb) require only a fixed, finite number of replications irrespective of how rare\nthe tail probability of interest is. Thus, by combining a traditional variance\nreduction technique such as conditional Monte Carlo with more recent use of\nauxiliary randomization to remove bias in a multi-level type representation, we\ndevelop an efficient and unbiased simulation algorithm for tail probabilities\nof $S$. These have many applications including in analysis of financial\ntime-series and stochastic recurrence equations arising in models in actuarial\nrisk and population biology.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 02:34:25 GMT"}], "update_date": "2016-09-08", "authors_parsed": [["Hult", "Henrik", ""], ["Juneja", "Sandeep", ""], ["Murthy", "Karthyek", ""]]}, {"id": "1609.02123", "submitter": "Ming Teng", "authors": "Ming Teng, Timothy Johnson, Farouk Nathoo", "title": "Time Series Analysis of fMRI Data: Spatial Modelling and Bayesian\n  Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series analysis of fMRI data is an important area of medical statistics\nfor neuroimaging data. The neuroimaging community has embraced mean-field\nvariational Bayes (VB) approximations, which are implemented in Statistical\nParametric Mapping (SPM) software. While computationally efficient, the quality\nof VB approximations remains unclear even though they are commonly used in the\nanalysis of neuroimaging data. For reliable statistical inference, it is\nimportant that these approximations be accurate and that users understand the\nscenarios under which they may not be accurate.\n  We consider this issue for a particular model that includes spatially-varying\ncoefficients. To examine the accuracy of the VB approximation we derive\nHamiltonian Monte Carlo (HMC) for this model and conduct simulation studies to\ncompare its performance with VB. As expected we find that the computation time\nrequired for VB is considerably less than that for HMC. In settings involving a\nhigh or moderate signal-to-noise ratio (SNR) we find that the two approaches\nproduce very similar results suggesting that the VB approximation is useful in\nthis setting. On the other hand, when one considers a low SNR, substantial\ndifferences are found, suggesting that the approximation may not be accurate in\nsuch cases and we demonstrate that VB produces Bayes estimators with larger\nmean squared error (MSE). A real application related to face perception is also\ncarried out. Overall, our work clarifies the usefulness of VB for the\nspatiotemporal analysis of fMRI data, while also pointing out the limitation of\nVB when the SNR is low and the utility of HMC in this case.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2016 19:24:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 00:02:28 GMT"}, {"version": "v3", "created": "Sun, 4 Mar 2018 02:45:55 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Teng", "Ming", ""], ["Johnson", "Timothy", ""], ["Nathoo", "Farouk", ""]]}, {"id": "1609.02354", "submitter": "Leopoldo Catania", "authors": "David Ardia, Kris Boudt, Leopoldo Catania", "title": "Generalized Autoregressive Score Models in R: The GAS Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the R package GAS for the analysis of time series under\nthe Generalized Autoregressive Score (GAS) framework of Creal et al. (2013) and\nHarvey (2013). The distinctive feature of the GAS approach is the use of the\nscore function as the driver of time-variation in the parameters of nonlinear\nmodels. The GAS package provides functions to simulate univariate and\nmultivariate GAS processes, estimate the GAS parameters and to make time series\nforecasts. We illustrate the use of the GAS package with a detailed case study\non estimating the time-varying conditional densities of a set of financial\nassets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 09:40:44 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Ardia", "David", ""], ["Boudt", "Kris", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1609.02409", "submitter": "Kardi Teknomo", "authors": "John Boaz Lee and Kardi Teknomo", "title": "Comparison of several short-term traffic speed forecasting models", "comments": "6 pages, Lee, J. B. and Teknomo, K. (2014) A review of various\n  short-term traffic speed forecasting models, Proceeding of the 12th National\n  Conference in Information Technology Education (NCITE 2014), October 23 - 25,\n  2014, Boracay, Philippines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread adoption of smartphones in recent years has made it possible\nfor us to collect large amounts of traffic data. Special software installed on\nthe phones of drivers allow us to gather GPS trajectories of their vehicles on\nthe road network. In this paper, we simulate the trajectories of multiple\nagents on a road network and use various models to forecast the short-term\ntraffic speed of various links. Our results show that traditional techniques\nlike multiple regression and artificial neural networks work well but simpler\nadaptive models that do not require prior training also perform comparatively\nwell.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2016 10:30:37 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Lee", "John Boaz", ""], ["Teknomo", "Kardi", ""]]}, {"id": "1609.02501", "submitter": "Yawen Guan", "authors": "Yawen Guan and Murali Haran", "title": "A Computationally Efficient Projection-Based Approach for Spatial\n  Generalized Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for spatial generalized linear mixed models (SGLMMs) for\nhigh-dimensional non-Gaussian spatial data is computationally intensive. The\ncomputational challenge is due to the high-dimensional random effects and\nbecause Markov chain Monte Carlo (MCMC) algorithms for these models tend to be\nslow mixing. Moreover, spatial confounding inflates the variance of fixed\neffect (regression coefficient) estimates. Our approach addresses both the\ncomputational and confounding issues by replacing the high-dimensional spatial\nrandom effects with a reduced-dimensional representation based on random\nprojections. Standard MCMC algorithms mix well and the reduced-dimensional\nsetting speeds up computations per iteration. We show, via simulated examples,\nthat Bayesian inference for this reduced-dimensional approach works well both\nin terms of inference as well as prediction, our methods also compare favorably\nto existing \"reduced-rank\" approaches. We also apply our methods to two real\nworld data examples, one on bird count data and the other classifying rock\ntypes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 17:18:58 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 03:21:55 GMT"}, {"version": "v3", "created": "Sun, 7 Oct 2018 02:21:03 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Guan", "Yawen", ""], ["Haran", "Murali", ""]]}, {"id": "1609.02541", "submitter": "Matti Vihola", "authors": "Matti Vihola, Jouni Helske and Jordan Franks", "title": "Importance sampling type estimators based on approximate marginal MCMC", "comments": "34 pages, 1 figure", "journal-ref": "Scand J Statist. 2020; 47: 1339-1376", "doi": "10.1111/sjos.12492", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider importance sampling (IS) type weighted estimators based on Markov\nchain Monte Carlo (MCMC) targeting an approximate marginal of the target\ndistribution. In the context of Bayesian latent variable models, the MCMC\ntypically operates on the hyperparameters, and the subsequent weighting may be\nbased on IS or sequential Monte Carlo (SMC), but allows for multilevel\ntechniques as well. The IS approach provides a natural alternative to delayed\nacceptance (DA) pseudo-marginal/particle MCMC, and has many advantages over DA,\nincluding a straightforward parallelisation and additional flexibility in MCMC\nimplementation. We detail minimal conditions which ensure strong consistency of\nthe suggested estimators, and provide central limit theorems with expressions\nfor asymptotic variances. We demonstrate how our method can make use of SMC in\nthe state space models context, using Laplace approximations and\ntime-discretised diffusions. Our experimental results are promising and show\nthat the IS type approach can provide substantial gains relative to an\nanalogous DA scheme, and is often competitive even without parallelisation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Sep 2016 19:29:29 GMT"}, {"version": "v2", "created": "Sat, 31 Dec 2016 11:33:56 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 18:06:54 GMT"}, {"version": "v4", "created": "Fri, 1 Dec 2017 12:38:30 GMT"}, {"version": "v5", "created": "Wed, 15 Aug 2018 11:52:16 GMT"}, {"version": "v6", "created": "Mon, 9 Mar 2020 14:09:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Vihola", "Matti", ""], ["Helske", "Jouni", ""], ["Franks", "Jordan", ""]]}, {"id": "1609.02901", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Aaron Smith", "title": "Rapid Mixing of Geodesic Walks on Manifolds with Positive Curvature", "comments": "To appear in the Annals of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS math.DG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Markov chain for sampling from the uniform distribution on a\nRiemannian manifold $\\mathcal{M}$, which we call the $\\textit{geodesic walk}$.\nWe prove that the mixing time of this walk on any manifold with positive\nsectional curvature $C_{x}(u,v)$ bounded both above and below by $0 <\n\\mathfrak{m}_{2} \\leq C_{x}(u,v) \\leq \\mathfrak{M}_2 < \\infty$ is\n$\\mathcal{O}^*\\left(\\frac{\\mathfrak{M}_2}{\\mathfrak{m}_2}\\right)$. In\nparticular, this bound on the mixing time does not depend explicitly on the\ndimension of the manifold. In the special case that $\\mathcal{M}$ is the\nboundary of a convex body, we give an explicit and computationally tractable\nalgorithm for approximating the exact geodesic walk. As a consequence, we\nobtain an algorithm for sampling uniformly from the surface of a convex body\nthat has running time bounded solely in terms of the curvature of the body.\n", "versions": [{"version": "v1", "created": "Fri, 9 Sep 2016 19:34:15 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 22:28:40 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Mangoubi", "Oren", ""], ["Smith", "Aaron", ""]]}, {"id": "1609.03295", "submitter": "Manuel Batram", "authors": "Manuel Batram and Dietmar Bauer", "title": "New results on the asymptotic and finite sample properties of the MaCML\n  approach to multinomial probit model estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the properties of the maximum approximate composite marginal\nlikelihood (MaCML) approach to the estimation of multinomial probit models\n(MNP) proposed by Chandra Bhat and coworkers is investigated in finite samples\nas well as with respect to asymptotic properties. Using a small illustration\nexample it is proven that the approach does not necessarily lead to consistent\nestimators for four different types of approximation of the Gaussian cumulative\ndistribution function (including the Solow-Joe approach proposed by Bhat). It\nis shown that the bias of parameter estimates can be substantial (while\ntypically it is small) and the bias in the corresponding implied probabilities\nis small but non-negligible. Furthermore in finite sample it is demonstrated by\nsimulation that between two versions of the Solow-Joe method and two versions\nof the Mendell-Elston approximation no method dominates the others in terms of\naccuracy and numerical speed. Moreover the system to be estimated, the ordering\nof the components in the approximation method and even the tolerance used for\nstopping the numerical optimization routine all have an influence on the\nrelative performance of the procedures corresponding to the various\napproximation methods. Jointly the paper thus points towards eminent research\nneeds in order to decide on the method to use for a particular estimation\nproblem at hand.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 07:47:19 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2016 08:48:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Batram", "Manuel", ""], ["Bauer", "Dietmar", ""]]}, {"id": "1609.03297", "submitter": "Wagner Hugo Bonat Bonat W. H.", "authors": "Wagner H. Bonat and C\\'elestin C. Kokonendji", "title": "Flexible Tweedie regression models for continuous data", "comments": "34 pages, 8 figures", "journal-ref": null, "doi": "10.1080/00949655.2017.1318876", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tweedie regression models provide a flexible family of distributions to deal\nwith non-negative highly right-skewed data as well as symmetric and heavy\ntailed data and can handle continuous data with probability mass at zero. The\nestimation and inference of Tweedie regression models based on the maximum\nlikelihood method are challenged by the presence of an infinity sum in the\nprobability function and non-trivial restrictions on the power parameter space.\nIn this paper, we propose two approaches for fitting Tweedie regression models,\nnamely, quasi- and pseudo-likelihood. We discuss the asymptotic properties of\nthe two approaches and perform simulation studies to compare our methods with\nthe maximum likelihood method. In particular, we show that the quasi-likelihood\nmethod provides asymptotically efficient estimation for regression parameters.\nThe computational implementation of the alternative methods is faster and\neasier than the orthodox maximum likelihood, relying on a simple Newton scoring\nalgorithm. Simulation studies showed that the quasi- and pseudo-likelihood\napproaches present estimates, standard errors and coverage rates similar to the\nmaximum likelihood method. Furthermore, the second-moment assumptions required\nby the quasi- and pseudo-likelihood methods enables us to extend the Tweedie\nregression models to the class of quasi-Tweedie regression models in the\nWedderburn's style. Moreover, it allows to eliminate the non-trivial\nrestriction on the power parameter space, and thus provides a flexible\nregression model to deal with continuous data. We provide \\texttt{R}\nimplementation and illustrate the application of Tweedie regression models\nusing three data sets.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 08:04:38 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Bonat", "Wagner H.", ""], ["Kokonendji", "C\u00e9lestin C.", ""]]}, {"id": "1609.03317", "submitter": "Roberto Di Mari Roberto Di Mari", "authors": "Roberto Rocci, Stefano Antonio Gattone, Roberto Di Mari", "title": "A data driven equivariant approach to constrained Gaussian mixture\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of Gaussian mixture models with different\nclass-specific covariance matrices is known to be problematic. This is due to\nthe unboundedness of the likelihood, together with the presence of spurious\nmaximizers. Existing methods to bypass this obstacle are based on the fact that\nunboundedness is avoided if the eigenvalues of the covariance matrices are\nbounded away from zero. This can be done imposing some constraints on the\ncovariance matrices, i.e. by incorporating a priori information on the\ncovariance structure of the mixture components. The present work introduces a\nconstrained equivariant approach, where the class conditional covariance\nmatrices are shrunk towards a pre-specified matrix Psi. Data-driven choices of\nthe matrix Psi, when a priori information is not available, and the optimal\namount of shrinkage are investigated. The effectiveness of the proposal is\nevaluated on the basis of a simulation study and an empirical example.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 09:02:33 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 07:59:57 GMT"}, {"version": "v3", "created": "Thu, 15 Sep 2016 08:45:31 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 14:00:51 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Rocci", "Roberto", ""], ["Gattone", "Stefano Antonio", ""], ["Di Mari", "Roberto", ""]]}, {"id": "1609.03344", "submitter": "Ning Xu", "authors": "Ning Xu, Jian Hong, Timothy C.G. Fisher", "title": "Finite-sample and asymptotic analysis of generalization ability with an\n  application to penalized regression", "comments": "The theoretical generalization and extension of arXiv:1606.00142", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST q-fin.EC stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the performance of extremum estimators from the\nperspective of generalization ability (GA): the ability of a model to predict\noutcomes in new samples from the same population. By adapting the classical\nconcentration inequalities, we derive upper bounds on the empirical\nout-of-sample prediction errors as a function of the in-sample errors,\nin-sample data size, heaviness in the tails of the error distribution, and\nmodel complexity. We show that the error bounds may be used for tuning key\nestimation hyper-parameters, such as the number of folds $K$ in\ncross-validation. We also show how $K$ affects the bias-variance trade-off for\ncross-validation. We demonstrate that the $\\mathcal{L}_2$-norm difference\nbetween penalized and the corresponding un-penalized regression estimates is\ndirectly explained by the GA of the estimates and the GA of empirical moment\nconditions. Lastly, we prove that all penalized regression estimates are\n$L_2$-consistent for both the $n \\geqslant p$ and the $n < p$ cases.\nSimulations are used to demonstrate key results.\n  Keywords: generalization ability, upper bound of generalization error,\npenalized regression, cross-validation, bias-variance trade-off,\n$\\mathcal{L}_2$ difference between penalized and unpenalized regression, lasso,\nhigh-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 11:09:50 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 09:34:17 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Xu", "Ning", ""], ["Hong", "Jian", ""], ["Fisher", "Timothy C. G.", ""]]}, {"id": "1609.03436", "submitter": "Adam Johansen", "authors": "Murray Pollock and Paul Fearnhead and Adam M. Johansen and Gareth O.\n  Roberts", "title": "Quasi-stationary Monte Carlo and the ScaLE Algorithm", "comments": "Substantially revised with clearer presentation and more extensive\n  simulation study. 59 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a class of Monte Carlo algorithms which are based upon\nthe simulation of a Markov process whose quasi-stationary distribution\ncoincides with a distribution of interest. This differs fundamentally from,\nsay, current Markov chain Monte Carlo methods which simulate a Markov chain\nwhose stationary distribution is the target. We show how to approximate\ndistributions of interest by carefully combining sequential Monte Carlo methods\nwith methodology for the exact simulation of diffusions. The methodology\nintroduced here is particularly promising in that it is applicable to the same\nclass of problems as gradient based Markov chain Monte Carlo algorithms but\nentirely circumvents the need to conduct Metropolis-Hastings type accept/reject\nsteps whilst retaining exactness: the paper gives theoretical guarantees\nensuring the algorithm has the correct limiting target distribution.\nFurthermore, this methodology is highly amenable to big data problems. By\nemploying a modification to existing na{\\\"\\i}ve sub-sampling and control\nvariate techniques it is possible to obtain an algorithm which is still exact\nbut has sub-linear iterative cost as a function of data size.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 15:08:51 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 16:05:05 GMT"}, {"version": "v3", "created": "Mon, 13 Apr 2020 17:08:39 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Pollock", "Murray", ""], ["Fearnhead", "Paul", ""], ["Johansen", "Adam M.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1609.03508", "submitter": "Umberto Picchini", "authors": "Umberto Picchini", "title": "Likelihood-free stochastic approximation EM for inference in complex\n  models", "comments": "Fixed a couple of typos, e.g. in algorithm 3, step (i), we now have\n  y*~p(Y|x*) instead of y*~p(Y|x). Similarly on page 7 (step 1 of Internal\n  SAEM-SL). Published in \"Communications in Statistics: Simulation and\n  Computation\" doi:10.1080/03610918.2017.1401082", "journal-ref": null, "doi": "10.1080/03610918.2017.1401082", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A maximum likelihood methodology for the parameters of models with an\nintractable likelihood is introduced. We produce a likelihood-free version of\nthe stochastic approximation expectation-maximization (SAEM) algorithm to\nmaximize the likelihood function of model parameters. While SAEM is best suited\nfor models having a tractable \"complete likelihood\" function, its application\nto moderately complex models is a difficult or even impossible task. We show\nhow to construct a likelihood-free version of SAEM by using the \"synthetic\nlikelihood\" paradigm. Our method is completely plug-and-play, requires almost\nno tuning and can be applied to both static and dynamic models. Four simulation\nstudies illustrate the method, including a stochastic differential equation\nmodel, a stochastic Lotka-Volterra model and data from $g$-and-$k$\ndistributions. MATLAB code is available as supplementary material.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2016 18:01:34 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 15:43:48 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 09:51:35 GMT"}, {"version": "v4", "created": "Wed, 1 Nov 2017 09:16:31 GMT"}, {"version": "v5", "created": "Tue, 16 Jan 2018 12:19:05 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Picchini", "Umberto", ""]]}, {"id": "1609.04057", "submitter": "Dootika Vats", "authors": "Dootika Vats", "title": "Geometric Ergodicity of Gibbs Samplers in Bayesian Penalized Regression\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three Bayesian penalized regression models and show that the\nrespective deterministic scan Gibbs samplers are geometrically ergodic\nregardless of the dimension of the regression problem. We prove geometric\nergodicity of the Gibbs samplers for the Bayesian fused lasso, the Bayesian\ngroup lasso, and the Bayesian sparse group lasso. Geometric ergodicity along\nwith a moment condition results in the existence of a Markov chain central\nlimit theorem for Monte Carlo averages and ensures reliable output analysis.\nOur results of geometric ergodicity allow us to also provide default starting\nvalues for the Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Tue, 13 Sep 2016 21:22:49 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 16:42:30 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Vats", "Dootika", ""]]}, {"id": "1609.04104", "submitter": "Morteza Mardani", "authors": "Morteza Mardani, Georgios B. Giannakis, and Kamil Ugurbil", "title": "Tracking Tensor Subspaces with Informative Random Sampling for Real-Time\n  MR Imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetic resonance imaging (MRI) nowadays serves as an important modality for\ndiagnostic and therapeutic guidance in clinics. However, the {\\it slow\nacquisition} process, the dynamic deformation of organs, as well as the need\nfor {\\it real-time} reconstruction, pose major challenges toward obtaining\nartifact-free images. To cope with these challenges, the present paper\nadvocates a novel subspace learning framework that permeates benefits from\nparallel factor (PARAFAC) decomposition of tensors (multiway data) to low-rank\nmodeling of temporal sequence of images. Treating images as multiway data\narrays, the novel method preserves spatial structures and unravels the latent\ncorrelations across various dimensions by means of the tensor subspace.\nLeveraging the spatio-temporal correlation of images, Tykhonov regularization\nis adopted as a rank surrogate for a least-squares optimization program.\nAlteranating majorization minimization is adopted to develop online algorithms\nthat recursively procure the reconstruction upon arrival of a new undersampled\n$k$-space frame. The developed algorithms are {\\it provably convergent} and\nhighly {\\it parallelizable} with lightweight FFT tasks per iteration. To\nfurther accelerate the acquisition process, randomized subsampling policies are\ndevised that leverage intermediate estimates of the tensor subspace, offered by\nthe online scheme, to {\\it randomly} acquire {\\it informative} $k$-space\nsamples. In a nutshell, the novel approach enables tracking motion dynamics\nunder low acquisition rates `on the fly.' GPU-based tests with real {\\it in\nvivo} MRI datasets of cardiac cine images corroborate the merits of the novel\napproach relative to state-of-the-art alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 01:23:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Mardani", "Morteza", ""], ["Giannakis", "Georgios B.", ""], ["Ugurbil", "Kamil", ""]]}, {"id": "1609.04232", "submitter": "Jia Guo", "authors": "Jia Guo, Bu Zhou and Jin-Ting Zhang", "title": "A Supremum-Norm Based Test for the Equality of Several Covariance\n  Functions", "comments": "arXiv admin note: text overlap with arXiv:1609.04231", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new test for the equality of several covariance\nfunctions for functional data. Its test statistic is taken as the supremum\nvalue of the sum of the squared differences between the estimated individual\ncovariance functions and the pooled sample covariance function, hoping to\nobtain a more powerful test than some existing tests for the same testing\nproblem. The asymptotic random expression of this test statistic under the null\nhypothesis is obtained. To approximate the null distribution of the proposed\ntest statistic, we describe a parametric bootstrap method and a non-parametric\nbootstrap method. The asymptotic random expression of the proposed test is also\nstudied under a local alternative and it is shown that the proposed test is\nroot-$n$ consistent. Intensive simulation studies are conducted to demonstrate\nthe finite sample performance of the proposed test and it turns out that the\nproposed test is indeed more powerful than some existing tests when functional\ndata are highly correlated. The proposed test is illustrated with three real\ndata examples.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2016 12:15:14 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Guo", "Jia", ""], ["Zhou", "Bu", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1609.04721", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on", "title": "Mixture model modal clustering", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two most extended density-based approaches to clustering are surely\nmixture model clustering and modal clustering. In the mixture model approach,\nthe density is represented as a mixture and clusters are associated to the\ndifferent mixture components. In modal clustering, clusters are understood as\nregions of high density separated from each other by zones of lower density, so\nthat they are closely related to certain regions around the density modes. If\nthe true density is indeed in the assumed class of mixture densities, then\nmixture model clustering allows to scrutinize more subtle situations than modal\nclustering. However, when mixture modeling is used in a nonparametric way,\ntaking advantage of the denseness of the sieve of mixture densities to\napproximate any density, then the correspondence between clusters and mixture\ncomponents may become questionable. In this paper we introduce two methods to\nadopt a modal clustering point of view after a mixture model fit. Numerous\nexamples are provided to illustrate that mixture modeling can also be used for\nclustering in a nonparametric sense, as long as clusters are understood as the\ndomains of attraction of the density modes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 16:24:43 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1609.04740", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira and Luca Martino and David Luengo and M\\'onica F.\n  Bugallo", "title": "Heretical Multiple Importance Sampling", "comments": "8 pages, 2 figures", "journal-ref": "IEEE Signal Processing Letter, Volume 23, Issue 10, October 2016", "doi": "10.1109/LSP.2016.2600678", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Importance Sampling (MIS) methods approximate moments of complicated\ndistributions by drawing samples from a set of proposal distributions. Several\nways to compute the importance weights assigned to each sample have been\nrecently proposed, with the so-called deterministic mixture (DM) weights\nproviding the best performance in terms of variance, at the expense of an\nincrease in the computational cost. A recent work has shown that it is possible\nto achieve a trade-off between variance reduction and computational effort by\nperforming an a priori random clustering of the proposals (partial DM\nalgorithm). In this paper, we propose a novel \"heretical\" MIS framework, where\nthe clustering is performed a posteriori with the goal of reducing the variance\nof the importance sampling weights. This approach yields biased estimators with\na potentially large reduction in variance. Numerical examples show that\nheretical MIS estimators can outperform, in terms of mean squared error (MSE),\nboth the standard and the partial MIS estimators, achieving a performance close\nto that of DM with less computational cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2016 17:13:45 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""], ["Luengo", "David", ""], ["Bugallo", "M\u00f3nica F.", ""]]}, {"id": "1609.05372", "submitter": "Joseph Guinness", "authors": "Joseph Guinness", "title": "Permutation and Grouping Methods for Sharpening Gaussian Process\n  Approximations", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2018.1437476", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vecchia's approximate likelihood for Gaussian process parameters depends on\nhow the observations are ordered, which can be viewed as a deficiency because\nthe exact likelihood is permutation-invariant. This article takes the\nalternative standpoint that the ordering of the observations can be tuned to\nsharpen the approximations. Advantageously chosen orderings can drastically\nimprove the approximations, and in fact, completely random orderings often\nproduce far more accurate approximations than default coordinate-based\norderings do. In addition to the permutation results, automatic methods for\ngrouping calculations of components of the approximation are introduced, having\nthe result of simultaneously improving the quality of the approximation and\nreducing its computational burden. In common settings, reordering combined with\ngrouping reduces Kullback-Leibler divergence from the target model by a factor\nof 80 and computation time by a factor of 2 compared to ungrouped\napproximations with default ordering. The claims are supported by theory and\nnumerical results with comparisons to other approximations, including tapered\ncovariances and stochastic partial differential equation approximations.\nComputational details are provided, including efficiently finding the orderings\nand ordered nearest neighbors, and profiling out linear mean parameters and\nusing the approximations for prediction and conditional simulation. An\napplication to space-time satellite data is presented.\n", "versions": [{"version": "v1", "created": "Sat, 17 Sep 2016 18:00:02 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 18:32:52 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 15:04:52 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Guinness", "Joseph", ""]]}, {"id": "1609.05615", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei", "title": "Discussion of \"Fast Approximate Inference for Arbitrarily Large\n  Semiparametric Regression Models via Message Passing\"", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion paper on \"Fast Approximate Inference for Arbitrarily Large\nSemiparametric Regression Models via Message Passing\" by Wand\n[arXiv:1602.07412].\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2016 07:24:21 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""]]}, {"id": "1609.06402", "submitter": "Zhipeng Liu", "authors": "Jose Blanchet, Jing Dong, Zhipeng Liu", "title": "Exact Sampling of the Infinite Horizon Maximum of a Random Walk Over a\n  Non-linear Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the first algorithm that samples\n$\\max_{n\\geq0}\\{S_{n}-n^{\\alpha}\\},$ where $S_n$ is a mean zero random walk,\nand $n^{\\alpha}$ with $\\alpha\\in(1/2,1)$ defines a nonliner boundary. We show\nthat our algorithm has finite expected running time. We also apply the\nalgorithm to construct the first exact simulation method for the steady-state\ndeparture process of a $GI/GI/\\infty$ queue where the service time distribution\nhas infinite mean.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2016 02:10:01 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 21:50:48 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Blanchet", "Jose", ""], ["Dong", "Jing", ""], ["Liu", "Zhipeng", ""]]}, {"id": "1609.06842", "submitter": "Xiaodong Luo", "authors": "Xiaodong Luo, Tuhin Bhakta, Morten Jakobsen, and Geir N{\\ae}vdal", "title": "Efficient big data assimilation through sparse representation: A 3D\n  benchmark case study in seismic history matching", "comments": "Overlapping with our conference paper at ECMOR XV, 2016. This is the\n  initial draft submitted for review", "journal-ref": null, "doi": "10.2118/185936-PA and 10.2118/180025-PA", "report-no": null, "categories": "physics.data-an math.NA math.OC physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a previous work \\citep{luo2016sparse2d_spej}, the authors proposed an\nensemble-based 4D seismic history matching (SHM) framework, which has some\nrelatively new ingredients, in terms of the type of seismic data in choice, the\nway to handle big seismic data and related data noise estimation, and the use\nof a recently developed iterative ensemble history matching algorithm.\n  In seismic history matching, it is customary to use inverted seismic\nattributes, such as acoustic impedance, as the observed data. In doing so,\nextra uncertainties may arise during the inversion processes. The proposed SHM\nframework avoids such intermediate inversion processes by adopting amplitude\nversus angle (AVA) data. In addition, SHM typically involves assimilating a\nlarge amount of observed seismic attributes into reservoir models. To handle\nthe big-data problem in SHM, the proposed framework adopts the following\nwavelet-based sparse representation procedure: First, a discrete wavelet\ntransform is applied to observed seismic attributes. Then, uncertainty analysis\nis conducted in the wavelet domain to estimate noise in the resulting wavelet\ncoefficients, and to calculate a corresponding threshold value. Wavelet\ncoefficients above the threshold value, called leading wavelet coefficients\nhereafter, are used as the data for history matching. The retained leading\nwavelet coefficients preserve the most salient features of the observed seismic\nattributes, whereas rendering a substantially smaller data size. Finally, an\niterative ensemble smoother is adopted to update reservoir models, in such a\nway that the leading wavelet coefficients of simulated seismic attributes\nbetter match those of observed seismic attributes.\n  (The rest of the abstract was omitted for the length restriction.)\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 07:19:01 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Luo", "Xiaodong", ""], ["Bhakta", "Tuhin", ""], ["Jakobsen", "Morten", ""], ["N\u00e6vdal", "Geir", ""]]}, {"id": "1609.06960", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Magnus Rattray", "title": "BayesBinMix: an R Package for Model Based Clustering of Multivariate\n  Binary Data", "comments": "Accepted to the R Journal. The package is available on CRAN:\n  https://CRAN.R-project.org/package=BayesBinMix", "journal-ref": "The R Journal (2017) 9:1, pages 403-420", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The BayesBinMix package offers a Bayesian framework for clustering binary\ndata with or without missing values by fitting mixtures of multivariate\nBernoulli distributions with an unknown number of components. It allows the\njoint estimation of the number of clusters and model parameters using Markov\nchain Monte Carlo sampling. Heated chains are run in parallel and accelerate\nthe convergence to the target posterior distribution. Identifiability issues\nare addressed by implementing label switching algorithms. The package is\ndemonstrated and benchmarked against the Expectation-Maximization algorithm\nusing a simulation study as well as a real dataset.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 13:17:08 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 12:47:41 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Rattray", "Magnus", ""]]}, {"id": "1609.07007", "submitter": "Jona Cederbaum", "authors": "Jona Cederbaum, Fabian Scheipl and Sonja Greven", "title": "Fast symmetric additive covariance smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast bivariate smoothing approach for symmetric surfaces that\nhas a wide range of applications. We show how it can be applied to estimate the\ncovariance function in longitudinal data as well as multiple additive\ncovariances in functional data with complex correlation structures. Our\nsymmetric smoother can handle (possibly noisy) data sampled on a common, dense\ngrid as well as irregularly or sparsely sampled data. Estimation is based on\nbivariate penalized spline smoothing using a mixed model representation and the\nsymmetry is used to reduce computation time compared to the usual non-symmetric\nsmoothers. We outline the application of our approach in functional principal\ncomponent analysis and demonstrate its practical value in two applications. The\napproach is evaluated in extensive simulations. We provide documented open\nsource software implementing our fast symmetric bivariate smoother building on\nestablished algorithms for additive models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 15:00:25 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Cederbaum", "Jona", ""], ["Scheipl", "Fabian", ""], ["Greven", "Sonja", ""]]}, {"id": "1609.07135", "submitter": "Wentao Li", "authors": "Wentao Li and Paul Fearnhead", "title": "Convergence of Regression Adjusted Approximate Bayesian Computation", "comments": "Main text is shortened and proof is revised. To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present asymptotic results for the regression-adjusted version of\napproximate Bayesian computation introduced by Beaumont(2002). We show that for\nan appropriate choice of the bandwidth, regression adjustment will lead to a\nposterior that, asymptotically, correctly quantifies uncertainty. Furthermore,\nfor such a choice of bandwidth we can implement an importance sampling\nalgorithm to sample from the posterior whose acceptance probability tends to\nunity as the data sample size increases. This compares favourably to results\nfor standard approximate Bayesian computation, where the only way to obtain a\nposterior that correctly quantifies uncertainty is to choose a much smaller\nbandwidth; one for which the acceptance probability tends to zero and hence for\nwhich Monte Carlo error will dominate.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 19:59:15 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 10:22:54 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Li", "Wentao", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1609.07180", "submitter": "Sarah Vall\\'elian", "authors": "D. Andrew Brown, Arvind Saibaba, Sarah Vall\\'elian", "title": "Low Rank Independence Samplers in Bayesian Inverse Problems", "comments": "23 pages, 9 figures, 4 tables; plus supplementary materials (15\n  pages, 16 figures, 1 table)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian inverse problems, the posterior distribution is used to quantify\nuncertainty about the reconstructed solution. In practice, Markov chain Monte\nCarlo algorithms often are used to draw samples from the posterior\ndistribution. However, implementations of such algorithms can be\ncomputationally expensive. We present a computationally efficient scheme for\nsampling high-dimensional Gaussian distributions in ill-posed Bayesian linear\ninverse problems. Our approach uses Metropolis-Hastings independence sampling\nwith a proposal distribution based on a low-rank approximation of the\nprior-preconditioned Hessian. We show the dependence of the acceptance rate on\nthe number of eigenvalues retained and discuss conditions under which the\nacceptance rate is high. We demonstrate our proposed sampler by using it with\nMetropolis-Hastings-within-Gibbs sampling in numerical experiments in image\ndeblurring, computerized tomography, and NMR relaxometry.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2016 22:23:49 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 04:15:51 GMT"}, {"version": "v3", "created": "Mon, 12 Mar 2018 02:54:32 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Brown", "D. Andrew", ""], ["Saibaba", "Arvind", ""], ["Vall\u00e9lian", "Sarah", ""]]}, {"id": "1609.07195", "submitter": "Johannes Lederer", "authors": "Mahsa Taheri, N\\'eh\\'emy Lim, and Johannes Lederer", "title": "Balancing Statistical and Computational Precision and Applications to\n  Penalized Linear Regression with Group Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to technological advances, large and high-dimensional data have become\nthe rule rather than the exception. Methods that allow for feature selection\nwith such data are thus highly sought after, in particular, since standard\nmethods, such as cross-validated lasso and group-lasso, can be challenging both\ncomputationally and mathematically. In this paper, we propose a novel approach\nto feature selection and group feature selection in linear regression. It\nconsists of simple optimization steps and tests, which makes it computationally\nmore efficient than standard approaches and suitable even for very large data\nsets. Moreover, it satisfies sharp guarantees for estimation and feature\nselection in terms of oracle inequalities. We thus expect that our contribution\ncan help to leverage the increasing volume of data in Biology, Public Health,\nAstronomy, Economics, and other fields.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 00:35:23 GMT"}, {"version": "v2", "created": "Sat, 30 Nov 2019 20:31:06 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Taheri", "Mahsa", ""], ["Lim", "N\u00e9h\u00e9my", ""], ["Lederer", "Johannes", ""]]}, {"id": "1609.07363", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead and Guillem Rigaill", "title": "Changepoint Detection in the Presence of Outliers", "comments": "Updated to include a proof of consistency and accuracy of estimating\n  change points using the biweight loss function", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many traditional methods for identifying changepoints can struggle in the\npresence of outliers, or when the noise is heavy-tailed. Often they will infer\nadditional changepoints in order to fit the outliers. To overcome this problem,\ndata often needs to be pre-processed to remove outliers, though this is\ndifficult for applications where the data needs to be analysed online. We\npresent an approach to changepoint detection that is robust to the presence of\noutliers. The idea is to adapt existing penalised cost approaches for detecting\nchanges so that they use loss functions that are less sensitive to outliers. We\nargue that loss functions that are bounded, such as the classical biweight\nloss, are particularly suitable -- as we show that only bounded loss functions\nare robust to arbitrarily extreme outliers. We present an efficient dynamic\nprogramming algorithm that can find the optimal segmentation under our\npenalised cost criteria. Importantly, this algorithm can be used in settings\nwhere the data needs to be analysed online. We show that we can consistently\nestimate the number of changepoints, and accurately estimate their locations,\nusing the biweight loss function. We demonstrate the usefulness of our approach\nfor applications such as analysing well-log data, detecting copy number\nvariation, and detecting tampering of wireless devices.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 13:49:23 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 10:56:16 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Fearnhead", "Paul", ""], ["Rigaill", "Guillem", ""]]}, {"id": "1609.07386", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad and Adam J. Rothman", "title": "A penalized likelihood method for classification with matrix-valued\n  predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a penalized likelihood method to fit the linear discriminant\nanalysis model when the predictor is matrix valued. We simultaneously estimate\nthe means and the precision matrix, which we assume has a Kronecker product\ndecomposition. Our penalties encourage pairs of response category mean matrices\nto have equal entries and also encourage zeros in the precision matrix. To\ncompute our estimators, we use a blockwise coordinate descent algorithm. To\nupdate the optimization variables corresponding to response category mean\nmatrices, we use an alternating minimization algorithm that takes advantage of\nthe Kronecker structure of the precision matrix. We show that our method can\noutperform relevant competitors in classification, even when our modeling\nassumptions are violated. We analyze an EEG dataset to demonstrate our method's\ninterpretability and classification accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 14:40:47 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 03:42:49 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1609.07444", "submitter": "Art Owen", "authors": "Kinjal Basu and Art B. Owen", "title": "Quasi-Monte Carlo for an Integrand with a Singularity along a Diagonal\n  in the Square", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo methods are designed for integrands of bounded variation,\nand this excludes singular integrands. Several methods are known for integrands\nthat become singular on the boundary of the unit cube $[0,1]^d$ or at isolated\npossibly unknown points within $[0,1]^d$. Here we consider functions on the\nsquare $[0,1]^2$ that may become singular as the point approaches the diagonal\nline $x_1=x_2$, and we study three quadrature methods. The first method splits\nthe square into two triangles separated by a region around the line of\nsingularity, and applies recently developed triangle QMC rules to the two\ntriangular parts. For functions with a singularity `no worse than\n$|x_1-x_2|^{-A}$ for $0<A<1$ that method yields an error of $O(\n(\\log(n)/n)^{(1-A)/2})$. We also consider methods extending the integrand into\na region containing the singularity and show that method will not improve up on\nusing two triangles. Finally, we consider transforming the integrand to have a\nmore QMC-friendly singularity along the boundary of the square. This then leads\nto error rates of $O(n^{-1+\\epsilon+A})$ when combined with some\ncorner-avoiding Halton points or with randomized QMC, but it requires some\nstronger assumptions on the original singular integrand.\n", "versions": [{"version": "v1", "created": "Fri, 23 Sep 2016 17:39:36 GMT"}, {"version": "v2", "created": "Thu, 13 Oct 2016 22:26:44 GMT"}, {"version": "v3", "created": "Tue, 11 Apr 2017 18:14:17 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Basu", "Kinjal", ""], ["Owen", "Art B.", ""]]}, {"id": "1609.07630", "submitter": "Renato J Cintra", "authors": "P. A. M. Oliveira, R. J. Cintra, F. M. Bayer, S. Kulasekera, A.\n  Madanayake, V. A. Coutinho", "title": "Low-complexity Image and Video Coding Based on an Approximate Discrete\n  Tchebichef Transform", "comments": "Fixed diagonal matrix, 11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": "10.1109/TCSVT.2016.2515378", "report-no": null, "categories": "cs.MM cs.CV cs.DS stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of linear transformations has great relevance for data\ndecorrelation applications, like image and video compression. In that sense,\nthe discrete Tchebichef transform (DTT) possesses useful coding and\ndecorrelation properties. The DTT transform kernel does not depend on the input\ndata and fast algorithms can be developed to real time applications. However,\nthe DTT fast algorithm presented in literature possess high computational\ncomplexity. In this work, we introduce a new low-complexity approximation for\nthe DTT. The fast algorithm of the proposed transform is multiplication-free\nand requires a reduced number of additions and bit-shifting operations. Image\nand video compression simulations in popular standards shows good performance\nof the proposed transform. Regarding hardware resource consumption for FPGA\nshows 43.1% reduction of configurable logic blocks and ASIC place and route\nrealization shows 57.7% reduction in the area-time figure when compared with\nthe 2-D version of the exact DTT.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 14:49:31 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 21:18:07 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2019 17:05:20 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Oliveira", "P. A. M.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""], ["Coutinho", "V. A.", ""]]}, {"id": "1609.07662", "submitter": "Evgeny Burnaev", "authors": "Alexey Artemov and Evgeny Burnaev", "title": "Detecting Performance Degradation of Software-Intensive Systems in the\n  Presence of Trends and Long-Range Dependence", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As contemporary software-intensive systems reach increasingly large scale, it\nis imperative that failure detection schemes be developed to help prevent\ncostly system downtimes. A promising direction towards the construction of such\nschemes is the exploitation of easily available measurements of system\nperformance characteristics such as average number of processed requests and\nqueue size per unit of time. In this work, we investigate a holistic\nmethodology for detection of abrupt changes in time series data in the presence\nof quasi-seasonal trends and long-range dependence with a focus on failure\ndetection in computer systems. We propose a trend estimation method enjoying\noptimality properties in the presence of long-range dependent noise to estimate\nwhat is considered \"normal\" system behaviour. To detect change-points and\nanomalies, we develop an approach based on the ensembles of \"weak\" detectors.\nWe demonstrate the performance of the proposed change-point detection scheme\nusing an artificial dataset, the publicly available Abilene dataset as well as\nthe proprietary geoinformation system dataset.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2016 19:20:18 GMT"}], "update_date": "2016-09-27", "authors_parsed": [["Artemov", "Alexey", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1609.07731", "submitter": "Luca Martino", "authors": "Luca Martino, Jesse Read, Victor Elvira, Francisco Louzada", "title": "Cooperative Parallel Particle Filters for online model selection and\n  applications to Urban Mobility", "comments": "A preliminary Matlab code is provided at\n  http://www.mathworks.com/matlabcentral/fileexchange/58597-model-averanging-particle-filter,\n  Digital Signal Processing, 2016", "journal-ref": "Digital Signal Processing, Volume 60, Pages 172-185, 2017", "doi": "10.1016/j.dsp.2016.09.011", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design a sequential Monte Carlo scheme for the dual purpose of Bayesian\ninference and model selection. We consider the application context of urban\nmobility, where several modalities of transport and different measurement\ndevices can be employed. Therefore, we address the joint problem of online\ntracking and detection of the current modality. For this purpose, we use\ninteracting parallel particle filters, each one addressing a different model.\nThey cooperate for providing a global estimator of the variable of interest\nand, at the same time, an approximation of the posterior density of each model\ngiven the data. The interaction occurs by a parsimonious distribution of the\ncomputational effort, with online adaptation for the number of particles of\neach filter according to the posterior probability of the corresponding model.\nThe resulting scheme is simple and flexible. We have tested the novel technique\nin different numerical experiments with artificial and real data, which confirm\nthe robustness of the proposed scheme.\n", "versions": [{"version": "v1", "created": "Sun, 25 Sep 2016 11:10:48 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""], ["Elvira", "Victor", ""], ["Louzada", "Francisco", ""]]}, {"id": "1609.07844", "submitter": "Vladimir Minin", "authors": "Amrit Dhar and Vladimir N. Minin", "title": "Calculating higher-order moments of phylogenetic stochastic mapping\n  summaries in linear time", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic mapping is a simulation-based method for probabilistically mapping\nsubstitution histories onto phylogenies according to continuous-time Markov\nmodels of evolution. This technique can be used to infer properties of the\nevolutionary process on the phylogeny and, unlike parsimony-based mapping,\nconditions on the observed data to randomly draw substitution mappings that do\nnot necessarily require the minimum number of events on a tree. Most stochastic\nmapping applications simulate substitution mappings only to estimate the mean\nand/or variance of two commonly used mapping summaries: the number of\nparticular types of substitutions (labeled substitution counts) and the time\nspent in a particular group of states (labeled dwelling times) on the tree.\nFast, simulation-free algorithms for calculating the mean of stochastic mapping\nsummaries exist. Importantly, these algorithms scale linearly in the number of\ntips/leaves of the phylogenetic tree. However, to our knowledge, no such\nalgorithm exists for calculating higher-order moments of stochastic mapping\nsummaries. We present one such simulation-free dynamic programming algorithm\nthat calculates prior and posterior mapping variances and scales linearly in\nthe number of phylogeny tips. Our procedure suggests a general framework that\ncan be used to efficiently compute higher-order moments of stochastic mapping\nsummaries without simulations. We demonstrate the usefulness of our algorithm\nby extending previously developed statistical tests for rate variation across\nsites and for detecting evolutionarily conserved regions in genomic sequences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2016 04:12:00 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 06:15:14 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Dhar", "Amrit", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1609.08569", "submitter": "Carolina Eu\\'an Campos", "authors": "Carolina Euan, Hernando Ombao and Joaquin Ortega", "title": "The Hierarchical Spectral Merger algorithm: A New Time Series Clustering\n  Procedure", "comments": "arXiv admin note: text overlap with arXiv:1507.05018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for time series clustering which we call the\nHierarchical Spectral Merger (HSM) method. This procedure is based on the\nspectral theory of time series and identifies series that share similar\noscillations or waveforms. The extent of similarity between a pair of time\nseries is measured using the total variation distance between their estimated\nspectral densities. At each step of the algorithm, every time two clusters\nmerge, a new spectral density is estimated using the whole information present\nin both clusters, which is representative of all the series in the new cluster.\nThe method is implemented in an R package HSMClust. We present two applications\nof the HSM method, one to data coming from wave-height measurements in\noceanography and the other to electroencefalogram (EEG) data.\n", "versions": [{"version": "v1", "created": "Tue, 27 Sep 2016 18:35:50 GMT"}], "update_date": "2016-09-28", "authors_parsed": [["Euan", "Carolina", ""], ["Ombao", "Hernando", ""], ["Ortega", "Joaquin", ""]]}, {"id": "1609.09174", "submitter": "Chaitanya Joshi Dr.", "authors": "Chaitanya Joshi, Paul T. Brown, Stephen Joe", "title": "Improving Grid Based Bayesian Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some cases, computational benefit can be gained by exploring the hyper\nparameter space using a deterministic set of grid points instead of a Markov\nchain. We view this as a numerical integration problem and make three unique\ncontributions. First, we explore the space using low discrepancy point sets\ninstead of a grid. This allows for accurate estimation of marginals of any\nshape at a much lower computational cost than a grid based approach and thus\nmakes it possible to extend the computational benefit to a hyper parameter\nspace with higher dimensionality (10 or more). Second, we propose a new, quick\nand easy method to estimate the marginal using a least squares polynomial and\nprove the conditions under which this polynomial will converge to the true\nmarginal. Our results are valid for a wide range of point sets including grids,\nrandom points and low discrepancy points. Third, we show that further accuracy\nand efficiency can be gained by taking into consideration the functional\ndecomposition of the integrand and illustrate how this can be done using\nanchored f-ANOVA on weighted spaces.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 02:02:09 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Joshi", "Chaitanya", ""], ["Brown", "Paul T.", ""], ["Joe", "Stephen", ""]]}, {"id": "1609.09286", "submitter": "Bruno Sudret", "authors": "Chu V. Mai and Bruno Sudret", "title": "Surrogate models for oscillatory systems using sparse polynomial chaos\n  expansions and stochastic time warping", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2016-010-V2", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCE) have proven efficiency in a number of\nfields for propagating parametric uncertainties through computational models of\ncomplex systems, namely structural and fluid mechanics, chemical reactions and\nelectromagnetism, etc. For problems involving oscillatory, time-dependent\noutput quantities of interest, it is well-known that reasonable accuracy of\nPCE-based approaches is difficult to reach in the long term. In this paper, we\npropose a fully non-intrusive approach based on stochastic time warping to\naddress this issue: each realization (trajectory) of the model response is\nfirst rescaled to its own time scale so as to put all sampled trajectories in\nphase in a common virtual time line. Principal component analysis is introduced\nto compress the information contained in these transformed trajectories and\nsparse PCE representations using least angle regression are finally used to\napproximate the components. The approach shows remarkably small prediction\nerror for particular trajectories as well as for second-order statistics of the\nlatter. It is illustrated on different benchmark problems well known in the\nliterature on time-dependent PCE problems, ranging from rigid body dynamics,\nchemical reactions to forced oscillations of a non linear system.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2016 10:20:59 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 14:20:09 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Mai", "Chu V.", ""], ["Sudret", "Bruno", ""]]}]