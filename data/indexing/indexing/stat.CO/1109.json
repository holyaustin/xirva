[{"id": "1109.0152", "submitter": "Bernd Fellinghauer", "authors": "Bernd Fellinghauer, Peter B\\\"uhlmann, Martin Ryffel, Michael von Rhein\n  and Jan D. Reinhardt", "title": "Stable Graphical Model Estimation with Random Forests for Discrete,\n  Continuous, and Mixed Variables", "comments": "The authors report no conflict of interest. There was no external\n  funding", "journal-ref": null, "doi": "10.1016/j.csda.2013.02.022", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional independence graph is a concise representation of pairwise\nconditional independence among many variables. Graphical Random Forests (GRaFo)\nare a novel method for estimating pairwise conditional independence\nrelationships among mixed-type, i.e. continuous and discrete, variables. The\nnumber of edges is a tuning parameter in any graphical model estimator and\nthere is no obvious number that constitutes a good choice. Stability Selection\nhelps choosing this parameter with respect to a bound on the expected number of\nfalse positives (error control).\n  The performance of GRaFo is evaluated and compared with various other methods\nfor p = 50, 100, and 200 possibly mixed-type variables while sample size is n =\n100 (n = 500 for maximum likelihood). Furthermore, GRaFo is applied to data\nfrom the Swiss Health Survey in order to evaluate how well it can reproduce the\ninterconnection of functional health components, personal, and environmental\nfactors, as hypothesized by the World Health Organization's International\nClassification of Functioning, Disability and Health (ICF). Finally, GRaFo is\nused to identify risk factors which may be associated with adverse\nneurodevelopment of children who suffer from trisomy 21 and experienced\nopen-heart surgery.\n  GRaFo performs well with mixed data and thanks to Stability Selection it\nprovides an error control mechanism for false positive selection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 11:20:49 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2012 21:30:12 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Fellinghauer", "Bernd", ""], ["B\u00fchlmann", "Peter", ""], ["Ryffel", "Martin", ""], ["von Rhein", "Michael", ""], ["Reinhardt", "Jan D.", ""]]}, {"id": "1109.0701", "submitter": "Bryony J. Hill", "authors": "Bryony J. Hill, Wilfrid S. Kendall, Elke Th\\\"onnes", "title": "Fibre-generated point processes and fields of orientations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS553 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 994-1020", "doi": "10.1214/12-AOAS553", "report-no": "IMS-AOAS-AOAS553", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to analyzing spatial point data\nclustered along or around a system of curves or \"fibres.\" Such data arise in\ncatalogues of galaxy locations, recorded locations of earthquakes, aerial\nimages of minefields and pore patterns on fingerprints. Finding the underlying\ncurvilinear structure of these point-pattern data sets may not only facilitate\na better understanding of how they arise but also aid reconstruction of missing\ndata. We base the space of fibres on the set of integral lines of an\norientation field. Using an empirical Bayes approach, we estimate the field of\norientations from anisotropic features of the data. We then sample from the\nposterior distribution of fibres, exploring models with different numbers of\nclusters, fitting fibres to the clusters as we proceed. The Bayesian approach\npermits inference on various properties of the clusters and associated fibres,\nand the results perform well on a number of very different curvilinear\nstructures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 11:36:33 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 16:48:33 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 06:45:09 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Hill", "Bryony J.", ""], ["Kendall", "Wilfrid S.", ""], ["Th\u00f6nnes", "Elke", ""]]}, {"id": "1109.0725", "submitter": "Chris Tofallis", "authors": "Chris Tofallis", "title": "Model Building with Multiple Dependent Variables and Constraints", "comments": null, "journal-ref": "J. Royal Stat. Soc. Series D: The Statistician, (1999), 48(3),\n  371-378", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used method for finding relationships between several\nquantities is multiple regression. This however is restricted to a single\ndependent variable. We present a more general method which allows models to be\nconstructed with multiple variables on both sides of an equation and which can\nbe computed easily using a spreadsheet program. The underlying principle\n(originating from canonical correlation analysis) is that of maximising the\ncorrelation between the two sides of the model equation. This paper presents a\nfitting procedure which makes it possible to force the estimated model to\nsatisfy constraint conditions which it is required to possess, these may arise\nfrom theory, prior knowledge or be intuitively obvious. We also show that the\nleast squares approach to the problem is inadequate as it produces models which\nare not scale invariant.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 16:36:42 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Tofallis", "Chris", ""]]}, {"id": "1109.1516", "submitter": "Youssef Marzouk", "authors": "Tarek A. El Moselhy and Youssef M. Marzouk", "title": "Bayesian Inference with Optimal Maps", "comments": "66 pages, 26 figures. Minor revisions and improvements throughout.\n  Published in Journal of Computational Physics", "journal-ref": null, "doi": "10.1016/j.jcp.2012.07.022", "report-no": null, "categories": "stat.CO math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to Bayesian inference that entirely avoids Markov\nchain simulation, by constructing a map that pushes forward the prior measure\nto the posterior measure. Existence and uniqueness of a suitable\nmeasure-preserving map is established by formulating the problem in the context\nof optimal transport theory. We discuss various means of explicitly\nparameterizing the map and computing it efficiently through solution of an\noptimization problem, exploiting gradient information from the forward model\nwhen possible. The resulting algorithm overcomes many of the computational\nbottlenecks associated with Markov chain Monte Carlo. Advantages of a map-based\nrepresentation of the posterior include analytical expressions for posterior\nmoments and the ability to generate arbitrary numbers of independent posterior\nsamples without additional likelihood evaluations or forward solves. The\noptimization approach also provides clear convergence criteria for posterior\napproximation and facilitates model selection through automatic evaluation of\nthe marginal likelihood. We demonstrate the accuracy and efficiency of the\napproach on nonlinear inverse problems of varying dimension, involving the\ninference of parameters appearing in ordinary and partial differential\nequations.\n", "versions": [{"version": "v1", "created": "Wed, 7 Sep 2011 17:24:01 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2011 06:06:58 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2012 06:22:03 GMT"}], "update_date": "2012-08-31", "authors_parsed": [["Moselhy", "Tarek A. El", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1109.2279", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott and Jesse Windle", "title": "The Bayesian Bridge", "comments": "Supplemental files are available from the second author's website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Bayesian bridge estimator for regularized regression and\nclassification. Two key mixture representations for the Bayesian bridge model\nare developed: (1) a scale mixture of normals with respect to an alpha-stable\nrandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle\ndensities) with respect to a two-component mixture of gamma random variables.\nBoth lead to MCMC methods for posterior simulation, and these methods turn out\nto have complementary domains of maximum efficiency. The first representation\nis a well known result due to West (1987), and is the better choice for\ncollinear design matrices. The second representation is new, and is more\nefficient for orthogonal problems, largely because it avoids the need to deal\nwith exponentially tilted stable random variables. It also provides insight\ninto the multimodality of the joint posterior distribution, a feature of the\nbridge model that is notably absent under ridge or lasso-type priors. We prove\na theorem that extends this representation to a wider class of densities\nrepresentable as scale mixtures of betas, and provide an explicit inversion\nformula for the mixing distribution. The connections with slice sampling and\nscale mixtures of normals are explored. On the practical side, we find that the\nBayesian bridge model outperforms its classical cousin in estimation and\nprediction across a variety of data sets, both simulated and real. We also show\nthat the MCMC for fitting the bridge model exhibits excellent mixing\nproperties, particularly for the global scale parameter. This makes for a\nfavorable contrast with analogous MCMC algorithms for other sparse Bayesian\nmodels. All methods described in this paper are implemented in the R package\nBayesBridge. An extensive set of simulation results are provided in two\nsupplemental files.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 04:57:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2012 21:01:16 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1109.2411", "submitter": "Kei Hirose", "authors": "Kei Hirose, Shohei Tateishi and Sadanori Konishi", "title": "Efficient algorithm to select tuning parameters in sparse regression\n  modeling with regularization", "comments": "24pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse regression modeling via regularization such as the lasso, it is\nimportant to select appropriate values of tuning parameters including\nregularization parameters. The choice of tuning parameters can be viewed as a\nmodel selection and evaluation problem. Mallows' $C_p$ type criteria may be\nused as a tuning parameter selection tool in lasso-type regularization methods,\nfor which the concept of degrees of freedom plays a key role. In the present\npaper, we propose an efficient algorithm that computes the degrees of freedom\nby extending the generalized path seeking algorithm. Our procedure allows us to\nconstruct model selection criteria for evaluating models estimated by\nregularization with a wide variety of convex and non-convex penalties. Monte\nCarlo simulations demonstrate that our methodology performs well in various\nsituations. A real data example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 09:33:12 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 01:01:26 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2012 04:41:51 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Hirose", "Kei", ""], ["Tateishi", "Shohei", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1109.2527", "submitter": "SM Enayetur Raheem", "authors": "SM Enayetur Raheem and S. Ejaz Ahmed", "title": "Positive-shrinkage and Pretest Estimation in Multiple Regression: A\n  Monte Carlo study with Applications", "comments": "17 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a problem of predicting a response variable using a set of\ncovariates in a linear regression model. If it is \\emph{a priori} known or\nsuspected that a subset of the covariates do not significantly contribute to\nthe overall fit of the model, a restricted model that excludes these\ncovariates, may be sufficient. If, on the other hand, the subset provides\nuseful information, shrinkage method combines restricted and unrestricted\nestimators to obtain the parameter estimates. Such an estimator outperforms the\nclassical maximum likelihood estimators. Any \\emph{prior} information may be\nvalidated through preliminary test (or pretest), and depending on the validity,\nmay be incorporated in the model as a parametric restriction. Thus, pretest\nestimator chooses between the restricted and unrestricted estimators depending\non the outcome of the preliminary test. Examples using three real life data\nsets are provided to illustrate the application of shrinkage and pretest\nestimation. Performance of positive-shrinkage and pretest estimators are\ncompared with unrestricted estimator under varying degree of uncertainty of the\nprior information. Monte Carlo study reconfirms the asymptotic properties of\nthe estimators available in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 16:40:36 GMT"}], "update_date": "2011-09-13", "authors_parsed": [["Raheem", "SM Enayetur", ""], ["Ahmed", "S. Ejaz", ""]]}, {"id": "1109.3254", "submitter": "Jannis Dimitriadis", "authors": "Jannis Dimitriadis", "title": "Rigorous Computing of Rectangle Scan Probabilities for Markov Increments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending recent work of Corrado, we derive an algorithm that computes\nrigorous upper and lower bounds for rectangle scan probabilities for Markov\nincrements. We experimentally examine the closeness of the bounds computed by\nthe algorithm and we examine the range of tractable input variables.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 03:48:23 GMT"}], "update_date": "2011-09-16", "authors_parsed": [["Dimitriadis", "Jannis", ""]]}, {"id": "1109.3409", "submitter": "Hao Wang", "authors": "Hao Wang and Natesh S. Pillai", "title": "On a Class of Shrinkage Priors for Covariance Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible class of models based on scale mixture of uniform\ndistributions to construct shrinkage priors for covariance matrix estimation.\nThis new class of priors enjoys a number of advantages over the traditional\nscale mixture of normal priors, including its simplicity and flexibility in\ncharacterizing the prior density. We also exhibit a simple, easy to implement\nGibbs sampler for posterior simulation which leads to efficient estimation in\nhigh dimensional problems. We first discuss the theory and computational\ndetails of this new approach and then extend the basic model to a new class of\nmultivariate conditional autoregressive models for analyzing multivariate areal\ndata. The proposed spatial model flexibly characterizes both the spatial and\nthe outcome correlation structures at an appealing computational cost. Examples\nconsisting of both synthetic and real-world data show the utility of this new\nframework in terms of robust estimation as well as improved predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 17:25:55 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2011 01:15:23 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Wang", "Hao", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1109.3488", "submitter": "Andrew Clark", "authors": "Andrew Clark and Jeff Kenyon", "title": "Using MOEAs To Outperform Stock Benchmarks In The Presence of Typical\n  Investment Constraints", "comments": "21 pages, Index Terms - multi-objective evolutionary algorithms\n  (MOEA), mean-variance optimization, financial constraints, multi-period MOEAs\n  Updated version of paper. Will appear in Journal of Investing in 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.CE cs.NE stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio managers are typically constrained by turnover limits, minimum and\nmaximum stock positions, cardinality, a target market capitalization and\nsometimes the need to hew to a style (such as growth or value). In addition,\nportfolio managers often use multifactor stock models to choose stocks based\nupon their respective fundamental data.\n  We use multiobjective evolutionary algorithms (MOEAs) to satisfy the above\nreal-world constraints. The portfolios generated consistently outperform\ntypical performance benchmarks and have statistically significant asset\nselection.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 21:15:22 GMT"}, {"version": "v2", "created": "Mon, 2 Jan 2012 21:33:25 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Clark", "Andrew", ""], ["Kenyon", "Jeff", ""]]}, {"id": "1109.3829", "submitter": "Pierre E. Jacob", "authors": "Luke Bornn (Harvard University), Pierre Jacob (Universite Paris\n  Dauphine), Pierre Del Moral (INRIA Bordeaux Sud-Ouest and Universite de\n  Bordeaux), Arnaud Doucet (University of Oxford)", "title": "An Adaptive Interacting Wang-Landau Algorithm for Automatic Density\n  Exploration", "comments": "33 pages, 20 figures (the supplementary materials are included as\n  appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statisticians are well-accustomed to performing exploratory analysis in\nthe modeling stage of an analysis, the notion of conducting preliminary\ngeneral-purpose exploratory analysis in the Monte Carlo stage (or more\ngenerally, the model-fitting stage) of an analysis is an area which we feel\ndeserves much further attention. Towards this aim, this paper proposes a\ngeneral-purpose algorithm for automatic density exploration. The proposed\nexploration algorithm combines and expands upon components from various\nadaptive Markov chain Monte Carlo methods, with the Wang-Landau algorithm at\nits heart. Additionally, the algorithm is run on interacting parallel chains --\na feature which both decreases computational cost as well as stabilizes the\nalgorithm, improving its ability to explore the density. Performance is studied\nin several applications. Through a Bayesian variable selection example, the\nauthors demonstrate the convergence gains obtained with interacting chains. The\nability of the algorithm's adaptive proposal to induce mode-jumping is\nillustrated through a trimodal density and a Bayesian mixture modeling\napplication. Lastly, through a 2D Ising model, the authors demonstrate the\nability of the algorithm to overcome the high correlations encountered in\nspatial models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2011 01:02:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 07:03:31 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2012 04:10:10 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Bornn", "Luke", "", "Harvard University"], ["Jacob", "Pierre", "", "Universite Paris\n  Dauphine"], ["Del Moral", "Pierre", "", "INRIA Bordeaux Sud-Ouest and Universite de\n  Bordeaux"], ["Doucet", "Arnaud", "", "University of Oxford"]]}, {"id": "1109.4003", "submitter": "Juerg Schelldorfer js", "authors": "J\\\"urg Schelldorfer, Lukas Meier and Peter B\\\"uhlmann", "title": "GLMMLasso: An Algorithm for High-Dimensional Generalized Linear Mixed\n  Models Using L1-Penalization", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics. Volume 23,\n  Issue 2, 2014, pages 460-477", "doi": "10.1080/10618600.2013.773239", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an L1-penalized algorithm for fitting high-dimensional generalized\nlinear mixed models. Generalized linear mixed models (GLMMs) can be viewed as\nan extension of generalized linear models for clustered observations. This\nLasso-type approach for GLMMs should be mainly used as variable screening\nmethod to reduce the number of variables below the sample size. We then suggest\na refitting by maximum likelihood based on the selected variables only. This is\nan effective correction to overcome problems stemming from the variable\nscreening procedure which are more severe with GLMMs. We illustrate the\nperformance of our algorithm on simulated as well as on real data examples.\nSupplemental materials are available online and the algorithm is implemented in\nthe R package glmmixedlasso.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 11:44:54 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2012 19:50:46 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Schelldorfer", "J\u00fcrg", ""], ["Meier", "Lukas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1109.4166", "submitter": "Robert Erhardt", "authors": "Robert J. Erhardt, Richard L. Smith", "title": "Approximate Bayesian Computing for Spatial Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of max-stable processes used to model spatial extremes\nhas been limited by the difficulty in calculating the joint likelihood\nfunction. This precludes all standard likelihood-based approaches, including\nBayesian approaches. In this paper we present a Bayesian approach through the\nuse of approximate Bayesian computing. This circumvents the need for a joint\nlikelihood function by instead relying on simulations from the (unavailable)\nlikelihood. This method is compared with an alternative approach based on the\ncomposite likelihood. We demonstrate that approximate Bayesian computing can\nresult in a lower mean square error than the composite likelihood approach when\nestimating the spatial dependence of extremes, though at an appreciably higher\ncomputational cost. We also illustrate the performance of the method with an\napplication to US temperature data to estimate the risk of crop loss due to an\nunlikely freeze event.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 20:37:21 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 17:13:54 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2011 17:58:27 GMT"}, {"version": "v4", "created": "Tue, 13 Dec 2011 00:13:16 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Erhardt", "Robert J.", ""], ["Smith", "Richard L.", ""]]}, {"id": "1109.4180", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Default Bayesian analysis for multi-way tables: a data-augmentation\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy for regularized estimation in multi-way\ncontingency tables, which are common in meta-analyses and multi-center clinical\ntrials. Our approach is based on data augmentation, and appeals heavily to a\nnovel class of Polya-Gamma distributions. Our main contributions are to build\nup the relevant distributional theory and to demonstrate three useful features\nof this data-augmentation scheme. First, it leads to simple EM and\nGibbs-sampling algorithms for posterior inference, circumventing the need for\nanalytic approximations, numerical integration, Metropolis--Hastings, or\nvariational methods. Second, it allows modelers much more flexibility when\nchoosing priors, which have traditionally come from the Dirichlet or\nlogistic-normal family. For example, our approach allows users to incorporate\nBayesian analogues of classical penalized-likelihood techniques (e.g. the lasso\nor bridge) in computing regularized estimates for log-odds ratios. Finally, our\ndata-augmentation scheme naturally suggests a default strategy for prior\nselection based on the logistic-Z model, which is strongly related to Jeffreys'\nprior for a binomial proportion. To illustrate the method we focus primarily on\nthe particular case of a meta-analysis/multi-center study (or a JxKxN table).\nBut the general approach encompasses many other common situations, of which we\nwill provide examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 22:04:04 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1109.4777", "submitter": "Federico Bassetti", "authors": "Federico Bassetti, Roberto Casarin, Fabrizio Leisen", "title": "Beta-Product Poisson-Dirichlet Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data may exhibit clustering over time and, in a multiple time\nseries context, the clustering behavior may differ across the series. This\npaper is motivated by the Bayesian non--parametric modeling of the dependence\nbetween the clustering structures and the distributions of different time\nseries. We follow a Dirichlet process mixture approach and introduce a new\nclass of multivariate dependent Dirichlet processes (DDP). The proposed DDP are\nrepresented in terms of vector of stick-breaking processes with dependent\nweights. The weights are beta random vectors that determine different and\ndependent clustering effects along the dimension of the DDP vector. We discuss\nsome theoretical properties and provide an efficient Monte Carlo Markov Chain\nalgorithm for posterior computation. The effectiveness of the method is\nillustrated with a simulation study and an application to the United States and\nthe European Union industrial production indexes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 11:32:31 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Bassetti", "Federico", ""], ["Casarin", "Roberto", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1109.6090", "submitter": "Eric Chi", "authors": "Eric C. Chi and David W. Scott", "title": "Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion", "comments": "41 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 23(1):111-128,\n  2014", "doi": "10.1080/10618600.2012.737296", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 04:09:37 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 04:29:12 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2012 23:15:18 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chi", "Eric C.", ""], ["Scott", "David W.", ""]]}, {"id": "1109.6779", "submitter": "Nick Whiteley", "authors": "Nick Whiteley", "title": "Stability properties of some particle filters", "comments": "Published in at http://dx.doi.org/10.1214/12-AAP909 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2013, Vol. 23, No. 6, 2500-2537", "doi": "10.1214/12-AAP909", "report-no": "IMS-AAP-AAP909", "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under multiplicative drift and other regularity conditions, it is established\nthat the asymptotic variance associated with a particle filter approximation of\nthe prediction filter is bounded uniformly in time, and the nonasymptotic,\nrelative variance associated with a particle approximation of the normalizing\nconstant is bounded linearly in time. The conditions are demonstrated to hold\nfor some hidden Markov models on noncompact state spaces. The particle\nstability results are obtained by proving $v$-norm multiplicative stability and\nexponential moment results for the underlying Feynman-Kac formulas.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2011 10:12:34 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2011 16:50:52 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2013 15:09:08 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Whiteley", "Nick", ""]]}]