[{"id": "1607.00021", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "The Simulator: An Engine to Streamline Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulator is an R package that streamlines the process of performing\nsimulations by creating a common infrastructure that can be easily used and\nreused across projects. Methodological statisticians routinely write\nsimulations to compare their methods to preexisting ones. While developing\nideas, there is a temptation to write \"quick and dirty\" simulations to try out\nideas. This approach of rapid prototyping is useful but can sometimes backfire\nif bugs are introduced. Using the simulator allows one to remove the \"dirty\"\nwithout sacrificing the \"quick.\" Coding is quick because the statistician\nfocuses exclusively on those aspects of the simulation that are specific to the\nparticular paper being written. Code written with the simulator is succinct,\nhighly readable, and easily shared with others. The modular nature of\nsimulations written with the simulator promotes code reusability, which saves\ntime and facilitates reproducibility. The syntax of the simulator leads to\nsimulation code that is easily human-readable. Other benefits of using the\nsimulator include the ability to \"step in\" to a simulation and change one\naspect without having to rerun the entire simulation from scratch, the\nstraightforward integration of parallel computing into simulations, and the\nability to rapidly generate plots, tables, and reports with minimal effort.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:04:45 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1607.00101", "submitter": "Zhaosong Lu", "authors": "Zhaosong Lu", "title": "Randomized block proximal damped Newton method for composite\n  self-concordant minimization", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG cs.NA math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the composite self-concordant (CSC) minimization\nproblem, which minimizes the sum of a self-concordant function $f$ and a\n(possibly nonsmooth) proper closed convex function $g$. The CSC minimization is\nthe cornerstone of the path-following interior point methods for solving a\nbroad class of convex optimization problems. It has also found numerous\napplications in machine learning. The proximal damped Newton (PDN) methods have\nbeen well studied in the literature for solving this problem that enjoy a nice\niteration complexity. Given that at each iteration these methods typically\nrequire evaluating or accessing the Hessian of $f$ and also need to solve a\nproximal Newton subproblem, the cost per iteration can be prohibitively high\nwhen applied to large-scale problems. Inspired by the recent success of block\ncoordinate descent methods, we propose a randomized block proximal damped\nNewton (RBPDN) method for solving the CSC minimization. Compared to the PDN\nmethods, the computational cost per iteration of RBPDN is usually significantly\nlower. The computational experiment on a class of regularized logistic\nregression problems demonstrate that RBPDN is indeed promising in solving\nlarge-scale CSC minimization problems. The convergence of RBPDN is also\nanalyzed in the paper. In particular, we show that RBPDN is globally convergent\nwhen $g$ is Lipschitz continuous. It is also shown that RBPDN enjoys a local\nlinear convergence. Moreover, we show that for a class of $g$ including the\ncase where $g$ is Lipschitz differentiable, RBPDN enjoys a global linear\nconvergence. As a striking consequence, it shows that the classical damped\nNewton methods [22,40] and the PDN [31] for such $g$ are globally linearly\nconvergent, which was previously unknown in the literature. Moreover, this\nresult can be used to sharpen the existing iteration complexity of these\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 03:16:57 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Lu", "Zhaosong", ""]]}, {"id": "1607.00866", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie", "title": "The Primal versus the Dual Ising Model", "comments": "Proc. 55th Annual Allerton Conference on Communication, Control, and\n  Computing, Monticello, USA, Oct 3-6, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent the Ising model of statistical physics by normal factor graphs\nin the primal and in the dual domains. By analogy with Kirchhoff's voltage and\ncurrent laws, we show that in the primal normal factor graphs, the dependency\namong the variables is along the cycles, whereas in the dual normal factor\ngraphs, the dependency is on the cutsets. In the primal (resp. dual) domain,\ndependent variables can be computed via their fundamental cycles (resp.\nfundamental cutsets). Using Onsager's closed form solution, we illustrate the\nopposite behavior of the uniform sampling estimator for estimating the\npartition function in the primal and in the dual of the homogeneous Ising model\non a two-dimensional torus.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 12:54:30 GMT"}, {"version": "v2", "created": "Tue, 3 Jan 2017 17:46:41 GMT"}, {"version": "v3", "created": "Sun, 22 Jul 2018 12:44:24 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Molkaraie", "Mehdi", ""]]}, {"id": "1607.00959", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko", "title": "Optimal Design of the Shiryaev-Roberts Chart: Give Your Shiryaev-Roberts\n  a Headstart", "comments": "21 pages, 20 figures, 2 tables; To appear in Proceedings of the 12th\n  International Workshop on Intelligent Statistical Quality Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer a numerical study of the effect of headstarting on the performance\nof a Shiryaev-Roberts (SR) chart set up to control the mean of a normal\nprocess. The study is a natural extension of that previously carried out by\nLucas and Crosier for the CUSUM scheme in their seminal 1982 paper published in\nTechnometrics. The Fast Initial Response (FIR) feature exhibited by a\nheadstarted CUSUM turns out to be also characteristic of an SR chart\n(re-)started off a nonzero initial score. However, our main result is the\nobservation that a FIR SR with a carefully designed {\\em optimal} headstart is\nnot just faster to react to an initial out-of-control situation, it is nearly\n{\\em the} fastest {\\em uniformly}, i.e., assuming the process under\nsurveillance is equally likely to go out of control effective any sample\nnumber. The performance improvement is the greater, the fainter the change. We\nexplain the optimization strategy, and tabulate the optimal initial score,\ncontrol limit, and the corresponding \"worst possible\" out-of-control Average\nRun Length (ARL), considering mean-shifts of diverse magnitudes and a wide\nrange of levels of the in-control ARL.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 16:54:48 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Polunchenko", "Aleksey S.", ""]]}, {"id": "1607.01375", "submitter": "Raghu Pasupathy", "authors": "Kalyani Nagaraj, Jie Xu, Raghu Pasupathy, and Soumyadip Ghosh", "title": "Efficient Estimation in the Tails of Gaussian Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of efficient estimation in the tails of Gaussian\ncopulas. Our special focus is estimating expectations over multi-dimensional\nconstrained sets that have a small implied measure under the Gaussian copula.\nWe propose three estimators, all of which rely on a simple idea: identify\ncertain \\emph{dominating} point(s) of the feasible set, and appropriately shift\nand scale an exponential distribution for subsequent use within an importance\nsampling measure. As we show, the efficiency of such estimators depends\ncrucially on the local structure of the feasible set around the dominating\npoints. The first of our proposed estimators $\\estOpt$ is the\n\"full-information\" estimator that actively exploits such local structure to\nachieve bounded relative error in Gaussian settings. The second and third\nestimators $\\estExp$, $\\estLap$ are \"partial-information\" estimators, for use\nwhen complete information about the constraint set is not available, they do\nnot exhibit bounded relative error but are shown to achieve polynomial\nefficiency. We provide sharp asymptotics for all three estimators. For the\nNORTA setting where no ready information about the dominating points or the\nfeasible set structure is assumed, we construct a multinomial mixture of the\npartial-information estimator $\\estLap$ resulting in a fourth estimator\n$\\estNt$ with polynomial efficiency, and implementable through the ecoNORTA\nalgorithm. Numerical results on various example problems are remarkable, and\nconsistent with theory.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:26:39 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Nagaraj", "Kalyani", ""], ["Xu", "Jie", ""], ["Pasupathy", "Raghu", ""], ["Ghosh", "Soumyadip", ""]]}, {"id": "1607.01458", "submitter": "JInglai Li", "authors": "Qingping Zhou, Zixi Hu, Zhewei Yao, Jinglai Li", "title": "A hybrid adaptive MCMC algorithm in function spaces", "comments": "arXiv admin note: text overlap with arXiv:1511.05838", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The preconditioned Crank-Nicolson (pCN) method is a Markov Chain Monte Carlo\n(MCMC) scheme, specifically designed to perform Bayesian inferences in function\nspaces. Unlike many standard MCMC algorithms, the pCN method can preserve the\nsampling efficiency under the mesh refinement, a property referred to as being\ndimension independent. In this work we consider an adaptive strategy to further\nimprove the efficiency of pCN. In particular we develop a hybrid adaptive MCMC\nmethod: the algorithm performs an adaptive Metropolis scheme in a chosen finite\ndimensional subspace, and a standard pCN algorithm in the complement space of\nthe chosen subspace. We show that the proposed algorithm satisfies certain\nimportant ergodicity conditions. Finally with numerical examples we demonstrate\nthat the proposed method has competitive performance with existing adaptive\nalgorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 02:02:51 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Zhou", "Qingping", ""], ["Hu", "Zixi", ""], ["Yao", "Zhewei", ""], ["Li", "Jinglai", ""]]}, {"id": "1607.01664", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "Personalized Optimization for Computer Experiments with Environmental\n  Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization problems with both control variables and environmental variables\narise in many fields. This paper introduces a framework of personalized\noptimization to han- dle such problems. Unlike traditional robust optimization,\npersonalized optimization devotes to finding a series of optimal control\nvariables for different values of environmental variables. Therefore, the\nsolution from personalized optimization consists of optimal surfaces defined on\nthe domain of the environmental variables. When the environmental variables can\nbe observed or measured, personalized optimization yields more reasonable and\nbetter solution- s than robust optimization. The implementation of personalized\noptimization for complex computer models is discussed. Based on statistical\nmodeling of computer experiments, we provide two algorithms to sequentially\ndesign input values for approximating the optimal surfaces. Numerical examples\nshow the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 15:16:44 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1607.01881", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Tiangang Cui, Karen Willcox, Luis Tenorio, Youssef\n  Marzouk", "title": "Goal-oriented optimal approximations of Bayesian linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose optimal dimensionality reduction techniques for the solution of\ngoal-oriented linear-Gaussian inverse problems, where the quantity of interest\n(QoI) is a function of the inversion parameters. These approximations are\nsuitable for large-scale applications. In particular, we study the\napproximation of the posterior covariance of the QoI as a low-rank negative\nupdate of its prior covariance, and prove optimality of this update with\nrespect to the natural geodesic distance on the manifold of symmetric positive\ndefinite matrices. Assuming exact knowledge of the posterior mean of the QoI,\nthe optimality results extend to optimality in distribution with respect to the\nKullback-Leibler divergence and the Hellinger distance between the associated\ndistributions. We also propose approximation of the posterior mean of the QoI\nas a low-rank linear function of the data, and prove optimality of this\napproximation with respect to a weighted Bayes risk. Both of these optimal\napproximations avoid the explicit computation of the full posterior\ndistribution of the parameters and instead focus on directions that are well\ninformed by the data and relevant to the QoI. These directions stem from a\nbalance among all the components of the goal-oriented inverse problem: prior\ninformation, forward model, measurement noise, and ultimate goals. We\nillustrate the theory using a high-dimensional inverse problem in heat\ntransfer.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 06:36:31 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 23:12:42 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Spantini", "Alessio", ""], ["Cui", "Tiangang", ""], ["Willcox", "Karen", ""], ["Tenorio", "Luis", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1607.01904", "submitter": "Zheng Wang", "authors": "Zheng Wang, Johnathan M. Bardsley, Antti Solonen, Tiangang Cui, and\n  Youssef M. Marzouk", "title": "Bayesian inverse problems with $l_1$ priors: a Randomize-then-Optimize\n  approach", "comments": "Preprint 24 pages, 13 figures. v1 submitted to SIAM Journal on\n  Scientific Computing on July 5, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior distributions for Bayesian inference that rely on the $l_1$-norm of the\nparameters are of considerable interest, in part because they promote parameter\nfields with less regularity than Gaussian priors (e.g., discontinuities and\nblockiness). These $l_1$-type priors include the total variation (TV) prior and\nthe Besov $B^s_{1,1}$ space prior, and in general yield non-Gaussian posterior\ndistributions. Sampling from these posteriors is challenging, particularly in\nthe inverse problem setting where the parameter space is high-dimensional and\nthe forward problem may be nonlinear. This paper extends the\nrandomize-then-optimize (RTO) method, an optimization-based sampling algorithm\ndeveloped for Bayesian inverse problems with Gaussian priors, to inverse\nproblems with $l_1$-type priors. We use a variable transformation to convert an\n$l_1$-type prior to a standard Gaussian prior, such that the posterior\ndistribution of the transformed parameters is amenable to Metropolized sampling\nvia RTO. We demonstrate this approach on several deconvolution problems and an\nelliptic PDE inverse problem, using TV or Besov $B^s_{1,1}$ space priors. Our\nresults show that the transformed RTO algorithm characterizes the correct\nposterior distribution and can be more efficient than other sampling\nalgorithms. The variable transformation can also be extended to other\nnon-Gaussian priors.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 07:52:32 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2016 21:44:49 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Wang", "Zheng", ""], ["Bardsley", "Johnathan M.", ""], ["Solonen", "Antti", ""], ["Cui", "Tiangang", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1607.01985", "submitter": "Khoa T. Tran", "authors": "Khoa T. Tran", "title": "A Common Derivation for Markov Chain Monte Carlo Algorithms with\n  Tractable and Intractable Targets", "comments": "Novel designs for multivariate, directional, elliptical and pseudo\n  marginal Hamiltonian slice sampling. This update improved the flow of ideas\n  and clarity up to section 3.6 where major enhancement in the notation,\n  explanation for Neal's recursive proposal generation mechanism. Strong\n  emphasis also on showing that MH-sampling is actually slice sampling in\n  disguise", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo is a class of algorithms for drawing Markovian\nsamples from high-dimensional target densities to approximate the numerical\nintegration associated with computing statistical expectation, especially in\nBayesian statistics. However, many Markov chain Monte Carlo algorithms do not\nseem to share the same theoretical support and each algorithm is proven in a\ndifferent way. This incurs many terminologies and ancillary concepts, which\nmakes Markov chain Monte Carlo literature seems to be scattered and\nintimidating to researchers from many other fields, including new researchers\nof Bayesian statistics.\n  A generalised version of the Metropolis-Hastings algorithm is constructed\nwith a random number generator and a self-reverse mapping. This formulation\nadmits many other Markov chain Monte Carlo algorithms as special cases. A\ncommon derivation for many Markov chain Monte Carlo algorithms is useful in\ndrawing connections and comparisons between these algorithms. As a result, we\nnow can construct many novel combinations of multiple Markov chain Monte Carlo\nalgorithms that amplify the efficiency of each individual algorithm.\nSpecifically, we propose two novel sampling schemes that combine slice sampling\nwith directional or Hamiltonian sampling. Our Hamiltonian slice sampling scheme\nis also applicable in the pseudo-marginal context where the target density is\nintractable but can be unbiasedly estimated, e.g. using particle filtering.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 12:28:14 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2016 12:10:16 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2016 04:57:38 GMT"}, {"version": "v4", "created": "Sun, 19 Mar 2017 07:07:58 GMT"}, {"version": "v5", "created": "Sun, 25 Mar 2018 03:45:48 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Tran", "Khoa T.", ""]]}, {"id": "1607.02188", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Jonas Wallin, Adam Johansson, Tufve\n  Nyholm, Thomas Asklund, Jun Yu", "title": "Whole-brain substitute CT generation using Markov random field mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computed tomography (CT) equivalent information is needed for attenuation\ncorrection in PET imaging and for dose planning in radiotherapy. Prior work has\nshown that Gaussian mixture models can be used to generate a substitute CT\n(s-CT) image from a specific set of MRI modalities. This work introduces a more\nflexible class of mixture models for s-CT generation, that incorporates spatial\ndependency in the data through a Markov random field prior on the latent field\nof class memberships associated with a mixture model. Furthermore, the mixture\ndistributions are extended from Gaussian to normal inverse Gaussian (NIG),\nallowing heavier tails and skewness. The amount of data needed to train a model\nfor s-CT generation is of the order of 100 million voxels. The computational\nefficiency of the parameter estimation and prediction methods are hence\nparamount, especially when spatial dependency is included in the models. A\nstochastic Expectation Maximization (EM) gradient algorithm is proposed in\norder to tackle this challenge. The advantages of the spatial model and NIG\ndistributions are evaluated with a cross-validation study based on data from 14\npatients. The study show that the proposed model enhances the predictive\nquality of the s-CT images by reducing the mean absolute error with 17.9%.\nAlso, the distribution of CT values conditioned on the MR images are better\nexplained by the proposed model as evaluated using continuous ranked\nprobability scores.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 22:52:36 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2016 15:11:38 GMT"}], "update_date": "2016-09-29", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Wallin", "Jonas", ""], ["Johansson", "Adam", ""], ["Nyholm", "Tufve", ""], ["Asklund", "Thomas", ""], ["Yu", "Jun", ""]]}, {"id": "1607.02194", "submitter": "Andres Christen", "authors": "J. Andr\\'es Christen, Marcos A. Capistr\\'an and Miguel \\'Angel Moreles", "title": "Numerical posterior distribution error control and expected Bayes\n  Factors in the bayesian Uncertainty Quantification of Inverse Problems", "comments": "3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the bayesian analysis of Inverse Problems most relevant cases the forward\nmaps (FM, or regressor function) are defined in terms of a system of (O, P)DE's\nwith intractable solutions. These necessarily involve a numerical method to\nfind approximate versions of such solutions and lead to a numerical/approximate\nposterior distribution. Recently several results have been published on the\nregularity conditions required on such numerical methods to ensure converge of\nthe numerical to the theoretical posterior. However, more practical guidelines\nare needed to ensure a suitable working numerical posterior. ]Capistran2016]\nprove for ODE's that the Bayes Factor of the approximate vs the theoretical\nmodel tends to 1 in the same order as the numerical method order. In this work\nwe generalize the latter paper in that we consider 1) also PDE's, 2) correlated\nobservations, 3) practical guidelines in a multidimensional setting and 4)\nexplore the use of expected Bayes Factors. This permits us to obtain bounds on\nthe absolute global errors to be tolerated by the FM numerical solver, which we\nillustrate with some examples. Since the Bayes Factor is kept above 0.95 we\nexpect that the resulting numerical posterior is basically indistinguishable\nfrom the theoretical posterior, even though we are using an approximate\nnumerical FM. The method is illustrated with some examples using synthetic\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 23:12:19 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 18:14:13 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Christen", "J. Andr\u00e9s", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Moreles", "Miguel \u00c1ngel", ""]]}, {"id": "1607.02391", "submitter": "Joachim Lebovits", "authors": "Joachim Lebovits, Mark Podolskij", "title": "Estimation of the global regularity of a multifractional Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new estimator of the global regularity index of a\nmultifractional Brownian motion. Our estimation method is based upon a ratio\nstatistic, which compares the realized global quadratic variation of a\nmultifractional Brownian motion at two different frequencies. We show that a\nlogarithmic transformation of this statistic converges in probability to the\nminimum of the Hurst function, which is, under weak assumptions, identical to\nthe global regularity index of the path.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 14:47:24 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Lebovits", "Joachim", ""], ["Podolskij", "Mark", ""]]}, {"id": "1607.02472", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Michel Broniatowski", "title": "Two Iterative Proximal-Point Algorithms for the Calculus of\n  Divergence-based Estimators with Application to Mixture Models", "comments": "Article submitted to IEEE Transactions on Information Theory so that\n  copy writes may be transfered if accepted. arXiv admin note: substantial text\n  overlap with arXiv:1603.07117", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators derived from an EM algorithm are not robust since they are based\non the maximization of the likelihood function. We propose a proximal-point\nalgorithm based on the EM algorithm which aim to minimize a divergence\ncriterion. Resulting estimators are generally robust against outliers and\nmisspecification. An EM-type proximal-point algorithm is also introduced in\norder to produce robust estimators for mixture models. Convergence properties\nof the two algorithms are treated. We relax an identifiability condition\nimposed on the proximal term in the literature; a condition which is generally\nnot fulfilled by mixture models. The convergence of the introduced algorithms\nis discussed on a two-component Weibull mixture and a two-component Gaussian\nmixture entailing a condition on the initialization of the EM algorithm in\norder for the later to converge. Simulations on mixture models using different\nstatistical divergences are provided to confirm the validity of our work and\nthe robustness of the resulting estimators against outliers in comparison to\nthe EM algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 15:04:31 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Broniatowski", "Michel", ""]]}, {"id": "1607.02633", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Julie Lyng Forman", "title": "Bayesian inference for stochastic differential equation mixed effects\n  models of a tumor xenography study", "comments": "Minor revision: posterior predictive checks for BSL have ben updated\n  (both theory and results). Code on GitHub has ben revised accordingly", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2019", "doi": "10.1111/rssc.12347", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference for stochastic differential equation mixed\neffects models (SDEMEMs) exemplifying tumor response to treatment and regrowth\nin mice. We produce an extensive study on how a SDEMEM can be fitted using both\nexact inference based on pseudo-marginal MCMC and approximate inference via\nBayesian synthetic likelihoods (BSL). We investigate a two-compartments SDEMEM,\nthese corresponding to the fractions of tumor cells killed by and survived to a\ntreatment, respectively. Case study data considers a tumor xenography study\nwith two treatment groups and one control, each containing 5-8 mice. Results\nfrom the case study and from simulations indicate that the SDEMEM is able to\nreproduce the observed growth patterns and that BSL is a robust tool for\ninference in SDEMEMs. Finally, we compare the fit of the SDEMEM to a similar\nordinary differential equation model. Due to small sample sizes, strong prior\ninformation is needed to identify all model parameters in the SDEMEM and it\ncannot be determined which of the two models is the better in terms of\npredicting tumor growth curves. In a simulation study we find that with a\nsample of 17 mice per group BSL is able to identify all model parameters and\ndistinguish treatment groups.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 16:30:33 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:03:46 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2018 17:19:19 GMT"}, {"version": "v5", "created": "Sun, 17 Feb 2019 10:51:35 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Picchini", "Umberto", ""], ["Forman", "Julie Lyng", ""]]}, {"id": "1607.02758", "submitter": "V\\'ictor Elvira", "authors": "V\\'ictor Elvira, Luca Martino, David Luengo, and M\\'onica F. Bugallo", "title": "Improving Population Monte Carlo: Alternative Weighting and Resampling\n  Schemes", "comments": "Signal Processing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population Monte Carlo (PMC) sampling methods are powerful tools for\napproximating distributions of static unknowns given a set of observations.\nThese methods are iterative in nature: at each step they generate samples from\na proposal distribution and assign them weights according to the importance\nsampling principle. Critical issues in applying PMC methods are the choice of\nthe generating functions for the samples and the avoidance of the sample\ndegeneracy. In this paper, we propose three new schemes that considerably\nimprove the performance of the original PMC formulation by allowing for better\nexploration of the space of unknowns and by selecting more adequately the\nsurviving samples. A theoretical analysis is performed, proving the superiority\nof the novel schemes in terms of variance of the associated estimators and\npreservation of the sample diversity. Furthermore, we show that they outperform\nother state of the art algorithms (both in terms of mean square error and\nrobustness w.r.t. initialization) through extensive numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 15:48:16 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Elvira", "V\u00edctor", ""], ["Martino", "Luca", ""], ["Luengo", "David", ""], ["Bugallo", "M\u00f3nica F.", ""]]}, {"id": "1607.02788", "submitter": "Youssef Marzouk", "authors": "Patrick Conrad, Andrew Davis, Youssef Marzouk, Natesh Pillai, Aaron\n  Smith", "title": "Parallel local approximation MCMC for expensive models", "comments": "34 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing Bayesian inference via Markov chain Monte Carlo (MCMC) can be\nexceedingly expensive when posterior evaluations invoke the evaluation of a\ncomputationally expensive model, such as a system of partial differential\nequations. In recent work [Conrad et al. JASA 2016, arXiv:1402.1694], we\ndescribed a framework for constructing and refining local approximations of\nsuch models during an MCMC simulation. These posterior--adapted approximations\nharness regularity of the model to reduce the computational cost of inference\nwhile preserving asymptotic exactness of the Markov chain. Here we describe two\nextensions of that work. First, we prove that samplers running in parallel can\ncollaboratively construct a shared posterior approximation while ensuring\nergodicity of each associated chain, providing a novel opportunity for\nexploiting parallel computation in MCMC. Second, focusing on the\nMetropolis--adjusted Langevin algorithm, we describe how a proposal\ndistribution can successfully employ gradients and other relevant information\nextracted from the approximation. We investigate the practical performance of\nour strategies using two challenging inference problems, the first in\nsubsurface hydrology and the second in glaciology. Using local approximations\nconstructed via parallel chains, we successfully reduce the run time needed to\ncharacterize the posterior distributions in these problems from days to hours\nand from months to days, respectively, dramatically improving the tractability\nof Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 21:46:43 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 21:50:54 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Conrad", "Patrick", ""], ["Davis", "Andrew", ""], ["Marzouk", "Youssef", ""], ["Pillai", "Natesh", ""], ["Smith", "Aaron", ""]]}, {"id": "1607.02896", "submitter": "Matteo Ruggiero", "authors": "Omiros Papaspiliopoulos, Matteo Ruggiero and Dario Span\\`o", "title": "Conjugacy properties of time-evolving Dirichlet and gamma random\n  measures", "comments": "To appear on the Electronic Journal of Statistics. arXiv admin note:\n  text overlap with arXiv:1411.4944", "journal-ref": "Electron. J. Statist. Volume 10, Number 2 (2016), 3452-3489", "doi": "10.1214/16-EJS1194", "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend classic characterisations of posterior distributions under\nDirichlet process and gamma random measures priors to a dynamic framework. We\nconsider the problem of learning, from indirect observations, two families of\ntime-dependent processes of interest in Bayesian nonparametrics: the first is a\ndependent Dirichlet process driven by a Fleming-Viot model, and the data are\nrandom samples from the process state at discrete times; the second is a\ncollection of dependent gamma random measures driven by a Dawson-Watanabe\nmodel, and the data are collected according to a Poisson point process with\nintensity given by the process state at discrete times. Both driving processes\nare diffusions taking values in the space of discrete measures whose support\nvaries with time, and are stationary and reversible with respect to Dirichlet\nand gamma priors respectively. A common methodology is developed to obtain in\nclosed form the time-marginal posteriors given past and present data. These are\nshown to belong to classes of finite mixtures of Dirichlet processes and gamma\nrandom measures for the two models respectively, yielding conjugacy of these\nclasses to the type of data we consider. We provide explicit results on the\nparameters of the mixture components and on the mixing weights, which are\ntime-varying and drive the mixtures towards the respective priors in absence of\nfurther data. Explicit algorithms are provided to recursively compute the\nparameters of the mixtures. Our results are based on the projective properties\nof the signals and on certain duality properties of their projections.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 10:49:28 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 13:02:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Papaspiliopoulos", "Omiros", ""], ["Ruggiero", "Matteo", ""], ["Span\u00f2", "Dario", ""]]}, {"id": "1607.03188", "submitter": "Joris Bierkens", "authors": "Joris Bierkens, Paul Fearnhead, Gareth Roberts", "title": "The Zig-Zag Process and Super-Efficient Sampling for Bayesian Analysis\n  of Big Data", "comments": null, "journal-ref": "Ann. Statist., Volume 47, Number 3 (2019), 1288-1320", "doi": "10.1214/18-AOS1715", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard MCMC methods can scale poorly to big data settings due to the need\nto evaluate the likelihood at each iteration. There have been a number of\napproximate MCMC algorithms that use sub-sampling ideas to reduce this\ncomputational burden, but with the drawback that these algorithms no longer\ntarget the true posterior distribution. We introduce a new family of Monte\nCarlo methods based upon a multi-dimensional version of the Zig-Zag process of\n(Bierkens, Roberts, 2017), a continuous time piecewise deterministic Markov\nprocess. While traditional MCMC methods are reversible by construction (a\nproperty which is known to inhibit rapid convergence) the Zig-Zag process\noffers a flexible non-reversible alternative which we observe to often have\nfavourable convergence properties. We show how the Zig-Zag process can be\nsimulated without discretisation error, and give conditions for the process to\nbe ergodic. Most importantly, we introduce a sub-sampling version of the\nZig-Zag process that is an example of an {\\em exact approximate scheme}, i.e.\nthe resulting approximate process still has the posterior as its stationary\ndistribution. Furthermore, if we use a control-variate idea to reduce the\nvariance of our unbiased estimator, then the Zig-Zag process can be\nsuper-efficient: after an initial pre-processing step, essentially independent\nsamples from the posterior distribution are obtained at a computational cost\nwhich does not depend on the size of the data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:30:50 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 13:30:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bierkens", "Joris", ""], ["Fearnhead", "Paul", ""], ["Roberts", "Gareth", ""]]}, {"id": "1607.03195", "submitter": "J Massey Cashore", "authors": "J. Massey Cashore, Lemuel Kumarga, Peter I. Frazier", "title": "Multi-Step Bayesian Optimization for One-Dimensional Feasibility\n  Determination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization methods allocate limited sampling budgets to maximize\nexpensive-to-evaluate functions. One-step-lookahead policies are often used,\nbut computing optimal multi-step-lookahead policies remains a challenge. We\nconsider a specialized Bayesian optimization problem: finding the superlevel\nset of an expensive one-dimensional function, with a Markov process prior. We\ncompute the Bayes-optimal sampling policy efficiently, and characterize the\nsuboptimality of one-step lookahead. Our numerical experiments demonstrate that\nthe one-step lookahead policy is close to optimal in this problem, performing\nwithin 98% of optimal in the experimental settings considered.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 23:09:52 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Cashore", "J. Massey", ""], ["Kumarga", "Lemuel", ""], ["Frazier", "Peter I.", ""]]}, {"id": "1607.03390", "submitter": "Shengxin Zhu", "authors": "Shengxin Zhu, Tongxiang Gu, Xiaowen Xu, Zeyao Mo", "title": "Information Splitting for Big Data Analytics", "comments": "arXiv admin note: text overlap with arXiv:1605.07646", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical models require an estimation of unknown (co)-variance\nparameter(s) in a model. The estimation usually obtained by maximizing a\nlog-likelihood which involves log determinant terms. In principle, one requires\nthe \\emph{observed information}--the negative Hessian matrix or the second\nderivative of the log-likelihood---to obtain an accurate maximum likelihood\nestimator according to the Newton method. When one uses the \\emph{Fisher\ninformation}, the expect value of the observed information, a simpler algorithm\nthan the Newton method is obtained as the Fisher scoring algorithm. With the\nadvance in high-throughput technologies in the biological sciences,\nrecommendation systems and social networks, the sizes of data sets---and the\ncorresponding statistical models---have suddenly increased by several orders of\nmagnitude. Neither the observed information nor the Fisher information is easy\nto obtained for these big data sets. This paper introduces an information\nsplitting technique to simplify the computation. After splitting the mean of\nthe observed information and the Fisher information, an simpler approximate\nHessian matrix for the log-likelihood can be obtained. This approximated\nHessian matrix can significantly reduce computations, and makes the linear\nmixed model applicable for big data sets. Such a spitting and simpler formulas\nheavily depends on matrix algebra transforms, and applicable to large scale\nbreeding model, genetics wide association analysis.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 15:16:52 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Zhu", "Shengxin", ""], ["Gu", "Tongxiang", ""], ["Xu", "Xiaowen", ""], ["Mo", "Zeyao", ""]]}, {"id": "1607.03518", "submitter": "Bamdad Hosseini Mr.", "authors": "Bamdad Hosseini and John M. Stockie", "title": "Airborne contaminant source estimation using a finite-volume forward\n  solver coupled with a Bayesian inversion approach", "comments": "Fixed a few typos in figures", "journal-ref": "Computers & Fluids, 154:27-43, 2017", "doi": "10.1016/j.compfluid.2017.05.025", "report-no": null, "categories": "math.NA stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a numerical algorithm for solving the atmospheric dispersion\nproblem with elevated point sources and ground-level deposition. The problem is\nmodelled by the 3D advection-diffusion equation with delta-distribution source\nterms, as well as height-dependent advection speed and diffusion coefficients.\nWe construct a finite volume scheme using a splitting approach in which the\nClawpack software package is used as the advection solver and an implicit time\ndiscretization is proposed for the diffusion terms. The algorithm is then\napplied to an actual industrial scenario involving emissions of airborne\nparticulates from a zinc smelter using actual wind measurements. We also\naddress various practical considerations such as choosing appropriate methods\nfor regularizing noisy wind data and quantifying sensitivity of the model to\nparameter uncertainty. Afterwards, we use the algorithm within a Bayesian\nframework for estimating emission rates of zinc from multiple sources over the\nindustrial site. We compare our finite volume solver with a Gaussian plume\nsolver within the Bayesian framework and demonstrate that the finite volume\nsolver results in tighter uncertainty bounds on the estimated emission rates.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 21:07:45 GMT"}, {"version": "v2", "created": "Mon, 19 Sep 2016 16:29:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hosseini", "Bamdad", ""], ["Stockie", "John M.", ""]]}, {"id": "1607.03592", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Azam Moosavi, Adrian Sandu", "title": "Cluster Sampling Filters for Non-Gaussian Data Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fully non-Gaussian version of the Hamiltonian Monte\nCarlo (HMC) sampling filter. The Gaussian prior assumption in the original HMC\nfilter is relaxed. Specifically, a clustering step is introduced after the\nforecast phase of the filter, and the prior density function is estimated by\nfitting a Gaussian Mixture Model (GMM) to the prior ensemble. Using the data\nlikelihood function, the posterior density is then formulated as a mixture\ndensity, and is sampled using a HMC approach (or any other scheme capable of\nsampling multimodal densities in high-dimensional subspaces). The main filter\ndeveloped herein is named \"cluster HMC sampling filter\" (ClHMC). A multi-chain\nversion of the ClHMC filter, namely MC-ClHMC is also proposed to guarantee that\nsamples are taken from the vicinities of all probability modes of the\nformulated posterior. The new methodologies are tested using a\nquasi-geostrophic (QG) model with double-gyre wind forcing and bi-harmonic\nfriction. Numerical results demonstrate the usefulness of using GMMs to relax\nthe Gaussian prior assumption in the HMC filtering paradigm.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 04:37:31 GMT"}, {"version": "v2", "created": "Thu, 18 Aug 2016 15:14:18 GMT"}], "update_date": "2016-08-19", "authors_parsed": [["Attia", "Ahmed", ""], ["Moosavi", "Azam", ""], ["Sandu", "Adrian", ""]]}, {"id": "1607.03954", "submitter": "Charles Matthews", "authors": "Charles Matthews and Jonathan Weare and Benedict Leimkuhler", "title": "Ensemble preconditioning for Markov chain Monte Carlo simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe parallel Markov chain Monte Carlo methods that propagate a\ncollective ensemble of paths, with local covariance information calculated from\nneighboring replicas. The use of collective dynamics eliminates multiplicative\nnoise and stabilizes the dynamics thus providing a practical approach to\ndifficult anisotropic sampling problems in high dimensions. Numerical\nexperiments with model problems demonstrate that dramatic potential speedups,\ncompared to various alternative schemes, are attainable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 23:12:05 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Matthews", "Charles", ""], ["Weare", "Jonathan", ""], ["Leimkuhler", "Benedict", ""]]}, {"id": "1607.04247", "submitter": "Michael O'Neil", "authors": "Sebastian Ament and Michael O'Neil", "title": "Accurate and efficient numerical calculation of stable densities via\n  optimized quadrature and asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable distributions are an important class of infinitely-divisible\nprobability distributions, of which two special cases are the Cauchy\ndistribution and the normal distribution. Aside from a few special cases, the\ndensity function for stable distributions has no known analytic form, and is\nexpressible only through the variate's characteristic function or other\nintegral forms. In this paper we present numerical schemes for evaluating the\ndensity function for stable distributions, its gradient, and distribution\nfunction in various parameter regimes of interest, some of which had no\npre-existing efficient method for their computation. The novel evaluation\nschemes consist of optimized generalized Gaussian quadrature rules for integral\nrepresentations of the density function, complemented by various asymptotic\nexpansions near various values of the shape and argument parameters. We report\nseveral numerical examples illustrating the efficiency of our methods. The\nresulting code has been made available online.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 18:55:24 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2017 15:13:50 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Ament", "Sebastian", ""], ["O'Neil", "Michael", ""]]}, {"id": "1607.04532", "submitter": "Gregor Kastner", "authors": "Florian Huber, Gregor Kastner, Martin Feldkircher", "title": "Should I stay or should I go? A latent threshold approach to large-scale\n  mixture innovation models", "comments": null, "journal-ref": "Journal of Applied Econometrics 34(5), 621-640 (2019)", "doi": "10.1002/jae.2680", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:42:32 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 16:28:16 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 12:25:18 GMT"}, {"version": "v4", "created": "Fri, 12 Jan 2018 15:18:18 GMT"}, {"version": "v5", "created": "Thu, 26 Jul 2018 05:38:46 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Huber", "Florian", ""], ["Kastner", "Gregor", ""], ["Feldkircher", "Martin", ""]]}, {"id": "1607.04543", "submitter": "Roberto Molinari Mr", "authors": "James Balamuta, Roberto Molinari, St\\'ephane Guerrier, Wenchao Yang", "title": "The gmwm R package: a comprehensive tool for time series analysis from\n  state-space models to robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gmwm R package for inference on time series models is mainly based on the\nquantity called wavelet variance which is derived from a wavelet decomposition\nof a time series. This quantity provides a means to summarize and graphically\nrepresent the features of time series in order to identify possible models.\nMoreover, it is used as a moment condition for model estimation through the\ngeneralized method of wavelet moments. Based on the latter method, this package\nnot only provides an alternative method to estimate classical ARMA models but\nalso delivers a general framework for the robust estimation of many time series\nmodels as well as a quick and efficient estimation of many linear state-space\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 15:03:42 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Balamuta", "James", ""], ["Molinari", "Roberto", ""], ["Guerrier", "St\u00e9phane", ""], ["Yang", "Wenchao", ""]]}, {"id": "1607.04751", "submitter": "Mingyuan Zhou", "authors": "Yulai Cong, Bo Chen, Mingyuan Zhou", "title": "Fast Simulation of Hyperplane-Truncated Multivariate Normal\n  Distributions", "comments": "To appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a fast and easy-to-implement simulation algorithm for a\nmultivariate normal distribution truncated on the intersection of a set of\nhyperplanes, and further generalize it to efficiently simulate random variables\nfrom a multivariate normal distribution whose covariance (precision) matrix can\nbe decomposed as a positive-definite matrix minus (plus) a low-rank symmetric\nmatrix. Example results illustrate the correctness and efficiency of the\nproposed simulation algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 15:36:04 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 23:19:32 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Cong", "Yulai", ""], ["Chen", "Bo", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1607.05336", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi and Jose Bioucas-Dias and Nicolas Dobigeon and\n  Gerald S. Buller and Steve McLaughlin", "title": "Fast Hyperspectral Unmixing in Presence of Nonlinearity or Mismodelling\n  Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two novel hyperspectral mixture models and associated\nunmixing algorithms. The two models assume a linear mixing model corrupted by\nan additive term whose expression can be adapted to account for multiple\nscattering nonlinearities (NL), or mismodelling effects (ME). The NL model\ngeneralizes bilinear models by taking into account higher order interaction\nterms. The ME model accounts for different effects such as endmember\nvariability or the presence of outliers. The abundance and residual parameters\nof these models are estimated by considering a convex formulation suitable for\nfast estimation algorithms. This formulation accounts for constraints such as\nthe sum-to-one and non-negativity of the abundances, the non-negativity of the\nnonlinearity coefficients, the spectral smoothness of the ME terms and the\nspatial sparseness of the residuals. The resulting convex problem is solved\nusing the alternating direction method of multipliers (ADMM) whose convergence\nis ensured theoretically. The proposed mixture models and their unmixing\nalgorithms are validated on both synthetic and real images showing competitive\nresults regarding the quality of the inference and the computational complexity\nwhen compared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 21:39:00 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Bioucas-Dias", "Jose", ""], ["Dobigeon", "Nicolas", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1607.05758", "submitter": "Yohan Petetin", "authors": "Roland Lamberti, Yohan Petetin, Fran\\c{c}ois Desbouvries, Fran\\c{c}ois\n  Septier", "title": "Independent Resampling Sequential Monte Carlo Algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2726971", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo algorithms, or Particle Filters, are Bayesian\nfiltering algorithms which propagate in time a discrete and random\napproximation of the a posteriori distribution of interest. Such algorithms are\nbased on Importance Sampling with a bootstrap resampling step which aims at\nstruggling against weights degeneracy. However, in some situations (informative\nmeasurements, high dimensional model), the resampling step can prove\ninefficient. In this paper, we revisit the fundamental resampling mechanism\nwhich leads us back to Rubin's static resampling mechanism. We propose an\nalternative rejuvenation scheme in which the resampled particles share the same\nmarginal distribution as in the classical setup, but are now independent. This\nset of independent particles provides a new alternative to compute a moment of\nthe target distribution and the resulting estimate is analyzed through a CLT.\nWe next adapt our results to the dynamic case and propose a particle filtering\nalgorithm based on independent resampling. This algorithm can be seen as a\nparticular auxiliary particle filter algorithm with a relevant choice of the\nfirst-stage weights and instrumental distributions. Finally we validate our\nresults via simulations which carefully take into account the computational\nbudget.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 20:54:51 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Lamberti", "Roland", ""], ["Petetin", "Yohan", ""], ["Desbouvries", "Fran\u00e7ois", ""], ["Septier", "Fran\u00e7ois", ""]]}, {"id": "1607.05981", "submitter": "Francesco Innocenti", "authors": "Leonardo Grilli (1) and Francesco Innocenti ((1) Dipartimento di\n  Statistica, Informatica, Applicazioni \"G. Parenti\", Universit\\`a di Firenze)", "title": "Fitting logistic multilevel models with crossed random effects via\n  Bayesian Integrated Nested Laplace Approximations: a simulation study", "comments": "26 pages,6 figures, 24 tables including the Supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting cross-classified multilevel models with binary response is\nchallenging. In this setting a promising method is Bayesian inference through\nIntegrated Nested Laplace Approximations (INLA), which performs well in several\nlatent variable models. Therefore we devise a systematic simulation study to\nassess the performance of INLA with cross-classified logistic data under\ndifferent scenarios defined by the magnitude of the random effects variances,\nthe number of observations, the number of clusters, and the degree of\ncross-classification. In the simulations INLA is systematically compared with\nthe popular method of Maximum Likelihood via Laplace Approximation. By an\napplication to the classical salamander mating data, we compare INLA with the\nbest performing methods. Given the computational speed and the generally good\nperformance, INLA turns out to be a valuable method for fitting the considered\ncross-classified models.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:51:18 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Grilli", "Leonardo", ""], ["Innocenti", "Francesco", ""]]}, {"id": "1607.06779", "submitter": "Robert J. B. Goudie", "authors": "Robert J. B. Goudie, Anne M. Presanis, David Lunn, Daniela De Angelis\n  and Lorenz Wernisch", "title": "Joining and splitting models with Markov melding", "comments": null, "journal-ref": null, "doi": "10.1214/18-BA1104", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing multiple evidence sources is often feasible only via a modular\napproach, with separate submodels specified for smaller components of the\navailable evidence. Here we introduce a generic framework that enables fully\nBayesian analysis in this setting. We propose a generic method for forming a\nsuitable joint model when joining submodels, and a convenient computational\nalgorithm for fitting this joint model in stages, rather than as a single,\nmonolithic model. The approach also enables splitting of large joint models\ninto smaller submodels, allowing inference for the original joint model to be\nconducted via our multi-stage algorithm. We motivate and demonstrate our\napproach through two examples: joining components of an evidence synthesis of\nA/H1N1 influenza, and splitting a large ecology model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:12:30 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:40:12 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 20:46:30 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Goudie", "Robert J. B.", ""], ["Presanis", "Anne M.", ""], ["Lunn", "David", ""], ["De Angelis", "Daniela", ""], ["Wernisch", "Lorenz", ""]]}, {"id": "1607.06903", "submitter": "Christian P. Robert", "authors": "David T. Frazier (Monash University), Gael M. Martin (Monash\n  University), Christian P. Robert (Universit\\'e Paris-Dauphine PSL and\n  University of Warwick, UK), and Judith Rousseau (University of Oxford, UK)", "title": "Asymptotic Properties of Approximate Bayesian Computation", "comments": "This 31 pages paper is a revised version of the paper, including\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation allows for statistical analysis in models\nwith intractable likelihoods. In this paper we consider the asymptotic\nbehaviour of the posterior distribution obtained by this method. We give\ngeneral results on the rate at which the posterior distribution concentrates on\nsets containing the true parameter, its limiting shape, and the asymptotic\ndistribution of the posterior mean. These results hold under given rates for\nthe tolerance used within the method, mild regularity conditions on the summary\nstatistics, and a condition linked to identification of the true parameters.\nImplications for practitioners are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 09:05:00 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:37:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 09:18:42 GMT"}, {"version": "v4", "created": "Tue, 8 May 2018 15:22:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Frazier", "David T.", "", "Monash University"], ["Martin", "Gael M.", "", "Monash\n  University"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine PSL and\n  University of Warwick, UK"], ["Rousseau", "Judith", "", "University of Oxford, UK"]]}, {"id": "1607.08110", "submitter": "Daniel Malinsky", "authors": "Joseph D. Ramsey, Daniel Malinsky, Kevin V. Bui", "title": "algcomparison: Comparing the Performance of Graphical Structure Learning\n  Algorithms with TETRAD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report we describe a tool for comparing the performance of graphical\ncausal structure learning algorithms implemented in the TETRAD freeware suite\nof causal analysis methods. Currently the tool is available as package in the\nTETRAD source code (written in Java). Simulations can be done varying the\nnumber of runs, sample sizes, and data modalities. Performance on this\nsimulated data can then be compared for a number of algorithms, with parameters\nvaried and with performance statistics as selected, producing a publishable\nreport. The package presented here may also be used to compare structure\nlearning methods across platforms and programming languages, i.e., to compare\nalgorithms implemented in TETRAD with those implemented in MATLAB, Python, or\nR.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:22:30 GMT"}, {"version": "v2", "created": "Sat, 1 Oct 2016 21:18:48 GMT"}, {"version": "v3", "created": "Wed, 26 Oct 2016 16:38:09 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 14:53:00 GMT"}, {"version": "v5", "created": "Tue, 24 Oct 2017 07:45:42 GMT"}, {"version": "v6", "created": "Wed, 18 Sep 2019 02:28:03 GMT"}, {"version": "v7", "created": "Mon, 13 Jul 2020 15:40:15 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Ramsey", "Joseph D.", ""], ["Malinsky", "Daniel", ""], ["Bui", "Kevin V.", ""]]}, {"id": "1607.08458", "submitter": "Alexandre Gramfort", "authors": "Daniel Strohmeier, Yousra Bekhti, Jens Haueisen, Alexandre Gramfort", "title": "The iterative reweighted Mixed-Norm Estimate for spatio-temporal MEG/EEG\n  source reconstruction", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2016.2553445", "report-no": null, "categories": "stat.AP q-bio.NC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Source imaging based on magnetoencephalography (MEG) and\nelectroencephalography (EEG) allows for the non-invasive analysis of brain\nactivity with high temporal and good spatial resolution. As the\nbioelectromagnetic inverse problem is ill-posed, constraints are required. For\nthe analysis of evoked brain activity, spatial sparsity of the neuronal\nactivation is a common assumption. It is often taken into account using convex\nconstraints based on the l1-norm. The resulting source estimates are however\nbiased in amplitude and often suboptimal in terms of source selection due to\nhigh correlations in the forward model. In this work, we demonstrate that an\ninverse solver based on a block-separable penalty with a Frobenius norm per\nblock and a l0.5-quasinorm over blocks addresses both of these issues. For\nsolving the resulting non-convex optimization problem, we propose the iterative\nreweighted Mixed Norm Estimate (irMxNE), an optimization scheme based on\niterative reweighted convex surrogate optimization problems, which are solved\nefficiently using a block coordinate descent scheme and an active set strategy.\nWe compare the proposed sparse imaging method to the dSPM and the RAP-MUSIC\napproach based on two MEG data sets. We provide empirical evidence based on\nsimulations and analysis of MEG data that the proposed method improves on the\nstandard Mixed Norm Estimate (MxNE) in terms of amplitude bias, support\nrecovery, and stability.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:56:04 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Strohmeier", "Daniel", ""], ["Bekhti", "Yousra", ""], ["Haueisen", "Jens", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "1607.08799", "submitter": "Yunpeng Li", "authors": "Yunpeng Li and Mark Coates", "title": "Particle Filtering with Invertible Particle Flow", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Vol. 65, No. 15, pp.\n  4102-4116 (August 1, 2017)", "doi": "10.1109/TSP.2017.2703684", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge when designing particle filters in high-dimensional state\nspaces is the construction of a proposal distribution that is close to the\nposterior distribution. Recent advances in particle flow filters provide a\npromising avenue to avoid weight degeneracy; particles drawn from the prior\ndistribution are migrated in the state-space to the posterior distribution by\nsolving partial differential equations. Numerous particle flow filters have\nbeen proposed based on different assumptions concerning the flow dynamics.\nApproximations are needed in the implementation of all of these filters; as a\nresult the articles do not exactly match a sample drawn from the desired\nposterior distribution. Past efforts to correct the discrepancies involve\nexpensive calculations of importance weights. In this paper, we present new\nfilters which incorporate deterministic particle flows into an encompassing\nparticle filter framework. The valuable theoretical guarantees concerning\nparticle filter performance still apply, but we can exploit the attractive\nperformance of the particle flow methods. The filters we describe involve a\ncomputationally efficient weight update step, arising because the embedded\nparticle flows we design possess an invertible mapping property. We evaluate\nthe proposed particle flow particle filters' performance through numerical\nsimulations of a challenging multi-target multi-sensor tracking scenario and\ncomplex high-dimensional filtering examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:20:23 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 18:27:49 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 02:28:24 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 03:27:23 GMT"}, {"version": "v5", "created": "Wed, 28 Jun 2017 20:27:51 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Li", "Yunpeng", ""], ["Coates", "Mark", ""]]}, {"id": "1607.08845", "submitter": "Joris Bierkens", "authors": "Joris Bierkens and Andrew Duncan", "title": "Limit theorems for the Zig-Zag process", "comments": null, "journal-ref": "Advances in Applied Probability, 49(3), 2017", "doi": "10.1017/apr.2017.22", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo methods provide an essential tool in statistics for\nsampling from complex probability distributions. While the standard approach to\nMCMC involves constructing discrete-time reversible Markov chains whose\ntransition kernel is obtained via the Metropolis- Hastings algorithm, there has\nbeen recent interest in alternative schemes based on piecewise deterministic\nMarkov processes (PDMPs). One such approach is based on the Zig-Zag process,\nintroduced in Bierkens and Roberts (2016), which proved to provide a highly\nscalable sampling scheme for sampling in the big data regime (Bierkens,\nFearnhead and Roberts (2016)). In this paper we study the performance of the\nZig-Zag sampler, focusing on the one-dimensional case. In particular, we\nidentify conditions under which a Central limit theorem (CLT) holds and\ncharacterize the asymptotic variance. Moreover, we study the influence of the\nswitching rate on the diffusivity of the Zig-Zag process by identifying a\ndiffusion limit as the switching rate tends to infinity. Based on our results\nwe compare the performance of the Zig-Zag sampler to existing Monte Carlo\nmethods, both analytically and through simulations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 15:17:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 09:51:34 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bierkens", "Joris", ""], ["Duncan", "Andrew", ""]]}]