[{"id": "1701.00285", "submitter": "Julio Castrillon PhD", "authors": "Julio E. Castrillon-Candas", "title": "High dimensional multilevel Kriging: A computational mathematics\n  approach", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of massive data sets much of the computational science and\nengineering communities have been moving toward data-driven approaches such as\nregression and classification. However, they present a significant challenge\ndue to the increasing size, complexity and dimensionality of the problems. In\nthis paper a multilevel Kriging method that scales well with the number of\nobservations and dimensions is developed. A multilevel basis is constructed\nthat is adapted to a kD-tree partitioning of the observations. Numerically\nunstable covariance matrices with large condition numbers are transformed into\nwell conditioned multilevel matrices without compromising accuracy. Moreover,\nit is shown that the multilevel prediction $exactly$ solves the Best Linear\nUnbiased Predictor (BLUP), but is numerically stable. The multilevel method is\ntested on numerically unstable problems of up 25 dimensions. Numerical results\nshow speedups of up to 42,050 for solving the BLUP problem but to the same\naccuracy than the traditional iterative approach.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2017 20:11:06 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 13:29:04 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Castrillon-Candas", "Julio E.", ""]]}, {"id": "1701.00857", "submitter": "Ming Teng", "authors": "Ming Teng, Farouk S. Nathoo, Timothy D. Johnson", "title": "Bayesian Computation for Log-Gaussian Cox Processes--A Comparative\n  Analysis of Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Log-Gaussian Cox Process is a commonly used model for the analysis of\nspatial point patterns. Fitting this model is difficult because of its\ndoubly-stochastic property, i.e., it is an hierarchical combination of a\nPoisson process at the first level and a Gaussian Process at the second level.\nDifferent methods have been proposed to estimate such a process, including\ntraditional likelihood-based approaches as well as Bayesian methods. We focus\nhere on Bayesian methods and several approaches that have been considered for\nmodel fitting within this framework, including Hamiltonian Monte Carlo, the\nIntegrated nested Laplace approximation, and Variational Bayes. We consider\nthese approaches and make comparisons with respect to statistical and\ncomputational efficiency. These comparisons are made through several\nsimulations studies as well as through applications examining both ecological\ndata and neuroimaging data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 22:47:35 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Teng", "Ming", ""], ["Nathoo", "Farouk S.", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "1701.01672", "submitter": "Paul Fearnhead", "authors": "Robert Maidstone, Paul Fearnhead and Adam Letchford", "title": "Detecting changes in slope with an $L_0$ penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are many approaches to detecting changes in mean for a\nunivariate time-series, the problem of detecting multiple changes in slope has\ncomparatively been ignored. Part of the reason for this is that detecting\nchanges in slope is much more challenging. For example, simple binary\nsegmentation procedures do not work for this problem, whilst efficient dynamic\nprogramming methods that work well for the change in mean problem cannot be\ndirectly used for detecting changes in slope. We present a novel dynamic\nprogramming approach, CPOP, for finding the \"best\" continuous piecewise-linear\nfit to data. We define best based on a criterion that measures fit to data\nusing the residual sum of squares, but penalises complexity based on an $L_0$\npenalty on changes in slope. We show that using such a criterion is more\nreliable at estimating changepoint locations than approaches that penalise\ncomplexity using an $L_1$ penalty. Empirically CPOP has good computational\nproperties, and can analyse a time-series with over 10,000 observations and\nover 100 changes in a few minutes. Our method is used to analyse data on the\nmotion of bacteria, and provides fits to the data that both have substantially\nsmaller residual sum of squares and are more parsimonious than two competing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 15:52:45 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 10:26:34 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Maidstone", "Robert", ""], ["Fearnhead", "Paul", ""], ["Letchford", "Adam", ""]]}, {"id": "1701.02002", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Smoothing with Couplings of Conditional Particle Filters", "comments": "This document is a self-contained and direct description of the\n  smoothing method introduced in Coupling of Particle Filters\n  (arXiv:1606.01156). Code is available at github.com/pierrejacob/CoupledCPF.\n  Compared to the previous version, a bug was fixed in the code, and the\n  numerical results were updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state space models, smoothing refers to the task of estimating a latent\nstochastic process given noisy measurements related to the process. We propose\nan unbiased estimator of smoothing expectations. The lack-of-bias property has\nmethodological benefits: independent estimators can be generated in parallel,\nand confidence intervals can be constructed from the central limit theorem to\nquantify the approximation error. To design unbiased estimators, we combine a\ngeneric debiasing technique for Markov chains with a Markov chain Monte Carlo\nalgorithm for smoothing. The resulting procedure is widely applicable and we\nshow in numerical experiments that the removal of the bias comes at a\nmanageable increase in variance. We establish the validity of the proposed\nestimators under mild assumptions. Numerical experiments are provided on toy\nmodels, including a setting of highly-informative observations, and a realistic\nLotka-Volterra model with an intractable transition density.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 19:24:59 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 20:07:43 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 19:52:16 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1701.02201", "submitter": "Pavel S. Ruzankin", "authors": "Pavel S. Ruzankin", "title": "A fast algorithm for maximal propensity score matching", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability, 2019", "doi": "10.1007/s11009-019-09718-4", "report-no": null, "categories": "stat.CO cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm which detects the maximal possible number of\nmatched disjoint pairs satisfying a given caliper when a bipartite matching is\ndone with respect to a scalar index (e.g., propensity score), and constructs a\ncorresponding matching. Variable width calipers are compatible with the\ntechnique, provided that the width of the caliper is a Lipschitz function of\nthe index. If the observations are ordered with respect to the index then the\nmatching needs $O(N)$ operations, where $N$ is the total number of subjects to\nbe matched. The case of 1-to-$n$ matching is also considered.\n  We offer also a new fast algorithm for optimal complete one-to-one matching\non a scalar index when the treatment and control groups are of the same size.\nThis allows us to improve greedy nearest neighbor matching on a scalar index.\n  Keywords: propensity score matching, nearest neighbor matching, matching with\ncaliper, variable width caliper.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 14:55:12 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 06:06:25 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 16:37:51 GMT"}, {"version": "v4", "created": "Mon, 23 Oct 2017 04:33:17 GMT"}, {"version": "v5", "created": "Fri, 23 Nov 2018 13:32:34 GMT"}, {"version": "v6", "created": "Sun, 10 Feb 2019 15:58:43 GMT"}, {"version": "v7", "created": "Tue, 12 Mar 2019 16:30:15 GMT"}, {"version": "v8", "created": "Mon, 29 Apr 2019 12:24:24 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Ruzankin", "Pavel S.", ""]]}, {"id": "1701.02265", "submitter": "Xingye Qiao", "authors": "Chong Zhang, Wenbo Wang, and Xingye Qiao", "title": "On Reject and Refine Options in Multicategory Classification", "comments": "A revised version of this paper was accepted for publication in the\n  Journal of the American Statistical Association Theory and Methods Section.\n  52 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real applications of statistical learning, a decision made from\nmisclassification can be too costly to afford; in this case, a reject option,\nwhich defers the decision until further investigation is conducted, is often\npreferred. In recent years, there has been much development for binary\nclassification with a reject option. Yet, little progress has been made for the\nmulticategory case. In this article, we propose margin-based multicategory\nclassification methods with a reject option. In addition, and more importantly,\nwe introduce a new and unique refine option for the multicategory problem,\nwhere the class of an observation is predicted to be from a set of class\nlabels, whose cardinality is not necessarily one. The main advantage of both\noptions lies in their capacity of identifying error-prone observations.\nMoreover, the refine option can provide more constructive information for\nclassification by effectively ruling out implausible classes. Efficient\nimplementations have been developed for the proposed methods. On the\ntheoretical side, we offer a novel statistical learning theory and show a fast\nconvergence rate of the excess $\\ell$-risk of our methods with emphasis on\ndiverging dimensionality and number of classes. The results can be further\nimproved under a low noise assumption. A set of comprehensive simulation and\nreal data studies has shown the usefulness of the new learning tools compared\nto regular multicategory classifiers. Detailed proofs of theorems and extended\nnumerical results are included in the supplemental materials available online.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 17:19:45 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Zhang", "Chong", ""], ["Wang", "Wenbo", ""], ["Qiao", "Xingye", ""]]}, {"id": "1701.02349", "submitter": "Benjamin Brown", "authors": "Benjamin Brown, Timothy Weaver, Julian Wolfson", "title": "MEBoost: Variable Selection in the Presence of Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for variable selection in regression models when\ncovariates are measured with error. The iterative algorithm we propose,\nMEBoost, follows a path defined by estimating equations that correct for\ncovariate measurement error. Via simulation, we evaluated our method and\ncompare its performance to the recently-proposed Convex Conditioned Lasso\n(CoCoLasso) and to the \"naive\" Lasso which does not correct for measurement\nerror. Increasing the degree of measurement error increased prediction error\nand decreased the probability of accurate covariate selection, but this loss of\naccuracy was least pronounced when using MEBoost. We illustrate the use of\nMEBoost in practice by analyzing data from the Box Lunch Study, a clinical\ntrial in nutrition where several variables are based on self-report and hence\nmeasured with error.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 21:00:46 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 03:34:06 GMT"}, {"version": "v3", "created": "Wed, 25 Oct 2017 13:41:25 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Brown", "Benjamin", ""], ["Weaver", "Timothy", ""], ["Wolfson", "Julian", ""]]}, {"id": "1701.02522", "submitter": "Shev MacNamara", "authors": "Arieh Iserles and Shev MacNamara", "title": "Magnus expansions and pseudospectra of Master Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New directions in research on master equations are showcased by example.\nMagnus expansions, time-varying rates, and pseudospectra are highlighted. Exact\neigenvalues are found and contrasted with the large errors produced by standard\nnumerical methods in some cases. Isomerisation provides a running example and\nan illustrative application to chemical kinetics. We also give a brief example\nof the totally asymmetric exclusion process.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 11:33:41 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Iserles", "Arieh", ""], ["MacNamara", "Shev", ""]]}, {"id": "1701.02969", "submitter": "Tommaso Rigon", "authors": "Tommaso Rigon and Daniele Durante", "title": "Tractable Bayesian Density Regression via Logit Stick-Breaking Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in learning how the distribution of a response\nvariable changes with a set of predictors. Bayesian nonparametric dependent\nmixture models provide a flexible approach to address this goal. However,\nseveral formulations require computationally demanding algorithms for posterior\ninference. Motivated by this issue, we study a class of predictor-dependent\ninfinite mixture models, which relies on a simple representation of the\nstick-breaking prior via sequential logistic regressions. This formulation\nmaintains the same desirable properties of popular predictor-dependent\nstick-breaking priors, and leverages a recent P\\'olya-gamma data augmentation\nto facilitate the implementation of several computational methods for posterior\ninference. These routines include Markov chain Monte Carlo via Gibbs sampling,\nexpectation-maximization algorithms, and mean-field variational Bayes for\nscalable inference, thereby stimulating a wider implementation of Bayesian\ndensity regression by practitioners. The algorithms associated with these\nmethods are presented in detail and tested in a toxicology study.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 13:44:01 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 11:26:15 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 10:49:08 GMT"}, {"version": "v4", "created": "Thu, 11 Jan 2018 18:45:56 GMT"}, {"version": "v5", "created": "Mon, 4 May 2020 19:20:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Rigon", "Tommaso", ""], ["Durante", "Daniele", ""]]}, {"id": "1701.03095", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis and Magnus Rattray", "title": "Bayesian estimation of Differential Transcript Usage from RNA-seq data", "comments": "Revised version, accepted to Statistical Applications in Genetics and\n  Molecular Biology", "journal-ref": null, "doi": "10.1515/sagmb-2017-0005", "report-no": null, "categories": "q-bio.GN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next generation sequencing allows the identification of genes consisting of\ndifferentially expressed transcripts, a term which usually refers to changes in\nthe overall expression level. A specific type of differential expression is\ndifferential transcript usage (DTU) and targets changes in the relative within\ngene expression of a transcript. The contribution of this paper is to: (a)\nextend the use of cjBitSeq to the DTU context, a previously introduced Bayesian\nmodel which is originally designed for identifying changes in overall\nexpression levels and (b) propose a Bayesian version of DRIMSeq, a frequentist\nmodel for inferring DTU. cjBitSeq is a read based model and performs fully\nBayesian inference by MCMC sampling on the space of latent state of each\ntranscript per gene. BayesDRIMSeq is a count based model and estimates the\nBayes Factor of a DTU model against a null model using Laplace's approximation.\nThe proposed models are benchmarked against the existing ones using a recent\nindependent simulation study as well as a real RNA-seq dataset. Our results\nsuggest that the Bayesian methods exhibit similar performance with DRIMSeq in\nterms of precision/recall but offer better calibration of False Discovery Rate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 18:51:54 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 14:05:32 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Papastamoulis", "Panagiotis", ""], ["Rattray", "Magnus", ""]]}, {"id": "1701.03267", "submitter": "Joaquin Ortega", "authors": "Diego Rivera-Garc\\'ia, Luis Angel Garc\\'ia-Escudero, Agust\\'in\n  Mayo-Iscar, Joaqu{\\i}n Ortega", "title": "Robust clustering for functional data based on trimming and constraints", "comments": "19 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering algorithms when the data are curves or functions have been\nrecently proposed. However, the presence of contamination in the sample of\ncurves can influence the performance of most of them. In this work we propose a\nrobust, model-based clustering method based on an approximation to the \"density\nfunction\" for functional data. The robustness results from the joint\napplication of trimming, for reducing the effect of contaminated observations,\nand constraints on the variances, for avoiding spurious clusters in the\nsolution. The proposed method has been evaluated through a simulation study.\nFinally, an application to a real data problem is given.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 08:44:14 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Rivera-Garc\u00eda", "Diego", ""], ["Garc\u00eda-Escudero", "Luis Angel", ""], ["Mayo-Iscar", "Agust\u00edn", ""], ["Ortega", "Joaqu\u0131n", ""]]}, {"id": "1701.03405", "submitter": "Alexander Gribov", "authors": "Alexander Gribov, Konstantin Krivoruchko", "title": "New Flexible Compact Covariance Model on a Sphere", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how the kernel convolution approach can be used to accurately\napproximate the spatial covariance model on a sphere using spherical distances\nbetween points. A detailed derivation of the required formulas is provided. The\nproposed covariance model approximation can be used for non-stationary spatial\nprediction and simulation in the case when the dataset is large and the\ncovariance model can be estimated separately in the data subsets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 16:56:29 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Gribov", "Alexander", ""], ["Krivoruchko", "Konstantin", ""]]}, {"id": "1701.03512", "submitter": "Sai Kumar Popuri", "authors": "Sai K. Popuri and Andrew M. Raim and Nagaraj K. Neerchal and Matthias\n  K. Gobbert", "title": "Parallelizing Computation of Expected Values in Recombinant Binomial\n  Trees", "comments": "19 pages and 5 figures (png/jpeg files)", "journal-ref": "J. Stat. Comp. & Sim. 88 (2018) 657-674", "doi": "10.1080/00949655.2017.1402898", "report-no": null, "categories": "stat.CO q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recombinant binomial trees are binary trees where each non-leaf node has two\nchild nodes, but adjacent parents share a common child node. Such trees arise\nin finance when pricing an option. For example, valuation of a European option\ncan be carried out by evaluating the expected value of asset payoffs with\nrespect to random paths in the tree. In many variants of the option valuation\nproblem, a closed form solution cannot be obtained and computational methods\nare needed. The cost to exactly compute expected values over random paths grows\nexponentially in the depth of the tree, rendering a serial computation of one\nbranch at a time impractical. We propose a parallelization method that\ntransforms the calculation of the expected value into an \"embarrassingly\nparallel\" problem by mapping the branches of the binomial tree to the processes\nin a multiprocessor computing environment. We also propose a parallel Monte\nCarlo method which takes advantage of the mapping to achieve a reduced variance\nover the basic Monte Carlo estimator. Performance results from R and Julia\nimplementations of the parallelization method on a distributed computing\ncluster indicate that both the implementations are scalable, but Julia is\nsignificantly faster than a similarly written R code. A simulation study is\ncarried out to verify the convergence and the variance reduction behavior in\nthe proposed Monte Carlo method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 21:34:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 07:07:46 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Popuri", "Sai K.", ""], ["Raim", "Andrew M.", ""], ["Neerchal", "Nagaraj K.", ""], ["Gobbert", "Matthias K.", ""]]}, {"id": "1701.03675", "submitter": "Agnieszka Krol", "authors": "Agnieszka Kr\\'ol, Audrey Mauguen, Yassin Mazroui, Alexandre Laurent,\n  Stefan Michiels, Virginie Rondeau", "title": "Tutorial in Joint Modeling and Prediction: a Statistical Software for\n  Correlated Longitudinal Outcomes, Recurrent Events and a Terminal Event", "comments": "Journal of Statistical Software (conditionally accepted for\n  publication)", "journal-ref": null, "doi": "10.18637/jss.v081.i03", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensions in the field of joint modeling of correlated data and dynamic\npredictions improve the development of prognosis research. The R package\nfrailtypack provides estimations of various joint models for longitudinal data\nand survival events. In particular, it fits models for recurrent events and a\nterminal event (frailtyPenal), models for two survival outcomes for clustered\ndata (frailtyPenal), models for two types of recurrent events and a terminal\nevent (multivPenal), models for a longitudinal biomarker and a terminal event\n(longiPenal) and models for a longitudinal biomarker, recurrent events and a\nterminal event (trivPenal). The estimators are obtained using a standard and\npenalized maximum likelihood approach, each model function allows to evaluate\ngoodness-of-fit analyses and plots of baseline hazard functions. Finally, the\npackage provides individual dynamic predictions of the terminal event and\nevaluation of predictive accuracy. This paper presents theoretical models with\nestimation techniques, applies the methods for predictions and illustrates\nfrailtypack functions details with examples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 14:05:41 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Kr\u00f3l", "Agnieszka", ""], ["Mauguen", "Audrey", ""], ["Mazroui", "Yassin", ""], ["Laurent", "Alexandre", ""], ["Michiels", "Stefan", ""], ["Rondeau", "Virginie", ""]]}, {"id": "1701.03757", "submitter": "Dustin Tran", "authors": "Dustin Tran, Matthew D. Hoffman, Rif A. Saurous, Eugene Brevdo, Kevin\n  Murphy, David M. Blei", "title": "Deep Probabilistic Programming", "comments": "Appears in International Conference on Learning Representations,\n  2017. A companion webpage for this paper is available at\n  http://edwardlib.org/iclr2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.PL stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Edward, a Turing-complete probabilistic programming language.\nEdward defines two compositional representations---random variables and\ninference. By treating inference as a first class citizen, on a par with\nmodeling, we show that probabilistic programming can be as flexible and\ncomputationally efficient as traditional deep learning. For flexibility, Edward\nmakes it easy to fit the same model using a variety of composable inference\nmethods, ranging from point estimation to variational inference to MCMC. In\naddition, Edward can reuse the modeling representation as part of inference,\nfacilitating the design of rich variational models and generative adversarial\nnetworks. For efficiency, Edward is integrated into TensorFlow, providing\nsignificant speedups over existing probabilistic systems. For example, we show\non a benchmark logistic regression task that Edward is at least 35x faster than\nStan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it\nis as fast as handwritten TensorFlow.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 17:52:07 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 18:41:45 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Tran", "Dustin", ""], ["Hoffman", "Matthew D.", ""], ["Saurous", "Rif A.", ""], ["Brevdo", "Eugene", ""], ["Murphy", "Kevin", ""], ["Blei", "David M.", ""]]}, {"id": "1701.03861", "submitter": "Jack Davis", "authors": "Jack Davis and Steven K. Thompson", "title": "Network Inference from a Link-Traced Sample using Approximate Bayesian\n  Computation", "comments": "35 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new inference method based on approximate Bayesian computation\nfor estimating parameters governing an entire network based on link-traced\nsamples of that network. To do this, we first take summary statistics from an\nobserved link-traced network sample, such as a recruitment network of subjects\nin a hard-to-reach population. Then we assume prior distributions, such as\nmultivariate uniform, for the distribution of some parameters governing the\nstructure of the network and behaviour of its nodes. Then, we draw many\nindependent and identically distributed values for these parameters. For each\nset of values, we simulate a population network, take a link-traced sample from\nthat network, and find the summary statistics for that sample. The statistics\nfrom the sample, and the parameters that eventually led to that sample, are\ncollectively treated as a single point. We take a Kernel Density estimate of\nthe points from many simulations, and observe the density across the hyperplane\ncoinciding with the statistic values of the originally observed sample. This\ndensity function is treat as a posterior estimate of the paramaters of the\nnetwork that provided the observed sample.\n  We also apply this method to a network of precedence citations between legal\ndocuments, centered around cases overseen by the Supreme Court of Canada, is\nobserved. The features of certain cases that lead to their frequent citation\nare inferred, and their effects estimated by ABC. Future work and extensions\nare also briefly discussed.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 01:15:23 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Davis", "Jack", ""], ["Thompson", "Steven K.", ""]]}, {"id": "1701.04244", "submitter": "Andrew Bruce Duncan", "authors": "Joris Bierkens, Alexandre Bouchard-C\\^ot\\'e, Arnaud Doucet, Andrew B.\n  Duncan, Paul Fearnhead, Thibaut Lienart, Gareth Roberts, Sebastian J. Vollmer", "title": "Piecewise Deterministic Markov Processes for Scalable Monte Carlo on\n  Restricted Domains", "comments": null, "journal-ref": "Statistics & Probability Letters Volume 136, May 2018, Pages\n  148-154", "doi": "10.1016/j.spl.2018.02.021", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise Deterministic Monte Carlo algorithms enable simulation from a\nposterior distribution, whilst only needing to access a sub-sample of data at\neach iteration. We show how they can be implemented in settings where the\nparameters live on a restricted domain.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:18:51 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 11:59:26 GMT"}, {"version": "v3", "created": "Sat, 17 Feb 2018 10:31:07 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bierkens", "Joris", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Doucet", "Arnaud", ""], ["Duncan", "Andrew B.", ""], ["Fearnhead", "Paul", ""], ["Lienart", "Thibaut", ""], ["Roberts", "Gareth", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "1701.04247", "submitter": "Andrew Bruce Duncan", "authors": "A. B. Duncan, G. A. Pavliotis, K. C. Zygalakis", "title": "Nonreversible Langevin Samplers: Splitting Schemes, Analysis and\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given target density, there exist an infinite number of diffusion\nprocesses which are ergodic with respect to this density. As observed in a\nnumber of papers, samplers based on nonreversible diffusion processes can\nsignificantly outperform their reversible counterparts both in terms of\nasymptotic variance and rate of convergence to equilibrium. In this paper, we\ntake advantage of this in order to construct efficient sampling algorithms\nbased on the Lie-Trotter decomposition of a nonreversible diffusion process\ninto reversible and nonreversible components. We show that samplers based on\nthis scheme can significantly outperform standard MCMC methods, at the cost of\nintroducing some controlled bias. In particular, we prove that numerical\nintegrators constructed according to this decomposition are geometrically\nergodic and characterise fully their asymptotic bias and variance, showing that\nthe sampler inherits the good mixing properties of the underlying nonreversible\ndiffusion. This is illustrated further with a number of numerical examples\nranging from highly correlated low dimensional distributions, to logistic\nregression problems in high dimensions as well as inference for spatial models\nwith many latent variables.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:28:50 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Duncan", "A. B.", ""], ["Pavliotis", "G. A.", ""], ["Zygalakis", "K. C.", ""]]}, {"id": "1701.04781", "submitter": "Michel Lang", "authors": "Michel Lang", "title": "checkmate: Fast Argument Checks for Defensive R Programming", "comments": null, "journal-ref": "R.Journal 9:1 (2017) 437-445", "doi": "10.32614/RJ-2017-028", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically typed programming languages like R allow programmers to write\ngeneric, flexible and concise code and to interact with the language using an\ninteractive Read-eval-print-loop (REPL). However, this flexibility has its\nprice: As the R interpreter has no information about the expected variable\ntype, many base functions automatically convert the input instead of raising an\nexception. Unfortunately, this frequently leads to runtime errors deeper down\nthe call stack which obfuscates the original problem and renders debugging\nchallenging. Even worse, unwanted conversions can remain undetected and skew or\ninvalidate the results of a statistical analysis. As a resort, assertions can\nbe employed to detect unexpected input during runtime and to signal\nunderstandable and traceable errors.\n  The package \"checkmate\" provides a plethora of functions to check the type\nand related properties of the most frequently used R objects and variable\ntypes. The package is mostly written in C to avoid any unnecessary performance\noverhead. Thus, the programmer can conveniently write concise, well-tested\nassertions which outperforms custom R code for many applications. Furthermore,\ncheckmate simplifies writing unit tests using the framework \"testthat\" by\nextending it with plenty of additional expectation functions, and registered C\nroutines are available for package developers to perform assertions on\narbitrary SEXPs (internal data structure for R objects implemented as struct in\nC) in compiled code.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 09:18:21 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Lang", "Michel", ""]]}, {"id": "1701.04858", "submitter": "Joseph Roy", "authors": "Christopher Eager and Joseph Roy", "title": "Mixed Effects Models are Sometimes Terrible", "comments": "Write up for poster presented at Linguistic Society of America 2017:\n  Eager, Christopher and Joseph Roy. Mixed Effects are Sometimes Terrible.\n  Linguistic Society of America, Poster (January 5-8, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-effects models have emerged as the gold standard of statistical\nanalysis in different sub-fields of linguistics (Baayen, Davidson & Bates,\n2008; Johnson, 2009; Barr, et al, 2013; Gries, 2015). One problematic feature\nof these models is their failure to converge under maximal (or even\nnear-maximal) random effects structures. The lack of convergence is relatively\nunaddressed in linguistics and when it is addressed has resulted in statistical\npractices (e.g. Jaeger, 2009; Gries, 2015; Bates, et al, 2015b) that are\npremised on the idea that non-convergence is an indication that a random\neffects structure is over-specified (or not parsimonious), the parsimonious\nconvergence hypothesis (PCH). We test the PCH by running simulations in lme4\nunder two sets of assumptions for both a linear dependent variable and a binary\ndependent variable in order to assess the rate of non-convergence for both\ntypes of mixed effects models when a known maximal effect structure is used to\ngenerate the data (i.e. when non-convergence cannot be explained by random\neffects with zero variance). Under the PCH, lack of convergence is treated as\nevidence against a more maximal random effects structure, but that result is\nnot upheld with our simulations. We provide an alternative model, fully\nspecified Bayesian models implemented in rstan (Stan Development Team, 2016;\nCarpenter, et al, in press) that removed the convergence problems almost\nentirely in simulations of the same conditions. These results indicate that\nwhen there is known non-zero variance for all slopes and intercepts, under\nrealistic distributions of data and with moderate to severe imbalance, mixed\neffects models in lme4 have moderate to high non-convergence rates which can\ncause linguistic researchers to wrongfully exclude random effect terms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 19:10:22 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Eager", "Christopher", ""], ["Roy", "Joseph", ""]]}, {"id": "1701.05128", "submitter": "Xiliang Lu", "authors": "Jian Huang, Yuling Jiao, Yanyan Liu and Xiliang Lu", "title": "A Constructive Approach to High-dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a constructive approach to estimating sparse, high-dimensional\nlinear regression models. The approach is a computational algorithm motivated\nfrom the KKT conditions for the $\\ell_0$-penalized least squares solutions. It\ngenerates a sequence of solutions iteratively, based on support detection using\nprimal and dual information and root finding. We refer to the algorithm as SDAR\nfor brevity. Under a sparse Rieze condition on the design matrix and certain\nother conditions, we show that with high probability, the $\\ell_2$ estimation\nerror of the solution sequence decays exponentially to the minimax error bound\nin $O(\\sqrt{J}\\log(R))$ steps; and under a mutual coherence condition and\ncertain other conditions, the $\\ell_{\\infty}$ estimation error decays to the\noptimal error bound in $O(\\log(R))$ steps, where $J$ is the number of important\npredictors, $R$ is the relative magnitude of the nonzero target coefficients.\nComputational complexity analysis shows that the cost of SDAR is $O(np)$ per\niteration. Moreover the oracle least squares estimator can be exactly recovered\nwith high probability at the same cost if we know the sparsity level. We also\nconsider an adaptive version of SDAR to make it more practical in applications.\nNumerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR\nis competitive with or outperforms them in accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:13:24 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Liu", "Yanyan", ""], ["Lu", "Xiliang", ""]]}, {"id": "1701.05146", "submitter": "Espen Bernton", "authors": "Espen Bernton (Harvard University), Pierre E. Jacob (Harvard\n  University), Mathieu Gerber (University of Bristol), Christian P. Robert\n  (Universit\\'e Paris-Dauphine, PSL and University of Warwick)", "title": "On parameter estimation with the Wasserstein distance", "comments": "29 pages (+18 pages of appendices), 6 figures. To appear in\n  Information and Inference: A Journal of the IMA. A previous version of this\n  paper contained work on approximate Bayesian computation with the Wasserstein\n  distance, which can now be found at arxiv:1905.03747", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference can be performed by minimizing, over the parameter\nspace, the Wasserstein distance between model distributions and the empirical\ndistribution of the data. We study asymptotic properties of such minimum\nWasserstein distance estimators, complementing results derived by Bassetti,\nBodini and Regazzini in 2006. In particular, our results cover the misspecified\nsetting, in which the data-generating process is not assumed to be part of the\nfamily of distributions described by the model. Our results are motivated by\nrecent applications of minimum Wasserstein estimators to complex generative\nmodels. We discuss some difficulties arising in the approximation of these\nestimators and illustrate their behavior in several numerical experiments. Two\nof our examples are taken from the literature on approximate Bayesian\ncomputation and have likelihood functions that are not analytically tractable.\nTwo other examples involve misspecified models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:59:55 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 22:47:12 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 02:03:12 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bernton", "Espen", "", "Harvard University"], ["Jacob", "Pierre E.", "", "Harvard\n  University"], ["Gerber", "Mathieu", "", "University of Bristol"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, PSL and University of Warwick"]]}, {"id": "1701.05358", "submitter": "Saeid Rezakhah", "authors": "Ferdous Mohammadi and Saeid Rezakhah", "title": "Smooth Transition HYGARCH Model: Stability and Forecasting", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HYGARCH process is the commonly used long memory process in modeling the\nlong-rang dependence in volatility.\n  Financial time series are characterized by transition between phases of\ndifferent volatility levels. The smooth transition HYGARCH (ST-HYGARCH) model\nis proposed to model time-varying structure with long memory property. The\nasymptotic behavior of the second moment is studied and an upper bound for it\nis derived. A score test is developed to check the smooth transition property.\nThe asymptotic behavior of the proposed model and the score test is examined by\nsimulation. The proposed model is applied to the \\textit{S}\\&\\textit{P}500\nindices for some period which show evidence of smooth transition property and\ndemonstrates out-performance of the ST-HYGARCH than HYGARCH in forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:26:48 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Mohammadi", "Ferdous", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1701.05512", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Fisher consistency for prior probability shift", "comments": "28 pages, 2 figures, 8 tables, introduction extended", "journal-ref": "Journal of Machine Learning Research 18, 3338-3369, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Fisher consistency in the sense of unbiasedness as a desirable\nproperty for estimators of class prior probabilities. Lack of Fisher\nconsistency could be used as a criterion to dismiss estimators that are\nunlikely to deliver precise estimates in test datasets under prior probability\nand more general dataset shift. The usefulness of this unbiasedness concept is\ndemonstrated with three examples of classifiers used for quantification:\nAdjusted Classify & Count, EM-algorithm and CDE-Iterate. We find that Adjusted\nClassify & Count and EM-algorithm are Fisher consistent. A counter-example\nshows that CDE-Iterate is not Fisher consistent and, therefore, cannot be\ntrusted to deliver reliable estimates of class probabilities.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:07:21 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 20:04:39 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1701.05609", "submitter": "Majnu John", "authors": "Majnu John, Yihren Wu", "title": "Confidence Intervals for Finite Difference Solutions", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2017.1335409", "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although applications of Bayesian analysis for numerical quadrature problems\nhave been considered before, it's only very recently that statisticians have\nfocused on the connections between statistics and numerical analysis of\ndifferential equations. In line with this very recent trend, we show how\ncertain commonly used finite difference schemes for numerical solutions of\nordinary and partial differential equations can be considered in a regression\nsetting. Focusing on this regression framework, we apply a simple Bayesian\nstrategy to obtain confidence intervals for the finite difference solutions. We\napply this framework on several examples to show how the confidence intervals\nare related to truncation error and illustrate the utility of the confidence\nintervals for the examples considered.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 21:31:39 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["John", "Majnu", ""], ["Wu", "Yihren", ""]]}, {"id": "1701.05892", "submitter": "Kody Law", "authors": "Ajay Jasra, Kengo Kamatani, Kody J. H. Law, and Yan Zhou", "title": "Bayesian Static Parameter Estimation for Partially Observed Diffusions\n  via Multilevel Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider static Bayesian parameter estimation for\npartially observed diffusions that are discretely observed. We work under the\nassumption that one must resort to discretizing the underlying diffusion\nprocess, for instance using the Euler-Maruyama method. Given this assumption,\nwe show how one can use Markov chain Monte Carlo (MCMC) and particularly\nparticle MCMC [Andrieu, C., Doucet, A. and Holenstein, R. (2010). Particle\nMarkov chain Monte Carlo methods (with discussion). J. R. Statist. Soc. Ser. B,\n72, 269--342] to implement a new approximation of the multilevel (ML) Monte\nCarlo (MC) collapsing sum identity. Our approach comprises constructing an\napproximate coupling of the posterior density of the joint distribution over\nparameter and hidden variables at two different discretization levels and then\ncorrecting by an importance sampling method. The variance of the weights are\nindependent of the length of the observed data set. The utility of such a\nmethod is that, for a prescribed level of mean square error, the cost of this\nMLMC method is provably less than i.i.d. sampling from the posterior associated\nto the most precise discretization. However the method here comprises using\nonly known and efficient simulation methodologies. The theoretical results are\nillustrated by inference of the parameters of two prototypical processes given\nnoisy partial observations of the process: the first is an Ornstein Uhlenbeck\nprocess and the second is a more general Langevin equation.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 18:50:50 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Jasra", "Ajay", ""], ["Kamatani", "Kengo", ""], ["Law", "Kody J. H.", ""], ["Zhou", "Yan", ""]]}, {"id": "1701.05936", "submitter": "Yaohui Zeng", "authors": "Yaohui Zeng, Patrick Breheny", "title": "The biglasso Package: A Memory- and Computation-Efficient Solver for\n  Lasso Model Fitting with Big Data in R", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression models such as the lasso have been extensively applied\nto analyzing high-dimensional data sets. However, due to memory limitations,\nexisting R packages like glmnet and ncvreg are not capable of fitting\nlasso-type models for ultrahigh-dimensional, multi-gigabyte data sets that are\nincreasingly seen in many areas such as genetics, genomics, biomedical imaging,\nand high-frequency finance. In this research, we implement an R package called\nbiglasso that tackles this challenge. biglasso utilizes memory-mapped files to\nstore the massive data on the disk, only reading data into memory when\nnecessary during model fitting, and is thus able to handle out-of-core\ncomputation seamlessly. Moreover, it's equipped with newly proposed, more\nefficient feature screening rules, which substantially accelerate the\ncomputation. Benchmarking experiments show that our biglasso package, as\ncompared to existing popular ones like glmnet, is much more memory- and\ncomputation-efficient. We further analyze a 31 GB real data set on a laptop\nwith only 16 GB RAM to demonstrate the out-of-core computation capability of\nbiglasso in analyzing massive data sets that cannot be accommodated by existing\nR packages.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 22:08:49 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 23:29:42 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Zeng", "Yaohui", ""], ["Breheny", "Patrick", ""]]}, {"id": "1701.06619", "submitter": "Jaewoo Park", "authors": "Jaewoo Park and Murali Haran", "title": "Bayesian Inference in the Presence of Intractable Normalizing Functions", "comments": "main paper (40 pages), supplementary (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models with intractable normalizing functions arise frequently in statistics.\nCommon examples of such models include exponential random graph models for\nsocial networks and Markov point processes for ecology and disease modeling.\nInference for these models is complicated because the normalizing functions of\ntheir probability distributions include the parameters of interest. In Bayesian\nanalysis they result in so-called doubly intractable posterior distributions\nwhich pose significant computational challenges. Several Monte Carlo methods\nhave emerged in recent years to address Bayesian inference for such models. We\nprovide a framework for understanding the algorithms and elucidate connections\namong them. Through multiple simulated and real data examples, we compare and\ncontrast the computational and statistical efficiency of these algorithms and\ndiscuss their theoretical bases. Our study provides practical recommendations\nfor practitioners along with directions for future research for MCMC\nmethodologists.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 20:26:42 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:31:51 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Park", "Jaewoo", ""], ["Haran", "Murali", ""]]}, {"id": "1701.07496", "submitter": "Max Tolkoff", "authors": "Max R. Tolkoff, Michael L. Alfaro, Guy Baele, Philippe Lemey, and Marc\n  A. Suchard", "title": "Phylogenetic Factor Analysis", "comments": "51 pages (42 main, 9 supplemental), 9 figures (5 main, 4\n  supplemental), 4 tables (2 main, 2 supplemental), submitted to Systematic\n  Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic comparative methods explore the relationships between\nquantitative traits adjusting for shared evolutionary history. This adjustment\noften occurs through a Brownian diffusion process along the branches of the\nphylogeny that generates model residuals or the traits themselves. For\nhigh-dimensional traits, inferring all pair-wise correlations within the\nmultivariate diffusion is limiting. To circumvent this problem, we propose\nphylogenetic factor analysis (PFA) that assumes a small unknown number of\nindependent evolutionary factors arise along the phylogeny and these factors\ngenerate clusters of dependent traits. Set in a Bayesian framework, PFA\nprovides measures of uncertainty on the factor number and groupings, combines\nboth continuous and discrete traits, integrates over missing measurements and\nincorporates phylogenetic uncertainty with the help of molecular sequences. We\ndevelop Gibbs samplers based on dynamic programming to estimate the PFA\nposterior distribution, over three-fold faster than for multivariate diffusion\nand a further order-of-magnitude more efficiently in the presence of latent\ntraits. We further propose a novel marginal likelihood estimator for previously\nimpractical models with discrete data and find that PFA also provides a better\nfit than multivariate diffusion in evolutionary questions in columbine flower\ndevelopment, placental reproduction transitions and triggerfish fin\nmorphometry.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:43:03 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Tolkoff", "Max R.", ""], ["Alfaro", "Michael L.", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1701.07787", "submitter": "Jere Koskela", "authors": "Jere Koskela", "title": "Multi-locus data distinguishes between population growth and multiple\n  merger coalescents", "comments": "24 pages, 13 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology\n  17(3):20170011, 2018", "doi": "10.1515/sagmb-2017-0011", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a low dimensional function of the site frequency spectrum that\nis tailor-made for distinguishing coalescent models with multiple mergers from\nKingman coalescent models with population growth, and use this function to\nconstruct a hypothesis test between these model classes. The null and\nalternative sampling distributions of the statistic are intractable, but its\nlow dimensionality renders them amenable to Monte Carlo estimation. We\nconstruct kernel density estimates of the sampling distributions based on\nsimulated data, and show that the resulting hypothesis test dramatically\nimproves on the statistical power of a current state-of-the-art method. A key\nreason for this improvement is the use of multi-locus data, in particular\naveraging observed site frequency spectra across unlinked loci to reduce\nsampling variance. We also demonstrate the robustness of our method to nuisance\nand tuning parameters. Finally we show that the same kernel density estimates\ncan be used to conduct parameter estimation, and argue that our method is\nreadily generalisable for applications in model selection, parameter inference\nand experimental design.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:40:31 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 17:18:01 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 12:18:42 GMT"}, {"version": "v4", "created": "Tue, 5 Sep 2017 14:18:29 GMT"}, {"version": "v5", "created": "Fri, 23 Mar 2018 08:48:37 GMT"}, {"version": "v6", "created": "Thu, 19 Apr 2018 15:17:51 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""]]}, {"id": "1701.07844", "submitter": "Virgilio Gomez-Rubio", "authors": "Virgilio G\\'omez-Rubio and H{\\aa}vard Rue", "title": "Markov Chain Monte Carlo with the Integrated Nested Laplace\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Integrated Nested Laplace Approximation (INLA) has established itself as\na widely used method for approximate inference on Bayesian hierarchical models\nwhich can be represented as a latent Gaussian model (LGM). INLA is based on\nproducing an accurate approximation to the posterior marginal distributions of\nthe parameters in the model and some other quantities of interest by using\nrepeated approximations to intermediate distributions and integrals that appear\nin the computation of the posterior marginals.\n  INLA focuses on models whose latent effects are a Gaussian Markov random\nfield (GMRF). For this reason, we have explored alternative ways of expanding\nthe number of possible models that can be fitted using the INLA methodology. In\nthis paper, we present a novel approach that combines INLA and Markov chain\nMonte Carlo (MCMC). The aim is to consider a wider range of models that cannot\nbe fitted with INLA unless some of the parameters of the model have been fixed.\nHence, conditioning on these parameters the model could be fitted with the\nR-INLA package. We show how new values of these parameters can be drawn from\ntheir posterior by using conditional models fitted with INLA and standard MCMC\nalgorithms, such as Metropolis-Hastings. Hence, this will extend the use of\nINLA to fit models that can be expressed as a conditional LGM. Also, this new\napproach can be used to build simpler MCMC samplers for complex models as it\nallows sampling only on a limited number parameters in the model.\n  We will demonstrate how our approach can extend the class of models that\ncould benefit from INLA, and how the R-INLA package will ease its\nimplementation. We will go through simple examples of this new approach before\nwe discuss more advanced problems with datasets taken from relevant literature.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 19:07:53 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 11:49:09 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["G\u00f3mez-Rubio", "Virgilio", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1701.08142", "submitter": "Clara Grazian", "authors": "Clara Grazian, Fabrizio Leisen, Brunero Liseo", "title": "Modelling Preference Data with the Wallenius Distribution", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wallenius distribution is a generalisation of the Hypergeometric\ndistribution where weights are assigned to balls of different colours. This\nnaturally defines a model for ranking categories which can be used for\nclassification purposes. Since, in general, the resulting likelihood is not\nanalytically available, we adopt an approximate Bayesian computational (ABC)\napproach for estimating the importance of the categories. We illustrate the\nperformance of the estimation procedure on simulated datasets. Finally, we use\nthe new model for analysing two datasets about movies ratings and Italian\nacademic statisticians' journal preferences. The latter is a novel dataset\ncollected by the authors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:30:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:45:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:54:59 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 12:03:47 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 14:46:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Grazian", "Clara", ""], ["Leisen", "Fabrizio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1701.08299", "submitter": "Viktor Witkovsky", "authors": "Viktor Witkovsky, Gejza Wimmer, Tomas Duby", "title": "Computing the aggregate loss distribution based on numerical inversion\n  of the compound empirical characteristic function of frequency and severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-parametric method for evaluation of the aggregate loss distribution\n(ALD) by combining and numerically inverting the empirical characteristic\nfunctions (CFs) is presented and illustrated. This approach to evaluate ALD is\nbased on purely non-parametric considerations, i.e., based on the empirical CFs\nof frequency and severity of the claims in the actuarial risk applications.\nThis approach can be, however, naturally generalized to a more complex\nsemi-parametric modeling approach, e.g., by incorporating the generalized\nPareto distribution fit of the severity distribution heavy tails, and/or by\nconsidering the weighted mixture of the parametric CFs (used to model the\nexpert knowledge) and the empirical CFs (used to incorporate the knowledge\nbased on the historical data - internal and/or external). Here we present a\nsimple and yet efficient method and algorithms for numerical inversion of the\nCF, suitable for evaluation of the ALDs and the associated measures of interest\nimportant for applications, as, e.g., the value at risk (VaR). The presented\napproach is based on combination of the Gil-Pelaez inversion formulae for\nderiving the probability distribution (PDF and CDF) from the compound\n(empirical) CF and the trapezoidal rule used for numerical integration. The\napplicability of the suggested approach is illustrated by analysis of a well\nknow insurance dataset, the Danish fire loss data.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 16:31:25 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Witkovsky", "Viktor", ""], ["Wimmer", "Gejza", ""], ["Duby", "Tomas", ""]]}]