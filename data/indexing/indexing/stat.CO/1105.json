[{"id": "1105.0269", "submitter": "Olivier Francois", "authors": "Olivier Francois, Guillaume Laval", "title": "Deviance Information Criteria for Model Selection in Approximate\n  Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a class of algorithmic methods in\nBayesian inference using statistical summaries and computer simulations. ABC\nhas become popular in evolutionary genetics and in other branches of biology.\nHowever model selection under ABC algorithms has been a subject of intense\ndebate during the recent years. Here we propose novel approaches to model\nselection based on posterior predictive distributions and approximations of the\ndeviance. We argue that this framework can settle some contradictions between\nthe computation of model probabilities and posterior predictive checks using\nABC posterior distributions. A simulation study and an analysis of a\nresequencing data set of human DNA show that the deviance criteria lead to\nsensible results in a number of model choice problems of interest to population\ngeneticists.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2011 09:15:18 GMT"}], "update_date": "2011-05-03", "authors_parsed": [["Francois", "Olivier", ""], ["Laval", "Guillaume", ""]]}, {"id": "1105.0638", "submitter": "Zizhuo Wang", "authors": "Xiaojun Chen, Dongdong Ge, Zizhuo Wang and Yinyu Ye", "title": "Complexity of Unconstrained L_2-L_p Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the unconstrained $L_2$-$L_p$ minimization: find a minimizer of\n$\\|Ax-b\\|^2_2+\\lambda \\|x\\|^p_p$ for given $A \\in R^{m\\times n}$, $b\\in R^m$\nand parameters $\\lambda>0$, $p\\in [0,1)$. This problem has been studied\nextensively in variable selection and sparse least squares fitting for high\ndimensional data. Theoretical results show that the minimizers of the\n$L_2$-$L_p$ problem have various attractive features due to the concavity and\nnon-Lipschitzian property of the regularization function $\\|\\cdot\\|^p_p$. In\nthis paper, we show that the $L_q$-$L_p$ minimization problem is strongly\nNP-hard for any $p\\in [0,1)$ and $q\\ge 1$, including its smoothed version. On\nthe other hand, we show that, by choosing parameters $(p,\\lambda)$ carefully, a\nminimizer, global or local, will have certain desired sparsity. We believe that\nthese results provide new theoretical insights to the studies and applications\nof the concave regularized optimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 17:24:06 GMT"}], "update_date": "2011-05-04", "authors_parsed": [["Chen", "Xiaojun", ""], ["Ge", "Dongdong", ""], ["Wang", "Zizhuo", ""], ["Ye", "Yinyu", ""]]}, {"id": "1105.0725", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao", "title": "Exploiting Correlation in Sparse Signal Recovery Problems: Multiple\n  Measurement Vectors, Block Sparsity, and Time-Varying Sparsity", "comments": "Extended abstract for ICML 2011 Structured Sparsity: Learning and\n  Inference Workshop. Experiment codes can be downloaded from:\n  http://dsp.ucsd.edu/~zhilin/papers/ICMLworkshop_code.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A trend in compressed sensing (CS) is to exploit structure for improved\nreconstruction performance. In the basic CS model, exploiting the clustering\nstructure among nonzero elements in the solution vector has drawn much\nattention, and many algorithms have been proposed. However, few algorithms\nexplicitly consider correlation within a cluster. Meanwhile, in the multiple\nmeasurement vector (MMV) model correlation among multiple solution vectors is\nlargely ignored. Although several recently developed algorithms consider the\nexploitation of the correlation, these algorithms need to know a priori the\ncorrelation structure, thus limiting their effectiveness in practical problems.\n  Recently, we developed a sparse Bayesian learning (SBL) algorithm, namely\nT-SBL, and its variants, which adaptively learn the correlation structure and\nexploit such correlation information to significantly improve reconstruction\nperformance. Here we establish their connections to other popular algorithms,\nsuch as the group Lasso, iterative reweighted $\\ell_1$ and $\\ell_2$ algorithms,\nand algorithms for time-varying sparsity. We also provide strategies to improve\nthese existing algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 02:43:57 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2011 23:43:22 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1105.0871", "submitter": "Jean-Michel Marin", "authors": "Yves Auffray and Pierre Barbillon and Jean-Michel Marin", "title": "Bounding rare event probabilities in computer experiments", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in bounding probabilities of rare events in the context of\ncomputer experiments. These rare events depend on the output of a physical\nmodel with random input variables. Since the model is only known through an\nexpensive black box function, standard efficient Monte Carlo methods designed\nfor rare events cannot be used. We then propose a strategy to deal with this\ndifficulty based on importance sampling methods. This proposal relies on\nKriging metamodeling and is able to achieve sharp upper confidence bounds on\nthe rare event probabilities. The variability due to the Kriging metamodeling\nstep is properly taken into account. The proposed methodology is applied to a\ntoy example and compared to more standard Bayesian bounds. Finally, a\nchallenging real case study is analyzed. It consists of finding an upper bound\nof the probability that the trajectory of an airborne load will collide with\nthe aircraft that has released it.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 16:56:47 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2012 12:27:04 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Auffray", "Yves", ""], ["Barbillon", "Pierre", ""], ["Marin", "Jean-Michel", ""]]}, {"id": "1105.0902", "submitter": "Drew Conway", "authors": "Drew Conway", "title": "Modeling Network Evolution Using Graph Motifs", "comments": "33 pages, 7 pages, GMM code available at:\n  https://github.com/drewconway/GMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Network structures are extremely important to the study of political science.\nMuch of the data in its subfields are naturally represented as networks. This\nincludes trade, diplomatic and conflict relationships. The social structure of\nseveral organization is also of interest to many researchers, such as the\naffiliations of legislators or the relationships among terrorist. A key aspect\nof studying social networks is understanding the evolutionary dynamics and the\nmechanism by which these structures grow and change over time. While current\nmethods are well suited to describe static features of networks, they are less\ncapable of specifying models of change and simulating network evolution. In the\nfollowing paper I present a new method for modeling network growth and\nevolution. This method relies on graph motifs to generate simulated network\ndata with particular structural characteristic. This technique departs notably\nfrom current methods both in form and function. Rather than a closed-form\nmodel, or stochastic implementation from a single class of graphs, the proposed\n\"graph motif model\" provides a framework for building flexible and complex\nmodels of network evolution. The paper proceeds as follows: first a brief\nreview of the current literature on network modeling is provided to place the\ngraph motif model in context. Next, the graph motif model is introduced, and a\nsimple example is provided. As a proof of concept, three classic random graph\nmodels are recovered using the graph motif modeling method: the Erdos-Renyi\nbinomial random graph, the Watts-Strogatz \"small world\" model, and the\nBarabasi-Albert preferential attachment model. In the final section I discuss\nthe results of these simulations and subsequent advantage and disadvantages\npresented by using this technique to model social networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 19:23:48 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Conway", "Drew", ""]]}, {"id": "1105.1288", "submitter": "Nikolai Gagunashvili", "authors": "Nikolai Gagunashvili", "title": "CHICOM: A code of tests for comparing unweighted and weighted histograms\n  and two weighted histograms", "comments": "9 pages", "journal-ref": "Computer Physics Communications, Volume 183, Issue 1, 2012, Pages\n  193-196", "doi": "10.1016/j.cpc.2011.08.014", "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex nucl-ex stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A self-contained Fortran-77 program for calculating test statistics to\ncompare weighted histogram with unweighted histogram and two histograms with\nweighted entries is presented. The code calculates test statistics for cases of\nhistograms with normalized weights of events and unnormalized weights of\nevents.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2011 14:09:56 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2011 14:51:52 GMT"}, {"version": "v3", "created": "Mon, 10 Oct 2011 12:32:18 GMT"}], "update_date": "2011-11-18", "authors_parsed": [["Gagunashvili", "Nikolai", ""]]}, {"id": "1105.1430", "submitter": "Stephane Chretien", "authors": "Stephane Chretien and Sebastien Darses", "title": "On the generic uniform uniqueness of the LASSO estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The LASSO is a variable subset selection procedure in statistical linear\nregression based on $\\ell_1$ penalization of the least-squares operator.\nUniqueness of the LASSO is an important issue, especially for the study of the\nLASSO path. The goal of the present paper is to provide a generic sufficient\ncondition on the design matrix for the LASSO minimizer to be unique. Unlike\nprevious works on the question of uniqueness, our condition only depends on the\ndesign matrix. Our study is based on a general position condition on the design\nmatrix which holds with probability one for most experimental models.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2011 09:56:49 GMT"}, {"version": "v2", "created": "Tue, 1 Mar 2016 09:10:18 GMT"}], "update_date": "2016-03-02", "authors_parsed": [["Chretien", "Stephane", ""], ["Darses", "Sebastien", ""]]}, {"id": "1105.1476", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "EM algorithm and variants: an informal tutorial", "comments": "Unpublished", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm introduced by Dempster et al in\n1977 is a very general method to solve maximum likelihood estimation problems.\nIn this informal report, we review the theory behind EM as well as a number of\nEM variants, suggesting that beyond the current state of the art is an even\nmuch wider territory still to be discovered.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2011 21:46:34 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 09:31:45 GMT"}], "update_date": "2012-09-10", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1105.1486", "submitter": "Mladen Pavicic", "authors": "Norman D. Megill and Mladen Pavicic", "title": "Estimating Bernoulli trial probability from a small sample", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard textbook method for estimating the probability of a biased coin\nfrom finite tosses implicitly assumes the sample sizes are large and gives\nincorrect results for small samples. We describe the exact solution, which is\ncorrect for any sample size.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2011 01:46:29 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Megill", "Norman D.", ""], ["Pavicic", "Mladen", ""]]}, {"id": "1105.1508", "submitter": "Alexis Roche", "authors": "Alexis Roche", "title": "Approximate inference via variational sampling", "comments": "Additional logistic regression experiments, minor edits. Retrieved\n  from http://www.sciencepubco.com/index.php/IJASP/article/view/1293", "journal-ref": "International Journal Of Advanced Statistics And Probability,\n  1(3), 110-120", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method called \"variational sampling\" is proposed to estimate integrals\nunder probability distributions that can be evaluated up to a normalizing\nconstant. The key idea is to fit the target distribution with an exponential\nfamily model by minimizing a strongly consistent empirical approximation to the\nKullback-Leibler divergence computed using either deterministic or random\nsampling. It is shown how variational sampling differs conceptually from both\nquadrature and importance sampling and established that, in the case of random\nindependence sampling, it may have much faster stochastic convergence than\nimportance sampling under mild conditions. The variational sampling\nimplementation presented in this paper requires a rough initial approximation\nto the target distribution, which may be found, e.g. using the Laplace method,\nand is shown to then have the potential to substantially improve over several\nexisting approximate inference techniques to estimate moments of order up to\ntwo of nearly-Gaussian distributions, which occur frequently in Bayesian\nanalysis. In particular, an application of variational sampling to Bayesian\nlogistic regression in moderate dimension is presented.\n", "versions": [{"version": "v1", "created": "Sun, 8 May 2011 12:30:08 GMT"}, {"version": "v2", "created": "Tue, 31 May 2011 08:13:56 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 14:25:12 GMT"}, {"version": "v4", "created": "Tue, 11 Jun 2013 05:40:14 GMT"}, {"version": "v5", "created": "Mon, 7 Oct 2013 10:20:08 GMT"}, {"version": "v6", "created": "Mon, 14 Oct 2013 12:38:08 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Roche", "Alexis", ""]]}, {"id": "1105.2982", "submitter": "Daniel Simpson", "authors": "Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Fast approximate inference with INLA: the past, the present and the\n  future", "comments": "8 Pages, 2 Figures. Presented at ISI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models are an extremely popular, flexible class of models.\nBayesian inference for these models is, however, tricky and time consuming.\nRecently, Rue, Martino and Chopin introduced the Integrated Nested Laplace\nApproximation (INLA) method for deterministic fast approximate inference. In\nthis paper, we outline the INLA approximation and its related R package. We\nwill discuss the newer components of the r-INLA program as well as some\npossible extensions.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2011 00:23:49 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1105.3605", "submitter": "Pierre-Andr\\'e Cornillon", "authors": "P. A. Cornillon, N. Hengartner, E. Matzner-L{\\o}ber", "title": "Iterative bias reduction multivariate smoothing in R: The ibr package", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate nonparametric analysis, sparseness of the covariates also\ncalled curse of dimensionality, forces one to use large smoothing parameters.\nThis leads to a biased smoother. Instead of focusing on optimally selecting the\nsmoothing parameter, we fix it to some reasonably large value to ensure an\nover-smoothing of the data. The resulting base smoother has a small variance\nbut a substantial bias. In this paper, we propose an R package named ibr to\niteratively correct the initial bias of the (base) estimator by an estimate of\nthe bias obtained by smoothing the residuals. After a brief description of\nIterated Bias Reduction smoothers, we examine the base smoothers implemented in\nthe packages: Nadaraya-Watson kernel smoothers and thin plate splines\nsmoothers. Then, we explain the stopping rules available in the package and\ntheir implementation. Finally we illustrate the package on two examples: a toy\nexample in RxR and the original Los Angeles ozone dataset.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2011 12:33:48 GMT"}], "update_date": "2011-05-19", "authors_parsed": [["Cornillon", "P. A.", ""], ["Hengartner", "N.", ""], ["Matzner-L\u00f8ber", "E.", ""]]}, {"id": "1105.4385", "submitter": "Ping Li", "authors": "Ping Li and Joshua Moore and Christian Konig", "title": "b-Bit Minwise Hashing for Large-Scale Linear SVM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to (seamlessly) integrate b-bit minwise hashing\nwith linear SVM to substantially improve the training (and testing) efficiency\nusing much smaller memory, with essentially no loss of accuracy. Theoretically,\nwe prove that the resemblance matrix, the minwise hashing matrix, and the b-bit\nminwise hashing matrix are all positive definite matrices (kernels).\nInterestingly, our proof for the positive definiteness of the b-bit minwise\nhashing kernel naturally suggests a simple strategy to integrate b-bit hashing\nwith linear SVM. Our technique is particularly useful when the data can not fit\nin memory, which is an increasingly critical issue in large-scale machine\nlearning. Our preliminary experimental results on a publicly available webspam\ndataset (350K samples and 16 million dimensions) verified the effectiveness of\nour algorithm. For example, the training time was reduced to merely a few\nseconds. In addition, our technique can be easily extended to many other linear\nand nonlinear machine learning applications such as logistic regression.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 01:56:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Li", "Ping", ""], ["Moore", "Joshua", ""], ["Konig", "Christian", ""]]}, {"id": "1105.4823", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (University Paris-Dauphine, IUF, and CREST)", "title": "Simulation in Statistics", "comments": "Draft of an advanced tutorial paper for the Proceedings of the 2011\n  Winter Simulation Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation has become a standard tool in statistics because it may be the\nonly tool available for analysing some classes of probabilistic models. We\nreview in this paper simulation tools that have been specifically derived to\naddress statistical challenges and, in particular, recent advances in the areas\nof adaptive Markov chain Monte Carlo (MCMC) algorithms, and approximate\nBayesian calculation (ABC) algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2011 16:35:59 GMT"}], "update_date": "2011-05-25", "authors_parsed": [["Robert", "Christian P.", "", "University Paris-Dauphine, IUF, and CREST"]]}, {"id": "1105.5256", "submitter": "Erlend Aune", "authors": "Erlend Aune and Daniel P. Simpson", "title": "Parameter estimation in high dimensional Gaussian distributions", "comments": "1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to compute the log-likelihood for high dimensional spatial Gaussian\nmodels, it is necessary to compute the determinant of the large, sparse,\nsymmetric positive definite precision matrix, Q. Traditional methods for\nevaluating the log-likelihood for very large models may fail due to the massive\nmemory requirements. We present a novel approach for evaluating such\nlikelihoods when the matrix-vector product, Qv, is fast to compute. In this\napproach we utilise matrix functions, Krylov subspaces, and probing vectors to\nconstruct an iterative method for computing the log-likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2011 10:52:54 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Aune", "Erlend", ""], ["Simpson", "Daniel P.", ""]]}, {"id": "1105.5542", "submitter": "Mehdi Molkaraie", "authors": "Mehdi Molkaraie and Hans-Andrea Loeliger", "title": "Monte Carlo Algorithms for the Partition Function and Information Rates\n  of Two-Dimensional Channels", "comments": null, "journal-ref": "IEEE Trans. on Information Theory, Volume 59, Jan. 2013, pp.\n  495-503", "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes Monte Carlo algorithms for the computation of the\ninformation rate of two-dimensional source/channel models. The focus of the\npaper is on binary-input channels with constraints on the allowed input\nconfigurations. The problem of numerically computing the information rate, and\neven the noiseless capacity, of such channels has so far remained largely\nunsolved. Both problems can be reduced to computing a Monte Carlo estimate of a\npartition function. The proposed algorithms use tree-based Gibbs sampling and\nmultilayer (multitemperature) importance sampling. The viability of the\nproposed algorithms is demonstrated by simulation results.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 12:36:42 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 16:33:38 GMT"}], "update_date": "2012-12-27", "authors_parsed": [["Molkaraie", "Mehdi", ""], ["Loeliger", "Hans-Andrea", ""]]}, {"id": "1105.5850", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Mark Briers, Pavel V. Shevchenko and Arnaud Doucet", "title": "Calibration and filtering for multi factor commodity models with\n  seasonality: incorporating panel data from futures contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a general multi-factor model for commodity spot prices and futures\nvaluation. We extend the multi-factor long-short model in Schwartz and Smith\n(2000) and Yan (2002) in two important aspects: firstly we allow for both the\nlong and short term dynamic factors to be mean reverting incorporating\nstochastic volatility factors and secondly we develop an additive structural\nseasonality model. Then a Milstein discretized non-linear stochastic volatility\nstate space representation for the model is developed which allows for futures\nand options contracts in the observation equation. We then develop numerical\nmethodology based on an advanced Sequential Monte Carlo algorithm utilising\nParticle Markov chain Monte Carlo to perform calibration of the model jointly\nwith the filtering of the latent processes for the long-short dynamics and\nvolatility factors. In this regard we explore and develop a novel methodology\nbased on an adaptive Rao-Blackwellised version of the Particle Markov chain\nMonte Carlo methodology. In doing this we deal accurately with the\nnon-linearities in the state-space model which are therefore introduced into\nthe filtering framework. We perform analysis on synthetic and real data for oil\ncommodities.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 02:55:30 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Peters", "Gareth W.", ""], ["Briers", "Mark", ""], ["Shevchenko", "Pavel V.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1105.5887", "submitter": "Francois Orieux", "authors": "F. Orieux, and O. F\\'eron, and J.-F. Giovannelli", "title": "Efficient sampling of high-dimensional Gaussian fields: the\n  non-stationary / non-sparse case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the problem of sampling Gaussian fields in high\ndimension. Solutions exist for two specific structures of inverse covariance :\nsparse and circulant. The proposed approach is valid in a more general case and\nespecially as it emerges in inverse problems. It relies on a\nperturbation-optimization principle: adequate stochastic perturbation of a\ncriterion and optimization of the perturbed criterion. It is shown that the\ncriterion minimizer is a sample of the target density. The motivation in\ninverse problems is related to general (non-convolutive) linear observation\nmodels and their resolution in a Bayesian framework implemented through\nsampling algorithms when existing samplers are not feasible. It finds a direct\napplication in myopic and/or unsupervised inversion as well as in some\nnon-Gaussian inversion. An illustration focused on hyperparameter estimation\nfor super-resolution problems assesses the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 07:31:01 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Orieux", "F.", ""], ["F\u00e9ron", "O.", ""], ["Giovannelli", "J. -F.", ""]]}, {"id": "1105.5924", "submitter": "Andriyan Suksmono Bayu", "authors": "Andriyan Bayu Suksmono", "title": "Reconstruction of Fractional Brownian Motion Signals From Its Sparse\n  Samples Based on Compressive Sampling", "comments": "6 double-column pages, 5 figures, submitted to ICEEI-2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.PR physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new fBm (fractional Brownian motion)\ninterpolation/reconstruction method from partially known samples based on CS\n(Compressive Sampling). Since 1/f property implies power law decay of the fBm\nspectrum, the fBm signals should be sparse in frequency domain. This property\nmotivates the adoption of CS in the development of the reconstruction method.\nHurst parameter H that occurs in the power law determines the sparsity level,\ntherefore the CS reconstruction quality of an fBm signal for a given number of\nknown subsamples will depend on H. However, the proposed method does not\nrequire the information of H to reconstruct the fBm signal from its partial\nsamples. The method employs DFT (Discrete Fourier Transform) as the sparsity\nbasis and a random matrix derived from known samples positions as the\nprojection basis. Simulated fBm signals with various values of H are used to\nshow the relationship between the Hurst parameter and the reconstruction\nquality. Additionally, US-DJIA (Dow Jones Industrial Average) stock index\nmonthly values time-series are also used to show the applicability of the\nproposed method to reconstruct a real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 10:11:06 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Suksmono", "Andriyan Bayu", ""]]}]