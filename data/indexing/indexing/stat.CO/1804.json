[{"id": "1804.00430", "submitter": "Ahmad Mouri Sardarabadi", "authors": "Ahmad Mouri Sardarabadi, Alle-Jan van der Veen, L.V.E. Koopmans", "title": "Constrained Least Squares for Extended Complex Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO astro-ph.IM cs.SY math.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For subspace estimation with an unknown colored noise, Factor Analysis (FA)\nis a good candidate for replacing the popular eigenvalue decomposition (EVD).\nFinding the unknowns in factor analysis can be done by solving a non-linear\nleast square problem. For this type of optimization problems, the Gauss-Newton\n(GN) algorithm is a powerful and simple method. The most expensive part of the\nGN algorithm is finding the direction of descent by solving a system of\nequations at each iteration. In this paper we show that for FA, the matrices\ninvolved in solving these systems of equations can be diagonalized in a closed\nform fashion and the solution can be found in a computationally efficient way.\nWe show how the unknown parameters can be updated without actually constructing\nthese matrices. The convergence performance of the algorithm is studied via\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 08:35:23 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Sardarabadi", "Ahmad Mouri", ""], ["van der Veen", "Alle-Jan", ""], ["Koopmans", "L. V. E.", ""]]}, {"id": "1804.00735", "submitter": "Yan Wang", "authors": "Yan Wang, Nathan Palmer, Qian Di, Joel Schwartz, Isaac Kohane, Tianxi\n  Cai", "title": "A Fast Divide-and-Conquer Sparse Cox Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally and statistically efficient divide-and-conquer\n(DAC) algorithm to fit sparse Cox regression to massive datasets where the\nsample size $n_0$ is exceedingly large and the covariate dimension $p$ is not\nsmall but $n_0\\gg p$. The proposed algorithm achieves computational efficiency\nthrough a one-step linear approximation followed by a least square\napproximation to the partial likelihood (PL). These sequences of linearization\nenable us to maximize the PL with only a small subset and perform penalized\nestimation via a fast approximation to the PL. The algorithm is applicable for\nthe analysis of both time-independent and time-dependent survival data.\nSimulations suggest that the proposed DAC algorithm substantially outperforms\nthe full sample-based estimators and the existing DAC algorithm with respect to\nthe computational speed, while it achieves similar statistical efficiency as\nthe full sample-based estimators. The proposed algorithm was applied to an\nextraordinarily large time-independent survival dataset and an extraordinarily\nlarge time-dependent survival dataset for the prediction of heart\nfailure-specific readmission within 30 days among Medicare heart failure\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 21:25:59 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Yan", ""], ["Palmer", "Nathan", ""], ["Di", "Qian", ""], ["Schwartz", "Joel", ""], ["Kohane", "Isaac", ""], ["Cai", "Tianxi", ""]]}, {"id": "1804.00766", "submitter": "Pavel Prikhodko", "authors": "Pavel Prikhodko, Nikita Kotlyarov", "title": "Calibration of Sobol indices estimates in case of noisy output", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simple noise correction method for Sobol' indices\nestimation. Sobol' indices, especially total Sobol' indices are quite sensitive\nto the noise in the output and tend to be severly biased (overestimated) if no\nnoise correction is done, which may make their computation meaningless in case\nof even quite moderate noise levels. Proposed method allows to get\napproximately unbiased noise free estimation of Sobol' indices at the cost of\nvariance of estimate increase if noise can be represented as a combination of\nadditive and multiplicative stationary noise. %Also we show that it is\nimpossible to do precise noise correction for more complex noise settings.\nProposed method is more straightforward than schemes found in the literature\nand does not introduce any assumptions on the function and noise distribution\n(except that it assumes noise to be stationary and be a combination of additive\nand multiplicative). One of the appealing features is that there is actual\nanalytical noise correction expression derived.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 23:59:41 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Prikhodko", "Pavel", ""], ["Kotlyarov", "Nikita", ""]]}, {"id": "1804.01431", "submitter": "Karla Monterrubio G\\'omez", "authors": "Karla Monterrubio-G\\'omez, Lassi Roininen, Sara Wade, Theo Damoulas,\n  and Mark Girolami", "title": "Posterior Inference for Sparse Hierarchical Non-stationary Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are valuable tools for non-parametric modelling, where\ntypically an assumption of stationarity is employed. While removing this\nassumption can improve prediction, fitting such models is challenging. In this\nwork, hierarchical models are constructed based on Gaussian Markov random\nfields with stochastic spatially varying parameters. Importantly, this allows\nfor non-stationarity while also addressing the computational burden through a\nsparse banded representation of the precision matrix. In this setting,\nefficient Markov chain Monte Carlo (MCMC) sampling is challenging due to the\nstrong coupling a posteriori of the parameters and hyperparameters. We develop\nand compare three adaptive MCMC schemes and make use of banded matrix\noperations for faster inference. Furthermore, a novel extension to\nmulti-dimensional settings is proposed through an additive structure that\nretains the flexibility and scalability of the model, while also inheriting\ninterpretability from the additive approach. A thorough assessment of the\nefficiency and accuracy of the methods in nonstationary settings is presented\nfor both simulated experiments and a computer emulation problem.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 14:26:06 GMT"}, {"version": "v2", "created": "Tue, 15 Jan 2019 13:51:46 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 13:35:50 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Monterrubio-G\u00f3mez", "Karla", ""], ["Roininen", "Lassi", ""], ["Wade", "Sara", ""], ["Damoulas", "Theo", ""], ["Girolami", "Mark", ""]]}, {"id": "1804.01811", "submitter": "Jere Koskela", "authors": "Jere Koskela, Paul A. Jenkins, Adam M. Johansen, Dario Spano", "title": "Asymptotic genealogies of interacting particle systems with an\n  application to sequential Monte Carlo", "comments": "28 pages, 1 figure. An earlier version of this manuscript contained\n  an error, which we have been able to correct and in so doing give a stronger\n  result under cleaner conditions. v7: Added several technical lemmas which\n  make the overall argument more explicit", "journal-ref": "Annals of Statistics 48(1):560-583, 2020", "doi": "10.1214/19-AOS1823", "report-no": null, "categories": "math.ST q-bio.PE stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study weighted particle systems in which new generations are resampled\nfrom current particles with probabilities proportional to their weights. This\ncovers a broad class of sequential Monte Carlo (SMC) methods, widely-used in\napplied statistics and cognate disciplines. We consider the genealogical tree\nembedded into such particle systems, and identify conditions, as well as an\nappropriate time-scaling, under which they converge to the Kingman n-coalescent\nin the infinite system size limit in the sense of finite-dimensional\ndistributions. Thus, the tractable n-coalescent can be used to predict the\nshape and size of SMC genealogies, as we illustrate by characterising the\nlimiting mean and variance of the tree height. SMC genealogies are known to be\nconnected to algorithm performance, so that our results are likely to have\napplications in the design of new methods as well. Our conditions for\nconvergence are strong, but we show by simulation that they do not appear to be\nnecessary.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 12:43:21 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 13:29:16 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 16:41:04 GMT"}, {"version": "v4", "created": "Thu, 23 May 2019 19:48:44 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 09:42:48 GMT"}, {"version": "v6", "created": "Mon, 12 Aug 2019 10:39:56 GMT"}, {"version": "v7", "created": "Fri, 16 Jul 2021 19:44:36 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Koskela", "Jere", ""], ["Jenkins", "Paul A.", ""], ["Johansen", "Adam M.", ""], ["Spano", "Dario", ""]]}, {"id": "1804.01858", "submitter": "Alejandro  Cholaquidis", "authors": "Catherine Aaron, Alejandro Cholaquidis, Ricardo Fraiman, Badih Ghattas", "title": "Robust Fusion Methods for Structured Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address one of the important problems in Big Data, namely how to combine\nestimators from different subsamples by robust fusion procedures, when we are\nunable to deal with the whole sample. We propose a general framework based on\nthe classic idea of `divide and conquer'. In particular we address in some\ndetail the case of a multivariate location and scatter matrix, the covariance\noperator for functional data, and clustering problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 14:02:54 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Aaron", "Catherine", ""], ["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""]]}, {"id": "1804.02274", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli, Florian Maire and Nial Friel", "title": "Computationally efficient inference for latent position network models", "comments": "39 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent position models are widely used for the analysis of networks in a\nvariety of research fields. In fact, these models possess a number of desirable\ntheoretical properties, and are particularly easy to interpret. However,\nstatistical methodologies to fit these models generally incur a computational\ncost which grows with the square of the number of nodes in the graph. This\nmakes the analysis of large social networks impractical. In this paper, we\npropose a new method characterised by a linear computational complexity, which\ncan be used to fit latent position models on networks of several tens of\nthousands nodes. Our approach relies on an approximation of the likelihood\nfunction, where the amount of noise introduced by the approximation can be\narbitrarily reduced at the expense of computational efficiency. We establish\nseveral theoretical results that show how the likelihood error propagates to\nthe invariant distribution of the Markov chain Monte Carlo sampler. In\nparticular, we demonstrate that one can achieve a substantial reduction in\ncomputing time and still obtain a good estimate of the latent structure.\nFinally, we propose applications of our method to simulated networks and to a\nlarge coauthorships network, highlighting the usefulness of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:55:44 GMT"}, {"version": "v2", "created": "Thu, 4 Jul 2019 14:45:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Maire", "Florian", ""], ["Friel", "Nial", ""]]}, {"id": "1804.02502", "submitter": "Cesar H Comin Prof.", "authors": "Felipe L. Gewers, Gustavo R. Ferreira, Henrique F. de Arruda, Filipi\n  N. Silva, Cesar H. Comin, Diego R. Amancio, Luciano da F. Costa", "title": "Principal Component Analysis: A Natural Approach to Data Exploration", "comments": null, "journal-ref": "ACM Computing Surveys (CSUR), 54(4), pp.1-34 (2021)", "doi": "10.1145/3447755", "report-no": null, "categories": "cs.CE stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is often used for analyzing data in the\nmost diverse areas. In this work, we report an integrated approach to several\ntheoretical and practical aspects of PCA. We start by providing, in an\nintuitive and accessible manner, the basic principles underlying PCA and its\napplications. Next, we present a systematic, though no exclusive, survey of\nsome representative works illustrating the potential of PCA applications to a\nwide range of areas. An experimental investigation of the ability of PCA for\nvariance explanation and dimensionality reduction is also developed, which\nconfirms the efficacy of PCA and also shows that standardizing or not the\noriginal data can have important effects on the obtained results. Overall, we\nbelieve the several covered issues can assist researchers from the most diverse\nareas in using and interpreting PCA.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 03:48:49 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 13:20:08 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gewers", "Felipe L.", ""], ["Ferreira", "Gustavo R.", ""], ["de Arruda", "Henrique F.", ""], ["Silva", "Filipi N.", ""], ["Comin", "Cesar H.", ""], ["Amancio", "Diego R.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1804.02655", "submitter": "Wei Gao", "authors": "Jiangtao Duan, Wei Gao and Hon Keung Tony Ng", "title": "Efficient Computational Algorithm for Optimal Continuous Experimental\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple yet efficient computational algorithm for computing the continuous\noptimal experimental design for linear models is proposed. An alternative proof\nthe monotonic convergence for $D$-optimal criterion on continuous design spaces\nare provided. We further show that the proposed algorithm converges to the\n$D$-optimal design. We also provide an algorithm for the $A$-optimality and\nconjecture that the algorithm convergence monotonically on continuous design\nspaces. Different numerical examples are used to demonstrated the usefulness\nand performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 09:01:41 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Duan", "Jiangtao", ""], ["Gao", "Wei", ""], ["Ng", "Hon Keung Tony", ""]]}, {"id": "1804.02719", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (University Paris Dauphine PSL, and University of\n  Warwick), Victor Elvira (IMT Lille Douai), Nick Tawn (University of Warwick),\n  Changye Wu (University Paris Dauphine PSL)", "title": "Accelerating MCMC Algorithms", "comments": "This is a survey paper, submitted WIREs Computational Statistics, to\n  with 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo algorithms are used to simulate from complex\nstatistical distributions by way of a local exploration of these distributions.\nThis local feature avoids heavy requests on understanding the nature of the\ntarget, but it also potentially induces a lengthy exploration of this target,\nwith a requirement on the number of simulations that grows with the dimension\nof the problem and with the complexity of the data behind it. Several\ntechniques are available towards accelerating the convergence of these Monte\nCarlo algorithms, either at the exploration level (as in tempering, Hamiltonian\nMonte Carlo and partly deterministic methods) or at the exploitation level\n(with Rao-Blackwellisation and scalable methods).\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 17:09:03 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 04:43:07 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Robert", "Christian P.", "", "University Paris Dauphine PSL, and University of\n  Warwick"], ["Elvira", "Victor", "", "IMT Lille Douai"], ["Tawn", "Nick", "", "University of Warwick"], ["Wu", "Changye", "", "University Paris Dauphine PSL"]]}, {"id": "1804.03523", "submitter": "Bradley Gram-Hansen", "authors": "Bradley Gram-Hansen, Yuan Zhou, Tobias Kohn, Tom Rainforth, Hongseok\n  Yang, Frank Wood", "title": "Hamiltonian Monte Carlo for Probabilistic Programs with Discontinuities", "comments": "4 pages, 2 figures", "journal-ref": "Inaugural Conference on Probabilistic Programming, 2018", "doi": null, "report-no": null, "categories": "stat.CO cs.PL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is arguably the dominant statistical inference\nalgorithm used in most popular \"first-order differentiable\" Probabilistic\nProgramming Languages (PPLs). However, the fact that HMC uses derivative\ninformation causes complications when the target distribution is\nnon-differentiable with respect to one or more of the latent variables. In this\npaper, we show how to use extensions to HMC to perform inference in\nprobabilistic programs that contain discontinuities. To do this, we design a\nSimple first-order Probabilistic Programming Language (SPPL) that contains a\nsufficient set of language restrictions together with a compilation scheme.\nThis enables us to preserve both the statistical and syntactic interpretation\nof if-else statements in the probabilistic program, within the scope of\nfirst-order PPLs. We also provide a corresponding mathematical formalism that\nensures any joint density denoted in such a language has a suitably low measure\nof discontinuities.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2018 09:22:56 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 16:32:10 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Gram-Hansen", "Bradley", ""], ["Zhou", "Yuan", ""], ["Kohn", "Tobias", ""], ["Rainforth", "Tom", ""], ["Yang", "Hongseok", ""], ["Wood", "Frank", ""]]}, {"id": "1804.03674", "submitter": "Hiroaki Kaido", "authors": "Hiroaki Kaido, Jiaxuan Li, Marc Rysman", "title": "Moment Inequalities in the Context of Simulated and Predicted Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the effects of simulated moments on the performance of\ninference methods based on moment inequalities. Commonly used confidence sets\nfor parameters are level sets of criterion functions whose boundary points may\ndepend on sample moments in an irregular manner. Due to this feature,\nsimulation errors can affect the performance of inference in non-standard ways.\nIn particular, a (first-order) bias due to the simulation errors may remain in\nthe estimated boundary of the confidence set. We demonstrate, through Monte\nCarlo experiments, that simulation errors can significantly reduce the coverage\nprobabilities of confidence sets in small samples. The size distortion is\nparticularly severe when the number of inequality restrictions is large. These\nresults highlight the danger of ignoring the sampling variations due to the\nsimulation errors in moment inequality models. Similar issues arise when using\npredicted variables in moment inequalities models. We propose a method for\nproperly correcting for these variations based on regularizing the intersection\nof moments in parameter space, and we show that our proposed method performs\nwell theoretically and in practice.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2018 18:15:33 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Kaido", "Hiroaki", ""], ["Li", "Jiaxuan", ""], ["Rysman", "Marc", ""]]}, {"id": "1804.03963", "submitter": "Simon Taylor", "authors": "Simon Taylor, Chris Sherlock, Gareth Ridall and Paul Fearnhead", "title": "Motor Unit Number Estimation via Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A change in the number of motor units that operate a particular muscle is an\nimportant indicator for the progress of a neuromuscular disease and the\nefficacy of a therapy. Inference for realistic statistical models of the\ntypical data produced when testing muscle function is difficult, and estimating\nthe number of motor units from these data is an ongoing statistical challenge.\nWe consider a set of models for the data, each with a different number of\nworking motor units, and present a novel method for Bayesian inference, based\non sequential Monte Carlo, which provides estimates of the marginal likelihood\nand, hence, a posterior probability for each model. To implement this approach\nin practice we require sequential Monte Carlo methods that have excellent\ncomputational and Monte Carlo properties. We achieve this by leveraging the\nconditional independence structure in the model, where given knowledge of which\nmotor units fired as a result of a particular stimulus, parameters that specify\nthe size of each unit's response are independent of the parameters defining the\nprobability that a unit will respond at all. The scalability of our methodology\nrelies on the natural conjugacy structure that we create for the former and an\nenforced, approximate conjugate structure for the latter. A simulation study\ndemonstrates the accuracy of our method, and inferences are consistent across\ntwo different datasets arising from the same rat tibial muscle.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 12:58:04 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Taylor", "Simon", ""], ["Sherlock", "Chris", ""], ["Ridall", "Gareth", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1804.04299", "submitter": "Wai Hoh Tang", "authors": "Wai Hoh Tang and Adrian R\\\"ollin", "title": "Model identification for ARMA time series through convolutional neural\n  networks", "comments": "17 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we use convolutional neural networks to address the problem of\nmodel identification for autoregressive moving average time series models. We\ncompare the performance of several neural network architectures, trained on\nsimulated time series, with likelihood based methods, in particular the Akaike\nand Bayesian information criteria. We find that our neural networks can\nsignificantly outperform these likelihood based methods in terms of accuracy\nand, by orders of magnitude, in terms of speed.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 03:33:27 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 03:35:35 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tang", "Wai Hoh", ""], ["R\u00f6llin", "Adrian", ""]]}, {"id": "1804.04444", "submitter": "Kody Law", "authors": "Ajay Jasra, Kody J.H. Law, Prince Peprah Osei", "title": "Multilevel Particle Filters for L\\'evy-driven stochastic differential\n  equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop algorithms for computing expectations of the laws of models\nassociated to stochastic differential equations (SDEs) driven by pure L\\'evy\nprocesses. We consider filtering such processes and well as pricing of path\ndependent options. We propose a multilevel particle filter (MLPF) to address\nthe computational issues involved in solving these continuum problems. We show\nvia numerical simulations and theoretical results that under suitable\nassumptions of the discretization of the underlying driving L\\'evy proccess,\nour proposed method achieves optimal convergence rates. The cost to obtain MSE\n$O(\\epsilon^2)$ scales like $O(\\epsilon^{-2})$ for our method, as compared with\nthe standard particle filter $O(\\epsilon^{-3})$.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 11:47:53 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 21:07:13 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Osei", "Prince Peprah", ""]]}, {"id": "1804.04587", "submitter": "William Artman", "authors": "William J. Artman, Inbal Nahum-Shani, Tianshuang Wu, James R. McKay,\n  Ashkan Ertefaie", "title": "Power Analysis in a SMART Design: Sample Size Estimation for Determining\n  the Best Dynamic Treatment Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential, multiple assignment, randomized trial (SMART) designs have become\nincreasingly popular in the field of precision medicine by providing a means\nfor comparing sequences of treatments tailored to the individual patient, i.e.,\ndynamic treatment regime (DTR). The construction of evidence-based DTRs\npromises a replacement to adhoc one-size-fits-all decisions pervasive in\npatient care. However, there are substantial statistical challenges in sizing\nSMART designs due to the complex correlation structure between the DTRs\nembedded in the design. Since the primary goal of SMARTs is the construction of\nan optimal DTR, investigators are interested in sizing SMARTs based on the\nability to screen out DTRs inferior to the optimal DTR by a given amount which\ncannot be done using existing methods. In this paper, we fill this gap by\ndeveloping a rigorous power analysis framework that leverages multiple\ncomparisons with the best methodology. Our method employs Monte Carlo\nsimulation in order to compute the minimum number of individuals to enroll in\nan arbitrary SMART. We will evaluate our method through extensive simulation\nstudies. We will illustrate our method by retrospectively computing the power\nin the Extending Treatment Effectiveness of Naltrexone SMART study.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 18:17:06 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Artman", "William J.", ""], ["Nahum-Shani", "Inbal", ""], ["Wu", "Tianshuang", ""], ["McKay", "James R.", ""], ["Ertefaie", "Ashkan", ""]]}, {"id": "1804.04678", "submitter": "Leonardo Bastos", "authors": "Leonardo S Bastos and Natalia S Paiva and Francisco I Bastos and\n  Daniel A M Villela", "title": "Fast approaches for Bayesian estimation of size of hard-to-reach\n  populations using Network Scale-up", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network scale-up method is commonly used to overcome difficulties in\nestimating the size of hard-to-reach populations. The method uses indirect\ninformation based on social network of each participant taken from the general\npopulation, but in some applications a fast computational approach would be\nhighly recommended. We propose a Gibbs sampling method and a Monte Carlo\napproach to sample from the random degree model. We applied the abovementioned\nanalytical strategies to previous data on heavy drug users from Curitiba,\nBrazil.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 18:12:08 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Bastos", "Leonardo S", ""], ["Paiva", "Natalia S", ""], ["Bastos", "Francisco I", ""], ["Villela", "Daniel A M", ""]]}, {"id": "1804.04859", "submitter": "Jonas Wallin", "authors": "Jonas Wallin and Sreekar Vadlamani", "title": "Infinite dimensional adaptive MCMC for Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian processes are widely applied in many fields like, statistics,\ninverse problems and machine learning. A popular method for inference is\nthrough the posterior distribution, which is typically carried out by Markov\nChain Monte Carlo (MCMC) algorithms. Most Gaussian processes can be represented\nas a Gaussian measure in a infinite dimensional space. This is an issue for\nstandard algorithms as they break down in an infinite dimensional setting, thus\nthe need for appropriate infinite dimensional samplers for implementing\nprobabilistic inference in such framework. In this paper, we introduce several\nadaptive versions of the preconditioned Crank-Nicolson Langevin (pCNL)\nalgorithm, which can be viewed as an infinite dimensional version of the well\nknown Metropolis adjusted Langevin algorithm (MALA) algorithm for Gaussian\nprocesses. The basic premise for all our proposals lies in the idea of\nimplementing change of measure formulation to adapt the algorithms to greatly\nimprove their efficiency. A gradient-free version of pCNL is introduced, which\nis a hybrid of an adaptive independence sampler and an adaptive random walk\nsampler, and is shown to outperform the standard preconditioned Crank-Nicolson\n(pCN) scheme. Finally, we demonstrate the efficiency of our proposed algorithm\nfor three different statistical models.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:43:13 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Wallin", "Jonas", ""], ["Vadlamani", "Sreekar", ""]]}, {"id": "1804.05185", "submitter": "Roberto Di Mari", "authors": "R. Di Mari, R. Rocci, and S.A. Gattone", "title": "Constrained maximum likelihood estimation of clusterwise linear\n  regression models with unknown number of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an equivariant approach imposing data-driven bounds for the\nvariances to avoid singular and spurious solutions in maximum likelihood (ML)\nestimation of clusterwise linear regression models. We investigate its use in\nthe choice of the number of components and we propose a computational shortcut,\nwhich significantly reduces the computational time needed to tune the bounds on\nthe data. In the simulation study and the two real-data applications, we show\nthat the proposed methods guarantee a reliable assessment of the number of\ncomponents compared to standard unconstrained methods, together with accurate\nmodel parameters estimation and cluster recovery.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 08:37:47 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Di Mari", "R.", ""], ["Rocci", "R.", ""], ["Gattone", "S. A.", ""]]}, {"id": "1804.05521", "submitter": "Linda Altieri", "authors": "Linda Altieri, Daniela Cocchi, Giulia Roli", "title": "SpatEntropy: Spatial Entropy Measures in R", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article illustrates how to measure the heterogeneity of spatial data\npresenting a finite number of categories via computation of spatial entropy.\nThe R package SpatEntropy contains functions for the computation of entropy and\nspatial entropy measures. The extension to spatial entropy measures is a unique\nfeature of SpatEntropy. In addition to the traditional version of Shannon's\nentropy, the package includes Batty's spatial entropy, O'Neill's entropy, Li\nand Reynolds' contagion index, Karlstrom and Ceccato's entropy, Leibovici's\nentropy, Parresol and Edwards' entropy and Altieri's entropy. The package is\nable to work with both areal and point data. This paper is a general\ndescription of SpatEntropy, as well as its necessary theoretical background,\nand an introduction for new users.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 07:01:00 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Altieri", "Linda", ""], ["Cocchi", "Daniela", ""], ["Roli", "Giulia", ""]]}, {"id": "1804.05678", "submitter": "Georg  Stadler", "authors": "Chen Li and Georg Stadler", "title": "Sparse solutions in optimal control of PDEs with uncertain parameters:\n  the linear case", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sparse solutions of optimal control problems governed by PDEs with\nuncertain coefficients. We propose two formulations, one where the solution is\na deterministic control optimizing the mean objective, and a formulation aiming\nat stochastic controls that share the same sparsity structure. In both\nformulations, regions where the controls do not vanish can be interpreted as\noptimal locations for placing control devices. In this paper, we focus on\nlinear PDEs with linearly entering uncertain parameters. Under these\nassumptions, the deterministic formulation reduces to a problem with known\nstructure, and thus we mainly focus on the stochastic control formulation.\nHere, shared sparsity is achieved by incorporating the $L^1$-norm of the mean\nof the pointwise squared controls in the objective. We reformulate the problem\nusing a norm reweighting function that is defined over physical space only and\nthus helps to avoid approximation of the random space using samples or\nquadrature. We show that a fixed point algorithm applied to the norm\nreweighting formulation leads to a variant of the well-studied iterative\nreweighted least squares (IRLS) algorithm, and we propose a novel\npreconditioned Newton-conjugate gradient method to speed up the IRLS algorithm.\nWe combine our algorithms with low-rank operator approximations, for which we\nprovide estimates of the truncation error. We carefully examine the\ncomputational complexity of the resulting algorithms. The sparsity structure of\nthe optimal controls and the performance of the solution algorithms are studied\nnumerically using control problems governed by the Laplace and Helmholtz\nequations. In these experiments the Newton variant clearly outperforms the IRLS\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 13:48:57 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 23:47:59 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 13:05:35 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Li", "Chen", ""], ["Stadler", "Georg", ""]]}, {"id": "1804.05809", "submitter": "Maxime Vono", "authors": "Maxime Vono, Nicolas Dobigeon and Pierre Chainais", "title": "Split-and-augmented Gibbs sampler - Application to large-scale inference\n  problems", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2894825", "report-no": null, "categories": "stat.ME eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives two new optimization-driven Monte Carlo algorithms\ninspired from variable splitting and data augmentation. In particular, the\nformulation of one of the proposed approaches is closely related to the\nalternating direction method of multipliers (ADMM) main steps. The proposed\nframework enables to derive faster and more efficient sampling schemes than the\ncurrent state-of-the-art methods and can embed the latter. By sampling\nefficiently the parameter to infer as well as the hyperparameters of the\nproblem, the generated samples can be used to approximate Bayesian estimators\nof the parameters to infer. Additionally, the proposed approach brings\nconfidence intervals at a low cost contrary to optimization methods.\nSimulations on two often-studied signal processing problems illustrate the\nperformance of the two proposed samplers. All results are compared to those\nobtained by recent state-of-the-art optimization and MCMC algorithms used to\nsolve these problems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:28:42 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 17:01:35 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Vono", "Maxime", ""], ["Dobigeon", "Nicolas", ""], ["Chainais", "Pierre", ""]]}, {"id": "1804.05975", "submitter": "James M. Flegal", "authors": "Ying Liu and Dootika Vats and James M. Flegal", "title": "Batch size selection for variance estimators in MCMC", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider batch size selection for a general class of multivariate batch\nmeans variance estimators, which are computationally viable for\nhigh-dimensional Markov chain Monte Carlo simulations. We derive the asymptotic\nmean squared error for this class of estimators. Further, we propose a\nparametric technique for estimating optimal batch sizes and discuss practical\nissues regarding the estimating process. Vector auto-regressive, Bayesian\nlogistic regression, and Bayesian dynamic space-time examples illustrate the\nquality of the estimation procedure where the proposed optimal batch sizes\noutperform current batch size selection methods.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 23:16:29 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 15:45:28 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 15:43:39 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Liu", "Ying", ""], ["Vats", "Dootika", ""], ["Flegal", "James M.", ""]]}, {"id": "1804.06406", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Mike Hobson, Anthony Lasenby", "title": "nestcheck: diagnostic tests for nested sampling calculations", "comments": "Minor updates and improvements to text. Added extra figure. 12 pages\n  + appendix, 15 figures", "journal-ref": "Mon. Notices Royal Astron. Soc. 483, 2 (2019) p2044-2056", "doi": "10.1093/mnras/sty3090", "report-no": null, "categories": "stat.CO astro-ph.CO astro-ph.IM physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling is an increasingly popular technique for Bayesian\ncomputation, in particular for multimodal, degenerate problems of moderate to\nhigh dimensionality. Without appropriate settings, however, nested sampling\nsoftware may fail to explore such posteriors correctly; for example producing\ncorrelated samples or missing important modes. This paper introduces new\ndiagnostic tests to assess the reliability both of parameter estimation and\nevidence calculations using nested sampling software, and demonstrates them\nempirically. We present two new diagnostic plots for nested sampling, and give\npractical advice for nested sampling software users in astronomy and beyond.\nOur diagnostic tests and diagrams are implemented in nestcheck: a publicly\navailable Python package for analysing nested sampling calculations, which is\ncompatible with output from MultiNest, PolyChord and dyPolyChord.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:00:02 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 18:00:01 GMT"}, {"version": "v3", "created": "Sat, 6 Oct 2018 09:16:22 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1804.06622", "submitter": "Michael Beard", "authors": "Michael Beard, Ba Tuong Vo, Ba-Ngu Vo", "title": "A Solution for Large-scale Multi-object Tracking", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large-scale multi-object tracker based on the generalised labeled\nmulti-Bernoulli (GLMB) filter is proposed. The algorithm is capable of tracking\na very large, unknown and time-varying number of objects simultaneously, in the\npresence of a high number of false alarms, as well as misdetections and\nmeasurement origin uncertainty due to closely spaced objects. The algorithm is\ndemonstrated on a simulated large-scale tracking scenario, where the peak\nnumber objects appearing simultaneously exceeds one million. To evaluate the\nperformance of the proposed tracker, we also introduce a new method of applying\nthe optimal sub-pattern assignment (OSPA) metric, and an efficient strategy for\nits evaluation in large-scale scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 09:43:10 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Beard", "Michael", ""], ["Vo", "Ba Tuong", ""], ["Vo", "Ba-Ngu", ""]]}, {"id": "1804.06742", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer, Martin Braun", "title": "An efficient open-source implementation to compute the Jacobian matrix\n  for the Newton-Raphson power flow algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power flow calculations for systems with a large number of buses, e.g. grids\nwith multiple voltage levels, or time series based calculations result in a\nhigh computational effort. A common power flow solver for the efficient\nanalysis of power systems is the Newton-Raphson algorithm. The main\ncomputational effort of this method results from the linearization of the\nnonlinear power flow problem and solving the resulting linear equation. This\npaper presents an algorithm for the fast linearization of the power flow\nproblem by creating the Jacobian matrix directly in CRS format. The increase in\nspeed is achieved by reducing the number of iterations over the nonzero\nelements of the sparse Jacobian matrix. This allows to efficiently create the\nJacobian matrix without having to approximate the problem. A comparison of the\ncalculation time of three power grids shows that comparable open-source\nimplementations need 3-14x the time to create the Jacobian matrix.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 14:07:41 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Braun", "Martin", ""]]}, {"id": "1804.07262", "submitter": "Jarno Hartog", "authors": "Jarno Hartog, Harry van Zanten", "title": "Nonparametric Bayesian label prediction on a large graph using truncated\n  Laplacian regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes an implementation of a nonparametric Bayesian approach\nto solving binary classification problems on graphs. We consider a hierarchical\nBayesian approach with a prior that is constructed by truncating a series\nexpansion of the soft label function using the graph Laplacian eigenfunctions\nas basis functions. We compare our truncated prior to the untruncated Laplacian\nbased prior in simulated and real data examples to illustrate the improved\nscalability in terms of size of the underlying graph.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 09:36:43 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Hartog", "Jarno", ""], ["van Zanten", "Harry", ""]]}, {"id": "1804.07600", "submitter": "Yesim Guney", "authors": "Yesim Guney, Yetkin Tuac, Senay Ozdemir, and Olcay Arslan", "title": "Conditional Maximum Lq-Likelihood Estimation for Regression Model with\n  Autoregressive Error Terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the parameter estimation of regression model\nwith pth order autoregressive (AR(p)) error term. We use the Maximum\nLq-likelihood (MLq) estimation method that is proposed by Ferrari and Yang\n(2010a), as a robust alternative to the classical maximum likelihood (ML)\nestimation method to handle the outliers in the data. After exploring the MLq\nestimators for the parameters of interest, we provide some asymptotic\nproperties of the resulting MLq estimators. We give a simulation study and a\nreal data example to illustrate the performance of the new estimators over the\nML estimators and observe that the MLq estimators have superiority over the ML\nestimators when outliers are present in the data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 13:22:46 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Guney", "Yesim", ""], ["Tuac", "Yetkin", ""], ["Ozdemir", "Senay", ""], ["Arslan", "Olcay", ""]]}, {"id": "1804.07833", "submitter": "Bamdad Hosseini Dr.", "authors": "Bamdad Hosseini", "title": "Two Metropolis-Hastings algorithms for posterior measures with\n  non-Gaussian priors in infinite dimensions", "comments": "Minor revisions", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two classes of Metropolis-Hastings algorithms for sampling\ntarget measures that are absolutely continuous with respect to non-Gaussian\nprior measures on infinite-dimensional Hilbert spaces. In particular, we focus\non certain classes of prior measures for which prior-reversible proposal\nkernels of the autoregressive type can be designed. We then use these proposal\nkernels to design algorithms that satisfy detailed balance with respect to the\ntarget measures. Afterwards, we introduce a new class of prior measures, called\nthe Bessel-K priors, as a generalization of the gamma distribution to measures\nin infinite dimensions. The Bessel-K priors interpolate between well-known\npriors such as the gamma distribution and Besov priors and can model sparse or\ncompressible parameters. We present concrete instances of our algorithms for\nthe Bessel-K priors in the context of numerical examples in density estimation,\nfinite-dimensional denoising and deconvolution on the circle.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 21:36:08 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 16:24:16 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 23:00:00 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2019 22:50:13 GMT"}], "update_date": "2019-04-24", "authors_parsed": [["Hosseini", "Bamdad", ""]]}, {"id": "1804.08137", "submitter": "Marco Scutari", "authors": "Marco Scutari, Claudia Vitolo, Allan Tucker", "title": "Learning Bayesian Networks from Big Data with Greedy Search:\n  Computational Complexity and Efficient Implementation", "comments": "15 pages, 3 figures", "journal-ref": "Statistics and Computing 2019, 29(5):1095-1108", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the structure of Bayesian networks from data is known to be a\ncomputationally challenging, NP-hard problem. The literature has long\ninvestigated how to perform structure learning from data containing large\nnumbers of variables, following a general interest in high-dimensional\napplications (\"small n, large p\") in systems biology and genetics.\n  More recently, data sets with large numbers of observations (the so-called\n\"big data\") have become increasingly common; and these data sets are not\nnecessarily high-dimensional, sometimes having only a few tens of variables\ndepending on the application. We revisit the computational complexity of\nBayesian network structure learning in this setting, showing that the common\nchoice of measuring it with the number of estimated local distributions leads\nto unrealistic time complexity estimates for the most common class of\nscore-based algorithms, greedy search. We then derive more accurate expressions\nunder common distributional assumptions. These expressions suggest that the\nspeed of Bayesian network learning can be improved by taking advantage of the\navailability of closed form estimators for local distributions with few\nparents. Furthermore, we find that using predictive instead of in-sample\ngoodness-of-fit scores improves speed; and we confirm that is improves the\naccuracy of network reconstruction as well, as previously observed by\nChickering and Heckerman (2000). We demonstrate these results on large\nreal-world environmental and epidemiological data; and on reference data sets\navailable from public repositories.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 16:57:30 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:42:13 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Scutari", "Marco", ""], ["Vitolo", "Claudia", ""], ["Tucker", "Allan", ""]]}, {"id": "1804.08341", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Dianhui Wang and Geoffrey J. McLachlan", "title": "Randomized Mixture Models for Probability Density Approximation and\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized neural networks (NNs) are an interesting alternative to\nconventional NNs that are more used for data modeling. The random vector\nfunctional-link (RVFL) network is an established and theoretically\nwell-grounded randomized learning model. A key theoretical result for RVFL\nnetworks is that they provide universal approximation for continuous maps, on\naverage, almost surely. We specialize and modify this result, and show that\nRFVL networks can provide functional approximations that converge in\nKullback-Leibler divergence, when the target function is a probability density\nfunction. Expanding on the approximation results, we demonstrate the the RFVL\nnetworks lead to a simple randomized mixture model (MM) construction for\ndensity estimation from random data. An expectation-maximization (EM) algorithm\nis derived for the maximum likelihood estimation of our randomized MM. The EM\nalgorithm is proved to be globally convergent and the maximum likelihood\nestimator is proved to be consistent. A set of simulation studies is given to\nprovide empirical evidence towards our approximation and density estimation\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:21:20 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Wang", "Dianhui", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1804.08365", "submitter": "Hien Nguyen", "authors": "Andrew T. Jones and Hien D. Nguyen and Geoffrey J. McLachlan", "title": "Positive data kernel density estimation via the logKDE package for R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimators (KDEs) are ubiquitous tools for nonparametric\nestimation of probability density functions (PDFs), when data are obtained from\nunknown data generating processes. The KDEs that are typically available in\nsoftware packages are defined, and designed, to estimate real-valued data. When\napplied to positive data, these typical KDEs do not yield bona fide PDFs. A\nlog-transformation methodology can be applied to produce a nonparametric\nestimator that is appropriate and yields proper PDFs over positive supports. We\ncall the KDEs obtained via this transformation log-KDEs. We derive expressions\nfor the pointwise biases, variances, and mean-squared errors of the log- KDEs\nthat are obtained via various kernel functions. Mean integrated squared error\n(MISE) and asymptotic MISE results are also provided and a plug-in rule for\nlog-KDE bandwidths is derived. We demonstrate the log-KDEs methodology via our\nR package, logKDE. Real data case studies are provided to demonstrate the\nlog-KDE approach.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 12:28:25 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 23:59:48 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Jones", "Andrew T.", ""], ["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1804.08552", "submitter": "I\\~naki Ucar", "authors": "I\\~naki Ucar, Edzer Pebesma, Arturo Azcorra", "title": "Measurement Errors in R", "comments": "10 pages, 2 figures", "journal-ref": "The R Journal (2018) 10:2, pages 549-557", "doi": "10.32614/RJ-2018-075", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an R package to handle and represent measurements with\nerrors in a very simple way. We briefly introduce the main concepts of\nmetrology and propagation of uncertainty, and discuss related R packages.\nBuilding upon this, we introduce the 'errors' package, which provides a class\nfor associating uncertainty metadata, automated propagation and reporting.\nWorking with 'errors' enables transparent, lightweight, less error-prone\nhandling and convenient representation of measurements with errors. Finally, we\ndiscuss the advantages, limitations and future work of computing with errors.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 16:40:29 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 12:40:07 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ucar", "I\u00f1aki", ""], ["Pebesma", "Edzer", ""], ["Azcorra", "Arturo", ""]]}, {"id": "1804.08694", "submitter": "Natalie Karavarsamis", "authors": "Natalie Karavarsamis and Richard M. huggins", "title": "Two-stage approaches to the analysis of occupancy data I: The\n  homogeneous case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models are used in statistical ecology to estimate species\ndispersion. The two components of an occupancy model are the detection and\noccupancy probabilities, with the main interest being in the occupancy\nprobabilities.\n  We show that for the homogeneous occupancy model there is an orthogonal\ntransformation of the parameters that gives a natural two-stage inference\nprocedure based on a conditional likelihood. We then extend this to a partial\nlikelihood that gives explicit estimators of the model parameters. By allowing\nthe separate modelling of the detection and occupancy probabilities, the\nextension of the two-stage approach to more general models has the potential to\nsimplify the computational routines used there.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 03:51:43 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Karavarsamis", "Natalie", ""], ["huggins", "Richard M.", ""]]}, {"id": "1804.08738", "submitter": "Thomas Catanach", "authors": "Thomas A. Catanach and James L. Beck", "title": "Bayesian Updating and Uncertainty Quantification using Sequential\n  Tempered MCMC with the Rank-One Modified Metropolis Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are critical for quantifying the behaviors of systems. They\ncapture our uncertainty about a system's behavior using probability\ndistributions and update this understanding as new information becomes\navailable. Probabilistic predictions that incorporate this uncertainty can then\nbe made to evaluate system performance and make decisions. While Bayesian\nmethods are very useful, they are often computationally intensive. This\nnecessitates the development of more efficient algorithms. Here, we discuss a\ngroup of population Markov Chain Monte Carlo (MCMC) methods for Bayesian\nupdating and system reliability assessment that we call Sequential Tempered\nMCMC (ST-MCMC) algorithms. These algorithms combine 1) a notion of tempering to\ngradually transform a population of samples from the prior to the posterior\nthrough a series of intermediate distributions, 2) importance resampling, and\n3) MCMC. They are a form of Sequential Monte Carlo and include algorithms like\nTransitional Markov Chain Monte Carlo and Subset Simulation. We also introduce\na new sampling algorithm called the Rank-One Modified Metropolis Algorithm\n(ROMMA), which builds upon the Modified Metropolis Algorithm used within Subset\nSimulation to improve performance in high dimensions. Finally, we formulate a\nsingle algorithm to solve combined Bayesian updating and reliability assessment\nproblems to make posterior assessments of system reliability. The algorithms\nare then illustrated by performing prior and posterior reliability assessment\nof a water distribution system with unknown leaks and demands.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 21:10:06 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Catanach", "Thomas A.", ""], ["Beck", "James L.", ""]]}, {"id": "1804.08841", "submitter": "Haoyang Liu", "authors": "Haoyang Liu and Rina Foygel Barber", "title": "Between hard and soft thresholding: optimal iterative thresholding\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Iterative thresholding algorithms seek to optimize a differentiable objective\nfunction over a sparsity or rank constraint by alternating between gradient\nsteps that reduce the objective, and thresholding steps that enforce the\nconstraint. This work examines the choice of the thresholding operator, and\nasks whether it is possible to achieve stronger guarantees than what is\npossible with hard thresholding. We develop the notion of relative concavity of\na thresholding operator, a quantity that characterizes the worst-case\nconvergence performance of any thresholding operator on the target optimization\nproblem. Surprisingly, we find that commonly used thresholding operators, such\nas hard thresholding and soft thresholding, are suboptimal in terms of\nworst-case convergence guarantees. Instead, a general class of thresholding\noperators, lying between hard thresholding and soft thresholding, is shown to\nbe optimal with the strongest possible convergence guarantee among all\nthresholding operators. Examples of this general class includes $\\ell_q$\nthresholding with appropriate choices of $q$, and a newly defined {\\em\nreciprocal thresholding} operator. We also investigate the implications of the\nimproved optimization guarantee in the statistical setting of sparse linear\nregression, and show that this new class of thresholding operators attain the\noptimal rate for computationally efficient estimators, matching the Lasso.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 05:19:44 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 17:04:50 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 01:06:22 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 19:54:59 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Liu", "Haoyang", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1804.10889", "submitter": "Chengcheng Huang", "authors": "Chengcheng Huang, Housen Li, Lizhi Cheng and Wei Peng", "title": "A linear time algorithm for multiscale quantile simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change-point problems have appeared in a great many applications for example\ncancer genetics, econometrics and climate change. Modern multiscale type\nsegmentation methods are considered to be a statistically efficient approach\nfor multiple change-point detection, which minimize the number of change-points\nunder a multiscale side-constraint. The constraint threshold plays a critical\nrole in balancing the data-fit and model complexity. However, the computation\ntime of such a threshold is quadratic in terms of sample size $n$, making it\nimpractical for large scale problems. In this paper we proposed an\n$\\mathcal{O}(n)$ algorithm by utilizing the hidden quasiconvexity structure of\nthe problem. It applies to all regression models in exponential family with\narbitrary convex scale penalties. Simulations verify its computational\nefficiency and accuracy. An implementation is provided in R-package \"linearQ\"\non CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 08:15:36 GMT"}, {"version": "v2", "created": "Thu, 3 May 2018 09:09:07 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Huang", "Chengcheng", ""], ["Li", "Housen", ""], ["Cheng", "Lizhi", ""], ["Peng", "Wei", ""]]}, {"id": "1804.11058", "submitter": "Florian Gerber", "authors": "Florian Gerber and Reinhard Furrer", "title": "optimParallel: an R Package Providing Parallel Versions of the\n  Gradient-Based Optimization Methods of optim()", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package optimParallel provides a parallel version of the gradient-based\noptimization methods of optim(). The main function of the package is\noptimParallel(), which has the same usage and output as optim(). Using\noptimParallel() can significantly reduce optimization times. We introduce the R\npackage and illustrate its implementation, which takes advantage of the lexical\nscoping mechanism of R.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 06:51:50 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Gerber", "Florian", ""], ["Furrer", "Reinhard", ""]]}, {"id": "1804.11157", "submitter": "Jonas Latz", "authors": "Jonas Latz, Marvin Eisenberger, Elisabeth Ullmann", "title": "Fast sampling of parameterised Gaussian random fields", "comments": "35 pages", "journal-ref": "Comput. Methods Appl. Mech. Engrg. (2019)", "doi": "10.1016/j.cma.2019.02.003", "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields are popular models for spatially varying\nuncertainties, arising for instance in geotechnical engineering, hydrology or\nimage processing. A Gaussian random field is fully characterised by its mean\nfunction and covariance operator. In more complex models these can also be\npartially unknown. In this case we need to handle a family of Gaussian random\nfields indexed with hyperparameters. Sampling for a fixed configuration of\nhyperparameters is already very expensive due to the nonlocal nature of many\nclassical covariance operators. Sampling from multiple configurations increases\nthe total computational cost severely. In this report we employ parameterised\nKarhunen-Lo\\`eve expansions for sampling. To reduce the cost we construct a\nreduced basis surrogate built from snapshots of Karhunen-Lo\\`eve eigenvectors.\nIn particular, we consider Mat\\'ern-type covariance operators with unknown\ncorrelation length and standard deviation. We suggest a linearisation of the\ncovariance function and describe the associated online-offline decomposition.\nIn numerical experiments we investigate the approximation error of the reduced\neigenpairs. As an application we consider forward uncertainty propagation and\nBayesian inversion with an elliptic partial differential equation where the\nlogarithm of the diffusion coefficient is a parameterised Gaussian random\nfield. In the Bayesian inverse problem we employ Markov chain Monte Carlo on\nthe reduced space to generate samples from the posterior measure. All numerical\nexperiments are conducted in 2D physical space, with non-separable covariance\noperators, and finite element grids with $\\sim 10^4$ degrees of freedom.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 12:38:13 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 10:45:21 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Latz", "Jonas", ""], ["Eisenberger", "Marvin", ""], ["Ullmann", "Elisabeth", ""]]}, {"id": "1804.11224", "submitter": "Craig Wang", "authors": "Craig Wang and Reinhard Furrer", "title": "eggCounts: a Bayesian hierarchical toolkit to model faecal egg count\n  reductions", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a vignette for the R package eggCounts version 2.0. The package\nimplements a suite of Bayesian hierarchical models dealing with faecal egg\ncount reductions. The models are designed for a variety of practical\nsituations, including individual treatment efficacy, zero inflation, small\nsample size (less than 10) and potential outliers. The functions are intuitive\nto use and their output are easy to interpret, such that users are protected\nfrom being exposed to complex Bayesian hierarchical modelling tasks. In\naddition, the package includes plotting functions to display data and results\nin a visually appealing manner. The models are implemented in Stan modelling\nlanguage, which provides efficient sampling technique to obtain posterior\nsamples. This vignette briefly introduces different models, and provides a\nshort walk-through analysis with example data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:16:27 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wang", "Craig", ""], ["Furrer", "Reinhard", ""]]}]