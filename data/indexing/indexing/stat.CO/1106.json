[{"id": "1106.0321", "submitter": "Anatoli Juditsky B.", "authors": "Elmar Diederichs, Anatoli Juditsky, Arkadi Nemirovski, and Vladimir\n  Spokoiny", "title": "Sparse Non Gaussian Component Analysis by Semidefinite Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse non-Gaussian component analysis (SNGCA) is an unsupervised method of\nextracting a linear structure from a high dimensional data based on estimating\na low-dimensional non-Gaussian data component. In this paper we discuss a new\napproach to direct estimation of the projector on the target space based on\nsemidefinite programming which improves the method sensitivity to a broad\nvariety of deviations from normality. We also discuss the procedures which\nallows to recover the structure when its effective dimension is unknown.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 21:04:00 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2011 21:26:58 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2012 23:32:26 GMT"}], "update_date": "2012-01-17", "authors_parsed": [["Diederichs", "Elmar", ""], ["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1106.0322", "submitter": "Anthony Lee", "authors": "Anthony Lee, Francois Caron, Arnaud Doucet, Chris Holmes", "title": "Bayesian Sparsity-Path-Analysis of Genetic Association Signal using\n  Generalized t Priors", "comments": null, "journal-ref": "Statistical Applications in Genetics and Molecular Biology. 11(2)\n  (2012)", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the use of generalized t priors on regression coefficients to help\nunderstand the nature of association signal within \"hit regions\" of genome-wide\nassociation studies. The particular generalized t distribution we adopt is a\nStudent distribution on the absolute value of its argument. For low degrees of\nfreedom we show that the generalized t exhibits 'sparsity-prior' properties\nwith some attractive features over other common forms of sparse priors and\nincludes the well known double-exponential distribution as the degrees of\nfreedom tends to infinity. We pay particular attention to graphical\nrepresentations of posterior statistics obtained from sparsity-path-analysis\n(SPA) where we sweep over the setting of the scale (shrinkage / precision)\nparameter in the prior to explore the space of posterior models obtained over a\nrange of complexities, from very sparse models with all coefficient\ndistributions heavily concentrated around zero, to models with diffuse priors\nand coefficients distributed around their maximum likelihood estimates. The SPA\nplots are akin to LASSO plots of maximum a posteriori (MAP) estimates but they\ncharacterise the complete marginal posterior distributions of the coefficients\nplotted as a function of the precision of the prior. Generating posterior\ndistributions over a range of prior precisions is computationally challenging\nbut naturally amenable to sequential Monte Carlo (SMC) algorithms indexed on\nthe scale parameter. We show how SMC simulation on graphic-processing-units\n(GPUs) provides very efficient inference for SPA. We also present a\nscale-mixture representation of the generalized t prior that leads to an EM\nalgorithm to obtain MAP estimates should only these be required.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 21:06:50 GMT"}], "update_date": "2012-10-26", "authors_parsed": [["Lee", "Anthony", ""], ["Caron", "Francois", ""], ["Doucet", "Arnaud", ""], ["Holmes", "Chris", ""]]}, {"id": "1106.1733", "submitter": "Morteza Amini", "authors": "Morteza Amini, M. Mehdizadeh and N. R. Arghami", "title": "Improved estimator of the entropy and goodness of fit tests in ranked\n  set sampling", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The entropy is one of the most applicable uncertainty measures in many\nstatistical and en- gineering problems. In statistical literature, the entropy\nis used in calculation of the Kullback- Leibler (KL) information which is a\npowerful mean for performing goodness of fit tests. Ranked Set Sampling (RSS)\nseems to provide improved estimators of many parameters of the popu- lation in\nthe huge studied problems in the literature. It is developed for situations\nwhere the variable of interest is difficult or expensive to measure, but where\nranking in small sub-samples is easy. In This paper, we introduced two\nestimators for the entropy and compare them with each other and the estimator\nof the entropy in Simple Random Sampling (SRS) in the sense of bias and Root of\nMean Square Errors (RMSE). It is observed that the RSS scheme would improve\nthis estimator. The best estimator of the entropy is used along with the\nestimator of the mean and two biased and unbiased estimators of variance based\non RSS scheme, to esti- mate the KL information and perform goodness of fit\ntests for exponentiality and normality. The desired critical values and powers\nare calculated. It is also observed that RSS estimators would increase powers.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 08:05:05 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Amini", "Morteza", ""], ["Mehdizadeh", "M.", ""], ["Arghami", "N. R.", ""]]}, {"id": "1106.1980", "submitter": "David  Bolin", "authors": "David Bolin and Finn Lindgren", "title": "How do Markov approximations compare with other methods for large\n  spatial data sets?", "comments": "Updated title and revised Section 4 to clarify the simulation setup", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mat\\'ern covariance function is a popular choice for modeling dependence\nin spatial environmental data. Standard Mat\\'ern covariance models are,\nhowever, often computationally infeasible for large data sets. In this work,\nrecent results for Markov approximations of Gaussian Mat\\'{e}rn fields based on\nHilbert space approximations are extended using wavelet basis functions. These\nMarkov approximations are compared with two of the most popular methods for\nefficient covariance approximations; covariance tapering and the process\nconvolution method. The results show that, for a given computational cost, the\nMarkov methods have a substantial gain in accuracy compared with the other\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 08:46:53 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2011 14:48:05 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bolin", "David", ""], ["Lindgren", "Finn", ""]]}, {"id": "1106.2125", "submitter": "Art B. Owen", "authors": "Art B. Owen, Dean Eckles", "title": "Bootstrapping data arrays of arbitrary order", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS547 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 895-927", "doi": "10.1214/12-AOAS547", "report-no": "IMS-AOAS-AOAS547", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study a bootstrap strategy for estimating the variance of a\nmean taken over large multifactor crossed random effects data sets. We apply\nbootstrap reweighting independently to the levels of each factor, giving each\nobservation the product of independently sampled factor weights. No exact\nbootstrap exists for this problem [McCullagh (2000) Bernoulli 6 285-301]. We\nshow that the proposed bootstrap is mildly conservative, meaning biased toward\noverestimating the variance, under sufficient conditions that allow very\nunbalanced and heteroscedastic inputs. Earlier results for a resampling\nbootstrap only apply to two factors and use multinomial weights that are poorly\nsuited to online computation. The proposed reweighting approach can be\nimplemented in parallel and online settings. The results for this method apply\nto any number of factors. The method is illustrated using a 3 factor data set\nof comment lengths from Facebook.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 17:26:33 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2012 01:28:14 GMT"}, {"version": "v3", "created": "Thu, 27 Sep 2012 11:07:10 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Owen", "Art B.", ""], ["Eckles", "Dean", ""]]}, {"id": "1106.2525", "submitter": "Sumeetpal S Singh", "authors": "Pierre Del Moral, Arnaud Doucet and Sumeetpal Singh", "title": "Uniform Stability of a Particle Approximation of the Optimal Filter\n  Derivative", "comments": null, "journal-ref": null, "doi": null, "report-no": "Cambridge University Engineering Department Technical Report\n  CUED/F-INFENG/TR.668", "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo methods, also known as particle methods, are a widely\nused set of computational tools for inference in non-linear non-Gaussian\nstate-space models. In many applications it may be necessary to compute the\nsensitivity, or derivative, of the optimal filter with respect to the static\nparameters of the state-space model; for instance, in order to obtain maximum\nlikelihood model parameters of interest, or to compute the optimal controller\nin an optimal control problem. In Poyiadjis et al. [2011] an original particle\nalgorithm to compute the filter derivative was proposed and it was shown using\nnumerical examples that the particle estimate was numerically stable in the\nsense that it did not deteriorate over time. In this paper we substantiate this\nclaim with a detailed theoretical study. Lp bounds and a central limit theorem\nfor this particle approximation of the filter derivative are presented. It is\nfurther shown that under mixing conditions these Lp bounds and the asymptotic\nvariance characterized by the central limit theorem are uniformly bounded with\nrespect to the time index. We demon- strate the performance predicted by theory\nwith several numerical examples. We also use the particle approximation of the\nfilter derivative to perform online maximum likelihood parameter estimation for\na stochastic volatility model.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 19:09:05 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Del Moral", "Pierre", ""], ["Doucet", "Arnaud", ""], ["Singh", "Sumeetpal", ""]]}, {"id": "1106.2887", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi, Fateh Chebana, and Abdelhakim Necir", "title": "Copula representation of bivariate L-moments : A new estimation method\n  for multiparameter 2-dimentional copula models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Serfling and Xiao (2007) extended the L-moment theory (Hosking,\n1990) to the multivariate setting. In the present paper, we focus on the\ntwo-dimension random vectors to establish a link between the bivariate\nL-moments (BLM) and the underlying bivariate copula functions. This connection\nprovides a new estimate of dependence parameters of bivariate statistical data.\nConsistency and asymptotic normality of the proposed estimator are established.\nExtensive simulation study is carried out to compare estimators based on the\nBLM, the maximum likelihood, the minimum distance and rank approximate\nZ-estimation. The obtained results show that, when the sample size increases,\nBLM-based estimation performs better as far as the bias and computation time\nare concerned. Moreover, the root mean squared error (RMSE) is quite reasonable\nand less sensitive in general to outliers than those of the above cited\nmethods. Further, we expect that the BLM method is an easy-to-use tool for the\nestimation of multiparameter copula models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 06:33:39 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 11:06:09 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Brahimi", "Brahim", ""], ["Chebana", "Fateh", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1106.3562", "submitter": "Hidemaro Suwa", "authors": "Hidemaro Suwa and Synge Todo", "title": "Geometric Allocation Approach for Transition Kernel of Markov Chain", "comments": "9 pages, 3 figures, submitted to proceedings of Eighth IMACS Seminar\n  on Monte Carlo Methods, to be published in Monte Carlo Methods and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP math.NA physics.comp-ph q-fin.CP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new geometric approach that constructs a transition kernel of\nMarkov chain. Our method always minimizes the average rejection rate and even\nreduce it to zero in many relevant cases, which cannot be achieved by\nconventional methods, such as the Metropolis-Hastings algorithm or the heat\nbath algorithm (Gibbs sampler). Moreover, the geometric approach makes it\npossible to find not only a reversible but also an irreversible solution of\nrejection-free transition probabilities. This is the first versatile method\nthat can construct an irreversible transition kernel in general cases. We\ndemonstrate that the autocorrelation time (asymptotic variance) of the Potts\nmodel becomes more than 6 times as short as that by the conventional\nMetropolis-Hastings algorithm. Our algorithms are applicable to almost all\nkinds of Markov chain Monte Carlo methods and will improve the efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 15:39:08 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2012 23:47:39 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Suwa", "Hidemaro", ""], ["Todo", "Synge", ""]]}, {"id": "1106.3885", "submitter": "Ryan Martin", "authors": "Ryan Martin and Surya T. Tokdar", "title": "A nonparametric empirical Bayes framework for large-scale multiple\n  testing", "comments": "18 pages, 4 figures, 3 tables", "journal-ref": "Biostatistics 13(3):427-439, 2012", "doi": "10.1093/biostatistics/kxr039", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible and identifiable version of the two-groups model,\nmotivated by hierarchical Bayes considerations, that features an empirical null\nand a semiparametric mixture model for the non-null cases. We use a\ncomputationally efficient predictive recursion marginal likelihood procedure to\nestimate the model parameters, even the nonparametric mixing distribution. This\nleads to a nonparametric empirical Bayes testing procedure, which we call\nPRtest, based on thresholding the estimated local false discovery rates.\nSimulations and real-data examples demonstrate that, compared to existing\napproaches, PRtest's careful handling of the non-null density can give a much\nbetter fit in the tails of the mixture distribution which, in turn, can lead to\nmore realistic conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 13:02:46 GMT"}, {"version": "v2", "created": "Sun, 14 Aug 2011 15:56:42 GMT"}, {"version": "v3", "created": "Sat, 1 Oct 2011 14:54:14 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Martin", "Ryan", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1106.4333", "submitter": "Alfredo A. Kalaitzis Mr", "authors": "Alfredo A. Kalaitzis and Neil D. Lawrence", "title": "Residual Component Analysis", "comments": "9 pages, 8 figures, submitted to NIPS2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic principal component analysis (PPCA) seeks a low dimensional\nrepresentation of a data set in the presence of independent spherical Gaussian\nnoise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an\neigenvalue problem on the sample covariance matrix. In this paper we consider\nthe situation where the data variance is already partially explained by other\nfactors, e.g. covariates of interest, or temporal correlations leaving some\nresidual variance. We decompose the residual variance into its components\nthrough a generalized eigenvalue problem, which we call residual component\nanalysis (RCA). We show that canonical covariates analysis (CCA) is a special\ncase of our algorithm and explore a range of new algorithms that arise from the\nframework. We illustrate the ideas on a gene expression time series data set\nand the recovery of human pose from silhouette.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 21:17:51 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Kalaitzis", "Alfredo A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1106.4432", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "An approximate Bayesian marginal likelihood approach for estimating\n  finite mixtures", "comments": "16 pages, 1 figure, 3 tables", "journal-ref": "Communications in Statistics--Computation and Simulation, 42(7),\n  1533-1548, 2013", "doi": "10.1080/03610918.2012.667476", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of finite mixture models when the mixing distribution support is\nunknown is an important problem. This paper gives a new approach based on a\nmarginal likelihood for the unknown support. Motivated by a Bayesian Dirichlet\nprior model, a computationally efficient stochastic approximation version of\nthe marginal likelihood is proposed and large-sample theory is presented. By\nrestricting the support to a finite grid, a simulated annealing method is\nemployed to maximize the marginal likelihood and estimate the support. Real and\nsimulated data examples show that this novel stochastic\napproximation--simulated annealing procedure compares favorably to existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 12:25:10 GMT"}, {"version": "v2", "created": "Fri, 14 Oct 2011 17:16:21 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 14:52:48 GMT"}, {"version": "v4", "created": "Mon, 13 Feb 2012 14:43:18 GMT"}], "update_date": "2013-02-11", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1106.4739", "submitter": "Krzysztof {\\L}atuszy\\'{n}ski", "authors": "Krzysztof {\\L}atuszy\\'nski, B{\\l}a\\.zej Miasojedow, Wojciech Niemiro", "title": "Nonasymptotic bounds on the estimation error of MCMC algorithms", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ442 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm). arXiv admin\n  note: text overlap with arXiv:0907.4915", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5A, 2033-2066", "doi": "10.3150/12-BEJ442", "report-no": "IMS-BEJ-BEJ442", "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of upper bounding the mean square error of MCMC\nestimators. Our analysis is nonasymptotic. We first establish a general result\nvalid for essentially all ergodic Markov chains encountered in Bayesian\ncomputation and a possibly unbounded target function $f$. The bound is sharp in\nthe sense that the leading term is exactly $\\sigma_{\\mathrm {as}}^2(P,f)/n$,\nwhere $\\sigma_{\\mathrm{as}}^2(P,f)$ is the CLT asymptotic variance. Next, we\nproceed to specific additional assumptions and give explicit computable bounds\nfor geometrically and polynomially ergodic Markov chains under quantitative\ndrift conditions. As a corollary, we provide results on confidence estimation.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 14:35:04 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2012 10:35:24 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2013 10:09:55 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Niemiro", "Wojciech", ""]]}, {"id": "1106.5348", "submitter": "Elvira Romano Dr", "authors": "Elvira Romano, Antonio Balzanella, Rosanna Verde", "title": "Revealing spatial variability structures of geostatistical functional\n  data via Dynamic Clustering", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several environmental applications data are functions of time, essentially\ncon- tinuous, observed and recorded discretely, and spatially correlated. Most\nof the methods for analyzing such data are extensions of spatial statistical\ntools which deal with spatially dependent functional data. In such framework,\nthis paper introduces a new clustering method. The main features are that it\nfinds groups of functions that are similar to each other in terms of their\nspatial functional variability and that it locates a set of centers which\nsummarize the spatial functional variability of each cluster. The method\noptimizes, through an iterative algorithm, a best fit criterion between the\npartition of the curves and the representative element of the clusters, assumed\nto be a variogram function. The performance of the proposed clustering method\nwas evaluated by studying the results obtained through the application on\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 10:24:32 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Romano", "Elvira", ""], ["Balzanella", "Antonio", ""], ["Verde", "Rosanna", ""]]}, {"id": "1106.5850", "submitter": "Somak Dutta", "authors": "Somak Dutta and Sourabh Bhattacharya", "title": "Markov Chain Monte Carlo Based on Deterministic Transformations", "comments": "28 pages, 3 figures; Longer abstract inside article", "journal-ref": "Statistical Methodology, 16, 100-116, 2014", "doi": "10.1016/j.stamet.2013.08.006", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we propose a novel MCMC method based on deterministic\ntransformations T: X x D --> X where X is the state-space and D is some set\nwhich may or may not be a subset of X. We refer to our new methodology as\nTransformation-based Markov chain Monte Carlo (TMCMC). One of the remarkable\nadvantages of our proposal is that even if the underlying target distribution\nis very high-dimensional, deterministic transformation of a one-dimensional\nrandom variable is sufficient to generate an appropriate Markov chain that is\nguaranteed to converge to the high-dimensional target distribution. Apart from\nclearly leading to massive computational savings, this idea of\ndeterministically transforming a single random variable very generally leads to\nexcellent acceptance rates, even though all the random variables associated\nwith the high-dimensional target distribution are updated in a single block.\nSince it is well-known that joint updating of many random variables using\nMetropolis-Hastings (MH) algorithm generally leads to poor acceptance rates,\nTMCMC, in this regard, seems to provide a significant advance. We validate our\nproposal theoretically, establishing the convergence properties. Furthermore,\nwe show that TMCMC can be very effectively adopted for simulating from doubly\nintractable distributions.\n  TMCMC is compared with MH using the well-known Challenger data, demonstrating\nthe effectiveness of of the former in the case of highly correlated variables.\nMoreover, we apply our methodology to a challenging posterior simulation\nproblem associated with the geostatistical model of Diggle et al. (1998),\nupdating 160 unknown parameters jointly, using a deterministic transformation\nof a one-dimensional random variable. Remarkable computational savings as well\nas good convergence properties and acceptance rates are the results.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 06:41:42 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 05:39:53 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2013 20:17:11 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Dutta", "Somak", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1106.5941", "submitter": "Babak Shahbaba", "authors": "Babak Shahbaba, Shiwei Lan, Wesley O. Johnson, and Radford M. Neal", "title": "Split Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the Hamiltonian Monte Carlo algorithm can sometimes be speeded up\nby \"splitting\" the Hamiltonian in a way that allows much of the movement around\nthe state space to be done at low computational cost. One context where this is\npossible is when the log density of the distribution of interest (the potential\nenergy function) can be written as the log of a Gaussian density, which is a\nquadratic function, plus a slowly varying function. Hamiltonian dynamics for\nquadratic energy functions can be analytically solved. With the splitting\ntechnique, only the slowly-varying part of the energy needs to be handled\nnumerically, and this can be done with a larger stepsize (and hence fewer\nsteps) than would be necessary with a direct simulation of the dynamics.\nAnother context where splitting helps is when the most important terms of the\npotential energy function and its gradient can be evaluated quickly, with only\na slowly-varying part requiring costly computations. With splitting, the quick\nportion can be handled with a small stepsize, while the costly portion uses a\nlarger stepsize. We show that both of these splitting approaches can reduce the\ncomputational cost of sampling from the posterior distribution for a logistic\nregression model, using either a Gaussian approximation centered on the\nposterior mode, or a Hamiltonian split into a term that depends on only a small\nnumber of critical cases, and another term that involves the larger number of\ncases whose influence on the posterior distribution is small. Supplemental\nmaterials for this paper are available online.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 14:14:34 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2012 06:41:31 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2012 14:07:46 GMT"}], "update_date": "2012-07-17", "authors_parsed": [["Shahbaba", "Babak", ""], ["Lan", "Shiwei", ""], ["Johnson", "Wesley O.", ""], ["Neal", "Radford M.", ""]]}, {"id": "1106.6280", "submitter": "Sarah Filippi", "authors": "Sarah Filippi and Chris Barnes and Julien Cornebise and Michael P.H.\n  Stumpf", "title": "On optimality of kernels for approximate Bayesian computation using\n  sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) has gained popularity over the past\nfew years for the analysis of complex models arising in population genetic,\nepidemiology and system biology. Sequential Monte Carlo (SMC) approaches have\nbecome work horses in ABC. Here we discuss how to construct the perturbation\nkernels that are required in ABC SMC approaches, in order to construct a set of\ndistributions that start out from a suitably defined prior and converge towards\nthe unknown posterior. We derive optimality criteria for different kernels,\nwhich are based on the Kullback-Leibler divergence between a distribution and\nthe distribution of the perturbed particles. We will show that for many\ncomplicated posterior distributions, locally adapted kernels tend to show the\nbest performance. In cases where it is possible to estimate the Fisher\ninformation we can construct particularly efficient perturbation kernels. We\nfind that the added moderate cost of adapting kernel functions is easily\nregained in terms of the higher acceptance rate. We demonstrate the\ncomputational efficiency gains in a range of toy-examples which illustrate some\nof the challenges faced in real-world applications of ABC, before turning to\ntwo demanding parameter inference problem in molecular biology, which highlight\nthe huge increases in efficiency that can be gained from choice of optimal\nmodels. We conclude with a general discussion of rational choice of\nperturbation kernels in ABC SMC settings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 15:54:26 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2011 08:27:53 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2011 16:47:44 GMT"}, {"version": "v4", "created": "Mon, 15 Oct 2012 14:52:40 GMT"}], "update_date": "2012-10-16", "authors_parsed": [["Filippi", "Sarah", ""], ["Barnes", "Chris", ""], ["Cornebise", "Julien", ""], ["Stumpf", "Michael P. H.", ""]]}, {"id": "1106.6281", "submitter": "Sarah Filippi", "authors": "Chris Barnes and Sarah Filippi and Michael P.H. Stumpf and Thomas\n  Thorne", "title": "Considerate Approaches to Achieving Sufficiency for ABC model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For nearly any challenging scientific problem evaluation of the likelihood is\nproblematic if not impossible. Approximate Bayesian computation (ABC) allows us\nto employ the whole Bayesian formalism to problems where we can use simulations\nfrom a model, but cannot evaluate the likelihood directly. When summary\nstatistics of real and simulated data are compared --- rather than the data\ndirectly --- information is lost, unless the summary statistics are sufficient.\nHere we employ an information-theoretical framework that can be used to\nconstruct (approximately) sufficient statistics by combining different\nstatistics until the loss of information is minimized. Such sufficient sets of\nstatistics are constructed for both parameter estimation and model selection\nproblems. We apply our approach to a range of illustrative and real-world model\nselection problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 15:58:23 GMT"}, {"version": "v2", "created": "Fri, 1 Jul 2011 08:25:21 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Barnes", "Chris", ""], ["Filippi", "Sarah", ""], ["Stumpf", "Michael P. H.", ""], ["Thorne", "Thomas", ""]]}]