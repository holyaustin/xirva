[{"id": "1006.0042", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Computing the confidence levels for a root-mean-square test of\n  goodness-of-fit", "comments": "19 pages, 8 figures, 3 tables", "journal-ref": "Applied Mathematics and Computation, 217 (22): 9072-9084, 2011", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic chi-squared statistic for testing goodness-of-fit has long been a\ncornerstone of modern statistical practice. The statistic consists of a sum in\nwhich each summand involves division by the probability associated with the\ncorresponding bin in the distribution being tested for goodness-of-fit.\nTypically this division should precipitate rebinning to uniformize the\nprobabilities associated with the bins, in order to make the test reasonably\npowerful. With the now widespread availability of computers, there is no longer\nany need for this. The present paper provides efficient black-box algorithms\nfor calculating the asymptotic confidence levels of a variant on the classic\nchi-squared test which omits the problematic division. In many circumstances,\nit is also feasible to compute the exact confidence levels via Monte Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 00:42:24 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 14:37:23 GMT"}, {"version": "v3", "created": "Tue, 9 Nov 2010 19:10:29 GMT"}, {"version": "v4", "created": "Thu, 2 Dec 2010 19:25:34 GMT"}, {"version": "v5", "created": "Mon, 6 Dec 2010 19:59:29 GMT"}, {"version": "v6", "created": "Wed, 12 Jan 2011 17:19:00 GMT"}, {"version": "v7", "created": "Mon, 7 Mar 2011 20:35:09 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1006.0554", "submitter": "Christian P. Robert", "authors": "Nicolas Chopin (CREST, Paris), Alessandra Iacobucci (Paris-Dauphine),\n  Jean-Michel Marin (I3M, Montpellier 2), Kerrie Mengersen (QUT), Christian P.\n  Robert (Paris-Dauphine and CREST), Robin Ryder (Paris-Dauphine and CREST),\n  and Christian Sch\\\"afer (Paris-Dauphine and CREST)", "title": "On Particle Learning", "comments": "14 pages, 9 figures, discussions on the invited paper of Lopes,\n  Carvalho, Johannes, and Polson, for the Ninth Valencia International Meeting\n  on Bayesian Statistics, held in Benidorm, Spain, on June 3-8, 2010. To appear\n  in Bayesian Statistics 9, Oxford University Press (except for the final\n  discussion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the aggregation of six discussions of Lopes et al. (2010)\nthat we submitted to the proceedings of the Ninth Valencia Meeting, held in\nBenidorm, Spain, on June 3-8, 2010, in conjunction with Hedibert Lopes' talk at\nthis meeting, and of a further discussion of the rejoinder by Lopes et al.\n(2010). The main point in those discussions is the potential for degeneracy in\nthe particle learning methodology, related with the exponential forgetting of\nthe past simulations. We illustrate in particular the resulting difficulties in\nthe case of mixtures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 05:08:16 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 16:42:16 GMT"}, {"version": "v3", "created": "Fri, 19 Nov 2010 08:32:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chopin", "Nicolas", "", "CREST, Paris"], ["Iacobucci", "Alessandra", "", "Paris-Dauphine"], ["Marin", "Jean-Michel", "", "I3M, Montpellier 2"], ["Mengersen", "Kerrie", "", "QUT"], ["Robert", "Christian P.", "", "Paris-Dauphine and CREST"], ["Ryder", "Robin", "", "Paris-Dauphine and CREST"], ["Sch\u00e4fer", "Christian", "", "Paris-Dauphine and CREST"]]}, {"id": "1006.0764", "submitter": "Peter Ruckdeschel", "authors": "Peter Ruckdeschel and Matthias Kohl", "title": "General Purpose Convolution Algorithm in S4-Classes by means of FFT", "comments": null, "journal-ref": "J. Statist. Softw. 59(4), 1--25, 2014", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object orientation provides a flexible framework for the implementation of\nthe convolution of arbitrary distributions of real-valued random variables.\n  We discuss an algorithm which is based on the discrete Fourier transformation\n(DFT) and its fast computability via the fast Fourier transformation (FFT). It\ndirectly applies to lattice-supported distributions. In the case of continuous\ndistributions an additional discretization to a linear lattice is necessary and\nthe resulting lattice-supported distributions are suitably smoothed after\nconvolution.\n  We compare our algorithm to other approaches aiming at a similar generality\nas to accuracy and speed. In situations where the exact results are known,\nseveral checks confirm a high accuracy of the proposed algorithm which is also\nillustrated at approximations of non-central $\\chi^2$-distributions.\n  By means of object orientation this default algorithm can be overloaded by\nmore specific algorithms where possible, in particular where explicit\nconvolution formulae are available.\n  Our focus is on R package distr which includes an implementation of this\napproach overloading operator \"+\" for convolution; based on this convolution,\nwe define a whole arithmetics of mathematical operations acting on distribution\nobjects, comprising, among others, operators \"+\", \"-\", \"*\", \"/\", and \"^\".\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 01:23:03 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 13:52:28 GMT"}, {"version": "v3", "created": "Wed, 24 Oct 2012 01:15:08 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Ruckdeschel", "Peter", ""], ["Kohl", "Matthias", ""]]}, {"id": "1006.0868", "submitter": "Iain Murray", "authors": "Iain Murray and Ryan Prescott Adams", "title": "Slice sampling covariance hyperparameters of latent Gaussian models", "comments": "9 pages, 4 figures, 4 algorithms. Minor corrections to previous\n  version. This version to appear in Advances in Neural Information Processing\n  Systems (NIPS) 23, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process (GP) is a popular way to specify dependencies between\nrandom variables in a probabilistic model. In the Bayesian framework the\ncovariance structure can be specified using unknown hyperparameters.\nIntegrating over these hyperparameters considers different possible\nexplanations for the data when making predictions. This integration is often\nperformed using Markov chain Monte Carlo (MCMC) sampling. However, with\nnon-Gaussian observations standard hyperparameter sampling approaches require\ncareful tuning and may converge slowly. In this paper we present a slice\nsampling approach that requires little tuning while mixing well in both strong-\nand weak-data regimes.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 11:32:16 GMT"}, {"version": "v2", "created": "Thu, 28 Oct 2010 20:47:52 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Murray", "Iain", ""], ["Adams", "Ryan Prescott", ""]]}, {"id": "1006.1015", "submitter": "Susan Holmes", "authors": "John Chakerian and Susan Holmes", "title": "Computational Tools for Evaluating Phylogenetic and Hierarchical\n  Clustering Trees", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferential summaries of tree estimates are useful in the setting of\nevolutionary biology, where phylogenetic trees have been built from DNA data\nsince the 1960's. In bioinformatics, psychometrics and data mining,\nhierarchical clustering techniques output the same mathematical objects, and\npractitioners have similar questions about the stability and `generalizability'\nof these summaries. This paper provides an implementation of the geometric\ndistance between trees developed by Billera, Holmes and Vogtmann (2001) [BHV]\nequally applicable to phylogenetic trees and hieirarchical clustering trees,\nand shows some of the applications in statistical inference for which this\ndistance can be useful. In particular, since BHV have shown that the space of\ntrees is negatively curved (a CAT(0) space), a natural representation of a\ncollection of trees is a tree. We compare this representation to the Euclidean\napproximations of treespace made available through Multidimensional Scaling of\nthe matrix of distances between trees. We also provide applications of the\ndistances between trees to hierarchical clustering trees constructed from\nmicroarrays. Our method gives a new way of evaluating the influence both of\ncertain columns (positions, variables or genes) and of certain rows (whether\nspecies, observations or arrays).\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 01:26:59 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Chakerian", "John", ""], ["Holmes", "Susan", ""]]}, {"id": "1006.1860", "submitter": "Jan C. Neddermeyer", "authors": "Rainer Dahlhaus and Jan C. Neddermeyer", "title": "On-line Spot Volatility-Estimation and Decomposition with Nonlinear\n  Market Microstructure Noise Models", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for on-line estimation of spot volatility for high-frequency data\nis developed. The algorithm works directly on the transaction data and updates\nthe volatility estimate immediately after the occurrence of a new transaction.\nFurthermore, a nonlinear market microstructure noise model is proposed that\nreproduces several stylized facts of high-frequency data. A computationally\nefficient particle filter is used that allows for the approximation of the\nunknown efficient prices and, in combination with a recursive EM algorithm, for\nthe estimation of the volatility curve. We neither assume that the transaction\ntimes are equidistant nor do we use interpolated prices. We also make a\ndistinction between volatility per time unit and volatility per transaction and\nprovide estimators for both. More precisely we use a model with random time\nchange where spot volatility is decomposed into spot volatility per transaction\ntimes the trading intensity - thus highlighting the influence of trading\nintensity on volatility.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 17:33:56 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 19:27:37 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2012 18:45:28 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2013 21:14:28 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1006.2592", "submitter": "Yiyuan She", "authors": "Yiyuan She and Art B. Owen", "title": "Outlier Detection Using Nonconvex Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the outlier detection problem from the point of view of\npenalized regressions. Our regression model adds one mean shift parameter for\neach of the $n$ data points. We then apply a regularization favoring a sparse\nvector of mean shift parameters. The usual $L_1$ penalty yields a convex\ncriterion, but we find that it fails to deliver a robust estimator. The $L_1$\npenalty corresponds to soft thresholding. We introduce a thresholding (denoted\nby $\\Theta$) based iterative procedure for outlier detection ($\\Theta$-IPOD). A\nversion based on hard thresholding correctly identifies outliers on some hard\ntest problems. We find that $\\Theta$-IPOD is much faster than iteratively\nreweighted least squares for large data because each iteration costs at most\n$O(np)$ (and sometimes much less) avoiding an $O(np^2)$ least squares estimate.\nWe describe the connection between $\\Theta$-IPOD and $M$-estimators. Our\nproposed method has one tuning parameter with which to both identify outliers\nand estimate regression coefficients. A data-dependent choice can be made based\non BIC. The tuned $\\Theta$-IPOD shows outstanding performance in identifying\noutliers in various situations in comparison to other existing approaches. This\nmethodology extends to high-dimensional modeling with $p\\gg n$, if both the\ncoefficient vector and the outlier pattern are sparse.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 02:51:41 GMT"}, {"version": "v2", "created": "Thu, 30 Sep 2010 19:04:02 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2011 02:23:15 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["She", "Yiyuan", ""], ["Owen", "Art B.", ""]]}, {"id": "1006.2940", "submitter": "Zhou Fang", "authors": "Zhou Fang and Nicolai Meinshausen", "title": "LASSO ISOtone for High Dimensional Additive Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive isotonic regression attempts to determine the relationship between a\nmulti-dimensional observation variable and a response, under the constraint\nthat the estimate is the additive sum of univariate component effects that are\nmonotonically increasing. In this article, we present a new method for such\nregression called LASSO Isotone (LISO). LISO adapts ideas from sparse linear\nmodelling to additive isotonic regression. Thus, it is viable in many\nsituations with high dimensional predictor variables, where selection of\nsignificant versus insignificant variables are required. We suggest an\nalgorithm involving a modification of the backfitting algorithm CPAV. We give a\nnumerical convergence result, and finally examine some of its properties\nthrough simulations. We also suggest some possible extensions that improve\nperformance, and allow calculation to be carried out when the direction of the\nmonotonicity is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 09:52:14 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Fang", "Zhou", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1006.3002", "submitter": "Pierre Jacob", "authors": "Nicolas Chopin, Pierre Jacob", "title": "Free energy Sequential Monte Carlo, application to mixture modelling", "comments": "presented at \"Bayesian Statistics 9\" (Valencia meetings, 4-8 June\n  2010, Benidorm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of Sequential Monte Carlo (SMC) methods, which we\ncall free energy SMC. This class is inspired by free energy methods, which\noriginate from Physics, and where one samples from a biased distribution such\nthat a given function $\\xi(\\theta)$ of the state $\\theta$ is forced to be\nuniformly distributed over a given interval. From an initial sequence of\ndistributions $(\\pi_t)$ of interest, and a particular choice of $\\xi(\\theta)$,\na free energy SMC sampler computes sequentially a sequence of biased\ndistributions $(\\tilde{\\pi}_{t})$ with the following properties: (a) the\nmarginal distribution of $\\xi(\\theta)$ with respect to $\\tilde{\\pi}_{t}$ is\napproximatively uniform over a specified interval, and (b) $\\tilde{\\pi}_{t}$\nand $\\pi_{t}$ have the same conditional distribution with respect to $\\xi$. We\napply our methodology to mixture posterior distributions, which are highly\nmultimodal. In the mixture context, forcing certain hyper-parameters to higher\nvalues greatly faciliates mode swapping, and makes it possible to recover a\nsymetric output. We illustrate our approach with univariate and bivariate\nGaussian mixtures and two real-world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 14:38:26 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Chopin", "Nicolas", ""], ["Jacob", "Pierre", ""]]}, {"id": "1006.5081", "submitter": "Piet Reegen", "authors": "Piet Reegen", "title": "SigSpec User's Manual", "comments": "99 pages, 42 figures, Communications in Asteroseismology -\n  Complementary Topics (in press)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  {\\sc SigSpec} computes the spectral significance levels for the DFT amplitude\nspectrum of a time series at arbitrarily given sampling. It is based on the\nanalytical solution for the Probability Density Function (PDF) of an amplitude\nlevel, including dependencies on frequency and phase and referring to white\nnoise. Using a time series dataset as input, an iterative procedure including\nstep-by-step prewhitening of the most significant signal components and\nMultiSine least-squares fitting is provided to determine a whole set of signal\ncomponents, which makes the program a powerful tool for multi-frequency\nanalysis. Instead of the step-by-step prewhitening of the most significant\npeaks, the program is also able to take into account several steps of the\nprewhitening sequence simultaneously and check for the combination associated\nto a minimum residual scatter. This option is designed to overcome the aliasing\nproblem caused by periodic time gaps in the dataset. {\\sc SigSpec} can detect\nnon-sinusoidal periodicities in a dataset by simultaneously taking into account\na fundamental frequency plus a set of harmonics. Time-resolved spectral\nsignificance analysis using a set of intervals of the time series is supported\nto investigate the development of eigenfrequencies over the observation time.\nFurthermore, an extension is available to perform the {\\sc SigSpec} analysis\nfor multiple time series input files at once. In this MultiFile mode, time\nseries may be tagged as target and comparison data. Based on this selection,\n{\\sc SigSpec} is capable of determining differential significance spectra for\nthe target datasets with respect to coincidences in the comparison spectra. A\nbuilt-in simulator to generate and superpose a variety of sinusoids and trends\nas well as different types of noise completes the software package at the\npresent stage of development.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 23:20:36 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Reegen", "Piet", ""]]}, {"id": "1006.5086", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye and Xiaohui Xie", "title": "Split Bregman method for large scale fused Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  rdering of regression or classification coefficients occurs in many\nreal-world applications. Fused Lasso exploits this ordering by explicitly\nregularizing the differences between neighboring coefficients through an\n$\\ell_1$ norm regularizer. However, due to nonseparability and nonsmoothness of\nthe regularization term, solving the fused Lasso problem is computationally\ndemanding. Existing solvers can only deal with problems of small or medium\nsize, or a special case of the fused Lasso problem in which the predictor\nmatrix is identity matrix. In this paper, we propose an iterative algorithm\nbased on split Bregman method to solve a class of large-scale fused Lasso\nproblems, including a generalized fused Lasso and a fused Lasso support vector\nclassifier. We derive our algorithm using augmented Lagrangian method and prove\nits convergence properties. The performance of our method is tested on both\nartificial data and real-world applications including proteomic data from mass\nspectrometry and genomic data from array CGH. We demonstrate that our method is\nmany times faster than the existing solvers, and show that it is especially\nefficient for large p, small n problems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2010 00:17:32 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Xie", "Xiaohui", ""]]}]