[{"id": "2002.00033", "submitter": "Leah F. South", "authors": "Leah F. South, Toni Karvonen, Chris Nemeth, Mark Girolami and Chris.\n  J. Oates", "title": "Semi-Exact Control Functionals From Sard's Method", "comments": "There are 17 pages of main text. This revision provides an extended\n  version of Theorem 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical approximation of posterior expected quantities of interest is\nconsidered. A novel control variate technique is proposed for post-processing\nof Markov chain Monte Carlo output, based both on Stein's method and an\napproach to numerical integration due to Sard. The resulting estimators are\nproven to be polynomially exact in the Gaussian context, while empirical\nresults suggest the estimators approximate a Gaussian cubature method near the\nBernstein-von-Mises limit. The main theoretical result establishes a\nbias-correction property in settings where the Markov chain does not leave the\nposterior invariant. Empirical results are presented across a selection of\nBayesian inference tasks. All methods used in this paper are available in the R\npackage ZVCV.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 19:27:04 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 17:24:46 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 06:11:17 GMT"}, {"version": "v4", "created": "Thu, 6 May 2021 06:37:40 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["South", "Leah F.", ""], ["Karvonen", "Toni", ""], ["Nemeth", "Chris", ""], ["Girolami", "Mark", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2002.00326", "submitter": "Michael Betancourt", "authors": "Michael Betancourt and Charles C. Margossian and Vianey Leos-Barajas", "title": "The Discrete Adjoint Method: Efficient Derivatives for Functions of\n  Discrete Sequences", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based techniques are becoming increasingly critical in quantitative\nfields, notably in statistics and computer science. The utility of these\ntechniques, however, ultimately depends on how efficiently we can evaluate the\nderivatives of the complex mathematical functions that arise in applications.\nIn this paper we introduce a discrete adjoint method that efficiently evaluates\nderivatives for functions of discrete sequences.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 05:02:27 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Betancourt", "Michael", ""], ["Margossian", "Charles C.", ""], ["Leos-Barajas", "Vianey", ""]]}, {"id": "2002.00351", "submitter": "Freeh Alenezi", "authors": "Freeh Alenezi and Chris. Tsokos", "title": "Bayesian Reliability Analysis of the Power Law Process with Respect to\n  the Higgins-Tsokos Loss Function for Modeling Software Failure Times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Power Law Process, also known as Non-Homogeneous Poisson Process, has\nbeen used in various aspects, one of which is the software reliability\nassessment. Specifically, by using its intensity function to compute the rate\nof change of a software reliability as time-varying function. Justification of\nBayesian analysis applicability to the Power Law Process was shown using real\ndata. The probability distribution that best characterizes the behavior of the\nkey parameter of the intensity function was first identified, then the\nlikelihood-based Bayesian reliability estimate of the Power Law Process under\nthe Higgins-Tsokos loss function was obtained. As a result of a simulation\nstudy and using real data, the Bayesian estimate shows an outstanding\nperformance compared to the maximum likelihood estimate using different sample\nsizes. In addition, a sensitivity analysis was performed, resulting in the\nBayesian estimate being sensitive to the prior selection; whether parametric or\nnon-parametric.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 08:34:32 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Alenezi", "Freeh", ""], ["Tsokos", "Chris.", ""]]}, {"id": "2002.00413", "submitter": "Yiyan Qi", "authors": "Yiyan Qi, Pinghui Wang, Yuanming Zhang, Junzhou Zhao, Guangjian Tian,\n  and Xiaohong Guan", "title": "Fast Generating A Large Number of Gumbel-Max Variables", "comments": "Accepted by WebConf2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known Gumbel-Max Trick for sampling elements from a categorical\ndistribution (or more generally a nonnegative vector) and its variants have\nbeen widely used in areas such as machine learning and information retrieval.\nTo sample a random element $i$ (or a Gumbel-Max variable $i$) in proportion to\nits positive weight $v_i$, the Gumbel-Max Trick first computes a Gumbel random\nvariable $g_i$ for each positive weight element $i$, and then samples the\nelement $i$ with the largest value of $g_i+\\ln v_i$. Recently, applications\nincluding similarity estimation and graph embedding require to generate $k$\nindependent Gumbel-Max variables from high dimensional vectors. However, it is\ncomputationally expensive for a large $k$ (e.g., hundreds or even thousands)\nwhen using the traditional Gumbel-Max Trick. To solve this problem, we propose\na novel algorithm, \\emph{FastGM}, that reduces the time complexity from\n$O(kn^+)$ to $O(k \\ln k + n^+)$, where $n^+$ is the number of positive elements\nin the vector of interest. Instead of computing $k$ independent Gumbel random\nvariables directly, we find that there exists a technique to generate these\nvariables in descending order. Using this technique, our method FastGM computes\nvariables $g_i+\\ln v_i$ for all positive elements $i$ in descending order. As a\nresult, FastGM significantly reduces the computation time because we can stop\nthe procedure of Gumbel random variables computing for many elements especially\nfor those with small weights. Experiments on a variety of real-world datasets\nshow that FastGM is orders of magnitude faster than state-of-the-art methods\nwithout sacrificing accuracy and incurring additional expenses.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 15:15:44 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Qi", "Yiyan", ""], ["Wang", "Pinghui", ""], ["Zhang", "Yuanming", ""], ["Zhao", "Junzhou", ""], ["Tian", "Guangjian", ""], ["Guan", "Xiaohong", ""]]}, {"id": "2002.00499", "submitter": "Cole Sodja", "authors": "Cole Sodja", "title": "Detecting Anomalous Time Series by GAMLSS-Akaike-Weights-Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An extensible statistical framework for detecting anomalous time series\nincluding those with heavy-tailed distributions and non-stationarity in\nhigher-order moments is introduced based on penalized likelihood distributional\nregression. Specifically, generalized additive models for location, scale, and\nshape are used to infer sample path representations defined by a parametric\ndistribution with parameters comprised of basis functions. Akaike weights are\nthen applied to each model and time series, yielding a probability measure that\ncan be effectively used to classify and rank anomalous time series. A\nmathematical exposition is also given to justify the proposed Akaike weight\nscoring under a suitable model embedding as a way to asymptotically identify\nanomalous time series. Studies evaluating the methodology on both multiple\nsimulations and real-world datasets also confirm that high accuracy can be\nobtained detecting many different and complex types of shape anomalies. Both\ncode implementing GAWS for running on a local machine and the datasets\nreferenced in this paper are available online.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 22:01:54 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sodja", "Cole", ""]]}, {"id": "2002.01019", "submitter": "Yu Wang", "authors": "Yu Wang, Nhu D. Le, James V. Zidek", "title": "Approximately Optimal Spatial Design: How Good is it?", "comments": "Accepted in Spatial Statistics, Special Issues in Spatial Data\n  Science", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100409", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing recognition of the association between adverse human health\nconditions and many environmental substances as well as processes has led to\nthe need to monitor them. An important problem that arises in environmental\nstatistics is the design of the locations of the monitoring stations for those\nenvironmental processes of interest. One particular design criterion for\nmonitoring networks that tries to reduce the uncertainty about predictions of\nunseen processes is called the maximum-entropy design. However, this design\ncriterion involves a hard optimization problem that is computationally\nintractable for large data sets. Previous work of Wang et al. (2017) examined a\nprobabilistic model that can be implemented efficiently to approximate the\nunderlying optimization problem. In this paper, we attempt to establish\nstatistically sound tools for assessing the quality of the approximations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 21:17:36 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Wang", "Yu", ""], ["Le", "Nhu D.", ""], ["Zidek", "James V.", ""]]}, {"id": "2002.01184", "submitter": "Junpeng Lao", "authors": "Junpeng Lao, Christopher Suter, Ian Langmore, Cyril Chimisov, Ashish\n  Saxena, Pavel Sountsov, Dave Moore, Rif A. Saurous, Matthew D. Hoffman, and\n  Joshua V. Dillon", "title": "tfp.mcmc: Modern Markov Chain Monte Carlo Tools Built for Modern\n  Hardware", "comments": "Based on extended abstract submitted to PROBPROG 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.PL stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is widely regarded as one of the most\nimportant algorithms of the 20th century. Its guarantees of asymptotic\nconvergence, stability, and estimator-variance bounds using only unnormalized\nprobability functions make it indispensable to probabilistic programming. In\nthis paper, we introduce the TensorFlow Probability MCMC toolkit, and discuss\nsome of the considerations that motivated its design.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:27:26 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Lao", "Junpeng", ""], ["Suter", "Christopher", ""], ["Langmore", "Ian", ""], ["Chimisov", "Cyril", ""], ["Saxena", "Ashish", ""], ["Sountsov", "Pavel", ""], ["Moore", "Dave", ""], ["Saurous", "Rif A.", ""], ["Hoffman", "Matthew D.", ""], ["Dillon", "Joshua V.", ""]]}, {"id": "2002.01270", "submitter": "Hamza M. Ruzayqat", "authors": "Hamza M. Ruzayqat, Ajay Jasra", "title": "Unbiased Estimation of the Solution to Zakai's Equation", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following article we consider the non-linear filtering problem in\ncontinuous-time and in particular the solution to Zakai's equation or the\nnormalizing constant. We develop a methodology to produce finite variance,\nalmost surely unbiased estimators of the solution to Zakai's equation. That is,\ngiven access to only a first order discretization of solution to the Zakai\nequation, we present a method which can remove this discretization bias. The\napproach, under assumptions, is proved to have finite variance and is\nnumerically compared to using a particular multilevel Monte Carlo method.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 13:08:22 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 06:44:34 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Ruzayqat", "Hamza M.", ""], ["Jasra", "Ajay", ""]]}, {"id": "2002.01290", "submitter": "Nora L\\\"uthen", "authors": "Nora L\\\"uthen, Stefano Marelli and Bruno Sudret", "title": "Sparse Polynomial Chaos Expansions: Literature Survey and Benchmark", "comments": null, "journal-ref": "SIAM/ASA J. Uncertainty Quantification (2021), 9(2), 593-649", "doi": "10.1137/20M1315774", "report-no": "RSUQ-2020-002C", "categories": "math.NA cs.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse polynomial chaos expansions (PCE) are a popular surrogate modelling\nmethod that takes advantage of the properties of PCE, the sparsity-of-effects\nprinciple, and powerful sparse regression solvers to approximate computer\nmodels with many input parameters, relying on only few model evaluations.\nWithin the last decade, a large number of algorithms for the computation of\nsparse PCE have been published in the applied math and engineering literature.\nWe present an extensive review of the existing methods and develop a framework\nfor classifying the algorithms. Furthermore, we conduct a unique benchmark on a\nselection of methods to identify which approaches work best in practical\napplications. Comparing their accuracy on several benchmark models of varying\ndimensionality and complexity, we find that the choice of sparse regression\nsolver and sampling scheme for the computation of a sparse PCE surrogate can\nmake a significant difference, of up to several orders of magnitude in the\nresulting mean-squared error. Different methods seem to be superior in\ndifferent regimes of model dimensionality and experimental design size.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 14:14:16 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 14:38:47 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2021 14:57:44 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 13:00:19 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["L\u00fcthen", "Nora", ""], ["Marelli", "Stefano", ""], ["Sudret", "Bruno", ""]]}, {"id": "2002.01431", "submitter": "Martino Trassinelli", "authors": "M. Trassinelli (INSP-E10, INSP), Pierre Ciccodicola (INSP-E10, INSP)", "title": "Mean shift cluster recognition method implementation in the nested\n  sampling algorithm", "comments": null, "journal-ref": null, "doi": "10.3390/e22020185", "report-no": null, "categories": "stat.CO astro-ph.IM physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nested sampling is an efficient algorithm for the calculation of the Bayesian\nevidence and posterior parameter probability distributions. It is based on the\nstep-by-step exploration of the parameter space by Monte Carlo sampling with a\nseries of values sets called live points that evolve towards the region of\ninterest, i.e. where the likelihood function is maximal. In presence of several\nlocal likelihood maxima, the algorithm converges with difficulty. Some\nsystematic errors can also be introduced by unexplored parameter volume\nregions. In order to avoid this, different methods are proposed in the\nliterature for an efficient search of new live points, even in presence of\nlocal maxima. Here we present a new solution based on the mean shift cluster\nrecognition method implemented in a random walk search algorithm. The\nclustering recognition is integrated within the Bayesian analysis program\nNestedFit. It is tested with the analysis of some difficult cases. Compared to\nthe analysis results without cluster recognition, the computation time is\nconsiderably reduced. At the same time, the entire parameter space is\nefficiently explored, which translates into a smaller uncertainty of the\nextracted value of the Bayesian evidence.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 15:04:30 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Trassinelli", "M.", "", "INSP-E10, INSP"], ["Ciccodicola", "Pierre", "", "INSP-E10, INSP"]]}, {"id": "2002.01706", "submitter": "Aleksandar Kolev", "authors": "Aleksandar A. Kolev, Gordon J. Ross", "title": "Semiparametric Bayesian Forecasting of Spatial Earthquake Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-exciting Hawkes processes are used to model events which cluster in time\nand space, and have been widely studied in seismology under the name of the\nEpidemic Type Aftershock Sequence (ETAS) model. In the ETAS framework, the\noccurrence of the mainshock earthquakes in a geographical region is assumed to\nfollow an inhomogeneous spatial point process, and aftershock events are then\nmodelled via a separate triggering kernel. Most previous studies of the ETAS\nmodel have relied on point estimates of the model parameters due to the\ncomplexity of the likelihood function, and the difficulty in estimating an\nappropriate mainshock distribution. In order to take estimation uncertainty\ninto account, we instead propose a fully Bayesian formulation of the ETAS model\nwhich uses a nonparametric Dirichlet process mixture prior to capture the\nspatial mainshock process. Direct inference for the resulting model is\nproblematic due to the strong correlation of the parameters for the mainshock\nand triggering processes, so we instead use an auxiliary latent variable\nroutine to perform efficient inference.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 10:11:26 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Kolev", "Aleksandar A.", ""], ["Ross", "Gordon J.", ""]]}, {"id": "2002.01859", "submitter": "Maria Dolores (Lola) Ugarte", "authors": "U. P\\'erez-Goya, M. Montesino-SanMartin, A.F. Militino, M.D. Ugarte", "title": "RGISTools: Downloading, Customizing, and Processing Time Series of\n  Remote Sensing Data in R", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large number of data archives and web services offering free\naccess to multispectral satellite imagery. Images from multiple sources are\nincreasingly combined to improve the spatio-temporal coverage of measurements\nwhile achieving more accurate results. Archives and web services differ in\ntheir protocols, formats, and data standards, which are barriers to combine\ndatasets. Here, we present RGISTools, an R package to create time-series of\nmultispectral satellite images from multiple platforms in a harmonized and\nstandardized way. We first provide an overview of the package functionalities,\nnamely downloading, customizing, and processing multispectral satellite imagery\nfor a region and time period of interest as well as a recent statistical method\nfor gap-filling and smoothing series of images, called interpolation of the\nmean anomalies. We further show the capabilities of the package through a case\nstudy that combines Landsat-8 and Sentinel-2 satellite optical imagery to\nestimate the level of a water reservoir in Northern Spain. We expect RGISTools\nto foster research on data fusion and spatio-temporal modelling using satellite\nimages from multiple programs.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 16:43:02 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["P\u00e9rez-Goya", "U.", ""], ["Montesino-SanMartin", "M.", ""], ["Militino", "A. F.", ""], ["Ugarte", "M. D.", ""]]}, {"id": "2002.01890", "submitter": "Thais Fonseca Dr", "authors": "Victhor S. Sart\\'orio and Tha\\'is C. O. Fonseca", "title": "Dynamic clustering of time series data", "comments": "27 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for clustering multivariate time-series data based on\nDynamic Linear Models. Whereas usual time-series clustering methods obtain\nstatic membership parameters, our proposal allows each time-series to\ndynamically change their cluster memberships over time. In this context, a\nmixture model is assumed for the time series and a flexible Dirichlet evolution\nfor mixture weights allows for smooth membership changes over time. Posterior\nestimates and predictions can be obtained through Gibbs sampling, but a more\nefficient method for obtaining point estimates is presented, based on\nStochastic Expectation-Maximization and Gradient Descent. Finally, two\napplications illustrate the usefulness of our proposed model to model both\nunivariate and multivariate time-series: World Bank indicators for the\nrenewable energy consumption of EU nations and the famous Gapminder dataset\ncontaining life-expectancy and GDP per capita for various countries.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 12:01:28 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Sart\u00f3rio", "Victhor S.", ""], ["Fonseca", "Tha\u00eds C. O.", ""]]}, {"id": "2002.01973", "submitter": "Raul Rojas Prof.", "authors": "Raul Rojas", "title": "Exploring Maximum Entropy Distributions with Evolutionary Algorithms", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG cs.NE math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows how to evolve numerically the maximum entropy probability\ndistributions for a given set of constraints, which is a variational calculus\nproblem. An evolutionary algorithm can obtain approximations to some well-known\nanalytical results, but is even more flexible and can find distributions for\nwhich a closed formula cannot be readily stated. The numerical approach handles\ndistributions over finite intervals. We show that there are two ways of\nconducting the procedure: by direct optimization of the Lagrangian of the\nconstrained problem, or by optimizing the entropy among the subset of\ndistributions which fulfill the constraints. An incremental evolutionary\nstrategy easily obtains the uniform, the exponential, the Gaussian, the\nlog-normal, the Laplace, among other distributions, once the constrained\nproblem is solved with any of the two methods. Solutions for mixed (\"chimera\")\ndistributions can be also found. We explain why many of the distributions are\nsymmetrical and continuous, but some are not.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 19:52:05 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Rojas", "Raul", ""]]}, {"id": "2002.02024", "submitter": "Michelle Carey", "authors": "Michelle Carey and James O. Ramsay", "title": "Fast Stable Parameter Estimation for Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2020.107124", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems describe the changes in processes that arise naturally from\ntheir underlying physical principles, such as the laws of motion or the\nconservation of mass, energy or momentum. These models facilitate a causal\nexplanation for the drivers and impediments of the processes. But do they\ndescribe the behaviour of the observed data? And how can we quantify the\nmodels' parameters that cannot be measured directly? This paper addresses these\ntwo questions by providing a methodology for estimating the solution; and the\nparameters of linear dynamical systems from incomplete and noisy observations\nof the processes.\n  The proposed procedure builds on the parameter cascading approach, where a\nlinear combination of basis functions approximates the implicitly defined\nsolution of the dynamical system. The systems' parameters are then estimated so\nthat this approximating solution adheres to the data. By taking advantage of\nthe linearity of the system, we have simplified the parameter cascading\nestimation procedure, and by developing a new iterative scheme, we achieve fast\nand stable computation.\n  We illustrate our approach by obtaining a linear differential equation that\nrepresents real data from biomechanics. Comparing our approach with popular\nmethods for estimating the parameters of linear dynamical systems, namely, the\nnon-linear least-squares approach, simulated annealing, parameter cascading and\nsmooth functional tempering reveals a considerable reduction in computation and\nan improved bias and sampling variance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 22:37:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Carey", "Michelle", ""], ["Ramsay", "James O.", ""]]}, {"id": "2002.02405", "submitter": "Sebastian Nowozin", "authors": "Florian Wenzel, Kevin Roth, Bastiaan S. Veeling, Jakub\n  \\'Swi\\k{a}tkowski, Linh Tran, Stephan Mandt, Jasper Snoek, Tim Salimans,\n  Rodolphe Jenatton, Sebastian Nowozin", "title": "How Good is the Bayes Posterior in Deep Neural Networks Really?", "comments": "Full version (main paper and appendix) of the ICML 2020 publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past five years the Bayesian deep learning community has developed\nincreasingly accurate and efficient approximate inference procedures that allow\nfor Bayesian inference in deep neural networks. However, despite this\nalgorithmic progress and the promise of improved uncertainty quantification and\nsample efficiency there are---as of early 2020---no publicized deployments of\nBayesian neural networks in industrial practice. In this work we cast doubt on\nthe current understanding of Bayes posteriors in popular deep neural networks:\nwe demonstrate through careful MCMC sampling that the posterior predictive\ninduced by the Bayes posterior yields systematically worse predictions compared\nto simpler methods including point estimates obtained from SGD. Furthermore, we\ndemonstrate that predictive performance is improved significantly through the\nuse of a \"cold posterior\" that overcounts evidence. Such cold posteriors\nsharply deviate from the Bayesian paradigm but are commonly used as heuristic\nin Bayesian deep learning papers. We put forward several hypotheses that could\nexplain cold posteriors and evaluate the hypotheses through experiments. Our\nwork questions the goal of accurate posterior approximations in Bayesian deep\nlearning: If the true Bayes posterior is poor, what is the use of more accurate\napproximations? Instead, we argue that it is timely to focus on understanding\nthe origin of the improved performance of cold posteriors.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 17:38:48 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 22:18:12 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Wenzel", "Florian", ""], ["Roth", "Kevin", ""], ["Veeling", "Bastiaan S.", ""], ["\u015awi\u0105tkowski", "Jakub", ""], ["Tran", "Linh", ""], ["Mandt", "Stephan", ""], ["Snoek", "Jasper", ""], ["Salimans", "Tim", ""], ["Jenatton", "Rodolphe", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "2002.02481", "submitter": "Francois Belletti", "authors": "Francois Belletti, Davis King, James Lottes, Yi-Fan Chen, John\n  Anderson", "title": "Sensitivity Analysis in the Dupire Local Volatility Model with\n  Tensorflow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC q-fin.CP stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In a recent paper, we have demonstrated how the affinity between TPUs and\nmulti-dimensional financial simulation resulted in fast Monte Carlo simulations\nthat could be setup in a few lines of python Tensorflow code. We also presented\na major benefit from writing high performance simulations in an automated\ndifferentiation language such as Tensorflow: a single line of code enabled us\nto estimate sensitivities, i.e. the rate of change in price of financial\ninstrument with respect to another input such as the interest rate, the current\nprice of the underlying, or volatility. Such sensitivities (otherwise known as\nthe famous financial \"Greeks\") are fundamental for risk assessment and risk\nmitigation. In the present follow-up short paper, we extend the developments\nexposed in our previous work about the use of Tensor Processing Units and\nTensorflow for TPUs.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 19:27:44 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Belletti", "Francois", ""], ["King", "Davis", ""], ["Lottes", "James", ""], ["Chen", "Yi-Fan", ""], ["Anderson", "John", ""]]}, {"id": "2002.02571", "submitter": "Shijia Wang", "authors": "Shijia Wang and Liangliang Wang", "title": "Adaptive semiparametric Bayesian differential equations via sequential\n  Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear differential equations (DEs) are used in a wide range of scientific\nproblems to model complex dynamic systems. The differential equations often\ncontain unknown parameters that are of scientific interest, which have to be\nestimated from noisy measurements of the dynamic system. Generally, there is no\nclosed-form solution for nonlinear DEs, and the likelihood surface for the\nparameter of interest is multi-modal and very sensitive to different parameter\nvalues. We propose a fully Bayesian framework for nonlinear DEs system. A\nflexible nonparametric function is used to represent the dynamic process such\nthat expensive numerical solvers can be avoided. A sequential Monte Carlo in\nthe annealing framework is proposed to conduct Bayesian inference for\nparameters in DEs. In our numerical experiments, we use examples of ordinary\ndifferential equations and delay differential equations to demonstrate the\neffectiveness of the proposed algorithm. We developed an R package that is\navailable at \\url{https://github.com/shijiaw/smcDE}.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 00:52:09 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Wang", "Shijia", ""], ["Wang", "Liangliang", ""]]}, {"id": "2002.02919", "submitter": "Qifan Song", "authors": "Qifan Song, Yan Sun, Mao Ye, Faming Liang", "title": "Extended Stochastic Gradient MCMC for Large-Scale Bayesian Variable\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (MCMC) algorithms have received\nmuch attention in Bayesian computing for big data problems, but they are only\napplicable to a small class of problems for which the parameter space has a\nfixed dimension and the log-posterior density is differentiable with respect to\nthe parameters. This paper proposes an extended stochastic gradient MCMC\nlgoriathm which, by introducing appropriate latent variables, can be applied to\nmore general large-scale Bayesian computing problems, such as those involving\ndimension jumping and missing data. Numerical studies show that the proposed\nalgorithm is highly scalable and much more efficient than traditional MCMC\nalgorithms. The proposed algorithms have much alleviated the pain of Bayesian\nmethods in big data computing.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 17:47:07 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Song", "Qifan", ""], ["Sun", "Yan", ""], ["Ye", "Mao", ""], ["Liang", "Faming", ""]]}, {"id": "2002.03318", "submitter": "Yuling Jiao", "authors": "Xingdong Feng, Jian Huang, Yuling Jiao, Shuang Zhang", "title": "$\\ell_0$-Regularized High-dimensional Accelerated Failure Time Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a constructive approach for $\\ell_0$-penalized estimation in the\nsparse accelerated failure time (AFT) model with high-dimensional covariates.\nOur proposed method is based on Stute's weighted least squares criterion\ncombined with $\\ell_0$-penalization. This method is a computational algorithm\nthat generates a sequence of solutions iteratively, based on active sets\nderived from primal and dual information and root finding according to the KKT\nconditions. We refer to the proposed method as AFT-SDAR (for support detection\nand root finding). An important aspect of our theoretical results is that we\ndirectly concern the sequence of solutions generated based on the AFT-SDAR\nalgorithm. We prove that the estimation errors of the solution sequence decay\nexponentially to the optimal error bound with high probability, as long as the\ncovariate matrix satisfies a mild regularity condition which is necessary and\nsufficient for model identification even in the setting of high-dimensional\nlinear regression. We also proposed an adaptive version of AFT-SDAR, or\nAFT-ASDAR, which determines the support size of the estimated coefficient in a\ndata-driven fashion. We conduct simulation studies to demonstrate the superior\nperformance of the proposed method over the lasso and MCP in terms of accuracy\nand speed. We also apply the proposed method to a real data set to illustrate\nits application.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:50:36 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Feng", "Xingdong", ""], ["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Zhang", "Shuang", ""]]}, {"id": "2002.03490", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Forough Fazeli Asl, and Zahra Saberi", "title": "A Test for Independence Via Bayesian Nonparametric Estimation of Mutual\n  Information", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutual information is a well-known tool to measure the mutual dependence\nbetween variables. In this paper, a Bayesian nonparametric estimation of mutual\ninformation is established by means of the Dirichlet process and the\n$k$-nearest neighbor distance. As a direct outcome of the estimation, an\neasy-to-implement test of independence is introduced through the relative\nbelief ratio. Several theoretical properties of the approach are presented. The\nprocedure is investigated through various examples where the results are\ncompared to its frequentist counterpart and demonstrate a good performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 01:29:08 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Asl", "Forough Fazeli", ""], ["Saberi", "Zahra", ""]]}, {"id": "2002.03646", "submitter": "Vincent Runge", "authors": "Vincent Runge, Toby Dylan Hocking, Gaetano Romano, Fatemeh Afghah,\n  Paul Fearnhead and Guillem Rigaill", "title": "gfpop: an R Package for Univariate Graph-Constrained Change-point\n  Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world with data that change rapidly and abruptly, it is important to\ndetect those changes accurately. In this paper we describe an R package\nimplementing an algorithm recently proposed by Hocking et al. [2017] for\npenalised maximum likelihood inference of constrained multiple change-point\nmodels. This algorithm can be used to pinpoint the precise locations of abrupt\nchanges in large data sequences. There are many application domains for such\nmodels, such as medicine, neuroscience or genomics. Often, practitioners have\nprior knowledge about the changes they are looking for. For example in genomic\ndata, biologists sometimes expect peaks: up changes followed by down changes.\nTaking advantage of such prior information can substantially improve the\naccuracy with which we can detect and estimate changes. Hocking et al. [2017]\ndescribed a graph framework to encode many examples of such prior information\nand a generic algorithm to infer the optimal model parameters, but implemented\nthe algorithm for just a single scenario. We present the gfpop package that\nimplements the algorithm in a generic manner in R/C++. gfpop works for a\nuser-defined graph that can encode the prior nformation of the types of change\nand implements several loss functions (Gauss, Poisson, Binomial, Biweight and\nHuber). We then illustrate the use of gfpop on isotonic simulations and several\napplications in biology. For a number of graphs the algorithm runs in a matter\nof seconds or minutes for 10^5 datapoints.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 10:49:51 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Runge", "Vincent", ""], ["Hocking", "Toby Dylan", ""], ["Romano", "Gaetano", ""], ["Afghah", "Fatemeh", ""], ["Fearnhead", "Paul", ""], ["Rigaill", "Guillem", ""]]}, {"id": "2002.03747", "submitter": "Ajay Jasra", "authors": "Ajay Jasra, Kody Law, Fangyuan Yu", "title": "Unbiased Filtering of a Class of Partially Observed Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider a Monte Carlo-based method to filter partially\nobserved diffusions observed at regular and discrete times. Given access only\nto Euler discretizations of the diffusion process, we present a new procedure\nwhich can return online estimates of the filtering distribution with no\ndiscretization bias and finite variance. Our approach is based upon a novel\ndouble application of the randomization methods of Rhee & Glynn (2015) along\nwith the multilevel particle filter (MLPF) approach of Jasra et al (2017). A\nnumerical comparison of our new approach with the MLPF, on a single processor,\nshows that similar errors are possible for a mild increase in computational\ncost. However, the new method scales strongly to arbitrarily many processors.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 13:48:49 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 12:30:30 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Yu", "Fangyuan", ""]]}, {"id": "2002.03853", "submitter": "Samuel Thiriot", "authors": "Samuel Thiriot and Marie Sevenet", "title": "Pairing for Generation of Synthetic Populations: the Direct\n  Probabilistic Pairing method", "comments": "Author draft prior to submission elsewhere. 49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for the Generation of Synthetic Populations do generate the entities\nrequired for micro models or multi-agent models, such as they match field\nobservations or hypothesis on the population under study. We tackle here the\nspecific question of creating synthetic populations made of two types of\nentities linked together by 0, 1 or more links. Potential applications include\nthe creation of dwellings inhabited by households, households owning cars,\ndwellings equipped with appliances, worker employed by firms, etc. We propose a\ntheoretical framework to tackle this problem. We then highlight how this\nproblem is over-constrained and requires relaxation of some constraints to be\nsolved. We propose a method to solve the problem analytically which lets the\nuser select which input data should be preserved and adapts the others in order\nto make the data consistent. We illustrate this method by synthesizing a\npopulation made of dwellings containing 0, 1 or 2 households in the city of\nLille (France). In this population, the distributions of the dwellings' and\nhouseholds' characteristics are preserved, and both are linked according to\nstatistical pairing statistics.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 10:35:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Thiriot", "Samuel", ""], ["Sevenet", "Marie", ""]]}, {"id": "2002.04081", "submitter": "Andrea Arf\\'e", "authors": "Andrea Arf\\`e, Pietro Muliere", "title": "A general Bayesian bootstrap for censored data based on the beta-Stacy\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel procedure to perform Bayesian non-parametric inference\nwith right-censored data, the \\emph{beta-Stacy bootstrap}. This approximates\nthe posterior law of summaries of the survival distribution (e.g. the mean\nsurvival time), which is often difficult in the non-parametric case. More\nprecisely, our procedure approximates the joint posterior law of functionals of\nthe beta-Stacy process, a non-parametric process prior widely used in survival\nanalysis. It also represents the missing link that unifies other common\nBayesian bootstraps for complete or censored data based on non-parametric\npriors. It is defined by an exact sampling algorithm that does not require\ntuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy\nbootstrap by analyzing survival data from a real clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:41:35 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Arf\u00e8", "Andrea", ""], ["Muliere", "Pietro", ""]]}, {"id": "2002.04121", "submitter": "Kevin Tian", "authors": "Yin Tat Lee, Ruoqi Shen, Kevin Tian", "title": "Logsmooth Gradient Concentration and Tighter Runtimes for Metropolized\n  Hamiltonian Monte Carlo", "comments": "31 pages. v2 propagates changes from COLT 2020 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the gradient norm $\\|\\nabla f(x)\\|$ for $x \\sim \\exp(-f(x))$,\nwhere $f$ is strongly convex and smooth, concentrates tightly around its mean.\nThis removes a barrier in the prior state-of-the-art analysis for the\nwell-studied Metropolized Hamiltonian Monte Carlo (HMC) algorithm for sampling\nfrom a strongly logconcave distribution. We correspondingly demonstrate that\nMetropolized HMC mixes in $\\tilde{O}(\\kappa d)$ iterations, improving upon the\n$\\tilde{O}(\\kappa^{1.5}\\sqrt{d} + \\kappa d)$ runtime of (Dwivedi et. al. '18,\nChen et. al. '19) by a factor $(\\kappa/d)^{1/2}$ when the condition number\n$\\kappa$ is large. Our mixing time analysis introduces several techniques which\nto our knowledge have not appeared in the literature and may be of independent\ninterest, including restrictions to a nonconvex set with good conductance\nbehavior, and a new reduction technique for boosting a constant-accuracy total\nvariation guarantee under weak warmness assumptions. This is the first\nhigh-accuracy mixing time result for logconcave distributions using only\nfirst-order function information which achieves linear dependence on $\\kappa$;\nwe also give evidence that this dependence is likely to be necessary for\nstandard Metropolized first-order methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:44:50 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 05:24:03 GMT"}, {"version": "v3", "created": "Sun, 14 Jun 2020 02:12:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lee", "Yin Tat", ""], ["Shen", "Ruoqi", ""], ["Tian", "Kevin", ""]]}, {"id": "2002.04123", "submitter": "Kamran Javid Mr", "authors": "Kamran Javid", "title": "Geometric nested sampling: sampling from distributions defined on\n  non-trivial geometries", "comments": "Peer reviewed and published in JOSS. arXiv admin note: substantial\n  text overlap with arXiv:1905.09110", "journal-ref": "JOSS 5(46), 1809, 2020", "doi": "10.21105/joss.01809", "report-no": null, "categories": "stat.CO astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metropolis Hastings nested sampling evolves a Markov chain, accepting new\npoints along the chain according to a version of the Metropolis Hastings\nacceptance ratio, which has been modified to satisfy the nested sampling\nlikelihood constraint. The geometric nested sampling algorithm I present here\nis based on the Metropolis Hastings method, but treats parameters as though\nthey represent points on certain geometric objects, namely circles, tori and\nspheres. For parameters which represent points on a circle or torus, the trial\ndistribution is \"wrapped\" around the domain of the posterior distribution such\nthat samples cannot be rejected automatically when evaluating the Metropolis\nratio due to being outside the sampling domain. Furthermore, this enhances the\nmobility of the sampler. For parameters which represent coordinates on the\nsurface of a sphere, the algorithm transforms the parameters into a Cartesian\ncoordinate system before sampling which again makes sure no samples are\nautomatically rejected, and provides a physically intuitive way of the sampling\nthe parameter space.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:48:18 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Javid", "Kamran", ""]]}, {"id": "2002.04255", "submitter": "Chiara Tommasi", "authors": "Deldossi Laura and Tommasi Chiara", "title": "Big Data and model-based survey sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data are huge amounts of digital information that are automatically\naccrued or merged from several sources and rarely result from properly planned\nsurveys. A Big Dataset is herein conceived of as a collection of information\nconcerning a finite population. We suggest selecting a sample of observations\nto get the inferential goal. We assume a super-population model has generated\nthe Big Dataset. With this assumption, we can apply the theory of optimal\ndesign to draw a sample from the Big Dataset that contains the majority of the\ninformation about the unknown parameters.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:45:24 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Laura", "Deldossi", ""], ["Chiara", "Tommasi", ""]]}, {"id": "2002.04320", "submitter": "Mathias Staudigl", "authors": "Pavel Dvurechensky, Petr Ostroukhov, Kamil Safin, Shimrit Shtern,\n  Mathias Staudigl", "title": "Self-Concordant Analysis of Frank-Wolfe Algorithms", "comments": "Proceedings of the 37th International Conference on Machine Learning\n  (ICML2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Projection-free optimization via different variants of the Frank-Wolfe (FW),\na.k.a. Conditional Gradient method has become one of the cornerstones in\noptimization for machine learning since in many cases the linear minimization\noracle is much cheaper to implement than projections and some sparsity needs to\nbe preserved. In a number of applications, e.g. Poisson inverse problems or\nquantum state tomography, the loss is given by a self-concordant (SC) function\nhaving unbounded curvature, implying absence of theoretical guarantees for the\nexisting FW methods. We use the theory of SC functions to provide a new\nadaptive step size for FW methods and prove global convergence rate O(1/k)\nafter k iterations. If the problem admits a stronger local linear minimization\noracle, we construct a novel FW method with linear convergence rate for SC\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 11:30:33 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 17:08:42 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 04:35:35 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Dvurechensky", "Pavel", ""], ["Ostroukhov", "Petr", ""], ["Safin", "Kamil", ""], ["Shtern", "Shimrit", ""], ["Staudigl", "Mathias", ""]]}, {"id": "2002.04495", "submitter": "Alireza Doostan", "authors": "Subhayan De, Jolene Britton, Matthew Reynolds, Ryan Skinner, Kenneth\n  Jansen, and Alireza Doostan", "title": "On transfer learning of neural networks using bi-fidelity data for\n  uncertainty propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their high degree of expressiveness, neural networks have recently\nbeen used as surrogate models for mapping inputs of an engineering system to\noutputs of interest. Once trained, neural networks are computationally\ninexpensive to evaluate and remove the need for repeated evaluations of\ncomputationally expensive models in uncertainty quantification applications.\nHowever, given the highly parameterized construction of neural networks,\nespecially deep neural networks, accurate training often requires large amounts\nof simulation data that may not be available in the case of computationally\nexpensive systems. In this paper, to alleviate this issue for uncertainty\npropagation, we explore the application of transfer learning techniques using\ntraining data generated from both high- and low-fidelity models. We explore two\nstrategies for coupling these two datasets during the training procedure,\nnamely, the standard transfer learning and the bi-fidelity weighted learning.\nIn the former approach, a neural network model mapping the inputs to the\noutputs of interest is trained based on the low-fidelity data. The\nhigh-fidelity data is then used to adapt the parameters of the upper layer(s)\nof the low-fidelity network, or train a simpler neural network to map the\noutput of the low-fidelity network to that of the high-fidelity model. In the\nlatter approach, the entire low-fidelity network parameters are updated using\ndata generated via a Gaussian process model trained with a small high-fidelity\ndataset. The parameter updates are performed via a variant of stochastic\ngradient descent with learning rates given by the Gaussian process model. Using\nthree numerical examples, we illustrate the utility of these bi-fidelity\ntransfer learning methods where we focus on accuracy improvement achieved by\ntransfer learning over standard training approaches.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:56:11 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["De", "Subhayan", ""], ["Britton", "Jolene", ""], ["Reynolds", "Matthew", ""], ["Skinner", "Ryan", ""], ["Jansen", "Kenneth", ""], ["Doostan", "Alireza", ""]]}, {"id": "2002.04592", "submitter": "Min Zhou", "authors": "Yang Feng, Min Zhou, Xin Tong", "title": "Imbalanced classification: a paradigm-based review", "comments": "34 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common issue for classification in scientific research and industry is the\nexistence of imbalanced classes. When sample sizes of different classes are\nimbalanced in training data, naively implementing a classification method often\nleads to unsatisfactory prediction results on test data. Multiple resampling\ntechniques have been proposed to address the class imbalance issues. Yet, there\nis no general guidance on when to use each technique. In this article, we\nprovide a paradigm-based review of the common resampling techniques for binary\nclassification under imbalanced class sizes. The paradigms we consider include\nthe classical paradigm that minimizes the overall classification error, the\ncost-sensitive learning paradigm that minimizes a cost-adjusted weighted type I\nand type II errors, and the Neyman-Pearson paradigm that minimizes the type II\nerror subject to a type I error constraint. Under each paradigm, we investigate\nthe combination of the resampling techniques and a few state-of-the-art\nclassification methods. For each pair of resampling techniques and\nclassification methods, we use simulation studies and a real data set on credit\ncard fraud to study the performance under different evaluation metrics. From\nthese extensive numerical experiments, we demonstrate under each classification\nparadigm, the complex dynamics among resampling techniques, base classification\nmethods, evaluation metrics, and imbalance ratios. We also summarize a few\ntakeaway messages regarding the choices of resampling techniques and base\nclassification methods, which could be helpful for practitioners.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 18:34:48 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 02:08:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Feng", "Yang", ""], ["Zhou", "Min", ""], ["Tong", "Xin", ""]]}, {"id": "2002.04691", "submitter": "Michail Tsagris", "authors": "M. Tsagris, A. Alenazi, and S. Fafalios", "title": "Computationally efficient univariate filtering for massive data", "comments": "The paper has been submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast availability of large scale, massive and big data has increased the\ncomputational cost of data analysis. One such case is the computational cost of\nthe univariate filtering which typically involves fitting many univariate\nregression models and is essential for numerous variable selection algorithms\nto reduce the number of predictor variables. The paper manifests how to\ndramatically reduce that computational cost by employing the score test or the\nsimple Pearson correlation (or the t-test for binary responses). Extensive\nMonte Carlo simulation studies will demonstrate their advantages and\ndisadvantages compared to the likelihood ratio test and examples with real data\nwill illustrate the performance of the score test and the log-likelihood ratio\ntest under realistic scenarios. Depending on the regression model used, the\nscore test is 30 - 60,000 times faster than the log-likelihood ratio test and\nproduces nearly the same results. Hence this paper strongly recommends to\nsubstitute the log-likelihood ratio test with the score test when coping with\nlarge scale data, massive data, big data, or even with data whose sample size\nis in the order of a few tens of thousands or higher.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:20:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Tsagris", "M.", ""], ["Alenazi", "A.", ""], ["Fafalios", "S.", ""]]}, {"id": "2002.04704", "submitter": "Robert Hu", "authors": "Robert Hu, Geoff K. Nicholls, Dino Sejdinovic", "title": "Large Scale Tensor Regression using Kernels and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We outline an inherent weakness of tensor factorization models when latent\nfactors are expressed as a function of side information and propose a novel\nmethod to mitigate this weakness. We coin our method \\textit{Kernel Fried\nTensor}(KFT) and present it as a large scale forecasting tool for high\ndimensional data. Our results show superior performance against\n\\textit{LightGBM} and \\textit{Field Aware Factorization Machines}(FFM), two\nalgorithms with proven track records widely used in industrial forecasting. We\nalso develop a variational inference framework for KFT and associate our\nforecasts with calibrated uncertainty estimates on three large scale datasets.\nFurthermore, KFT is empirically shown to be robust against uninformative side\ninformation in terms of constants and Gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 21:46:52 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Hu", "Robert", ""], ["Nicholls", "Geoff K.", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2002.05297", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Solution manifold and Its Statistical Applications", "comments": "36 page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A solution manifold is the collection of points in a $d$-dimensional space\nsatisfying a system of $s$ equations with $s<d$. Solution manifolds occur in\nseveral statistical problems including hypothesis testing, curved-exponential\nfamilies, constrained mixture models, partial identifications, and\nnonparametric set estimation. We analyze solution manifolds both theoretically\nand algorithmically. In terms of theory, we derive five useful results: the\nsmoothness theorem, the stability theorem (which implies the consistency of a\nplug-in estimator), the convergence of a gradient flow, the local center\nmanifold theorem and the convergence of the gradient descent algorithm. To\nnumerically approximate a solution manifold, we propose a Monte Carlo gradient\ndescent algorithm. In the case of likelihood inference, we design a manifold\nconstraint maximization procedure to find the maximum likelihood estimator on\nthe manifold. We also develop a method to approximate a posterior distribution\ndefined on a solution manifold.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:00:30 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "2002.05465", "submitter": "\\\"Omer Deniz Akyildiz", "authors": "\\\"Omer Deniz Akyildiz, Sotirios Sabanis", "title": "Nonasymptotic analysis of Stochastic Gradient Hamiltonian Monte Carlo\n  under local conditions for nonconvex optimization", "comments": "Some proofs are fixed and polished", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a nonasymptotic analysis of the convergence of the stochastic\ngradient Hamiltonian Monte Carlo (SGHMC) to a target measure in Wasserstein-2\ndistance without assuming log-concavity. Our analysis quantifies key\ntheoretical properties of the SGHMC as a sampler under local conditions which\nsignificantly improves the findings of previous results. In particular, we\nprove that the Wasserstein-2 distance between the target and the law of the\nSGHMC is uniformly controlled by the step-size of the algorithm, therefore\ndemonstrate that the SGHMC can provide high-precision results uniformly in the\nnumber of iterations. The analysis also allows us to obtain nonasymptotic\nbounds for nonconvex optimization problems under local conditions and implies\nthat the SGHMC, when viewed as a nonconvex optimizer, converges to a global\nminimum with the best known rates. We apply our results to obtain nonasymptotic\nbounds for scalable Bayesian inference and nonasymptotic generalization bounds.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 12:10:07 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 23:47:55 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 22:30:32 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Akyildiz", "\u00d6mer Deniz", ""], ["Sabanis", "Sotirios", ""]]}, {"id": "2002.05489", "submitter": "Andrea Meil\\'an-Vila", "authors": "Andrea Meil\\'an-Vila, Rub\\'en Fern\\'andez-Casal, Rosa M. Crujeiras\n  Mario Francisco-Fern\\'andez", "title": "A computational validation for nonparametric assessment of spatial\n  trends", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of continuously spatially varying processes usually considers\ntwo sources of variation, namely, the large-scale variation collected by the\ntrend of the process, and the small-scale variation. Parametric trend models on\nlatitude and longitude are easy to fit and to interpret. However, the use of\nsimple parametric models for characterizing spatially varying processes may\nlead to misspecification problems if the model is not appropriate. Recently,\nMeil\\'an-Vila et al. (2019) proposed a goodness-of-fit test based on an\nL2-distance for assessing a parametric trend model with correlated errors,\nunder random design, comparing a parametric and a nonparametric trend\nestimators. The present work aims to provide a detailed computational analysis\nof the behavior of this approach using different bootstrap algorithms for\ncalibration, under a fixed-design geostatistical framework. Asymptotic results\nfor the test are provided and an extensive simulation study, considering\ncomplexities that usually arise in geostatistics, is carried out to illustrate\nthe performance of the proposal.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 13:27:32 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Meil\u00e1n-Vila", "Andrea", ""], ["Fern\u00e1ndez-Casal", "Rub\u00e9n", ""], ["Francisco-Fern\u00e1ndez", "Rosa M. Crujeiras Mario", ""]]}, {"id": "2002.05519", "submitter": "Yixuan Qiu", "authors": "Yixuan Qiu and Xiao Wang", "title": "Stochastic Approximate Gradient Descent via the Langevin Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel and efficient algorithm called the stochastic\napproximate gradient descent (SAGD), as an alternative to the stochastic\ngradient descent for cases where unbiased stochastic gradients cannot be\ntrivially obtained. Traditional methods for such problems rely on\ngeneral-purpose sampling techniques such as Markov chain Monte Carlo, which\ntypically requires manual intervention for tuning parameters and does not work\nefficiently in practice. Instead, SAGD makes use of the Langevin algorithm to\nconstruct stochastic gradients that are biased in finite steps but accurate\nasymptotically, enabling us to theoretically establish the convergence\nguarantee for SAGD. Inspired by our theoretical analysis, we also provide\nuseful guidelines for its practical implementation. Finally, we show that SAGD\nperforms well experimentally in popular statistical and machine learning\nproblems such as the expectation-maximization algorithm and the variational\nautoencoders.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 14:29:21 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Qiu", "Yixuan", ""], ["Wang", "Xiao", ""]]}, {"id": "2002.05550", "submitter": "Dino Sejdinovic", "authors": "Qinyi Zhang, Sarah Filippi, Seth Flaxman, Dino Sejdinovic", "title": "Bayesian Kernel Two-Sample Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern data analysis, nonparametric measures of discrepancies between\nrandom variables are particularly important. The subject is well-studied in the\nfrequentist literature, while the development in the Bayesian setting is\nlimited where applications are often restricted to univariate cases. Here, we\npropose a Bayesian kernel two-sample testing procedure based on modelling the\ndifference between kernel mean embeddings in the reproducing kernel Hilbert\nspace utilising the framework established by Flaxman et al (2016). The use of\nkernel methods enables its application to random variables in generic domains\nbeyond the multivariate Euclidean spaces. The proposed procedure results in a\nposterior inference scheme that allows an automatic selection of the kernel\nparameters relevant to the problem at hand. In a series of synthetic\nexperiments and two real data experiments (i.e. testing network heterogeneity\nfrom high-dimensional data and six-membered monocyclic ring conformation\ncomparison), we illustrate the advantages of our approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 15:01:10 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Zhang", "Qinyi", ""], ["Filippi", "Sarah", ""], ["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2002.05953", "submitter": "Stephen Johnson", "authors": "Stephen R. Johnson, Daniel A. Henderson and Richard J. Boys", "title": "On Bayesian inference for the Extended Plackett-Luce model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of rank ordered data has a long history in the statistical\nliterature across a diverse range of applications. In this paper we consider\nthe Extended Plackett-Luce model that induces a flexible (discrete)\ndistribution over permutations. The parameter space of this distribution is a\ncombination of potentially high-dimensional discrete and continuous components\nand this presents challenges for parameter interpretability and also posterior\ncomputation. Particular emphasis is placed on the interpretation of the\nparameters in terms of observable quantities and we propose a general framework\nfor preserving the mode of the prior predictive distribution. Posterior\nsampling is achieved using an effective simulation based approach that does not\nrequire imposing restrictions on the parameter space. Working in the Bayesian\nframework permits a natural representation of the posterior predictive\ndistribution and we draw on this distribution to address the rank aggregation\nproblem and also to identify potential lack of model fit. The flexibility of\nthe Extended Plackett-Luce model along with the effectiveness of the proposed\nsampling scheme are demonstrated using several simulation studies and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:19:20 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Johnson", "Stephen R.", ""], ["Henderson", "Daniel A.", ""], ["Boys", "Richard J.", ""]]}, {"id": "2002.06212", "submitter": "Minas Karamanis", "authors": "Minas Karamanis and Florian Beutler", "title": "Ensemble Slice Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML astro-ph.CO astro-ph.IM cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm\nthat adapts to the characteristics of the target distribution with minimal\nhand-tuning. However, Slice Sampling's performance is highly sensitive to the\nuser-specified initial length scale hyperparameter. Moreover, Slice Sampling\ngenerally struggles with poorly scaled or strongly correlated distributions.\nThis paper introduces Ensemble Slice Sampling, a new class of algorithms that\nbypasses such difficulties by adaptively tuning the length scale. Furthermore,\nEnsemble Slice Sampling's performance is immune to linear correlations by\nexploiting an ensemble of parallel walkers. These algorithms are trivial to\nconstruct, require no hand-tuning, and can easily be implemented in parallel\ncomputing environments. Empirical tests show that Ensemble Slice Sampling can\nimprove efficiency by more than an order of magnitude compared to conventional\nMCMC methods on highly correlated target distributions such as the\nAutoregressive Process of Order 1 and the Correlated Funnel distribution.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 19:00:12 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Karamanis", "Minas", ""], ["Beutler", "Florian", ""]]}, {"id": "2002.06358", "submitter": "Tiangang Cui", "authors": "Johnathan Bardsley, Tiangang Cui", "title": "Optimization-Based MCMC Methods for Nonlinear Hierarchical Statistical\n  Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many hierarchical inverse problems, not only do we want to estimate high-\nor infinite-dimensional model parameters in the parameter-to-observable maps,\nbut we also have to estimate hyperparameters that represent critical\nassumptions in the statistical and mathematical modeling processes. As a joint\neffect of high-dimensionality, nonlinear dependence, and non-concave structures\nin the joint posterior posterior distribution over model parameters and\nhyperparameters, solving inverse problems in the hierarchical Bayesian setting\nposes a significant computational challenge. In this work, we aim to develop\nscalable optimization-based Markov chain Monte Carlo (MCMC) methods for solving\nhierarchical Bayesian inverse problems with nonlinear parameter-to-observable\nmaps and a broader class of hyperparameters. Our algorithmic development is\nbased on the recently developed scalable randomize-then-optimize (RTO) method\n[4] for exploring the high- or infinite-dimensional model parameter space. By\nusing RTO either as a proposal distribution in a Metropolis-within-Gibbs update\nor as a biasing distribution in the pseudo-marginal MCMC [2], we are able to\ndesign efficient sampling tools for hierarchical Bayesian inversion. In\nparticular, the integration of RTO and the pseudo-marginal MCMC has sampling\nperformance robust to model parameter dimensions. We also extend our methods to\nnonlinear inverse problems with Poisson-distributed measurements. Numerical\nexamples in PDE-constrained inverse problems and positron emission tomography\n(PET) are used to demonstrate the performance of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 10:19:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bardsley", "Johnathan", ""], ["Cui", "Tiangang", ""]]}, {"id": "2002.06633", "submitter": "Solt Kov\\'acs", "authors": "Solt Kov\\'acs, Housen Li, Peter B\\\"uhlmann, Axel Munk", "title": "Seeded Binary Segmentation: A general methodology for fast and optimal\n  change point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an increasing demand on efficient algorithms\nfor large scale change point detection problems. To this end, we propose seeded\nbinary segmentation, an approach relying on a deterministic construction of\nbackground intervals, called seeded intervals, in which single change points\nare searched. The final selection of change points based on the candidates from\nseeded intervals can be done in various ways, adapted to the problem at hand.\nThus, seeded binary segmentation is easy to adapt to a wide range of change\npoint detection problems, let that be univariate, multivariate or even\nhigh-dimensional.\n  We consider the univariate Gaussian change in mean setup in detail. For this\nspecific case we show that seeded binary segmentation leads to a near-linear\ntime approach (i.e. linear up to a logarithmic factor) independent of the\nunderlying number of change points. Furthermore, using appropriate selection\nmethods, the methodology is shown to be asymptotically minimax optimal. While\ncomputationally more efficient, the finite sample estimation performance\nremains competitive compared to state of the art procedures. Moreover, we\nillustrate the methodology for high-dimensional settings with an inverse\ncovariance change point detection problem where our proposal leads to massive\ncomputational gains while still exhibiting good statistical performance.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 18:08:10 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kov\u00e1cs", "Solt", ""], ["Li", "Housen", ""], ["B\u00fchlmann", "Peter", ""], ["Munk", "Axel", ""]]}, {"id": "2002.06678", "submitter": "Peng Zhao", "authors": "Peng Zhao, Hou-Cheng Yang, Dipak K. Dey, Guanyu Hu", "title": "Bayesian Spatial Homogeneity Pursuit Regression for Count Value Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial regression models are ubiquitous in many different areas such as\nenvironmental science, geoscience, and public health. Exploring relationships\nbetween response variables and covariates with complex spatial patterns is a\nvery important work. In this paper, we propose a novel spatially clustered\ncoefficients regression model for count value data based on nonparametric\nBayesian methods. Our proposed method detects the spatial homogeneity of the\nPoisson regression coefficients. A Markov random field constraint mixture of\nfinite mixtures prior provides a consistent estimator of the number of the\nclusters of regression coefficients with the geographically neighborhood\ninformation. The theoretical properties of our proposed method are established.\nAn efficient Markov chain Monte Carlo algorithm is developed by using\nmultivariate log gamma distribution as a base distribution. Extensive\nsimulation studies are carried out to examine empirical performance of the\nproposed method. Additionally, we analyze Georgia premature deaths data as an\nillustration of the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 21:05:45 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Zhao", "Peng", ""], ["Yang", "Hou-Cheng", ""], ["Dey", "Dipak K.", ""], ["Hu", "Guanyu", ""]]}, {"id": "2002.06777", "submitter": "Sam Davanloo", "authors": "Yin Liu and Sam Davanloo Tajbakhsh", "title": "Fitting ARMA Time Series Models without Identification: A Proximal\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting autoregressive moving average (ARMA) time series models requires\nmodel identification before parameter estimation. Model identification involves\ndetermining the order of the autoregressive and moving average components which\nis generally performed by inspection of the autocorrelation and partial\nautocorrelation functions or other offline methods. In this work, we regularize\nthe parameter estimation optimization problem with a nonsmooth hierarchical\nsparsity-inducing penalty based on two path graphs that allows performing model\nidentification and parameter estimation simultaneously. A proximal block\ncoordinate descent algorithm is then proposed to solve the underlying\noptimization problem efficiently. The resulting model satisfies the required\nstationarity and invertibility conditions for ARMA models. Numerical studies\nsupporting the performance of the proposed method and comparing it with other\nschemes are presented.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 05:41:24 GMT"}, {"version": "v2", "created": "Fri, 18 Sep 2020 02:17:55 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Liu", "Yin", ""], ["Tajbakhsh", "Sam Davanloo", ""]]}, {"id": "2002.07002", "submitter": "Alexandros Keros", "authors": "Alexandros D. Keros, Divakaran Divakaran and Kartic Subr", "title": "Jittering Samples using a kd-Tree Stratification", "comments": "24 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo sampling techniques are used to estimate high-dimensional\nintegrals that model the physics of light transport in virtual scenes for\ncomputer graphics applications. These methods rely on the law of large numbers\nto estimate expectations via simulation, typically resulting in slow\nconvergence. Their errors usually manifest as undesirable grain in the pictures\ngenerated by image synthesis algorithms. It is well known that these errors\ndiminish when the samples are chosen appropriately. A well known technique for\nreducing error operates by subdividing the integration domain, estimating\nintegrals in each \\emph{stratum} and aggregating these values into a stratified\nsampling estimate. Na\\\"{i}ve methods for stratification, based on a lattice\n(grid) are known to improve the convergence rate of Monte Carlo, but require\nsamples that grow exponentially with the dimensionality of the domain.\n  We propose a simple stratification scheme for $d$ dimensional hypercubes\nusing the kd-tree data structure. Our scheme enables the generation of an\narbitrary number of equal volume partitions of the rectangular domain, and $n$\nsamples can be generated in $O(n)$ time. Since we do not always need to\nexplicitly build a kd-tree, we provide a simple procedure that allows the\nsample set to be drawn fully in parallel without any precomputation or storage,\nspeeding up sampling to $O(\\log n)$ time per sample when executed on $n$ cores.\nIf the tree is implicitly precomputed ($O(n)$ storage) the parallelised run\ntime reduces to $O(1)$ on $n$ cores. In addition to these benefits, we provide\nan upper bound on the worst case star-discrepancy for $n$ samples matching that\nof lattice-based sampling strategies, which occur as a special case of our\nproposed method. We use a number of quantitative and qualitative tests to\ncompare our method against state of the art samplers for image synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 15:27:52 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Keros", "Alexandros D.", ""], ["Divakaran", "Divakaran", ""], ["Subr", "Kartic", ""]]}, {"id": "2002.07367", "submitter": "Khai Nguyen", "authors": "Khai Nguyen and Nhat Ho and Tung Pham and Hung Bui", "title": "Distributional Sliced-Wasserstein and Applications to Generative\n  Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced-Wasserstein distance (SW) and its variant, Max Sliced-Wasserstein\ndistance (Max-SW), have been used widely in the recent years due to their fast\ncomputation and scalability even when the probability measures lie in a very\nhigh dimensional space. However, SW requires many unnecessary projection\nsamples to approximate its value while Max-SW only uses the most important\nprojection, which ignores the information of other useful directions. In order\nto account for these weaknesses, we propose a novel distance, named\nDistributional Sliced-Wasserstein distance (DSW), that finds an optimal\ndistribution over projections that can balance between exploring distinctive\nprojecting directions and the informativeness of projections themselves. We\nshow that the DSW is a generalization of Max-SW, and it can be computed\nefficiently by searching for the optimal push-forward measure over a set of\nprobability measures over the unit sphere satisfying certain regularizing\nconstraints that favor distinct directions. Finally, we conduct extensive\nexperiments with large-scale datasets to demonstrate the favorable performances\nof the proposed distances over the previous sliced-based distances in\ngenerative modeling applications.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 04:35:16 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 07:21:55 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Nguyen", "Khai", ""], ["Ho", "Nhat", ""], ["Pham", "Tung", ""], ["Bui", "Hung", ""]]}, {"id": "2002.07467", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en and Fredrik Lindsten", "title": "Deep Gaussian Markov Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Markov random fields (GMRFs) are probabilistic graphical models\nwidely used in spatial statistics and related fields to model dependencies over\nspatial structures. We establish a formal connection between GMRFs and\nconvolutional neural networks (CNNs). Common GMRFs are special cases of a\ngenerative model where the inverse mapping from data to latent variables is\ngiven by a 1-layer linear CNN. This connection allows us to generalize GMRFs to\nmulti-layer CNN architectures, effectively increasing the order of the\ncorresponding GMRF in a way which has favorable computational scaling. We\ndescribe how well-established tools, such as autodiff and variational\ninference, can be used for simple and efficient inference and learning of the\ndeep GMRF. We demonstrate the flexibility of the proposed model and show that\nit outperforms the state-of-the-art on a dataset of satellite temperatures, in\nterms of prediction and predictive uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:06:39 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:19:04 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "2002.07472", "submitter": "Mark van der Loo", "authors": "Mark P.J. van der Loo", "title": "A method for deriving information from running R code", "comments": "11 pages, 1 figure. Accepted for publication by the R Journal\n  (2020-02-20)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often useful to tap information from a running R script. Obvious use\ncases include monitoring the consumption of resources (time, memory) and\nlogging. Perhaps less obvious cases include tracking changes in R objects\norcollecting output of unit tests. In this paper we demonstrate an approach\nthat abstracts collection and processing of such secondary information from the\nrunning R script. Our approach is based on a combination of three elements. The\nfirst element is to build a customized way to evaluate code. The second is\nlabeled \\emph{local masking} and it involves temporarily masking auser-facing\nfunction so an alternative version of it is called. The third element we label\n\\emph{local side effect}. This refers to the fact that the masking function\nexports information to the secondary information flow without altering a global\nstate. The result is a method for building systems in pure R that lets users\ncreate and control secondary flows of information with minimal impact on their\nworkflow, and no global side effects.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 10:23:59 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 15:32:24 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["van der Loo", "Mark P. J.", ""]]}, {"id": "2002.07774", "submitter": "Michael O'Malley", "authors": "Michael O'Malley, Adam M. Sykulski, Romuald Laso-Jadart, Mohammed-Amin\n  Madoui", "title": "Estimating the travel time and the most likely path from Lagrangian\n  drifters", "comments": "27 pages, 10 figures in the main text. 13 pages, 8 figures in the\n  supplemental material", "journal-ref": null, "doi": "10.1175/JTECH-D-20-0134.1", "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an physics.flu-dyn stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel methodology for computing the most likely path taken by\ndrifters between arbitrary fixed locations in the ocean. We also provide an\nestimate of the travel time associated with this path. Lagrangian pathways and\ntravel times are of practical value not just in understanding surface\nvelocities, but also in modelling the transport of ocean-borne species such as\nplanktonic organisms, and floating debris such as plastics. In particular, the\nestimated travel time can be used to compute an estimated Lagrangian distance,\nwhich is often more informative than Euclidean distance in understanding\nconnectivity between locations. Our methodology is purely data-driven, and\nrequires no simulations of drifter trajectories, in contrast to existing\napproaches. Our method scales globally and can simultaneously handle multiple\nlocations in the ocean. Furthermore, we provide estimates of the error and\nuncertainty associated with both the most likely path and the associated travel\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 18:06:51 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 17:49:13 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 18:08:33 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 18:35:21 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["O'Malley", "Michael", ""], ["Sykulski", "Adam M.", ""], ["Laso-Jadart", "Romuald", ""], ["Madoui", "Mohammed-Amin", ""]]}, {"id": "2002.07859", "submitter": "Art Owen", "authors": "Art B. Owen and Daniel Rudolf", "title": "A strong law of large numbers for scrambled net integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a strong law of large numbers for integration on\ndigital nets randomized by a nested uniform scramble. The motivating problem is\noptimization over some variables of an integral over others, arising in\nBayesian optimization. This strong law requires that the integrand have a\nfinite moment of order $p$ for some $p>1$. Previously known results implied a\nstrong law only for Riemann integrable functions. Previous general weak laws of\nlarge numbers for scrambled nets require a square integrable integrand. We\ngeneralize from $L^2$ to $L^p$ for $p>1$ via the Riesz-Thorin interpolation\ntheorem\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 20:07:55 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 00:12:38 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 22:41:47 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Owen", "Art B.", ""], ["Rudolf", "Daniel", ""]]}, {"id": "2002.08129", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael U. Gutmann", "title": "Bayesian Experimental Design for Implicit Models by Mutual Information\n  Neural Estimation", "comments": "Accepted at the thirty-seventh International Conference on Machine\n  Learning (ICML) 2020. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Implicit stochastic models, where the data-generation distribution is\nintractable but sampling is possible, are ubiquitous in the natural sciences.\nThe models typically have free parameters that need to be inferred from data\ncollected in scientific experiments. A fundamental question is how to design\nthe experiments so that the collected data are most useful. The field of\nBayesian experimental design advocates that, ideally, we should choose designs\nthat maximise the mutual information (MI) between the data and the parameters.\nFor implicit models, however, this approach is severely hampered by the high\ncomputational cost of computing posteriors and maximising MI, in particular\nwhen we have more than a handful of design variables to optimise. In this\npaper, we propose a new approach to Bayesian experimental design for implicit\nmodels that leverages recent advances in neural MI estimation to deal with\nthese issues. We show that training a neural network to maximise a lower bound\non MI allows us to jointly determine the optimal design and the posterior.\nSimulation studies illustrate that this gracefully extends Bayesian\nexperimental design for implicit models to higher design dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 12:09:42 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 17:28:45 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 15:04:46 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2002.08757", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Asymptotically Optimal Bias Reduction for Parametric Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.11541", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. This problem is magnified in high-dimensional\nsettings where the number of variables $p$ diverges with the sample size $n$,\nas well as for nonlinear models and/or models with discrete data. For these\ncomplex settings, we propose to use a general simulation-based approach and\nshow that the resulting estimator has a bias of order $\\mathcal{O}(0)$, hence\nproviding an asymptotically optimal bias reduction. It is based on an initial\nestimator that can be slightly asymptotically biased, making the approach very\ngenerally applicable. This is particularly relevant when classical estimators,\nsuch as the maximum likelihood estimator, can only be (numerically)\napproximated. We show that the iterative bootstrap of Kuk (1995) provides a\ncomputationally efficient approach to compute this bias reduced estimator. We\nillustrate our theoretical results in simulation studies for which we develop\nnew bias reduced estimators for the logistic regression, with and without\nrandom effects. These estimators enjoy additional properties such as robustness\nto data contamination and to the problem of separability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:11:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "2002.08831", "submitter": "Don March", "authors": "Don March and Vandy Tombs", "title": "Efficiently updating a covariance matrix and its LDL decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Equations are presented which efficiently update or downdate the covariance\nmatrix of a large number of $m$-dimensional observations. Updates and downdates\nto the covariance matrix, as well as mixed updates/downdates, are shown to be\nrank-$k$ modifications, where $k$ is the number of new observations added plus\nthe number of old observations removed. As a result, the update and downdate\nequations decrease the required number of multiplications for a modification to\n$\\Theta((k+1)m^2)$ instead of $\\Theta((n+k+1)m^2)$ or $\\Theta((n-k+1)m^2)$,\nwhere $n$ is the number of initial observations. Having the rank-$k$ formulas\nfor the updates also allows a number of other known identities to be applied,\nproviding a way of applying updates and downdates directly to the inverse and\ndecompositions of the covariance matrix. To illustrate, we provide an efficient\nalgorithm for applying the rank-$k$ update to the LDL decomposition of a\ncovariance matrix.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:14:12 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["March", "Don", ""], ["Tombs", "Vandy", ""]]}, {"id": "2002.09006", "submitter": "Shin Harase", "authors": "Shin Harase", "title": "A table of short-period Tausworthe generators for Markov chain\n  quasi-Monte Carlo", "comments": null, "journal-ref": "Journal of Computational and Applied Mathematics Volume 384, 1\n  March 2021, 113136, 12 pp", "doi": "10.1016/j.cam.2020.113136", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating expectations by using Markov chain\nMonte Carlo methods and improving the accuracy by replacing IID uniform random\npoints with quasi-Monte Carlo (QMC) points. Recently, it has been shown that\nMarkov chain QMC remains consistent when the driving sequences are completely\nuniformly distributed (CUD). However, the definition of CUD sequences is not\nconstructive, so an implementation method using short-period Tausworthe\ngenerators (i.e., linear feedback shift register generators over the\ntwo-element field) that approximate CUD sequences has been proposed. In this\npaper, we conduct an exhaustive search of short-period Tausworthe generators\nfor Markov chain QMC in terms of the $t$-value, which is a criterion of\nuniformity widely used in the study of QMC methods. We provide a parameter\ntable of Tausworthe generators and show the effectiveness in numerical examples\nusing Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 20:30:27 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 16:36:18 GMT"}, {"version": "v3", "created": "Wed, 15 Jul 2020 15:02:11 GMT"}, {"version": "v4", "created": "Sun, 2 Aug 2020 15:47:19 GMT"}, {"version": "v5", "created": "Mon, 24 Aug 2020 12:27:16 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Harase", "Shin", ""]]}, {"id": "2002.09209", "submitter": "Christian Thiele", "authors": "Christian Thiele, Gerrit Hirschfeld", "title": "cutpointr: Improved Estimation and Validation of Optimal Cutpoints in R", "comments": "27 pages, 2 tables, 6 figures. To be published in the Journal of\n  Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  'Optimal cutpoints' for binary classification tasks are often established by\ntesting which cutpoint yields the best discrimination, for example the Youden\nindex, in a specific sample. This results in 'optimal' cutpoints that are\nhighly variable and systematically overestimate the out-of-sample performance.\nTo address these concerns, the cutpointr package offers robust methods for\nestimating optimal cutpoints and the out-of-sample performance. The robust\nmethods include bootstrapping and smoothing based on kernel estimation,\ngeneralized additive models, smoothing splines, and local regression. These\nmethods can be applied to a wide range of binary-classification and cost-based\nmetrics. cutpointr also provides mechanisms to utilize user-defined metrics and\nestimation methods. The package has capabilities for parallelization of the\nbootstrapping, including reproducible random number generation. Furthermore, it\nis pipe-friendly, for example for compatibility with functions from tidyverse.\nVarious functions for plotting receiver operating characteristic curves,\nprecision recall graphs, bootstrap results and other representations of the\ndata are included. The package contains example data from a study on\npsychological characteristics and suicide attempts suitable for applying binary\nclassification algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 10:32:25 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Thiele", "Christian", ""], ["Hirschfeld", "Gerrit", ""]]}, {"id": "2002.09264", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Practical Estimation of Renyi Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT math.IT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy Estimation is an important problem with many applications in\ncryptography, statistic,machine learning. Although the estimators optimal with\nrespect to the sample complexity have beenrecently developed, there are still\nsome challenges we address in this paper.The contribution is a novel estimator\nwhich is built directly on the birthday paradox. Theanalysis turns out to be\nconsiderably simpler and offer superior confidence bounds with\nexplicitconstants. We also discuss how streaming algorithm can be used to\nmassively improve memoryconsumption. Last but not least, we study the problem\nof estimation in low or moderate regimes,adapting the estimator and proving\nrigorus bounds.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:20:21 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2002.09275", "submitter": "Rahul Bhadani", "authors": "Rahul Bhadani, Michael Grace, Ivan B. Djordjevic, Jonathan Sprinkle,\n  Saikat Guha", "title": "Programming the Kennedy Receiver for Capacity Maximization versus\n  Minimizing One-shot Error Probability", "comments": "Updating email address format for this replacement submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph eess.SP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We find the capacity attained by the Kennedy receiver for coherent-state BPSK\nwhen the symbol prior p and pre-detection displacement are optimized. The\noptimal displacement is different than what minimizes error probability for\nsingle-shot BPSK state discrimination.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 22:23:59 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 03:23:00 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 00:34:36 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Bhadani", "Rahul", ""], ["Grace", "Michael", ""], ["Djordjevic", "Ivan B.", ""], ["Sprinkle", "Jonathan", ""], ["Guha", "Saikat", ""]]}, {"id": "2002.09309", "submitter": "Alexander Terenin", "authors": "James T. Wilson and Viacheslav Borovitskiy and Alexander Terenin and\n  Peter Mostowsky and Marc Peter Deisenroth", "title": "Efficiently Sampling Functions from Gaussian Process Posteriors", "comments": null, "journal-ref": "International Conference on Machine Learning, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes are the gold standard for many real-world modeling\nproblems, especially in cases where a model's success hinges upon its ability\nto faithfully represent predictive uncertainty. These problems typically exist\nas parts of larger frameworks, wherein quantities of interest are ultimately\ndefined by integrating over posterior distributions. These quantities are\nfrequently intractable, motivating the use of Monte Carlo methods. Despite\nsubstantial progress in scaling up Gaussian processes to large training sets,\nmethods for accurately generating draws from their posterior distributions\nstill scale cubically in the number of test locations. We identify a\ndecomposition of Gaussian processes that naturally lends itself to scalable\nsampling by separating out the prior from the data. Building off of this\nfactorization, we propose an easy-to-use and general-purpose approach for fast\nposterior sampling, which seamlessly pairs with sparse approximations to afford\nscalability both during training and at test time. In a series of experiments\ndesigned to test competing sampling schemes' statistical properties and\npractical ramifications, we demonstrate how decoupled sample paths accurately\nrepresent Gaussian process posteriors at a fraction of the usual cost.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:03:16 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 16:22:27 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 18:13:39 GMT"}, {"version": "v4", "created": "Sun, 16 Aug 2020 13:37:40 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wilson", "James T.", ""], ["Borovitskiy", "Viacheslav", ""], ["Terenin", "Alexander", ""], ["Mostowsky", "Peter", ""], ["Deisenroth", "Marc Peter", ""]]}, {"id": "2002.09377", "submitter": "Owen Thomas", "authors": "Owen Thomas, Henri Pesonen, Raquel S\\'a-Le\\~ao, Herm\\'inia de\n  Lencastre, Samuel Kaski, Jukka Corander", "title": "Split-BOLFI for for misspecification-robust likelihood free inference in\n  high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free inference for simulator-based statistical models has recently\ngrown rapidly from its infancy to a useful tool for practitioners. However,\nmodels with more than a very small number of parameters as the target of\ninference have remained an enigma, in particular for the approximate Bayesian\ncomputation (ABC) community. To advance the possibilities for performing\nlikelihood-free inference in high-dimensional parameter spaces, here we\nintroduce an extension of the popular Bayesian optimisation based approach to\napproximate discrepancy functions in a probabilistic manner which lends itself\nto an efficient exploration of the parameter space. Our method achieves\ncomputational scalability by using separate acquisition procedures for the\ndiscrepancies defined for different parameters. These efficient\nhigh-dimensional simulation acquisitions are combined with exponentiated\nloss-likelihoods to provide a misspecification-robust characterisation of the\nmarginal posterior distribution for all model parameters. The method\nsuccessfully performs computationally efficient inference in a 100-dimensional\nspace on canonical examples and compares favourably to existing Copula-ABC\nmethods. We further illustrate the potential of this approach by fitting a\nbacterial transmission dynamics model to daycare centre data, which provides\nbiologically coherent results on the strain competition in a 30-dimensional\nparameter space.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 16:06:11 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["S\u00e1-Le\u00e3o", "Raquel", ""], ["de Lencastre", "Herm\u00ednia", ""], ["Kaski", "Samuel", ""], ["Corander", "Jukka", ""]]}, {"id": "2002.09633", "submitter": "Sam Brilleman", "authors": "Samuel L. Brilleman (1), Eren M. Elci (2), Jacqueline Buros Novik (3),\n  Rory Wolfe (1) ((1) Monash University, Melbourne, Australia, (2) Bayer AG,\n  Berlin, Germany, (3) Generable Inc, New York, USA)", "title": "Bayesian Survival Analysis Using the rstanarm R Package", "comments": "50 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival data is encountered in a range of disciplines, most notably health\nand medical research. Although Bayesian approaches to the analysis of survival\ndata can provide a number of benefits, they are less widely used than classical\n(e.g. likelihood-based) approaches. This may be in part due to a relative\nabsence of user-friendly implementations of Bayesian survival models. In this\narticle we describe how the rstanarm R package can be used to fit a wide range\nof Bayesian survival models. The rstanarm package facilitates Bayesian\nregression modelling by providing a user-friendly interface (users specify\ntheir model using customary R formula syntax and data frames) and using the\nStan software (a C++ library for Bayesian inference) for the back-end\nestimation. The suite of models that can be estimated using rstanarm is broad\nand includes generalised linear models (GLMs), generalised linear mixed models\n(GLMMs), generalised additive models (GAMs) and more. In this article we focus\nonly on the survival modelling functionality. This includes standard parametric\n(exponential, Weibull, Gompertz) and flexible parametric (spline-based) hazard\nmodels, as well as standard parametric accelerated failure time (AFT) models.\nAll types of censoring (left, right, interval) are allowed, as is delayed entry\n(left truncation), time-varying covariates, time-varying effects, and frailty\neffects. We demonstrate the functionality through worked examples. We\nanticipate these implementations will increase the uptake of Bayesian survival\nanalysis in applied research.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 05:39:32 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Brilleman", "Samuel L.", ""], ["Elci", "Eren M.", ""], ["Novik", "Jacqueline Buros", ""], ["Wolfe", "Rory", ""]]}, {"id": "2002.09716", "submitter": "Jingchen Hu", "authors": "Jim Albert and Jingchen Hu", "title": "Bayesian Computing in the Undergraduate Statistics Curriculum", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian statistics has gained great momentum since the computational\ndevelopments of the 1990s. Gradually, advances in Bayesian methodology and\nsoftware have made Bayesian techniques much more accessible to applied\nstatisticians and, in turn, have potentially transformed Bayesian education at\nthe undergraduate level. This article provides an overview on the various\noptions for implementing Bayesian computational methods motivated to achieve\nparticular learning outcomes. The advantages and disadvantages of each\ncomputational method are described based on the authors' experience in using\nthese methods in the classroom. The goal is to present guidance on the choice\nof computation for the instructors who are introducing Bayesian methods in\ntheir undergraduate statistics curriculum.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 15:01:25 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 12:52:35 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 15:42:21 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Albert", "Jim", ""], ["Hu", "Jingchen", ""]]}, {"id": "2002.09998", "submitter": "Ayman Boustati", "authors": "Ayman Boustati, \\\"Omer Deniz Akyildiz, Theodoros Damoulas, Adam M.\n  Johansen", "title": "Generalized Bayesian Filtering via Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for inference in general state-space hidden Markov\nmodels (HMMs) under likelihood misspecification. In particular, we leverage the\nloss-theoretic perspective of Generalized Bayesian Inference (GBI) to define\ngeneralised filtering recursions in HMMs, that can tackle the problem of\ninference under model misspecification. In doing so, we arrive at principled\nprocedures for robust inference against observation contamination by utilising\nthe $\\beta$-divergence. Operationalising the proposed framework is made\npossible via sequential Monte Carlo methods (SMC), where most standard particle\nmethods, and their associated convergence results, are readily adapted to the\nnew setting. We apply our approach to object tracking and Gaussian process\nregression problems, and observe improved performance over both standard\nfiltering algorithms and other robust filters.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 22:15:52 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 15:05:58 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Boustati", "Ayman", ""], ["Akyildiz", "\u00d6mer Deniz", ""], ["Damoulas", "Theodoros", ""], ["Johansen", "Adam M.", ""]]}, {"id": "2002.10046", "submitter": "Anderson Winkler", "authors": "Anderson M. Winkler, Olivier Renaud, Stephen M. Smith, Thomas E.\n  Nichols", "title": "Permutation Inference for Canonical Correlation Analysis", "comments": "49 pages, 2 figures, 10 tables, 3 algorithms, 119 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has become a key tool for population\nneuroimaging, allowing investigation of associations between many imaging and\nnon-imaging measurements. As other variables are often a source of variability\nnot of direct interest, previous work has used CCA on residuals from a model\nthat removes these effects, then proceeded directly to permutation inference.\nWe show that such a simple permutation test leads to inflated error rates. The\nreason is that residualisation introduces dependencies among the observations\nthat violate the exchangeability assumption. Even in the absence of nuisance\nvariables, however, a simple permutation test for CCA also leads to excess\nerror rates for all canonical correlations other than the first. The reason is\nthat a simple permutation scheme does not ignore the variability already\nexplained by previous canonical variables. Here we propose solutions for both\nproblems: in the case of nuisance variables, we show that transforming the\nresiduals to a lower dimensional basis where exchangeability holds results in a\nvalid permutation test; for more general cases, with or without nuisance\nvariables, we propose estimating the canonical correlations in a stepwise\nmanner, removing at each iteration the variance already explained, while\ndealing with different number of variables in both sides. We also discuss how\nto address the multiplicity of tests, proposing an admissible test that is not\nconservative, and provide a complete algorithm for permutation inference for\nCCA.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:47:01 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:45:59 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 18:23:36 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 01:15:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Winkler", "Anderson M.", ""], ["Renaud", "Olivier", ""], ["Smith", "Stephen M.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2002.10071", "submitter": "Dao Nguyen", "authors": "Anh Duc Doan, Xin Dang, Dao Nguyen", "title": "Black-box sampling for weakly smooth Langevin Monte Carlo using\n  p-generalized Gaussian smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discretization of continuous-time diffusion processes is a widely recognized\nmethod for sampling. However, the canonical Euler-Maruyama discretization of\nthe Langevin diffusion process, also named as Langevin Monte Carlo (LMC),\nstudied mostly in the context of smooth (gradient-Lipschitz) and strongly\nlog-concave densities, a significant constraint for its deployment in many\nsciences, including computational statistics and statistical learning. In this\npaper, we establish several theoretical contributions to the literature on such\nsampling methods. Particularly, we generalize the Gaussian smoothing,\napproximate the gradient using p-generalized Gaussian smoothing and take\nadvantage of it in the context of black-box sampling. We first present a\nnon-strongly concave and weakly smooth black-box LMC algorithm, ideal for\npractical applicability of sampling challenges in a general setting.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 04:18:15 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 15:04:24 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Doan", "Anh Duc", ""], ["Dang", "Xin", ""], ["Nguyen", "Dao", ""]]}, {"id": "2002.10335", "submitter": "Fabio Rapallo", "authors": "Giovanni Pistone, Fabio Rapallo, Maria Piera Rogantin", "title": "Finite space Kantorovich problem with an MCMC of table moves", "comments": "25 pages; a proof has been added and some notational issues have been\n  fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Optimal Transport (OT) on a finite metric space, one defines a distance on\nthe probability simplex that extends the distance on the ground space. The\ndistance is the value of a Linear Programming (LP) problem on the set of\nnon-negative-valued 2-way tables with assigned probability functions as\nmargins. We apply to this case the methodology of moves from Algebraic\nStatistics (AS) and use it to derive a Monte Carlo Markov Chain (MCMC) solution\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 16:05:39 GMT"}, {"version": "v2", "created": "Wed, 25 Mar 2020 16:20:37 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 14:18:06 GMT"}], "update_date": "2021-01-14", "authors_parsed": [["Pistone", "Giovanni", ""], ["Rapallo", "Fabio", ""], ["Rogantin", "Maria Piera", ""]]}, {"id": "2002.10883", "submitter": "Guosheng Yin", "authors": "Guosheng Yin and Haolun Shi", "title": "Demystify Lindley's Paradox by Interpreting P-value as Posterior\n  Probability", "comments": "arXiv admin note: text overlap with arXiv:1809.08503", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the hypothesis testing framework, p-value is often computed to determine\nrejection of the null hypothesis or not. On the other hand, Bayesian approaches\ntypically compute the posterior probability of the null hypothesis to evaluate\nits plausibility. We revisit Lindley's paradox (Lindley, 1957) and demystify\nthe conflicting results between Bayesian and frequentist hypothesis testing\nprocedures by casting a two-sided hypothesis as a combination of two one-sided\nhypotheses along the opposite directions. This can naturally circumvent the\nambiguities of assigning a point mass to the null and choices of using local or\nnon-local prior distributions. As p-value solely depends on the observed data\nwithout incorporating any prior information, we consider non-informative prior\ndistributions for fair comparisons with p-value. The equivalence of p-value and\nthe Bayesian posterior probability of the null hypothesis can be established to\nreconcile Lindley's paradox. Extensive simulation studies are conducted with\nmultivariate normal data and random effects models to examine the relationship\nbetween the p-value and posterior probability.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 05:18:03 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Yin", "Guosheng", ""], ["Shi", "Haolun", ""]]}, {"id": "2002.10902", "submitter": "Owen Thomas", "authors": "Owen Thomas, Henri Pesonen, Jukka Corander", "title": "Probabilistic elicitation of expert knowledge through assessment of\n  computer simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for probabilistic elicitation of expert knowledge\nusing binary responses of human experts assessing simulated data from a\nstatistical model, where the parameters are subject to uncertainty. The binary\nresponses describe either the absolute realism of individual simulations or the\nrelative realism of a pair of simulations in the two alternative versions of\nout approach. Each version provides a nonparametric representation of the\nexpert belief distribution over the values of a model parameter, without\ndemanding the assertion of any opinion on the parameter values themselves. Our\nframework also integrates the use of active learning to efficiently query the\nexperts, with the possibility to additionally provide a useful misspecification\ndiagnostic. We validate both methods on an automatic expert judging a binomial\ndistribution, and on human experts judging the distribution of voters across\npolitical parties in the United States and Norway. Both methods provide\nflexible and meaningful representations of the human experts' beliefs,\ncorrectly identifying the higher dispersion of voters between parties in\nNorway.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 14:42:48 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 13:28:23 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 14:18:20 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Thomas", "Owen", ""], ["Pesonen", "Henri", ""], ["Corander", "Jukka", ""]]}, {"id": "2002.11243", "submitter": "Roel Ceballos", "authors": "Rena Sandy H. Baculinao and Roel F. Ceballos", "title": "Correspondence Analysis between the Location and the Leading Causes of\n  Death in the United States", "comments": null, "journal-ref": "International Journal of Ecological Economics and Statistics,\n  41(1), 47-54, 2020", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correspondence Analysis analyzes two-way or multi-way tables withe each row\nand column becoming a point ion a multidimensional graphical map called biplot.\nIt can be used to extract essential dimensions allowing simplification of the\ndata matrix. This study aims to measure the association between the location\nand the leading causes of death in the United States of America and to\ndetermine the location where a particular disease is highly associated. The\nresearch data consists of two variables with 510 data points. Results show that\nthere is a significant association between the location ad leading cause of\ndeath in the United States, and 61% of the variance in the model are explained\nby the first two dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:08:41 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Baculinao", "Rena Sandy H.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "2002.11275", "submitter": "Alex Luedtke", "authors": "Alex Luedtke, Incheoul Chung, Oleg Sofrygin", "title": "Adversarial Monte Carlo Meta-Learning of Optimal Prediction Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame the meta-learning of prediction procedures as a search for an\noptimal strategy in a two-player game. In this game, Nature selects a prior\nover distributions that generate labeled data consisting of features and an\nassociated outcome, and the Predictor observes data sampled from a distribution\ndrawn from this prior. The Predictor's objective is to learn a function that\nmaps from a new feature to an estimate of the associated outcome. We establish\nthat, under reasonable conditions, the Predictor has an optimal strategy that\nis equivariant to shifts and rescalings of the outcome and is invariant to\npermutations of the observations and to shifts, rescalings, and permutations of\nthe features. We introduce a neural network architecture that satisfies these\nproperties. The proposed strategy performs favorably compared to standard\npractice in both parametric and nonparametric experiments.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:16:05 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 22:26:02 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Luedtke", "Alex", ""], ["Chung", "Incheoul", ""], ["Sofrygin", "Oleg", ""]]}, {"id": "2002.11543", "submitter": "Julien Bect", "authors": "S\\'ebastien Petit (L2S, GdR MASCOT-NUM), Julien Bect (L2S, GdR\n  MASCOT-NUM), S\\'ebastien da Veiga (GdR MASCOT-NUM), Paul Feliot (GdR\n  MASCOT-NUM), Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "Towards new cross-validation-based estimators for Gaussian process\n  regression: efficient adjoint computation of gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters of the covariance\nfunction of a Gaussian process by cross-validation. We suggest using new\ncross-validation criteria derived from the literature of scoring rules. We also\nprovide an efficient method for computing the gradient of a cross-validation\ncriterion. To the best of our knowledge, our method is more efficient than what\nhas been proposed in the literature so far. It makes it possible to lower the\ncomplexity of jointly evaluating leave-one-out criteria and their gradients.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:50:54 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 11:25:04 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Petit", "S\u00e9bastien", "", "L2S, GdR MASCOT-NUM"], ["Bect", "Julien", "", "L2S, GdR\n  MASCOT-NUM"], ["da Veiga", "S\u00e9bastien", "", "GdR MASCOT-NUM"], ["Feliot", "Paul", "", "GdR\n  MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2002.11577", "submitter": "Etienne C\\^ome", "authors": "Etienne C\\^ome, Nicolas Jouvin, Pierre Latouche and Charles Bouveyron", "title": "Hierarchical clustering with discrete latent variable models and the\n  integrated classification likelihood", "comments": "Adv Data Anal Classif (2021)", "journal-ref": null, "doi": "10.1007/s11634-021-00440-z", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding a set of nested partitions of a dataset is useful to uncover relevant\nstructure at different scales, and is often dealt with a data-dependent\nmethodology. In this paper, we introduce a general two-step methodology for\nmodel-based hierarchical clustering. Considering the integrated classification\nlikelihood criterion as an objective function, this work applies to every\ndiscrete latent variable models (DLVMs) where this quantity is tractable. The\nfirst step of the methodology involves maximizing the criterion with respect to\nthe partition. Addressing the known problem of sub-optimal local maxima found\nby greedy hill climbing heuristics, we introduce a new hybrid algorithm based\non a genetic algorithm efficiently exploring the space of solutions. The\nresulting algorithm carefully combines and merges different solutions, and\nallows the joint inference of the number $K$ of clusters as well as the\nclusters themselves. Starting from this natural partition, the second step of\nthe methodology is based on a bottom-up greedy procedure to extract a hierarchy\nof clusters. In a Bayesian context, this is achieved by considering the\nDirichlet cluster proportion prior parameter $\\alpha$ as a regularization term\ncontrolling the granularity of the clustering. A new approximation of the\ncriterion is derived as a log-linear function of $\\alpha$, enabling a simple\nfunctional form of the merge decision criterion. This second step allows the\nexploration of the clustering at coarser scales. The proposed approach is\ncompared with existing strategies on simulated as well as real settings, and\nits results are shown to be particularly relevant. A reference implementation\nof this work is available in the R package greed accompanying the paper.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:50:44 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 11:55:00 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 10:48:22 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["C\u00f4me", "Etienne", ""], ["Jouvin", "Nicolas", ""], ["Latouche", "Pierre", ""], ["Bouveyron", "Charles", ""]]}, {"id": "2002.11618", "submitter": "Till Koebe", "authors": "Till Koebe", "title": "Better coverage, better outcomes? Mapping mobile network data to\n  official statistics using satellite imagery and radio propagation modelling", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0241981", "report-no": null, "categories": "cs.CY stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile sensing data has become a popular data source for geo-spatial\nanalysis, however, mapping it accurately to other sources of information such\nas statistical data remains a challenge. Popular mapping approaches such as\npoint allocation or voronoi tessellation provide only crude approximations of\nthe mobile network coverage as they do not consider holes, overlaps and\nwithin-cell heterogeneity. More elaborate mapping schemes often require\nadditional proprietary data operators are highly reluctant to share. In this\npaper, I use human settlement information extracted from publicly available\nsatellite imagery in combination with stochastic radio propagation modelling\ntechniques to account for that. I investigate in a simulation study and a\nreal-world application on unemployment estimates in Senegal whether better\ncoverage approximations lead to better outcome predictions. The good news is:\nit does not have to be complicated.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 14:19:19 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Koebe", "Till", ""]]}, {"id": "2002.11631", "submitter": "Zhenyu Zhao", "authors": "Huigang Chen, Totte Harinen, Jeong-Yoon Lee, Mike Yung, Zhenyu Zhao", "title": "CausalML: Python Package for Causal Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CausalML is a Python implementation of algorithms related to causal inference\nand machine learning. Algorithms combining causal inference and machine\nlearning have been a trending topic in recent years. This package tries to\nbridge the gap between theoretical work on methodology and practical\napplications by making a collection of methods in this field available in\nPython. This paper introduces the key concepts, scope, and use cases of this\npackage.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 17:35:33 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 18:34:29 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Chen", "Huigang", ""], ["Harinen", "Totte", ""], ["Lee", "Jeong-Yoon", ""], ["Yung", "Mike", ""], ["Zhao", "Zhenyu", ""]]}, {"id": "2002.11900", "submitter": "Kartik Iyer", "authors": "Kartik P. Iyer, Katepalli R. Sreenivasan and P. K. Yeung", "title": "Scaling exponents saturate in three-dimensional isotropic turbulence", "comments": null, "journal-ref": "Phys. Rev. Fluids 5, 054605 (2020)", "doi": "10.1103/PhysRevFluids.5.054605", "report-no": null, "categories": "physics.flu-dyn physics.comp-ph physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From a database of direct numerical simulations of homogeneous and isotropic\nturbulence, generated in periodic boxes of various sizes, we extract the\nspherically symmetric part of moments of velocity increments and first verify\nthe following (somewhat contested) results: the $4/5$-ths law holds in an\nintermediate range of scales and that the second order exponent over the same\nrange of scales is {\\it{anomalous}}, departing from the self-similar value of\n$2/3$ and approaching a constant of $0.72$ at high Reynolds numbers. We compare\nwith some typical theories the dependence of longitudinal exponents as well as\ntheir derivatives with respect to the moment order $n$, and estimate the most\nprobable value of the H\\\"older exponent. We demonstrate that the transverse\nscaling exponents saturate for large $n$, and trace this trend to the presence\nof large localized jumps in the signal. The saturation value of about $2$ at\nthe highest Reynolds number suggests, when interpreted in the spirit of\nfractals, the presence of vortex sheets rather than more complex singularities.\nIn general, the scaling concept in hydrodynamic turbulence appears to be more\ncomplex than even the multifractal description.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 03:48:50 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 13:26:14 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Iyer", "Kartik P.", ""], ["Sreenivasan", "Katepalli R.", ""], ["Yeung", "P. K.", ""]]}, {"id": "2002.11916", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Fang Xie, and Johannes Lederer", "title": "Tuning-free ridge estimators for high-dimensional generalized linear\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge estimators regularize the squared Euclidean lengths of parameters. Such\nestimators are mathematically and computationally attractive but involve tuning\nparameters that can be difficult to calibrate. In this paper, we show that\nridge estimators can be modified such that tuning parameters can be avoided\naltogether. We also show that these modified versions can improve on the\nempirical prediction accuracies of standard ridge estimators combined with\ncross-validation, and we provide first theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 05:01:42 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Huang", "Shih-Ting", ""], ["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "2002.12024", "submitter": "Elmar Plischke", "authors": "Elmar Plischke, Giovanni Rabitti, Emanuele Borgonovo", "title": "Computing Shapley Effects for Sensitivity Analysis", "comments": "16 pages, 5 figures, 3 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley effects are attracting increasing attention as sensitivity measures.\nWhen the value function is the conditional variance, they account for the\nindividual and higher order effects of a model input. They are also well\ndefined under model input dependence. However, one of the issues associated\nwith their use is computational cost. We present a new algorithm that offers\nmajor improvements for the computation of Shapley effects, reducing\ncomputational burden by several orders of magnitude (from $k!\\cdot k$ to $2^k$,\nwhere $k$ is the number of inputs) with respect to currently available\nimplementations. The algorithm works in the presence of input dependencies. The\nalgorithm also makes it possible to estimate all generalized (Shapley-Owen)\neffects for interactions.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 10:34:26 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Plischke", "Elmar", ""], ["Rabitti", "Giovanni", ""], ["Borgonovo", "Emanuele", ""]]}, {"id": "2002.12253", "submitter": "Maxim Panov", "authors": "Achille Thin, Nikita Kotelevskii, Jean-Stanislas Denain, Leo\n  Grinsztajn, Alain Durmus, Maxim Panov and Eric Moulines", "title": "MetFlow: A New Efficient Method for Bridging the Gap between Markov\n  Chain Monte Carlo and Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this contribution, we propose a new computationally efficient method to\ncombine Variational Inference (VI) with Markov Chain Monte Carlo (MCMC). This\napproach can be used with generic MCMC kernels, but is especially well suited\nto \\textit{MetFlow}, a novel family of MCMC algorithms we introduce, in which\nproposals are obtained using Normalizing Flows. The marginal distribution\nproduced by such MCMC algorithms is a mixture of flow-based distributions, thus\ndrastically increasing the expressivity of the variational family. Unlike\nprevious methods following this direction, our approach is amenable to the\nreparametrization trick and does not rely on computationally expensive reverse\nkernels. Extensive numerical experiments show clear computational and\nperformance improvements over state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 16:50:30 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Thin", "Achille", ""], ["Kotelevskii", "Nikita", ""], ["Denain", "Jean-Stanislas", ""], ["Grinsztajn", "Leo", ""], ["Durmus", "Alain", ""], ["Panov", "Maxim", ""], ["Moulines", "Eric", ""]]}, {"id": "2002.12606", "submitter": "Benjamin Stokell", "authors": "Benjamin G. Stokell, Rajen D. Shah, Ryan J. Tibshirani", "title": "Modelling High-Dimensional Categorical Data Using Nonconvex Fusion\n  Penalties", "comments": "52 pages, 10 figures; to appear in JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimation in high-dimensional linear models with\nnominal categorical data. Our estimator, called SCOPE, fuses levels together by\nmaking their corresponding coefficients exactly equal. This is achieved using\nthe minimax concave penalty on differences between the order statistics of the\ncoefficients for a categorical variable, thereby clustering the coefficients.\nWe provide an algorithm for exact and efficient computation of the global\nminimum of the resulting nonconvex objective in the case with a single variable\nwith potentially many levels, and use this within a block coordinate descent\nprocedure in the multivariate case. We show that an oracle least squares\nsolution that exploits the unknown level fusions is a limit point of the\ncoordinate descent with high probability, provided the true levels have a\ncertain minimum separation; these conditions are known to be minimal in the\nunivariate case. We demonstrate the favourable performance of SCOPE across a\nrange of real and simulated datasets. An R package CatReg implementing SCOPE\nfor linear models and also a version for logistic regression is available on\nCRAN.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:20:41 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:52:13 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:45:06 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 14:48:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Stokell", "Benjamin G.", ""], ["Shah", "Rajen D.", ""], ["Tibshirani", "Ryan J.", ""]]}]