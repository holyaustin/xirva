[{"id": "1403.0211", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts, Rob Hall, and Stephen E. Fienberg", "title": "SMERED: A Bayesian Approach to Graphical Record Linkage and\n  De-duplication", "comments": "AISTATS (2014), to appear; 9 pages with references, 2 page\n  supplement, 4 figures. Shorter version of arXiv:1312.4645", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel unsupervised approach for linking records across\narbitrarily many files, while simultaneously detecting duplicate records within\nfiles. Our key innovation is to represent the pattern of links between records\nas a {\\em bipartite} graph, in which records are directly linked to latent true\nindividuals, and only indirectly linked to other records. This flexible new\nrepresentation of the linkage structure naturally allows us to estimate the\nattributes of the unique observable people in the population, calculate $k$-way\nposterior probabilities of matches across records, and propagate the\nuncertainty of record linkage into later analyses. Our linkage structure lends\nitself to an efficient, linear-time, hybrid Markov chain Monte Carlo algorithm,\nwhich overcomes many obstacles encountered by previously proposed methods of\nrecord linkage, despite the high dimensional parameter space. We assess our\nresults on real and simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 14:21:20 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Hall", "Rob", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1403.0387", "submitter": "Clara Grazian", "authors": "Clara Grazian and Brunero Liseo", "title": "Approximate Integrated Likelihood via ABC methods", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel use of a recent new computational tool for Bayesian\ninference, namely the Approximate Bayesian Computation (ABC) methodology. ABC\nis a way to handle models for which the likelihood function may be intractable\nor even unavailable and/or too costly to evaluate; in particular, we consider\nthe problem of eliminating the nuisance parameters from a complex statistical\nmodel in order to produce a likelihood function depending on the quantity of\ninterest only. Given a proper prior for the entire vector parameter, we propose\nto approximate the integrated likelihood by the ratio of kernel estimators of\nthe marginal posterior and prior for the quantity of interest. We present\nseveral examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 11:04:20 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Grazian", "Clara", ""], ["Liseo", "Brunero", ""]]}, {"id": "1403.0532", "submitter": "Alejandro Frery", "authors": "R. Ospina and A. M. Larangeiras and A. C. Frery", "title": "Visualization of Skewed Data: A Tool in R", "comments": "Submitted to the Revista Colombiana de Estad\\'istica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a visualization tool specifically tailored to deal\nwith skewed data. The technique is based upon the use of two types of notched\nboxplots (the usual one, and one which is tuned for the skewness of the data),\nthe violin plot, the histogram and a nonparametric estimate of the density. The\ndata is assumed to lie on the same line, so the plots are compatible. We show\nthat a good deal of information can be extracted from the inspection of this\ntool; in particular, we apply the technique to analyze data from synthetic\naperture radar images. We provide the implementation in R.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 19:27:33 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Ospina", "R.", ""], ["Larangeiras", "A. M.", ""], ["Frery", "A. C.", ""]]}, {"id": "1403.1562", "submitter": "Krzysztof Bartoszek", "authors": "Krzysztof Bartoszek", "title": "The Laplace Motion in Phylogenetic Comparative Methods", "comments": "http://kkzmbm.mimuw.edu.pl/?pageId=4&sprawId=18", "journal-ref": "Proceedings of the Eighteenth National Conference on Applications\n  of Mathematics in Biology and Medicine, 2012", "doi": null, "report-no": null, "categories": "q-bio.PE math.PR stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of current phylogenetic comparative methods assume that the\nstochastic evolutionary process is homogeneous over the phylogeny or offer\nrelaxations of this in rather limited and usually parameter expensive ways.\nHere we make a preliminary investigation, by means of a numerical experiment,\nwhether the Laplace motion process can offer an alternative approach.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 20:24:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Bartoszek", "Krzysztof", ""]]}, {"id": "1403.1913", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Bayesian bandwidth estimation for a nonparametric functional regression\n  model with mixed types of regressors and unknown error density", "comments": null, "journal-ref": "Journal of Nonparametric Statistics, 2014, 26(3), 599-615", "doi": "10.1080/10485252.2014.916806", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the issue of bandwidth estimation in a nonparametric\nfunctional regression model with function-valued, continuous real-valued and\ndiscrete-valued regressors under the framework of unknown error density.\nExtending from the recent work of Shang (2013, Computational Statistics & Data\nAnalysis), we approximate the unknown error density by a kernel density\nestimator of residuals, where the regression function is estimated by the\nfunctional Nadaraya-Watson estimator that admits mixed types of regressors. We\nderive a kernel likelihood and posterior density for the bandwidth parameters\nunder the kernel-form error density, and put forward a Bayesian bandwidth\nestimation approach that can simultaneously estimate the bandwidths. Simulation\nstudies demonstrated the estimation accuracy of the regression function and\nerror density for the proposed Bayesian approach. Illustrated by a spectroscopy\ndata set in the food quality control, we applied the proposed Bayesian approach\nto select the optimal bandwidths in a nonparametric functional regression model\nwith mixed types of regressors.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 01:30:41 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1403.2036", "submitter": "Mathew McLean", "authors": "Mathew W. McLean", "title": "Straightforward Bibliography Management in R with the RefManageR Package", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces the R package RefManageR, which provides tools for\nimporting and working with bibliographic references. It extends the bibentry\nclass in R in a number of useful ways, including providing R with previously\nunavailable support for BibLaTeX. BibLaTeX provides a superset of the\nfunctionality of BibTeX, including full Unicode support, no memory limitations,\nadditional fields and entry types, and more sophisticated sorting of\nreferences. RefManageR provides functions for citing and generating a\nbibliography with hyperlinks for documents prepared with RMarkdown or RHTML.\nExisting .bib files can be read into R and converted from BibTeX to BibLaTeX\nand vice versa. References can also be imported via queries to NCBI's Entrez,\nZotero libraries, Google Scholar, and CrossRef. Additionally, references can be\ncreated by reading PDFs stored on the user's machine with the help of Poppler.\nEntries stored in the reference manager can be easily searched by any field, by\ndate ranges, and by various formats for name lists (author by last names,\ntranslator by full names, etc.). Entries can also be updated, combined, sorted,\nprinted in a number of styles, and exported.\n", "versions": [{"version": "v1", "created": "Sun, 9 Mar 2014 08:09:02 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["McLean", "Mathew W.", ""]]}, {"id": "1403.2397", "submitter": "Li Ma", "authors": "Li Ma", "title": "Scalable Bayesian model averaging through local information propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a probabilistic version of the classical forward-stepwise\nvariable inclusion procedure can serve as a general data-augmentation scheme\nfor model space distributions in (generalized) linear models. This latent\nvariable representation takes the form of a Markov process, thereby allowing\ninformation propagation algorithms to be applied for sampling from model space\nposteriors. In particular, we propose a sequential Monte Carlo method for\nachieving effective unbiased Bayesian model averaging in high-dimensional\nproblems, utilizing proposal distributions constructed using local information\npropagation. We illustrate our method---called LIPS for local information\npropagation based sampling---through real and simulated examples with\ndimensionality ranging from 15 to 1,000, and compare its performance in\nestimating posterior inclusion probabilities and in out-of-sample prediction to\nthose of several other methods---namely, MCMC, BAS, iBMA, and LASSO. In\naddition, we show that the latent variable representation can also serve as a\nmodeling tool for specifying model space priors that account for knowledge\nregarding model complexity and conditional inclusion relationships.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 20:11:01 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 00:14:46 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1403.2649", "submitter": "Art Owen", "authors": "Kinjal Basu and Art B. Owen", "title": "Low discrepancy constructions in the triangle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most quasi-Monte Carlo research focuses on sampling from the unit cube. Many\nproblems, especially in computer graphics, are defined via quadrature over the\nunit triangle. Quasi-Monte Carlo methods for the triangle have been developed\nby Pillands and Cools (2005) and by Brandolini et al. (2013). This paper\npresents two QMC constructions in the triangle with a vanishing discrepancy.\nThe first is a version of the van der Corput sequence customized to the unit\ntriangle. It is an extensible digital construction that attains a discrepancy\nbelow 12/sqrt(N). The second construction rotates an integer lattice through an\nangle whose tangent is a quadratic irrational number. It attains a discrepancy\nof O(log(N)/N) which is the best possible rate. Previous work strongly\nindicated that such a discrepancy was possible, but no constructions were\navailable. Scrambling the digits of the first construction improves its\naccuracy for integration of smooth functions. Both constructions also yield\nconvergent estimates for integrands that are Riemann integrable on the triangle\nwithout requiring bounded variation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 17:02:59 GMT"}], "update_date": "2014-03-12", "authors_parsed": [["Basu", "Kinjal", ""], ["Owen", "Art B.", ""]]}, {"id": "1403.2805", "submitter": "Jeroen Ooms", "authors": "Jeroen Ooms", "title": "The jsonlite Package: A Practical and Consistent Mapping Between JSON\n  Data and R Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.MS cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A naive realization of JSON data in R maps JSON arrays to an unnamed list,\nand JSON objects to a named list. However, in practice a list is an awkward,\ninefficient type to store and manipulate data. Most statistical applications\nwork with (homogeneous) vectors, matrices or data frames. Therefore JSON\npackages in R typically define certain special cases of JSON structures which\nmap to simpler R types. Currently there exist no formal guidelines, or even\nconsensus between implementations on how R data should be represented in JSON.\nFurthermore, upon closer inspection, even the most basic data structures in R\nactually do not perfectly map to their JSON counterparts and leave some\nambiguity for edge cases. These problems have resulted in different behavior\nbetween implementations and can lead to unexpected output. This paper\nexplicitly describes a mapping between R classes and JSON data, highlights\npotential problems, and proposes conventions that generalize the mapping to\ncover all common structures. We emphasize the importance of type consistency\nwhen using JSON to exchange dynamic data, and illustrate using examples and\nanecdotes. The jsonlite R package is used throughout the paper as a reference\nimplementation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 04:21:10 GMT"}], "update_date": "2014-03-13", "authors_parsed": [["Ooms", "Jeroen", ""]]}, {"id": "1403.2940", "submitter": "Robert B. Gramacy", "authors": "Timothy Graves, Robert B. Gramacy, Christian Franzke, Nicholas Watkins", "title": "Efficient Bayesian inference for long memory processes", "comments": "33 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In forecasting problems it is important to know whether or not recent events\nrepresent a regime change (low long-term predictive potential), or rather a\nlocal manifestation of longer term effects (potentially higher predictive\npotential). Mathematically, a key question is about whether the underlying\nstochastic process exhibits \"memory\", and if so whether the memory is \"long\" in\na precise sense. Being able to detect or rule out such effects can have a\nprofound impact on speculative investment (e.g., in financial markets) and\ninform public policy (e.g., characterising the size and timescales of the earth\nsystem's response to the anthropogenic CO2 perturbation). Most previous work on\ninference of long memory effects is frequentist in nature. Here we provide a\nsystematic treatment of Bayesian inference for long memory processes via the\nAutoregressive Fractional Integrated Moving Average (ARFIMA) model. In\nparticular, we provide a new approximate likelihood for efficient parameter\ninference, and show how nuisance parameters (e.g., short memory effects) can be\nintegrated over in order to focus on long memory parameters and hypothesis\ntesting more directly than ever before. We illustrate our new methodology on\nboth synthetic and observational data, with favorable comparison to the\nstandard estimators.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 14:14:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 01:31:46 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Graves", "Timothy", ""], ["Gramacy", "Robert B.", ""], ["Franzke", "Christian", ""], ["Watkins", "Nicholas", ""]]}, {"id": "1403.2963", "submitter": "Patrick Breheny", "authors": "Sangin Lee and Patrick Breheny", "title": "Strong rules for nonconvex penalties and their implications for\n  efficient algorithms in high-dimensional regression", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 24: 1074-1091\n  (2015)", "doi": "10.1080/10618600.2014.975231", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider approaches for improving the efficiency of algorithms for fitting\nnonconvex penalized regression models such as SCAD and MCP in high dimensions.\nIn particular, we develop rules for discarding variables during cyclic\ncoordinate descent. This dimension reduction leads to a substantial improvement\nin the speed of these algorithms for high-dimensional problems. The rules we\npropose here eliminate a substantial fraction of the variables from the\ncoordinate descent algorithm. Violations are quite rare, especially in the\nlocally convex region of the solution path, and furthermore, may be easily\ndetected and corrected by checking the Karush-Kuhn-Tucker conditions. We extend\nthese rules to generalized linear models, as well as to other nonconvex\npenalties such as the $\\ell_2$-stabilized Mnet penalty, group MCP, and group\nSCAD. We explore three variants of the coordinate decent algorithm that\nincorporate these rules and study the efficiency of these algorithms in fitting\nmodels to both simulated data and on real data from a genome-wide association\nstudy.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 15:09:27 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Lee", "Sangin", ""], ["Breheny", "Patrick", ""]]}, {"id": "1403.3231", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis, Efthimia Bora-Senta", "title": "Simulation of Multivariate Non-Gaussian Autoregressive Time Series with\n  Given Autocovariance and Marginals", "comments": "21 pages, 6 figures, accepted in Simulation Modelling Practice and\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-analytic method is proposed for the generation of realizations of a\nmultivariate process of a given linear correlation structure and marginal\ndistribution. This is an extension of a similar method for univariate\nprocesses, transforming the autocorrelation of the non-Gaussian process to that\nof a Gaussian process based on a piece-wise linear marginal transform from\nnon-Gaussian to Gaussian marginal. The extension to multivariate processes\ninvolves the derivation of the autocorrelation matrix from the marginal\ntransforms, which determines the generating vector autoregressive process. The\neffectiveness of the approach is demonstrated on systems designed under\ndifferent scenarios of autocovariance and marginals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 10:59:34 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Kugiumtzis", "Dimitris", ""], ["Bora-Senta", "Efthimia", ""]]}, {"id": "1403.4054", "submitter": "Daniel Lawson", "authors": "Daniel John Lawson and Niall M Adams", "title": "A general decision framework for structuring computation using Data\n  Directional Scaling to process massive similarity matrices", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As datasets grow it becomes infeasible to process them completely with a\ndesired model. For giant datasets, we frame the order in which computation is\nperformed as a decision problem. The order is designed so that partial\ncomputations are of value and early stopping yields useful results. Our\napproach comprises two related tools: a decision framework to choose the order\nto perform computations, and an emulation framework to enable estimation of the\nunevaluated computations. The approach is applied to the problem of computing\nsimilarity matrices, for which the cost of computation grows quadratically with\nthe number of objects. Reasoning about similarities before they are observed\nintroduces difficulties as there is no natural space and hence comparisons are\ndifficult. We solve this by introducing a computationally convenient form of\nmultidimensional scaling we call `data directional scaling'. High quality\nestimation is possible with massively reduced computation from the naive\napproach, and can be scaled to very large matrices. The approach is applied to\nthe practical problem of assessing genetic similarity in population genetics.\nThe use of statistical reasoning in decision making for large scale problems\npromises to be an important tool in applying statistical methodology to Big\nData.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 10:22:16 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Lawson", "Daniel John", ""], ["Adams", "Niall M", ""]]}, {"id": "1403.4290", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Youssef M. Marzouk and Karen E. Willcox", "title": "Data-Driven Model Reduction for the Bayesian Solution of Inverse\n  Problems", "comments": null, "journal-ref": "International Journal for Numerical Methods in Engineering, 102\n  (5), 966-990 (2015)", "doi": "10.1002/nme.4748", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in the Bayesian solution of inverse problems\ngoverned by partial differential equations (PDEs) is the computational cost of\nrepeatedly evaluating numerical PDE models, as required by Markov chain Monte\nCarlo (MCMC) methods for posterior sampling. This paper proposes a data-driven\nprojection-based model reduction technique to reduce this computational cost.\nThe proposed technique has two distinctive features. First, the model reduction\nstrategy is tailored to inverse problems: the snapshots used to construct the\nreduced-order model are computed adaptively from the posterior distribution.\nPosterior exploration and model reduction are thus pursued simultaneously.\nSecond, to avoid repeated evaluations of the full-scale numerical model as in a\nstandard MCMC method, we couple the full-scale model and the reduced-order\nmodel together in the MCMC algorithm. This maintains accurate inference while\nreducing its overall computational cost. In numerical experiments considering\nsteady-state flow in a porous medium, the data-driven reduced-order model\nachieves better accuracy than a reduced-order model constructed using the\nclassical approach. It also improves posterior sampling efficiency by several\norders of magnitude compared to a standard MCMC method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:21:38 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 04:02:51 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Marzouk", "Youssef M.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1403.4291", "submitter": "Mathieu Cambou", "authors": "Philipp Arbenz, Mathieu Cambou and Marius Hofert", "title": "An importance sampling approach for copula models in insurance", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An importance sampling approach for sampling copula models is introduced. We\npropose two algorithms that improve Monte Carlo estimators when the functional\nof interest depends mainly on the behaviour of the underlying random vector\nwhen at least one of the components is large. Such problems often arise from\ndependence models in finance and insurance. The importance sampling framework\nwe propose is general and can be easily implemented for all classes of copula\nmodels from which sampling is feasible. We show how the proposal distribution\nof the two algorithms can be optimized to reduce the sampling error. In a case\nstudy inspired by a typical multivariate insurance application, we obtain\nvariance reduction factors between 10 and 30 in comparison to standard Monte\nCarlo estimators.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:28:14 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 22:09:12 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 17:57:36 GMT"}], "update_date": "2015-04-08", "authors_parsed": [["Arbenz", "Philipp", ""], ["Cambou", "Mathieu", ""], ["Hofert", "Marius", ""]]}, {"id": "1403.4359", "submitter": "Matthew Moores", "authors": "Matthew T. Moores and Christopher C. Drovandi and Kerrie Mengersen and\n  Christian P. Robert", "title": "Pre-processing for approximate Bayesian computation in image analysis", "comments": "5th IMS-ISBA joint meeting (MCMSki IV)", "journal-ref": null, "doi": "10.1007/s11222-014-9525-6", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing algorithms for approximate Bayesian computation (ABC)\nassume that it is feasible to simulate pseudo-data from the model at each\niteration. However, the computational cost of these simulations can be\nprohibitive for high dimensional data. An important example is the Potts model,\nwhich is commonly used in image analysis. Images encountered in real world\napplications can have millions of pixels, therefore scalability is a major\nconcern. We apply ABC with a synthetic likelihood to the hidden Potts model\nwith additive Gaussian noise. Using a pre-processing step, we fit a binding\nfunction to model the relationship between the model parameters and the\nsynthetic likelihood parameters. Our numerical experiments demonstrate that the\nprecomputed binding function dramatically improves the scalability of ABC,\nreducing the average runtime required for model fitting from 71 hours to only 7\nminutes. We also illustrate the method by estimating the smoothing parameter\nfor remotely sensed satellite imagery. Without precomputation, Bayesian\ninference is impractical for datasets of that scale.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 06:55:50 GMT"}, {"version": "v2", "created": "Fri, 5 Sep 2014 17:38:55 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Moores", "Matthew T.", ""], ["Drovandi", "Christopher C.", ""], ["Mengersen", "Kerrie", ""], ["Robert", "Christian P.", ""]]}, {"id": "1403.4402", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Antonietta Mira", "title": "Efficient computational strategies for doubly intractable problems with\n  applications to Bayesian social networks", "comments": "23 pages, 8 figures. Accepted to appear in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful ideas recently appeared in the literature are adjusted and combined\nto design improved samplers for Bayesian exponential random graph models.\nDifferent forms of adaptive Metropolis-Hastings proposals (vertical, horizontal\nand rectangular) are tested and combined with the Delayed rejection (DR)\nstrategy with the aim of reducing the variance of the resulting Markov chain\nMonte Carlo estimators for a given computational time. In the examples treated\nin this paper the best combination, namely horizontal adaptation with delayed\nrejection, leads to a variance reduction that varies between 92% and 144%\nrelative to the adaptive direction sampling approximate exchange algorithm of\nCaimo and Friel (2011). These results correspond to an increased performance\nwhich varies from 10% to 94% if we take simulation time into account. The\nhighest improvements are obtained when highly correlated posterior\ndistributions are considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 10:47:44 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2014 14:25:11 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Caimo", "Alberto", ""], ["Mira", "Antonietta", ""]]}, {"id": "1403.4680", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, James Martin, Youssef M. Marzouk, Antti Solonen and\n  Alessio Spantini", "title": "Likelihood-informed dimension reduction for nonlinear inverse problems", "comments": null, "journal-ref": "Inverse Problems, 30, 114015 (2014)", "doi": "10.1088/0266-5611/30/11/114015", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intrinsic dimensionality of an inverse problem is affected by prior\ninformation, the accuracy and number of observations, and the smoothing\nproperties of the forward operator. From a Bayesian perspective, changes from\nthe prior to the posterior may, in many problems, be confined to a relatively\nlow-dimensional subspace of the parameter space. We present a dimension\nreduction approach that defines and identifies such a subspace, called the\n\"likelihood-informed subspace\" (LIS), by characterizing the relative influences\nof the prior and the likelihood over the support of the posterior distribution.\nThis identification enables new and more efficient computational methods for\nBayesian inference with nonlinear forward models and Gaussian priors. In\nparticular, we approximate the posterior distribution as the product of a\nlower-dimensional posterior defined on the LIS and the prior distribution\nmarginalized onto the complementary subspace. Markov chain Monte Carlo sampling\ncan then proceed in lower dimensions, with significant gains in computational\nefficiency. We also introduce a Rao-Blackwellization strategy that\nde-randomizes Monte Carlo estimates of posterior expectations for additional\nvariance reduction. We demonstrate the efficiency of our methods using two\nnumerical examples: inference of permeability in a groundwater system governed\nby an elliptic PDE, and an atmospheric remote sensing problem based on Global\nOzone Monitoring System (GOMOS) observations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 03:22:21 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 04:50:31 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Martin", "James", ""], ["Marzouk", "Youssef M.", ""], ["Solonen", "Antti", ""], ["Spantini", "Alessio", ""]]}, {"id": "1403.4890", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Genetha A. Gray, Sebastien Le Digabel, Herbert K.H.\n  Lee, Pritam Ranjan, Garth Wells, Stefan M. Wild", "title": "Modeling an Augmented Lagrangian for Blackbox Constrained Optimization", "comments": "22 Pages, 2 additional supplementary, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained blackbox optimization is a difficult problem, with most\napproaches coming from the mathematical programming literature. The statistical\nliterature is sparse, especially in addressing problems with nontrivial\nconstraints. This situation is unfortunate because statistical methods have\nmany attractive properties: global scope, handling noisy objectives,\nsensitivity analysis, and so forth. To narrow that gap, we propose a\ncombination of response surface modeling, expected improvement, and the\naugmented Lagrangian numerical optimization framework. This hybrid approach\nallows the statistical model to think globally and the augmented Lagrangian to\nact locally. We focus on problems where the constraints are the primary\nbottleneck, requiring expensive simulation to evaluate and substantial modeling\neffort to map out. In that context, our hybridization presents a simple yet\neffective solution that allows existing objective-oriented statistical\napproaches, like those based on Gaussian process surrogates and expected\nimprovement heuristics, to be applied to the constrained setting with minor\nmodification. This work is motivated by a challenging, real-data benchmark\nproblem from hydrology where, even with a simple linear objective function,\nlearning a nontrivial valid region complicates the search for a global minimum.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 17:36:37 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 15:37:55 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 18:41:10 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Gray", "Genetha A.", ""], ["Digabel", "Sebastien Le", ""], ["Lee", "Herbert K. H.", ""], ["Ranjan", "Pritam", ""], ["Wells", "Garth", ""], ["Wild", "Stefan M.", ""]]}, {"id": "1403.5040", "submitter": "Vladimir Minin", "authors": "Jan Irvahn and Vladimir N. Minin", "title": "Phylogenetic Stochastic Mapping without Matrix Exponentiation", "comments": "33 pages, including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic stochastic mapping is a method for reconstructing the history of\ntrait changes on a phylogenetic tree relating species/organisms carrying the\ntrait. State-of-the-art methods assume that the trait evolves according to a\ncontinuous-time Markov chain (CTMC) and work well for small state spaces. The\ncomputations slow down considerably for larger state spaces (e.g. space of\ncodons), because current methodology relies on exponentiating CTMC\ninfinitesimal rate matrices -- an operation whose computational complexity\ngrows as the size of the CTMC state space cubed. In this work, we introduce a\nnew approach, based on a CTMC technique called uniformization, that does not\nuse matrix exponentiation for phylogenetic stochastic mapping. Our method is\nbased on a new Markov chain Monte Carlo (MCMC) algorithm that targets the\ndistribution of trait histories conditional on the trait data observed at the\ntips of the tree. The computational complexity of our MCMC method grows as the\nsize of the CTMC state space squared. Moreover, in contrast to competing matrix\nexponentiation methods, if the rate matrix is sparse, we can leverage this\nsparsity and increase the computational efficiency of our algorithm further.\nUsing simulated data, we illustrate advantages of our MCMC algorithm and\ninvestigate how large the state space needs to be for our method to outperform\nmatrix exponentiation approaches. We show that even on the moderately large\nstate space of codons our MCMC method can be significantly faster than\ncurrently used matrix exponentiation methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 05:06:11 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Irvahn", "Jan", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1403.5065", "submitter": "Dario Gasbarra", "authors": "Dario Gasbarra, Jia Liu, Juha Railavo", "title": "Data augmentation in Rician noise model and Bayesian Diffusion Tensor\n  Imaging", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping white matter tracts is an essential step towards understanding brain\nfunction. Diffusion Magnetic Resonance Imaging (dMRI) is the only noninvasive\ntechnique which can detect in vivo anisotropies in the 3-dimensional diffusion\nof water molecules, which correspond to nervous fibers in the living brain. In\nthis process, spectral data from the displacement distribution of water\nmolecules is collected by a magnetic resonance scanner. From the statistical\npoint of view, inverting the Fourier transform from such sparse and noisy\nspectral measurements leads to a non-linear regression problem. Diffusion\ntensor imaging (DTI) is the simplest modeling approach postulating a Gaussian\ndisplacement distribution at each volume element (voxel). Typically the\ninference is based on a linearized log-normal regression model that can fit the\nspectral data at low frequencies. However such approximation fails to fit the\nhigh frequency measurements which contain information about the details of the\ndisplacement distribution but have a low signal to noise ratio. In this paper,\nwe directly work with the Rice noise model and cover the full range of\n$b$-values. Using data augmentation to represent the likelihood, we reduce the\nnon-linear regression problem to the framework of generalized linear models.\nThen we construct a Bayesian hierarchical model in order to perform\nsimultaneously estimation and regularization of the tensor field. Finally the\nBayesian paradigm is implemented by using Markov chain Monte Carlo.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 08:37:14 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Gasbarra", "Dario", ""], ["Liu", "Jia", ""], ["Railavo", "Juha", ""]]}, {"id": "1403.5207", "submitter": "Sourabh Bhattacharya", "authors": "Moumita Das and Sourabh Bhattacharya", "title": "Transdimensional Transformation based Markov Chain Monte Carlo", "comments": "A very significantly updated version, demonstrating superiority of\n  TTMCMC over RJMCMC with many more simulations with respect to gamma and\n  normal mixtures, for both simulated and real datasets, including even\n  20-dimensional normal mixtures. We are not aware of any successful RJMCMC\n  applications in dimensions this large", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a novel and general dimension-hopping MCMC\nmethodology that can update all the parameters as well as the number of\nparameters simultaneously using simple deterministic transformations of some\nlow-dimensional (often one-dimensional) random variable. This methodology,\nwhich has been inspired by the recent Transformation based MCMC (TMCMC) for\nupdating all the parameters simultaneously in general fixed-dimensional set-ups\nusing low-dimensional random variables, facilitates great speed in terms of\ncomputation time and provides high acceptance rates, thanks to the\nlow-dimensional random variables which effectively reduce the dimension\ndramatically. Quite importantly, our transformation based approach provides a\nnatural way to automate the move-types in the variable dimensional problems. We\nrefer to this methodology as Transdimensional Transformation based Markov Chain\nMonte Carlo (TTMCMC).\n  We develop the theory of TTMCMC, illustrating it with gamma and normal\nmixtures with unknown number of components, for both simulated and real data\nsets. Comparisons with RJMCMC demonstrates far superior performance of TTMCMC\nin terms of mixing, acceptance rate, computational speed and automation.\nFurthermore, we demonstrate good performance of TTMCMC in multivariate normal\nmixtures, even for dimension as large as 20. To our knowledge, there exists no\napplication of RJMCMC for such high-dimensional mixtures.\n  Further, we propose a novel methodology to summarize the posterior, providing\na way to obtain the mode of the posterior distribution of the densities and the\nassociated highest posterior density credible regions. Based on our method we\nalso propose a criterion to assess convergence of variable-dimensional\nalgorithms. These methods of summarization and convergence assessment are\napplicable to general problems, not just to mixtures.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 17:21:11 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 16:50:58 GMT"}, {"version": "v3", "created": "Tue, 8 Dec 2015 17:22:18 GMT"}, {"version": "v4", "created": "Wed, 26 Oct 2016 15:28:50 GMT"}, {"version": "v5", "created": "Wed, 15 Mar 2017 06:44:20 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Das", "Moumita", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1403.5536", "submitter": "James M. Flegal", "authors": "Lei Gong and James M. Flegal", "title": "A practical sequential stopping rule for high-dimensional MCMC and its\n  application to spatial-temporal Bayesian models", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current challenge for many Bayesian analyses is determining when to\nterminate high-dimensional Markov chain Monte Carlo simulations. To this end,\nwe propose using an automated sequential stopping procedure that terminates the\nsimulation when the computational uncertainty is small relative to the\nposterior uncertainty. Such a stopping rule has previously been shown to work\nwell in settings with posteriors of moderate dimension. In this paper, we\nillustrate its utility in high-dimensional simulations while overcoming some\ncurrent computational issues. Further, we investigate the relationship between\nthe stopping rule and effective sample size. As examples, we consider two\ncomplex Bayesian analyses on spatially and temporally correlated datasets. The\nfirst involves a dynamic space-time model on weather station data and the\nsecond a spatial variable selection model on fMRI brain imaging data. Our\nresults show the sequential stopping rule is easy to implement, provides\nuncertainty estimates, and performs well in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:40:26 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Gong", "Lei", ""], ["Flegal", "James M.", ""]]}, {"id": "1403.5537", "submitter": "Alexandre Janon", "authors": "Yohann De Castro (LM-Orsay), Alexandre Janon (LM-Orsay, - M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques)", "title": "Randomized pick-freeze for sparse Sobol indices estimation in high\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a new procedure to estimate the influence of each\nvariable of a given function defined on a high-dimensional space. More\nprecisely, we are concerned with describing a function of a large number $p$ of\nparameters that depends only on a small number $s$ of them. Our proposed method\nis an unconstrained $\\ell_{1}$-minimization based on the Sobol's method. We\nprove that, with only $\\mathcal O(s\\log p)$ evaluations of $f$, one can find\nwhich are the relevant parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:41:09 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["De Castro", "Yohann", "", "LM-Orsay"], ["Janon", "Alexandre", "", "LM-Orsay, - M\u00e9thodes\n  d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"]]}, {"id": "1403.5693", "submitter": "Dougal Maclaurin", "authors": "Dougal Maclaurin and Ryan P. Adams", "title": "Firefly Monte Carlo: Exact MCMC with Subsets of Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is a popular and successful general-purpose\ntool for Bayesian inference. However, MCMC cannot be practically applied to\nlarge data sets because of the prohibitive cost of evaluating every likelihood\nterm at every iteration. Here we present Firefly Monte Carlo (FlyMC) an\nauxiliary variable MCMC algorithm that only queries the likelihoods of a\npotentially small subset of the data at each iteration yet simulates from the\nexact posterior distribution, in contrast to recent proposals that are\napproximate even in the asymptotic limit. FlyMC is compatible with a wide\nvariety of modern MCMC algorithms, and only requires a lower bound on the\nper-datum likelihood factors. In experiments, we find that FlyMC generates\nsamples from the posterior more than an order of magnitude faster than regular\nMCMC, opening up MCMC methods to larger datasets than were previously\nconsidered feasible.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 18:21:29 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["Maclaurin", "Dougal", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.6886", "submitter": "Jamie Owen", "authors": "Jamie Owen, Darren J. Wilkinson, Colin S. Gillespie", "title": "Scalable Inference for Markov Processes with Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference for Markov processes has become increasingly relevant in\nrecent years. Problems of this type often have intractable likelihoods and\nprior knowledge about model rate parameters is often poor. Markov Chain Monte\nCarlo (MCMC) techniques can lead to exact inference in such models but in\npractice can suffer performance issues including long burn-in periods and poor\nmixing. On the other hand approximate Bayesian computation techniques can allow\nrapid exploration of a large parameter space but yield only approximate\nposterior distributions. Here we consider the combined use of approximate\nBayesian computation (ABC) and MCMC techniques for improved computational\nefficiency while retaining exact inference on parallel hardware.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 23:05:57 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 14:50:05 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Owen", "Jamie", ""], ["Wilkinson", "Darren J.", ""], ["Gillespie", "Colin S.", ""]]}, {"id": "1403.7093", "submitter": "Manuela Cattelan", "authors": "Manuela Cattelan and Nicola Sartori", "title": "Empirical and Simulated Adjustments of Composite Likelihood Ratio\n  Statistics", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Composite likelihood inference has gained much popularity thanks to its\ncomputational manageability and its theoretical properties. Unfortunately,\nperforming composite likelihood ratio tests is inconvenient because of their\nawkward asymptotic distribution. There are many proposals for adjusting\ncomposite likelihood ratio tests in order to recover an asymptotic chi square\ndistribution, but they all depend on the sensitivity and variability matrices.\nThe same is true for Wald-type and score-type counterparts. In realistic\napplications sensitivity and variability matrices usually need to be estimated,\nbut there are no comparisons of the performance of composite likelihood based\nstatistics in such an instance. A comparison of the accuracy of inference based\non the statistics considering two methods typically employed for estimation of\nsensitivity and variability matrices, namely an empirical method that exploits\nindependent observations, and Monte Carlo simulation, is performed. The results\nin two examples involving the pairwise likelihood show that a very large number\nof independent observations should be available in order to obtain accurate\ncoverages using empirical estimation, while limited simulation from the full\nmodel provides accurate results regardless of the availability of independent\nobservations.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 15:45:31 GMT"}, {"version": "v2", "created": "Thu, 31 Jul 2014 09:56:45 GMT"}], "update_date": "2014-08-01", "authors_parsed": [["Cattelan", "Manuela", ""], ["Sartori", "Nicola", ""]]}, {"id": "1403.7137", "submitter": "Ahmed Attia", "authors": "Ahmed Attia, Adrian Sandu", "title": "A Sampling Filter for Non-Gaussian Data Assimilation", "comments": "52 pages, 24 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": "CSTR-4/2014", "categories": "cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation combines information from models, measurements, and priors\nto estimate the state of a dynamical system such as the atmosphere. The\nEnsemble Kalman filter (EnKF) is a family of ensemble-based data assimilation\napproaches that has gained wide popularity due its simple formulation, ease of\nimplementation, and good practical results. Most EnKF algorithms assume that\nthe underlying probability distributions are Gaussian. Although this assumption\nis well accepted, it is too restrictive when applied to large nonlinear models,\nnonlinear observation operators, and large levels of uncertainty. Several\napproaches have been proposed in order to avoid the Gaussianity assumption. One\nof the most successful strategies is the maximum likelihood ensemble filter\n(MLEF) which computes a maximum a posteriori estimate of the state assuming the\nposterior distribution is Gaussian. MLEF is designed to work with nonlinear and\neven non-differentiable observation operators, and shows good practical\nperformance. However, there are limits to the degree of nonlinearity that MLEF\ncan handle. This paper proposes a new ensemble-based data assimilation method,\nnamed the \"sampling filter\", which obtains the analysis by sampling directly\nfrom the posterior distribution. The sampling strategy is based on a Hybrid\nMonte Carlo (HMC) approach that can handle non-Gaussian probability\ndistributions. Numerical experiments are carried out using the Lorenz-96 model\nand observation operators with different levels of non-linearity and\ndifferentiability. The proposed filter is also tested with shallow water model\non a sphere with linear observation operator. The results show that the\nsampling filter can perform well even in highly nonlinear situations were EnKF\nand MLEF filters diverge.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 17:21:08 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 22:06:20 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Attia", "Ahmed", ""], ["Sandu", "Adrian", ""]]}, {"id": "1403.7265", "submitter": "Elaine Angelino", "authors": "Elaine Angelino, Eddie Kohler, Amos Waterland, Margo Seltzer and Ryan\n  P. Adams", "title": "Accelerating MCMC via Parallel Predictive Prefetching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general framework for accelerating a large class of widely used\nMarkov chain Monte Carlo (MCMC) algorithms. Our approach exploits fast,\niterative approximations to the target density to speculatively evaluate many\npotential future steps of the chain in parallel. The approach can accelerate\ncomputation of the target distribution of a Bayesian inference problem, without\ncompromising exactness, by exploiting subsets of data. It takes advantage of\nwhatever parallel resources are available, but produces results exactly\nequivalent to standard serial execution. In the initial burn-in phase of chain\nevaluation, it achieves speedup over serial evaluation that is close to linear\nin the number of available cores.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 01:28:52 GMT"}], "update_date": "2014-03-31", "authors_parsed": [["Angelino", "Elaine", ""], ["Kohler", "Eddie", ""], ["Waterland", "Amos", ""], ["Seltzer", "Margo", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1403.7644", "submitter": "Andrew Karl", "authors": "Andrew T. Karl, Yan Yang and Sharon L. Lohr", "title": "Efficient Maximum Likelihood Estimation of Multiple Membership Linear\n  Mixed Models, with an Application to Educational Value-Added Assessments", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 59, March 2013,\n  Pages 13-27", "doi": "10.1016/j.csda.2012.10.004", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized persistence (GP) model, developed in the context of\nestimating ``value added'' by individual teachers to their students' current\nand future test scores, is one of the most flexible value-added models in the\nliterature. Although developed in the educational setting, the GP model can\npotentially be applied to any structure where each sequential response of a\nlower-level unit may be associated with a different higher-level unit, and the\neffects of the higher-level units may persist over time. The flexibility of the\nGP model, however, and its multiple membership random effects structure lead to\ncomputational challenges that have limited the model's availability. We develop\nan EM algorithm to compute maximum likelihood estimates efficiently for the GP\nmodel, making use of the sparse structure of the random effects and error\ncovariance matrices. The algorithm is implemented in the package GPvam in R\nstatistical software. We give examples of the computations and illustrate the\ngains in computational efficiency achieved by our estimation procedure.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 16:22:59 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Karl", "Andrew T.", ""], ["Yang", "Yan", ""], ["Lohr", "Sharon L.", ""]]}, {"id": "1403.7645", "submitter": "Andrew Karl", "authors": "Andrew T. Karl, Randy Eubank, Jelena Milovanovic, Mark Reiser and\n  Dennis Young", "title": "Using RngStreams for Parallel Random Number Generation in C++ and R", "comments": "This paper has been accepted by Computational Statistics and is\n  currently in press", "journal-ref": "Computational Statistics, 2014, 29:1301-1320", "doi": "10.1007/s00180-014-0492-3", "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The RngStreams software package provides one viable solution to the problem\nof creating independent random number streams for simulations in parallel\nprocessing environments. Techniques are presented for effectively using\nRngStreams with C++ programs that are parallelized via OpenMP or MPI. Ways to\naccess the backbone generator from RngStreams in R through the parallel and\nrstream packages are also described. The ideas in the paper are illustrated\nwith both a simple running example and a Monte Carlo integration application.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 16:30:21 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Karl", "Andrew T.", ""], ["Eubank", "Randy", ""], ["Milovanovic", "Jelena", ""], ["Reiser", "Mark", ""], ["Young", "Dennis", ""]]}, {"id": "1403.7676", "submitter": "Andrew Karl", "authors": "Andrew T. Karl, Yan Yang and Sharon L. Lohr", "title": "Computation of Maximum Likelihood Estimates for Multiresponse\n  Generalized Linear Mixed Models with Non-nested, Correlated Random Effects", "comments": "20 pages, 2 figures. Supplementary code in arXiv source package", "journal-ref": "Computational Statistics & Data Analysis, Volume 73, May 2014,\n  Pages 146-162", "doi": "10.1016/j.csda.2013.11.019", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of generalized linear mixed models (GLMMs) with non-nested random\neffects structures requires approximation of high-dimensional integrals. Many\nexisting methods are tailored to the low-dimensional integrals produced by\nnested designs. We explore the modifications that are required in order to\nadapt an EM algorithm with first-order and fully exponential Laplace\napproximations to a non-nested, multiple response model. The equations in the\nestimation routine are expressed as functions of the first four derivatives of\nthe conditional likelihood of an arbitrary GLMM, providing a template for\nfuture applications. We apply the method to a joint Poisson-binary model for\nranking sporting teams, and discuss the estimation of a correlated random\neffects model designed to evaluate the sensitivity of value-added models for\nteacher evaluation to assumptions about the missing data process. Source code\nin R is provided in the online supplementary material.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 22:34:21 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Karl", "Andrew T.", ""], ["Yang", "Yan", ""], ["Lohr", "Sharon L.", ""]]}, {"id": "1403.7957", "submitter": "Sam Livingstone", "authors": "Samuel Livingstone and Mark Girolami", "title": "Information-geometric Markov Chain Monte Carlo methods using Diffusions", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": "10.3390/e16063074", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work incorporating geometric ideas in Markov chain Monte Carlo is\nreviewed in order to highlight these advances and their possible application in\na range of domains beyond Statistics. A full exposition of Markov chains and\ntheir use in Monte Carlo simulation for Statistical inference and molecular\ndynamics is provided, with particular emphasis on methods based on Langevin\ndiffusions. After this geometric concepts in Markov chain Monte Carlo are\nintroduced. A full derivation of the Langevin diffusion on a Riemannian\nmanifold is given, together with a discussion of appropriate Riemannian metric\nchoice for different problems. A survey of applications is provided, and some\nopen questions are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 11:35:00 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 09:58:17 GMT"}, {"version": "v3", "created": "Fri, 18 Apr 2014 11:48:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Girolami", "Mark", ""]]}, {"id": "1403.8057", "submitter": "Iain Johnston", "authors": "Iain G. Johnston", "title": "Efficient parametric inference for stochastic biological systems with\n  measured variability", "comments": "11 pages, 4 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic systems in biology often exhibit substantial variability within\nand between cells. This variability, as well as having dramatic functional\nconsequences, provides information about the underlying details of the system's\nbehaviour. It is often desirable to infer properties of the parameters\ngoverning such systems given experimental observations of the mean and variance\nof observed quantities. In some circumstances, analytic forms for the\nlikelihood of these observations allow very efficient inference: we present\nthese forms and demonstrate their usage. When likelihood functions are\nunavailable or difficult to calculate, we show that an implementation of\napproximate Bayesian computation (ABC) is a powerful tool for parametric\ninference in these systems. However, the calculations required to apply ABC to\nthese systems can also be computationally expensive, relying on repeated\nstochastic simulations. We propose an ABC approach that cheaply eliminates\nunimportant regions of parameter space, by addressing computationally simple\nmean behaviour before explicitly simulating the more computationally demanding\nvariance behaviour. We show that this approach leads to a substantial increase\nin speed when applied to synthetic and experimental datasets.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 15:42:01 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 15:01:50 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Johnston", "Iain G.", ""]]}, {"id": "1403.8144", "submitter": "Ping Li", "authors": "Ping Li, Michael Mitzenmacher, Anshumali Shrivastava", "title": "Coding for Random Projections and Approximate Near Neighbor Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DB cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This technical note compares two coding (quantization) schemes for random\nprojections in the context of sub-linear time approximate near neighbor search.\nThe first scheme is based on uniform quantization while the second scheme\nutilizes a uniform quantization plus a uniformly random offset (which has been\npopular in practice). The prior work compared the two schemes in the context of\nsimilarity estimation and training linear classifiers, with the conclusion that\nthe step of random offset is not necessary and may hurt the performance\n(depending on the similarity level). The task of near neighbor search is\nrelated to similarity estimation with importance distinctions and requires own\nstudy. In this paper, we demonstrate that in the context of near neighbor\nsearch, the step of random offset is not needed either and may hurt the\nperformance (sometimes significantly so, depending on the similarity and other\nparameters).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 19:43:53 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Li", "Ping", ""], ["Mitzenmacher", "Michael", ""], ["Shrivastava", "Anshumali", ""]]}]