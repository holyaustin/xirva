[{"id": "1802.00151", "submitter": "Zachariah Neville", "authors": "Zachariah Neville, Naomi Brownstein", "title": "Macros to Conduct Tests of Multimodality in SAS", "comments": "14 pages plus appendix and references. 3 tables and 3 figures in\n  separate Tables and Figures sections at the end", "journal-ref": null, "doi": "10.1080/00949655.2018.1509979", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dip Test of Unimodality and Silverman's Critical Bandwidth Test are two\npopular tests to determine if an unknown density contains more than one mode.\nWhile the tests can be easily run in R, they are not included in SAS software.\nWe provide implementations of the Dip Test and Silverman Test as macros in the\nSAS software, capitalizing on the capability of SAS to execute R code\ninternally. Descriptions of the macro parameters, installation steps, and\nsample macro calls are provided, along with an appendix for troubleshooting. We\nillustrate the use of the macros on data simulated from one or more Gaussian\ndistributions as well as on the famous $\\textit{iris}$ dataset.\n", "versions": [{"version": "v1", "created": "Thu, 1 Feb 2018 04:10:17 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Neville", "Zachariah", ""], ["Brownstein", "Naomi", ""]]}, {"id": "1802.00828", "submitter": "Deisy Morselli Gysi", "authors": "Deisy Morselli Gysi, Tiago Miranda Fragoso, Volker Buskamp, Eivind\n  Almaas and Katja Nowick", "title": "Comparing multiple networks using the Co-expression Differential Network\n  Analysis (CoDiNA)", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0240523", "report-no": null, "categories": "stat.CO q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical sciences are increasingly recognising the relevance of gene\nco-expression-networks for analysing complex-systems, phenotypes or diseases.\nWhen the goal is investigating complex-phenotypes under varying conditions, it\ncomes naturally to employ comparative network methods. While approaches for\ncomparing two networks exist, this is not the case for multiple networks. Here\nwe present a method for the systematic comparison of an unlimited number of\nnetworks: Co-expression Differential Network Analysis (CoDiNA) for detecting\nlinks and nodes that are common, specific or different to the networks.\nApplying CoDiNA to a neurogenesis study identified genes for neuron\ndifferentiation. Experimentally overexpressing one candidate resulted in\nsignificant disturbance in the underlying neurogenesis' gene regulatory\nnetwork. We compared data from adults and children with active tuberculosis to\ntest for signatures of HIV. We also identified common and distinct network\nfeatures for particular cancer types with CoDiNA. These studies show that\nCoDiNA successfully detects genes associated with the diseases.\n", "versions": [{"version": "v1", "created": "Fri, 2 Feb 2018 19:53:08 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 19:59:58 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gysi", "Deisy Morselli", ""], ["Fragoso", "Tiago Miranda", ""], ["Buskamp", "Volker", ""], ["Almaas", "Eivind", ""], ["Nowick", "Katja", ""]]}, {"id": "1802.01053", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Nitin Viswanathan", "title": "Using Poisson Binomial GLMs to Reveal Voter Preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new modeling technique for solving the problem of ecological\ninference, in which individual-level associations are inferred from labeled\ndata available only at the aggregate level. We model aggregate count data as\narising from the Poisson binomial, the distribution of the sum of independent\nbut not identically distributed Bernoulli random variables. We relate\nindividual-level probabilities to individual covariates using both a logistic\nregression and a neural network. A normal approximation is derived via the\nLyapunov Central Limit Theorem, allowing us to efficiently fit these models on\nlarge datasets. We apply this technique to the problem of revealing voter\npreferences in the 2016 presidential election, fitting a model to a sample of\nover four million voters from the highly contested swing state of Pennsylvania.\nWe validate the model at the precinct level via a holdout set, and at the\nindividual level using weak labels, finding that the model is predictive and it\nlearns intuitively reasonable associations.\n", "versions": [{"version": "v1", "created": "Sun, 4 Feb 2018 00:55:46 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Rosenman", "Evan", ""], ["Viswanathan", "Nitin", ""]]}, {"id": "1802.01610", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller", "title": "Fast and accurate approximation of the full conditional for gamma shape\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gamma distribution arises frequently in Bayesian models, but there is not\nan easy-to-use conjugate prior for the shape parameter of a gamma. This\ninconvenience is usually dealt with by using either Metropolis-Hastings moves,\nrejection sampling methods, or numerical integration. However, in models with a\nlarge number of shape parameters, these existing methods are slower or more\ncomplicated than one would like, making them burdensome in practice. It turns\nout that the full conditional distribution of the gamma shape parameter is well\napproximated by a gamma distribution, even for small sample sizes, when the\nprior on the shape parameter is also a gamma distribution. This article\nintroduces a quick and easy algorithm for finding a gamma distribution that\napproximates the full conditional distribution of the shape parameter. We\nempirically demonstrate the speed and accuracy of the approximation across a\nwide range of conditions. If exactness is required, the approximation can be\nused as a proposal distribution for Metropolis-Hastings.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 19:22:34 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 22:46:24 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Miller", "Jeffrey W.", ""]]}, {"id": "1802.01630", "submitter": "J\\\"uri Lember", "authors": "J\\\"uri Lember, Dario Gasbarra, Alexey Koloydenko, Kristi Kuljus", "title": "Estimation of Viterbi path in Bayesian hidden Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article studies different methods for estimating the Viterbi path in the\nBayesian framework. The Viterbi path is an estimate of the underlying state\npath in hidden Markov models (HMMs), which has a maximum posterior probability\n(MAP). For an HMM with given parameters, the Viterbi path can be easily found\nwith the Viterbi algorithm. In the Bayesian framework the Viterbi algorithm is\nnot applicable and several iterative methods can be used instead. We introduce\na new EM-type algorithm for finding the MAP path and compare it with various\nother methods for finding the MAP path, including the variational Bayes\napproach and MCMC methods. Examples with simulated data are used to compare the\nperformance of the methods. The main focus is on non-stochastic iterative\nmethods and our results show that the best of those methods work as well or\nbetter than the best MCMC methods. Our results demonstrate that when the\nprimary goal is segmentation, then it is more reasonable to perform\nsegmentation directly by considering the transition and emission parameters as\nnuisance parameters.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 20:11:24 GMT"}, {"version": "v2", "created": "Sat, 11 May 2019 11:05:02 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Gasbarra", "Dario", ""], ["Koloydenko", "Alexey", ""], ["Kuljus", "Kristi", ""]]}, {"id": "1802.01737", "submitter": "Trevor Campbell", "authors": "Trevor Campbell, Tamara Broderick", "title": "Bayesian Coreset Construction via Greedy Iterative Geodesic Ascent", "comments": "Appearing in the 2018 International Conference on Machine Learning\n  (ICML). 13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coherent uncertainty quantification is a key strength of Bayesian methods.\nBut modern algorithms for approximate Bayesian posterior inference often\nsacrifice accurate posterior uncertainty estimation in the pursuit of\nscalability. This work shows that previous Bayesian coreset construction\nalgorithms---which build a small, weighted subset of the data that approximates\nthe full dataset---are no exception. We demonstrate that these algorithms scale\nthe coreset log-likelihood suboptimally, resulting in underestimated posterior\nuncertainty. To address this shortcoming, we develop greedy iterative geodesic\nascent (GIGA), a novel algorithm for Bayesian coreset construction that scales\nthe coreset log-likelihood optimally. GIGA provides geometric decay in\nposterior approximation error as a function of coreset size, and maintains the\nfast running time of its predecessors. The paper concludes with validation of\nGIGA on both synthetic and real datasets, demonstrating that it reduces\nposterior approximation error by orders of magnitude compared with previous\ncoreset constructions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Feb 2018 23:52:12 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 23:46:26 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Campbell", "Trevor", ""], ["Broderick", "Tamara", ""]]}, {"id": "1802.01987", "submitter": "Clement Lee", "authors": "Clement Lee and Darren J Wilkinson", "title": "A hierarchical model of non-homogeneous Poisson processes for Twitter\n  retweets", "comments": "48 pages, 13 figures, 2 tables", "journal-ref": null, "doi": "10.1080/01621459.2019.1585358", "report-no": null, "categories": "stat.AP cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hierarchical model of non-homogeneous Poisson processes (NHPP)\nfor information diffusion on online social media, in particular Twitter\nretweets. The retweets of each original tweet are modelled by a NHPP, for which\nthe intensity function is a product of time-decaying components and another\ncomponent that depends on the follower count of the original tweet author. The\nlatter allows us to explain or predict the ultimate retweet count by a network\ncentrality-related covariate. The inference algorithm enables the Bayes factor\nto be computed, in order to facilitate model selection. Finally, the model is\napplied to the retweet data sets of two hashtags.\n", "versions": [{"version": "v1", "created": "Tue, 6 Feb 2018 15:04:02 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 16:09:06 GMT"}, {"version": "v3", "created": "Sun, 17 Feb 2019 17:19:28 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Lee", "Clement", ""], ["Wilkinson", "Darren J", ""]]}, {"id": "1802.02538", "submitter": "Yuling Yao", "authors": "Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman", "title": "Yes, but Did It Work?: Evaluating Variational Inference", "comments": "Appearing at International Conference on Machine Learning 2018", "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning, PMLR 80:5581-5590, 2018.\n  http://proceedings.mlr.press/v80/yao18a.html", "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it's always possible to compute a variational approximation to a\nposterior distribution, it can be difficult to discover problems with this\napproximation. We propose two diagnostic algorithms to alleviate this problem.\nThe Pareto-smoothed importance sampling (PSIS) diagnostic gives a goodness of\nfit measurement for joint distributions, while simultaneously improving the\nerror in the estimate. The variational simulation-based calibration (VSBC)\nassesses the average performance of point estimates.\n", "versions": [{"version": "v1", "created": "Wed, 7 Feb 2018 17:25:36 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 21:21:25 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Yao", "Yuling", ""], ["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "1802.03335", "submitter": "Dennis Prangle", "authors": "Thomas Ryder, Andrew Golightly, A. Stephen McGough, Dennis Prangle", "title": "Black-box Variational Inference for Stochastic Differential Equations", "comments": "V3 - revised based on ICML reviewer comments V2 - added\n  acknowledgements and link to code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter inference for stochastic differential equations is challenging due\nto the presence of a latent diffusion process. Working with an Euler-Maruyama\ndiscretisation for the diffusion, we use variational inference to jointly learn\nthe parameters and the diffusion paths. We use a standard mean-field\nvariational approximation of the parameter posterior, and introduce a recurrent\nneural network to approximate the posterior for the diffusion paths conditional\non the parameters. This neural network learns how to provide Gaussian state\ntransitions which bridge between observations in a very similar way to the\nconditioned diffusion process. The resulting black-box inference method can be\napplied to any SDE system with light tuning requirements. We illustrate the\nmethod on a Lotka-Volterra system and an epidemic model, producing accurate\nparameter estimates in a few hours.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 16:36:28 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 14:21:28 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 10:31:52 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Ryder", "Thomas", ""], ["Golightly", "Andrew", ""], ["McGough", "A. Stephen", ""], ["Prangle", "Dennis", ""]]}, {"id": "1802.03451", "submitter": "Ryan Adams", "authors": "Ryan P. Adams, Jeffrey Pennington, Matthew J. Johnson, Jamie Smith,\n  Yaniv Ovadia, Brian Patton, James Saunderson", "title": "Estimating the Spectral Density of Large Implicit Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important problems are characterized by the eigenvalues of a large\nmatrix. For example, the difficulty of many optimization problems, such as\nthose arising from the fitting of large models in statistics and machine\nlearning, can be investigated via the spectrum of the Hessian of the empirical\nloss function. Network data can be understood via the eigenstructure of a graph\nLaplacian matrix using spectral graph theory. Quantum simulations and other\nmany-body problems are often characterized via the eigenvalues of the solution\nspace, as are various dynamic systems. However, naive eigenvalue estimation is\ncomputationally expensive even when the matrix can be represented; in many of\nthese situations the matrix is so large as to only be available implicitly via\nproducts with vectors. Even worse, one may only have noisy estimates of such\nmatrix vector products. In this work, we combine several different techniques\nfor randomized estimation and show that it is possible to construct unbiased\nestimators to answer a broad class of questions about the spectra of such\nimplicit matrices, even in the presence of noise. We validate these methods on\nlarge-scale problems in which graph theory and random matrix theory provide\nground truth.\n", "versions": [{"version": "v1", "created": "Fri, 9 Feb 2018 21:26:11 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Adams", "Ryan P.", ""], ["Pennington", "Jeffrey", ""], ["Johnson", "Matthew J.", ""], ["Smith", "Jamie", ""], ["Ovadia", "Yaniv", ""], ["Patton", "Brian", ""], ["Saunderson", "James", ""]]}, {"id": "1802.03653", "submitter": "Michael Betancourt", "authors": "Michael Betancourt, Michael I. Jordan, Ashia C. Wilson", "title": "On Symplectic Optimization", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated gradient methods have had significant impact in machine learning\n-- in particular the theoretical side of machine learning -- due to their\nability to achieve oracle lower bounds. But their heuristic construction has\nhindered their full integration into the practical machine-learning algorithmic\ntoolbox, and has limited their scope. In this paper we build on recent work\nwhich casts acceleration as a phenomenon best explained in continuous time, and\nwe augment that picture by providing a systematic methodology for converting\ncontinuous-time dynamics into discrete-time algorithms while retaining oracle\nrates. Our framework is based on ideas from Hamiltonian dynamical systems and\nsymplectic integration. These ideas have had major impact in many areas in\napplied mathematics, but have not yet been seen to have a relationship with\noptimization.\n", "versions": [{"version": "v1", "created": "Sat, 10 Feb 2018 21:45:46 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 16:40:22 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Betancourt", "Michael", ""], ["Jordan", "Michael I.", ""], ["Wilson", "Ashia C.", ""]]}, {"id": "1802.03967", "submitter": "Sebastian Meyer", "authors": "Sebastian Meyer", "title": "Self-exciting Point Processes: Infections and Implementations", "comments": "comment on arXiv:1708.02647v1, submitted to Statistical Science, 4\n  pages", "journal-ref": "Statistical Science (2018); 33(3):327-329", "doi": "10.1214/18-STS653", "report-no": null, "categories": "stat.ME physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on Reinhart's \"Review of Self-Exciting Spatio-Temporal\nPoint Processes and Their Applications\" (arXiv:1708.02647v1). I contribute some\nexperiences from modelling the spread of infectious diseases. Furthermore, I\ntry to complement the review with regard to the availability of software for\nthe described models, which I think is essential in \"paving the way for new\nuses\".\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 10:47:22 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Meyer", "Sebastian", ""]]}, {"id": "1802.04366", "submitter": "Jelena Markovic", "authors": "Jelena Markovic and Amir Sepehri", "title": "Bouncy Hybrid Sampler as a Unifying Device", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a class of rejection-free Markov chain Monte Carlo\n(MCMC) samplers, named the Bouncy Hybrid Sampler, which unifies several\nexisting methods from the literature. Examples include the Bouncy Particle\nSampler of Peters and de With (2012), Bouchard-Cote et al. (2015) and the\nHamiltonian MCMC. Following the introduced general framework, we derive a new\nsampler called the Quadratic Bouncy Hybrid Sampler. We apply this novel sampler\nto the problem of sampling from a truncated Gaussian distribution.\n", "versions": [{"version": "v1", "created": "Mon, 12 Feb 2018 21:30:10 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 21:54:09 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Markovic", "Jelena", ""], ["Sepehri", "Amir", ""]]}, {"id": "1802.04452", "submitter": "Edgar Merkle", "authors": "E. C. Merkle, D. Furr, S. Rabe-Hesketh", "title": "Bayesian comparison of latent variable models: Conditional vs marginal\n  likelihoods", "comments": "Manuscript in press at Psychometrika; 31 pages, 8 figures", "journal-ref": null, "doi": "10.1007/s11336-019-09679-0", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical Bayesian methods for models with latent variables (or random effects)\ninvolve directly sampling the latent variables along with the model parameters.\nIn high-level software code for model definitions (using, e.g., BUGS, JAGS,\nStan), the likelihood is therefore specified as conditional on the latent\nvariables. This can lead researchers to perform model comparisons via\nconditional likelihoods, where the latent variables are considered model\nparameters. In other settings, however, typical model comparisons involve\nmarginal likelihoods where the latent variables are integrated out. This\ndistinction is often overlooked despite the fact that it can have a large\nimpact on the comparisons of interest. In this paper, we clarify and illustrate\nthese issues, focusing on the comparison of conditional and marginal Deviance\nInformation Criteria (DICs) and Watanabe-Akaike Information Criteria (WAICs) in\npsychometric modeling. The conditional/marginal distinction corresponds to\nwhether the model should be predictive for the clusters that are in the data or\nfor new clusters (where \"clusters\" typically correspond to higher-level units\nlike people or schools). Correspondingly, we show that marginal WAIC\ncorresponds to leave-one-cluster out (LOcO) cross-validation, whereas\nconditional WAIC corresponds to leave-one-unit out (LOuO). These results lead\nto recommendations on the general application of the criteria to models with\nlatent variables.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 03:13:15 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 03:44:03 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 13:10:40 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Merkle", "E. C.", ""], ["Furr", "D.", ""], ["Rabe-Hesketh", "S.", ""]]}, {"id": "1802.04791", "submitter": "Quanquan Gu", "authors": "Difan Zou and Pan Xu and Quanquan Gu", "title": "Stochastic Variance-Reduced Hamilton Monte Carlo Methods", "comments": "23 pages, 3 figures, 4 tables. In ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast stochastic Hamilton Monte Carlo (HMC) method, for sampling\nfrom a smooth and strongly log-concave distribution. At the core of our\nproposed method is a variance reduction technique inspired by the recent\nadvance in stochastic optimization. We show that, to achieve $\\epsilon$\naccuracy in 2-Wasserstein distance, our algorithm achieves $\\tilde\nO(n+\\kappa^{2}d^{1/2}/\\epsilon+\\kappa^{4/3}d^{1/3}n^{2/3}/\\epsilon^{2/3})$\ngradient complexity (i.e., number of component gradient evaluations), which\noutperforms the state-of-the-art HMC and stochastic gradient HMC methods in a\nwide regime. We also extend our algorithm for sampling from smooth and general\nlog-concave distributions, and prove the corresponding gradient complexity as\nwell. Experiments on both synthetic and real data demonstrate the superior\nperformance of our algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Feb 2018 18:54:54 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 00:40:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Zou", "Difan", ""], ["Xu", "Pan", ""], ["Gu", "Quanquan", ""]]}, {"id": "1802.04911", "submitter": "Richard Zhang", "authors": "Richard Y. Zhang, Salar Fattahi, Somayeh Sojoudi", "title": "Large-Scale Sparse Inverse Covariance Estimation via Thresholding and\n  Max-Det Matrix Completion", "comments": "35-th International Conference on Machine Learning (ICML 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sparse inverse covariance estimation problem is commonly solved using an\n$\\ell_{1}$-regularized Gaussian maximum likelihood estimator known as\n\"graphical lasso\", but its computational cost becomes prohibitive for large\ndata sets. A recent line of results showed--under mild assumptions--that the\ngraphical lasso estimator can be retrieved by soft-thresholding the sample\ncovariance matrix and solving a maximum determinant matrix completion (MDMC)\nproblem. This paper proves an extension of this result, and describes a\nNewton-CG algorithm to efficiently solve the MDMC problem. Assuming that the\nthresholded sample covariance matrix is sparse with a sparse Cholesky\nfactorization, we prove that the algorithm converges to an $\\epsilon$-accurate\nsolution in $O(n\\log(1/\\epsilon))$ time and $O(n)$ memory. The algorithm is\nhighly efficient in practice: we solve the associated MDMC problems with as\nmany as 200,000 variables to 7-9 digits of accuracy in less than an hour on a\nstandard laptop computer running MATLAB.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 01:00:10 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 03:24:32 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 01:13:24 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zhang", "Richard Y.", ""], ["Fattahi", "Salar", ""], ["Sojoudi", "Somayeh", ""]]}, {"id": "1802.05570", "submitter": "Yoav Zemel", "authors": "Max Sommerfeld, J\\\"orn Schrieber, Yoav Zemel, Axel Munk", "title": "Optimal Transport: Fast Probabilistic Approximation with Exact Solvers", "comments": "to appear in Journal of Machine Learning Research", "journal-ref": "Journal of Machine Learning Research 20(105):1-23, 2019", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple subsampling scheme for fast randomized approximate\ncomputation of optimal transport distances. This scheme operates on a random\nsubset of the full data and can use any exact algorithm as a black-box\nback-end, including state-of-the-art solvers and entropically penalized\nversions. It is based on averaging the exact distances between empirical\nmeasures generated from independent samples from the original measures and can\neasily be tuned towards higher accuracy or shorter computation times. To this\nend, we give non-asymptotic deviation bounds for its accuracy in the case of\ndiscrete optimal transport problems. In particular, we show that in many\nimportant instances, including images (2D-histograms), the approximation error\nis independent of the size of the full problem. We present numerical\nexperiments that demonstrate that a very good approximation in typical\napplications can be obtained in a computation time that is several orders of\nmagnitude smaller than what is required for exact computation of the full\nproblem.\n", "versions": [{"version": "v1", "created": "Wed, 14 Feb 2018 14:59:32 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:11:37 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 23:45:54 GMT"}, {"version": "v4", "created": "Fri, 5 Jul 2019 13:11:07 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Sommerfeld", "Max", ""], ["Schrieber", "J\u00f6rn", ""], ["Zemel", "Yoav", ""], ["Munk", "Axel", ""]]}, {"id": "1802.05936", "submitter": "Viviana Lobo", "authors": "Viviana G R Lobo, Tha\\'is C O da Fonseca, Fernando A S Moura", "title": "Bayesian cross-validation of geostatistical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of validating or criticising models for georeferenced data is\nchallenging, since the conclusions can vary significantly depending on the\nlocations of the validation set. This work proposes the use of cross-validation\ntechniques to assess the goodness of fit of spatial models in different regions\nof the spatial domain to account for uncertainty in the choice of the\nvalidation sets. An obvious problem with the basic cross-validation scheme is\nthat it is based on selecting only a few out of sample locations to validate\nthe model, possibily making the conclusions sensitive to which partition of the\ndata into training and validation cases is utilized. A possible solution to\nthis issue would be to consider all possible configurations of data divided\ninto training and validation observations. From a Bayesian point of view, this\ncould be computationally demanding, as estimation of parameters usually\nrequires Monte Carlo Markov Chain methods. To deal with this problem, we\npropose the use of estimated discrepancy functions considering all\nconfigurations of data partition in a computationally efficient manner based on\nsampling importance resampling. In particular, we consider uncertainty in the\nlocations by assigning a prior distribution to them. Furthermore, we propose a\nstratified cross-validation scheme to take into account spatial heterogeneity,\nreducing the total variance of estimated predictive discrepancy measures\nconsidered for model assessment. We illustrate the advantages of our proposal\nwith simulated examples of homogeneous and inhomogeneous spatial processes to\ninvestigate the effects of our proposal in scenarios of preferential sampling\ndesigns. The methods are illustrated with an application to a rainfall dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 14:03:33 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Lobo", "Viviana G R", ""], ["da Fonseca", "Tha\u00eds C O", ""], ["Moura", "Fernando A S", ""]]}, {"id": "1802.06151", "submitter": "Shinichiro Shirota Dr", "authors": "Shinichiro Shirota, Sudipto Banerjee", "title": "Scalable Inference for Space-Time Gaussian Cox Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-Gaussian Cox process is a flexible and popular class of point pattern\nmodels for capturing spatial and space-time dependence for point patterns.\nModel fitting requires approximation of stochastic integrals which is\nimplemented through discretization over the domain of interest. With fine scale\ndiscretization, inference based on Markov chain Monte Carlo is computationally\nburdensome because of the cost of matrix decompositions and storage, such as\nthe Cholesky, for high dimensional covariance matrices associated with latent\nGaussian variables. This article addresses these computational bottlenecks by\ncombining two recent developments: (i) a data augmentation strategy that has\nbeen proposed for space-time Gaussian Cox processes that is based on exact\nBayesian inference and does not require fine grid approximations for infinite\ndimensional integrals, and (ii) a recently developed family of\nsparsity-inducing Gaussian processes, called nearest-neighbor Gaussian\nprocesses, to avoid expensive matrix computations. Our inference is delivered\nwithin the fully model-based Bayesian paradigm and does not sacrifice the\nrichness of traditional log-Gaussian Cox processes. We apply our method to\ncrime event data in San Francisco and investigate the recovery of the intensity\nsurface.\n", "versions": [{"version": "v1", "created": "Fri, 16 Feb 2018 22:22:48 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 18:17:55 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1802.06350", "submitter": "Haakon Bakka", "authors": "Haakon Bakka, H{\\aa}vard Rue, Geir-Arne Fuglstad, Andrea Riebler,\n  David Bolin, Elias Krainski, Daniel Simpson, Finn Lindgren", "title": "Spatial modelling with R-INLA: A review", "comments": "Extensive update, restructuring of sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coming up with Bayesian models for spatial data is easy, but performing\ninference with them can be challenging. Writing fast inference code for a\ncomplex spatial model with realistically-sized datasets from scratch is\ntime-consuming, and if changes are made to the model, there is little guarantee\nthat the code performs well. The key advantages of R-INLA are the ease with\nwhich complex models can be created and modified, without the need to write\ncomplex code, and the speed at which inference can be done even for spatial\nproblems with hundreds of thousands of observations.\n  R-INLA handles latent Gaussian models, where fixed effects, structured and\nunstructured Gaussian random effects are combined linearly in a linear\npredictor, and the elements of the linear predictor are observed through one or\nmore likelihoods. The structured random effects can be both standard areal\nmodel such as the Besag and the BYM models, and geostatistical models from a\nsubset of the Mat\\'ern Gaussian random fields. In this review, we discuss the\nlarge success of spatial modelling with R-INLA and the types of spatial models\nthat can be fitted, we give an overview of recent developments for areal\nmodels, and we give an overview of the stochastic partial differential equation\n(SPDE) approach and some of the ways it can be extended beyond the assumptions\nof isotropy and separability. In particular, we describe how slight changes to\nthe SPDE approach leads to straight-forward approaches for non-stationary\nspatial models and non-separable space-time models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Feb 2018 08:46:22 GMT"}, {"version": "v2", "created": "Tue, 8 May 2018 07:33:31 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Bakka", "Haakon", ""], ["Rue", "H\u00e5vard", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""], ["Bolin", "David", ""], ["Krainski", "Elias", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""]]}, {"id": "1802.06678", "submitter": "Roi Naveiro", "authors": "Roi Naveiro, Sim\\'on Rodr\\'iguez, David R\\'ios Insua", "title": "Large Scale Automated Forecasting for Monitoring Network Safety and\n  Security", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real time large scale streaming data pose major challenges to forecasting, in\nparticular defying the presence of human experts to perform the corresponding\nanalysis. We present here a class of models and methods used to develop an\nautomated, scalable and versatile system for large scale forecasting oriented\ntowards safety and security monitoring. Our system provides short and long term\nforecasts and uses them to detect safety and security issues in relation with\nmultiple internet connected devices well in advance they might take place.\n", "versions": [{"version": "v1", "created": "Mon, 19 Feb 2018 15:50:02 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 16:16:43 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Naveiro", "Roi", ""], ["Rodr\u00edguez", "Sim\u00f3n", ""], ["Insua", "David R\u00edos", ""]]}, {"id": "1802.06966", "submitter": "Paul Van Mulbregt", "authors": "Paul van Mulbregt", "title": "Computing the Cumulative Distribution Function and Quantiles of the\n  One-sided Kolmogorov-Smirnov Statistic", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulative distribution and quantile functions for the one-sided one\nsample Kolmogorov-Smirnov probability distributions are used for\ngoodness-of-fit testing. While the Smirnov-Birnbaum-Tingey formula for the CDF\nappears straight forward, its numerical evaluation generates intermediate\nresults spanning many hundreds of orders of magnitude and at times requires\nvery precise accurate representations. Computing the quantile function for any\nspecific probability may require evaluating both the CDF and its derivative,\nboth of which are computationally expensive. To work around avoid these issues,\ndifferent algorithms can be used across different parts of the domain, and\napproximations can be used to reduce the computational requirements. We show\nhere that straight forward implementation incurs accuracy loss for sample sizes\nof well under 1000. Further the approximations in use inside the open source\nSciPy python software often result in increased computation, not just reduced\naccuracy, and at times suffer catastrophic loss of accuracy for any sample\nsize. Then we provide alternate algorithms which restore accuracy and\nefficiency across the whole domain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 04:49:32 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["van Mulbregt", "Paul", ""]]}, {"id": "1802.07148", "submitter": "Andrew Golightly", "authors": "Andrew Golightly, Emma Bradley, Tom Lowe, Colin S. Gillespie", "title": "Correlated pseudo-marginal schemes for time-discretised stochastic\n  kinetic models", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenging problem of conducting fully Bayesian inference for the\nreaction rate constants governing stochastic kinetic models (SKMs) is\nconsidered. Given the challenges underlying this problem, the Markov jump\nprocess representation is routinely replaced by an approximation based on a\nsuitable time discretisation of the system of interest. Improving the accuracy\nof these schemes amounts to using an ever finer discretisation level, which in\nthe context of the inference problem, requires integrating over the uncertainty\nin the process at a predetermined number of intermediate times between\nobservations. Pseudo-marginal Metropolis-Hastings schemes are increasingly\nused, since for a given discretisation level, the observed data likelihood can\nbe unbiasedly estimated using a particle filter. When observations are\nparticularly informative an auxiliary particle filter can be implemented, by\nemploying an appropriate construct to push the state particles towards the\nobservations in a sensible way. Recent work in state-space settings has shown\nhow the pseudo-marginal approach can be made much more efficient by correlating\nthe underlying pseudo-random numbers used to form the likelihood estimate at\nthe current and proposed values of the unknown parameters. We extend this\napproach to the time-discretised SKM framework by correlating the innovations\nthat drive the auxiliary particle filter. We find that the resulting approach\noffers substantial gains in efficiency over a standard implementation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Feb 2018 15:20:30 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 09:58:41 GMT"}, {"version": "v3", "created": "Wed, 9 Jan 2019 13:29:27 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Golightly", "Andrew", ""], ["Bradley", "Emma", ""], ["Lowe", "Tom", ""], ["Gillespie", "Colin S.", ""]]}, {"id": "1802.08318", "submitter": "Uthaipon Tantipongpipat", "authors": "Aleksandar Nikolov and Mohit Singh and Uthaipon Tao Tantipongpipat", "title": "Proportional Volume Sampling and Approximation Algorithms for A-Optimal\n  Design", "comments": "Add that proportional volume sampling also solves D-optimal and\n  generalized ratio problem. Add some reference from last version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the optimal design problems where the goal is to choose a set of\nlinear measurements to obtain the most accurate estimate of an unknown vector\nin $d$ dimensions. We study the $A$-optimal design variant where the objective\nis to minimize the average variance of the error in the maximum likelihood\nestimate of the vector being measured. The problem also finds applications in\nsensor placement in wireless networks, sparse least squares regression, feature\nselection for $k$-means clustering, and matrix approximation. In this paper, we\nintroduce proportional volume sampling to obtain improved approximation\nalgorithms for $A$-optimal design. Our main result is to obtain improved\napproximation algorithms for the $A$-optimal design problem by introducing the\nproportional volume sampling algorithm. Our results nearly optimal bounds in\nthe asymptotic regime when the number of measurements done, $k$, is\nsignificantly more than the dimension $d$. We also give first approximation\nalgorithms when $k$ is small including when $k=d$. The proportional\nvolume-sampling algorithm also gives approximation algorithms for other optimal\ndesign objectives such as $D$-optimal design and generalized ratio objective\nmatching or improving previous best known results. Interestingly, we show that\na similar guarantee cannot be obtained for the $E$-optimal design problem. We\nalso show that the $A$-optimal design problem is NP-hard to approximate within\na fixed constant when $k=d$.\n", "versions": [{"version": "v1", "created": "Thu, 22 Feb 2018 22:02:42 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 21:17:45 GMT"}, {"version": "v3", "created": "Fri, 1 Jun 2018 14:45:10 GMT"}, {"version": "v4", "created": "Wed, 11 Jul 2018 19:05:11 GMT"}, {"version": "v5", "created": "Tue, 17 Jul 2018 15:10:52 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Nikolov", "Aleksandar", ""], ["Singh", "Mohit", ""], ["Tantipongpipat", "Uthaipon Tao", ""]]}, {"id": "1802.08363", "submitter": "Ranjan Maitra", "authors": "Andrew Lithio and Ranjan Maitra", "title": "An efficient $k$-means-type algorithm for clustering datasets with\n  incomplete records", "comments": "21 pages, 12 figures, 3 tables, in press, Statistical Analysis and\n  Data Mining -- The ASA Data Science Journal, 2018", "journal-ref": null, "doi": "10.1002/sam.11392", "report-no": null, "categories": "stat.ML astro-ph.HE cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$-means algorithm is arguably the most popular nonparametric clustering\nmethod but cannot generally be applied to datasets with incomplete records. The\nusual practice then is to either impute missing values under an assumed\nmissing-completely-at-random mechanism or to ignore the incomplete records, and\napply the algorithm on the resulting dataset. We develop an efficient version\nof the $k$-means algorithm that allows for clustering in the presence of\nincomplete records. Our extension is called $k_m$-means and reduces to the\n$k$-means algorithm when all records are complete. We also provide\ninitialization strategies for our algorithm and methods to estimate the number\nof groups in the dataset. Illustrations and simulations demonstrate the\nefficacy of our approach in a variety of settings and patterns of missing data.\nOur methods are also applied to the analysis of activation images obtained from\na functional Magnetic Resonance Imaging experiment.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 02:24:14 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 13:15:48 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Lithio", "Andrew", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1802.08471", "submitter": "Simon Barthelm\\'e", "authors": "Nicolas Tremblay, Simon Barthelme and Pierre-Olivier Amblard", "title": "Optimized Algorithms to Sample Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we discuss several sampling algorithms for\nDeterminantal Point Processes (DPP). DPPs have recently gained a broad interest\nin the machine learning and statistics literature as random point processes\nwith negative correlation, i.e., ones that can generate a \"diverse\" sample from\na set of items. They are parametrized by a matrix $\\mathbf{L}$, called\n$L$-ensemble, that encodes the correlations between items. The standard\nsampling algorithm is separated in three phases: 1/~eigendecomposition of\n$\\mathbf{L}$, 2/~an eigenvector sampling phase where $\\mathbf{L}$'s\neigenvectors are sampled independently via a Bernoulli variable parametrized by\ntheir associated eigenvalue, 3/~a Gram-Schmidt-type orthogonalisation procedure\nof the sampled eigenvectors.\n  In a naive implementation, the computational cost of the third step is on\naverage $\\mathcal{O}(N\\mu^3)$ where $\\mu$ is the average number of samples of\nthe DPP. We give an algorithm which runs in $\\mathcal{O}(N\\mu^2)$ and is\nextremely simple to implement. If memory is a constraint, we also describe a\ndual variant with reduced memory costs. In addition, we discuss implementation\ndetails often missing in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 10:30:41 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Tremblay", "Nicolas", ""], ["Barthelme", "Simon", ""], ["Amblard", "Pierre-Olivier", ""]]}, {"id": "1802.08622", "submitter": "Ngom Papa", "authors": "Jean Claude Utazirubanda, Tomas Leon, Papa Ngom", "title": "Variable selection via Group LASSO Approach : Application to the Cox\n  Regression and frailty model", "comments": "26 pages , 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survival outcome supplemented with both clinical\ninformation and high-dimensional gene expression data, use of the traditional\nCox proportional hazards model (1972) fails to meet some emerging needs in\nbiomedical research. First, the number of covariates is generally much larger\nthe sample size. Secondly, predicting an outcome based on individual gene\nexpression is inadequate because multiple biological processes and functional\npathways regulate the expression associated with a gene. Another challenge is\nthat the Cox model assumes that populations are homogenous, implying that all\nindividuals have the same risk of death, which is rarely true due to unmeasured\nrisk factors among populations. In this paper we propose group LASSO with\ngamma-distributed frailty for variable selection in Cox regression by extending\nprevious scholarship to account for heterogeneity among group structures\nrelated to exposure and susceptibility. The consistency property of the\nproposed method is established. This method is appropriate for addressing a\nwide variety of research questions from genetics to air pollution. Simulated\nanalysis shows promising performance by group LASSO compared with other\nmethods, including group SCAD and group MCP. Future directions include\nexpanding the use of frailty with adaptive group LASSO and sparse group LASS.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 16:28:15 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Utazirubanda", "Jean Claude", ""], ["Leon", "Tomas", ""], ["Ngom", "Papa", ""]]}, {"id": "1802.08671", "submitter": "Espen Bernton", "authors": "Espen Bernton", "title": "Langevin Monte Carlo and JKO splitting", "comments": "24 pages. Similar to arxiv:1802.08089", "journal-ref": "Proceedings of the 31st Conference On Learning Theory, PMLR\n  75:1777-1798, 2018", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms based on discretizing Langevin diffusion are popular tools for\nsampling from high-dimensional distributions. We develop novel connections\nbetween such Monte Carlo algorithms, the theory of Wasserstein gradient flow,\nand the operator splitting approach to solving PDEs. In particular, we show\nthat a proximal version of the Unadjusted Langevin Algorithm corresponds to a\nscheme that alternates between solving the gradient flows of two specific\nfunctionals on the space of probability measures. Using this perspective, we\nderive some new non-asymptotic results on the convergence properties of this\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 23 Feb 2018 18:40:26 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 19:39:47 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bernton", "Espen", ""]]}, {"id": "1802.08798", "submitter": "Dao Nguyen", "authors": "Dao Nguyen, Perry de Valpine, Yves Atchade, Daniel Turek, Nicholas\n  Michaud and Christopher Paciorek", "title": "Automatic adaptation of MCMC algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are ubiquitous tools for\nsimulation-based inference in many fields but designing and identifying good\nMCMC samplers is still an open question. This paper introduces a novel MCMC\nalgorithm, namely, Auto Adapt MCMC. For sampling variables or blocks of\nvariables, we use two levels of adaptation where the inner adaptation optimizes\nthe MCMC performance within each sampler, while the outer adaptation explores\nthe valid space of kernels to find the optimal samplers. We provide a\ntheoretical foundation for our approach. To show the generality and usefulness\nof the approach, we describe a framework using only standard MCMC samplers as\ncandidate samplers and some adaptation schemes for both inner and outer\niterations. In several benchmark problems, we show that our proposed approach\nsubstantially outperforms other approaches, including an automatic blocking\nalgorithm, in terms of MCMC efficiency and computational time.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 04:40:52 GMT"}, {"version": "v2", "created": "Mon, 18 Nov 2019 17:49:02 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Nguyen", "Dao", ""], ["de Valpine", "Perry", ""], ["Atchade", "Yves", ""], ["Turek", "Daniel", ""], ["Michaud", "Nicholas", ""], ["Paciorek", "Christopher", ""]]}, {"id": "1802.08895", "submitter": "Yueyong Shi", "authors": "Yueyong Shi, Jian Huang, Yuling Jiao, Qinglong Yang", "title": "A Semi-Smooth Newton Algorithm for High-Dimensional Nonconvex Sparse\n  Learning", "comments": "4th revision submitted to IEEE Transactions on Neural Networks and\n  Learning Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The smoothly clipped absolute deviation (SCAD) and the minimax concave\npenalty (MCP) penalized regression models are two important and widely used\nnonconvex sparse learning tools that can handle variable selection and\nparameter estimation simultaneously, and thus have potential applications in\nvarious fields such as mining biological data in high-throughput biomedical\nstudies. Theoretically, these two models enjoy the oracle property even in the\nhigh-dimensional settings, where the number of predictors $p$ may be much\nlarger than the number of observations $n$. However, numerically, it is quite\nchallenging to develop fast and stable algorithms due to their non-convexity\nand non-smoothness. In this paper we develop a fast algorithm for SCAD and MCP\npenalized learning problems. First, we show that the global minimizers of both\nmodels are roots of the nonsmooth equations. Then, a semi-smooth Newton (SSN)\nalgorithm is employed to solve the equations. We prove that the SSN algorithm\nconverges locally and superlinearly to the Karush-Kuhn-Tucker (KKT) points.\nComputational complexity analysis shows that the cost of the SSN algorithm per\niteration is $O(np)$. Combined with the warm-start technique, the SSN algorithm\ncan be very efficient and accurate. Simulation studies and a real data example\nsuggest that our SSN algorithm, with comparable solution accuracy with the\ncoordinate descent (CD) and the difference of convex (DC) proximal Newton\nalgorithms, is more computationally efficient.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:08:22 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2018 01:31:58 GMT"}, {"version": "v3", "created": "Sun, 24 Feb 2019 08:51:01 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 19:29:31 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Shi", "Yueyong", ""], ["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Yang", "Qinglong", ""]]}, {"id": "1802.08898", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Nisheeth K. Vishnoi", "title": "Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.PR stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from\nhigh-dimensional distributions in Statistics and Machine learning. HMC is known\nto run very efficiently in practice and its popular second-order \"leapfrog\"\nimplementation has long been conjectured to run in $d^{1/4}$ gradient\nevaluations. Here we show that this conjecture is true when sampling from\nstrongly log-concave target distributions that satisfy a weak third-order\nregularity property associated with the input data. Our regularity condition is\nweaker than the Lipschitz Hessian property and allows us to show faster\nconvergence bounds for a much larger class of distributions than would be\npossible with the usual Lipschitz Hessian constant alone. Important\ndistributions that satisfy our regularity condition include posterior\ndistributions used in Bayesian logistic regression for which the data satisfies\nan \"incoherence\" property. Our result compares favorably with the best\navailable bounds for the class of strongly log-concave distributions, which\ngrow like $d^{{1}/{2}}$ gradient evaluations with the dimension. Moreover, our\nsimulations on synthetic data suggest that, when our regularity condition is\nsatisfied, leapfrog HMC performs better than its competitors -- both in terms\nof accuracy and in terms of the number of gradient evaluations it requires.\n", "versions": [{"version": "v1", "created": "Sat, 24 Feb 2018 19:23:21 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 00:27:58 GMT"}, {"version": "v3", "created": "Tue, 31 Jul 2018 16:26:27 GMT"}, {"version": "v4", "created": "Thu, 2 Aug 2018 15:31:10 GMT"}, {"version": "v5", "created": "Thu, 9 Aug 2018 18:24:09 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Mangoubi", "Oren", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1802.09188", "submitter": "Alain Durmus", "authors": "Alain Durmus, Szymon Majewski, B{\\l}a\\.zej Miasojedow", "title": "Analysis of Langevin Monte Carlo via convex optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide new insights on the Unadjusted Langevin Algorithm.\nWe show that this method can be formulated as a first order optimization\nalgorithm of an objective functional defined on the Wasserstein space of order\n$2$. Using this interpretation and techniques borrowed from convex\noptimization, we give a non-asymptotic analysis of this method to sample from\nlogconcave smooth target distribution on $\\mathbb{R}^d$. Based on this\ninterpretation, we propose two new methods for sampling from a non-smooth\ntarget distribution, which we analyze as well. Besides, these new algorithms\nare natural extensions of the Stochastic Gradient Langevin Dynamics (SGLD)\nalgorithm, which is a popular extension of the Unadjusted Langevin Algorithm.\nSimilar to SGLD, they only rely on approximations of the gradient of the target\nlog density and can be used for large-scale Bayesian inference.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 07:50:57 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 22:15:40 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Durmus", "Alain", ""], ["Majewski", "Szymon", ""], ["Miasojedow", "B\u0142a\u017cej", ""]]}, {"id": "1802.09243", "submitter": "Thomas Hotz", "authors": "Gabriele Eichfelder, Thomas Hotz, Johannes Wieditz", "title": "An algorithm for computing Fr\\'echet means on the sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most optimisation methods an essential assumption is the vector space\nstructure of the feasible set. This condition is not fulfilled if we consider\noptimisation problems over the sphere. We present an algorithm for solving a\nspecial global problem over the sphere, namely the determination of Fr\\'echet\nmeans, which are points minimising the mean distance to a given set of points.\nThe Branch and Bound method derived needs no further assumptions on the input\ndata, but is able to cope with this objective function which is neither convex\nnor differentiable. The algorithm's performance is tested on simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 10:57:03 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Eichfelder", "Gabriele", ""], ["Hotz", "Thomas", ""], ["Wieditz", "Johannes", ""]]}, {"id": "1802.09565", "submitter": "Daniele Durante", "authors": "Daniele Durante", "title": "Conjugate Bayes for probit regression via unified skew-normal\n  distributions", "comments": null, "journal-ref": "Biometrika (2019). 106, 765-779", "doi": "10.1093/biomet/asz034", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models for dichotomous data are ubiquitous in statistics. Besides\nbeing useful for inference on binary responses, these methods serve also as\nbuilding blocks in more complex formulations, such as density regression,\nnonparametric classification and graphical models. Within the Bayesian\nframework, inference proceeds by updating the priors for the coefficients,\ntypically set to be Gaussians, with the likelihood induced by probit or logit\nregressions for the responses. In this updating, the apparent absence of a\ntractable posterior has motivated a variety of computational methods, including\nMarkov Chain Monte Carlo routines and algorithms which approximate the\nposterior. Despite being routinely implemented, Markov Chain Monte Carlo\nstrategies face mixing or time-inefficiency issues in large p and small n\nstudies, whereas approximate routines fail to capture the skewness typically\nobserved in the posterior. This article proves that the posterior distribution\nfor the probit coefficients has a unified skew-normal kernel, under Gaussian\npriors. Such a novel result allows efficient Bayesian inference for a wide\nclass of applications, especially in large p and small-to-moderate n studies\nwhere state-of-the-art computational methods face notable issues. These\nadvances are outlined in a genetic study, and further motivate the development\nof a wider class of conjugate priors for probit models along with methods to\nobtain independent and identically distributed samples from the unified\nskew-normal posterior.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 19:29:27 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 13:37:47 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 15:41:21 GMT"}, {"version": "v4", "created": "Tue, 23 Oct 2018 10:13:05 GMT"}, {"version": "v5", "created": "Sun, 17 Nov 2019 18:11:53 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""]]}, {"id": "1802.09578", "submitter": "Yining Wang", "authors": "Yining Wang, Yi Wu, Simon S. Du", "title": "Near-Linear Time Local Polynomial Nonparametric Estimation with Box\n  Kernels", "comments": "Accepted to INFORMS Journal on Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DS cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local polynomial regression (Fan and Gijbels 1996) is an important class of\nmethods for nonparametric density estimation and regression problems. However,\nstraightforward implementation of local polynomial regression has quadratic\ntime complexity which hinders its applicability in large-scale data analysis.\nIn this paper, we significantly accelerate the computation of local polynomial\nestimates by novel applications of multi-dimensional binary indexed trees\n(Fenwick 1994). Both time and space complexity of our proposed algorithm is\nnearly linear in the number of input data points. Simulation results confirm\nthe efficiency and effectiveness of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 20:04:58 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 15:42:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Yining", ""], ["Wu", "Yi", ""], ["Du", "Simon S.", ""]]}, {"id": "1802.09650", "submitter": "Scott Sisson", "authors": "Y. Fan and S. A. Sisson", "title": "ABC Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"ABC Samplers\", is to appear in the forthcoming Handbook of\nApproximate Bayesian Computation (2018). It details the main ideas and\nalgorithms used to sample from the ABC approximation to the posterior\ndistribution, including methods based on rejection/importance sampling, MCMC\nand sequential Monte Carlo.\n", "versions": [{"version": "v1", "created": "Mon, 26 Feb 2018 23:57:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.09720", "submitter": "Scott Sisson", "authors": "S. A. Sisson and Y. Fan and M. A. Beaumont", "title": "Overview of Approximate Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"Overview of Approximate Bayesian Computation\", is to appear as\nthe first chapter in the forthcoming Handbook of Approximate Bayesian\nComputation (2018). It details the main ideas and concepts behind ABC methods\nwith many examples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:18:12 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Sisson", "S. A.", ""], ["Fan", "Y.", ""], ["Beaumont", "M. A.", ""]]}, {"id": "1802.09725", "submitter": "Scott Sisson", "authors": "D. J. Nott and V. M.-H. Ong and Y. Fan and S. A. Sisson", "title": "High-dimensional ABC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Chapter, \"High-dimensional ABC\", is to appear in the forthcoming\nHandbook of Approximate Bayesian Computation (2018). It details the main ideas\nand concepts behind extending ABC methods to higher dimensions, with supporting\nexamples and illustrations.\n", "versions": [{"version": "v1", "created": "Tue, 27 Feb 2018 05:47:13 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Nott", "D. J.", ""], ["Ong", "V. M. -H.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1802.10529", "submitter": "Maurits Kaptein", "authors": "Maurits Kaptein and Paul Ketelaar", "title": "Maximum likelihood estimation of a finite mixture of logistic regression\n  models in a continuous data stream", "comments": "1 figure. Working paper including [R] package", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In marketing we are often confronted with a continuous stream of responses to\nmarketing messages. Such streaming data provide invaluable information\nregarding message effectiveness and segmentation. However, streaming data are\nhard to analyze using conventional methods: their high volume and the fact that\nthey are continuously augmented means that it takes considerable time to\nanalyze them. We propose a method for estimating a finite mixture of logistic\nregression models which can be used to cluster customers based on a continuous\nstream of responses. This method, which we coin oFMLR, allows segments to be\nidentified in data streams or extremely large static datasets. Contrary to\nblack box algorithms, oFMLR provides model estimates that are directly\ninterpretable. We first introduce oFMLR, explaining in passing general topics\nsuch as online estimation and the EM algorithm, making this paper a high level\noverview of possible methods of dealing with large data streams in marketing\npractice. Next, we discuss model convergence, identifiability, and relations to\nalternative, Bayesian, methods; we also identify more general issues that arise\nfrom dealing with continuously augmented data sets. Finally, we introduce the\noFMLR [R] package and evaluate the method by numerical simulation and by\nanalyzing a large customer clickstream dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 16:43:16 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kaptein", "Maurits", ""], ["Ketelaar", "Paul", ""]]}]