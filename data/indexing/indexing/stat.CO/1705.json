[{"id": "1705.00163", "submitter": "Iickho Song", "authors": "Iickho Song", "title": "A Proof of the Explicit Formula for Product Moments of Multivariate\n  Gaussian Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A detailed proof of a recent result on explicit formulae for the product\nmoments $E \\left \\{ X_1^{a_1} X_2^{a_2} \\cdots X_n^{a_n}\\right \\}$ of\nmultivariate Gaussian random variables is provided in this note.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 09:53:59 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Song", "Iickho", ""]]}, {"id": "1705.00166", "submitter": "Alain Durmus", "authors": "Alain Durmus, Eric Moulines, Eero Saksman", "title": "On the convergence of Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the irreducibility and geometric ergodicity of the\nHamiltonian Monte Carlo (HMC) algorithm. We consider cases where the number of\nsteps of the symplectic integrator is either fixed or random. Under mild\nconditions on the potential $\\F$ associated with target distribution $\\pi$, we\nfirst show that the Markov kernel associated to the HMC algorithm is\nirreducible and recurrent. Under more stringent conditions, we then establish\nthat the Markov kernel is Harris recurrent. Finally, we provide verifiable\nconditions on $\\F$ under which the HMC sampler is geometrically ergodic.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 10:49:58 GMT"}, {"version": "v2", "created": "Fri, 10 May 2019 21:02:54 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Saksman", "Eero", ""]]}, {"id": "1705.00219", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Steve Hanneke and Liu Yang", "title": "Learning with Changing Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the setting where features are added or change\ninterpretation over time, which has applications in multiple domains such as\nretail, manufacturing, finance. In particular, we propose an approach to\nprovably determine the time instant from which the new/changed features start\nbecoming relevant with respect to an output variable in an agnostic\n(supervised) learning setting. We also suggest an efficient version of our\napproach which has the same asymptotic performance. Moreover, our theory also\napplies when we have more than one such change point. Independent post analysis\nof a change point identified by our method for a large retailer revealed that\nit corresponded in time with certain unflattering news stories about a brand\nthat resulted in the change in customer behavior. We also applied our method to\ndata from an advanced manufacturing plant identifying the time instant from\nwhich downstream features became relevant. To the best of our knowledge this is\nthe first work that formally studies change point detection in a distribution\nindependent agnostic setting, where the change point is based on the changing\nrelationship between input and output.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 18:11:10 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Hanneke", "Steve", ""], ["Yang", "Liu", ""]]}, {"id": "1705.00546", "submitter": "Fernando J. Iglesias-Garcia", "authors": "Fernando J. Iglesias-Garcia, Pranab K. Mandal, M\\'elanie Bocquel,\n  Antonio G. Marques", "title": "Riemann-Langevin Particle Filtering in Track-Before-Detect", "comments": "Minor grammatical updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Track-before-detect (TBD) is a powerful approach that consists in providing\nthe tracker with sensor measurements directly without pre-detection. Due to the\nmeasurement model non-linearities, online state estimation in TBD is most\ncommonly solved via particle filtering. Existing particle filters for TBD do\nnot incorporate measurement information in their proposal distribution. The\nLangevin Monte Carlo (LMC) is a sampling method whose proposal is able to\nexploit all available knowledge of the posterior (that is, both prior and\nmeasurement information). This letter synthesizes recent advances in LMC-based\nfiltering to describe the Riemann-Langevin particle filter and introduces its\nnovel application to TBD. The benefits of our approach are illustrated in a\nchallenging low-noise scenario.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:41:50 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 22:57:05 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Iglesias-Garcia", "Fernando J.", ""], ["Mandal", "Pranab K.", ""], ["Bocquel", "M\u00e9lanie", ""], ["Marques", "Antonio G.", ""]]}, {"id": "1705.00554", "submitter": "Peter Green", "authors": "Peter J Green, Alun Thomas", "title": "A structural Markov property for decomposable graph laws that allows\n  control of clique intersections", "comments": "10 pages, 3 figures; updated from V1 following journal review, new\n  more explicit title and added section on inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new kind of structural Markov property for probabilistic laws on\ndecomposable graphs, which allows the explicit control of interactions between\ncliques, so is capable of encoding some interesting structure. We prove the\nequivalence of this property to an exponential family assumption, and discuss\nidentifiability, modelling, inferential and computational implications.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:59:56 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 09:52:18 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Green", "Peter J", ""], ["Thomas", "Alun", ""]]}, {"id": "1705.00841", "submitter": "James Johndrow", "authors": "James E. Johndrow and Paulo Orenstein and Anirban Bhattacharya", "title": "Bayes Shrinkage at GWAS scale: Convergence and Approximation Theory of a\n  Scalable MCMC Algorithm for the Horseshoe Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The horseshoe prior is frequently employed in Bayesian analysis of\nhigh-dimensional models, and has been shown to achieve minimax optimal risk\nproperties when the truth is sparse. While optimization-based algorithms for\nthe extremely popular Lasso and elastic net procedures can scale to dimension\nin the hundreds of thousands, algorithms for the horseshoe that use Markov\nchain Monte Carlo (MCMC) for computation are limited to problems an order of\nmagnitude smaller. This is due to high computational cost per step and growth\nof the variance of time-averaging estimators as a function of dimension. We\npropose two new MCMC algorithms for computation in these models that have\nimproved performance compared to existing alternatives. One of the algorithms\nalso approximates an expensive matrix product to give orders of magnitude\nspeedup in high-dimensional applications. We prove that the exact algorithm is\ngeometrically ergodic, and give guarantees for the accuracy of the approximate\nalgorithm using perturbation theory. Versions of the approximation algorithm\nthat gradually decrease the approximation error as the chain extends are shown\nto be exact. The scalability of the algorithm is illustrated in simulations\nwith problem size as large as $N=5,000$ observations and $p=50,000$ predictors,\nand an application to a genome-wide association study with $N=2,267$ and\n$p=98,385$. The empirical results also show that the new algorithm yields\nestimates with lower mean squared error, intervals with better coverage, and\nelucidates features of the posterior that were often missed by previous\nalgorithms in high dimensions, including bimodality of posterior marginals\nindicating uncertainty about which covariates belong in the model.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 08:03:29 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 14:49:16 GMT"}, {"version": "v3", "created": "Mon, 15 Oct 2018 05:48:14 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Johndrow", "James E.", ""], ["Orenstein", "Paulo", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1705.00946", "submitter": "Mohammed Sedki", "authors": "Gilles Celeux, Cathy Maugis-Rabusseau, Mohammed Sedki", "title": "Variable selection in model-based clustering and discriminant analysis\n  with a regularization approach", "comments": "Submitted to Advances in Data Analysis and Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relevant methods of variable selection have been proposed in model-based\nclustering and classification. These methods are making use of backward or\nforward procedures to define the roles of the variables. Unfortunately, these\nstepwise procedures are terribly slow and make these variable selection\nalgorithms inefficient to treat large data sets. In this paper, an alternative\nregularization approach of variable selection is proposed for model-based\nclustering and classification. In this approach, the variables are first ranked\nwith a lasso-like procedure in order to avoid painfully slow stepwise\nalgorithms. Thus, the variable selection methodology of Maugis et al (2009b)\ncan be efficiently applied on high-dimensional data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 12:59:38 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Celeux", "Gilles", ""], ["Maugis-Rabusseau", "Cathy", ""], ["Sedki", "Mohammed", ""]]}, {"id": "1705.01024", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu and Jelena Bradic", "title": "A projection pursuit framework for testing general high-dimensional\n  hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a framework for testing general hypothesis in\nhigh-dimensional models where the number of variables may far exceed the number\nof observations. Existing literature has considered less than a handful of\nhypotheses, such as testing individual coordinates of the model parameter.\nHowever, the problem of testing general and complex hypotheses remains widely\nopen. We propose a new inference method developed around the hypothesis\nadaptive projection pursuit framework, which solves the testing problems in the\nmost general case. The proposed inference is centered around a new class of\nestimators defined as $l_1$ projection of the initial guess of the unknown onto\nthe space defined by the null. This projection automatically takes into account\nthe structure of the null hypothesis and allows us to study formal inference\nfor a number of long-standing problems. For example, we can directly conduct\ninference on the sparsity level of the model parameters and the minimum signal\nstrength. This is especially significant given the fact that the former is a\nfundamental condition underlying most of the theoretical development in\nhigh-dimensional statistics, while the latter is a key condition used to\nestablish variable selection properties. Moreover, the proposed method is\nasymptotically exact and has satisfactory power properties for testing very\ngeneral functionals of the high-dimensional parameters. The simulation studies\nlend further support to our theoretical claims and additionally show excellent\nfinite-sample size and power properties of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 15:30:54 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Zhu", "Yinchu", ""], ["Bradic", "Jelena", ""]]}, {"id": "1705.01282", "submitter": "Antonio Parisi", "authors": "Antonio Parisi and Brunero Liseo", "title": "Objective Bayesian analysis for the multivariate skew-t model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a Bayesian analysis of the p-variate skew-t model, providing a new\nparameterization, a set of non-informative priors and a sampler specifically\ndesigned to explore the posterior density of the model parameters. Extensions,\nsuch as the multivariate regression model with skewed errors and the stochastic\nfrontiers model, are easily accommodated. A novelty introduced in the paper is\ngiven by the extension of the bivariate skew-normal model given in Liseo &\nParisi (2013) to a more realistic p-variate skew-t model. We also introduce the\nR package mvst, which allows to estimate the multivariate skew-t model.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 07:29:10 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Parisi", "Antonio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1705.01614", "submitter": "Daniel Bryant", "authors": "Daniel S. Bryant, Ba Tuong Vo, Ba Ngu Vo, Brandon A. Jones", "title": "A Generalized Labeled Multi-Bernoulli Filter with Object Spawning", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2872856", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous labeled random finite set filter developments use a motion model\nthat only accounts for survival and birth. While such a model provides the\nmeans for a multi-object tracking filter such as the Generalized Labeled\nMulti-Bernoulli (GLMB) filter to capture object births and deaths in a wide\nvariety of applications, it lacks the capability to capture spawned tracks and\ntheir lineages. In this paper, we propose a new GLMB based filter that formally\nincorporates spawning, in addition to birth. This formulation enables the joint\nestimation of a spawned object's state and information regarding its lineage.\nSimulations results demonstrate the efficacy of the proposed formulation.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 20:36:24 GMT"}, {"version": "v2", "created": "Wed, 23 Aug 2017 09:04:50 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Bryant", "Daniel S.", ""], ["Vo", "Ba Tuong", ""], ["Vo", "Ba Ngu", ""], ["Jones", "Brandon A.", ""]]}, {"id": "1705.01660", "submitter": "Lykourgos Kekempanos", "authors": "Jeyarajan Thiyagalingam, Lykourgos Kekempanos, Simon Maskell", "title": "MapReduce Particle Filtering with Exact Resampling and Deterministic\n  Runtime", "comments": "31 pages, 16 figures", "journal-ref": null, "doi": "10.1186/s13634-017-0505-9", "report-no": null, "categories": "stat.CO cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle filtering is a numerical Bayesian technique that has great potential\nfor solving sequential estimation problems involving non-linear and\nnon-Gaussian models. Since the estimation accuracy achieved by particle filters\nimproves as the number of particles increases, it is natural to consider as\nmany particles as possible. MapReduce is a generic programming model that makes\nit possible to scale a wide variety of algorithms to Big data. However, despite\nthe application of particle filters across many domains, little attention has\nbeen devoted to implementing particle filters using MapReduce.\n  In this paper, we describe an implementation of a particle filter using\nMapReduce. We focus on a component that what would otherwise be a bottleneck to\nparallel execution, the resampling component. We devise a new implementation of\nthis component, which requires no approximations, has $O\\left(N\\right)$ spatial\ncomplexity and deterministic $O\\left(\\left(\\log N\\right)^2\\right)$ time\ncomplexity. Results demonstrate the utility of this new component and culminate\nin consideration of a particle filter with $2^{24}$ particles being distributed\nacross $512$ processor cores.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 00:05:30 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Thiyagalingam", "Jeyarajan", ""], ["Kekempanos", "Lykourgos", ""], ["Maskell", "Simon", ""]]}, {"id": "1705.01727", "submitter": "Jun Yu", "authors": "Kristi Kuljus, Fekadu L. Bayisa, David Bolin, J\\\"uri Lember, and Jun\n  Yu", "title": "Comparison of hidden Markov chain models and hidden Markov random field\n  models in estimation of computed tomography images", "comments": "17 pages, 5 figures, corresponding author, e-mail:\n  kristi.kuljus@ut.ee", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an interest to replace computed tomography (CT) images with magnetic\nresonance (MR) images for a number of diagnostic and therapeutic workflows. In\nthis article, predicting CT images from a number of magnetic resonance imaging\n(MRI) sequences using regression approach is explored. Two principal areas of\napplication for estimated CT images are dose calculations in MRI-based\nradiotherapy treatment planning and attenuation correction for positron\nemission tomography (PET)/MRI. The main purpose of this work is to investigate\nthe performance of hidden Markov (chain) models (HMMs) in comparison to hidden\nMarkov random field (HMRF) models when predicting CT images of head. Our study\nshows that HMMs have clear advantages over HMRF models in this particular\napplication. Obtained results suggest that HMMs deserve a further study for\ninvestigating their potential in modelling applications where the most natural\ntheoretical choice would be the class of HMRF models.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:03:13 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Kuljus", "Kristi", ""], ["Bayisa", "Fekadu L.", ""], ["Bolin", "David", ""], ["Lember", "J\u00fcri", ""], ["Yu", "Jun", ""]]}, {"id": "1705.01795", "submitter": "Eduardo Calvo", "authors": "Eduardo Calvo", "title": "An\\'alisis econom\\'etrico de series temporales en Gretl: La Ley de Okun", "comments": "in French. JEL Classification: C22, C51, E24", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Gretl is an econometrics package, including a shared library, a command-line\nclient program and a graphical user interface which offers an intuitive user\ninterface. This paper explains the Okun's Law, which is an empirically observed\nrelationship between unemployment and losses in a country's production, with\nspanish data processed in Gretl.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 11:01:50 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Calvo", "Eduardo", ""]]}, {"id": "1705.02003", "submitter": "Marta D'Elia", "authors": "Marta D'Elia, Eric Phipps, Ahmad Rushdi, Mohamed Ebeida", "title": "Surrogate-based Ensemble Grouping Strategies for Embedded Sampling-based\n  Uncertainty Quantification", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": "Sandia National Laboratories Tech Report SAND2017-4829", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The embedded ensemble propagation approach introduced in [49] has been\ndemonstrated to be a powerful means of reducing the computational cost of\nsampling-based uncertainty quantification methods, particularly on emerging\ncomputational architectures. A substantial challenge with this method however\nis ensemble-divergence, whereby different samples within an ensemble choose\ndifferent code paths. This can reduce the effectiveness of the method and\nincrease computational cost. Therefore grouping samples together to minimize\nthis divergence is paramount in making the method effective for challenging\ncomputational simulations. In this work, a new grouping approach based on a\nsurrogate for computational cost built up during the uncertainty propagation is\ndeveloped and applied to model diffusion problems where computational cost is\ndriven by the number of (preconditioned) linear solver iterations. The approach\nis developed within the context of locally adaptive stochastic collocation\nmethods, where a surrogate for the number of linear solver iterations,\ngenerated from previous levels of the adaptive grid generation, is used to\npredict iterations for subsequent samples, and group them based on similar\nnumbers of iterations. The effectiveness of the method is demonstrated by\napplying it to highly anisotropic diffusion problems with a wide variation in\nsolver iterations from sample to sample. It extends the parameter-based\ngrouping approach developed in [17] to more general problems without requiring\ndetailed knowledge of how the uncertain parameters affect the simulation's\ncost, and is also less intrusive to the simulation code.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 20:23:59 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["D'Elia", "Marta", ""], ["Phipps", "Eric", ""], ["Rushdi", "Ahmad", ""], ["Ebeida", "Mohamed", ""]]}, {"id": "1705.02679", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak, Linglong Kong", "title": "Nonasymptotic estimation and support recovery for high dimensional\n  sparse covariance matrices", "comments": "33 pages, 3 figures, 6 tables", "journal-ref": "Stat (2020) e316", "doi": "10.1002/sta4.316", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for nonasymptotic covariance matrix estimation\nmaking use of concentration inequality-based confidence sets. We specify this\nframework for the estimation of large sparse covariance matrices through\nincorporation of past thresholding estimators with key emphasis on support\nrecovery. This technique goes beyond past results for thresholding estimators\nby allowing for a wide range of distributional assumptions beyond merely\nsub-Gaussian tails. This methodology can furthermore be adapted to a wide range\nof other estimators and settings. The usage of nonasymptotic dimension-free\nconfidence sets yields good theoretical performance. Through extensive\nsimulations, it is demonstrated to have superior performance when compared with\nother such methods. In the context of support recovery, we are able to specify\na false positive rate and optimize to maximize the true recoveries.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 18:47:49 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 23:05:09 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 15:56:34 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""], ["Kong", "Linglong", ""]]}, {"id": "1705.02891", "submitter": "Alessandro Barp", "authors": "Alessandro Barp, Francois-Xavier Briol, Anthony D. Kennedy, Mark\n  Girolami", "title": "Geometry and Dynamics for Markov Chain Monte Carlo", "comments": "Submitted to \"Annual Review of Statistics and Its Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG hep-lat math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo methods have revolutionised mathematical computation\nand enabled statistical inference within many previously intractable models. In\nthis context, Hamiltonian dynamics have been proposed as an efficient way of\nbuilding chains which can explore probability densities efficiently. The method\nemerges from physics and geometry and these links have been extensively studied\nby a series of authors through the last thirty years. However, there is\ncurrently a gap between the intuitions and knowledge of users of the\nmethodology and our deep understanding of these theoretical foundations. The\naim of this review is to provide a comprehensive introduction to the geometric\ntools used in Hamiltonian Monte Carlo at a level accessible to statisticians,\nmachine learners and other users of the methodology with only a basic\nunderstanding of Monte Carlo methods. This will be complemented with some\ndiscussion of the most recent advances in the field which we believe will\nbecome increasingly relevant to applied scientists.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:19:53 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Barp", "Alessandro", ""], ["Briol", "Francois-Xavier", ""], ["Kennedy", "Anthony D.", ""], ["Girolami", "Mark", ""]]}, {"id": "1705.03130", "submitter": "Paul McNicholas", "authors": "Yang Tang, Ryan P. Browne and Paul D. McNicholas", "title": "Flexible Clustering for High-Dimensional Data via Mixtures of Joint\n  Generalized Hyperbolic Models", "comments": null, "journal-ref": null, "doi": "10.1002/sta4.177", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of joint generalized hyperbolic distributions (MJGHD) is introduced\nfor asymmetric clustering for high-dimensional data. The MJGHD approach takes\ninto account the cluster-specific subspace, thereby limiting the number of\nparameters to estimate while also facilitating visualization of results.\nIdentifiability is discussed, and a multi-cycle ECM algorithm is outlined for\nparameter estimation. The MJGHD approach is illustrated on two real data sets,\nwhere the Bayesian information criterion is used for model selection.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:25:32 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 18:04:08 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Tang", "Yang", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1705.03134", "submitter": "Paul McNicholas", "authors": "Yang Tang and Paul D. McNicholas", "title": "Clustering Airbnb Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, online customer reviews increasingly exert influence on\nconsumers' decision when booking accommodation online. The renewal importance\nto the concept of word-of mouth is reflected in the growing interests in\ninvestigating consumers' experience by analyzing their online reviews through\nthe process of text mining and sentiment analysis. A clustering approach is\ndeveloped for Boston Airbnb reviews submitted in the English language and\ncollected from 2009 to 2016. This approach is based on a mixture of latent\nvariable models, which provides an appealing framework for handling clustered\nbinary data. We address here the problem of discovering meaningful segments of\nconsumers that are coherent from both the underlying topics and the sentiment\nbehind the reviews. A penalized mixture of latent traits approach is developed\nto reduce the number of parameters and identify variables that are not\ninformative for clustering. The introduction of component-specific rate\nparameters avoids the over-penalization that can occur when inferring a shared\nrate parameter on clustered data. We divided the guests into four groups --\nproperty driven guests, host driven guests, guests with recent overall negative\nstay and guests with some negative experiences.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:53:36 GMT"}, {"version": "v2", "created": "Tue, 4 Jul 2017 00:13:54 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 23:34:08 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1705.03196", "submitter": "Zdravko Botev", "authors": "Zdravko Botev and Robert Salomone and Daniel MacKinlay", "title": "Accurate Computation of the Distribution of Sums of Dependent\n  Log-Normals with Applications to the Black-Scholes Model", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new Monte Carlo methodology for the accurate estimation of the\ndistribution of the sum of dependent log-normal random variables. The\nmethodology delivers statistically unbiased estimators for three distributional\nquantities of significant interest in finance and risk management: the left\ntail, or cumulative distribution function, the probability density function,\nand the right tail, or complementary distribution function of the sum of\ndependent log-normal factors. In all of these three cases our methodology\ndelivers fast and highly accurate estimators in settings for which existing\nmethodology delivers estimators with large variance that tend to underestimate\nthe true quantity of interest. We provide insight into the computational\nchallenges using theory and numerical experiments, and explain their much wider\nimplications for Monte Carlo statistical estimators of rare-event\nprobabilities. In particular, we find that theoretically strongly-efficient\nestimators should be used with great caution in practice, because they may\nyield inaccurate results in the pre-limit. Further, this inaccuracy may not be\ndetectable from the output of the Monte Carlo simulation, because the\nsimulation output may severely underestimate the true variance of the\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 06:22:27 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 07:10:26 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Botev", "Zdravko", ""], ["Salomone", "Robert", ""], ["MacKinlay", "Daniel", ""]]}, {"id": "1705.03809", "submitter": "Simon Wessing", "authors": "Simon Wessing", "title": "Experimental Analysis of a Generalized Stratified Sampling Algorithm for\n  Hypercubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratified sampling is a fast and simple method to generate point sets with\nuniform distribution in hypercubes. However, for the most common paraxial\nstratfication it has the prominent drawback that the number of sampled points\nin n dimensions has to be an n-th power of an integer number. This exponential\ngrowth makes its application unattractive or even infeasible in high\ndimensions. We present a stratification procedure that eliminates this problem\nby a recursive binary partitioning of the hypercube. The algorithm runs in\nlinear time and tries to minimize the hyperboxes' deviation from the cubic\nshape. We analyze the properties of the algorithm using discrepancy and\ncovering radius, and directly in practical applications, comparing it to\nquasirandom and other sampling methods. We also discuss a potential combination\nwith Latin hypercube sampling, which positively affects discrepancy.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:08:04 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 15:30:47 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Wessing", "Simon", ""]]}, {"id": "1705.03831", "submitter": "Youhan Fang", "authors": "Youhan Fang and Yudong Cao and Robert D. Skeel", "title": "Quasi-Reliable Estimates of Effective Sample Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The efficiency of a Markov chain Monte Carlo algorithm might be measured by\nthe cost of generating one independent sample, or equivalently, the total cost\ndivided by the effective sample size, defined in terms of the integrated\nautocorrelation time. To ensure the reliability of such an estimate, it is\nsuggested that there be an adequate sampling of state space--- to the extent\nthat this can be determined from the available samples. A possible method for\ndoing this is derived and evaluated.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:50:41 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 12:57:17 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Fang", "Youhan", ""], ["Cao", "Yudong", ""], ["Skeel", "Robert D.", ""]]}, {"id": "1705.03864", "submitter": "Daniele Durante", "authors": "Daniele Durante, Antonio Canale and Tommaso Rigon", "title": "A nested expectation-maximization algorithm for latent class models with\n  covariates", "comments": null, "journal-ref": "Statistics & Probability Letters (2019). 146, 97-103", "doi": "10.1016/j.spl.2018.10.015", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a nested EM routine for latent class models with covariates which\nallows maximization of the full-model log-likelihood and, differently from\ncurrent methods, guarantees monotone log-likelihood sequences along with\nimproved convergence rates.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:29:10 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 15:27:13 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:19:16 GMT"}, {"version": "v4", "created": "Tue, 16 Jan 2018 17:58:27 GMT"}, {"version": "v5", "created": "Thu, 2 Aug 2018 14:37:31 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""], ["Canale", "Antonio", ""], ["Rigon", "Tommaso", ""]]}, {"id": "1705.03944", "submitter": "Bruno Sudret", "authors": "E. Burnaev and I. Panin and B. Sudret", "title": "Efficient design of experiments for sensitivity analysis based on\n  polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-005", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis aims at quantifying respective effects of input\nrandom variables (or combinations thereof) onto variance of a physical or\nmathematical model response. Among the abundant literature on sensitivity\nmeasures, Sobol' indices have received much attention since they provide\naccurate information for most of models. We consider a problem of experimental\ndesign points selection for Sobol' indices estimation. Based on the concept of\n$D$-optimality, we propose a method for constructing an adaptive design of\nexperiments, effective for calculation of Sobol' indices based on Polynomial\nChaos Expansions. We provide a set of applications that demonstrate the\nefficiency of the proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 20:19:05 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Burnaev", "E.", ""], ["Panin", "I.", ""], ["Sudret", "B.", ""]]}, {"id": "1705.03947", "submitter": "Bruno Sudret", "authors": "R. Sch\\\"obi and B. Sudret", "title": "Structural reliability analysis for p-boxes using multi-level\n  meta-models", "comments": null, "journal-ref": null, "doi": "10.1016/j.probengmech.2017.04.001", "report-no": "RSUQ-2017-006", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern engineering, computer simulations are a popular tool to analyse,\ndesign, and optimize systems. Furthermore, concepts of uncertainty and the\nrelated reliability analysis and robust design are of increasing importance.\nHence, an efficient quantification of uncertainty is an important aspect of the\nengineer's workflow. In this context, the characterization of uncertainty in\nthe input variables is crucial. In this paper, input variables are modelled by\nprobability-boxes, which account for both aleatory and epistemic uncertainty.\nTwo types of probability-boxes are distinguished: free and parametric (also\ncalled distributional) p-boxes. The use of probability-boxes generally\nincreases the complexity of structural reliability analyses compared to\ntraditional probabilistic input models. In this paper, the complexity is\nhandled by two-level approaches which use Kriging meta-models with adaptive\nexperimental designs at different levels of the structural reliability\nanalysis. For both types of probability-boxes, the extensive use of meta-models\nallows for an efficient estimation of the failure probability at a limited\nnumber of runs of the performance function. The capabilities of the proposed\napproaches are illustrated through a benchmark analytical function and two\nrealistic engineering problems.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 20:32:40 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Sch\u00f6bi", "R.", ""], ["Sudret", "B.", ""]]}, {"id": "1705.04219", "submitter": "Pierre Carmier Ph.D.", "authors": "Pierre Carmier, Olexiy Kyrgyzov, Paul-Henry Courn\\`ede", "title": "A critical analysis of resampling strategies for the regularized\n  particle filter", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the performance of different resampling strategies for the\nregularized particle filter regarding parameter estimation. We show in\nparticular, building on analytical insight obtained in the linear Gaussian\ncase, that resampling systematically can prevent the filtered density from\nconverging towards the true posterior distribution. We discuss several means to\novercome this limitation, including kernel bandwidth modulation, and provide\nevidence that the resulting particle filter clearly outperforms traditional\nbootstrap particle filters. Our results are supported by numerical simulations\non a linear textbook example, the logistic map and a non-linear plant growth\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 14:47:30 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Carmier", "Pierre", ""], ["Kyrgyzov", "Olexiy", ""], ["Courn\u00e8de", "Paul-Henry", ""]]}, {"id": "1705.04374", "submitter": "Jonas \\v{S}ukys", "authors": "Jonas \\v{S}ukys, Ursula Rasthofer, Fabian Wermelinger, Panagiotis\n  Hadjidoukas, and Petros Koumoutsakos", "title": "Optimal fidelity multi-level Monte Carlo for quantification of\n  uncertainty in simulations of cloud cavitation collapse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.DC math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify uncertainties in the location and magnitude of extreme pressure\nspots revealed from large scale multi-phase flow simulations of cloud\ncavitation collapse. We examine clouds containing 500 cavities and quantify\nuncertainties related to their initial spatial arrangement. The resulting\n2000-dimensional space is sampled using a non-intrusive and computationally\nefficient Multi-Level Monte Carlo (MLMC) methodology. We introduce novel\noptimal control variate coefficients to enhance the variance reduction in MLMC.\nThe proposed optimal fidelity MLMC leads to more than two orders of magnitude\nspeedup when compared to standard Monte Carlo methods. We identify large\nuncertainties in the location and magnitude of the peak pressure pulse and\npresent its statistical correlations and joint probability density functions\nwith the geometrical characteristics of the cloud. Characteristic properties of\nspatial cloud structure are identified as potential causes of significant\nuncertainties in exerted collapse pressures.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 20:54:51 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["\u0160ukys", "Jonas", ""], ["Rasthofer", "Ursula", ""], ["Wermelinger", "Fabian", ""], ["Hadjidoukas", "Panagiotis", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1705.04453", "submitter": "Karl Breitung", "authors": "Karl Breitung", "title": "The Geometry of Limit State Function Graphs and Subset Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last fifteen the subset sampling method has often been used in\nreliability problems as a tool for calculating small probabilities. This method\nis extrapolating from an initial Monte Carlo estimate for the probability\ncontent of a failure domain found by a suitable higher level of the original\nlimit state function. Then iteratively conditional probabilities are estimated\nfor failures domains decreasing to the original failure domain.\n  But there are assumptions not immediately obvious about the structure of the\nfailure domains which must be fulfilled that the method works properly. Here\nexamples are studied that show that at least in some cases if these premises\nare not fulfilled, inaccurate results may be obtained. For the further\ndevelopment of the subset sampling method it is certainly desirable to find\napproaches where it is possible to check that these implicit assumptions are\nnot violated. Also it would be probably important to develop further\nimprovements of the concept to get rid of these limitations.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 07:26:09 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Breitung", "Karl", ""]]}, {"id": "1705.04579", "submitter": "George Deligiannidis", "authors": "George Deligiannidis, Alexandre Bouchard-C\\^ot\\'e and Arnaud Doucet", "title": "Exponential Ergodicity of the Bouncy Particle Sampler", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-reversible Markov chain Monte Carlo schemes based on piecewise\ndeterministic Markov processes have been recently introduced in applied\nprobability, automatic control, physics and statistics. Although these\nalgorithms demonstrate experimentally good performance and are accordingly\nincreasingly used in a wide range of applications, geometric ergodicity results\nfor such schemes have only been established so far under very restrictive\nassumptions. We give here verifiable conditions on the target distribution\nunder which the Bouncy Particle Sampler algorithm introduced in \\cite{P_dW_12}\nis geometrically ergodic. This holds whenever the target satisfies a curvature\ncondition and has tails decaying at least as fast as an exponential and at most\nas fast as a Gaussian distribution. This allows us to provide a central limit\ntheorem for the associated ergodic averages. When the target has tails thinner\nthan a Gaussian distribution, we propose an original modification of this\nscheme that is geometrically ergodic. For thick-tailed target distributions,\nsuch as $t$-distributions, we extend the idea pioneered in \\cite{J_G_12} in a\nrandom walk Metropolis context. We apply a change of variable to obtain a\ntransformed target satisfying the tail conditions for geometric ergodicity. By\nsampling the transformed target using the Bouncy Particle Sampler and mapping\nback the Markov process to the original parameterization, we obtain a\ngeometrically ergodic algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 14:10:07 GMT"}, {"version": "v2", "created": "Sun, 27 Aug 2017 08:41:35 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Deligiannidis", "George", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1705.04584", "submitter": "Haiming Zhou", "authors": "Haiming Zhou, Timothy Hanson, Jiajia Zhang", "title": "spBayesSurv: Fitting Bayesian Spatial Survival Models Using R", "comments": "arXiv admin note: text overlap with arXiv:1701.06976", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial survival analysis has received a great deal of attention over the\nlast 20 years due to the important role that geographical information can play\nin predicting survival. This paper provides an introduction to a set of\nprograms for implementing some Bayesian spatial survival models in R using the\npackage spBayesSurv. The function survregbayes includes the three most\ncommonly-used semiparametric models: proportional hazards, proportional odds,\nand accelerated failure time. All manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated. Time-dependent covariates are allowed under the\npiecewise constant assumption. Both georeferenced and areally observed spatial\nlocations are handled via frailties. Model fit is assessed with conditional\nCox-Snell residual plots, and model choice is carried out via the log pseudo\nmarginal likelihood, the deviance information criterion and the Watanabe-Akaike\ninformation criterion. The accelerated failure time frailty model with a\ncovariate-dependent baseline is included in the function frailtyGAFT. In\naddition, the package also provides two marginal survival models: proportional\nhazards and linear dependent Dirichlet process mixture, where the spatial\ndependence is modeled via spatial copulas. Note that the package can also\nhandle non-spatial data using non-spatial versions of aforementioned models.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 19:24:03 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:07:15 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 16:36:00 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Zhou", "Haiming", ""], ["Hanson", "Timothy", ""], ["Zhang", "Jiajia", ""]]}, {"id": "1705.04651", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Geoffrey J. McLachlan", "title": "Iteratively-Reweighted Least-Squares Fitting of Support Vector Machines:\n  A Majorization--Minimization Algorithm Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector machines (SVMs) are an important tool in modern data analysis.\nTraditionally, support vector machines have been fitted via quadratic\nprogramming, either using purpose-built or off-the-shelf algorithms. We present\nan alternative approach to SVM fitting via the majorization--minimization (MM)\nparadigm. Algorithms that are derived via MM algorithm constructions can be\nshown to monotonically decrease their objectives at each iteration, as well as\nbe globally convergent to stationary points. We demonstrate the construction of\niteratively-reweighted least-squares (IRLS) algorithms, via the MM paradigm,\nfor SVM risk minimization problems involving the hinge, least-square,\nsquared-hinge, and logistic losses, and 1-norm, 2-norm, and elastic net\npenalizations. Successful implementations of our algorithms are presented via\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 16:44:03 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1705.04661", "submitter": "Lucas Lacasa", "authors": "Lucas Lacasa, In\\'es P. Mari\\~no, Joaqu\\'in Miguez, Vincenzo Nicosia,\n  Edgar Rold\\'an, Ana Lisica, Stephan W. Grill, Jes\\'us G\\'omez-Garde\\~nes", "title": "Multiplex decomposition of non-Markovian dynamics and the hidden layer\n  reconstruction problem", "comments": "40 pages, 24 figures", "journal-ref": "Phys. Rev. X 8, 031038 (2018)", "doi": "10.1103/PhysRevX.8.031038", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elements composing complex systems usually interact in several different ways\nand as such the interaction architecture is well modelled by a multiplex\nnetwork. However often this architecture is hidden, as one usually only has\nexperimental access to an aggregated projection. A fundamental challenge is\nthus to determine whether the hidden underlying architecture of complex systems\nis better modelled as a single interaction layer or results from the\naggregation and interplay of multiple layers. Here we show that using local\ninformation provided by a random walker navigating the aggregated network one\ncan decide in a robust way if the underlying structure is a multiplex or not\nand, in the former case, to determine the most probable number of hidden\nlayers. As a byproduct, we show that the mathematical formalism also provides a\nprincipled solution for the optimal decomposition and projection of complex,\nnon-Markovian dynamics into a Markov switching combination of diffusive modes.\nWe validate the proposed methodology with numerical simulations of both (i)\nrandom walks navigating hidden multiplex networks (thereby reconstructing the\ntrue hidden architecture) and (ii) Markovian and non-Markovian continuous\nstochastic processes (thereby reconstructing an effective multiplex\ndecomposition where each layer accounts for a different diffusive mode). We\nalso state and prove two existence theorems guaranteeing that an exact\nreconstruction of the dynamics in terms of these hidden jump-Markov models is\nalways possible for arbitrary finite-order Markovian and fully non-Markovian\nprocesses. Finally, we showcase the applicability of the method to experimental\nrecordings from (i) the mobility dynamics of human players in an online\nmultiplayer game and (ii) the dynamics of RNA polymerases at the\nsingle-molecule level.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:22:13 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 10:16:00 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Lacasa", "Lucas", ""], ["Mari\u00f1o", "In\u00e9s P.", ""], ["Miguez", "Joaqu\u00edn", ""], ["Nicosia", "Vincenzo", ""], ["Rold\u00e1n", "Edgar", ""], ["Lisica", "Ana", ""], ["Grill", "Stephan W.", ""], ["G\u00f3mez-Garde\u00f1es", "Jes\u00fas", ""]]}, {"id": "1705.04678", "submitter": "Nikhil Galagali", "authors": "Nikhil Galagali and Youssef M. Marzouk", "title": "Exploiting network topology for large-scale inference of nonlinear\n  reaction models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of chemical reaction models aids understanding and prediction\nin areas ranging from biology to electrochemistry and combustion. A systematic\napproach to building reaction network models uses observational data not only\nto estimate unknown parameters, but also to learn model structure. Bayesian\ninference provides a natural approach to this data-driven construction of\nmodels. Yet traditional Bayesian model inference methodologies that numerically\nevaluate the evidence for each model are often infeasible for nonlinear\nreaction network inference, as the number of plausible models can be\ncombinatorially large. Alternative approaches based on model-space sampling can\nenable large-scale network inference, but their realization presents many\nchallenges. In this paper, we present new computational methods that make\nlarge-scale nonlinear network inference tractable. First, we exploit the\ntopology of networks describing potential interactions among chemical species\nto design improved \"between-model\" proposals for reversible-jump Markov chain\nMonte Carlo. Second, we introduce a sensitivity-based determination of move\ntypes which, when combined with network-aware proposals, yields significant\nadditional gains in sampling performance. These algorithms are demonstrated on\ninference problems drawn from systems biology, with nonlinear differential\nequation models of species interactions.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:55:44 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 18:35:07 GMT"}, {"version": "v3", "created": "Sun, 14 Oct 2018 16:11:46 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 01:26:55 GMT"}, {"version": "v5", "created": "Sat, 19 Jan 2019 03:43:48 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Galagali", "Nikhil", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1705.04756", "submitter": "Peter DeWitt", "authors": "Peter E. DeWitt, Samantha MaWhinney, Nichole E. Carlson", "title": "cpr: An R Package For Finding Parsimonious B-Spline Regression Models\n  via Control Polygon Reduction and Control Net Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package cpr provides tools for selection of parsimonious B-spline\nregression models via algorithms coined `control polygon reduction' (CPR) and\n`control net reduction' (CNR). B-Splines are commonly used in regression models\nto smooth data and approximate unknown functional forms. B-Splines are defined\nby a polynomial order and a knot sequence. Defining the knot sequence is\nnon-trivial, but is critical with respect to the quality of the regression\nmodels. The focus of the CPR and CNR algorithms is to reduce a large knot\nsequence down to a parsimonious collection of elements while maintaining a high\nquality of fit. The algorithms are quick to implement and are flexible enough\nto support many types of data and regression approaches. The cpr package\nprovides the end user collections of tools for the construction of B-spline\nbasis matrices, construction of control polygons and control nets, and the use\nof diagnostics of the CPR and CNR algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 21:32:38 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["DeWitt", "Peter E.", ""], ["MaWhinney", "Samantha", ""], ["Carlson", "Nichole E.", ""]]}, {"id": "1705.04768", "submitter": "Ryan Tibshirani", "authors": "Ryan J. Tibshirani", "title": "Dykstra's Algorithm, ADMM, and Coordinate Descent: Connections,\n  Insights, and Extensions", "comments": "8 pages, 13 pages of appendix, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study connections between Dykstra's algorithm for projecting onto an\nintersection of convex sets, the augmented Lagrangian method of multipliers or\nADMM, and block coordinate descent. We prove that coordinate descent for a\nregularized regression problem, in which the (separable) penalty functions are\nseminorms, is exactly equivalent to Dykstra's algorithm applied to the dual\nproblem. ADMM on the dual problem is also seen to be equivalent, in the special\ncase of two sets, with one being a linear subspace. These connections, aside\nfrom being interesting in their own right, suggest new ways of analyzing and\nextending coordinate descent. For example, from existing convergence theory on\nDykstra's algorithm over polyhedra, we discern that coordinate descent for the\nlasso problem converges at an (asymptotically) linear rate. We also develop two\nparallel versions of coordinate descent, based on the Dykstra and ADMM\nconnections.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 23:18:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "1705.06565", "submitter": "Kristin Kirchner", "authors": "David Bolin, Kristin Kirchner, Mih\\'aly Kov\\'acs", "title": "Numerical solution of fractional elliptic stochastic PDEs with spatial\n  white noise", "comments": "21 pages, 1 figure", "journal-ref": "IMA J. Numer. Anal. (2018)", "doi": "10.1093/imanum/dry091", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The numerical approximation of solutions to stochastic partial differential\nequations with additive spatial white noise on bounded domains in\n$\\mathbb{R}^d$ is considered. The differential operator is given by the\nfractional power $L^\\beta$, $\\beta\\in(0,1)$, of an integer order elliptic\ndifferential operator $L$ and is therefore non-local. Its inverse $L^{-\\beta}$\nis represented by a Bochner integral from the Dunford-Taylor functional\ncalculus. By applying a quadrature formula to this integral representation, the\ninverse fractional power operator $L^{-\\beta}$ is approximated by a weighted\nsum of non-fractional resolvents $(I + t_j^2 L)^{-1}$ at certain quadrature\nnodes $t_j>0$. The resolvents are then discretized in space by a standard\nfinite element method.\n  This approach is combined with an approximation of the white noise, which is\nbased only on the mass matrix of the finite element discretization. In this\nway, an efficient numerical algorithm for computing samples of the approximate\nsolution is obtained. For the resulting approximation, the strong mean-square\nerror is analyzed and an explicit rate of convergence is derived. Numerical\nexperiments for $L=\\kappa^2-\\Delta$, $\\kappa > 0$, with homogeneous Dirichlet\nboundary conditions on the unit cube $(0,1)^d$ in $d=1,2,3$ spatial dimensions\nfor varying $\\beta\\in(0,1)$ attest the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:06:44 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 13:55:03 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Bolin", "David", ""], ["Kirchner", "Kristin", ""], ["Kov\u00e1cs", "Mih\u00e1ly", ""]]}, {"id": "1705.06567", "submitter": "Krzysztof Bisewski", "authors": "Krzysztof Bisewski, Daan Crommelin and Michel Mandjes", "title": "Controlling the time discretization bias for the supremum of Brownian\n  Motion", "comments": "30 pages", "journal-ref": "ACM Transactions on Modeling and Computer Simulation (TOMACS),\n  28(3):24 (2018)", "doi": "10.1145/3177775", "report-no": null, "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bias arising from time discretization when estimating the\nthreshold crossing probability $w(b) := \\mathbb{P}(\\sup_{t\\in[0,1]} B_t > b)$,\nwith $(B_t)_{t\\in[0,1]}$ a standard Brownian Motion. We prove that if the\ndiscretization is equidistant, then to reach a given target value of the\nrelative bias, the number of grid points has to grow quadratically in $b$, as\n$b$ grows. When considering non-equidistant discretizations (with\nthreshold-dependent grid points), we can substantially improve on this: we show\nthat for such grids the required number of grid points is independent of $b$,\nand in addition we point out how they can be used to construct a strongly\nefficient algorithm for the estimation of $w(b)$. Finally, we show how to apply\nthe resulting algorithm for a broad class of stochastic processes; it is\nempirically shown that the threshold-dependent grid significantly outperforms\nits equidistant counterpart.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:10:17 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 12:29:17 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bisewski", "Krzysztof", ""], ["Crommelin", "Daan", ""], ["Mandjes", "Michel", ""]]}, {"id": "1705.06916", "submitter": "Julian Taylor", "authors": "Julian Taylor, David Butler", "title": "R Package ASMap: Efficient Genetic Linkage Map Construction and\n  Diagnosis", "comments": "Conditionally accepted for publication in Journal of Statistical\n  Software", "journal-ref": null, "doi": "10.18637/jss.v079.i06", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although various forms of linkage map construction software are widely\navailable, there is a distinct lack of packages for use in the R statistical\ncomputing environment. This article introduces the ASMap linkage map\nconstruction R package which contains functions that use the efficient MSTmap\nalgorithm for clustering and optimally ordering large sets of markers.\nAdditional to the construction functions, the package also contains a suite of\ntools to assist in the rapid diagnosis and repair of a constructed linkage map.\nThe package functions can also be used for post linkage map construction\ntechniques such as fine mapping or combining maps of the same population. To\nshowcase the efficiency and functionality of ASMap, the complete linkage map\nconstruction process is demonstrated with a high density barley backcross\nmarker data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:55:03 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Taylor", "Julian", ""], ["Butler", "David", ""]]}, {"id": "1705.07194", "submitter": "Brendan Ames", "authors": "Summer Atkins, Gudmundur Einarsson, Brendan Ames, Line Clemmensen", "title": "Proximal Methods for Sparse Optimal Scoring and Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear discriminant analysis (LDA) is a classical method for dimensionality\nreduction, where discriminant vectors are sought to project data to a lower\ndimensional space for optimal separability of classes. Several recent papers\nhave outlined strategies for exploiting sparsity for using LDA with\nhigh-dimensional data. However, many lack scalable methods for solution of the\nunderlying optimization problems. We propose three new numerical optimization\nschemes for solving the sparse optimal scoring formulation of LDA based on\nblock coordinate descent, the proximal gradient method, and the alternating\ndirection method of multipliers. We show that the per-iteration cost of these\nmethods scales linearly in the dimension of the data provided restricted\nregularization terms are employed, and cubically in the dimension of the data\nin the worst case. Furthermore, we establish that if our block coordinate\ndescent framework generates convergent subsequences of iterates, then these\nsubsequences converge to the stationary points of the sparse optimal scoring\nproblem. We demonstrate the effectiveness of our new methods with empirical\nresults for classification of Gaussian data and data sets drawn from\nbenchmarking repositories, including time-series and multispectral X-ray data,\nand provide Matlab and R implementations of our optimization schemes.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 21:24:47 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 16:57:11 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 22:19:37 GMT"}, {"version": "v4", "created": "Fri, 11 Jun 2021 15:48:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Atkins", "Summer", ""], ["Einarsson", "Gudmundur", ""], ["Ames", "Brendan", ""], ["Clemmensen", "Line", ""]]}, {"id": "1705.07382", "submitter": "Nicolas Garcia Trillos", "authors": "Nicolas Garcia Trillos and Daniel Sanz-Alonso", "title": "The Bayesian update: variational formulations and gradient flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian update can be viewed as a variational problem by characterizing\nthe posterior as the minimizer of a functional. The variational viewpoint is\nfar from new and is at the heart of popular methods for posterior\napproximation. However, some of its consequences seem largely unexplored. We\nfocus on the following one: defining the posterior as the minimizer of a\nfunctional gives a natural path towards the posterior by moving in the\ndirection of steepest descent of the functional. This idea is made precise\nthrough the theory of gradient flows, allowing to bring new tools to the study\nof Bayesian models and algorithms. Since the posterior may be characterized as\nthe minimizer of different functionals, several variational formulations may be\nconsidered. We study three of them and their three associated gradient flows.\nWe show that, in all cases, the rate of convergence of the flows to the\nposterior can be bounded by the geodesic convexity of the functional to be\nminimized. Each gradient flow naturally suggests a nonlinear diffusion with the\nposterior as invariant distribution. These diffusions may be discretized to\nbuild proposals for Markov chain Monte Carlo (MCMC) algorithms. By\nconstruction, the diffusions are guaranteed to satisfy a certain optimality\ncondition, and rates of convergence are given by the convexity of the\nfunctionals. We use this observation to propose a criterion for the choice of\nmetric in Riemannian MCMC methods.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 02:44:07 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 02:55:57 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "1705.07443", "submitter": "Matthew Staib", "authors": "Matthew Staib, Sebastian Claici, Justin Solomon, Stefanie Jegelka", "title": "Parallel Streaming Wasserstein Barycenters", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficiently aggregating data from different sources is a challenging problem,\nparticularly when samples from each source are distributed differently. These\ndifferences can be inherent to the inference task or present for other reasons:\nsensors in a sensor network may be placed far apart, affecting their individual\nmeasurements. Conversely, it is computationally advantageous to split Bayesian\ninference tasks across subsets of data, but data need not be identically\ndistributed across subsets. One principled way to fuse probability\ndistributions is via the lens of optimal transport: the Wasserstein barycenter\nis a single distribution that summarizes a collection of input measures while\nrespecting their geometry. However, computing the barycenter scales poorly and\nrequires discretization of all input distributions and the barycenter itself.\nImproving on this situation, we present a scalable, communication-efficient,\nparallel algorithm for computing the Wasserstein barycenter of arbitrary\ndistributions. Our algorithm can operate directly on continuous input\ndistributions and is optimized for streaming data. Our method is even robust to\nnonstationary input distributions and produces a barycenter estimate that\ntracks the input measures over time. The algorithm is semi-discrete, needing to\ndiscretize only the barycenter estimate. To the best of our knowledge, we also\nprovide the first bounds on the quality of the approximate barycenter as the\ndiscretization becomes finer. Finally, we demonstrate the practical\neffectiveness of our method, both in tracking moving distributions on a sphere,\nas well as in a large-scale Bayesian inference task.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 12:24:27 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 04:19:11 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Staib", "Matthew", ""], ["Claici", "Sebastian", ""], ["Solomon", "Justin", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1705.07598", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Giorgio M. Vitetta and Emilio Sirignano and Francesco Montorsi", "title": "Rao-Blackwellized Particle Smoothing as Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript the fixed-lag smoothing problem for conditionally linear\nGaussian state-space models is investigated from a factor graph perspective.\nMore specifically, after formulating Bayesian smoothing for an arbitrary\nstate-space model as forward-backward message passing over a factor graph, we\nfocus on the above mentioned class of models and derive a novel\nRao-Blackwellized particle smoother for it. Then, we show how our technique can\nbe modified to estimate a point mass approximation of the so called joint\nsmoothing distribution. Finally, the estimation accuracy and the computational\nrequirements of our smoothing algorithms are analysed for a specific\nstate-space model.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:06:03 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Vitetta", "Giorgio M.", ""], ["Sirignano", "Emilio", ""], ["Montorsi", "Francesco", ""]]}, {"id": "1705.07616", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik, Florian Frommlet", "title": "A novel algorithmic approach to Bayesian Logic Regression", "comments": "19 pages, 10 tables", "journal-ref": "Bayesian Analysis, Volume 15, Number 1 (2020)", "doi": "10.1214/18-BA1141", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic regression was developed more than a decade ago as a tool to construct\npredictors from Boolean combinations of binary covariates. It has been mainly\nused to model epistatic effects in genetic association studies, which is very\nappealing due to the intuitive interpretation of logic expressions to describe\nthe interaction between genetic variations. Nevertheless logic regression has\n(partly due to computational challenges) remained less well known than other\napproaches to epistatic association mapping. Here we will adapt an advanced\nevolutionary algorithm called GMJMCMC (Genetically modified Mode Jumping Markov\nChain Monte Carlo) to perform Bayesian model selection in the space of logic\nregression models. After describing the algorithmic details of GMJMCMC we\nperform a comprehensive simulation study that illustrates its performance given\nlogic regression terms of various complexity. Specifically GMJMCMC is shown to\nbe able to identify three-way and even four-way interactions with relatively\nlarge power, a level of complexity which has not been achieved by previous\nimplementations of logic regression. We apply GMJMCMC to reanalyze QTL mapping\ndata for Recombinant Inbred Lines in \\textit{Arabidopsis thaliana} and from a\nbackcross population in \\textit{Drosophila} where we identify several\ninteresting epistatic effects. The method is implemented in an R package which\nis available on github.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:59:43 GMT"}, {"version": "v2", "created": "Sat, 2 Jun 2018 12:08:57 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 13:30:12 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "1705.07646", "submitter": "JInglai Li", "authors": "Qingping Zhou, Wenqing Liu, Jinglai Li, Youssef M. Marzouk", "title": "An approximate empirical Bayesian method for large-scale linear-Gaussian\n  inverse problems", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aac287", "report-no": null, "categories": "math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Bayesian inference methods for solving linear inverse problems,\nfocusing on hierarchical formulations where the prior or the likelihood\nfunction depend on unspecified hyperparameters. In practice, these\nhyperparameters are often determined via an empirical Bayesian method that\nmaximizes the marginal likelihood function, i.e., the probability density of\nthe data conditional on the hyperparameters. Evaluating the marginal\nlikelihood, however, is computationally challenging for large-scale problems.\nIn this work, we present a method to approximately evaluate marginal likelihood\nfunctions, based on a low-rank approximation of the update from the prior\ncovariance to the posterior covariance. We show that this approximation is\noptimal in a minimax sense. Moreover, we provide an efficient algorithm to\nimplement the proposed method, based on a combination of the randomized SVD and\na spectral approximation method to compute square roots of the prior covariance\nmatrix. Several numerical examples demonstrate good performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 10:23:00 GMT"}, {"version": "v2", "created": "Sun, 11 Mar 2018 06:34:35 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Zhou", "Qingping", ""], ["Liu", "Wenqing", ""], ["Li", "Jinglai", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1705.07880", "submitter": "Andrew Miller", "authors": "Andrew C. Miller, Nicholas J. Foti, Alexander D'Amour, Ryan P. Adams", "title": "Reducing Reparameterization Gradient Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization with noisy gradients has become ubiquitous in statistics and\nmachine learning. Reparameterization gradients, or gradient estimates computed\nvia the \"reparameterization trick,\" represent a class of noisy gradients often\nused in Monte Carlo variational inference (MCVI). However, when these gradient\nestimators are too noisy, the optimization procedure can be slow or fail to\nconverge. One way to reduce noise is to use more samples for the gradient\nestimate, but this can be computationally expensive. Instead, we view the noisy\ngradient as a random variable, and form an inexpensive approximation of the\ngenerating procedure for the gradient sample. This approximation has high\ncorrelation with the noisy gradient by construction, making it a useful control\nvariate for variance reduction. We demonstrate our approach on non-conjugate\nmulti-level hierarchical models and a Bayesian neural net where we observed\ngradient variance reductions of multiple orders of magnitude (20-2,000x).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:51:13 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["D'Amour", "Alexander", ""], ["Adams", "Ryan P.", ""]]}, {"id": "1705.07959", "submitter": "Pat Scott", "authors": "The GAMBIT Scanner Workgroup: Gregory D. Martinez, James McKay, Ben\n  Farmer, Pat Scott, Elinore Roebber, Antje Putze, and Jan Conrad", "title": "Comparison of statistical sampling methods with ScannerBit, the GAMBIT\n  scanning module", "comments": "51 pages, 25 figures, 2 tables, matches version accepted for\n  publication in EPJC", "journal-ref": "Eur. Phys. J. C (2017) 77:761", "doi": "10.1140/epjc/s10052-017-5274-y", "report-no": "gambit-code-2017", "categories": "hep-ph astro-ph.CO astro-ph.IM physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce ScannerBit, the statistics and sampling module of the public,\nopen-source global fitting framework GAMBIT. ScannerBit provides a standardised\ninterface to different sampling algorithms, enabling the use and comparison of\nmultiple computational methods for inferring profile likelihoods, Bayesian\nposteriors, and other statistical quantities. The current version offers\nrandom, grid, raster, nested sampling, differential evolution, Markov Chain\nMonte Carlo (MCMC) and ensemble Monte Carlo samplers. We also announce the\nrelease of a new standalone differential evolution sampler, Diver, and describe\nits design, usage and interface to ScannerBit. We subject Diver and three other\nsamplers (the nested sampler MultiNest, the MCMC GreAT, and the native\nScannerBit implementation of the ensemble Monte Carlo algorithm T-Walk) to a\nbattery of statistical tests. For this we use a realistic physical likelihood\nfunction, based on the scalar singlet model of dark matter. We examine the\nperformance of each sampler as a function of its adjustable settings, and the\ndimensionality of the sampling problem. We evaluate performance on four\nmetrics: optimality of the best fit found, completeness in exploring the\nbest-fit region, number of likelihood evaluations, and total runtime. For\nBayesian posterior estimation at high resolution, T-Walk provides the most\naccurate and timely mapping of the full parameter space. For profile likelihood\nanalysis in less than about ten dimensions, we find that Diver and MultiNest\nscore similarly in terms of best fit and speed, outperforming GreAT and T-Walk;\nin ten or more dimensions, Diver substantially outperforms the other three\nsamplers on all metrics.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:24:51 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 19:08:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Workgroup", "The GAMBIT Scanner", ""], [":", "", ""], ["Martinez", "Gregory D.", ""], ["McKay", "James", ""], ["Farmer", "Ben", ""], ["Scott", "Pat", ""], ["Roebber", "Elinore", ""], ["Putze", "Antje", ""], ["Conrad", "Jan", ""]]}, {"id": "1705.08096", "submitter": "Antony Overstall", "authors": "Antony Overstall, David Woods, Maria Adamou", "title": "acebayes: An R Package for Bayesian Optimal Design of Experiments via\n  Approximate Coordinate Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the R package acebayes and demonstrate its use to find Bayesian\noptimal experimental designs. A decision-theoretic approach is adopted, with\nthe optimal design maximising an expected utility. Finding Bayesian optimal\ndesigns for realistic problems is challenging, as the expected utility is\ntypically intractable and the design space may be high-dimensional. The package\nimplements the approximate coordinate exchange algorithm to optimise (an\napproximation to) the expected utility via a sequence of conditional\none-dimensional optimisation steps. At each step, a Gaussian process regression\nmodel is used to approximate, and subsequently optimise, the expected utility\nas the function of a single design coordinate (the value taken by one\ncontrollable variable for one run of the experiment). In addition to functions\nfor bespoke design problems with user-defined utility functions, acebayes\nprovides functions tailored to finding designs for common generalised linear\nand nonlinear models. The package provides a step-change in the complexity of\nproblems that can be addressed, enabling designs to be found for much larger\nnumbers of variables and runs than previously possible. We provide tutorials on\nthe application of the methodology for four illustrative examples of varying\ncomplexity where designs are found for the goals of parameter estimation, model\nselection and prediction. These examples demonstrate previously unseen\nfunctionality of acebayes.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 06:52:29 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 07:48:39 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 07:58:24 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Overstall", "Antony", ""], ["Woods", "David", ""], ["Adamou", "Maria", ""]]}, {"id": "1705.08105", "submitter": "Andrew Zammit-Mangion", "authors": "Andrew Zammit-Mangion and Noel Cressie", "title": "FRK: An R Package for Spatial and Spatio-Temporal Prediction with Large\n  Datasets", "comments": "44 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  FRK is an R software package for spatial/spatio-temporal modelling and\nprediction with large datasets. It facilitates optimal spatial prediction\n(kriging) on the most commonly used manifolds (in Euclidean space and on the\nsurface of the sphere), for both spatial and spatio-temporal fields. It differs\nfrom many of the packages for spatial modelling and prediction by avoiding\nstationary and isotropic covariance and variogram models, instead constructing\na spatial random effects (SRE) model on a fine-resolution discretised spatial\ndomain. The discrete element is known as a basic areal unit (BAU), whose\nintroduction in the software leads to several practical advantages. The\nsoftware can be used to (i) integrate multiple observations with different\nsupports with relative ease; (ii) obtain exact predictions at millions of\nprediction locations (without conditional simulation); and (iii) distinguish\nbetween measurement error and fine-scale variation at the resolution of the\nBAU, thereby allowing for reliable uncertainty quantification. The temporal\ncomponent is included by adding another dimension. A key component of the SRE\nmodel is the specification of spatial or spatio-temporal basis functions; in\nthe package, they can be generated automatically or by the user. The package\nalso offers automatic BAU construction, an expectation-maximisation (EM)\nalgorithm for parameter estimation, and functionality for prediction over any\nuser-specified polygons or BAUs. Use of the package is illustrated on several\nspatial and spatio-temporal datasets, and its predictions and the model it\nimplements are extensively compared to others commonly used for spatial\nprediction and modelling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 07:29:57 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 05:18:41 GMT"}, {"version": "v3", "created": "Thu, 7 Jun 2018 22:29:09 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zammit-Mangion", "Andrew", ""], ["Cressie", "Noel", ""]]}, {"id": "1705.08510", "submitter": "Akihiko Nishimura", "authors": "Akihiko Nishimura, David Dunson, and Jianfeng Lu", "title": "Discontinuous Hamiltonian Monte Carlo for discrete parameters and\n  discontinuous likelihoods", "comments": "15 pages (4 figures) + 18 page (3 figures) supplement. Accepted by\n  Biometrika. Code available at\n  https://github.com/aki-nishimura/discontinuous-hmc", "journal-ref": "Biometrika (2020)", "doi": "10.1093/biomet/asz083", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo has emerged as a standard tool for posterior\ncomputation. In this article, we present an extension that can efficiently\nexplore target distributions with discontinuous densities. Our extension in\nparticular enables efficient sampling from ordinal parameters though embedding\nof probability mass functions into continuous spaces. We motivate our approach\nthrough a theory of discontinuous Hamiltonian dynamics and develop a\ncorresponding numerical solver. The proposed solver is the first of its kind,\nwith a remarkable ability to exactly preserve the Hamiltonian. We apply our\nalgorithm to challenging posterior inference problems to demonstrate its wide\napplicability and competitive performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:56:00 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 00:12:33 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 00:02:29 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 00:44:11 GMT"}, {"version": "v5", "created": "Fri, 23 Aug 2019 00:32:36 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Nishimura", "Akihiko", ""], ["Dunson", "David", ""], ["Lu", "Jianfeng", ""]]}, {"id": "1705.08656", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Finn Lindgren, David Bolin and Mattias Villani", "title": "Efficient Covariance Approximations for Large Sparse Precision Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of sparse precision (inverse covariance) matrices has become popular\nbecause they allow for efficient algorithms for joint inference in\nhigh-dimensional models. Many applications require the computation of certain\nelements of the covariance matrix, such as the marginal variances, which may be\nnon-trivial to obtain when the dimension is large. This paper introduces a fast\nRao-Blackwellized Monte Carlo sampling based method for efficiently\napproximating selected elements of the covariance matrix. The variance and\nconfidence bounds of the approximations can be precisely estimated without\nadditional computational costs. Furthermore, a method that iterates over\nsubdomains is introduced, and is shown to additionally reduce the approximation\nerrors to practically negligible levels in an application on functional\nmagnetic resonance imaging data. Both methods have low memory requirements,\nwhich is typically the bottleneck for competing direct methods.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 08:32:46 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:00:05 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindgren", "Finn", ""], ["Bolin", "David", ""], ["Villani", "Mattias", ""]]}, {"id": "1705.08931", "submitter": "Jaan Altosaar", "authors": "Jaan Altosaar, Rajesh Ranganath, David M. Blei", "title": "Proximity Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a powerful approach for approximate posterior\ninference. However, it is sensitive to initialization and can be subject to\npoor local optima. In this paper, we develop proximity variational inference\n(PVI). PVI is a new method for optimizing the variational objective that\nconstrains subsequent iterates of the variational parameters to robustify the\noptimization path. Consequently, PVI is less sensitive to initialization and\noptimization quirks and finds better local optima. We demonstrate our method on\nthree proximity statistics. We study PVI on a Bernoulli factor model and\nsigmoid belief network with both real and synthetic data and compare to\ndeterministic annealing (Katahira et al., 2008). We highlight the flexibility\nof PVI by designing a proximity statistic for Bayesian deep learning models\nsuch as the variational autoencoder (Kingma and Welling, 2014; Rezende et al.,\n2014). Empirically, we show that PVI consistently finds better local optima and\ngives better predictive performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 19:06:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Altosaar", "Jaan", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}, {"id": "1705.09395", "submitter": "John Jakeman", "authors": "Scott N. Walsh, Tim M. Wildey, John D. Jakeman", "title": "Optimal Experimental Design Using A Consistent Bayesian Approach", "comments": null, "journal-ref": null, "doi": "10.1115/1.4037457", "report-no": null, "categories": "stat.CO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the utilization of a computational model to guide the optimal\nacquisition of experimental data to inform the stochastic description of model\ninput parameters. Our formulation is based on the recently developed consistent\nBayesian approach for solving stochastic inverse problems which seeks a\nposterior probability density that is consistent with the model and the data in\nthe sense that the push-forward of the posterior (through the computational\nmodel) matches the observed density on the observations almost everywhere.\nGiven a set a potential observations, our optimal experimental design (OED)\nseeks the observation, or set of observations, that maximizes the expected\ninformation gain from the prior probability density on the model parameters. We\ndiscuss the characterization of the space of observed densities and a\ncomputationally efficient approach for rescaling observed densities to satisfy\nthe fundamental assumptions of the consistent Bayesian approach. Numerical\nresults are presented to compare our approach with existing OED methodologies\nusing the classical/statistical Bayesian approach and to demonstrate our OED on\na set of representative PDE-based models.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 23:24:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Walsh", "Scott N.", ""], ["Wildey", "Tim M.", ""], ["Jakeman", "John D.", ""]]}, {"id": "1705.09457", "submitter": "Christiane G\\\"orgen", "authors": "Christiane G\\\"orgen, Anna Bigatti, Eva Riccomagno, Jim Q. Smith", "title": "Discovery of statistical equivalence classes using computer algebra", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete statistical models supported on labelled event trees can be\nspecified using so-called interpolating polynomials which are generalizations\nof generating functions. These admit a nested representation. A new algorithm\nexploits the primary decomposition of monomial ideals associated with an\ninterpolating polynomial to quickly compute all nested representations of that\npolynomial. It hereby determines an important subclass of all trees\nrepresenting the same statistical model. To illustrate this method we analyze\nthe full polynomial equivalence class of a staged tree representing the best\nfitting model inferred from a real-world dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 07:17:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["G\u00f6rgen", "Christiane", ""], ["Bigatti", "Anna", ""], ["Riccomagno", "Eva", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1705.09530", "submitter": "Nicholas Horton", "authors": "Beverly L. Wood and Megan Mocko and Michelle Everson and Nicholas J.\n  Horton and Paul Velleman", "title": "Updated guidelines, updated curriculum: The GAISE College Report and\n  introductory statistics for the modern student", "comments": "in press, CHANCE", "journal-ref": null, "doi": "10.1080/09332480.2018.1467642", "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the 2005 American Statistical Association's (ASA) endorsement of the\nGuidelines for Assessment and Instruction in Statistics Education (GAISE)\nCollege Report, changes in the statistics field and statistics education have\nhad a major impact on the teaching and learning of statistics. We now live in a\nworld where \"Statistics - the science of learning from data - is the\nfastest-growing science, technology, engineering, and math (STEM) undergraduate\ndegree in the United States,\" according to the ASA, and where many jobs demand\nan understanding of how to explore and make sense of data. In light of these\nnew reports and other changes and demands on the discipline, a group of\nvolunteers revised the 2005 GAISE College Report. The updated report was\nendorsed by the Board of Directors of the American Statistical Association in\nJuly 2016. To help shed additional light on the revision process and subsequent\nchanges in the report, we review the report and share insights into the\ncommittee's thoughts and assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 11:09:15 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Wood", "Beverly L.", ""], ["Mocko", "Megan", ""], ["Everson", "Michelle", ""], ["Horton", "Nicholas J.", ""], ["Velleman", "Paul", ""]]}, {"id": "1705.09746", "submitter": "I\\~naki Ucar", "authors": "I\\~naki Ucar, Bart Smeets, Arturo Azcorra", "title": "simmer: Discrete-Event Simulation for R", "comments": "31 pages, 4 figures", "journal-ref": "Journal of Statistical Software, v. 90, Issue 2, p. 1 - 30, july\n  2019", "doi": "10.18637/jss.v090.i02", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simmer package brings discrete-event simulation to R. It is designed as a\ngeneric yet powerful process-oriented framework. The architecture encloses a\nrobust and fast simulation core written in C++ with automatic monitoring\ncapabilities. It provides a rich and flexible R API that revolves around the\nconcept of trajectory, a common path in the simulation model for entities of\nthe same type.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 00:38:10 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 19:55:40 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Ucar", "I\u00f1aki", ""], ["Smeets", "Bart", ""], ["Azcorra", "Arturo", ""]]}, {"id": "1705.09874", "submitter": "Oleg Sofrygin", "authors": "Oleg Sofrygin, Zheng Zhu, Julie A Schmittdiel, Alyce S. Adams, Richard\n  W. Grant, Mark J. van der Laan, and Romain Neugebauer", "title": "Targeted Learning with Daily EHR Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHR) data provide a cost and time-effective\nopportunity to conduct cohort studies of the effects of multiple time-point\ninterventions in the diverse patient population found in real-world clinical\nsettings. Because the computational cost of analyzing EHR data at daily (or\nmore granular) scale can be quite high, a pragmatic approach has been to\npartition the follow-up into coarser intervals of pre-specified length. Current\nguidelines suggest employing a 'small' interval, but the feasibility and\npractical impact of this recommendation has not been evaluated and no formal\nmethodology to inform this choice has been developed. We start filling these\ngaps by leveraging large-scale EHR data from a diabetes study to develop and\nillustrate a fast and scalable targeted learning approach that allows to follow\nthe current recommendation and study its practical impact on inference. More\nspecifically, we map daily EHR data into four analytic datasets using 90, 30,\n15 and 5-day intervals. We apply a semi-parametric and doubly robust estimation\napproach, the longitudinal TMLE, to estimate the causal effects of four dynamic\ntreatment rules with each dataset, and compare the resulting inferences. To\novercome the computational challenges presented by the size of these data, we\npropose a novel TMLE implementation, the 'long-format TMLE', and rely on the\nlatest advances in scalable data-adaptive machine-learning software, xgboost\nand h2o, for estimation of the TMLE nuisance parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 22:43:08 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 21:53:24 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Sofrygin", "Oleg", ""], ["Zhu", "Zheng", ""], ["Schmittdiel", "Julie A", ""], ["Adams", "Alyce S.", ""], ["Grant", "Richard W.", ""], ["van der Laan", "Mark J.", ""], ["Neugebauer", "Romain", ""]]}, {"id": "1705.09998", "submitter": "Andr\\'es F. Barrientos", "authors": "Andr\\'es F. Barrientos and V\\'ictor Pe\\~na", "title": "Bayesian Bootstraps for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present data-subsetting algorithms that allow for the\napproximate and scalable implementation of the Bayesian bootstrap. They are\nanalogous to two existing algorithms in the frequentist literature: the bag of\nlittle bootstraps (Kleiner et al., 2014) and the subsampled double bootstrap\n(SDB; Sengupta et al., 2016). Our algorithms have appealing theoretical and\ncomputational properties that are comparable to those of their frequentist\ncounterparts. Additionally, we provide a strategy for performing lossless\ninference for a class of functionals of the Bayesian bootstrap, and briefly\nintroduce extensions to the Dirichlet Process.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 22:25:21 GMT"}, {"version": "v2", "created": "Fri, 22 Mar 2019 00:59:04 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Barrientos", "Andr\u00e9s F.", ""], ["Pe\u00f1a", "V\u00edctor", ""]]}, {"id": "1705.10061", "submitter": "Bruno Sudret", "authors": "R. Sch\\\"obi and B. Sudret", "title": "Global sensitivity analysis in the context of imprecise probabilities\n  (p-boxes) using sparse polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2017-007", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis aims at determining which uncertain input\nparameters of a computational model primarily drives the variance of the output\nquantities of interest. Sobol' indices are now routinely applied in this\ncontext when the input parameters are modelled by classical probability theory\nusing random variables. In many practical applications however, input\nparameters are affected by both aleatory and epistemic (so-called polymorphic)\nuncertainty, for which imprecise probability representations have become\npopular in the last decade. In this paper, we consider that the uncertain input\nparameters are modelled by parametric probability boxes (p-boxes). We propose\ninterval-valued (so-called imprecise) Sobol' indices as an extension of their\nclassical definition. An original algorithm based on the concepts of augmented\nspace, isoprobabilistic transforms and sparse polynomial chaos expansions is\ndevised to allow for the computation of these imprecise Sobol' indices at\nextremely low cost. In particular, phantoms points are introduced to build an\nexperimental design in the augmented space (necessary for the calibration of\nthe sparse PCE) which leads to a smart reuse of runs of the original\ncomputational model. The approach is illustrated on three analytical and\nengineering examples which allows one to validate the proposed algorithms\nagainst brute-force double-loop Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 07:55:25 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Sch\u00f6bi", "R.", ""], ["Sudret", "B.", ""]]}, {"id": "1705.10347", "submitter": "Suzanne Thornton", "authors": "Suzanne Thornton, Wentao Li, Min-ge Xie", "title": "An effective likelihood-free approximate computing method with\n  statistical inferential guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computing is a powerful likelihood-free method that has\ngrown increasingly popular since early applications in population genetics.\nHowever, complications arise in the theoretical justification for Bayesian\ninference conducted from this method with a non-sufficient summary statistic.\nIn this paper, we seek to re-frame approximate Bayesian computing within a\nfrequentist context and justify its performance by standards set on the\nfrequency coverage rate. In doing so, we develop a new computational technique\ncalled approximate confidence distribution computing, yielding theoretical\nsupport for the use of non-sufficient summary statistics in likelihood-free\nmethods. Furthermore, we demonstrate that approximate confidence distribution\ncomputing extends the scope of approximate Bayesian computing to include\ndata-dependent priors without damaging the inferential integrity. This\ndata-dependent prior can be viewed as an initial `distribution estimate' of the\ntarget parameter which is updated with the results of the approximate\nconfidence distribution computing method. A general strategy for constructing\nan appropriate data-dependent prior is also discussed and is shown to often\nincrease the computing speed while maintaining statistical inferential\nguarantees. We supplement the theory with simulation studies illustrating the\nbenefits of the proposed method, namely the potential for broader applications\nand the increased computing speed compared to the standard approximate Bayesian\ncomputing methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:28:14 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:57:02 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 16:52:27 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 17:04:55 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Thornton", "Suzanne", ""], ["Li", "Wentao", ""], ["Xie", "Min-ge", ""]]}, {"id": "1705.10376", "submitter": "Oleg Sofrygin", "authors": "Oleg Sofrygin, Romain Neugebauer, Mark J. van der Laan", "title": "Conducting Simulations in Causal Inference with Networks-Based\n  Structural Equation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen an increasing body of literature devoted to the\nestimation of causal effects in network-dependent data. However, the validity\nof many classical statistical methods in such data is often questioned. There\nis an emerging need for objective and practical ways to assess which causal\nmethodologies might be applicable and valid in network-dependent data. This\npaper describes a set of tools implemented in the simcausal R package that\nallow simulating data based on user-specified structural equation model for\nconnected units. Specification and simulation of counterfactual data is\nimplemented for static, dynamic and stochastic interventions. A new interface\naims to simplify the specification of network-based functional relationships\nbetween connected units. A set of examples illustrates how these simulations\nmay be applied to evaluation of different statistical methods for estimation of\ncausal effects in network-dependent data.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 19:45:41 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Sofrygin", "Oleg", ""], ["Neugebauer", "Romain", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1705.10498", "submitter": "Guillaume Gautier", "authors": "Guillaume Gautier, R\\'emi Bardenet, Michal Valko", "title": "Zonotope hit-and-run for efficient sampling from projection DPPs", "comments": "12 pages, 12 figures, 2 columns, accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are distributions over sets of items\nthat model diversity using kernels. Their applications in machine learning\ninclude summary extraction and recommendation systems. Yet, the cost of\nsampling from a DPP is prohibitive in large-scale applications, which has\ntriggered an effort towards efficient approximate samplers. We build a novel\nMCMC sampler that combines ideas from combinatorial geometry, linear\nprogramming, and Monte Carlo methods to sample from DPPs with a fixed sample\ncardinality, also called projection DPPs. Our sampler leverages the ability of\nthe hit-and-run MCMC kernel to efficiently move across convex bodies. Previous\ntheoretical results yield a fast mixing time of our chain when targeting a\ndistribution that is close to a projection DPP, but not a DPP in general. Our\nempirical results demonstrate that this extends to sampling projection DPPs,\ni.e., our sampler is more sample-efficient than previous approaches which in\nturn translates to faster convergence when dealing with costly-to-evaluate\nfunctions, such as summary extraction in our experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 08:07:19 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 08:36:00 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Gautier", "Guillaume", ""], ["Bardenet", "R\u00e9mi", ""], ["Valko", "Michal", ""]]}, {"id": "1705.10662", "submitter": "David R\\\"ugamer", "authors": "Sarah Brockhaus and David R\\\"ugamer and Sonja Greven", "title": "Boosting Functional Regression Models with FDboost", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R add-on package FDboost is a flexible toolbox for the estimation of\nfunctional regression models by model-based boosting. It provides the\npossibility to fit regression models for scalar and functional response with\neffects of scalar as well as functional covariates, i.e., scalar-on-function,\nfunction-on-scalar and function-on-function regression models. In addition to\nmean regression, quantile regression models as well as generalized additive\nmodels for location scale and shape can be fitted with FDboost. Furthermore,\nboosting can be used in high-dimensional data settings with more covariates\nthan observations. We provide a hands-on tutorial on model fitting and tuning,\nincluding the visualization of results. The methods for scalar-on-function\nregression are illustrated with spectrometric data of fossil fuels and those\nfor functional response regression with a data set including bioelectrical\nsignals for emotional episodes.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 14:15:38 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 07:14:58 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 09:45:04 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Brockhaus", "Sarah", ""], ["R\u00fcgamer", "David", ""], ["Greven", "Sonja", ""]]}, {"id": "1705.10922", "submitter": "Sahil Shah", "authors": "Sahil D. Shah and Rosemary Braun", "title": "Network-based identification of disease genes in expression data: the\n  GeneSurrounder method", "comments": "We have extended the application and evaluation of our GeneSurrounder\n  method to a second disease (gene expression data from bladder cancer) and\n  added additional analyses of GeneSurrounder's ability to identify known\n  cancer-associated genes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN q-bio.MN stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of high--throughput transcription profiling technologies has\nenabled identification of genes and pathways associated with disease, providing\nnew avenues for precision medicine. A key challenge is to analyze this data in\nthe context of the regulatory networks and pathways that control cellular\nprocesses, while still obtaining insights that can be used to design new\ndiagnostic and therapeutic interventions. While classical differential\nexpression analysis provides specific and hence targetable gene-level insights,\nit does not include any systems-level information. On the other hand, pathway\nanalyses integrate systems-level information with expression data, but are\noften limited in their ability to indicate specific molecular targets. We\nintroduce GeneSurrounder, an analysis method that takes into account the\ncomplex structure of interaction networks to identify specific genes that\ndisrupt pathway activity in a disease-specific manner. GeneSurrounder\nintegrates transcriptomic data and pathway network information in a novel\ntwo-step procedure to detect genes that (i) appear to influence the expression\nof other genes local to it in the network and (ii) are part of a subnetwork of\ndifferentially expressed genes. Combined, this evidence can be used to pinpoint\nspecific genes that have a mechanistic role in the phenotype of interest.\nApplying GeneSurrounder to three distinct ovarian cancer studies using a global\nKEGG network, we show that our method is able to identify biologically relevant\ngenes and genes missed by single-gene association tests, integrate pathway and\nexpression data, and yield more consistent results across multiple studies of\nthe same phenotype than competing methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:40:18 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 23:08:36 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Shah", "Sahil D.", ""], ["Braun", "Rosemary", ""]]}, {"id": "1705.11123", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner", "title": "Advanced Bayesian Multilevel Modeling with the R Package brms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brms package allows R users to easily specify a wide range of Bayesian\nsingle-level and multilevel models, which are fitted with the probabilistic\nprogramming language Stan behind the scenes. Several response distributions are\nsupported, of which all parameters (e.g., location, scale, and shape) can be\npredicted at the same time thus allowing for distributional regression.\nNon-linear relationships may be specified using non-linear predictor terms or\nsemi-parametric approaches such as splines or Gaussian processes. To make all\nof these modeling options possible in a multilevel framework, brms provides an\nintuitive and powerful formula syntax, which extends the well known formula\nsyntax of lme4. The purpose of the present paper is to introduce this syntax in\ndetail and to demonstrate its usefulness with four examples, each showing other\nrelevant aspects of the syntax.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:59:09 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 20:23:40 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""]]}, {"id": "1705.11140", "submitter": "Christian A. Naesseth", "authors": "Christian A. Naesseth and Scott W. Linderman and Rajesh Ranganath and\n  David M. Blei", "title": "Variational Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent advances in large scale probabilistic inference rely on\nvariational methods. The success of variational approaches depends on (i)\nformulating a flexible parametric family of distributions, and (ii) optimizing\nthe parameters to find the member of this family that most closely approximates\nthe exact posterior. In this paper we present a new approximating family of\ndistributions, the variational sequential Monte Carlo (VSMC) family, and show\nhow to optimize it in variational inference. VSMC melds variational inference\n(VI) and sequential Monte Carlo (SMC), providing practitioners with flexible,\naccurate, and powerful Bayesian inference. The VSMC family is a variational\nfamily that can approximate the posterior arbitrarily well, while still\nallowing for efficient optimization of its parameters. We demonstrate its\nutility on state space models, stochastic volatility models for financial data,\nand deep Markov models of brain neural circuits.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 15:23:42 GMT"}, {"version": "v2", "created": "Wed, 21 Feb 2018 08:58:41 GMT"}], "update_date": "2018-02-22", "authors_parsed": [["Naesseth", "Christian A.", ""], ["Linderman", "Scott W.", ""], ["Ranganath", "Rajesh", ""], ["Blei", "David M.", ""]]}]