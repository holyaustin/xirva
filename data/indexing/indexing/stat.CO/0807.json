[{"id": "0807.0725", "submitter": "Mario Peruggia", "authors": "Ilenia Epifani, Steven N. MacEachern, Mario Peruggia", "title": "Case-deletion importance sampling estimators: Central limit theorems and\n  related results", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS259 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2008, Vol. 2, 774-806", "doi": "10.1214/08-EJS259", "report-no": "IMS-EJS-EJS_2008_259", "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-deleted analysis is a popular method for evaluating the influence of a\nsubset of cases on inference. The use of Monte Carlo estimation strategies in\ncomplicated Bayesian settings leads naturally to the use of importance sampling\ntechniques to assess the divergence between full-data and case-deleted\nposteriors and to provide estimates under the case-deleted posteriors. However,\nthe dependability of the importance sampling estimators depends critically on\nthe variability of the case-deleted weights. We provide theoretical results\nconcerning the assessment of the dependability of case-deleted importance\nsampling estimators in several Bayesian models. In particular, these results\nallow us to establish whether or not the estimators satisfy a central limit\ntheorem. Because the conditions we derive are of a simple analytical nature,\nthe assessment of the dependability of the estimators can be verified routinely\nbefore estimation is performed. We illustrate the use of the results in several\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jul 2008 11:36:38 GMT"}, {"version": "v2", "created": "Wed, 17 Sep 2008 06:35:28 GMT"}], "update_date": "2008-09-17", "authors_parsed": [["Epifani", "Ilenia", ""], ["MacEachern", "Steven N.", ""], ["Peruggia", "Mario", ""]]}, {"id": "0807.2569", "submitter": "Jeffrey Solka", "authors": "Jeffrey Solka", "title": "Text Data Mining: Theory and Methods", "comments": "Published in at http://dx.doi.org/10.1214/07-SS016 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, 94-112", "doi": "10.1214/07-SS016", "report-no": "IMS-SS-SS_2007_16", "categories": "stat.ML cs.IR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides the reader with a very brief introduction to some of the\ntheory and methods of text data mining. The intent of this article is to\nintroduce the reader to some of the current methodologies that are employed\nwithin this discipline area while at the same time making the reader aware of\nsome of the interesting challenges that remain to be solved within the area.\nFinally, the articles serves as a very rudimentary tutorial on some of\ntechniques while also providing the reader with a list of references for\nadditional study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2008 13:39:32 GMT"}], "update_date": "2008-07-17", "authors_parsed": [["Solka", "Jeffrey", ""]]}, {"id": "0807.2598", "submitter": "Robb Muirhead", "authors": "Morris L. Eaton and Robb J. Muirhead", "title": "A decomposition result for the Haar distribution on the orthogonal group", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let H be a Haar distributed random matrix on the group of pxp real orthogonal\nmatrices. Partition H into four blocks: (1) the (1,1) element, (2)the rest of\nthe first row, (3) the rest of the first column, and (4)the remaining\n(p-1)x(p-1) matrix. The marginal distribution of (1) is well known. In this\npaper, we give the conditional distribution of (2) and (3) given (1), and the\nconditional distribution of (4) given (1), (2), (3). This conditional\nspecification uniquely determines the Haar distribution. The two conditional\ndistributions involve well known probability distributions namely, the uniform\ndistribution on the unit sphere in p-1 dimensional space and the Haar\ndistribution on (p-2)x(p-2) orthogonal matrices. Our results show how to\nconstruct the Haar distribution on pxp orthogonal matrices from the Haar\ndistribution on (p-2)x(p-2) orthogonal matrices coupled with the uniform\ndistribution on the unit sphere in p-1 dimensions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jul 2008 16:02:42 GMT"}], "update_date": "2008-07-17", "authors_parsed": [["Eaton", "Morris L.", ""], ["Muirhead", "Robb J.", ""]]}, {"id": "0807.2767", "submitter": "Jean-Michel Marin", "authors": "Aude Grelaud, Christian Robert, Jean-Michel Marin, Francois Rodolphe,\n  Jean-Francois Taly", "title": "ABC likelihood-freee methods for model choice in Gibbs random fields", "comments": "19 pages, 5 figures, to appear in Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields (GRF) are polymorphous statistical models that can be\nused to analyse different types of dependence, in particular for spatially\ncorrelated data. However, when those models are faced with the challenge of\nselecting a dependence structure from many, the use of standard model choice\nmethods is hampered by the unavailability of the normalising constant in the\nGibbs likelihood. In particular, from a Bayesian perspective, the computation\nof the posterior probabilities of the models under competition requires special\nlikelihood-free simulation techniques like the Approximate Bayesian Computation\n(ABC) algorithm that is intensively used in population genetics. We show in\nthis paper how to implement an ABC algorithm geared towards model choice in the\ngeneral setting of Gibbs random fields, demonstrating in particular that there\nexists a sufficient statistic across models. The accuracy of the approximation\nto the posterior probabilities can be further improved by importance sampling\non the distribution of the models. The practical aspects of the method are\ndetailed through two applications, the test of an iid Bernoulli model versus a\nfirst-order Markov chain, and the choice of a folding structure for two\nproteins.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jul 2008 12:27:20 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2009 16:11:48 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2009 09:40:08 GMT"}], "update_date": "2009-04-03", "authors_parsed": [["Grelaud", "Aude", ""], ["Robert", "Christian", ""], ["Marin", "Jean-Michel", ""], ["Rodolphe", "Francois", ""], ["Taly", "Jean-Francois", ""]]}, {"id": "0807.3151", "submitter": "Ioana Cosma", "authors": "Ioana A. Cosma and Masoud Asgharian", "title": "Principle of detailed balance and convergence assessment of Markov Chain\n  Monte Carlo methods and simulated annealing", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) methods are employed to sample from a given\ndistribution of interest, whenever either the distribution does not exist in\nclosed form, or, if it does, no efficient method to simulate an independent\nsample from it is available. Although a wealth of diagnostic tools for\nconvergence assessment of MCMC methods have been proposed in the last two\ndecades, the search for a dependable and easy to implement tool is ongoing. We\npresent in this article a criterion based on the principle of detailed balance\nwhich provides a qualitative assessment of the convergence of a given chain.\nThe criterion is based on the behaviour of a one-dimensional statistic, whose\nasymptotic distribution under the assumption of stationarity is derived; our\nresults apply under weak conditions and have the advantage of being completely\nintuitive. We implement this criterion as a stopping rule for simulated\nannealing in the problem of finding maximum likelihood estimators for\nparameters of a 20-component mixture model. We also apply it to the problem of\nsampling from a 10-dimensional funnel distribution via slice sampling and the\nMetropolis-Hastings algorithm. Furthermore, based on this convergence criterion\nwe define a measure of efficiency of one algorithm versus another.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jul 2008 11:18:29 GMT"}], "update_date": "2008-07-22", "authors_parsed": [["Cosma", "Ioana A.", ""], ["Asgharian", "Masoud", ""]]}, {"id": "0807.3734", "submitter": "Guilherme  Rocha", "authors": "Guilherme V. Rocha, Peng Zhao, Bin Yu", "title": "A path following algorithm for Sparse Pseudo-Likelihood Inverse\n  Covariance Estimation (SPLICE)", "comments": "33 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given n observations of a p-dimensional random vector, the covariance matrix\nand its inverse (precision matrix) are needed in a wide range of applications.\nSample covariance (e.g. its eigenstructure) can misbehave when p is comparable\nto the sample size n. Regularization is often used to mitigate the problem.\n  In this paper, we proposed an l1-norm penalized pseudo-likelihood estimate\nfor the inverse covariance matrix. This estimate is sparse due to the l1-norm\npenalty, and we term this method SPLICE. Its regularization path can be\ncomputed via an algorithm based on the homotopy/LARS-Lasso algorithm.\nSimulation studies are carried out for various inverse covariance structures\nfor p=15 and n=20, 1000. We compare SPLICE with the l1-norm penalized\nlikelihood estimate and a l1-norm penalized Cholesky decomposition based\nmethod. SPLICE gives the best overall performance in terms of three metrics on\nthe precision matrix and ROC curve for model selection. Moreover, our\nsimulation results demonstrate that the SPLICE estimates are positive-definite\nfor most of the regularization path even though the restriction is not\nenforced.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jul 2008 19:24:42 GMT"}], "update_date": "2008-07-24", "authors_parsed": [["Rocha", "Guilherme V.", ""], ["Zhao", "Peng", ""], ["Yu", "Bin", ""]]}, {"id": "0807.4231", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "QR-Adjustment for Clustering Tests Based on Nearest Neighbor Contingency\n  Tables", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report # KU-EC-08-5", "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial interaction between two or more classes of points may cause\nspatial clustering patterns such as segregation or association, which can be\ntested using a nearest neighbor contingency table (NNCT). A NNCT is constructed\nusing the frequencies of class types of points in nearest neighbor (NN) pairs.\nFor the NNCT-tests, the null pattern is either complete spatial randomness\n(CSR) of the points from two or more classes (called CSR independence) or\nrandom labeling (RL). The distributions of the NNCT-test statistics depend on\nthe number of reflexive NNs (denoted by $R$) and the number of shared NNs\n(denoted by $Q$), both of which depend on the allocation of the points. Hence\n$Q$ and $R$ are fixed quantities under RL, but random variables under CSR\nindependence. Using their observed values in NNCT analysis makes the\ndistributions of the NNCT-test statistics conditional on $Q$ and $R$ under CSR\nindependence. In this article, I use the empirically estimated expected values\nof $Q$ and $R$ under CSR independence pattern to remove the conditioning of\nNNCT-tests (such a correction is called the \\emph{QR-adjustment}, henceforth).\nI present a Monte Carlo simulation study to compare the conditional NNCT-tests\nand QR-adjusted tests under CSR independence and segregation and association\nalternatives. I demonstrate that QR-adjustment does not significantly improve\nthe empirical size estimates under CSR independence and power estimates under\nsegregation or association alternatives. For illustrative purposes, I apply the\nconditional and empirically corrected tests on two example data sets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jul 2008 11:10:07 GMT"}], "update_date": "2008-07-29", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "0807.4690", "submitter": "Nikolay Balov", "authors": "Nikolay H. Balov", "title": "Covariance fields", "comments": "28 pages, core thesis paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study covariance fields of distributions on a Riemannian\nmanifold. At each point on the manifold, covariance is defined to be a\nsymmetric and positive definite (2,0)-tensor. Its product with the metric\ntensor specifies a linear operator on the respected tangent space.\nCollectively, these operators form a covariance operator field. We show that,\nin most circumstances, covariance fields are continuous. We also solve the\ninverse problem: recovering distribution from a covariance field. Surprisingly,\nthis is not possible on Euclidean spaces. On non-Euclidean manifolds however,\ncovariance fields are true distribution representations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2008 15:39:27 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2008 15:03:47 GMT"}, {"version": "v3", "created": "Thu, 15 Jan 2009 21:25:18 GMT"}], "update_date": "2009-01-15", "authors_parsed": [["Balov", "Nikolay H.", ""]]}, {"id": "0807.4719", "submitter": "Lutz Duembgen", "authors": "Madeleine L. Cule and Lutz Duembgen", "title": "On an Auxiliary Function for Log-Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical report 71, IMSV, University of Bern", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we provide explicit expressions and expansions for a special\nfunction which appears in nonparametric estimation of log-densities. This\nfunction returns the integral of a log-linear function on a simplex of\narbitrary dimension. In particular it is used in the R-package \"LogCondDEAD\" by\nCule et al. (2007).\n", "versions": [{"version": "v1", "created": "Tue, 29 Jul 2008 18:37:17 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 16:43:13 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Cule", "Madeleine L.", ""], ["Duembgen", "Lutz", ""]]}, {"id": "0807.4858", "submitter": "Art B. Owen", "authors": "Seth D. Tribble, Art B. Owen", "title": "Construction of weakly CUD sequences for MCMC sampling", "comments": "Published in at http://dx.doi.org/10.1214/07-EJS162 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2008, Vol. 2, 634-660", "doi": "10.1214/07-EJS162", "report-no": "IMS-EJS-EJS_2007_162", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Markov chain Monte Carlo (MCMC) sampling considerable thought goes into\nconstructing random transitions. But those transitions are almost always driven\nby a simulated IID sequence. Recently it has been shown that replacing an IID\nsequence by a weakly completely uniformly distributed (WCUD) sequence leads to\nconsistent estimation in finite state spaces. Unfortunately, few WCUD sequences\nare known. This paper gives general methods for proving that a sequence is\nWCUD, shows that some specific sequences are WCUD, and shows that certain\noperations on WCUD sequences yield new WCUD sequences. A numerical example on a\n42 dimensional continuous Gibbs sampler found that some WCUD inputs sequences\nproduced variance reductions ranging from tens to hundreds for posterior means\nof the parameters, compared to IID inputs.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jul 2008 13:17:11 GMT"}], "update_date": "2008-07-31", "authors_parsed": [["Tribble", "Seth D.", ""], ["Owen", "Art B.", ""]]}, {"id": "0807.5008", "submitter": "Elvira Di Nardo Ph.D.", "authors": "E. Di Nardo, G. Guarino, D. Senato", "title": "A new method for fast computing unbiased estimators of cumulants", "comments": "A table with computational times, obtained with the forthcoming\n  MathStatica release 2 (Colin Rose, private communication), has been added. In\n  press Stat. Comp", "journal-ref": null, "doi": "10.1007/s11222-008-9080-0", "report-no": null, "categories": "math.ST math.CO stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new algorithms for generating $k$-statistics, multivariate\n$k$-statistics, polykays and multivariate polykays. The resulting computational\ntimes are very fast compared with procedures existing in the literature. Such\nspeeding up is obtained by means of a symbolic method arising from the\nclassical umbral calculus. The classical umbral calculus is a light syntax that\ninvolves only elementary rules to managing sequences of numbers or polynomials.\nThe cornerstone of the procedures here introduced is the connection between\ncumulants of a random variable and a suitable compound Poisson random variable.\nSuch a connection holds also for multivariate random variables.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jul 2008 08:14:58 GMT"}], "update_date": "2008-08-01", "authors_parsed": [["Di Nardo", "E.", ""], ["Guarino", "G.", ""], ["Senato", "D.", ""]]}]