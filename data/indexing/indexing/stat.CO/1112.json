[{"id": "1112.0241", "submitter": "An Zeng", "authors": "Weiping Liu, An Zeng, Yanbo Zhou", "title": "Degree heterogeneity in spatial networks with total cost constraint", "comments": "4 pages, 4 figures", "journal-ref": "Europhysics Letter 98, 28003 (2012)", "doi": "10.1209/0295-5075/98/28003", "report-no": null, "categories": "physics.soc-ph cs.SI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, In [Phys. Rev. Lett. 104, 018701 (2010)] the authors studied a\nspatial network which is constructed from a regular lattice by adding\nlong-range edges (shortcuts) with probability $P_{ij}\\sim r_{ij}^{-\\alpha}$,\nwhere $r_{ij}$ is the Manhattan length of the long-range edges. The total\nlength of the additional edges is subject to a cost constraint ($\\sum r=C$).\nThese networks have fixed optimal exponent $\\alpha$ for transportation\n(measured by the average shortest-path length). However, we observe that the\ndegree in such spatial networks is homogenously distributed, which is far\ndifferent from real networks such as airline systems. In this paper, we propose\na method to introduce degree heterogeneity in spatial networks with total cost\nconstraint. Results show that with degree heterogeneity the optimal exponent\nshifts to a smaller value and the average shortest-path length can further\ndecrease. Moreover, we consider the synchronization on the spatial networks and\nrelated results are discussed. Our new model may better reproduce the features\nof many real transportation systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 16:58:30 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Liu", "Weiping", ""], ["Zeng", "An", ""], ["Zhou", "Yanbo", ""]]}, {"id": "1112.0295", "submitter": "Chavent Marie", "authors": "M. Chavent, V. Kuentz, B. Liquet and L. Saracco", "title": "ClustOfVar: An R Package for the Clustering of Variables", "comments": null, "journal-ref": "Journal of Statistical Software (2012), 50(13), 1-16", "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering of variables is as a way to arrange variables into homogeneous\nclusters, i.e., groups of variables which are strongly related to each other\nand thus bring the same information. These approaches can then be useful for\ndimension reduction and variable selection. Several specific methods have been\ndeveloped for the clustering of numerical variables. However concerning\nqualitative variables or mixtures of quantitative and qualitative variables,\nfar fewer methods have been proposed. The R package ClustOfVar was specifically\ndeveloped for this purpose. The homogeneity criterion of a cluster is defined\nas the sum of correlation ratios (for qualitative variables) and squared\ncorrelations (for quantitative variables) to a synthetic quantitative variable,\nsummarizing \"as good as possible\" the variables in the cluster. This synthetic\nvariable is the first principal component obtained with the PCAMIX method. Two\nalgorithms for the clustering of variables are proposed: iterative relocation\nalgorithm and ascendant hierarchical clustering. We also propose a bootstrap\napproach in order to determine suitable numbers of clusters. We illustrate the\nmethodologies and the associated package on small datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 20:33:07 GMT"}], "update_date": "2012-12-12", "authors_parsed": [["Chavent", "M.", ""], ["Kuentz", "V.", ""], ["Liquet", "B.", ""], ["Saracco", "L.", ""]]}, {"id": "1112.0301", "submitter": "Chavent Marie", "authors": "M. Chavent, K. Vanessa and J. Saracco", "title": "Orthogonal rotation in PCAMIX", "comments": null, "journal-ref": "Adv Data Anal Classif (2012) 6:131-146", "doi": "10.1007/s11634-012-0105-3", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kiers (1991) considered the orthogonal rotation in PCAMIX, a principal\ncomponent method for a mixture of qualitative and quantitative variables.\nPCAMIX includes the ordinary principal component analysis (PCA) and multiple\ncorrespondence analysis (MCA) as special cases. In this paper, we give a new\npresentation of PCAMIX where the principal components and the squared loadings\nare obtained from a Singular Value Decomposition. The loadings of the\nquantitative variables and the principal coordinates of the categories of the\nqualitative variables are also obtained directly. In this context, we propose a\ncomputationaly efficient procedure for varimax rotation in PCAMIX and a direct\nsolution for the optimal angle of rotation. A simulation study shows the good\ncomputational behavior of the proposed algorithm. An application on a real data\nset illustrates the interest of using rotation in MCA. All source codes are\navailable in the R package \"PCAmixdata\".\n", "versions": [{"version": "v1", "created": "Thu, 1 Dec 2011 20:41:01 GMT"}], "update_date": "2012-12-11", "authors_parsed": [["Chavent", "M.", ""], ["Vanessa", "K.", ""], ["Saracco", "J.", ""]]}, {"id": "1112.0918", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "On best subset regression", "comments": "This paper has been withdrawn by the author. A related paper entitled\n  \"Better subset regression\" is arXiv:1212.0634", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss the variable selection method from \\ell0-norm\nconstrained regression, which is equivalent to the problem of finding the best\nsubset of a fixed size. Our study focuses on two aspects, consistency and\ncomputation. We prove that the sparse estimator from such a method can retain\nall of the important variables asymptotically for even exponentially growing\ndimensionality under regularity conditions. This indicates that the best subset\nregression method can efficiently shrink the full model down to a submodel of a\nsize less than the sample size, which can be analyzed by well-developed\nregression techniques for such cases in a follow-up study. We provide an\niterative algorithm, called orthogonalizing subset selection (OSS), to address\ncomputational issues in best subset regression. OSS is an EM algorithm, and\nthus possesses the monotonicity property. For any sparse estimator, OSS can\nimprove its fit of the model by putting it as an initial point. After this\nimprovement, the sparsity of the estimator is kept. Another appealing feature\nof OSS is that, similarly to an effective algorithm for a continuous\noptimization problem, OSS can converge to the global solution to the \\ell0-norm\nconstrained regression problem if the initial point lies in a neighborhood of\nthe global solution. An accelerating algorithm of OSS and its combination with\nforward stepwise selection are also investigated. Simulations and a real\nexample are presented to evaluate the performances of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2011 13:13:25 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2012 07:29:32 GMT"}], "update_date": "2013-03-20", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1112.1544", "submitter": "Ajay Jasra", "authors": "Alexandros Beskos, Dan Crisan, Ajay Jasra, Nick Whiteley", "title": "Error Bounds and Normalizing Constants for Sequential Monte Carlo in\n  High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Beskos et al (2011), the Sequential Monte Carlo (SMC)\nsampler introduced in Del Moral et al (2006), Neal (2001) has been shown to be\nasymptotically stable in the dimension of the state space d at a cost that is\nonly polynomial in d, when N the number of Monte Carlo samples, is fixed. More\nprecisely, it has been established that the effective sample size (ESS) of the\nensuing (approximate) sample and the Monte Carlo error of fixed dimensional\nmarginals will converge as $d$ grows, with a computational cost of\n$\\mathcal{O}(Nd^2)$. In the present work, further results on SMC methods in\nhigh dimensions are provided as $d\\to\\infty$ and with $N$ fixed. We deduce an\nexplicit bound on the Monte-Carlo error for estimates derived using the SMC\nsampler and the exact asymptotic relative $\\mathbb{L}_2$-error of the estimate\nof the normalizing constant. We also establish marginal propagation of chaos\nproperties of the algorithm. The accuracy in high-dimensions of some\napproximate SMC-based filtering schemes is also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Dec 2011 12:35:41 GMT"}], "update_date": "2011-12-08", "authors_parsed": [["Beskos", "Alexandros", ""], ["Crisan", "Dan", ""], ["Jasra", "Ajay", ""], ["Whiteley", "Nick", ""]]}, {"id": "1112.2889", "submitter": "Juan Jimenez", "authors": "I. Garcia and J. Jimenez", "title": "Estimating financial risk using piecewise Gaussian processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational method for measuring financial risk by estimating\nthe Value at Risk and Expected Shortfall from financial series. We have made\ntwo assumptions: First, that the predictive distributions of the values of an\nasset are conditioned by information on the way in which the variable evolves\nfrom similar conditions, and secondly, that the underlying random processes can\nbe described using piecewise Gaussian processes. The performance of the method\nwas evaluated by using it to estimate VaR and ES for a daily data series taken\nfrom the S&P500 index and applying a backtesting procedure recommended by the\nBasel Committee on Banking Supervision. The results indicated a satisfactory\nperformance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 13:48:23 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Garcia", "I.", ""], ["Jimenez", "J.", ""]]}, {"id": "1112.2986", "submitter": "Peter Imkeller", "authors": "Peter Imkeller, N. Sri Namachchivaya, Nicolas Perkowski, Hoong C.\n  Yeong", "title": "Dimensional reduction in nonlinear filtering: A homogenization approach", "comments": "Published in at http://dx.doi.org/10.1214/12-AAP901 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2013, Vol. 23, No. 6, 2290-2326", "doi": "10.1214/12-AAP901", "report-no": "IMS-AAP-AAP901", "categories": "math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a homogenized filter for multiscale signals, which allows us to\nreduce the dimension of the system. We prove that the nonlinear filter\nconverges to our homogenized filter with rate $\\sqrt{\\varepsilon}$. This is\nachieved by a suitable asymptotic expansion of the dual of the Zakai equation,\nand by probabilistically representing the correction terms with the help of\nBDSDEs.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2011 18:07:19 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 13:49:10 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Imkeller", "Peter", ""], ["Namachchivaya", "N. Sri", ""], ["Perkowski", "Nicolas", ""], ["Yeong", "Hoong C.", ""]]}, {"id": "1112.3692", "submitter": "Mark Huber", "authors": "Mark Huber and Sarah Schott", "title": "Random construction of interpolating sets for high dimensional\n  integration", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional integrals can be reduced to the problem of finding the\nrelative measures of two sets. Often one set will be exponentially larger than\nthe other, making it difficult to compare the sizes. A standard method of\ndealing with this problem is to interpolate between the sets with a sequence of\nnested sets where neighboring sets have relative measures bounded above by a\nconstant. Choosing such a well balanced sequence can be very difficult in\npractice. Here a new approach that automatically creates such sets is\npresented. These well balanced sets allow for faster approximation algorithms\nfor integrals and sums, and better tempering and annealing Markov chains for\ngenerating random samples. Applications such as finding the partition function\nof the Ising model and normalizing constants for posterior distributions in\nBayesian methods are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 00:32:45 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Huber", "Mark", ""], ["Schott", "Sarah", ""]]}, {"id": "1112.3777", "submitter": "Alexandre Brouste", "authors": "Alexandre Brouste and Stefano M. Iacus", "title": "Parameter estimation for the discretely observed fractional\n  Ornstein-Uhlenbeck process and the Yuima R package", "comments": "15 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes consistent and asymptotically Gaussian estimators for the\ndrift, the diffusion coefficient and the Hurst exponent of the discretely\nobserved fractional Ornstein-Uhlenbeck process. For the estimation of the\ndrift, the results are obtained only in the case when 1/2 < H < 3/4. This paper\nalso provides ready-to-use software for the R statistical environment based on\nthe YUIMA package.\n", "versions": [{"version": "v1", "created": "Fri, 16 Dec 2011 12:22:29 GMT"}], "update_date": "2011-12-19", "authors_parsed": [["Brouste", "Alexandre", ""], ["Iacus", "Stefano M.", ""]]}, {"id": "1112.4048", "submitter": "Luca Martino", "authors": "Luca Martino and Victor Pascual Del Olmo and Jesse Read", "title": "A multi-point Metropolis scheme with generic weight functions", "comments": null, "journal-ref": "Statistics & Probability Letters, Volume 82, Issue 7, Pages\n  1445-1453, 2012", "doi": "10.1016/j.spl.2012.04.008", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-point Metropolis algorithm is an advanced MCMC technique based on\ndrawing several correlated samples at each step and choosing one of them\naccording to some normalized weights. We propose a variation of this technique\nwhere the weight functions are not specified, i.e., the analytic form can be\nchosen arbitrarily. This has the advantage of greater flexibility in the design\nof high-performance MCMC samplers. We prove that our method fulfills the\nbalance condition, and provide a numerical simulation. We also give new insight\ninto the functionality of different MCMC algorithms, and the connections\nbetween them.\n", "versions": [{"version": "v1", "created": "Sat, 17 Dec 2011 11:47:30 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2011 11:18:23 GMT"}, {"version": "v3", "created": "Fri, 27 Apr 2012 23:31:17 GMT"}], "update_date": "2012-10-18", "authors_parsed": [["Martino", "Luca", ""], ["Del Olmo", "Victor Pascual", ""], ["Read", "Jesse", ""]]}, {"id": "1112.4160", "submitter": "Richard Brent", "authors": "Richard P. Brent, William Orrick, Judy-anne Osborn, Paul Zimmermann", "title": "Maximal determinants and saturated D-optimal designs of orders 19 and 37", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A saturated D-optimal design is a {+1,-1} square matrix of given order with\nmaximal determinant. We search for saturated D-optimal designs of orders 19 and\n37, and find that known matrices due to Smith, Cohn, Orrick and Solomon are\noptimal. For order 19 we find all inequivalent saturated D-optimal designs with\nmaximal determinant, 2^30 x 7^2 x 17, and confirm that the three known designs\ncomprise a complete set. For order 37 we prove that the maximal determinant is\n2^39 x 3^36, and find a sample of inequivalent saturated D-optimal designs. Our\nmethod is an extension of that used by Orrick to resolve the previously\nsmallest unknown order of 15; and by Chadjipantelis, Kounias and Moyssiadis to\nresolve orders 17 and 21. The method is a two-step computation which first\nsearches for candidate Gram matrices and then attempts to decompose them. Using\na similar method, we also find the complete spectrum of determinant values for\n{+1,-1} matrices of order 13.\n", "versions": [{"version": "v1", "created": "Sun, 18 Dec 2011 14:09:00 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Brent", "Richard P.", ""], ["Orrick", "William", ""], ["Osborn", "Judy-anne", ""], ["Zimmermann", "Paul", ""]]}, {"id": "1112.4671", "submitter": "Richard Brent", "authors": "Richard P. Brent", "title": "Finding D-optimal designs by randomised decomposition and switching", "comments": "18 pages, 3 figures, 5 tables (figures corrected in v4). v5 added a\n  reference and made minor improvements. Presented at the International\n  Workshop on Hadamard Matrices held in honour of Kathy Horadam's 60th\n  birthday, Melbourne, Nov. 2011. Data files are available at\n  http://maths.anu.edu.au/~brent/maxdet/", "journal-ref": "Australasian Journal of Combinatorics 55 (2013), 15-30. Erratum\n  http://maths-people.anu.edu.au/~brent/pub/pub245_errata.html", "doi": null, "report-no": null, "categories": "math.CO cs.DS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hadamard maximal determinant (maxdet) problem is to find the maximum\ndeterminant D(n) of a square {+1, -1} matrix of given order n. Such a matrix\nwith maximum determinant is called a saturated D-optimal design. We consider\nsome cases where n > 2 is not divisible by 4, so the Hadamard bound is not\nattainable, but bounds due to Barba or Ehlich and Wojtas may be attainable. If\nR is a matrix with maximal (or conjectured maximal) determinant, then G = RR^T\nis the corresponding Gram matrix. For the cases that we consider, maximal or\nconjectured maximal Gram matrices are known. We show how to generate many\nHadamard equivalence classes of solutions from a given Gram matrix G, using a\nrandomised decomposition algorithm and row/column switching. In particular, we\nconsider orders 26, 27 and 33, and obtain new saturated D-optimal designs (for\norder 26) and new conjectured saturated D-optimal designs (for orders 27 and\n33).\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 12:41:50 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2011 01:27:17 GMT"}, {"version": "v3", "created": "Mon, 2 Jan 2012 05:00:40 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2012 13:59:04 GMT"}, {"version": "v5", "created": "Sat, 11 Aug 2012 06:37:39 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Brent", "Richard P.", ""]]}, {"id": "1112.4675", "submitter": "Siew Li  Tan", "authors": "Siew Li Tan and David J. Nott", "title": "Variational approximation for mixtures of linear mixed models", "comments": "36 pages, 5 figures, 2 tables, submitted to JCGS", "journal-ref": "Journal of Computational and Graphical Statistics. Volume 23,\n  Issue 2, 2014, pages 564-585", "doi": "10.1080/10618600.2012.761138", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of linear mixed models (MLMMs) are useful for clustering grouped\ndata and can be estimated by likelihood maximization through the EM algorithm.\nThe conventional approach to determining a suitable number of components is to\ncompare different mixture models using penalized log-likelihood criteria such\nas BIC.We propose fitting MLMMs with variational methods which can perform\nparameter estimation and model selection simultaneously. A variational\napproximation is described where the variational lower bound and parameter\nupdates are in closed form, allowing fast evaluation. A new variational greedy\nalgorithm is developed for model selection and learning of the mixture\ncomponents. This approach allows an automatic initialization of the algorithm\nand returns a plausible number of mixture components automatically. In cases of\nweak identifiability of certain model parameters, we use hierarchical centering\nto reparametrize the model and show empirically that there is a gain in\nefficiency by variational algorithms similar to that in MCMC algorithms.\nRelated to this, we prove that the approximate rate of convergence of\nvariational algorithms by Gaussian approximation is equal to that of the\ncorresponding Gibbs sampler which suggests that reparametrizations can lead to\nimproved convergence in variational algorithms as well.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 12:59:09 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2012 09:28:56 GMT"}], "update_date": "2014-05-26", "authors_parsed": [["Tan", "Siew Li", ""], ["Nott", "David J.", ""]]}, {"id": "1112.4755", "submitter": "Scott Sisson", "authors": "D. J. Nott, Y. Fan, L. Marshall, S. A. Sisson", "title": "Approximate Bayesian computation and Bayes linear analysis: Towards\n  high-dimensional ABC", "comments": "To appear in Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayes linear analysis and approximate Bayesian computation (ABC) are\ntechniques commonly used in the Bayesian analysis of complex models. In this\narticle we connect these ideas by demonstrating that regression-adjustment ABC\nalgorithms produce samples for which first and second order moment summaries\napproximate adjusted expectation and variance for a Bayes linear analysis. This\ngives regression-adjustment methods a useful interpretation and role in\nexploratory analysis in high-dimensional problems. As a result, we propose a\nnew method for combining high-dimensional, regression-adjustment ABC with\nlower-dimensional approaches (such as using MCMC for ABC). This method first\nobtains a rough estimate of the joint posterior via regression-adjustment ABC,\nand then estimates each univariate marginal posterior distribution separately\nin a lower-dimensional analysis. The marginal distributions of the initial\nestimate are then modified to equal the separately estimated marginals, thereby\nproviding an improved estimate of the joint posterior. We illustrate this\nmethod with several examples. Supplementary materials for this article are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 20 Dec 2011 16:30:09 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2012 03:02:51 GMT"}], "update_date": "2012-12-10", "authors_parsed": [["Nott", "D. J.", ""], ["Fan", "Y.", ""], ["Marshall", "L.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1112.5006", "submitter": "Gilles Guillot", "authors": "Gilles Guillot, Sabrina Renaud, Ronan Ledevin, Joahn Michaux, Julien\n  Claude", "title": "A Unifying Model for the Analysis of Phenotypic, Genetic and Geographic\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognition of evolutionary units (species, populations) requires integrating\nseveral kinds of data such as genetic or phenotypic markers or spatial\ninformation, in order to get a comprehensive view concerning the\ndifferentiation of the units. We propose a statistical model with a double\noriginal advantage: (i) it incorporates information about the spatial\ndistribution of the samples, with the aim to increase inference power and to\nrelate more explicitly observed patterns to geography; and (ii) it allows one\nto analyze genetic and phenotypic data within a unified model and inference\nframework, thus opening the way to robust comparisons between markers and\npossibly combined analyzes. We show from simulated data as well are real data\nfrom the literature that our method estimates parameters accurately and\nimproves alternative approaches in many situations. The interest of this method\nis exemplified using an intricate case of inter- and intra-species\ndifferentiation based on an original data-set of georeferenced genetic and\nmorphometric markers obtained on {\\em Myodes} voles from Sweden. A computer\nprogram is made available as an extension of the R package Geneland.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 12:34:40 GMT"}], "update_date": "2011-12-22", "authors_parsed": [["Guillot", "Gilles", ""], ["Renaud", "Sabrina", ""], ["Ledevin", "Ronan", ""], ["Michaux", "Joahn", ""], ["Claude", "Julien", ""]]}, {"id": "1112.5016", "submitter": "Ariel Kleiner", "authors": "Ariel Kleiner, Ameet Talwalkar, Purnamrita Sarkar, Michael I. Jordan", "title": "A Scalable Bootstrap for Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap provides a simple and powerful means of assessing the quality\nof estimators. However, in settings involving large datasets---which are\nincreasingly prevalent---the computation of bootstrap-based quantities can be\nprohibitively demanding computationally. While variants such as subsampling and\nthe $m$ out of $n$ bootstrap can be used in principle to reduce the cost of\nbootstrap computations, we find that these methods are generally not robust to\nspecification of hyperparameters (such as the number of subsampled data\npoints), and they often require use of more prior information (such as rates of\nconvergence of estimators) than the bootstrap. As an alternative, we introduce\nthe Bag of Little Bootstraps (BLB), a new procedure which incorporates features\nof both the bootstrap and subsampling to yield a robust, computationally\nefficient means of assessing the quality of estimators. BLB is well suited to\nmodern parallel and distributed computing architectures and furthermore retains\nthe generic applicability and statistical efficiency of the bootstrap. We\ndemonstrate BLB's favorable statistical performance via a theoretical analysis\nelucidating the procedure's properties, as well as a simulation study comparing\nBLB to the bootstrap, the $m$ out of $n$ bootstrap, and subsampling. In\naddition, we present results from a large-scale distributed implementation of\nBLB demonstrating its computational superiority on massive data, a method for\nadaptively selecting BLB's hyperparameters, an empirical study applying BLB to\nseveral real datasets, and an extension of BLB to time series data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Dec 2011 13:18:57 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 03:30:16 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Kleiner", "Ariel", ""], ["Talwalkar", "Ameet", ""], ["Sarkar", "Purnamrita", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1112.5304", "submitter": "Carlo Albert", "authors": "Carlo Albert", "title": "A Mechanistic Dynamic Emulator", "comments": "12 pages, 3 figures", "journal-ref": "J. Nonlinear Analysis B 13, 2747-2754 (2012)", "doi": "10.1016/j.nonrwa.2012.04.003", "report-no": null, "categories": "stat.ME physics.comp-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applied sciences, we often deal with deterministic simulation models that\nare too slow for simulation-intensive tasks such as calibration or real-time\ncontrol. In this paper, an emulator for a generic dynamic model, given by a\nsystem of ordinary non-linear differential equations, is developed. The\nnon-linear differential equations are linearized and Gaussian white noise is\nadded to account for the non-linearities. The resulting linear stochastic\nsystem is conditioned on a set of solutions of the non-linear equations that\nhave been calculated prior to the emulation. A path-integral approach is used\nto derive the Gaussian distribution of the emulated solution. The solution\nreveals that most of the computational burden can be shifted to the\nconditioning phase of the emulator and the complexity of the actual emulation\nstep only scales like $\\mathcal O(Nn)$ in multiplications of matrices of the\ndimension of the state space. Here, $N$ is the number of time-points at which\nthe solution is to be emulated and $n$ the number of solutions the emulator is\nconditioned on.\n  The applicability of the algorithm is demonstrated with the hydrological\nmodel logSPM.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2011 13:27:06 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2012 13:46:06 GMT"}], "update_date": "2012-07-06", "authors_parsed": [["Albert", "Carlo", ""]]}, {"id": "1112.5969", "submitter": "Mikko Tuomi", "authors": "Mikko Tuomi and Hugh R. A. Jones", "title": "Probabilities of exoplanet signals from posterior samplings", "comments": "7 pages, 2 Figs. Accepted for publication in the Astronomy and\n  Astrophysics", "journal-ref": null, "doi": "10.1051/0004-6361/201118114", "report-no": null, "categories": "astro-ph.EP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the marginal likelihoods is an essential feature of model\nselection in the Bayesian context. It is especially crucial to have good\nestimates when assessing the number of planets orbiting stars when the models\nexplain the noisy data with different numbers of Keplerian signals. We\nintroduce a simple method for approximating the marginal likelihoods in\npractice when a statistically representative sample from the parameter\nposterior density is available.\n  We use our truncated posterior mixture estimate to receive accurate model\nprobabilities for models with differing number of Keplerian signals in radial\nvelocity data. We test this estimate in simple scenarios to assess its accuracy\nand rate of convergence in practice when the corresponding estimates calculated\nusing deviance information criterion can be applied to receive trustworthy\nresults for reliable comparison. As a test case, we determine the posterior\nprobability of a planet orbiting HD 3651 given Lick and Keck radial velocity\ndata.\n  The posterior mixture estimate appears to be a simple and an accurate way of\ncalculating marginal integrals from posterior samples. We show, that it can be\nused to estimate the marginal integrals reliably in practice, given a suitable\nselection of parameter \\lambda, that controls its accuracy and convergence\nrate. It is also more accurate than the one block Metropolis-Hastings estimate\nand can be used in any application because it is not based on assumptions on\nthe nature of the posterior density nor the amount of data or parameters in the\nstatistical model.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2011 15:14:53 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2011 13:33:17 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2012 20:16:07 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2012 14:09:54 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Tuomi", "Mikko", ""], ["Jones", "Hugh R. A.", ""]]}, {"id": "1112.6162", "submitter": "Xiaohui Liu", "authors": "Xiaohui Liu, Yijun Zuo, Zhizhong Wang", "title": "Exactly computing bivariate projection depth contours and median", "comments": "15 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among their competitors, projection depth and its induced estimators are very\nfavorable because they can enjoy very high breakdown point robustness without\nhaving to pay the price of low efficiency, meanwhile providing a promising\ncenter-outward ordering of multi-dimensional data. However, their further\napplications have been severely hindered due to their computational challenge\nin practice. In this paper, we derive a simple form of the projection depth\nfunction, when (\\mu, \\sigma) = (Med, MAD). This simple form enables us to\nextend the existing result of point-wise exact computation of projection depth\n(PD) of Zuo and Lai (2011) to depth contours and median for bivariate data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Dec 2011 18:31:21 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Liu", "Xiaohui", ""], ["Zuo", "Yijun", ""], ["Wang", "Zhizhong", ""]]}, {"id": "1112.6212", "submitter": "Xiaochuan Zhao", "authors": "Xiaochuan Zhao, Sheng-Yuan Tu, and Ali H. Sayed", "title": "Diffusion Adaptation over Networks under Imperfect Information Exchange\n  and Non-stationary Data", "comments": "36 pages, 7 figures, to appear in IEEE Transactions on Signal\n  Processing, June 2012", "journal-ref": null, "doi": "10.1109/TSP.2012.2192928", "report-no": null, "categories": "math.OC cs.SI physics.soc-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive networks rely on in-network and collaborative processing among\ndistributed agents to deliver enhanced performance in estimation and inference\ntasks. Information is exchanged among the nodes, usually over noisy links. The\ncombination weights that are used by the nodes to fuse information from their\nneighbors play a critical role in influencing the adaptation and tracking\nabilities of the network. This paper first investigates the mean-square\nperformance of general adaptive diffusion algorithms in the presence of various\nsources of imperfect information exchanges, quantization errors, and model\nnon-stationarities. Among other results, the analysis reveals that link noise\nover the regression data modifies the dynamics of the network evolution in a\ndistinct way, and leads to biased estimates in steady-state. The analysis also\nreveals how the network mean-square performance is dependent on the combination\nweights. We use these observations to show how the combination weights can be\noptimized and adapted. Simulation results illustrate the theoretical findings\nand match well with theory.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2011 01:18:02 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 17:23:07 GMT"}, {"version": "v3", "created": "Thu, 31 May 2012 20:56:11 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Zhao", "Xiaochuan", ""], ["Tu", "Sheng-Yuan", ""], ["Sayed", "Ali H.", ""]]}]