[{"id": "2012.00513", "submitter": "S{\\o}ren B. Vilsen", "authors": "S{\\o}ren B. Vilsen, Torben Tvedebrink, and Poul Svante Eriksen", "title": "DNA mixture deconvolution using an evolutionary algorithm with multiple\n  populations, hill-climbing, and guided mutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DNA samples crime cases analysed in forensic genetics, frequently contain DNA\nfrom multiple contributors. These occur as convolutions of the DNA profiles of\nthe individual contributors to the DNA sample. Thus, in cases where one or more\nof the contributors were unknown, an objective of interest would be the\nseparation, often called deconvolution, of these unknown profiles. In order to\nobtain deconvolutions of the unknown DNA profiles, we introduced a multiple\npopulation evolutionary algorithm (MEA). We allowed the mutation operator of\nthe MEA to utilise that the fitness is based on a probabilistic model and guide\nit by using the deviations between the observed and the expected value for\nevery element of the encoded individual. This guided mutation operator (GM) was\ndesigned such that the larger the deviation the higher probability of mutation.\nFurthermore, the GM was inhomogeneous in time, decreasing to a specified lower\nbound as the number of iterations increased. We analysed 102 two-person DNA\nmixture samples in varying mixture proportions. The samples were quantified\nusing two different DNA prep. kits: (1) Illumina ForenSeq Panel B (30 samples),\nand (2) Applied Biosystems Precision ID Globalfiler NGS STR panel (72 samples).\nThe DNA mixtures were deconvoluted by the MEA and compared to the true DNA\nprofiles of the sample. We analysed three scenarios where we assumed: (1) the\nDNA profile of the major contributor was unknown, (2) DNA profile of the minor\nwas unknown, and (3) both DNA profiles were unknown. Furthermore, we conducted\na series of sensitivity experiments on the ForenSeq panel by varying the\nsub-population size, comparing a completely random homogeneous mutation\noperator to the guided operator with varying mutation decay rates, and allowing\nfor hill-climbing of the parent population.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:23:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Vilsen", "S\u00f8ren B.", ""], ["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""]]}, {"id": "2012.00601", "submitter": "Roee Gutman", "authors": "Edwin Farley and Roee Gutman", "title": "A Bayesian Approach to Linking Data Without Unique Identifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Existing file linkage methods may produce sub-optimal results because they\nconsider neither the interactions between different pairs of matched records\nnor relationships between variables that are exclusive to one of the files. In\naddition, many of the current methods fail to address the uncertainty in the\nlinkage, which may result in overly precise estimates of relationships between\nvariables that are exclusive to one of the files. Bayesian methods for record\nlinkage can reduce the bias in the estimation of scientific relationships of\ninterest and provide interval estimates that account for the uncertainty in the\nlinkage; however, implementation of these methods can often be complex and\ncomputationally intensive. This article presents the GFS package for the R\nprogramming language that utilizes a Bayesian approach for file linkage. The\nlinking procedure implemented in GFS samples from the joint posterior\ndistribution of model parameters and the linking permutations. The algorithm\napproaches file linkage as a missing data problem and generates multiple linked\ndata sets. For computational efficiency, only the linkage permutations are\nstored and multiple analyses are performed using each of the permutations\nseparately. This implementation reduces the computational complexity of the\nlinking process and the expertise required of researchers analyzing linked data\nsets. We describe the algorithm implemented in the GFS package and its\nstatistical basis, and demonstrate its use on a sample data set.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 16:12:52 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Farley", "Edwin", ""], ["Gutman", "Roee", ""]]}, {"id": "2012.00943", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, David B. Dunson", "title": "Spatial Multivariate Trees for Big Data Bayesian Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  High resolution geospatial data are challenging because standard\ngeostatistical models based on Gaussian processes are known to not scale to\nlarge data sizes. While progress has been made towards methods that can be\ncomputed more efficiently, considerably less attention has been devoted to big\ndata methods that allow the description of complex relationships between\nseveral outcomes recorded at high resolutions by different sensors. Our\nBayesian multivariate regression models based on spatial multivariate trees\n(SpamTrees) achieve scalability via conditional independence assumptions on\nlatent random effects following a treed directed acyclic graph.\nInformation-theoretic arguments and considerations on computational efficiency\nguide the construction of the tree and the related efficient sampling\nalgorithms in imbalanced multivariate settings. In addition to simulated data\nexamples, we illustrate SpamTrees using a large climate data set which combines\nsatellite data with land-based station data. Source code is available at\nhttps://github.com/mkln/spamtree\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 03:10:28 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Peruzzi", "Michele", ""], ["Dunson", "David B.", ""]]}, {"id": "2012.01029", "submitter": "Damjan \\v{S}kulj", "authors": "Damjan \\v{S}kulj", "title": "Computing bounds for imprecise continuous-time Markov chains using\n  normal cones", "comments": "32 pages; The paper was presented at the UQOP 2020 conference\n  (http://utopiae.eu/uqop-2020/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.DS math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of imprecise Markov chains has achieved significant progress in\nrecent years. Its applicability, however, is still very much limited, due in\nlarge part to the lack of efficient computational methods for calculating\nhigher-dimensional models. The high computational complexity shows itself\nespecially in the calculation of the imprecise version of the Kolmogorov\nbackward equation. The equation is represented at every point of an interval in\nthe form of a minimization problem, solvable merely with linear programming\ntechniques. Consequently, finding an exact solution on an entire interval is\ninfeasible, whence approximation approaches have been developed. To achieve\nsufficient accuracy, in general, the linear programming optimization methods\nneed to be used in a large number of time points.\n  The principal goal of this paper is to provide a new, more efficient approach\nfor solving the imprecise Kolmogorov backward equation. It is based on the\nLipschitz continuity of the solutions of the equation with respect to time,\ncausing the linear programming problems appearing in proximate points of the\ntime interval to have similar optimal solutions. This property is exploited by\nutilizing the theory of normal cones of convex sets. The present article is\nprimarily devoted to providing the theoretical basis for the novel technique,\nyet, the initial testing shows that in most cases it decisively outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 08:53:13 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["\u0160kulj", "Damjan", ""]]}, {"id": "2012.01520", "submitter": "Daniel Dunlavy", "authors": "Jeremy M. Myers, Daniel M. Dunlavy, Keita Teranishi, D. S. Hollman", "title": "Parameter Sensitivity Analysis of the SparTen High Performance Sparse\n  Tensor Decomposition Software: Extended Analysis", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": "SAND2020-11901R", "categories": "math.NA cs.MS cs.NA cs.PF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor decomposition models play an increasingly important role in modern\ndata science applications. One problem of particular interest is fitting a\nlow-rank Canonical Polyadic (CP) tensor decomposition model when the tensor has\nsparse structure and the tensor elements are nonnegative count data. SparTen is\na high-performance C++ library which computes a low-rank decomposition using\ndifferent solvers: a first-order quasi-Newton or a second-order damped Newton\nmethod, along with the appropriate choice of runtime parameters. Since default\nparameters in SparTen are tuned to experimental results in prior published work\non a single real-world dataset conducted using MATLAB implementations of these\nmethods, it remains unclear if the parameter defaults in SparTen are\nappropriate for general tensor data. Furthermore, it is unknown how sensitive\nalgorithm convergence is to changes in the input parameter values. This report\naddresses these unresolved issues with large-scale experimentation on three\nbenchmark tensor data sets. Experiments were conducted on several different CPU\narchitectures and replicated with many initial states to establish generalized\nprofiles of algorithm convergence behavior.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 20:47:29 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Myers", "Jeremy M.", ""], ["Dunlavy", "Daniel M.", ""], ["Teranishi", "Keita", ""], ["Hollman", "D. S.", ""]]}, {"id": "2012.02293", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe J Medina-Aguayo, J Andr\\'es Christen", "title": "Penalised t-walk MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Handling multimodality that commonly arises from complicated statistical\nmodels remains a challenge. Current Markov chain Monte Carlo (MCMC) methodology\ntackling this subject is based on an ensemble of chains targeting a product of\npower-tempered distributions. Despite the theoretical validity of such methods,\npractical implementations typically suffer from bad mixing and slow convergence\ndue to the high-computation cost involved. In this work we study novel\nextensions of the t-walk algorithm, an existing MCMC method that is inexpensive\nand invariant to affine transformations of the state space, for dealing with\nmultimodal distributions. We acknowledge that the effectiveness of the new\nmethod will be problem dependent and might struggle in complex scenarios; for\nsuch cases we propose a post-processing technique based on pseudo-marginal\ntheory for combining isolated samples.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 21:56:13 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Medina-Aguayo", "Felipe J", ""], ["Christen", "J Andr\u00e9s", ""]]}, {"id": "2012.02302", "submitter": "Cai Li", "authors": "Cai Li, Luo Xiao, Sheng Luo", "title": "Joint Model for Survival and Multivariate Sparse Functional Data with\n  Application to a Study of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13427", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Studies of Alzheimer's disease (AD) often collect multiple longitudinal\nclinical outcomes, which are correlated and predictive of AD progression. It is\nof great scientific interest to investigate the association between the\noutcomes and time to AD onset. We model the multiple longitudinal outcomes as\nmultivariate sparse functional data and propose a functional joint model\nlinking multivariate functional data to event time data. In particular, we\npropose a multivariate functional mixed model (MFMM) to identify the shared\nprogression pattern and outcome-specific progression patterns of the outcomes,\nwhich enables more interpretable modeling of associations between outcomes and\nAD onset. The proposed method is applied to the Alzheimer's Disease\nNeuroimaging Initiative study (ADNI) and the functional joint model sheds new\nlight on inference of five longitudinal outcomes and their associations with AD\nonset. Simulation studies also confirm the validity of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:04:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Cai", ""], ["Xiao", "Luo", ""], ["Luo", "Sheng", ""]]}, {"id": "2012.02395", "submitter": "Peter Hansen", "authors": "Ilya Archakov and Peter Reinhard Hansen", "title": "A New Parametrization of Correlation Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel parametrization of the correlation matrix. The\nreparametrization facilitates modeling of correlation and covariance matrices\nby an unrestricted vector, where positive definiteness is an innate property.\nThis parametrization can be viewed as a generalization of Fisther's\nZ-transformation to higher dimensions and has a wide range of potential\napplications. An algorithm for reconstructing the unique n x n correlation\nmatrix from any d-dimensional vector (with d = n(n-1)/2) is provided, and we\nderive its numerical complexity.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 04:14:47 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Archakov", "Ilya", ""], ["Hansen", "Peter Reinhard", ""]]}, {"id": "2012.02704", "submitter": "Sergei Manzhos", "authors": "Owen Ren, Mohamed Ali Boussaidi, Dmitry Voytsekhovsky, Manabu Ihara,\n  and Sergei Manzhos", "title": "Random Sampling High Dimensional Model Representation Gaussian Process\n  Regression (RS-HDMR-GPR): a Python module for representing multidimensional\n  functions with machine-learned lower-dimensional terms", "comments": "48 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a Python implementation for RS-HDMR-GPR (Random Sampling High\nDimensional Model Representation Gaussian Process Regression). The method\nbuilds representations of multivariate functions with lower-dimensional terms,\neither as an expansion over orders of coupling or using terms of only a given\ndimensionality. This facilitates, in particular, recovering functional\ndependence from sparse data. The code also allows for imputation of missing\nvalues of the variables and for a significant pruning of the useful number of\nHDMR terms. The code can also be used for estimating relative importance of\ndifferent combinations of input variables, thereby adding an element of insight\nto a general machine learning method. The capabilities of this regression tool\nare demonstrated on test cases involving synthetic analytic functions, the\npotential energy surface of the water molecule, kinetic energy densities of\nmaterials (crystalline magnesium, aluminum, and silicon), and financial market\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:12:05 GMT"}, {"version": "v2", "created": "Mon, 15 Feb 2021 18:12:59 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2021 10:01:57 GMT"}, {"version": "v4", "created": "Thu, 8 Jul 2021 10:27:38 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Ren", "Owen", ""], ["Boussaidi", "Mohamed Ali", ""], ["Voytsekhovsky", "Dmitry", ""], ["Ihara", "Manabu", ""], ["Manzhos", "Sergei", ""]]}, {"id": "2012.02750", "submitter": "Geoffrey Bomarito", "authors": "Geoffrey F. Bomarito and Patrick E. Leser and James E. Warner and\n  William P. Leser", "title": "On the Optimization of Approximate Control Variates with Parametrically\n  Defined Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-model Monte Carlo methods, such as multi-level Monte Carlo (MLMC) and\nmultifidelity Monte Carlo (MFMC), allow for efficient estimation of the\nexpectation of a quantity of interest given a set of models of varying\nfidelities. Recently, it was shown that the MLMC and MFMC estimators are both\ninstances of the approximate control variates (ACV) framework [Gorodetsky et\nal. 2020]. In that same work, it was also shown that hand-tailored ACV\nestimators could outperform MLMC and MFMC for a variety of model scenarios.\nBecause there is no reason to believe that these hand-tailored estimators are\nthe best among a myriad of possible ACV estimators, a more general approach to\nestimator construction is pursued in this work. First, a general form of the\nACV estimator variance is formulated. Then, the formulation is utilized to\ngenerate parametrically-defined estimators. These parametrically-defined\nestimators allow for an optimization to be pursued over a larger domain of\npossible ACV estimators. The parametrically-defined estimators are tested on a\nlarge set of model scenarios, and it is found that the broader search domain\nenabled by parametrically-defined estimators leads to greater variance\nreduction.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:00:57 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Bomarito", "Geoffrey F.", ""], ["Leser", "Patrick E.", ""], ["Warner", "James E.", ""], ["Leser", "William P.", ""]]}, {"id": "2012.03103", "submitter": "Nianqiao Ju", "authors": "Jeremy Heng, Pierre E. Jacob, Nianqiao Ju", "title": "A simple Markov chain for independent Bernoulli variables conditioned on\n  their sum", "comments": "16 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a vector of $N$ independent binary variables, each with a\ndifferent probability of success. The distribution of the vector conditional on\nits sum is known as the conditional Bernoulli distribution. Assuming that $N$\ngoes to infinity and that the sum is proportional to $N$, exact sampling costs\norder $N^2$, while a simple Markov chain Monte Carlo algorithm using 'swaps'\nhas constant cost per iteration. We provide conditions under which this Markov\nchain converges in order $N \\log N$ iterations. Our proof relies on couplings\nand an auxiliary Markov chain defined on a partition of the space into\nfavorable and unfavorable pairs.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:00:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heng", "Jeremy", ""], ["Jacob", "Pierre E.", ""], ["Ju", "Nianqiao", ""]]}, {"id": "2012.03761", "submitter": "Raghu Pasupathy", "authors": "Raghu Pasupathy and Yongjia Song", "title": "Adaptive Sequential SAA for Solving Two-stage Stochastic Linear Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present adaptive sequential SAA (sample average approximation) algorithms\nto solve large-scale two-stage stochastic linear programs. The iterative\nalgorithm framework we propose is organized into \\emph{outer} and \\emph{inner}\niterations as follows: during each outer iteration, a sample-path problem is\nimplicitly generated using a sample of observations or ``scenarios,\" and solved\nonly \\emph{imprecisely}, to within a tolerance that is chosen\n\\emph{adaptively}, by balancing the estimated statistical error against\nsolution error. The solutions from prior iterations serve as \\emph{warm starts}\nto aid efficient solution of the (piecewise linear convex) sample-path\noptimization problems generated on subsequent iterations. The generated\nscenarios can be independent and identically distributed (iid), or dependent,\nas in Monte Carlo generation using Latin-hypercube sampling, antithetic\nvariates, or randomized quasi-Monte Carlo. We first characterize the\nalmost-sure convergence (and convergence in mean) of the optimality gap and the\ndistance of the generated stochastic iterates to the true solution set. We then\ncharacterize the corresponding iteration complexity and work complexity rates\nas a function of the sample size schedule, demonstrating that the best\nachievable work complexity rate is Monte Carlo canonical and analogous to the\ngeneric $\\mathcal{O}(\\epsilon^{-2})$ optimal complexity for non-smooth convex\noptimization. We report extensive numerical tests that indicate favorable\nperformance, due primarily to the use of a sequential framework with an optimal\nsample size schedule, and the use of warm starts. The proposed algorithm can be\nstopped in finite-time to return a solution endowed with a probabilistic\nguarantee on quality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:58:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Pasupathy", "Raghu", ""], ["Song", "Yongjia", ""]]}, {"id": "2012.04245", "submitter": "Matthias Sachs", "authors": "Benedict Leimkuhler and Matthias Sachs", "title": "Efficient Numerical Algorithms for the Generalized Langevin Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cond-mat.mtrl-sci cs.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the design and implementation of numerical methods to solve the\ngeneralized Langevin equation (GLE) focusing on canonical sampling properties\nof numerical integrators. For this purpose, we cast the GLE in an extended\nphase space formulation and derive a family of splitting methods which\ngeneralize existing Langevin dynamics integration methods. We show exponential\nconvergence in law and the validity of a central limit theorem for the Markov\nchains obtained via these integration methods, and we show that the dynamics of\na suggested integration scheme is consistent with asymptotic limits of the\nexact dynamics and can reproduce (in the short memory limit) a superconvergence\nproperty for the analogous splitting of underdamped Langevin dynamics. We then\napply our proposed integration method to several model systems, including a\nBayesian inference problem. We demonstrate in numerical experiments that our\nmethod outperforms other proposed GLE integration schemes in terms of the\naccuracy of sampling. Moreover, using a parameterization of the memory kernel\nin the GLE as proposed by Ceriotti et al [9], our experiments indicate that the\nobtained GLE-based sampling scheme outperforms state-of-the-art sampling\nschemes based on underdamped Langevin dynamics in terms of robustness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 06:49:00 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Leimkuhler", "Benedict", ""], ["Sachs", "Matthias", ""]]}, {"id": "2012.04378", "submitter": "Christoph Schlembach", "authors": "Christoph Schlembach, Sascha L. Schmidt, Dominik Schreyer, Linus\n  Wunderlich", "title": "Forecasting the Olympic medal distribution during a pandemic: a\n  socio-economic machine learning model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the number of Olympic medals for each nation is highly relevant\nfor different stakeholders: Ex ante, sports betting companies can determine the\nodds while sponsors and media companies can allocate their resources to\npromising teams. Ex post, sports politicians and managers can benchmark the\nperformance of their teams and evaluate the drivers of success. To\nsignificantly increase the Olympic medal forecasting accuracy, we apply machine\nlearning, more specifically a two-staged Random Forest, thus outperforming more\ntraditional na\\\"ive forecast for three previous Olympics held between 2008 and\n2016 for the first time. Regarding the Tokyo 2020 Games in 2021, our model\nsuggests that the United States will lead the Olympic medal table, winning 120\nmedals, followed by China (87) and Great Britain (74). Intriguingly, we predict\nthat the current COVID-19 pandemic will not significantly alter the medal count\nas all countries suffer from the pandemic to some extent (data inherent) and\nlimited historical data points on comparable diseases (model inherent).\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 11:50:14 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:35:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schlembach", "Christoph", ""], ["Schmidt", "Sascha L.", ""], ["Schreyer", "Dominik", ""], ["Wunderlich", "Linus", ""]]}, {"id": "2012.04646", "submitter": "Yang Feng", "authors": "Sihan Huang, Haolei Weng, Yang Feng", "title": "Spectral clustering via adaptive layer aggregation for multi-layer\n  networks", "comments": "71 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in network analysis is detecting community\nstructure in multi-layer networks, of which each layer represents one type of\nedge information among the nodes. We propose integrative spectral clustering\napproaches based on effective convex layer aggregations. Our aggregation\nmethods are strongly motivated by a delicate asymptotic analysis of the\nspectral embedding of weighted adjacency matrices and the downstream $k$-means\nclustering, in a challenging regime where community detection consistency is\nimpossible. In fact, the methods are shown to estimate the optimal convex\naggregation, which minimizes the mis-clustering error under some specialized\nmulti-layer network models. Our analysis further suggests that clustering using\nGaussian mixture models is generally superior to the commonly used $k$-means in\nspectral clustering. Extensive numerical studies demonstrate that our adaptive\naggregation techniques, together with Gaussian mixture model clustering, make\nthe new spectral clustering remarkably competitive compared to several\npopularly used methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:58:18 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Huang", "Sihan", ""], ["Weng", "Haolei", ""], ["Feng", "Yang", ""]]}, {"id": "2012.04798", "submitter": "Niloy Biswas", "authors": "Niloy Biswas, Anirban Bhattacharya, Pierre E. Jacob, James E. Johndrow", "title": "Coupling-based convergence assessment of some Gibbs samplers for\n  high-dimensional Bayesian regression with shrinkage priors", "comments": "61 pages, 9 figures. Alternative coupling strategies, a second GWAS\n  example, and some results on the meeting times of the coupling algorithms\n  have been added. R package available at\n  https://github.com/niloyb/CoupledHalfT", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider Markov chain Monte Carlo (MCMC) algorithms for Bayesian\nhigh-dimensional regression with continuous shrinkage priors. A common\nchallenge with these algorithms is the choice of the number of iterations to\nperform. This is critical when each iteration is expensive, as is the case when\ndealing with modern data sets, such as genome-wide association studies with\nthousands of rows and up to hundred of thousands of columns. We develop\ncoupling techniques tailored to the setting of high-dimensional regression with\nshrinkage priors, which enable practical, non-asymptotic diagnostics of\nconvergence without relying on traceplots or long-run asymptotics. By\nestablishing geometric drift and minorization conditions for the algorithm\nunder consideration, we prove that the proposed couplings have finite expected\nmeeting time. Focusing on a class of shrinkage priors which includes the\n'Horseshoe', we empirically demonstrate the scalability of the proposed\ncouplings. A highlight of our findings is that less than 1000 iterations can be\nenough for a Gibbs sampler to reach stationarity in a regression on 100,000\ncovariates. The numerical results also illustrate the impact of the prior on\nthe computational efficiency of the coupling, and suggest the use of priors\nwhere the local precisions are Half-t distributed with degree of freedom larger\nthan one.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 00:16:40 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 23:51:33 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 03:03:51 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Biswas", "Niloy", ""], ["Bhattacharya", "Anirban", ""], ["Jacob", "Pierre E.", ""], ["Johndrow", "James E.", ""]]}, {"id": "2012.05294", "submitter": "Imelda Trejo", "authors": "Imelda Trejo and Nicolas Hengartner", "title": "A modified Susceptible-Infected-Recovered model for observed\n  under-reported incidence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting Susceptible-Infected-Recovered (SIR) models to incidence data is\nproblematic when a fraction $q$ of the infected individuals are not reported.\nAssuming an underlying SIR model with general but known distribution for the\ntime to recovery, this paper derives the implied differential-integral\nequations for observed incidence data when a fixed fraction $q$ of newly\ninfected individuals are not observed. The parameters of the resulting system\nof differential equations are identifiable. Using these differential equations,\nwe develop a stochastic model for the conditional distribution of current\ndisease incidence given the entire past history of incidences. This results in\nan epidemic model that can track complex epidemic dynamics, such as outbreaks\nwith multiple waves. We propose to estimate of model parameters using Bayesian\nMonte-Carlo Markov Chain sampling of the posterior distribution. We apply our\nmodel to estimate the infection rate and fraction of asymptomatic individuals\nfor the current Coronavirus 2019 outbreak in eight countries in North and South\nAmerica. Our analysis reveals that consistently, about 70-90\\% of infected\nindividuals were not observed in the American outbreaks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:05:49 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Trejo", "Imelda", ""], ["Hengartner", "Nicolas", ""]]}, {"id": "2012.05298", "submitter": "J. Cricelio Montesinos-L\\'opez", "authors": "J. Cricelio Montesinos-L\\'opez, Antonio Capella, J. Andr\\'es Christen\n  and Josu\\'e Tago", "title": "Uncertainty quantification for fault slip inversion", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.geo-ph stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient Bayesian approach to infer a fault displacement from\ngeodetic data in a slow slip event. Our physical model of the slip process\nreduces to a multiple linear regression subject to constraints. Assuming a\nGaussian model for the geodetic data and considering a multivariate truncated\nnormal prior distribution for the unknown fault slip, the resulting posterior\ndistribution is also multivariate truncated normal. Regarding the posterior, we\npropose an algorithm based on Optimal Directional Gibbs that allows us to\nefficiently sample from the resulting high-dimensional posterior distribution\nof along dip and along strike movements of our fault grid division. A synthetic\nfault slip example illustrates the flexibility and accuracy of the proposed\napproach. The methodology is also applied to a real data set, for the 2006\nGuerrero, Mexico, Slow Slip Event, where the objective is to recover the fault\nslip on a known interface that produces displacements observed at ground\ngeodetic stations. As a by-product of our approach, we are able to estimate\nmoment magnitude for the 2006 Guerrero Event with uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:18:48 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 00:57:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Montesinos-L\u00f3pez", "J. Cricelio", ""], ["Capella", "Antonio", ""], ["Christen", "J. Andr\u00e9s", ""], ["Tago", "Josu\u00e9", ""]]}, {"id": "2012.05351", "submitter": "Maikol Sol\\'is", "authors": "Alberto J Hern\\'andez and Maikol Sol\\'is and Ronald A.\n  Z\\'u\\~niga-Rojas", "title": "Estimation of first-order sensitivity indices based on symmetric\n  reflected Vietoris-Rips complexes areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we estimate the first-order sensitivity index of random\nvariables within a model by reconstructing the embedding manifold of a\ntwo-dimensional cloud point. The model assumed has p predictors and a\ncontinuous outcome Y . Our method gauges the manifold through a Vietoris-Rips\ncomplex with a fixed radius for each variable. With this object, and using the\narea and its symmetric reflection, we can estimate an index of relevance for\neach predictor. The index reveals the geometric nature of the data points.\nAlso, given the method used, we can decide whether a pair of non-correlated\nrandom variables have some structural pattern in their interaction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:45:16 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hern\u00e1ndez", "Alberto J", ""], ["Sol\u00eds", "Maikol", ""], ["Z\u00fa\u00f1iga-Rojas", "Ronald A.", ""]]}, {"id": "2012.05405", "submitter": "Angus McLure", "authors": "Angus McLure, Ben O'Neill, Helen Mayfield, Colleen Lau, Brady\n  McPherson", "title": "PoolTestR: An R package for estimating prevalence and regression\n  modelling with pooled samples", "comments": "18 pages, 2 figures, 1 table, 2 boxes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pooled testing (also known as group testing), where diagnostic tests are\nperformed on pooled samples, has broad applications in the surveillance of\ndiseases in animals and humans. An increasingly common use case is molecular\nxenomonitoring (MX), where surveillance of vector-borne diseases is conducted\nby capturing and testing large numbers of vectors (e.g. mosquitoes). The R\npackage PoolTestR was developed to meet the needs of increasingly large and\ncomplex molecular xenomonitoring surveys but can be applied to analyse any data\ninvolving pooled testing. PoolTestR includes simple and flexible tools to\nestimate prevalence and fit fixed- and mixed-effect generalised linear models\nfor pooled data in frequentist and Bayesian frameworks. Mixed-effect models\nallow users to account for the hierarchical sampling designs that are often\nemployed in surveys, including MX. We demonstrate the utility of PoolTestR by\napplying it to a large synthetic dataset that emulates a MX survey with a\nhierarchical sampling design.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 01:53:20 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["McLure", "Angus", ""], ["O'Neill", "Ben", ""], ["Mayfield", "Helen", ""], ["Lau", "Colleen", ""], ["McPherson", "Brady", ""]]}, {"id": "2012.05668", "submitter": "Mikkel Bue Lykkegaard", "authors": "Mikkel B. Lykkegaard and Grigorios Mingas and Robert Scheichl and\n  Colin Fox and Tim J. Dodwell", "title": "Multilevel Delayed Acceptance MCMC with an Adaptive Error Model in PyMC3", "comments": "8 pages, 4 figures, accepted for Machine Learning for Engineering\n  Modeling, Simulation, and Design Workshop at Neural Information Processing\n  Systems 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty Quantification through Markov Chain Monte Carlo (MCMC) can be\nprohibitively expensive for target probability densities with expensive\nlikelihood functions, for instance when the evaluation it involves solving a\nPartial Differential Equation (PDE), as is the case in a wide range of\nengineering applications. Multilevel Delayed Acceptance (MLDA) with an Adaptive\nError Model (AEM) is a novel approach, which alleviates this problem by\nexploiting a hierarchy of models, with increasing complexity and cost, and\ncorrecting the inexpensive models on-the-fly. The method has been integrated\nwithin the open-source probabilistic programming package PyMC3 and is available\nin the latest development version. In this paper, the algorithm is presented\nalong with an illustrative example.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:49:10 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Lykkegaard", "Mikkel B.", ""], ["Mingas", "Grigorios", ""], ["Scheichl", "Robert", ""], ["Fox", "Colin", ""], ["Dodwell", "Tim J.", ""]]}, {"id": "2012.05722", "submitter": "Maren Hackenberg", "authors": "Maren Hackenberg, Marlon Grodd, Clemens Kreutz, Martina Fischer,\n  Janina Esins, Linus Grabenhenrich, Christian Karagiannidis, Harald Binder", "title": "Using Differentiable Programming for Flexible Statistical Modeling", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differentiable programming has recently received much interest as a paradigm\nthat facilitates taking gradients of computer programs. While the corresponding\nflexible gradient-based optimization approaches so far have been used\npredominantly for deep learning or enriching the latter with modeling\ncomponents, we want to demonstrate that they can also be useful for statistical\nmodeling per se, e.g., for quick prototyping when classical maximum likelihood\napproaches are challenging or not feasible. In an application from a COVID-19\nsetting, we utilize differentiable programming to quickly build and optimize a\nflexible prediction model adapted to the data quality challenges at hand.\nSpecifically, we develop a regression model, inspired by delay differential\nequations, that can bridge temporal gaps of observations in the central German\nregistry of COVID-19 intensive care cases for predicting future demand. With\nthis exemplary modeling challenge, we illustrate how differentiable programming\ncan enable simple gradient-based optimization of the model by automatic\ndifferentiation. This allowed us to quickly prototype a model under time\npressure that outperforms simpler benchmark models. We thus exemplify the\npotential of differentiable programming also outside deep learning\napplications, to provide more options for flexible applied statistical\nmodeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 12:33:49 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Hackenberg", "Maren", ""], ["Grodd", "Marlon", ""], ["Kreutz", "Clemens", ""], ["Fischer", "Martina", ""], ["Esins", "Janina", ""], ["Grabenhenrich", "Linus", ""], ["Karagiannidis", "Christian", ""], ["Binder", "Harald", ""]]}, {"id": "2012.05967", "submitter": "Matthias Katzfuss", "authors": "Brian Kidd, Matthias Katzfuss", "title": "Bayesian nonstationary and nonparametric covariance estimation for large\n  spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, it is often assumed that the spatial field of interest\nis stationary and its covariance has a simple parametric form, but these\nassumptions are not appropriate in many applications. Given replicate\nobservations of a Gaussian spatial field, we propose nonstationary and\nnonparametric Bayesian inference on the spatial dependence. Instead of\nestimating the quadratic (in the number of spatial locations) entries of the\ncovariance matrix, the idea is to infer a near-linear number of nonzero entries\nin a sparse Cholesky factor of the precision matrix. Our prior assumptions are\nmotivated by recent results on the exponential decay of the entries of this\nCholesky factor for Matern-type covariances under a specific ordering scheme.\nOur methods are highly scalable and parallelizable. We conduct numerical\ncomparisons and apply our methodology to climate-model output, enabling\nstatistical emulation of an expensive physical model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:48:43 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kidd", "Brian", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "2012.06102", "submitter": "Arsalane Chouaib Guidoum", "authors": "Arsalane Chouaib Guidoum", "title": "Kernel Estimator and Bandwidth Selection for Density and its\n  Derivatives: The kedd Package", "comments": "22 pages, 9 figures, part of the kedd package", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The kedd package providing additional smoothing techniques to the R\nstatistical system. Although various packages on the Comprehensive R Archive\nNetwork (CRAN) provide functions useful to nonparametric statistics, kedd aims\nto serve as a central location for more specifically of a nonparametric\nfunctions and data sets. The current feature set of the package can be split in\nfour main categories: compute the convolutions and derivatives of a kernel\nfunction, compute the kernel estimators for a density of probability and its\nderivatives, computing the bandwidth selectors with different methods,\ndisplaying the kernel estimators and selection functions of the bandwidth.\nMoreover, the package follows the general R philosophy of working with model\nobjects. This means that instead of merely returning, say, a kernel estimator\nof rth derivative of a density, many functions will return an object\ncontaining, it's functions are S3 classes (S3method). The object can then be\nmanipulated at one's will using various extraction, summary or plotting\nfunctions. Whenever possible, we develop a graphical user interface of the\nvarious functions of a coherent whole, to facilitate the use of this package.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 03:49:27 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Guidoum", "Arsalane Chouaib", ""]]}, {"id": "2012.06270", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Handy Formulas for Binomial Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the relevance of the binomial distribution for probability theory and\napplied statistical inference, its higher-order moments are poorly understood.\nThe existing formulas are either not general enough, or not structured or\nsimplified enough for intended applications. This paper introduces novel\nformulas for binomial moments, in terms of \\emph{variance} rather than success\nprobability. The obtained formulas are arguably better structured and simpler\ncompared to prior works. In addition, the paper presents algorithms to derive\nthese formulas along with working implementation in the Python symbolic algebra\npackage. The novel approach is a combinatorial argument coupled with clever\nalgebraic simplifications which rely on symmetrization theory. As an\ninteresting byproduct we establish \\emph{asymptotically sharp estimates for\ncentral binomial moments}, improving upon partial results from prior works.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:04:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2012.06287", "submitter": "Michael Stephanou", "authors": "Michael Stephanou and Melvin Varughese", "title": "Sequential estimation of Spearman rank correlation using Hermite series\n  estimators", "comments": "Revised article incorporating comments by the reviewers, associate\n  editor and editor of the Journal of Multivariate Analysis (accepted\n  manuscript). Changes include a modified title and a significantly shortened\n  article", "journal-ref": null, "doi": "10.1016/j.jmva.2021.104783", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this article we describe a new Hermite series based sequential estimator\nfor the Spearman rank correlation coefficient and provide algorithms applicable\nin both the stationary and non-stationary settings. To treat the non-stationary\nsetting, we introduce a novel, exponentially weighted estimator for the\nSpearman rank correlation, which allows the local nonparametric correlation of\na bivariate data stream to be tracked. To the best of our knowledge this is the\nfirst algorithm to be proposed for estimating a time varying Spearman rank\ncorrelation that does not rely on a moving window approach. We explore the\npractical effectiveness of the Hermite series based estimators through real\ndata and simulation studies demonstrating good practical performance. The\nsimulation studies in particular reveal competitive performance compared to an\nexisting algorithm. The potential applications of this work are manifold. The\nHermite series based Spearman rank correlation estimator can be applied to fast\nand robust online calculation of correlation which may vary over time. Possible\nmachine learning applications include, amongst others, fast feature selection\nand hierarchical clustering on massive data sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:43:19 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 09:46:09 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Stephanou", "Michael", ""], ["Varughese", "Melvin", ""]]}, {"id": "2012.06397", "submitter": "Florian Heinemann", "authors": "Florian Heinemann, Axel Munk, Yoav Zemel", "title": "Randomised Wasserstein Barycenter Computation: Resampling with\n  Statistical Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid resampling method to approximate finitely supported\nWasserstein barycenters on large-scale datasets, which can be combined with any\nexact solver. Nonasymptotic bounds on the expected error of the objective value\nas well as the barycenters themselves allow to calibrate computational cost and\nstatistical accuracy. The rate of these upper bounds is shown to be optimal and\nindependent of the underlying dimension, which appears only in the constants.\nUsing a simple modification of the subgradient descent algorithm of Cuturi and\nDoucet, we showcase the applicability of our method on a myriad of simulated\ndatasets, as well as a real-data example from cell microscopy which are out of\nreach for state of the art algorithms for computing Wasserstein barycenters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:54:04 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 13:45:18 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Heinemann", "Florian", ""], ["Munk", "Axel", ""], ["Zemel", "Yoav", ""]]}, {"id": "2012.07429", "submitter": "David Rossell", "authors": "David Rossell, Oriol Abril, Anirban Bhattacharya", "title": "Approximate Laplace approximations for scalable model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the approximate Laplace approximation (ALA) to evaluate integrated\nlikelihoods, a bottleneck in Bayesian model selection. The Laplace\napproximation (LA) is a popular tool that speeds up such computation and equips\nstrong model selection properties. However, when the sample size is large or\none considers many models the cost of the required optimizations becomes\nimpractical. ALA reduces the cost to that of solving a least-squares problem\nfor each model. Further, it enables efficient computation across models such as\nsharing pre-computed sufficient statistics and certain operations in matrix\ndecompositions. We prove that in generalized (possibly non-linear) models ALA\nachieves a strong form of model selection consistency for a suitably-defined\noptimal model, at the same functional rates as exact computation. We consider\nfixed- and high-dimensional problems, group and hierarchical constraints, and\nthe possibility that all models are misspecified. We also obtain ALA rates for\nGaussian regression under non-local priors, an important example where the LA\ncan be costly and does not consistently estimate the integrated likelihood. Our\nexamples include non-linear regression, logistic, Poisson and survival models.\nWe implement the methodology in the R package mombf.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 11:29:11 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 08:06:01 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Rossell", "David", ""], ["Abril", "Oriol", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "2012.08015", "submitter": "Annie Sauer", "authors": "Annie Sauer, Robert B. Gramacy, David Higdon", "title": "Active Learning for Deep Gaussian Process Surrogates", "comments": "34 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) are increasingly popular as predictive models\nin machine learning (ML) for their non-stationary flexibility and ability to\ncope with abrupt regime changes in training data. Here we explore DGPs as\nsurrogates for computer simulation experiments whose response surfaces exhibit\nsimilar characteristics. In particular, we transport a DGP's automatic warping\nof the input space and full uncertainty quantification (UQ), via a novel\nelliptical slice sampling (ESS) Bayesian posterior inferential scheme, through\nto active learning (AL) strategies that distribute runs non-uniformly in the\ninput space -- something an ordinary (stationary) GP could not do. Building up\nthe design sequentially in this way allows smaller training sets, limiting both\nexpensive evaluation of the simulator code and mitigating cubic costs of DGP\ninference. When training data sizes are kept small through careful acquisition,\nand with parsimonious layout of latent layers, the framework can be both\neffective and computationally tractable. Our methods are illustrated on\nsimulation data and two real computer experiments of varying input\ndimensionality. We provide an open source implementation in the \"deepgp\"\npackage on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 00:09:37 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Sauer", "Annie", ""], ["Gramacy", "Robert B.", ""], ["Higdon", "David", ""]]}, {"id": "2012.08065", "submitter": "Lei Yan", "authors": "Lei Yan, Xin Chen", "title": "Certifiably Optimal Sparse Sufficient Dimension Reduction", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction (SDR) is a popular tool in regression\nanalysis, which replaces the original predictors with a minimal set of their\nlinear combinations. However, the estimated linear combinations generally\ncontain all original predictors, which brings difficulties in interpreting the\nresults, especially when the number of predictors is large. In this paper, we\npropose a customized branch and bound algorithm, optimal sparse generalized\neigenvalue problem (Optimal SGEP), which combines a SGEP formulation of many\nSDR methods and efficient and accurate bounds allowing the algorithm to\nconverge quickly. Optimal SGEP exactly solves the underlying non-convex\noptimization problem and thus produces certifiably optimal solutions. We\ndemonstrate the effectiveness of the proposed algorithm through simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 03:23:15 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Yan", "Lei", ""], ["Chen", "Xin", ""]]}, {"id": "2012.08089", "submitter": "Pratishtha Batra", "authors": "Pratishtha Batra, Neil A. Spencer and Pritam Ranjan", "title": "IsoCheck: An R Package to check Isomorphism for Two-level Factorial\n  Designs with Randomization Restrictions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs are often used in various industrial and sociological\nexperiments to identify significant factors and factor combinations that may\naffect the process response. In the statistics literature, several studies have\ninvestigated the analysis, construction, and isomorphism of factorial and\nfractional factorial designs. When there are multiple choices for a design, it\nis helpful to have an easy-to-use tool for identifying which are distinct, and\nwhich of those can be efficiently analyzed/has good theoretical properties. For\nthis task, we present an R library called IsoCheck that checks the isomorphism\nof multi-stage 2^n factorial experiments with randomization restrictions.\nThrough representing the factors and their combinations as a finite projective\ngeometry, IsoCheck recasts the problem of searching over all possible\nrelabelings as a search over collineations, then exploits projective geometric\nproperties of the space to make the search much more efficient. Furthermore, a\nbitstring representation of the factorial effects is used to characterize all\npossible rearrangements of designs, thus facilitating quick comparisons after\nrelabeling. We present several examples with R code to illustrate the usage of\nthe main functions in IsoCheck. Besides checking equivalence and isomorphism of\n2^n multi-stage factorial designs, we demonstrate how the functions of the\npackage can be used to create a catalog of all non-isomorphic designs, and\nsubsequently rank these designs based on a suitably defined ranking criterion.\nIsoCheck is free software and distributed under the General Public License and\navailable from the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:55:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Batra", "Pratishtha", ""], ["Spencer", "Neil A.", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2012.08647", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak and Weicong Yuan", "title": "Computation-free Nonparametric testing for Local and Global Spatial\n  Autocorrelation with application to the Canadian Electorate", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of local and global spatial association are key tools for\nexploratory spatial data analysis. Many such measures exist including Moran's\n$I$, Geary's $C$, and the Getis-Ord $G$ and $G^*$ statistics. A parametric\napproach to testing for significance relies on strong assumptions, which are\noften not met by real world data. Alternatively, the most popular nonparametric\napproach, the permutation test, imposes a large computational burden especially\nfor massive graphical networks. Hence, we propose a computation-free approach\nto nonparametric permutation testing for local and global measures of spatial\nautocorrelation stemming from generalizations of the Khintchine inequality from\nfunctional analysis and the theory of $L^p$ spaces. Our methodology is\ndemonstrated on the results of the 2019 federal Canadian election in the\nprovince of Alberta. We recorded the percentage of the vote gained by the\nconservative candidate in each riding. This data is not normal, and the sample\nsize is fixed at $n=34$ ridings making the parametric approach invalid. In\ncontrast, running a classic permutation test for every riding, for multiple\ntest statistics, with various neighbourhood structures, and multiple testing\ncorrection would require the simulation of millions of permutations. We are\nable to achieve similar statistical power on this dataset to the permutation\ntest without the need for tedious simulation. We also consider data simulated\nacross the entire electoral map of Canada.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:12:25 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Kashlak", "Adam B", ""], ["Yuan", "Weicong", ""]]}, {"id": "2012.08649", "submitter": "Enrique Barrajon MDPhD", "authors": "Enrique Barraj\\'on, Laura Barraj\\'on", "title": "Effect of right censoring bias on survival analysis", "comments": "12 pages, 5 figures, supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kaplan-Meier survival analysis represents the most objective measure of\ntreatment efficacy in oncology, though subjected to potential bias, which is\nworrisome in an era of precision medicine. Independent of the bias inherent to\nthe design and execution of clinical trials, bias may be the result of patient\ncensoring, or incomplete observation. Unlike disease/progression free survival,\noverall survival is based on a well defined time point and thus avoids interval\ncensoring, but right-censoring, due to incomplete follow-up, may still be a\nsource of bias. We study three mechanisms of right-censoring and find that one\nof them, surrogate of patient lost to follow-up, is able to impact Kaplan-Meier\nsurvival, improving significantly the estimation of survival in comparison with\ncomplete follow-up datasets, as measured by the hazard ratio. We also present\ntwo bias indexes able to signal datasets with right-censoring associated\noverestimation of survival. These bias indexes can detect bias in public\navailable datasets\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:17:31 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Barraj\u00f3n", "Enrique", ""], ["Barraj\u00f3n", "Laura", ""]]}, {"id": "2012.08665", "submitter": "Mark Huber", "authors": "Mark Huber", "title": "Generating from the Strauss Process using stitching", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The STrauss process is a point process with unnormalized density with respect\nto a Poisson point process, where each pair of points within a specified\ndistance $r$ of each other contributes a factor $\\lambda \\in (0, 1)$ to the\ndensity. Basic Acceptance Rejection works spectacularly poorly for this\nproblem, which is why several other perfect simulation methods have been\ndeveloped. these methods, however, also work poorly for reasonably large values\nof $\\lambda$. *Acceptance Rejection Stitching* is a new method that works much\nfaster, allowing the simulation of point processes with values of $\\lambda$\nmuch larger than ever before.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 23:17:43 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Huber", "Mark", ""]]}, {"id": "2012.08848", "submitter": "JInglai Li", "authors": "Jiangqi Wu, Linjie Wen, Peter L Green, Jinglai Li, Simon Maskell", "title": "Ensemble Kalman filter based Sequential Monte Carlo Sampler for\n  sequential Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many real-world problems require one to estimate parameters of interest, in a\nBayesian framework, from data that are collected sequentially in time.\nConventional methods for sampling from posterior distributions, such as {Markov\nChain Monte Carlo} can not efficiently address such problems as they do not\ntake advantage of the data's sequential structure. To this end, sequential\nmethods which seek to update the posterior distribution whenever a new\ncollection of data become available are often used to solve these types of\nproblems. Two popular choices of sequential method are the Ensemble Kalman\nfilter (EnKF) and the sequential Monte Carlo sampler (SMCS). An advantage of\nthe SMCS method is that, unlike the EnKF method that only computes a Gaussian\napproximation of the posterior distribution, SMCS can draw samples directly\nfrom the posterior. Its performance, however, depends critically upon the\nkernels that are used. In this work, we present a method that constructs the\nkernels of SMCS using an EnKF formulation, and we demonstrate the performance\nof the method with numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 10:37:15 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Wu", "Jiangqi", ""], ["Wen", "Linjie", ""], ["Green", "Peter L", ""], ["Li", "Jinglai", ""], ["Maskell", "Simon", ""]]}, {"id": "2012.09096", "submitter": "Bastien Mallein", "authors": "Emilien Joly and Bastien Mallein", "title": "A tractable non-adaptative group testing method for non-binary\n  measurements", "comments": "16 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original problem of group testing consists in the identification of\ndefective items in a collection, by applying tests on groups of items that\ndetect the presence of at least one defective item in the group. The aim is\nthen to identify all defective items of the collection with as few tests as\npossible. This problem is relevant in several fields, among which biology and\ncomputer sciences. In the present article we consider that the tests applied to\ngroups of items returns a \\emph{load}, measuring how defective the most\ndefective item of the group is. In this setting, we propose a simple\nnon-adaptative algorithm allowing the detection of all defective items of the\ncollection. This method improves on classical group testing algorithms using\nonly the binary response of the test.\n  Group testing recently gained attraction as a potential tool to solve a\nshortage of COVID-19 test kits, in particular for RT-qPCR. These tests return\nthe viral load of the sample and the viral load varies greatly among\nindividuals. Therefore our model presents some of the key features of this\nproblem. We aim at using the extra piece of information that represents the\nviral load to construct a one-stage pool testing algorithm on this idealized\nversion. We show that under the right conditions, the total number of tests\nneeded to detect contaminated samples can be drastically diminished.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:40:30 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:21:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Joly", "Emilien", ""], ["Mallein", "Bastien", ""]]}, {"id": "2012.09112", "submitter": "C\\'edric Goeury", "authors": "Cedric Goeury, Yoann Audouin and Fabrice Zaoui", "title": "Interoperability and computational framework for simulating open channel\n  hydraulics: application to sensitivity analysis and calibration of Gironde\n  Estuary model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water resource management is of crucial societal and economic importance,\nrequiring a strong capacity for anticipating environmental change. Progress in\nphysical process knowledge, numerical methods and computational power, allows\nus to address hydro-environmental problems of growing complexity. Modeling of\nriver and marine flows is no exception. With the increase in IT resources,\nenvironmental modeling is evolving to meet the challenges of complex real-world\nproblems. This paper presents a new distributed Application Programming\nInterface (API) of the open source TELEMAC-MASCARET system to run\nhydro-environmental simulations with the help of the interoperability concept.\nUse of the API encourages and facilitates the combination of worldwide\nreference environmental libraries with the hydro-informatic system.\nConsequently, the objective of the paper is to promote the interoperability\nconcept for studies dealing with such issues as uncertainty propagation, global\nsensitivity analysis, optimization, multi-physics or multi-dimensional\ncoupling. To illustrate the capability of the API, an operational problem for\nimproving the navigation capacity of the Gironde Estuary is presented. The API\npotential is demonstrated in a re-calibration context. The API is used for a\nmultivariate sensitivity analysis to quickly reveal the most influential\nparameters which can then be optimally calibrated with the help of a data\nassimilation technique.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 07:14:05 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 11:22:36 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Goeury", "Cedric", ""], ["Audouin", "Yoann", ""], ["Zaoui", "Fabrice", ""]]}, {"id": "2012.09731", "submitter": "Samuel Livingstone", "authors": "Max Hird, Samuel Livingstone and Giacomo Zanella", "title": "A fresh take on 'Barker dynamics' for MCMC", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a recently introduced gradient-based Markov chain Monte Carlo method\nbased on 'Barker dynamics'. We provide a full derivation of the method from\nfirst principles, placing it within a wider class of continuous-time Markov\njump processes. We then evaluate the Barker approach numerically on a\nchallenging ill-conditioned logistic regression example with imbalanced data,\nshowing in particular that the algorithm is remarkably robust to irregularity\n(in this case a high degree of skew) in the target distribution.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 16:49:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 17:05:40 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Hird", "Max", ""], ["Livingstone", "Samuel", ""], ["Zanella", "Giacomo", ""]]}, {"id": "2012.09789", "submitter": "Amparo Gil", "authors": "Amparo Gil, Javier Segura and Nico M. Temme", "title": "A new asymptotic representation and inversion method for the Student's t\n  distribution", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA cs.NA math.NA stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some special functions are particularly relevant in applied probability and\nstatistics. For example, the incomplete beta function is the cumulative central\nbeta distribution. In this paper, we consider the inversion of the central\nStudent's-$t$ distribution which is a particular case of the central beta\ndistribution. The inversion of this distribution functions is useful in\nhypothesis testing as well as for generating random samples distributed\naccording to the corresponding probability density function. A new asymptotic\nrepresentation in terms of the complementary error function, will be one of the\nimportant ingredients in our analysis. As we will show, this asymptotic\nrepresentation is also useful in the computation of the distribution function.\nWe illustrate the performance of all the obtained approximations with numerical\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 17:53:45 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Gil", "Amparo", ""], ["Segura", "Javier", ""], ["Temme", "Nico M.", ""]]}, {"id": "2012.09821", "submitter": "Ritabrata Dutta", "authors": "Sherman Lo and Peter Watson and Peter Dueben and Ritabrata Dutta", "title": "High-resolution Probabilistic Precipitation Prediction for use in\n  Climate Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of precipitation is important to allow for reliable\nwarnings of flood or drought risk in a changing climate. However, to make\ntrust-worthy predictions of precipitation, at a local scale, is one of the most\ndifficult challenges for today's weather and climate models. This is because\nimportant features, such as individual clouds and high-resolution topography,\ncannot be resolved explicitly within simulations due to the significant\ncomputational cost of high-resolution simulations. Climate models are typically\nrun at $\\sim$50-100 km resolution which is insufficient to represent local\nprecipitation events in satisfying detail. Here, we develop a method to make\nprobabilistic precipitation predictions based on features that climate models\ncan resolve well and that is not highly sensitive to the approximations used in\nindividual models. To predict, we will use a temporal compound Poisson\ndistribution dependent on the output of climate models at a location. We use\nthe output of Earth System models at coarse resolution $\\sim$50 km as input and\ntrain the statistical models towards precipitation observations over Wales at\n$\\sim$10 km resolution. A Bayesian inferential scheme is provided so that the\ncompound-Poisson model can be inferred using a\nGibbs-within-Metropolis-Elliptic-Slice sampling scheme which enables us to\nquantify the uncertainty of our predictions. In addition, we use a Gaussian\nprocess regressor on the posterior samples of the model parameters, to infer a\nspatially coherent model and hence to produce spatially coherent rainfall\nprediction. We illustrate the prediction performance of our model by training\nover 5 years of the data up to 31st December 1999 and predicting precipitation\nfor 20 years afterwards for Cardiff and Wales.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:42:48 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:11:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lo", "Sherman", ""], ["Watson", "Peter", ""], ["Dueben", "Peter", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2012.09920", "submitter": "Miguel Angel Luque-Fernandez", "authors": "Matthew J. Smith, Camille Maringe, Bernard Rachet, Mohammad A.\n  Mansournia, Paul N. Zivich, Stephen R. Cole, Miguel Angel Luque-Fernandez", "title": "Tutorial: Introduction to computational causal inference using\n  reproducible Stata, R and Python code", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of many health studies is to estimate the effect of an exposure\non an outcome. It is not always ethical to assign an exposure to individuals in\nrandomised controlled trials, instead observational data and appropriate study\ndesign must be used. There are major challenges with observational studies, one\nof which is confounding that can lead to biased estimates of the causal\neffects. Controlling for confounding is commonly performed by simple adjustment\nfor measured confounders; although, often this is not enough. Recent advances\nin the field of causal inference have dealt with confounding by building on\nclassical standardisation methods. However, these recent advances have\nprogressed quickly with a relative paucity of computational-oriented applied\ntutorials contributing to some confusion in the use of these methods among\napplied researchers. In this tutorial, we show the computational implementation\nof different causal inference estimators from a historical perspective where\ndifferent estimators were developed to overcome the limitations of the previous\none. Furthermore, we also briefly introduce the potential outcomes framework,\nillustrate the use of different methods using an illustration from the health\ncare setting, and most importantly, we provide reproducible and commented code\nin Stata, R and Python for researchers to apply in their own observational\nstudy. The code can be accessed at\nhttps://github.com/migariane/TutorialCausalInferenceEstimators\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:31:30 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 17:01:06 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Smith", "Matthew J.", ""], ["Maringe", "Camille", ""], ["Rachet", "Bernard", ""], ["Mansournia", "Mohammad A.", ""], ["Zivich", "Paul N.", ""], ["Cole", "Stephen R.", ""], ["Luque-Fernandez", "Miguel Angel", ""]]}, {"id": "2012.10249", "submitter": "Ranjan Maitra", "authors": "Carlos Llosa-Vite and Ranjan Maitra", "title": "Reduced-Rank Tensor-on-Tensor Regression and Tensor-variate Analysis of\n  Variance", "comments": "30 pages, 12 figures, 2 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting regression models with many multivariate responses and covariates can\nbe challenging, but such responses and covariates sometimes have tensor-variate\nstructure. We extend the classical multivariate regression model to exploit\nsuch structure in two ways: first, we impose four types of low-rank tensor\nformats on the regression coefficients. Second, we model the errors using the\ntensor-variate normal distribution that imposes a Kronecker separable format on\nthe covariance matrix. We obtain maximum likelihood estimators via\nblock-relaxation algorithms and derive their asymptotic distributions. Our\nregression framework enables us to formulate tensor-variate analysis of\nvariance (TANOVA) methodology. Application of our methodology in a one-way\nTANOVA layout enables us to identify cerebral regions significantly associated\nwith the interaction of suicide attempters or non-attemptor ideators and\npositive-, negative- or death-connoting words. A separate application performs\nthree-way TANOVA on the Labeled Faces in the Wild image database to distinguish\nfacial characteristics related to ethnic origin, age group and gender.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:04:41 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 15:57:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Llosa-Vite", "Carlos", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2012.10263", "submitter": "Pierre Marion", "authors": "Pierre L'Ecuyer, Pierre Marion, Maxime Godin, Florian Puchhammer", "title": "A Tool for Custom Construction of QMC and RQMC Point Sets", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present LatNet Builder, a software tool to find good parameters for\nlattice rules, polynomial lattice rules, and digital nets in base 2, for\nquasi-Monte Carlo (QMC) and randomized quasi-Monte Carlo (RQMC) sampling over\nthe $s$-dimensional unit hypercube. The selection criteria are figures of merit\nthat give different weights to different subsets of coordinates. They are upper\nbounds on the worst-case error (for QMC) or variance (for RQMC) for integrands\nrescaled to have a norm of at most one in certain Hilbert spaces of functions.\nVarious Hilbert spaces, figures of merit, types of constructions, and search\nmethods are covered by the tool. We provide simple illustrations of what it can\ndo.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 14:28:01 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["L'Ecuyer", "Pierre", ""], ["Marion", "Pierre", ""], ["Godin", "Maxime", ""], ["Puchhammer", "Florian", ""]]}, {"id": "2012.10403", "submitter": "Jeffrey Wong", "authors": "Eskil Forsell, Julie Beckley, Simon Ejdemyr, Veronica Hannan, Andy\n  Rhines, Martin Tingley, Matthew Wardrop, Jeffrey Wong", "title": "Success Stories from a Democratized Experimentation Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate the effectiveness of democratization and efficient computation\nas key concepts of our experimentation platform (XP) by presenting four new\nmodels supported by the platform: 1) Weighted least squares, 2) Quantile\nbootstrapping, 3) Bayesian shrinkage, and 4) Dynamic treatment effects. Each\nmodel is motivated by a specific business problem but is generalizable and\nextensible. The modular structure of our platform allows independent innovation\non statistical and computational methods. In practice, a technical symbiosis is\ncreated where increasingly advanced user contributions inspire innovations to\nthe software that in turn enable further methodological improvements. This\ncycle adds further value to how the XP contributes to business solutions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 04:28:06 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Forsell", "Eskil", ""], ["Beckley", "Julie", ""], ["Ejdemyr", "Simon", ""], ["Hannan", "Veronica", ""], ["Rhines", "Andy", ""], ["Tingley", "Martin", ""], ["Wardrop", "Matthew", ""], ["Wong", "Jeffrey", ""]]}, {"id": "2012.10410", "submitter": "Gabriel Turinici", "authors": "Gabriel Turinici", "title": "Convergence dynamics of Generative Adversarial Networks: the dual metric\n  flows", "comments": null, "journal-ref": "CADL (Computational Aspects of Deep Learning) workshop held during\n  the 25th ICPR conference, Milano, Italy Jan 10-15 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.FA stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fitting neural networks often resorts to stochastic (or similar) gradient\ndescent which is a noise-tolerant (and efficient) resolution of a gradient\ndescent dynamics. It outputs a sequence of networks parameters, which sequence\nevolves during the training steps. The gradient descent is the limit, when the\nlearning rate is small and the batch size is infinite, of this set of\nincreasingly optimal network parameters obtained during training. In this\ncontribution, we investigate instead the convergence in the Generative\nAdversarial Networks used in machine learning. We study the limit of small\nlearning rate, and show that, similar to single network training, the GAN\nlearning dynamics tend, for vanishing learning rate to some limit dynamics.\nThis leads us to consider evolution equations in metric spaces (which is the\nnatural framework for evolving probability laws) that we call dual flows. We\ngive formal definitions of solutions and prove the convergence. The theory is\nthen applied to specific instances of GANs and we discuss how this insight\nhelps understand and mitigate the mode collapse.\n  Keywords: GAN; metric flow; generative network\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 18:00:12 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:59:17 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Turinici", "Gabriel", ""]]}, {"id": "2012.10637", "submitter": "Xiao Chen", "authors": "Xiao Chen", "title": "Robust mixture regression with Exponential Power distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assuming an exponential power distribution is one way to deal with outliers\nin regression and clustering, which can increase the robustness of the\nanalysis. Gaussian distribution is a special case of an exponential\ndistribution. And an exponential power distribution can be viewed as a scale\nmixture of normal distributions. Thus, model selection methods developed for\nthe Gaussian mixture model can be easily extended for the exponential power\nmixture model. Moreover, Gaussian mixture models tend to select more components\nthan exponential power mixture models in real-world cases, which means\nexponential power mixture models are easier to interpret. In this paper, We\ndevelop analyses for mixture regression models when the errors are assumed to\nfollow an exponential power distribution. It will be robust to outliers, and\nmodel selection for it is easy to implement.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 09:30:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Chen", "Xiao", ""]]}, {"id": "2012.10754", "submitter": "Osvaldo Martin", "authors": "Tom\\'as Capretto, Camen Piho, Ravin Kumar, Jacob Westfall, Tal\n  Yarkoni, Osvaldo A. Martin", "title": "Bambi: A simple interface for fitting Bayesian linear models in Python", "comments": "20 pages 9 figures, to be published in the Journal of Statistical\n  Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The popularity of Bayesian statistical methods has increased dramatically in\nrecent years across many research areas and industrial applications. This is\nthe result of a variety of methodological advances with faster and cheaper\nhardware as well as the development of new software tools. Here we introduce an\nopen source Python package named Bambi (BAyesian Model Building Interface) that\nis built on top of the PyMC3 probabilistic programming framework and the ArviZ\npackage for exploratory analysis of Bayesian models. Bambi makes it easy to\nspecify complex generalized linear hierarchical models using a formula notation\nsimilar to those found in the popular R packages lme4, nlme, rstanarm and brms.\nWe demonstrate Bambi's versatility and ease of use with a few examples spanning\na range of common statistical models including multiple regression, logistic\nregression, and mixed-effects modeling with crossed group specific effects.\nAdditionally we discuss how automatic priors are constructed. Finally, we\nconclude with a discussion of our plans for the future development of Bambi.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 18:26:01 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 13:56:14 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Capretto", "Tom\u00e1s", ""], ["Piho", "Camen", ""], ["Kumar", "Ravin", ""], ["Westfall", "Jacob", ""], ["Yarkoni", "Tal", ""], ["Martin", "Osvaldo A.", ""]]}, {"id": "2012.10943", "submitter": "Torben Sell", "authors": "Torben Sell, Sumeetpal S. Singh", "title": "Dimension-robust Function Space MCMC With Neural Network Priors", "comments": "24 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new prior on functions spaces which scales more\nfavourably in the dimension of the function's domain compared to the usual\nKarhunen-Lo\\'eve function space prior, a property we refer to as\ndimension-robustness. The proposed prior is a Bayesian neural network prior,\nwhere each weight and bias has an independent Gaussian prior, but with the key\ndifference that the variances decrease in the width of the network, such that\nthe variances form a summable sequence and the infinite width limit neural\nnetwork is well defined. We show that our resulting posterior of the unknown\nfunction is amenable to sampling using Hilbert space Markov chain Monte Carlo\nmethods. These sampling methods are favoured because they are stable under\nmesh-refinement, in the sense that the acceptance probability does not shrink\nto 0 as more parameters are introduced to better approximate the well-defined\ninfinite limit. We show that our priors are competitive and have distinct\nadvantages over other function space priors. Upon defining a suitable\nlikelihood for continuous value functions in a Bayesian approach to\nreinforcement learning, our new prior is used in numerical examples to\nillustrate its performance and dimension-robustness.\n", "versions": [{"version": "v1", "created": "Sun, 20 Dec 2020 14:52:57 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Sell", "Torben", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "2012.11094", "submitter": "Jianfeng Lu", "authors": "Jianfeng Lu and Lihan Wang", "title": "Complexity of zigzag sampling algorithm for strongly log-concave\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of zigzag sampling algorithm for\nstrongly log-concave distributions. The zigzag process has the advantage of not\nrequiring time discretization for implementation, and that each proposed\nbouncing event requires only one evaluation of partial derivative of the\npotential, while its convergence rate is dimension independent. Using these\nproperties, we prove that the zigzag sampling algorithm achieves $\\varepsilon$\nerror in chi-square divergence with a computational cost equivalent to\n$O\\bigl(\\kappa^2 d^\\frac{1}{2}(\\log\\frac{1}{\\varepsilon})^{\\frac{3}{2}}\\bigr)$\ngradient evaluations in the regime $\\kappa \\ll \\frac{d}{\\log d}$ under a warm\nstart assumption, where $\\kappa$ is the condition number and $d$ is the\ndimension.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 03:10:21 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Lu", "Jianfeng", ""], ["Wang", "Lihan", ""]]}, {"id": "2012.11122", "submitter": "Pritam Ranjan", "authors": "M. Harshvardhan, Pritam Ranjan", "title": "Statistical Modelling and Analysis of the Computer-Simulated Datasets", "comments": null, "journal-ref": null, "doi": "10.4018/978-1-5225-8407-0.ch011", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last two decades, the science has come a long way from relying on\nonly physical experiments and observations to experimentation using computer\nsimulators. This chapter focusses on the modelling and analysis of data arising\nfrom computer simulators. It turns out that traditional statistical metamodels\nare often not very useful for analyzing such datasets. For deterministic\ncomputer simulators, the realizations of Gaussian Process (GP) models are\ncommonly used for fitting a surrogate statistical metamodel of the simulator\noutput. The chapter starts with a quick review of the standard GP based\nstatistical surrogate model. The chapter also emphasizes on the numerical\ninstability due to near-singularity of the spatial correlation structure in the\nGP model fitting process. The authors also present a few generalizations of the\nGP model, reviews methods and algorithms specifically developed for analyzing\nbig data obtained from computer model runs, and reviews the popular analysis\ngoals of such computer experiments. A few real-life computer simulators are\nalso briefly outlined here.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 05:17:50 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Harshvardhan", "M.", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2012.11241", "submitter": "Maxime El Masri", "authors": "Maxime El Masri, J\\'er\\^ome Morio, Florian Simatos", "title": "Improvement of the cross-entropy method in high dimension through a\n  one-dimensional projection without gradient estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rare event probability estimation is an important topic in reliability\nanalysis. Stochastic methods, such as importance sampling, have been developed\nto estimate such probabilities but they often fail in high dimension. In this\npaper, we propose a simple cross-entropy-based importance sampling algorithm to\nimprove rare event estimation in high dimension. We consider the cross-entropy\nmethod with Gaussian auxiliary distributions and we suggest to update the\nGaussian covariance matrix only in a one-dimensional subspace. The idea is to\nproject in the one dimensional subspace spanned by the Gaussian mean, which\ngives an influential direction for the variance estimation. This approach does\nnot require any additional simulation budget compared to the basic\ncross-entropy algorithm and greatly improves its performance in high dimension,\nas we show on different numerical test cases.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 10:45:49 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Masri", "Maxime El", ""], ["Morio", "J\u00e9r\u00f4me", ""], ["Simatos", "Florian", ""]]}, {"id": "2012.11349", "submitter": "Ryan Martin", "authors": "Pei-Shien Wu and Ryan Martin", "title": "A comparison of learning rate selection methods in generalized Bayesian\n  inference", "comments": "22 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized Bayes posterior distributions are formed by putting a fractional\npower on the likelihood before combining with the prior via Bayes's formula.\nThis fractional power, which is often viewed as a remedy for potential model\nmisspecification bias, is called the learning rate, and a number of data-driven\nlearning rate selection methods have been proposed in the recent literature.\nEach of these proposals has a different focus, a different target they aim to\nachieve, which makes them difficult to compare. In this paper, we provide a\ndirect head-to-head comparison of these learning rate selection methods in\nvarious misspecified model scenarios, in terms of several relevant metrics, in\nparticular, coverage probability of the generalized Bayes credible regions. In\nsome examples all the methods perform well, while in others the\nmisspecification is too severe to be overcome, but we find that the so-called\ngeneralized posterior calibration algorithm tends to outperform the others in\nterms of credible region coverage probability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:02:19 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wu", "Pei-Shien", ""], ["Martin", "Ryan", ""]]}, {"id": "2012.11369", "submitter": "Oskar Allerbo", "authors": "Oskar Allerbo, Rebecka J\\\"ornsten", "title": "Flexible, Non-parametric Modeling Using Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric regression, such as generalized additive models (GAMs), is\nable to capture complex data dependencies in a flexible, yet interpretable way.\nHowever, choosing the format of the additive components often requires\nnon-trivial data exploration. Here, we propose an alternative to GAMs,\nPrAda-net, which uses a one hidden layer neural network, trained with proximal\ngradient descent and adaptive lasso. PrAda-net automatically adjusts the size\nand architecture of the neural network to capture the complexity and structure\nof the underlying data generative model. The compact network obtained by\nPrAda-net can be translated to additive model components, making it suitable\nfor non-parametric statistical modelling with automatic model selection. We\ndemonstrate PrAda-net on simulated data, where we compare the test error\nperformance, variable importance and variable subset identification properties\nof PrAda-net to other lasso-based approaches. We also apply Prada-net to the\nmassive U.K. black smoke data set, to demonstrate the capability of using\nPrada-net as an alternative to GAMs. In contrast to GAMs, which often require\ndomain knowledge to select the functional forms of the additive components,\nPrada-net requires no such pre-selection while still resulting in interpretable\nadditive components.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 08:49:04 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 09:36:55 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Allerbo", "Oskar", ""], ["J\u00f6rnsten", "Rebecka", ""]]}, {"id": "2012.11572", "submitter": "Carlos Am\\'endola", "authors": "Carlos Am\\'endola, Luis David Garc\\'ia-Puente, Roser Homs, Olga\n  Kuznetsova, Harshit J. Motwani", "title": "Computing Maximum Likelihood Estimates for Gaussian Graphical Models\n  with Macaulay2", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.AC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the package GraphicalModelsMLE for computing the maximum\nlikelihood estimator (MLE) of a Gaussian graphical model in the computer\nalgebra system Macaulay2. The package allows to compute for the class of\nloopless mixed graphs. Additional functionality allows to explore the\nunderlying algebraic structure of the model, such as its ML degree and the\nideal of score equations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:50:41 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Garc\u00eda-Puente", "Luis David", ""], ["Homs", "Roser", ""], ["Kuznetsova", "Olga", ""], ["Motwani", "Harshit J.", ""]]}, {"id": "2012.11573", "submitter": "Vincent Runge", "authors": "Vincent Runge, Marco Pascucci, Nicolas Deschamps de Boishebert", "title": "Change-in-Slope Optimal Partitioning Algorithm in a Finite-Size\n  Parameter Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of detecting change-points in univariate time series\nby fitting a continuous piecewise linear signal using the residual sum of\nsquares. Values of the inferred signal at slope breaks are restricted to a\nfinite set of size $m$. Using this finite parameter space, we build a dynamic\nprogramming algorithm with a controlled time complexity of $O(m^2n^2)$ for $n$\ndata points. Some accelerating strategies can be used to reduce the constant\nbefore $n^2$. The adapted classic inequality-based pruning is outperformed by a\nsimpler \"channel\" method on simulations. Besides, our finite parameter space\nsetting allows an easy introduction of constraints on the inferred signal. For\nexample, imposing a minimal angle between consecutive segment slopes provides\nrobustness to model misspecification and outliers. We test our algorithm with\nan isotonic constraint on an antibiogram image analysis problem for which\nalgorithmic efficiency is a cornerstone in the emerging context of\nmobile-health and embedded medical devices. For this application, a finite\nstate approach can be a valid compromise.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:50:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Runge", "Vincent", ""], ["Pascucci", "Marco", ""], ["de Boishebert", "Nicolas Deschamps", ""]]}, {"id": "2012.12144", "submitter": "Nicholas Horton", "authors": "Nicholas J. Horton and Johanna S. Hardin", "title": "Integrating computing in the statistics and data science curriculum:\n  Creative structures, novel skills and habits, and ways to teach computational\n  thinking", "comments": "In press, Journal of Statistics and Data Science Education", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nolan and Temple Lang (2010) argued for the fundamental role of computing in\nthe statistics curriculum. In the intervening decade the statistics education\ncommunity has acknowledged that computational skills are as important to\nstatistics and data science practice as mathematics. There remains a notable\ngap, however, between our intentions and our actions. In this special issue of\nthe *Journal of Statistics and Data Science Education* we have assembled a\ncollection of papers that (1) suggest creative structures to integrate\ncomputing, (2) describe novel data science skills and habits, and (3) propose\nways to teach computational thinking. We believe that it is critical for the\ncommunity to redouble our efforts to embrace sophisticated computing in the\nstatistics and data science curriculum. We hope that these papers provide\nuseful guidance for the community to move these efforts forward.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:28:18 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Horton", "Nicholas J.", ""], ["Hardin", "Johanna S.", ""]]}, {"id": "2012.12461", "submitter": "Janice Scealy", "authors": "Janice L. Scealy and Andrew T. A. Wood", "title": "Score matching for compositional distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Compositional data and multivariate count data with known totals are\nchallenging to analyse due to the non-negativity and sum-to-one constraints on\nthe sample space. It is often the case that many of the compositional\ncomponents are highly right-skewed, with large numbers of zeros. A major\nlimitation of currently available estimators for compositional models is that\nthey either cannot handle many zeros in the data or are not computationally\nfeasible in moderate to high dimensions. We derive a new set of novel score\nmatching estimators applicable to distributions on a Riemannian manifold with\nboundary, of which the standard simplex is a special case. The score matching\nmethod is applied to estimate the parameters in a new flexible truncation model\nfor compositional data and we show that the estimators are scalable and\navailable in closed form. Through extensive simulation studies, the scoring\nmethodology is demonstrated to work well for estimating the parameters in the\nnew truncation model and also for the Dirichlet distribution. We apply the new\nmodel and estimators to real microbiome compositional data and show that the\nmodel provides a good fit to the data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 02:53:15 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Scealy", "Janice L.", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "2012.13133", "submitter": "Suman Majumder", "authors": "Suman Majumder, Yawen Guan, Brian J. Reich and Arvind K. Saibaba", "title": "Kryging: Geostatistical analysis of large-scale datasets using Krylov\n  subspace methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing massive spatial datasets using Gaussian process model poses\ncomputational challenges. This is a problem prevailing heavily in applications\nsuch as environmental modeling, ecology, forestry and environmental heath. We\npresent a novel approximate inference methodology that uses profile likelihood\nand Krylov subspace methods to estimate the spatial covariance parameters and\nmakes spatial predictions with uncertainty quantification. The proposed method,\nKryging, applies for both observations on regular grid and irregularly-spaced\nobservations, and for any Gaussian process with a stationary covariance\nfunction, including the popular $\\Matern$ covariance family. We make use of the\nblock Toeplitz structure with Toeplitz blocks of the covariance matrix and use\nfast Fourier transform methods to alleviate the computational and memory\nbottlenecks. We perform extensive simulation studies to show the effectiveness\nof our model by varying sample sizes, spatial parameter values and sampling\ndesigns. A real data application is also performed on a dataset consisting of\nland surface temperature readings taken by the MODIS satellite. Compared to\nexisting methods, the proposed method performs satisfactorily with much less\ncomputation time and better scalability.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 06:35:17 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Majumder", "Suman", ""], ["Guan", "Yawen", ""], ["Reich", "Brian J.", ""], ["Saibaba", "Arvind K.", ""]]}, {"id": "2012.13769", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, V. Roshan Joseph, Simon Mak", "title": "Population Quasi-Monte Carlo", "comments": "Submitted to Journal of Computational and Graphical Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo methods are widely used for approximating complicated,\nmultidimensional integrals for Bayesian inference. Population Monte Carlo (PMC)\nis an important class of Monte Carlo methods, which utilizes a population of\nproposals to generate weighted samples that approximate the target\ndistribution. The generic PMC framework iterates over three steps: samples are\nsimulated from a set of proposals, weights are assigned to such samples to\ncorrect for mismatch between the proposal and target distributions, and the\nproposals are then adapted via resampling from the weighted samples. When the\ntarget distribution is expensive to evaluate, the PMC has its computational\nlimitation since the convergence rate is $\\mathcal{O}(N^{-1/2})$. To address\nthis, we propose in this paper a new Population Quasi-Monte Carlo (PQMC)\nframework, which integrates Quasi-Monte Carlo ideas within the sampling and\nadaptation steps of PMC. A key novelty in PQMC is the idea of importance\nsupport points resampling, a deterministic method for finding an \"optimal\"\nsubsample from the weighted proposal samples. Moreover, within the PQMC\nframework, we develop an efficient covariance adaptation strategy for\nmultivariate normal proposals. Lastly, a new set of correction weights is\nintroduced for the weighted PMC estimator to improve the efficiency from the\nstandard PMC estimator. We demonstrate the improved empirical convergence of\nPQMC over PMC in extensive numerical simulations and a friction drilling\napplication.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 16:10:54 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Huang", "Chaofan", ""], ["Joseph", "V. Roshan", ""], ["Mak", "Simon", ""]]}, {"id": "2012.13924", "submitter": "Andrea Bertazzi", "authors": "Andrea Bertazzi and Joris Bierkens", "title": "Adaptive Schemes for Piecewise Deterministic Monte Carlo Algorithms", "comments": "43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bouncy Particle sampler (BPS) and the Zig-Zag sampler (ZZS) are\ncontinuous time, non-reversible Monte Carlo methods based on piecewise\ndeterministic Markov processes. Experiments show that the speed of convergence\nof these samplers can be affected by the shape of the target distribution, as\nfor instance in the case of anisotropic targets. We propose an adaptive scheme\nthat iteratively learns all or part of the covariance matrix of the target and\ntakes advantage of the obtained information to modify the underlying process\nwith the aim of increasing the speed of convergence. Moreover, we define an\nadaptive scheme that automatically tunes the refreshment rate of the BPS or\nZZS. We prove ergodicity and a law of large numbers for all the proposed\nadaptive algorithms. Finally, we show the benefits of the adaptive samplers\nwith several numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:31:47 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bertazzi", "Andrea", ""], ["Bierkens", "Joris", ""]]}, {"id": "2012.14100", "submitter": "Huangjie Zheng", "authors": "Huangjie Zheng and Mingyuan Zhou", "title": "Exploiting Chain Rule and Bayes' Theorem to Compare Probability\n  Distributions", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To measure the difference between two probability distributions, referred to\nas the source and target, respectively, we exploit both the chain rule and\nBayes' theorem to construct conditional transport (CT), which is constituted by\nboth a forward component and a backward one. The forward CT is the expected\ncost of moving a source data point to a target one, with their joint\ndistribution defined by the product of the source probability density function\n(PDF) and a source-dependent conditional distribution, which is related to the\ntarget PDF via Bayes' theorem. The backward CT is defined by reversing the\ndirection. The CT cost can be approximated by replacing the source and target\nPDFs with their discrete empirical distributions supported on mini-batches,\nmaking it amenable to implicit distributions and stochastic gradient\ndescent-based optimization. When applied to train a generative model, CT is\nshown to strike a good balance between mode-covering and mode-seeking behaviors\nand strongly resist mode collapse. On a wide variety of benchmark datasets for\ngenerative modeling, substituting the default statistical distance of an\nexisting generative adversarial network with CT is shown to consistently\nimprove the performance. PyTorch-style code is provided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 05:14:22 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 05:34:22 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 02:00:37 GMT"}, {"version": "v4", "created": "Tue, 29 Jun 2021 16:28:43 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zheng", "Huangjie", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "2012.14847", "submitter": "Raazesh Sainudiin", "authors": "Raazesh Sainudiin and Warwick Tucker and Tilo Wiklund", "title": "Scalable Multivariate Histograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We give a distributed variant of an adaptive histogram estimation procedure\npreviously developed by the first author. The procedure is based on regular\npavings and is known to have numerous appealing statistical and arithmetical\nproperties. The distributed version makes it possible to process data sets\nsignificantly bigger than previously. We provide prototype implementation under\na permissive license.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 16:51:53 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sainudiin", "Raazesh", ""], ["Tucker", "Warwick", ""], ["Wiklund", "Tilo", ""]]}, {"id": "2012.14881", "submitter": "Christophe Andrieu", "authors": "Christophe Andrieu and Anthony Lee and Sam Livingstone", "title": "A general perspective on the Metropolis-Hastings kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its inception the Metropolis-Hastings kernel has been applied in\nsophisticated ways to address ever more challenging and diverse sampling\nproblems. Its success stems from the flexibility brought by the fact that its\nverification and sampling implementation rests on a local ``detailed balance''\ncondition, as opposed to a global condition in the form of a typically\nintractable integral equation. While checking the local condition is routine in\nthe simplest scenarios, this proves much more difficult for complicated\napplications involving auxiliary structures and variables. Our aim is to\ndevelop a framework making establishing correctness of complex Markov chain\nMonte Carlo kernels a purely mechanical or algebraic exercise, while making\ncommunication of ideas simpler and unambiguous by allowing a stronger focus on\nessential features -- a choice of embedding distribution, an involution and\noccasionally an acceptance function -- rather than the induced, boilerplate\nstructure of the kernels that often tends to obscure what is important. This\nframework can also be used to validate kernels that do not satisfy detailed\nbalance, i.e. which are not reversible, but a modified version thereof.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 18:15:21 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Andrieu", "Christophe", ""], ["Lee", "Anthony", ""], ["Livingstone", "Sam", ""]]}, {"id": "2012.15339", "submitter": "Florian Gerber", "authors": "Florian Gerber and Douglas W. Nychka", "title": "Fast covariance parameter estimation of spatial Gaussian process models\n  using neural networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are a popular model for spatially referenced data\nand allow descriptive statements, predictions at new locations, and simulation\nof new fields. Often a few parameters are sufficient to parameterize the\ncovariance function, and maximum likelihood (ML) methods can be used to\nestimate these parameters from data. ML methods, however, are computationally\ndemanding. For example, in the case of local likelihood estimation, even\nfitting covariance models on modest size windows can overwhelm typical\ncomputational resources for data analysis. This limitation motivates the idea\nof using neural network (NN) methods to approximate ML estimates. We train NNs\nto take moderate size spatial fields or variograms as input and return the\nrange and noise-to-signal covariance parameters. Once trained, the NNs provide\nestimates with a similar accuracy compared to ML estimation and at a speedup by\na factor of 100 or more. Although we focus on a specific covariance estimation\nproblem motivated by a climate science application, this work can be easily\nextended to other, more complex, spatial problems and provides a\nproof-of-concept for this use of machine learning in computational statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:06:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gerber", "Florian", ""], ["Nychka", "Douglas W.", ""]]}, {"id": "2012.15452", "submitter": "Jem Corcoran", "authors": "Jem N. Corcoran and Caleb Miller", "title": "Perfect Gibbs Sampling of Order Constrained Non-IID Ordered Random\n  Variates with Application to Bayesian Principal Components Analysis", "comments": "18 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Order statistics arising from $m$ independent but not identically distributed\nrandom variables are typically constructed by arranging some $X_{1}, X_{2},\n\\ldots, X_{m}$, with $X_{i}$ having distribution function $F_{i}(x)$, in\nincreasing order denoted as $X_{(1)} \\leq X_{(2)} \\leq \\ldots \\leq X_{(m)}$. In\nthis case, $X_{(i)}$ is not necessarily associated with $F_{i}(x)$. Assuming\none can simulate values from each distribution, one can generate such\n``non-iid\" order statistics by simulating $X_{i}$ from $F_{i}$, for\n$i=1,2,\\ldots, m$, and simply putting them in order. In this paper, we consider\nthe problem of simulating ordered values $X_{(1)}, X_{(2)}, \\ldots, X_{(m)}$\nsuch that the marginal distribution of $X_{(i)}$ is $F_{i}(x)$. This problem\narises in Bayesian principal components analysis (BPCA) where the $X_{i}$ are\nordered eigenvalues that are a posteriori independent but not identically\ndistributed. In this paper, we propose a novel {\\emph{coupling-from-the-past}}\nalgorithm to ``perfectly\" (up to computable order of accuracy) simulate such\n\"order-constrained non-iid\" order statistics. We demonstrate the effectiveness\nof our approach for several examples, including the BPCA problem.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 05:11:22 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Corcoran", "Jem N.", ""], ["Miller", "Caleb", ""]]}, {"id": "2012.15550", "submitter": "Maxim Panov", "authors": "Achille Thin, Nikita Kotelevskii, Christophe Andrieu, Alain Durmus,\n  Eric Moulines, Maxim Panov", "title": "Nonreversible MCMC from conditional invertible transforms: a complete\n  recipe with convergence guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) is a class of algorithms to sample complex\nand high-dimensional probability distributions. The Metropolis-Hastings (MH)\nalgorithm, the workhorse of MCMC, provides a simple recipe to construct\nreversible Markov kernels. Reversibility is a tractable property that implies a\nless tractable but essential property here, invariance. Reversibility is\nhowever not necessarily desirable when considering performance. This has\nprompted recent interest in designing kernels breaking this property. At the\nsame time, an active stream of research has focused on the design of novel\nversions of the MH kernel, some nonreversible, relying on the use of complex\ninvertible deterministic transforms. While standard implementations of the MH\nkernel are well understood, the aforementioned developments have not received\nthe same systematic treatment to ensure their validity. This paper fills the\ngap by developing general tools to ensure that a class of nonreversible Markov\nkernels, possibly relying on complex transforms, has the desired invariance\nproperty and leads to convergent algorithms. This leads to a set of simple and\npractically verifiable conditions.\n", "versions": [{"version": "v1", "created": "Thu, 31 Dec 2020 11:22:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 12:30:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Thin", "Achille", ""], ["Kotelevskii", "Nikita", ""], ["Andrieu", "Christophe", ""], ["Durmus", "Alain", ""], ["Moulines", "Eric", ""], ["Panov", "Maxim", ""]]}, {"id": "2012.15863", "submitter": "Ryan Langendorf", "authors": "Ryan E. Langendorf and Matthew G. Burgess", "title": "Empirically Classifying Network Mechanisms", "comments": "5 pages, 2 figures, 2 ancillary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are used to study interconnected systems across many physical,\nbiological, and social disciplines. Such models often assume a particular\nnetwork-generating mechanism, which when fit to data produces estimates of\nmechanism-specific parameters that describe how systems function. For instance,\na social network model might assume new individuals connect to others with\nprobability proportional to their number of pre-existing connections\n('preferential attachment'), and then estimate the disparity in interactions\nbetween famous and obscure individuals with similar qualifications. However,\nwithout a means of testing the relevance of the assumed mechanism, conclusions\nfrom such models could be misleading. Here we introduce a simple empirical\napproach which can mechanistically classify arbitrary network data. Our\napproach compares empirical networks to model networks from a user-provided\ncandidate set of mechanisms, and classifies each network--with high\naccuracy--as originating from either one of the mechanisms or none of them. We\ntested 373 empirical networks against five of the most widely studied network\nmechanisms and found that most (228) were unlike any of these mechanisms. This\nraises the possibility that some empirical networks arise from mixtures of\nmechanisms. We show that mixtures are often unidentifiable because different\nmixtures can produce functionally equivalent networks. In such systems, which\nare governed by multiple mechanisms, our approach can still accurately predict\nout-of-sample functional properties.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 01:41:34 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 16:57:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Langendorf", "Ryan E.", ""], ["Burgess", "Matthew G.", ""]]}]