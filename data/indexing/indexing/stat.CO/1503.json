[{"id": "1503.00021", "submitter": "Alex Gorodetsky", "authors": "Alex A. Gorodetsky and Youssef M. Marzouk", "title": "Mercer kernels and integrated variance experimental design: connections\n  between Gaussian process regression and polynomial approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines experimental design procedures used to develop surrogates\nof computational models, exploring the interplay between experimental designs\nand approximation algorithms. We focus on two widely used approximation\napproaches, Gaussian process (GP) regression and non-intrusive polynomial\napproximation. First, we introduce algorithms for minimizing a posterior\nintegrated variance (IVAR) design criterion for GP regression. Our formulation\ntreats design as a continuous optimization problem that can be solved with\ngradient-based methods on complex input domains, without resorting to greedy\napproximations. We show that minimizing IVAR in this way yields point sets with\ngood interpolation properties, and that it enables more accurate GP regression\nthan designs based on entropy minimization or mutual information maximization.\nSecond, using a Mercer kernel/eigenfunction perspective on GP regression, we\nidentify conditions under which GP regression coincides with pseudospectral\npolynomial approximation. Departures from these conditions can be understood as\nchanges either to the kernel or to the experimental design itself. We then show\nhow IVAR-optimal designs, while sacrificing discrete orthogonality of the\nkernel eigenfunctions, can yield lower approximation error than orthogonalizing\npoint sets. Finally, we compare the performance of adaptive Gaussian process\nregression and adaptive pseudospectral approximation for several classes of\ntarget functions, identifying features that are favorable to the GP + IVAR\napproach.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:50:03 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 19:58:16 GMT"}, {"version": "v3", "created": "Tue, 23 Feb 2016 18:41:42 GMT"}, {"version": "v4", "created": "Fri, 29 Apr 2016 03:05:27 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Gorodetsky", "Alex A.", ""], ["Marzouk", "Youssef M.", ""]]}, {"id": "1503.00266", "submitter": "Yan Zhou Yan Zhou", "authors": "Yan Zhou, Ajay Jasra", "title": "Biased Online Parameter Inference for State-Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian online static parameter estimation for state-space\nmodels. This is a very important problem, but is very computationally\nchallenging as the state- of-the art methods that are exact, often have a\ncomputational cost that grows with the time parameter; perhaps the most\nsuccessful algorithm is that of SMC2 [9]. We present a version of the SMC2\nalgorithm which has computational cost that does not grow with the time\nparameter. In addition, under assumptions, the algorithm is shown to provide\nconsistent estimates of expectations w.r.t. the posterior. However, the cost to\nachieve this consistency can be exponential in the dimension of the parameter\nspace; if this exponential cost is avoided, typically the algorithm is biased.\nThe bias is investigated from a theoretical perspective and, under assumptions,\nwe find that the bias does not accumulate as the time parameter grows. The\nalgorithm is implemented on several Bayesian statistical models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 12:50:15 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Zhou", "Yan", ""], ["Jasra", "Ajay", ""]]}, {"id": "1503.00635", "submitter": "Erin Conlon", "authors": "Alexey Miroshnikov, Evgeny Savel'ev, Erin M. Conlon", "title": "BayesSummaryStatLM: An R package for Bayesian Linear Models for Big Data\n  and Data Science", "comments": "Updated URL in reference [12]; added to description of zero.intercept\n  on p. 11; added minor clarifications throughout; results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in data science and big data research have produced an\nabundance of large data sets that are too big to be analyzed in their entirety,\ndue to limits on either computer memory or storage capacity. Here, we introduce\nour R package 'BayesSummaryStatLM' for Bayesian linear regression models with\nMarkov chain Monte Carlo implementation that overcomes these limitations. Our\nBayesian models use only summary statistics of data as input; these summary\nstatistics can be calculated from subsets of big data and combined over\nsubsets. Thus, complete data sets do not need to be read into memory in full,\nwhich removes any physical memory limitations of a user. Our package\nincorporates the R package 'ff' and its functions for reading in big data sets\nin chunks while simultaneously calculating summary statistics. We describe our\nBayesian linear regression models, including several choices of prior\ndistributions for unknown model parameters, and illustrate capabilities and\nfeatures of our R package using both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2015 17:44:38 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 23:01:04 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Miroshnikov", "Alexey", ""], ["Savel'ev", "Evgeny", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1503.00855", "submitter": "Nathan Uyttendaele", "authors": "Nathan Uyttendaele", "title": "How to speed up R code: an introduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.DC cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most calculations performed by the average R user are unremarkable in the\nsense that nowadays, any computer can crush the related code in a matter of\nseconds. But more and more often, heavy calculations are also performed using\nR, something especially true in some fields such as statistics. The user then\nfaces total execution times of his codes that are hard to work with: hours,\ndays, even weeks. In this paper, how to reduce the total execution time of\nvarious codes will be shown and typical bottlenecks will be discussed. As a\nlast resort, how to run your code on a cluster of computers (most workplaces\nhave one) in order to make use of a larger processing power than the one\navailable on an average computer will also be discussed through two examples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 08:21:32 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Uyttendaele", "Nathan", ""]]}, {"id": "1503.00890", "submitter": "C\\'ecile Proust-Lima", "authors": "C\\'ecile Proust-Lima, Viviane Philipps, Benoit Liquet", "title": "Estimation of extended mixed models using latent classes and latent\n  processes: the R package lcmm", "comments": null, "journal-ref": "Journal of Statistical Software (2017), 78(2), 1-56", "doi": "10.18637/jss.v078.i02", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package lcmm provides a series of functions to estimate statistical\nmodels based on linear mixed model theory. It includes the estimation of mixed\nmodels and latent class mixed models for Gaussian longitudinal outcomes (hlme),\ncurvilinear and ordinal univariate longitudinal outcomes (lcmm) and curvilinear\nmultivariate outcomes (multlcmm), as well as joint latent class mixed models\n(Jointlcmm) for a (Gaussian or curvilinear) longitudinal outcome and a\ntime-to-event that can be possibly left-truncated right-censored and defined in\na competing setting. Maximum likelihood esimators are obtained using a modified\nMarquardt algorithm with strict convergence criteria based on the parameters\nand likelihood stability, and on the negativity of the second derivatives. The\npackage also provides various post-fit functions including goodness-of-fit\nanalyses, classification, plots, predicted trajectories, individual dynamic\nprediction of the event and predictive accuracy assessment. This paper\nconstitutes a companion paper to the package by introducing each family of\nmodels, the estimation technique, some implementation details and giving\nexamples through a dataset on cognitive aging.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 10:47:08 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2016 13:01:19 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Proust-Lima", "C\u00e9cile", ""], ["Philipps", "Viviane", ""], ["Liquet", "Benoit", ""]]}, {"id": "1503.00966", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins and Daniel M. Roy", "title": "Sequential Monte Carlo as Approximate Sampling: bounds, adaptive\n  resampling via $\\infty$-ESS, and an application to Particle Gibbs", "comments": "34 pages", "journal-ref": "Bernoulli, Volume 25, Number 1 (2019), 584-622", "doi": "10.3150/17-BEJ999", "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC) algorithms were originally designed for\nestimating intractable conditional expectations within state-space models, but\nare now routinely used to generate approximate samples in the context of\ngeneral-purpose Bayesian inference. In particular, SMC algorithms are often\nused as subroutines within larger Monte Carlo schemes, and in this context, the\ndemands placed on SMC are different: control of mean-squared error is\ninsufficient---one needs to control the divergence from the target distribution\ndirectly. Towards this goal, we introduce the conditional adaptive resampling\nparticle filter, building on the work of Gordon, Salmond, and Smith (1993),\nAndrieu, Doucet, and Holenstein (2010), and Whiteley, Lee, and Heine (2016). By\ncontrolling a novel notion of effective sample size, the $\\infty$-ESS, we\nestablish the efficiency of the resulting SMC sampling algorithm, providing an\nadaptive resampling extension of the work of Andrieu, Lee, and Vihola (2013).\nWe apply our results to arrive at new divergence bounds for SMC samplers with\nadaptive resampling as well as an adaptive resampling version of the Particle\nGibbs algorithm with the same geometric-ergodicity guarantees as its\nnonadaptive counterpart.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 15:03:31 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 14:15:29 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1503.00996", "submitter": "Christian P. Robert", "authors": "Marco Banterle (U. Paris-Dauphine), Clara Grazian (Sapienza\n  Universit\\`a di Roma and U. Paris-Dauphine), Anthony Lee (U. Warwick), and\n  Christian P. Robert (U. Paris-Dauphine and U. Warwick)", "title": "Accelerating Metropolis-Hastings algorithms by Delayed Acceptance", "comments": "27 pages, 7 figures. This is an improved and extended version of the\n  earlier and unpublished arXiv:1406.2660", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MCMC algorithms such as Metropolis-Hastings algorithms are slowed down by the\ncomputation of complex target distributions as exemplified by huge datasets. We\noffer in this paper a useful generalisation of the Delayed Acceptance approach,\ndevised to reduce the computational costs of such algorithms by a simple and\nuniversal divide-and-conquer strategy. The idea behind the generic acceleration\nis to divide the acceptance step into several parts, aiming at a major\nreduction in computing time that out-ranks the corresponding reduction in\nacceptance probability. Each of the components can be sequentially compared\nwith a uniform variate, the first rejection signalling that the proposed value\nis considered no further. We develop moreover theoretical bounds for the\nvariance of associated estimators with respect to the variance of the standard\nMetropolis-Hastings and detail some results on optimal scaling and general\noptimisation of the procedure. We illustrate those accelerating features on a\nseries of examples\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 16:28:15 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 08:59:15 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Banterle", "Marco", "", "U. Paris-Dauphine"], ["Grazian", "Clara", "", "Sapienza\n  Universit\u00e0 di Roma and U. Paris-Dauphine"], ["Lee", "Anthony", "", "U. Warwick"], ["Robert", "Christian P.", "", "U. Paris-Dauphine and U. Warwick"]]}, {"id": "1503.01631", "submitter": "Mathieu Gerber", "authors": "Nicolas Chopin and Mathieu Gerber", "title": "Application of Sequential Quasi-Monte Carlo to Autonomous Positioning", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo algorithms (also known as particle filters) are\npopular methods to approximate filtering (and related) distributions of\nstate-space models. However, they converge at the slow $1/\\sqrt{N}$ rate, which\nmay be an issue in real-time data-intensive scenarios. We give a brief outline\nof SQMC (Sequential Quasi-Monte Carlo), a variant of SMC based on\nlow-discrepancy point sets proposed by Gerber and Chopin (2015), which\nconverges at a faster rate, and we illustrate the greater performance of SQMC\non autonomous positioning problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 13:37:13 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Chopin", "Nicolas", ""], ["Gerber", "Mathieu", ""]]}, {"id": "1503.01737", "submitter": "Ping Li", "authors": "Ping Li", "title": "Min-Max Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The min-max kernel is a generalization of the popular resemblance kernel\n(which is designed for binary data). In this paper, we demonstrate, through an\nextensive classification study using kernel machines, that the min-max kernel\noften provides an effective measure of similarity for nonnegative data. As the\nmin-max kernel is nonlinear and might be difficult to be used for industrial\napplications with massive data, we show that the min-max kernel can be\nlinearized via hashing techniques. This allows practitioners to apply min-max\nkernel to large-scale applications using well matured linear algorithms such as\nlinear SVM or logistic regression.\n  The previous remarkable work on consistent weighted sampling (CWS) produces\nsamples in the form of ($i^*, t^*$) where the $i^*$ records the location (and\nin fact also the weights) information analogous to the samples produced by\nclassical minwise hashing on binary data. Because the $t^*$ is theoretically\nunbounded, it was not immediately clear how to effectively implement CWS for\nbuilding large-scale linear classifiers. In this paper, we provide a simple\nsolution by discarding $t^*$ (which we refer to as the \"0-bit\" scheme). Via an\nextensive empirical study, we show that this 0-bit scheme does not lose\nessential information. We then apply the \"0-bit\" CWS for building linear\nclassifiers to approximate min-max kernel classifiers, as extensively validated\non a wide range of publicly available classification datasets. We expect this\nwork will generate interests among data mining practitioners who would like to\nefficiently utilize the nonlinear information of non-binary and nonnegative\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 19:29:03 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1503.01842", "submitter": "Benoit Liquet Dr", "authors": "Tim Benham and Qibin Duan and Dirk P. Kroese and Benoit Liquet", "title": "CEoptim: Cross-Entropy R Package for Optimization", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-entropy (CE) method is simple and versatile technique for\noptimization, based on Kullback-Leibler (or cross-entropy) minimization. The\nmethod can be applied to a wide range of optimization tasks, including\ncontinuous, discrete, mixed and constrained optimization problems. The new\npackage CEoptim provides the R implementation of the CE method for\noptimization. We describe the general CE methodology for optimization and well\nas some useful modifications. The usage and efficacy of CEoptim is demonstrated\nthrough a variety of optimization examples, including model fitting,\ncombinatorial optimization, and maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 03:43:34 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Benham", "Tim", ""], ["Duan", "Qibin", ""], ["Kroese", "Dirk P.", ""], ["Liquet", "Benoit", ""]]}, {"id": "1503.01890", "submitter": "Aristides Moustakas", "authors": "Aristides Moustakas, and Matthew R. Evans", "title": "Coupling models of cattle and farms with models of badgers for\n  predicting the dynamics of bovine tuberculosis (TB)", "comments": null, "journal-ref": "Stochastic Environmental Research and Risk Assessment (2015), Vol\n  29, Issue 3, pp 623-635", "doi": "10.1007/s00477-014-1016-y", "report-no": null, "categories": "q-bio.PE cs.CE math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bovine TB is a major problem for the agricultural industry in several\ncountries. TB can be contracted and spread by species other than cattle and\nthis can cause a problem for disease control. In the UK and Ireland, badgers\nare a recognised reservoir of infection and there has been substantial\ndiscussion about potential control strategies. We present a coupling of\nindividual based models of bovine TB in badgers and cattle, which aims to\ncapture the key details of the natural history of the disease and of both\nspecies at approximately county scale. The model is spatially explicit it\nfollows a very large number of cattle and badgers on a different grid size for\neach species and includes also winter housing. We show that the model can\nreplicate the reported dynamics of both cattle and badger populations as well\nas the increasing prevalence of the disease in cattle. Parameter space used as\ninput in simulations was swept out using Latin hypercube sampling and\nsensitivity analysis to model outputs was conducted using mixed effect models.\nBy exploring a large and computationally intensive parameter space we show that\nof the available control strategies it is the frequency of TB testing and\nwhether or not winter housing is practised that have the most significant\neffects on the number of infected cattle, with the effect of winter housing\nbecoming stronger as farm size increases. Whether badgers were culled or not\nexplained about 5%, while the accuracy of the test employed to detect infected\ncattle explained less than 3% of the variance in the number of infected cattle.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 09:43:45 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Moustakas", "Aristides", ""], ["Evans", "Matthew R.", ""]]}, {"id": "1503.02188", "submitter": "Nicholas Horton", "authors": "Nicholas Jon Horton", "title": "Challenges and opportunities for statistics and statistical education:\n  looking back, looking forward", "comments": "In press: The American Statistician", "journal-ref": null, "doi": "10.1080/00031305.2015.1032435", "report-no": null, "categories": "stat.OT stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 175th anniversary of the ASA provides an opportunity to look back into\nthe past and peer into the future. What led our forebears to found the\nassociation? What commonalities do we still see? What insights might we glean\nfrom their experiences and observations? I will use the anniversary as a chance\nto reflect on where we are now and where we are headed in terms of statistical\neducation amidst the growth of data science. Statistics is the science of\nlearning from data. By fostering more multivariable thinking, building\ndata-related skills, and developing simulation-based problem solving, we can\nhelp to ensure that statisticians are fully engaged in data science and the\nanalysis of the abundance of data now available to us.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 16:46:59 GMT"}, {"version": "v2", "created": "Sun, 15 Mar 2015 13:06:33 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 14:51:44 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Horton", "Nicholas Jon", ""]]}, {"id": "1503.02222", "submitter": "Yuefeng Wu", "authors": "Huaiye Zhang, Yuefeng Wu, Lulu Cheng and Inyoung Kim", "title": "Hit and Run ARMS: Adaptive Rejection Metropolis Sampling with Hit and\n  Run Random Direction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for sampling from non-log-concave multivariate distributions is\nproposed, which improves the adaptive rejection Metropolis sampling (ARMS)\nalgorithm by incorporating the hit and run sampling. It is not rare that the\nARMS is trapped away from some subspace with significant probability in the\nsupport of the multivariate distribution. While the ARMS updates samples only\nin the directions that are parallel to dimensions, our proposed method, the hit\nand run ARMS (HARARMS), updates samples in arbitrary directions determined by\nthe hit and run algorithm, which makes it almost not possible to be trapped in\nany isolated subspaces. The HARARMS performs the same as ARMS in a single\ndimension while more reliable in multidimensional spaces. Its performance is\nillustrated by a Bayesian free-knot spline regression example. We showed that\nit overcomes the well-known `lethargy' property and decisively find the global\noptimal number and locations of the knots of the spline function.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 22:35:13 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Zhang", "Huaiye", ""], ["Wu", "Yuefeng", ""], ["Cheng", "Lulu", ""], ["Kim", "Inyoung", ""]]}, {"id": "1503.02271", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis", "title": "label.switching: An R Package for Dealing with the Label Switching\n  Problem in MCMC Outputs", "comments": "Accepted to Journal of Statistical Software", "journal-ref": "Journal of Statistical Software, 2016, 69(1), 1-24", "doi": "10.18637/jss.v069.c01", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label switching is a well-known and fundamental problem in Bayesian\nestimation of mixture or hidden Markov models. In case that the prior\ndistribution of the model parameters is the same for all states, then both the\nlikelihood and posterior distribution are invariant to permutations of the\nparameters. This property makes Markov chain Monte Carlo (MCMC) samples\nsimulated from the posterior distribution non-identifiable. In this paper, the\n\\pkg{label.switching} package is introduced. It contains one probabilistic and\nseven deterministic relabelling algorithms in order to post-process a given\nMCMC sample, provided by the user. Each method returns a set of permutations\nthat can be used to reorder the MCMC output. Then, any parametric function of\ninterest can be inferred using the reordered MCMC sample. A set of user-defined\npermutations is also accepted, allowing the researcher to benchmark new\nrelabelling methods against the available ones\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2015 12:51:48 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Papastamoulis", "Panagiotis", ""]]}, {"id": "1503.02644", "submitter": "Jason Xu", "authors": "Jason Xu and Vladimir N. Minin", "title": "Efficient Transition Probability Computation for Continuous-Time\n  Branching Processes via Compressed Sensing", "comments": "18 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Branching processes are a class of continuous-time Markov chains (CTMCs) with\nubiquitous applications. A general difficulty in statistical inference under\npartially observed CTMC models arises in computing transition probabilities\nwhen the discrete state space is large or uncountable. Classical methods such\nas matrix exponentiation are infeasible for large or countably infinite state\nspaces, and sampling-based alternatives are computationally intensive,\nrequiring a large integration step to impute over all possible hidden events.\nRecent work has successfully applied generating function techniques to\ncomputing transition probabilities for linear multitype branching processes.\nWhile these techniques often require significantly fewer computations than\nmatrix exponentiation, they also become prohibitive in applications with large\npopulations. We propose a compressed sensing framework that significantly\naccelerates the generating function method, decreasing computational cost up to\na logarithmic factor by only assuming the probability mass of transitions is\nsparse. We demonstrate accurate and efficient transition probability\ncomputations in branching process models for hematopoiesis and transposable\nelement evolution.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 19:43:07 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Xu", "Jason", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1503.02737", "submitter": "Art Owen", "authors": "K. Basu and A. B. Owen", "title": "Scrambled geometric net integration over general product spaces", "comments": "29 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Monte Carlo (QMC) sampling has been developed for integration over\n$[0,1]^s$ where it has superior accuracy to Monte Carlo (MC) for integrands of\nbounded variation. Scrambled net quadrature gives allows replication based\nerror estimation for QMC with at least the same accuracy and for smooth enough\nintegrands even better accuracy than plain QMC. Integration over triangles,\nspheres, disks and Cartesian products of such spaces is more difficult for QMC\nbecause the induced integrand on a unit cube may fail to have the desired\nregularity. In this paper, we present a construction of point sets for\nnumerical integration over Cartesian products of $s$ spaces of dimension $d$,\nwith triangles ($d=2$) being of special interest. The point sets are\ntransformations of randomized $(t,m,s)$-nets using recursive geometric\npartitions. The resulting integral estimates are unbiased and their variance is\n$o(1/n)$ for any integrand in $L^2$ of the product space. Under smoothness\nassumptions on the integrand, our randomized QMC algorithm has variance\n$O(n^{-1 - 2/d} (\\log n)^{s-1})$, for integration over $s$-fold Cartesian\nproducts of $d$-dimensional domains, compared to $O(n^{-1})$ for ordinary Monte\nCarlo.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 00:12:57 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Basu", "K.", ""], ["Owen", "A. B.", ""]]}, {"id": "1503.02912", "submitter": "Clara Grazian", "authors": "Clara Grazian and Brunero Liseo", "title": "Approximate Bayesian inference in semiparametric copula models", "comments": "27 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a simple method for making inference on a functional of a\nmultivariate distribution. The method is based on a copula representation of\nthe multivariate distribution and it is based on the properties of an\nApproximate Bayesian Monte Carlo algorithm, where the proposed values of the\nfunctional of interest are weighed in terms of their empirical likelihood. This\nmethod is particularly useful when the \"true\" likelihood function associated\nwith the working model is too costly to evaluate or when the working model is\nonly partially specified.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:06:55 GMT"}, {"version": "v2", "created": "Fri, 15 Jul 2016 11:30:41 GMT"}, {"version": "v3", "created": "Tue, 27 Dec 2016 15:09:39 GMT"}, {"version": "v4", "created": "Sun, 16 Jul 2017 11:39:48 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Grazian", "Clara", ""], ["Liseo", "Brunero", ""]]}, {"id": "1503.03626", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi and Alan Edelman", "title": "Integral geometry for Markov chain Monte Carlo: overcoming the curse of\n  search-subspace dimensionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.DG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method that uses the Cauchy-Crofton formula and a new\ncurvature formula from integral geometry to reweight the sampling probabilities\nof Metropolis-within-Gibbs algorithms in order to increase their convergence\nspeed. We consider algorithms that sample from a probability density\nconditioned on a manifold $\\mathcal{M}$. Our method exploits the symmetries of\nthe algorithms' isotropic random search-direction subspaces to analytically\naverage out the variance in the intersection volume caused by the orientation\nof the search-subspace with respect to the manifold $\\mathcal{M}$ it\nintersects. This variance can grow exponentially with the dimension of the\nsearch-subspace, greatly slowing down the algorithm. Eliminating this variance\nallows us to use search-subspaces of dimensions many times greater than would\notherwise be possible, allowing us to sample very rare events that a\nlower-dimensional search-subspace would be unlikely to intersect.\n  To extend this method to events that are rare for reasons other than their\nsupport $\\mathcal{M}$ having a lower dimension, we formulate and prove a new\ntheorem in integral geometry that makes use of the curvature form of the\nChern-Gauss-Bonnet theorem to reweight sampling probabilities. On the side, we\nalso apply our theorem to obtain new theoretical bounds for the volumes of real\nalgebraic manifolds.\n  Finally, we demonstrate the computational effectiveness and speedup of our\nmethod by numerically applying it to the conditional stochastic Airy operator\nsampling problem in random matrix theory.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 08:43:53 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Mangoubi", "Oren", ""], ["Edelman", "Alan", ""]]}, {"id": "1503.03761", "submitter": "Xiaoyu Liu", "authors": "Xiaoyu Liu, Serge Guillas and Ming-Jun Lai", "title": "Efficient spatial modelling using the SPDE approach with bivariate\n  splines", "comments": "26 pages, 7 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian fields (GFs) are frequently used in spatial statistics for their\nversatility. The associated computational cost can be a bottleneck, especially\nin realistic applications. It has been shown that computational efficiency can\nbe gained by doing the computations using Gaussian Markov random fields (GMRFs)\nas the GFs can be seen as weak solutions to corresponding stochastic partial\ndifferential equations (SPDEs) using piecewise linear finite elements. We\nintroduce a new class of representations of GFs with bivariate splines instead\nof finite elements. This allows an easier implementation of piecewise\npolynomial representations of various degrees. It leads to GMRFs that can be\ninferred efficiently and can be easily extended to non-stationary fields. The\nsolutions approximated with higher order bivariate splines converge faster,\nhence the computational cost can be alleviated. Numerical simulations using\nboth real and simulated data also demonstrate that our framework increases the\nflexibility and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 15:12:03 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Liu", "Xiaoyu", ""], ["Guillas", "Serge", ""], ["Lai", "Ming-Jun", ""]]}, {"id": "1503.03769", "submitter": "Wang Junjie", "authors": "Wang Junjie and Zhao Lingling and Su Xiaohong and Ma Peijun", "title": "Distributed Computation Particle PHD filter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle probability hypothesis density filtering has become a promising\nmeans for multi-target tracking due to its capability of handling an unknown\nand time-varying number of targets in non-linear non-Gaussian system. However,\nits computational complexity grows linearly with the number of measurements and\nparticles assigned to each target, and this can be very time consuming\nespecially when numerous targets and clutter exist in the surveillance region.\nAddressing this issue, we present a distributed computation particle PHD filter\nfor target tracking. Its framework consists of several local particle PHD\nfilters at each processing element and a central unit. Each processing element\ntakes responsibility for part particles but full measurements and provides\nlocal estimates; central unit controls particle exchange between processing\nelements and specifies a fusion rule to match and fuse the estimates from\ndifferent local filters. The proposed framework is suitable for parallel\nimplementation and maintains the tracking accuracy. Simulations verify the\nproposed method can provide comparative accuracy as well as a significant\nspeedup with the standard particle PHD filter.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 14:55:05 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Junjie", "Wang", ""], ["Lingling", "Zhao", ""], ["Xiaohong", "Su", ""], ["Peijun", "Ma", ""]]}, {"id": "1503.03879", "submitter": "Sanjay Chaudhuri", "authors": "Sanjay Chaudhuri", "title": "Qualitative inequalities for squared partial correlations of a Gaussian\n  random vector", "comments": "21 pages, 13 figures", "journal-ref": "Annals of the Institute of Statistical Mathematics, 66(2),\n  345-367, 2014", "doi": "10.1007/s10463-013-0417-x", "report-no": "Department of Statistics and Applied Probability, National\n  University of Singapore technical report 201301", "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe various sets of conditional independence relationships,\nsufficient for qualitatively comparing non-vanishing squared partial\ncorrelations of a Gaussian random vector. These sufficient conditions are\nsatisfied by several graphical Markov models. Rules for comparing degree of\nassociation among the vertices of such Gaussian graphical models are also\ndeveloped. We apply these rules to compare conditional dependencies on Gaussian\ntrees. In particular for trees, we show that such dependence can be completely\ncharacterized by the length of the paths joining the dependent vertices to each\nother and to the vertices conditioned on. We also apply our results to\npostulate rules for model selection for polytree models. Our rules apply to\nmutual information of Gaussian random vectors as well.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 20:15:35 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Chaudhuri", "Sanjay", ""]]}, {"id": "1503.04123", "submitter": "Daniel Rudolf", "authors": "Daniel Rudolf and Nikolaus Schweizer", "title": "Perturbation theory for Markov chains via Wasserstein distance", "comments": "31 pages, accepted at Bernoulli Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perturbation theory for Markov chains addresses the question how small\ndifferences in the transitions of Markov chains are reflected in differences\nbetween their distributions. We prove powerful and flexible bounds on the\ndistance of the $n$th step distributions of two Markov chains when one of them\nsatisfies a Wasserstein ergodicity condition. Our work is motivated by the\nrecent interest in approximate Markov chain Monte Carlo (MCMC) methods in the\nanalysis of big data sets. By using an approach based on Lyapunov functions, we\nprovide estimates for geometrically ergodic Markov chains under weak\nassumptions. In an autoregressive model, our bounds cannot be improved in\ngeneral. We illustrate our theory by showing quantitative estimates for\napproximate versions of two prominent MCMC algorithms, the Metropolis-Hastings\nand stochastic Langevin algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 16:00:46 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 08:36:17 GMT"}, {"version": "v3", "created": "Thu, 23 Feb 2017 19:54:53 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Rudolf", "Daniel", ""], ["Schweizer", "Nikolaus", ""]]}, {"id": "1503.04363", "submitter": "Amit Moscovich", "authors": "Amit Moscovich, Boaz Nadler", "title": "Fast calculation of boundary crossing probabilities for Poisson\n  processes", "comments": "8 pages, 2 figures, associated C++ code is available at\n  http://www.wisdom.weizmann.ac.il/~amitmo", "journal-ref": "Statistics & Probability Letters 123 (2017) 177-182", "doi": "10.1016/j.spl.2016.11.027", "report-no": null, "categories": "stat.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The boundary crossing probability of a Poisson process with $n$ jumps is a\nfundamental quantity with numerous applications. We present a fast $O(n^2 \\log\nn)$ algorithm to calculate this probability for arbitrary upper and lower\nboundaries.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 23:55:23 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 14:22:41 GMT"}, {"version": "v3", "created": "Sun, 5 Jun 2016 09:32:11 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Moscovich", "Amit", ""], ["Nadler", "Boaz", ""]]}, {"id": "1503.04662", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine and University of\n  Warwick) and Jean-Michel Marin (Universite de Montpellier)", "title": "Bayesian Essentials with R: The Complete Solution Manual", "comments": "117 pages, 124 exercises, 22 figures. arXiv admin note: substantial\n  text overlap with arXiv:0910.4696", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the collection of solutions for all the exercises proposed in\nBayesian Essentials with R (2014).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 14:19:21 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine and University of\n  Warwick"], ["Marin", "Jean-Michel", "", "Universite de Montpellier"]]}, {"id": "1503.05210", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Speeding up lower bound estimation in powerlaw distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The traditional lower bound estimation method for powerlaw distributions\nbased on the Kolmogorov-Smirnov distance proved to perform better than other\ncompeting methods. However, if applied to very large collections of data, such\na method can be computationally demanding. In this paper, we propose two\nalternative methods with the aim to reduce the time required by the estimation\nprocedure. We apply the traditional method and the two proposed methods to\nlarge collections of data ($N = 500,000$) with varying values of the true lower\nbound. Both the proposed methods yield a significantly better performance and\naccuracy than the traditional method.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 20:20:15 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1503.05570", "submitter": "Benjamin Baumer", "authors": "Ben Baumer", "title": "A Data Science Course for Undergraduates: Thinking with Data", "comments": "21 pages total including supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data science is an emerging interdisciplinary field that combines elements of\nmathematics, statistics, computer science, and knowledge in a particular\napplication domain for the purpose of extracting meaningful information from\nthe increasingly sophisticated array of data available in many settings. These\ndata tend to be non-traditional, in the sense that they are often live, large,\ncomplex, and/or messy. A first course in statistics at the undergraduate level\ntypically introduces students with a variety of techniques to analyze small,\nneat, and clean data sets. However, whether they pursue more formal training in\nstatistics or not, many of these students will end up working with data that is\nconsiderably more complex, and will need facility with statistical computing\ntechniques. More importantly, these students require a framework for thinking\nstructurally about data. We describe an undergraduate course in a liberal arts\nenvironment that provides students with the tools necessary to apply data\nscience. The course emphasizes modern, practical, and useful skills that cover\nthe full data analysis spectrum, from asking an interesting question to\nacquiring, managing, manipulating, processing, querying, analyzing, and\nvisualizing data, as well communicating findings in written, graphical, and\noral forms.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 20:05:24 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Baumer", "Ben", ""]]}, {"id": "1503.05621", "submitter": "Daniel Turek", "authors": "Daniel Turek, Perry de Valpine, Christopher J. Paciorek, Clifford\n  Anderson-Bergman", "title": "Automated Parameter Blocking for Efficient Markov-Chain Monte Carlo\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) sampling is an important and commonly used\ntool for the analysis of hierarchical models. Nevertheless, practitioners\ngenerally have two options for MCMC: utilize existing software that generates a\nblack-box \"one size fits all\" algorithm, or the challenging (and time\nconsuming) task of implementing a problem-specific MCMC algorithm. Either\nchoice may result in inefficient sampling, and hence researchers have become\naccustomed to MCMC runtimes on the order of days (or longer) for large models.\nWe propose an automated procedure to determine an efficient MCMC algorithm for\na given model and computing platform. Our procedure dynamically determines\nblocks of parameters for joint sampling that result in efficient sampling of\nthe entire model. We test this procedure using a diverse suite of example\nmodels, and observe non-trivial improvements in MCMC efficiency for many\nmodels. Our procedure is the first attempt at such, and may be generalized to a\nbroader space of MCMC algorithms. Our results suggest that substantive\nimprovements in MCMC efficiency may be practically realized using our automated\nblocking procedure, or variants thereof, which warrants additional study and\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 00:20:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Turek", "Daniel", ""], ["de Valpine", "Perry", ""], ["Paciorek", "Christopher J.", ""], ["Anderson-Bergman", "Clifford", ""]]}, {"id": "1503.06058", "submitter": "Johan Dahlin", "authors": "Thomas B. Sch\\\"on, Fredrik Lindsten, Johan Dahlin, Johan W{\\aa}gberg,\n  Christian A. Naesseth, Andreas Svensson and Liang Dai", "title": "Sequential Monte Carlo Methods for System Identification", "comments": "In proceedings of the 17th IFAC Symposium on System Identification\n  (SYSID). Added cover page", "journal-ref": null, "doi": "10.1016/j.ifacol.2015.12.224", "report-no": null, "categories": "stat.CO math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in identifying nonlinear and possibly non-Gaussian\nstate space models (SSMs) is the intractability of estimating the system state.\nSequential Monte Carlo (SMC) methods, such as the particle filter (introduced\nmore than two decades ago), provide numerical solutions to the nonlinear state\nestimation problems arising in SSMs. When combined with additional\nidentification techniques, these algorithms provide solid solutions to the\nnonlinear system identification problem. We describe two general strategies for\ncreating such combinations and discuss why SMC is a natural tool for\nimplementing these strategies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 13:06:38 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 21:22:57 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 14:44:32 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Sch\u00f6n", "Thomas B.", ""], ["Lindsten", "Fredrik", ""], ["Dahlin", "Johan", ""], ["W\u00e5gberg", "Johan", ""], ["Naesseth", "Christian A.", ""], ["Svensson", "Andreas", ""], ["Dai", "Liang", ""]]}, {"id": "1503.06410", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "What the F-measure doesn't measure: Features, Flaws, Fallacies and Fixes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": "KIT-14-001", "categories": "cs.IR cs.CL cs.LG cs.NE stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The F-measure or F-score is one of the most commonly used single number\nmeasures in Information Retrieval, Natural Language Processing and Machine\nLearning, but it is based on a mistake, and the flawed assumptions render it\nunsuitable for use in most contexts! Fortunately, there are better\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 11:32:34 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 05:42:44 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1503.06606", "submitter": "Henri Nurminen M.Sc.", "authors": "Henri Nurminen, Tohid Ardeshiri, Robert Pich\\'e, and Fredrik\n  Gustafsson", "title": "Robust Inference for State-Space Models with Skewed Measurement Noise", "comments": "5 pages, 7 figures. Accepted for publication in IEEE Signal\n  Processing Letters", "journal-ref": "IEEE Signal Processing Letters 22(11) (2015) 1898-1902", "doi": "10.1109/LSP.2015.2437456", "report-no": null, "categories": "cs.SY stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering and smoothing algorithms for linear discrete-time state-space\nmodels with skewed and heavy-tailed measurement noise are presented. The\nalgorithms use a variational Bayes approximation of the posterior distribution\nof models that have normal prior and skew-t-distributed measurement noise. The\nproposed filter and smoother are compared with conventional low-complexity\nalternatives in a simulated pseudorange positioning scenario. In the\nsimulations the proposed methods achieve better accuracy than the alternative\nmethods, the computational complexity of the filter being roughly 5 to 10 times\nthat of the Kalman filter.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 11:40:19 GMT"}, {"version": "v2", "created": "Fri, 22 May 2015 12:36:27 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Nurminen", "Henri", ""], ["Ardeshiri", "Tohid", ""], ["Pich\u00e9", "Robert", ""], ["Gustafsson", "Fredrik", ""]]}, {"id": "1503.06769", "submitter": "Matthias Morzfeld", "authors": "Matthias Morzfeld and Daniel Hodyss", "title": "Analysis of the ensemble Kalman filter for marginal and joint posteriors", "comments": "I submitted a much improved and revised version which has very little\n  to do with this version of the article", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ensemble Kalman filter (EnKF) is widely used to sample a probability\ndensity function (pdf) generated by a stochastic model conditioned by noisy\ndata. This pdf can be either a joint posterior that describes the evolution of\nthe state of the system in time, conditioned on all the data up to the present,\nor a particular marginal of this posterior. We show that the EnKF collapses in\nthe same way and under even broader conditions as a particle filter when it\nsamples the joint posterior. However, this does not imply that EnKF collapses\nwhen it samples the marginal posterior. We we show that a localized and\ninflated EnKF can efficiently sample this marginal, and argue that the marginal\nposterior is often the more useful pdf in geophysics. This explains the wide\napplicability of EnKF in this field. We further investigate the typical tuning\nof EnKF, in which one attempts to match the mean square error (MSE) to the\nmarginal posterior variance, and show that sampling error may be huge, even if\nthe MSE is moderate.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 19:06:47 GMT"}, {"version": "v2", "created": "Fri, 29 May 2015 19:50:44 GMT"}, {"version": "v3", "created": "Fri, 11 Dec 2015 20:32:17 GMT"}, {"version": "v4", "created": "Fri, 5 Aug 2016 18:00:06 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Morzfeld", "Matthias", ""], ["Hodyss", "Daniel", ""]]}, {"id": "1503.06910", "submitter": "Enayetur Raheem", "authors": "Enayetur Raheem, A. K. Md. Ehsanes Saleh", "title": "Penalty, Shrinkage, and Preliminary Test Estimators under Full Model\n  Hypothesis", "comments": "28 pages, 4 figures, 10 tables. arXiv admin note: text overlap with\n  arXiv:1503.05160", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a multiple regression model and compares, under full\nmodel hypothesis, analytically as well as by simulation, the performance\ncharacteristics of some popular penalty estimators such as ridge regression,\nLASSO, adaptive LASSO, SCAD, and elastic net versus Least Squares Estimator,\nrestricted estimator, preliminary test estimator, and Stein-type estimators\nwhen the dimension of the parameter space is smaller than the sample space\ndimension. We find that RR uniformly dominates LSE, RE, PTE, SE and PRSE while\nLASSO, aLASSO, SCAD, and EN uniformly dominates LSE only. Further, it is\nobserved that neither penalty estimators nor Stein-type estimator dominate one\nanother.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 04:40:53 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Raheem", "Enayetur", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1503.07066", "submitter": "Felipe Medina-Aguayo", "authors": "Felipe J. Medina-Aguayo, Anthony Lee, Gareth O. Roberts", "title": "Stability of Noisy Metropolis-Hastings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pseudo-marginal Markov chain Monte Carlo methods for sampling from\nintractable distributions have gained recent interest and have been\ntheoretically studied in considerable depth. Their main appeal is that they are\nexact, in the sense that they target marginally the correct invariant\ndistribution. However, the pseudo-marginal Markov chain can exhibit poor mixing\nand slow convergence towards its target. As an alternative, a subtly different\nMarkov chain can be simulated, where better mixing is possible but the\nexactness property is sacrificed. This is the noisy algorithm, initially\nconceptualised as Monte Carlo within Metropolis (MCWM), which has also been\nstudied but to a lesser extent. The present article provides a further\ncharacterisation of the noisy algorithm, with a focus on fundamental stability\nproperties like positive recurrence and geometric ergodicity. Sufficient\nconditions for inheriting geometric ergodicity from a standard\nMetropolis-Hastings chain are given, as well as convergence of the invariant\ndistribution towards the true target distribution.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 15:06:41 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Medina-Aguayo", "Felipe J.", ""], ["Lee", "Anthony", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1503.07259", "submitter": "Ajay Jasra", "authors": "Alexandros Beskos, Ajay Jasra, Kody Law, Raul Tempone, Yan Zhou", "title": "Multilevel Sequential Monte Carlo Samplers", "comments": null, "journal-ref": null, "doi": "10.1016/j.spa.2016.08.004", "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the approximation of expectations w.r.t.\nprobability distributions associated to the solution of partial differential\nequations (PDEs); this scenario appears routinely in Bayesian inverse problems.\nIn practice, one often has to solve the associated PDE numerically, using, for\ninstance finite element methods and leading to a discretisation bias, with the\nstep-size level $h_L$. In addition, the expectation cannot be computed\nanalytically and one often resorts to Monte Carlo methods. In the context of\nthis problem, it is known that the introduction of the multilevel Monte Carlo\n(MLMC) method can reduce the amount of computational effort to estimate\nexpectations, for a given level of error. This is achieved via a telescoping\nidentity associated to a Monte Carlo approximation of a sequence of probability\ndistributions with discretisation levels $\\infty>h_0>h_1\\cdots>h_L$. In many\npractical problems of interest, one cannot achieve an i.i.d. sampling of the\nassociated sequence of probability distributions. A sequential Monte Carlo\n(SMC) version of the MLMC method is introduced to deal with this problem. It is\nshown that under appropriate assumptions, the attractive property of a\nreduction of the amount of computational effort to estimate expectations, for a\ngiven level of error, can be maintained within the SMC context.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 01:44:13 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Beskos", "Alexandros", ""], ["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Tempone", "Raul", ""], ["Zhou", "Yan", ""]]}, {"id": "1503.07307", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad and H{\\aa}vard Rue", "title": "Improving the INLA approach for approximate Bayesian inference for\n  latent Gaussian models", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 9, Number 2 (2015),\n  2706-2731", "doi": "10.1214/15-EJS1092", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new copula-based correction for generalized linear mixed\nmodels (GLMMs) within the integrated nested Laplace approximation (INLA)\napproach for approximate Bayesian inference for latent Gaussian models. While\nINLA is usually very accurate, some (rather extreme) cases of GLMMs with e.g.\nbinomial or Poisson data have been seen to be problematic. Inaccuracies can\noccur when there is a very low degree of smoothing or \"borrowing strength\"\nwithin the model, and we have therefore developed a correction aiming to push\nthe boundaries of the applicability of INLA. Our new correction has been\nimplemented as part of the R-INLA package, and adds only negligible\ncomputational cost. Empirical evaluations on both real and simulated data\nindicate that the method works well.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 09:00:22 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 17:14:30 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2015 14:24:17 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 11:21:11 GMT"}, {"version": "v5", "created": "Tue, 24 Nov 2015 09:45:06 GMT"}, {"version": "v6", "created": "Tue, 15 Dec 2015 09:23:47 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Ferkingstad", "Egil", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1503.07689", "submitter": "Jean-Michel Marin", "authors": "Jean-Michel Marin (U. Montpellier), Pierre Pudlo (Aix-Marseille U.),\n  Arnaud Estoup (CBGP, INRA, Montpellier) and Christian P. Robert (U.\n  Paris-Dauphine and U. Warwick)", "title": "Likelihood-free Model Choice", "comments": "21 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is an invited chapter covering the specificities of ABC model\nchoice, intended for the incoming Handbook of ABC by Sisson, Fan, and Beaumont\n(2017). Beyond exposing the potential pitfalls of ABC based posterior\nprobabilities, the review emphasizes mostly the solution proposed by Pudlo et\nal. (2016) on the use of random forests for aggregating summary statistics and\nand for estimating the posterior probability of the most likely model via a\nsecondary random fores.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 11:17:22 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:02:41 GMT"}, {"version": "v3", "created": "Fri, 16 Sep 2016 09:13:14 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["Marin", "Jean-Michel", "", "U. Montpellier"], ["Pudlo", "Pierre", "", "Aix-Marseille U."], ["Estoup", "Arnaud", "", "CBGP, INRA, Montpellier"], ["Robert", "Christian P.", "", "U.\n  Paris-Dauphine and U. Warwick"]]}, {"id": "1503.07791", "submitter": "Fernando V. Bonassi", "authors": "Fernando V. Bonassi, Mike West", "title": "Sequential Monte Carlo with Adaptive Weights for Approximate Bayesian\n  Computation", "comments": "Published at http://dx.doi.org/10.1214/14-BA891 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 1, 171-187", "doi": "10.1214/14-BA891", "report-no": "VTeX-BA-BA891", "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of approximate Bayesian computation (ABC) are increasingly used for\nanalysis of complex models. A major challenge for ABC is over-coming the often\ninherent problem of high rejection rates in the accept/reject methods based on\nprior:predictive sampling. A number of recent developments aim to address this\nwith extensions based on sequential Monte Carlo (SMC) strategies. We build on\nthis here, introducing an ABC SMC method that uses data-based adaptive weights.\nThis easily implemented and computationally trivial extension of ABC SMC can\nvery substantially improve acceptance rates, as is demonstrated in a series of\nexamples with simulated and real data sets, including a currently topical\nexample from dynamic modelling in systems biology applications.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 17:12:57 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Bonassi", "Fernando V.", ""], ["West", "Mike", ""]]}, {"id": "1503.08060", "submitter": "Guillaume Dehaene", "authors": "Guillaume Dehaene, Simon Barthelm\\'e", "title": "Expectation Propagation in the large-data limit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation Propagation (Minka, 2001) is a widely successful algorithm for\nvariational inference. EP is an iterative algorithm used to approximate\ncomplicated distributions, typically to find a Gaussian approximation of\nposterior distributions. In many applications of this type, EP performs\nextremely well. Surprisingly, despite its widespread use, there are very few\ntheoretical guarantees on Gaussian EP, and it is quite poorly understood.\n  In order to analyze EP, we first introduce a variant of EP: averaged-EP\n(aEP), which operates on a smaller parameter space. We then consider aEP and EP\nin the limit of infinite data, where the overall contribution of each\nlikelihood term is small and where posteriors are almost Gaussian. In this\nlimit, we prove that the iterations of both aEP and EP are simple: they behave\nlike iterations of Newton's algorithm for finding the mode of a function. We\nuse this limit behavior to prove that EP is asymptotically exact, and to obtain\nother insights into the dynamic behavior of EP: for example, that it may\ndiverge under poor initialization exactly like Newton's method. EP is a simple\nalgorithm to state, but a difficult one to study. Our results should facilitate\nfurther research into the theoretical properties of this important method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 13:15:42 GMT"}, {"version": "v2", "created": "Thu, 31 Mar 2016 11:32:47 GMT"}], "update_date": "2016-04-01", "authors_parsed": [["Dehaene", "Guillaume", ""], ["Barthelm\u00e9", "Simon", ""]]}, {"id": "1503.08066", "submitter": "Matthew Moores", "authors": "Matthew T. Moores, Geoff K. Nicholls, Anthony N. Pettitt and Kerrie\n  Mengersen", "title": "Scalable Bayesian Inference for the Inverse Temperature of a Hidden\n  Potts Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse temperature parameter of the Potts model governs the strength of\nspatial cohesion and therefore has a major influence over the resulting model\nfit. A difficulty arises from the dependence of an intractable normalising\nconstant on the value of this parameter and thus there is no closed-form\nsolution for sampling from the posterior distribution directly. There are a\nvariety of computational approaches for sampling from the posterior without\nevaluating the normalising constant, including the exchange algorithm and\napproximate Bayesian computation (ABC). A serious drawback of these algorithms\nis that they do not scale well for models with a large state space, such as\nimages with a million or more pixels. We introduce a parametric surrogate\nmodel, which approximates the score function using an integral curve. Our\nsurrogate model incorporates known properties of the likelihood, such as\nheteroskedasticity and critical temperature. We demonstrate this method using\nsynthetic data as well as remotely-sensed imagery from the Landsat-8 satellite.\nWe achieve up to a hundredfold improvement in the elapsed runtime, compared to\nthe exchange algorithm or ABC. An open source implementation of our algorithm\nis available in the R package \"bayesImageS.\"\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 13:28:27 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 17:54:34 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 12:32:31 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Moores", "Matthew T.", ""], ["Nicholls", "Geoff K.", ""], ["Pettitt", "Anthony N.", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "1503.08224", "submitter": "Hsin-Ta Wu", "authors": "Mark D.M. Leiserson, Hsin-Ta Wu, Fabio Vandin, Benjamin J. Raphael", "title": "CoMEt: A Statistical Approach to Identify Combinations of Mutually\n  Exclusive Alterations in Cancer", "comments": "Accepted to RECOMB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Cancer is a heterogeneous disease with different combinations of genetic and\nepigenetic alterations driving the development of cancer in different\nindividuals. While these alterations are believed to converge on genes in key\ncellular signaling and regulatory pathways, our knowledge of these pathways\nremains incomplete, making it difficult to identify driver alterations by their\nrecurrence across genes or known pathways. We introduce Combinations of\nMutually Exclusive Alterations (CoMEt), an algorithm to identify combinations\nof alterations de novo, without any prior biological knowledge (e.g. pathways\nor protein interactions). CoMEt searches for combinations of mutations that\nexhibit mutual exclusivity, a pattern expected for mutations in pathways. CoMEt\nhas several important feature that distinguish it from existing approaches to\nanalyze mutual exclusivity among alterations. These include: an exact\nstatistical test for mutual exclusivity that is more sensitive in detecting\ncombinations containing rare alterations; simultaneous identification of\ncollections of one or more combinations of mutually exclusive alterations;\nsimultaneous analysis of subtype-specific mutations; and summarization over an\nensemble of collections of mutually exclusive alterations. These features\nenable CoMEt to robustly identify alterations affecting multiple pathways, or\nhallmarks of cancer. We show that CoMEt outperforms existing approaches on\nsimulated and real data. Application of CoMEt to hundreds of samples from four\ndifferent cancer types from TCGA reveals multiple mutually exclusive sets\nwithin each cancer type. Many of these overlap known pathways, but others\nreveal novel putative cancer genes. *Equal contribution.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 20:39:32 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Leiserson", "Mark D. M.", ""], ["Wu", "Hsin-Ta", ""], ["Vandin", "Fabio", ""], ["Raphael", "Benjamin J.", ""]]}, {"id": "1503.08272", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck, Stephen Wu, Hui Li", "title": "Robust Bayesian compressive sensing with data loss recovery for\n  structural health monitoring signals", "comments": "41 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of compressive sensing (CS) to structural health monitoring\nis an emerging research topic. The basic idea in CS is to use a\nspecially-designed wireless sensor to sample signals that are sparse in some\nbasis (e.g. wavelet basis) directly in a compressed form, and then to\nreconstruct (decompress) these signals accurately using some inversion\nalgorithm after transmission to a central processing unit. However, most\nsignals in structural health monitoring are only approximately sparse, i.e.\nonly a relatively small number of the signal coefficients in some basis are\nsignificant, but the other coefficients are usually not exactly zero. In this\ncase, perfect reconstruction from compressed measurements is not expected. A\nnew Bayesian CS algorithm is proposed in which robust treatment of the\nuncertain parameters is explored, including integration over the\nprediction-error precision parameter to remove it as a \"nuisance\" parameter.\nThe performance of the new CS algorithm is investigated using compressed data\nfrom accelerometers installed on a space-frame structure and on a cable-stayed\nbridge. Compared with other state-of-the-art CS methods including our\npreviously-published Bayesian method which uses MAP (maximum a posteriori)\nestimation of the prediction-error precision parameter, the new algorithm shows\nsuperior performance in reconstruction robustness and posterior uncertainty\nquantification. Furthermore, our method can be utilized for recovery of lost\ndata during wireless transmission, regardless of the level of sparseness in the\nsignal.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 06:14:45 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Wu", "Stephen", ""], ["Li", "Hui", ""]]}, {"id": "1503.08376", "submitter": "Alexei Botchkarev", "authors": "Alexei Botchkarev", "title": "Assessing Excel VBA Suitability for Monte Carlo Simulation", "comments": null, "journal-ref": "Spreadsheets in Education (eJSiE): 2015, Vol. 8: Iss. 2, Article 3", "doi": null, "report-no": null, "categories": "cs.MS stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) simulation includes a wide range of stochastic techniques\nused to quantitatively evaluate the behavior of complex systems or processes.\nMicrosoft Excel spreadsheets with Visual Basic for Applications (VBA) software\nis, arguably, the most commonly employed general purpose tool for MC\nsimulation. Despite the popularity of the Excel in many industries and\neducational institutions, it has been repeatedly criticized for its flaws and\noften described as questionable, if not completely unsuitable, for statistical\nproblems. The purpose of this study is to assess suitability of the Excel\n(specifically its 2010 and 2013 versions) with VBA programming as a tool for MC\nsimulation. The results of the study indicate that Microsoft Excel (versions\n2010 and 2013) is a strong Monte Carlo simulation application offering a solid\nframework of core simulation components including spreadsheets for data input\nand output, VBA development environment and summary statistics functions. This\nframework should be complemented with an external high-quality pseudo-random\nnumber generator added as a VBA module. A large and diverse category of Excel\nincidental simulation components that includes statistical distributions,\nlinear and non-linear regression and other statistical, engineering and\nbusiness functions require execution of due diligence to determine their\nsuitability for a specific MC project.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 01:51:49 GMT"}], "update_date": "2015-07-22", "authors_parsed": [["Botchkarev", "Alexei", ""]]}]