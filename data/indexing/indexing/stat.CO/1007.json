[{"id": "1007.0089", "submitter": "Nayantara Bhatnagar", "authors": "Nayantara Bhatnagar, Andrej Bogdanov and Elchanan Mossel", "title": "The Computational Complexity of Estimating Convergence Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in the implementation of Markov Chain Monte Carlo\nalgorithms is to determine the convergence time, or the number of iterations\nbefore the chain is close to stationarity. For many Markov chains used in\npractice this time is not known. Even in cases where the convergence time is\nknown to be polynomial, the theoretical bounds are often too crude to be\npractical. Thus, practitioners like to carry out some form of statistical\nanalysis in order to assess convergence. This has led to the development of a\nnumber of methods known as convergence diagnostics which attempt to diagnose\nwhether the Markov chain is far from stationarity. We study the problem of\ntesting convergence in the following settings and prove that the problem is\nhard in a computational sense: Given a Markov chain that mixes rapidly, it is\nhard for Statistical Zero Knowledge (SZK-hard) to distinguish whether starting\nfrom a given state, the chain is close to stationarity by time t or far from\nstationarity at time ct for a constant c. We show the problem is in AM\nintersect coAM. Second, given a Markov chain that mixes rapidly it is coNP-hard\nto distinguish whether it is close to stationarity by time t or far from\nstationarity at time ct for a constant c. The problem is in coAM. Finally, it\nis PSPACE-complete to distinguish whether the Markov chain is close to\nstationarity by time t or far from being mixed at time ct for c at least 1.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jul 2010 07:34:58 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Bhatnagar", "Nayantara", ""], ["Bogdanov", "Andrej", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1007.0842", "submitter": "Josef Dick", "authors": "Josef Dick", "title": "Higher order scrambled digital nets achieve the optimal rate of the root\n  mean square error for smooth integrands", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS880 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 3, 1372-1398", "doi": "10.1214/11-AOS880", "report-no": "IMS-AOS-AOS880", "categories": "math.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a random sampling technique to approximate integrals\n$\\int_{[0,1]^s}f(\\mathbf{x})\\,\\mathrm{d}\\mathbf{x}$ by averaging the function\nat some sampling points. We focus on cases where the integrand is smooth, which\nis a problem which occurs in statistics. The convergence rate of the\napproximation error depends on the smoothness of the function $f$ and the\nsampling technique. For instance, Monte Carlo (MC) sampling yields a\nconvergence of the root mean square error (RMSE) of order $N^{-1/2}$ (where $N$\nis the number of samples) for functions $f$ with finite variance. Randomized\nQMC (RQMC), a combination of MC and quasi-Monte Carlo (QMC), achieves a RMSE of\norder $N^{-3/2+\\varepsilon}$ under the stronger assumption that the integrand\nhas bounded variation. A combination of RQMC with local antithetic sampling\nachieves a convergence of the RMSE of order $N^{-3/2-1/s+\\varepsilon}$ (where\n$s\\ge1$ is the dimension) for functions with mixed partial derivatives up to\norder two. Additional smoothness of the integrand does not improve the rate of\nconvergence of these algorithms in general. On the other hand, it is known that\nwithout additional smoothness of the integrand it is not possible to improve\nthe convergence rate. This paper introduces a new RQMC algorithm, for which we\nprove that it achieves a convergence of the root mean square error (RMSE) of\norder $N^{-\\alpha-1/2+\\varepsilon}$ provided the integrand satisfies the strong\nassumption that it has square integrable partial mixed derivatives up to order\n$\\alpha>1$ in each variable. Known lower bounds on the RMSE show that this rate\nof convergence cannot be improved in general for integrands with this\nsmoothness. We provide numerical examples for which the RMSE converges\napproximately with order $N^{-5/2}$ and $N^{-7/2}$, in accordance with the\ntheoretical upper bound.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2010 09:30:11 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2010 05:04:24 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2010 00:14:31 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2012 09:12:28 GMT"}], "update_date": "2012-11-21", "authors_parsed": [["Dick", "Josef", ""]]}, {"id": "1007.0995", "submitter": "Molei Tao", "authors": "Molei Tao, Houman Owhadi, Jerrold E. Marsden", "title": "Temperature and Friction Accelerated Sampling of Boltzmann-Gibbs\n  Distribution", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with tuning friction and temperature in Langevin\ndynamics for fast sampling from the canonical ensemble. We show that\nnear-optimal acceleration is achieved by choosing friction so that the local\nquadratic approximation of the Hamiltonian is a critical damped oscillator. The\nsystem is also over-heated and cooled down to its final temperature. The\nperformances of different cooling schedules are analyzed as functions of total\nsimulation time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jul 2010 20:04:13 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["Tao", "Molei", ""], ["Owhadi", "Houman", ""], ["Marsden", "Jerrold E.", ""]]}, {"id": "1007.1032", "submitter": "Reza Hosseini", "authors": "Reza Hosseini", "title": "Approximating quantiles in very large datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very large datasets are often encountered in climatology, either from a\nmultiplicity of observations over time and space or outputs from deterministic\nmodels (sometimes in petabytes= 1 million gigabytes). Loading a large data\nvector and sorting it, is impossible sometimes due to memory limitations or\ncomputing power. We show that a proposed algorithm to approximating the median,\n\"the median of the median\" performs poorly. Instead we develop an algorithm to\napproximate quantiles of very large datasets which works by partitioning the\ndata or use existing partitions (possibly of non-equal size). We show the\ndeterministic precision of this algorithm and how it can be adjusted to get\ncustomized precisions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jul 2010 01:24:41 GMT"}], "update_date": "2010-07-08", "authors_parsed": [["Hosseini", "Reza", ""]]}, {"id": "1007.2371", "submitter": "Hua Zhou", "authors": "Kenneth Lange, Hua Zhou", "title": "MM Algorithms for Geometric and Signomial Programming", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": "10.1007/s10107-012-0612-1", "report-no": null, "categories": "math.NA math.OC stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper derives new algorithms for signomial programming, a generalization\nof geometric programming. The algorithms are based on a generic principle for\noptimization called the MM algorithm. In this setting, one can apply the\ngeometric-arithmetic mean inequality and a supporting hyperplane inequality to\ncreate a surrogate function with parameters separated. Thus, unconstrained\nsignomial programming reduces to a sequence of one-dimensional minimization\nproblems. Simple examples demonstrate that the MM algorithm derived can\nconverge to a boundary point or to one point of a continuum of minimum points.\nConditions under which the minimum point is unique or occurs in the interior of\nparameter space are proved for geometric programming. Convergence to an\ninterior point occurs at a linear rate. Finally, the MM framework easily\naccommodates equality and inequality constraints of signomial type. For the\nmost important special case, constrained quadratic programming, the MM\nalgorithm involves very simple updates.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 16:30:00 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Lange", "Kenneth", ""], ["Zhou", "Hua", ""]]}, {"id": "1007.2656", "submitter": "John Noble", "authors": "John M. Noble", "title": "An Algorithm for Learning the Essential Graph", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an algorithm for learning the essential graph of a\nBayesian network. The basis of the algorithm is the Maximum Minimum Parents and\nChildren algorithm developed by previous authors, with three substantial\nmodifications. The MMPC algorithm is the first stage of the Maximum Minimum\nHill Climbing algorithm for learning the directed acyclic graph of a Bayesian\nnetwork, introduced by previous authors. The MMHC algorithm runs in two phases;\nfirstly, the MMPC algorithm to locate the skeleton and secondly an edge\norientation phase. The computationally expensive part is the edge orientation\nphase.\n  The first modification introduced to the MMPC algorithm, which requires\nlittle additional computational cost, is to obtain the immoralities and hence\nthe essential graph. This renders the edge orientation phase, the\ncomputationally expensive part, unnecessary, since the entire Markov structure\nthat can be derived from data is present in the essential graph.\n  Secondly, the MMPC algorithm can accept independence statements that are\nlogically inconsistent with those rejected, since with tests for independence,\na `do not reject' conclusion for a particular independence statement is taken\nas `accept' independence. An example is given to illustrate this and a\nmodification is suggested to ensure that the conditional independence\nstatements are logically consistent.\n  Thirdly, the MMHC algorithm makes an assumption of faithfulness. An example\nof a data set is given that does not satisfy this assumption and a modification\nis suggested to deal with some situations where the assumption is not\nsatisfied. The example in question also illustrates problems with the\n`faithfulness' assumption that cannot be tackled by this modification.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jul 2010 10:20:16 GMT"}], "update_date": "2010-07-19", "authors_parsed": [["Noble", "John M.", ""]]}, {"id": "1007.3424", "submitter": "Or Zuk", "authors": "Amnon Amir and Or Zuk", "title": "Bacterial Community Reconstruction Using A Single Sequencing Reaction", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.IT math.IT q-bio.QM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bacteria are the unseen majority on our planet, with millions of species and\ncomprising most of the living protoplasm. While current methods enable in-depth\nstudy of a small number of communities, a simple tool for breadth studies of\nbacterial population composition in a large number of samples is lacking. We\npropose a novel approach for reconstruction of the composition of an unknown\nmixture of bacteria using a single Sanger-sequencing reaction of the mixture.\nThis method is based on compressive sensing theory, which deals with\nreconstruction of a sparse signal using a small number of measurements.\nUtilizing the fact that in many cases each bacterial community is comprised of\na small subset of the known bacterial species, we show the feasibility of this\napproach for determining the composition of a bacterial mixture. Using\nsimulations, we show that sequencing a few hundred base-pairs of the 16S rRNA\ngene sequence may provide enough information for reconstruction of mixtures\ncontaining tens of species, out of tens of thousands, even in the presence of\nrealistic measurement noise. Finally, we show initial promising results when\napplying our method for the reconstruction of a toy experimental mixture with\nfive species. Our approach may have a potential for a practical and efficient\nway for identifying bacterial species compositions in biological samples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jul 2010 13:47:09 GMT"}], "update_date": "2010-07-21", "authors_parsed": [["Amir", "Amnon", ""], ["Zuk", "Or", ""]]}, {"id": "1007.3622", "submitter": "Alexey Koloydenko", "authors": "J\\\"uri Lember and Alexey A. Koloydenko", "title": "A generalized risk approach to path inference based on hidden Markov\n  models", "comments": "Section 5: corrected denominators of the scaled beta variables (pp.\n  27-30), => corrections in claims 1, 3, Prop. 12, bottom of Table 1. Decoder\n  (49), Corol. 14 are generalized to handle 0 probabilities. Notation is more\n  closely aligned with (Bishop, 2006). Details are inserted in eqn-s (43); the\n  positivity assumption in Prop. 11 is explicit. Fixed typing errors in\n  equation (41), Example 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the unceasing interest in hidden Markov models (HMMs), this\npaper re-examines hidden path inference in these models, using primarily a\nrisk-based framework. While the most common maximum a posteriori (MAP), or\nViterbi, path estimator and the minimum error, or Posterior Decoder (PD), have\nlong been around, other path estimators, or decoders, have been either only\nhinted at or applied more recently and in dedicated applications generally\nunfamiliar to the statistical learning community. Over a decade ago, however, a\nfamily of algorithmically defined decoders aiming to hybridize the two standard\nones was proposed (Brushe et al., 1998). The present paper gives a careful\nanalysis of this hybridization approach, identifies several problems and issues\nwith it and other previously proposed approaches, and proposes practical\nresolutions of those. Furthermore, simple modifications of the classical\ncriteria for hidden path recognition are shown to lead to a new class of\ndecoders. Dynamic programming algorithms to compute these decoders in the usual\nforward-backward manner are presented. A particularly interesting subclass of\nsuch estimators can be also viewed as hybrids of the MAP and PD estimators.\nSimilar to previously proposed MAP-PD hybrids, the new class is parameterized\nby a small number of tunable parameters. Unlike their algorithmic predecessors,\nthe new risk-based decoders are more clearly interpretable, and, most\nimportantly, work \"out of the box\" in practice, which is demonstrated on some\nreal bioinformatics tasks and data. Some further generalizations and\napplications are discussed in conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jul 2010 11:44:30 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2012 16:31:20 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2012 16:16:43 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2013 11:58:17 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Lember", "J\u00fcri", ""], ["Koloydenko", "Alexey A.", ""]]}, {"id": "1007.4580", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K.H. Lee", "title": "Cases for the nugget in modeling computer experiments", "comments": "17 pages, 4 figures, 3 tables; revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most surrogate models for computer experiments are interpolators, and the\nmost common interpolator is a Gaussian process (GP) that deliberately omits a\nsmall-scale (measurement) error term called the nugget. The explanation is that\ncomputer experiments are, by definition, \"deterministic\", and so there is no\nmeasurement error. We think this is too narrow a focus for a computer\nexperiment and a statistically inefficient way to model them. We show that\nestimating a (non-zero) nugget can lead to surrogate models with better\nstatistical properties, such as predictive accuracy and coverage, in a variety\nof common situations.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jul 2010 22:07:03 GMT"}, {"version": "v2", "created": "Sun, 21 Nov 2010 14:52:10 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}, {"id": "1007.5030", "submitter": "Jose Blanchet", "authors": "Jose Blanchet, Kevin Leder and Yixi Shi", "title": "Analysis of a Splitting Estimator for Rare Event Probabilities in\n  Jackson Networks", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a standard splitting algorithm for the rare-event simulation of\noverflow probabilities in any subset of stations in a Jackson network at level\nn, starting at a fixed initial position. It was shown in DeanDup09 that a\nsubsolution to the Isaacs equation guarantees that a subexponential number of\nfunction evaluations (in n) suffice to estimate such overflow probabilities\nwithin a given relative accuracy. Our analysis here shows that in fact\nO(n^{2{\\beta}+1}) function evaluations suffice to achieve a given relative\nprecision, where {\\beta} is the number of bottleneck stations in the network.\nThis is the first rigorous analysis that allows to favorably compare splitting\nagainst directly computing the overflow probability of interest, which can be\nevaluated by solving a linear system of equations with O(n^{d}) variables.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jul 2010 16:49:02 GMT"}], "update_date": "2010-07-29", "authors_parsed": [["Blanchet", "Jose", ""], ["Leder", "Kevin", ""], ["Shi", "Yixi", ""]]}, {"id": "1007.5510", "submitter": "Mark Tygert", "authors": "Nathan Halko, Per-Gunnar Martinsson, Yoel Shkolnisky, and Mark Tygert", "title": "An algorithm for the principal component analysis of large data sets", "comments": "17 pages, 3 figures (each with 2 or 3 subfigures), 2 tables (each\n  with 2 subtables)", "journal-ref": "SIAM Journal on Scientific Computing, 33 (5): 2580-2594, 2011", "doi": null, "report-no": null, "categories": "stat.CO cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently popularized randomized methods for principal component analysis\n(PCA) efficiently and reliably produce nearly optimal accuracy --- even on\nparallel processors --- unlike the classical (deterministic) alternatives. We\nadapt one of these randomized methods for use with data sets that are too large\nto be stored in random-access memory (RAM). (The traditional terminology is\nthat our procedure works efficiently \"out-of-core.\") We illustrate the\nperformance of the algorithm via several numerical examples. For example, we\nreport on the PCA of a data set stored on disk that is so large that less than\na hundredth of it can fit in our computer's RAM.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jul 2010 18:24:23 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2011 20:04:21 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Halko", "Nathan", ""], ["Martinsson", "Per-Gunnar", ""], ["Shkolnisky", "Yoel", ""], ["Tygert", "Mark", ""]]}]