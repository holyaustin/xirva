[{"id": "2106.02131", "submitter": "Nestor Parolya Dr.", "authors": "Taras Bodnar, Nestor Parolya and Erik Thorsen", "title": "Dynamic Shrinkage Estimation of the High-Dimensional Minimum-Variance\n  Portfolio", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST q-fin.PM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, new results in random matrix theory are derived which allow us\nto construct a shrinkage estimator of the global minimum variance (GMV)\nportfolio when the shrinkage target is a random object. More specifically, the\nshrinkage target is determined as the holding portfolio estimated from previous\ndata. The theoretical findings are applied to develop theory for dynamic\nestimation of the GMV portfolio, where the new estimator of its weights is\nshrunk to the holding portfolio at each time of reconstruction. Both cases with\nand without overlapping samples are considered in the paper. The\nnon-overlapping samples corresponds to the case when different data of the\nasset returns are used to construct the traditional estimator of the GMV\nportfolio weights and to determine the target portfolio, while the overlapping\ncase allows intersections between the samples. The theoretical results are\nderived under weak assumptions imposed on the data-generating process. No\nspecific distribution is assumed for the asset returns except from the\nassumption of finite $4+\\varepsilon$, $\\varepsilon>0$, moments. Also, the\npopulation covariance matrix with unbounded spectrum can be considered. The\nperformance of new trading strategies is investigated via an extensive\nsimulation. Finally, the theoretical findings are implemented in an empirical\nillustration based on the returns on stocks included in the S\\&P 500 index.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 21:08:08 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bodnar", "Taras", ""], ["Parolya", "Nestor", ""], ["Thorsen", "Erik", ""]]}, {"id": "2106.03417", "submitter": "Alvaro Arroyo", "authors": "Alvaro Arroyo, Bruno Scalzo, Ljubisa Stankovic, Danilo P. Mandic", "title": "Dynamic Portfolio Cuts: A Spectral Approach to Graph-Theoretic\n  Diversification", "comments": "5 pages, 3 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock market returns are typically analyzed using standard regression, yet\nthey reside on irregular domains which is a natural scenario for graph signal\nprocessing. To this end, we consider a market graph as an intuitive way to\nrepresent the relationships between financial assets. Traditional methods for\nestimating asset-return covariance operate under the assumption of statistical\ntime-invariance, and are thus unable to appropriately infer the underlying true\nstructure of the market graph. This work introduces a class of graph spectral\nestimators which cater for the nonstationarity inherent to asset price\nmovements, and serve as a basis to represent the time-varying interactions\nbetween assets through a dynamic spectral market graph. Such an account of the\ntime-varying nature of the asset-return covariance allows us to introduce the\nnotion of dynamic spectral portfolio cuts, whereby the graph is partitioned\ninto time-evolving clusters, allowing for online and robust asset allocation.\nThe advantages of the proposed framework over traditional methods are\ndemonstrated through numerical case studies using real-world price data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 08:35:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Arroyo", "Alvaro", ""], ["Scalzo", "Bruno", ""], ["Stankovic", "Ljubisa", ""], ["Mandic", "Danilo P.", ""]]}, {"id": "2106.04028", "submitter": "Jorge Guijarro Ordonez", "authors": "Jorge Guijarro-Ordonez, Markus Pelger and Greg Zanotti", "title": "Deep Learning Statistical Arbitrage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Statistical arbitrage identifies and exploits temporal price differences\nbetween similar assets. We propose a unifying conceptual framework for\nstatistical arbitrage and develop a novel deep learning solution, which finds\ncommonality and time-series patterns from large panels in a data-driven and\nflexible way. First, we construct arbitrage portfolios of similar assets as\nresidual portfolios from conditional latent asset pricing factors. Second, we\nextract the time series signals of these residual portfolios with one of the\nmost powerful machine learning time-series solutions, a convolutional\ntransformer. Last, we use these signals to form an optimal trading policy, that\nmaximizes risk-adjusted returns under constraints. We conduct a comprehensive\nempirical comparison study with daily large cap U.S. stocks. Our optimal\ntrading strategy obtains a consistently high out-of-sample Sharpe ratio and\nsubstantially outperforms all benchmark approaches. It is orthogonal to common\nrisk factors, and exploits asymmetric local trend and reversion patterns. Our\nstrategies remain profitable after taking into account trading frictions and\ncosts. Our findings suggest a high compensation for arbitrageurs to enforce the\nlaw of one price.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 00:48:25 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Guijarro-Ordonez", "Jorge", ""], ["Pelger", "Markus", ""], ["Zanotti", "Greg", ""]]}, {"id": "2106.04114", "submitter": "Liu Ziyin", "authors": "Liu Ziyin, Kentaro Minami, Kentaro Imajo", "title": "What Data Augmentation Do We Need for Deep-Learning-Based Finance?", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.GN q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task we consider is portfolio construction in a speculative market,\na fundamental problem in modern finance. While various empirical works now\nexist to explore deep learning in finance, the theory side is almost\nnon-existent. In this work, we focus on developing a theoretical framework for\nunderstanding the use of data augmentation for deep-learning-based approaches\nto quantitative finance. The proposed theory clarifies the role and necessity\nof data augmentation for finance; moreover, our theory motivates a simple\nalgorithm of injecting a random noise of strength $\\sqrt{|r_{t-1}|}$ to the\nobserved return $r_{t}$. This algorithm is shown to work well in practice.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 05:26:58 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Ziyin", "Liu", ""], ["Minami", "Kentaro", ""], ["Imajo", "Kentaro", ""]]}, {"id": "2106.04218", "submitter": "Gianmarco Vacca", "authors": "Piero Quatto, Gianmarco Vacca, Maria Grazia Zoia", "title": "Modeling Portfolios with Leptokurtic and Dependent Risk Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, an approach to modeling portfolio distribution with risk factors\ndistributed as Gram-Charlier (GC) expansions of the Gaussian law, has been\nconceived. GC expansions prove effective when dealing with moderately\nleptokurtic data. In order to cover the case of possibly severe leptokurtosis,\nthe so-called GC-like expansions have been devised by reshaping parent\nleptokurtic distributions by means of orthogonal polynomials specific to them.\nIn this paper, we focus on the hyperbolic-secant (HS) law as parent\ndistribution whose GC-like expansions fit with kurtosis levels up to 19.4. A\nportfolio distribution has been obtained with risk factors modeled as GClike\nexpansions of the HS law which duly account for excess kurtosis. Empirical\nevidence of the workings of the approach dealt with in the paper is included.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 09:57:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Quatto", "Piero", ""], ["Vacca", "Gianmarco", ""], ["Zoia", "Maria Grazia", ""]]}, {"id": "2106.06735", "submitter": "Roman Orus", "authors": "Samuel Palmer, Serkan Sahin, Rodrigo Hernandez, Samuel Mugel, Roman\n  Orus", "title": "Quantum Portfolio Optimization with Investment Bands and Target\n  Volatility", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to implement in a simple way some complex real-life\nconstraints on the portfolio optimization problem, so that it becomes amenable\nto quantum optimization algorithms. Specifically, first we explain how to\nobtain the best investment portfolio with a given target risk. This is\nimportant in order to produce portfolios with different risk profiles, as\ntypically offered by financial institutions. Second, we show how to implement\nindividual investment bands, i.e., minimum and maximum possible investments for\neach asset. This is also important in order to impose diversification and avoid\ncorner solutions. Quite remarkably, we show how to build the constrained cost\nfunction as a quadratic binary optimization (QUBO) problem, this being the\nnatural input of quantum annealers. The validity of our implementation is\nproven by finding the optimal portfolios, using D-Wave Hybrid and its Advantage\nquantum processor, on static portfolios taking assets from S&P100 and S&P500.\nOur results show how practical daily constraints found in quantitative finance\ncan be implemented in a simple way in current NISQ quantum processors, with\nreal data, and under realistic market conditions. In combination with\nclustering algorithms, our methods would allow to replicate the behaviour of\nmore complex indexes, such as Nasdaq Composite or others, in turn being\nparticularly useful to build and replicate Exchange Traded Funds (ETF).\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 10:17:17 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 17:40:18 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 09:01:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Palmer", "Samuel", ""], ["Sahin", "Serkan", ""], ["Hernandez", "Rodrigo", ""], ["Mugel", "Samuel", ""], ["Orus", "Roman", ""]]}, {"id": "2106.09055", "submitter": "Jaehyung Choi", "authors": "Jaehyung Choi, Hyangju Kim, Young Shin Kim", "title": "Diversified reward-risk parity in portfolio construction", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce diversified risk parity embedded with various reward-risk\nmeasures and more general allocation rules for portfolio construction. We\nempirically test advanced reward-risk parity strategies and compare their\nperformance with an equally-weighted risk portfolio in various asset universes.\nAll reward-risk parity strategies we tested exhibit consistent outperformance\nevidenced by higher average returns, Sharpe ratios, and Calmar ratios. The\nalternative allocations also reflect less downside risks in Value-at-Risk,\nconditional Value-at-Risk, and maximum drawdown. In addition to the enhanced\nperformance and reward-risk profile, transaction costs can be reduced by\nlowering turnover rates. The Carhart four-factor analysis also indicates that\nthe diversified reward-risk parity allocations gain superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:00:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Choi", "Jaehyung", ""], ["Kim", "Hyangju", ""], ["Kim", "Young Shin", ""]]}, {"id": "2106.09132", "submitter": "Tianyang Xie", "authors": "Chenyanzi Yu, Tianyang Xie", "title": "Multivariate Pair Trading by Volatility & Model Adaption Trade-off", "comments": "Submitting to Journal of Financial Economics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.CE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pair trading is one of the most discussed topics among financial researches.\nDespite a growing base of work, portfolio management for multivariate time\nseries is rarely discussed. On the other hand, most researches focus on\nrefining strategy rules instead of finding the optimal portfolio weight. In\nthis paper, we brought up a simple yet profitable strategy called Volatility &\nModel Adaption Trade-off (VMAT) to leverage the issues. Experiment studies show\nits superior profit performance over baselines.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 15:57:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yu", "Chenyanzi", ""], ["Xie", "Tianyang", ""]]}, {"id": "2106.10030", "submitter": "Alex Garivaltis", "authors": "Alex Garivaltis", "title": "Universal Risk Budgeting", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.TH q-fin.CP q-fin.MF q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I juxtapose Cover's vaunted universal portfolio selection algorithm (Cover\n1991) with the modern representation (Qian 2016; Roncalli 2013) of a portfolio\nas a certain allocation of risk among the available assets, rather than a mere\nallocation of capital. Thus, I define a Universal Risk Budgeting scheme that\nweights each risk budget (instead of each capital budget) by its historical\nperformance record (a la Cover). I prove that my scheme is mathematically\nequivalent to a novel type of Cover and Ordentlich 1996 universal portfolio\nthat uses a new family of prior densities that have hitherto not appeared in\nthe literature on universal portfolio theory. I argue that my universal risk\nbudget, so-defined, is a potentially more perspicuous and flexible type of\nuniversal portfolio; it allows the algorithmic trader to incorporate, with\nadvantage, his prior knowledge (or beliefs) about the particular covariance\nstructure of instantaneous asset returns. Say, if there is some dispersion in\nthe volatilities of the available assets, then the uniform (or Dirichlet)\npriors that are standard in the literature will generate a dangerously lopsided\nprior distribution over the possible risk budgets. In the author's opinion, the\nproposed \"Garivaltis prior\" makes for a nice improvement on Cover's timeless\nexpert system (Cover 1991), that is properly agnostic and open (from the very\nget-go) to different risk budgets. Inspired by Jamshidian 1992, the universal\nrisk budget is formulated as a new kind of exotic option in the continuous time\nBlack and Scholes 1973 market, with all the pleasure, elegance, and convenience\nthat that entails.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:06:02 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Garivaltis", "Alex", ""]]}, {"id": "2106.10033", "submitter": "Carlos Escudero", "authors": "Mauricio Elizalde, Carlos Escudero", "title": "Chances for the honest in honest versus insider trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.MF q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a Black-Scholes market with a finite time horizon and two investors:\nan honest and an insider trader. We analyze it with anticipating stochastic\ncalculus in two steps. First, we recover the classical result on portfolio\noptimization that shows that the expected logarithmic utility of the insider is\nstrictly greater than that of the honest trader. Then, we prove that, whenever\nthe market is viable, the honest trader can get a higher logarithmic utility,\nand therefore more wealth, than the insider with a strictly positive\nprobability. Our proof relies on the analysis of a sort of forward integral\nvariant of the Dol\\'eans-Dade exponential process. The main financial\nconclusion is that the logarithmic utility is perhaps too conservative for some\ninsiders.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:09:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Elizalde", "Mauricio", ""], ["Escudero", "Carlos", ""]]}, {"id": "2106.10491", "submitter": "Andrew Paskaramoorthy", "authors": "Andrew Paskaramoorthy, Tim Gebbie, Terence van Zyl", "title": "The efficient frontiers of mean-variance portfolio rules under\n  distribution misspecification", "comments": "8 pages, 2 figures, 4 tables, submitted to Fusion2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mean-variance portfolio decisions that combine prediction and optimisation\nhave been shown to have poor empirical performance. Here, we consider the\nperformance of various shrinkage methods by their efficient frontiers under\ndifferent distributional assumptions to study the impact of reasonable\ndepartures from Normality. Namely, we investigate the impact of first-order\nauto-correlation, second-order auto-correlation, skewness, and excess kurtosis.\nWe show that the shrinkage methods tend to re-scale the sample efficient\nfrontier, which can change based on the nature of local perturbations from\nNormality. This re-scaling implies that the standard approach of comparing\ndecision rules for a fixed level of risk aversion is problematic, and more so\nin a dynamic market setting. Our results suggest that comparing efficient\nfrontiers has serious implications which oppose the prevailing thinking in the\nliterature. Namely, that sample estimators out-perform Stein type estimators of\nthe mean, and that improving the prediction of the covariance has greater\nimportance than improving that of the means.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 12:49:00 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 12:30:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Paskaramoorthy", "Andrew", ""], ["Gebbie", "Tim", ""], ["van Zyl", "Terence", ""]]}, {"id": "2106.11484", "submitter": "Shiv Gupta Dr", "authors": "Vrinda Dhingra (1), Amita Sharma (2), Shiv K. Gupta (1) ((1) Indian\n  Institute of Technology, Roorkee, (2) Institute of Management Technology,\n  Ghaziabad)", "title": "Sectoral portfolio optimization by judicious selection of financial\n  ratios via PCA", "comments": "26 pages, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding value investment in portfolio optimization models has always been a\nchallenge. In this paper, we attempt to incorporate it by first employing\nprincipal component analysis (PCA) sector wise to filter out dominant financial\nratios from each sector and thereafter, use the portfolio optimization model\nincorporating second order stochastic dominance (SSD) criteria to derive the\nfinal optimal investment. We consider a total of 11 well known financial ratios\ncorresponding to each sector representing four categories of ratios, namely\nliquidity, solvency, profitability, and valuation. PCA is then applied sector\nwise over a period of 10 years from April 2004 to March 2014 to extract\ndominant ratios from each sector in two ways, one from the component solution\nand other from each category on the basis of their communalities. The two step\nSectoral Portfolio Optimization (SPO) model integrating the SSD criteria in\nconstraints is then utilized to build an optimal portfolio. The strategy formed\nusing the former extracted ratios is termed as PCA-SPO(A) and the latter one as\nPCA-SPO(B).\n  The results obtained from the proposed strategies are compared with the SPO\nmodel and two nominal SSD models, with and without financial ratios for\ncomputational study. Empirical performance of proposed strategies is assessed\nover the period of 6 years from April 2014 to March 2020 using a rolling window\nscheme with varying out-of-sample time line of 3, 6, 9, 12 and 24 months for\nS&P BSE 500 market. We observe that the proposed strategy PCA-SPO(B)\noutperforms all other models in terms of downside deviation, CVaR, VaR, Sortino\nratio, Rachev ratio, and STARR ratios over almost all out-of-sample periods.\nThis highlights the importance of value investment where ratios are carefully\nselected and embedded quantitatively in portfolio selection process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:13:20 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Dhingra", "Vrinda", ""], ["Sharma", "Amita", ""], ["Gupta", "Shiv K.", ""]]}, {"id": "2106.11510", "submitter": "Ruimeng Hu", "authors": "Jean-Pierre Fouque, Ruimeng Hu, Ronnie Sircar", "title": "Sub- and Super-solution Approach to Accuracy Analysis of Portfolio\n  Optimization Asymptotics in Multiscale Stochastic Factor Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.OC q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of portfolio optimization when stochastic factors drive returns\nand volatilities has been studied in previous works by the authors. In\nparticular, they proposed asymptotic approximations for value functions and\noptimal strategies in the regime where these factors are running on both slow\nand fast timescales. However, the rigorous justification of the accuracy of\nthese approximations has been limited to power utilities and a single factor.\nIn this paper, we provide an accuracy analysis for cases with general utility\nfunctions and two timescale factors by constructing sub- and super-solutions to\nthe fully nonlinear problem such that their difference is at the desired level\nof accuracy. This approach will be valuable in various related stochastic\ncontrol problems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 02:54:46 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Fouque", "Jean-Pierre", ""], ["Hu", "Ruimeng", ""], ["Sircar", "Ronnie", ""]]}, {"id": "2106.12292", "submitter": "Francesco Buono", "authors": "Francesco Buono, Camilla Cal\\`i and Maria Longobardi", "title": "Dispersion indexes based on bivariate measures of uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.PM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The concept of varentropy has been recently introduced as a dispersion index\nof the reliability of measure of information. In this paper, we introduce new\nmeasures of variability for two bivariate measures of uncertainty, the Kerridge\ninaccuracy measure and the Kullback-Leibler divergence. These new definitions\nand related properties, bounds and examples are presented. Finally we show an\napplication of Kullback-Leibler divergence and its dispersion index using the\nmean-variance rule introduced in portfolio theory.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 10:20:18 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Buono", "Francesco", ""], ["Cal\u00ec", "Camilla", ""], ["Longobardi", "Maria", ""]]}, {"id": "2106.12425", "submitter": "B{\\aa}rd St{\\o}ve", "authors": "Anders D. Sleire, B{\\aa}rd St{\\o}ve, H{\\aa}kon Otneim, Geir Drage\n  Berentsen, Dag Tj{\\o}stheim, Sverre Hauso Haugen", "title": "Portfolio Allocation under Asymmetric Dependence in Asset Returns using\n  Local Gaussian Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that there are asymmetric dependence structures between\nfinancial returns. In this paper we use a new nonparametric measure of local\ndependence, the local Gaussian correlation, to improve portfolio allocation. We\nextend the classical mean-variance framework, and show that the portfolio\noptimization is straightforward using our new approach, only relying on a\ntuning parameter (the bandwidth). The new method is shown to outperform the\nequally weighted (1/N) portfolio and the classical Markowitz portfolio for\nmonthly asset returns data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 19:29:33 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Sleire", "Anders D.", ""], ["St\u00f8ve", "B\u00e5rd", ""], ["Otneim", "H\u00e5kon", ""], ["Berentsen", "Geir Drage", ""], ["Tj\u00f8stheim", "Dag", ""], ["Haugen", "Sverre Hauso", ""]]}, {"id": "2106.13888", "submitter": "Katia Colaneri", "authors": "Katia Colaneri, Alessandra Cretarola, Benedetta Salterini", "title": "Optimal investment and proportional reinsurance in a regime-switching\n  market model under forward preferences", "comments": "32 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the optimal investment and reinsurance problem of an\ninsurance company whose investment preferences are described via a forward\ndynamic exponential utility in a regime-switching market model. Financial and\nactuarial frameworks are dependent since stock prices and insurance claims vary\naccording to a common factor given by a continuous time finite state Markov\nchain. We construct the value function and we prove that it is a forward\ndynamic utility. Then, we characterize the investment strategy and the optimal\nproportional level of reinsurance. We also perform numerical experiments and\nprovide sensitivity analyses with respect to some model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 21:28:41 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Colaneri", "Katia", ""], ["Cretarola", "Alessandra", ""], ["Salterini", "Benedetta", ""]]}, {"id": "2106.14404", "submitter": "Andreas Aigner", "authors": "Andreas A. Aigner and Gurvinder Dhaliwal", "title": "UNISWAP: Impermanent Loss and Risk Profile of a Liquidity Provider", "comments": "16 pages, 8 Figures, 1 Table", "journal-ref": null, "doi": "10.13140/RG.2.2.32419.58400/6", "report-no": null, "categories": "q-fin.TR q-fin.CP q-fin.GN q-fin.PM q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uniswap is a decentralized exchange (DEX) and was first launched on November\n2, 2018 on the Ethereum mainnet [1] and is part of an Ecosystem of products in\nDecentralized Finance (DeFi). It replaces a traditional order book type of\ntrading common on centralized exchanges (CEX) with a deterministic model that\nswaps currencies (or tokens/assets) along a fixed price function determined by\nthe amount of currencies supplied by the liquidity providers. Liquidity\nproviders can be regarded as investors in the decentralized exchange and earn\nfixed commissions per trade. They lock up funds in liquidity pools for distinct\npairs of currencies allowing market participants to swap them using the fixed\nprice function. Liquidity providers take on market risk as a liquidity provider\nin exchange for earning commissions on each trade. Here we analyze the risk\nprofile of a liquidity provider and the so called impermanent (unrealized) loss\nin particular. We provide an improved version of the commonly denoted\nimpermanent loss function for Uniswap v2 on the semi-infinite domain. The\ndifferences between Uniswap v2 and v3 are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:38:51 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Aigner", "Andreas A.", ""], ["Dhaliwal", "Gurvinder", ""]]}, {"id": "2106.14820", "submitter": "Alex Garivaltis", "authors": "Alex Garivaltis", "title": "Rational Pricing of Leveraged ETF Expense Ratios", "comments": "40 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.TH econ.GN q-fin.EC q-fin.GN q-fin.MF q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the general relationship between the gearing ratio of a\nLeveraged ETF and its corresponding expense ratio, viz., the investment\nmanagement fees that are charged for the provision of this levered financial\nservice. It must not be possible for an investor to combine two or more LETFs\nin such a way that his (continuously-rebalanced) LETF portfolio can match the\ngearing ratio of a given, professionally managed product and, at the same time,\nenjoy lower weighted-average expenses than the existing LETF. Given a finite\nset of LETFs that exist in the marketplace, I give necessary and sufficient\nconditions for these products to be undominated in the price-gearing plane. In\na beautiful application of the duality theorem of linear programming, I prove a\nkind of two-fund theorem for LETFs: given a target gearing ratio for the\ninvestor, the cheapest way to achieve it is to combine (uniquely) the two\nnearest undominated LETF products that bracket it on the leverage axis. This\nalso happens to be the implementation that has the lowest annual turnover. For\nthe writer's enjoyment, we supply a second proof of the Main Theorem on LETFs\nthat is based on Carath\\'eodory's theorem in convex geometry. Thus, say, a\ntriple-leveraged (\"UltraPro\") exchange-traded product should never be mixed\nwith cash, if the investor is able to trade in the underlying index. In terms\nof financial innovation, our two-fund theorem for LETFs implies that the\nintroduction of new, undominated 2.5x products would increase the welfare of\nall investors whose preferred gearing ratios lie between 2x (\"Ultra\") and 3x\n(\"UltraPro\"). Similarly for a 1.5x product.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 15:56:05 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Garivaltis", "Alex", ""]]}, {"id": "2106.15208", "submitter": "H\\'el\\`ene Halconruy", "authors": "H\\'el\\`ene Halconruy", "title": "The insider problem in the trinomial model: a discrete-time jump process\n  approach", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-fin.MF q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an incomplete market underpinned by the trinomial model, we consider two\ninvestors : an ordinary agent whose decisions are driven by public information\nand an insider who possesses from the beginning a surplus of information\nencoded through a random variable for which he or she knows the outcome.\nThrough the definition of an auxiliary model based on a marked binomial\nprocess, we handle the trinomial model as a volatility one, and use the\nstochastic analysis and Malliavin calculus toolboxes available in that context.\nIn particular, we connect the information drift, the drift to eliminate in\norder to preserve the martingale property within an initial enlargement of\nfiltration in terms of the Malliavin derivative. We solve explicitly the agent\nand the insider expected logarithmic utility maximisation problems and provide\na hedging formula for replicable claims. We identify the insider expected\nadditional utility with the Shannon entropy of the extra information, and\nexamine then the existence of arbitrage opportunities for the insider.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 09:48:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Halconruy", "H\u00e9l\u00e8ne", ""]]}, {"id": "2106.15426", "submitter": "Daniele Marazzina", "authors": "Guodong Ding, Daniele Marazzina", "title": "Effect of Labour Income on the Optimal Bankruptcy Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the optimal bankruptcy problem for an agent who\ncan optimally allocate her consumption rate, the amount of capital invested in\nthe risky asset as well as her leisure time. In our framework, the agent is\nendowed by an initial debt, and she is required to repay her debt continuously.\nDeclaring bankruptcy, the debt repayment is exempted at the cost of a wealth\nshrinkage. We implement the duality method to solve the problem analytically\nand conduct a sensitivity analysis to the cost and benefit parameters of\nbankruptcy. Introducing the flexible leisure/working rate, and therefore the\nlabour income, into the bankruptcy model, we investigate its effect on the\noptimal strategies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 13:54:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ding", "Guodong", ""], ["Marazzina", "Daniele", ""]]}]