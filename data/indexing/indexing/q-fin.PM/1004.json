[{"id": "1004.1053", "submitter": "Ulrich Kirchner", "authors": "Ulrich Kirchner", "title": "Managing Derivative Exposure", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to derivative exposure management based on subjective\nand implied probabilities. We suggest to maximize the valuation difference\nsubject to risk constraints and propose a class of risk measures derived from\nthe subjective distribution. We illustrate this process with specific examples\nfor the two and three dimensional case. In these cases the optimization can be\nperformed graphically.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 10:56:03 GMT"}], "update_date": "2010-04-08", "authors_parsed": [["Kirchner", "Ulrich", ""]]}, {"id": "1004.1489", "submitter": "Mike Ludkovski", "authors": "Michael Ludkovski and Hyekyung Min", "title": "Illiquidity Effects in Optimal Consumption-Investment Problems", "comments": "26 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of liquidity freezes on an economic agent optimizing her\nutility of consumption in a perturbed Black-Scholes-Merton model. The single\nrisky asset follows a geometric Brownian motion but is subject to liquidity\nshocks, during which no trading is possible and stock dynamics are modified.\nThe liquidity regime is governed by a two-state Markov chain. We derive the\nasymptotic effect of such freezes on optimal consumption and investment\nschedules in the two cases of (i) small probability of liquidity shock; (ii)\nfast-scale liquidity regime switching. Explicit formulas are obtained for\nlogarithmic and hyperbolic utility maximizers on infinite horizon. We also\nderive the corresponding loss in utility and compare with a recent related\nfinite-horizon model of Diesinger, Kraft and Seifried (2009).\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2010 08:12:50 GMT"}, {"version": "v2", "created": "Wed, 29 Sep 2010 06:04:08 GMT"}], "update_date": "2010-09-30", "authors_parsed": [["Ludkovski", "Michael", ""], ["Min", "Hyekyung", ""]]}, {"id": "1004.1670", "submitter": "Philip Maymin", "authors": "Philip Z. Maymin and Zakhar G. Maymin", "title": "Any Regulation of Risk Increases Risk", "comments": "25 pages; forthcoming in Financial Markets and Portfolio Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.GN q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that any objective risk measurement algorithm mandated by central\nbanks for regulated financial entities will result in more risk being taken on\nby those financial entities than would otherwise be the case. Furthermore, the\nrisks taken on by the regulated financial entities are far more systemically\nconcentrated than they would have been otherwise, making the entire financial\nsystem more fragile. This result leaves three directions for the future of\nfinancial regulation: continue regulating by enforcing risk measurement\nalgorithms at the cost of occasional severe crises, regulate more severely and\nsubjectively by fully nationalizing all financial entities, or abolish all\ncentral banking regulations including deposit insurance to let risk be\ndetermined by the entities themselves and, ultimately, by their depositors\nthrough voluntary market transactions rather than by the taxpayers through\nenforced government participation.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 02:24:12 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2010 13:55:56 GMT"}, {"version": "v3", "created": "Wed, 25 May 2011 02:32:30 GMT"}, {"version": "v4", "created": "Fri, 20 Apr 2012 19:05:19 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Maymin", "Philip Z.", ""], ["Maymin", "Zakhar G.", ""]]}, {"id": "1004.2947", "submitter": "Marcus Warfheimer", "authors": "Stig Larsson, Carl Lindberg and Marcus Warfheimer", "title": "Optimal closing of a pair trade with a model containing jumps", "comments": "17 pages, 4 figures.", "journal-ref": "Appl. Math. 58 (2013), 249-268", "doi": "10.1007/s10492-013-0012-8", "report-no": null, "categories": "q-fin.CP math.PR q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair trade is a portfolio consisting of a long position in one asset and a\nshort position in another, and it is a widely applied investment strategy in\nthe financial industry. Recently, Ekstr\\\"om, Lindberg and Tysk studied the\nproblem of optimally closing a pair trading strategy when the difference of the\ntwo assets is modelled by an Ornstein-Uhlenbeck process. In this paper we study\nthe same problem, but the model is generalized to also include jumps. More\nprecisely we assume that the above difference is an Ornstein-Uhlenbeck type\nprocess, driven by a L\\'evy process of finite activity. We prove a verification\ntheorem and analyze a numerical method for the associated free boundary\nproblem. We prove rigorous error estimates, which are used to draw some\nconclusions from numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2010 08:02:36 GMT"}], "update_date": "2013-07-16", "authors_parsed": [["Larsson", "Stig", ""], ["Lindberg", "Carl", ""], ["Warfheimer", "Marcus", ""]]}, {"id": "1004.3310", "submitter": "Zbigniew Palmowski", "authors": "Irmina Czarna and Zbigniew Palmowski", "title": "Dividend problem with Parisian delay for a spectrally negative L\\'evy\n  risk process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC math.PR q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider dividend problem for an insurance company whose\nrisk evolves as a spectrally negative L\\'{e}vy process (in the absence of\ndividend payments) when Parisian delay is applied. The objective function is\ngiven by the cumulative discounted dividends received until the moment of ruin\nwhen so-called barrier strategy is applied. Additionally we will consider two\npossibilities of delay. In the first scenario ruin happens when the surplus\nprocess stays below zero longer than fixed amount of time $\\zeta>0$. In the\nsecond case there is a time lag $d$ between decision of paying dividends and\nits implementation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2010 21:19:52 GMT"}, {"version": "v2", "created": "Sat, 31 Jul 2010 13:24:44 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2011 21:08:31 GMT"}], "update_date": "2011-10-19", "authors_parsed": [["Czarna", "Irmina", ""], ["Palmowski", "Zbigniew", ""]]}, {"id": "1004.3525", "submitter": "Lioudmila Vostrikova Professor", "authors": "S. Cawston, L. Vostrikova", "title": "$F$-divergence minimal equivalent martingale measures and optimal\n  portfolios for exponential Levy models with a change-point", "comments": "31 pages, no figures", "journal-ref": "In R. Dalang, M. Dozzi, F. Russo (Editors). Stochastic analysis,\n  random fields and applications VI. Progress in Probability 67, 2013, 285-305", "doi": null, "report-no": null, "categories": "q-fin.PM math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exponential Levy models with change-point which is a random\nvariable, independent from initial Levy processes. On canonical space with\ninitially enlarged filtration we describe all equivalent martingale measures\nfor change-point model and we give the conditions for the existence of\nf-divergence minimal equivalent martingale measure. Using the connection\nbetween utility maximisation and $f$-divergence minimisation, we obtain a\ngeneral formula for optimal strategy in change-point case for initially\nenlarged filtration and also for progressively enlarged filtration in the case\nof exponential utility. We illustrate our results considering the Black-Scholes\nmodel with change-point.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 18:35:46 GMT"}, {"version": "v2", "created": "Tue, 14 Dec 2010 18:54:57 GMT"}, {"version": "v3", "created": "Tue, 14 Jun 2011 15:02:13 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Cawston", "S.", ""], ["Vostrikova", "L.", ""]]}, {"id": "1004.3830", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters and Balakrishnan Kannan and Ben Lasscock and Chris\n  Mellen", "title": "Model Selection and Adaptive Markov chain Monte Carlo for Bayesian\n  Cointegrated VAR model", "comments": "to appear journal Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC)\nmethodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We\nreplace the popular approach to sampling Bayesian CVAR models, involving griddy\nGibbs, with an automated efficient alternative, based on the Adaptive\nMetropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive\nMCMC framework for Bayesian CVAR models allows for efficient estimation of\nposterior parameters in significantly higher dimensional CVAR series than\npreviously possible with existing griddy Gibbs samplers. For a n-dimensional\nCVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with\nsignificant correlation present between the blocks of matrix random variables.\nWe also treat the rank of the CVAR model as a random variable and perform joint\ninference on the rank and model parameters. This is achieved with a Bayesian\nposterior distribution defined over both the rank and the CVAR model\nparameters, and inference is made via Bayes Factor analysis of rank.\nPractically the adaptive sampler also aids in the development of automated\nBayesian cointegration models for algorithmic trading systems considering\ninstruments made up of several assets, such as currency baskets. Previously the\nliterature on financial applications of CVAR trading models typically only\nconsiders pairs trading (n=2) due to the computational cost of the griddy\nGibbs. We are able to extend under our adaptive framework to $n >> 2$ and\ndemonstrate an example with n = 10, resulting in a posterior distribution with\nparameters up to dimension 310. By also considering the rank as a random\nquantity we can ensure our resulting trading models are able to adjust to\npotentially time varying market conditions in a coherent statistical framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 02:26:17 GMT"}], "update_date": "2010-04-23", "authors_parsed": [["Peters", "Gareth W.", ""], ["Kannan", "Balakrishnan", ""], ["Lasscock", "Ben", ""], ["Mellen", "Chris", ""]]}, {"id": "1004.3939", "submitter": "Uwe Aickelin", "authors": "William Wilson, Phil Birkin, Uwe Aickelin", "title": "Price Trackers Inspired by Immune Memory", "comments": "14 pages, 5 figures, 3 tables, 5th International Conference on\n  Artificial Immune Systems (ICARIS2006)", "journal-ref": "Proceedings of the 5th International Conference on Artificial\n  Immune Systems (ICARIS2006), Lecture Notes in Computer Science 4163,\n  p362-375, 2006", "doi": null, "report-no": null, "categories": "cs.AI cs.NE physics.data-an q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we outline initial concepts for an immune inspired algorithm to\nevaluate price time series data. The proposed solution evolves a short term\npool of trackers dynamically through a process of proliferation and mutation,\nwith each member attempting to map to trends in price movements. Successful\ntrackers feed into a long term memory pool that can generalise across repeating\ntrend patterns. Tests are performed to examine the algorithm's ability to\nsuccessfully identify trends in a small data set. The influence of the long\nterm memory pool is then examined. We find the algorithm is able to identify\nprice trends presented successfully and efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 15:01:02 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Wilson", "William", ""], ["Birkin", "Phil", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1004.4169", "submitter": "Fabio Caccioli", "authors": "Fabio Caccioli, Susanne Still, Matteo Marsili and Imre Kondor", "title": "Optimal Liquidation Strategies Regularize Portfolio Selection", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of portfolio optimization in the presence of market\nimpact, and derive optimal liquidation strategies. We discuss in detail the\nproblem of finding the optimal portfolio under Expected Shortfall (ES) in the\ncase of linear market impact. We show that, once market impact is taken into\naccount, a regularized version of the usual optimization problem naturally\nemerges. We characterize the typical behavior of the optimal liquidation\nstrategies, in the limit of large portfolio sizes, and show how the market\nimpact removes the instability of ES in this context.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2010 16:06:32 GMT"}, {"version": "v2", "created": "Mon, 21 Feb 2011 15:49:43 GMT"}], "update_date": "2011-02-22", "authors_parsed": [["Caccioli", "Fabio", ""], ["Still", "Susanne", ""], ["Marsili", "Matteo", ""], ["Kondor", "Imre", ""]]}, {"id": "1004.4272", "submitter": "Michele Tumminello", "authors": "Ester Pantaleo, Michele Tumminello, Fabrizio Lillo and Rosario N.\n  Mantegna", "title": "When do improved covariance matrix estimators enhance portfolio\n  optimization? An empirical comparative study of nine estimators", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM physics.soc-ph q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of improved covariance matrix estimators as an alternative to the\nsample estimator is considered an important approach for enhancing portfolio\noptimization. Here we empirically compare the performance of 9 improved\ncovariance estimation procedures by using daily returns of 90 highly\ncapitalized US stocks for the period 1997-2007. We find that the usefulness of\ncovariance matrix estimators strongly depends on the ratio between estimation\nperiod T and number of stocks N, on the presence or absence of short selling,\nand on the performance metric considered. When short selling is allowed,\nseveral estimation methods achieve a realized risk that is significantly\nsmaller than the one obtained with the sample covariance method. This is\nparticularly true when T/N is close to one. Moreover many estimators reduce the\nfraction of negative portfolio weights, while little improvement is achieved in\nthe degree of diversification. On the contrary when short selling is not\nallowed and T>N, the considered methods are unable to outperform the sample\ncovariance in terms of realized risk but can give much more diversified\nportfolios than the one obtained with the sample covariance. When T<N the use\nof the sample covariance matrix and of the pseudoinverse gives portfolios with\nvery poor performance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2010 12:16:17 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Pantaleo", "Ester", ""], ["Tumminello", "Michele", ""], ["Lillo", "Fabrizio", ""], ["Mantegna", "Rosario N.", ""]]}, {"id": "1004.4956", "submitter": "Yingying Li", "authors": "Jianqing Fan, Yingying Li, Ke Yu", "title": "Vast Volatility Matrix Estimation using High Frequency Data for\n  Portfolio Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.ST q-fin.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio allocation with gross-exposure constraint is an effective method to\nincrease the efficiency and stability of selected portfolios among a vast pool\nof assets, as demonstrated in Fan et al (2008). The required high-dimensional\nvolatility matrix can be estimated by using high frequency financial data. This\nenables us to better adapt to the local volatilities and local correlations\namong vast number of assets and to increase significantly the sample size for\nestimating the volatility matrix. This paper studies the volatility matrix\nestimation using high-dimensional high-frequency data from the perspective of\nportfolio selection. Specifically, we propose the use of \"pairwise-refresh\ntime\" and \"all-refresh time\" methods proposed by Barndorff-Nielsen et al (2008)\nfor estimation of vast covariance matrix and compare their merits in the\nportfolio selection. We also establish the concentration inequalities of the\nestimates, which guarantee desirable properties of the estimated volatility\nmatrix in vast asset allocation with gross exposure constraints. Extensive\nnumerical studies are made via carefully designed simulations. Comparing with\nthe methods based on low frequency daily data, our methods can capture the most\nrecent trend of the time varying volatility and correlation, hence provide more\naccurate guidance for the portfolio allocation in the next time period. The\nadvantage of using high-frequency data is significant in our simulation and\nempirical studies, which consist of 50 simulated assets and 30 constituent\nstocks of Dow Jones Industrial Average index.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 06:11:20 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Li", "Yingying", ""], ["Yu", "Ke", ""]]}]