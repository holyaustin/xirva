[{"id": "2103.01123", "submitter": "Mois\\'es Rodr\\'iguez-Madrena", "authors": "Justo Puerto, Federica Ricca, Mois\\'es Rodr\\'iguez-Madrena, Andrea\n  Scozzari", "title": "A combinatorial optimization approach to scenario filtering in portfolio\n  selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies stressed the fact that covariance matrices computed from\nempirical financial time series appear to contain a high amount of noise. This\nmakes the classical Markowitz Mean-Variance Optimization model unable to\ncorrectly evaluate the performance associated to selected portfolios. Since the\nMarkowitz model is still one of the most used practitioner-oriented tool,\nseveral filtering methods have been proposed in the literature to fix the\nproblem. Among them, the two most promising ones refer to the Random Matrix\nTheory or to the Power Mapping strategy. The basic idea of these methods is to\ntransform the correlation matrix maintaining the Mean-Variance Optimization\nmodel. However, experimental analysis shows that these two strategies are not\nadequately effective when applied to real financial datasets.\n  In this paper we propose an alternative filtering method based on\nCombinatorial Optimization. We advance a new Mixed Integer Quadratic\nProgramming model to filter those observations that may influence the\nperformance of a portfolio in the future. We discuss the properties of this new\nmodel and we test it on some real financial datasets. We compare the\nout-of-sample performance of our portfolios with the one of the portfolios\nprovided by the two above mentioned alternative strategies. We show that our\nmethod outperforms them. Although our model can be solved efficiently with\nstandard optimization solvers the computational burden increases for large\ndatasets. To overcome this issue we also propose a heuristic procedure that\nempirically showed to be both efficient and effective.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:45:32 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Puerto", "Justo", ""], ["Ricca", "Federica", ""], ["Rodr\u00edguez-Madrena", "Mois\u00e9s", ""], ["Scozzari", "Andrea", ""]]}, {"id": "2103.01775", "submitter": "Shota Imaki", "authors": "Shota Imaki, Kentaro Imajo, Katsuya Ito, Kentaro Minami, Kei Nakagawa", "title": "No-Transaction Band Network: A Neural Network Architecture for Efficient\n  Deep Hedging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep hedging (Buehler et al. 2019) is a versatile framework to compute the\noptimal hedging strategy of derivatives in incomplete markets. However, this\noptimal strategy is hard to train due to action dependence, that is, the\nappropriate hedging action at the next step depends on the current action. To\novercome this issue, we leverage the idea of a no-transaction band strategy,\nwhich is an existing technique that gives optimal hedging strategies for\nEuropean options and the exponential utility. We theoretically prove that this\nstrategy is also optimal for a wider class of utilities and derivatives\nincluding exotics. Based on this result, we propose a no-transaction band\nnetwork, a neural network architecture that facilitates fast training and\nprecise evaluation of the optimal hedging strategy. We experimentally\ndemonstrate that for European and lookback options, our architecture quickly\nattains a better hedging strategy in comparison to a standard feed-forward\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:46:12 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Imaki", "Shota", ""], ["Imajo", "Kentaro", ""], ["Ito", "Katsuya", ""], ["Minami", "Kentaro", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2103.03275", "submitter": "Josselin Garnier", "authors": "Josselin Garnier", "title": "The Climate Extended Risk Model (CERM)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is directed to the financial community and focuses on the\nfinancial risks associated with climate change. It, specifically, addresses the\nestimate of climate risk embedded within a bank loan portfolio. During the 21st\ncentury, man-made carbon dioxide emissions in the atmosphere will raise global\ntemperatures, resulting in severe and unpredictable physical damage across the\nglobe. Another uncertainty associated with climate, known as the energy\ntransition risk, comes from the unpredictable pace of political and legal\nactions to limit its impact. The Climate Extended Risk Model (CERM) adapts well\nknown credit risk models. It proposes a method to calculate incremental credit\nlosses on a loan portfolio that are rooted into physical and transition risks.\nThe document provides detailed description of the model hypothesis and steps.\nThis work was initiated by the association Green RWA (Risk Weighted Assets). It\nwas written in collaboration with Jean-Baptiste Gaudemet, Anne Gruz, and\nOlivier Vinciguerra (cerm@greenrwa.org), who contributed their financial and\nrisk expertise, taking care of its application to a pilot-portfolio. It extends\nthe model proposed in a first white paper published by Green RWA\n(https://www.greenrwa.org/).\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:23:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Garnier", "Josselin", ""]]}, {"id": "2103.04375", "submitter": "Fabio Caccioli", "authors": "G\\'abor Papp, Imre Kondor, Fabio Caccioli", "title": "Optimizing Expected Shortfall under an $\\ell_1$ constraint -- an\n  analytic approach", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": "10.3390/e23050523", "report-no": null, "categories": "q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expected Shortfall (ES), the average loss above a high quantile, is the\ncurrent financial regulatory market risk measure. Its estimation and\noptimization are highly unstable against sample fluctuations and become\nimpossible above a critical ratio $r=N/T$, where $N$ is the number of different\nassets in the portfolio, and $T$ is the length of the available time series.\nThe critical ratio depends on the confidence level $\\alpha$, which means we\nhave a line of critical points on the $\\alpha-r$ plane. The large fluctuations\nin the estimation of ES can be attenuated by the application of regularizers.\nIn this paper, we calculate ES analytically under an $\\ell_1$ regularizer by\nthe method of replicas borrowed from the statistical physics of random systems.\nThe ban on short selling, i.e. a constraint rendering all the portfolio weights\nnon-negative, is a special case of an asymmetric $\\ell_1$ regularizer. Results\nare presented for the out-of-sample and the in-sample estimator of the\nregularized ES, the estimation error, the distribution of the optimal portfolio\nweights and the density of the assets eliminated from the portfolio by the\nregularizer. It is shown that the no-short constraint acts as a high volatility\ncutoff, in the sense that it sets the weights of the high volatility elements\nto zero with higher probability than those of the low volatility items. This\ncutoff renormalizes the aspect ratio $r=N/T$, thereby extending the range of\nthe feasibility of optimization. We find that there is a nontrivial mapping\nbetween the regularized and unregularized problems, corresponding to a\nrenormalization of the order parameters.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 15:25:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Papp", "G\u00e1bor", ""], ["Kondor", "Imre", ""], ["Caccioli", "Fabio", ""]]}, {"id": "2103.04432", "submitter": "W. Brent Lindquist", "authors": "Yuan Hu and W. Brent Lindquist", "title": "Portfolio Optimization Constrained by Performance Attribution", "comments": "15 pages, 7 figures. Submitted to Journal of Risk and Financial\n  Management", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates performance attribution measures as a basis for\nconstraining portfolio optimization. We employ optimizations that minimize\nexpected tail loss and investigate both asset allocation (AA) and the selection\neffect (SE) as hard constraints on asset weights. The test portfolio consists\nof stocks from the Dow Jones Industrial Average index; the benchmark is an\nequi-weighted portfolio of the same stocks. Performance of the optimized\nportfolios is judged using comparisons of cumulative price and the\nrisk-measures maximum drawdown, Sharpe ratio, and Rachev ratio. The results\nsuggest a positive role in price and risk-measure performance for the\nimposition of constraints on AA and SE, with SE constraints producing the\nlarger performance enhancement.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:23:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hu", "Yuan", ""], ["Lindquist", "W. Brent", ""]]}, {"id": "2103.04898", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh", "title": "On Asymptotic Log-Optimal Buy-and-Hold Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.CP q-fin.MF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we consider a frequency-based portfolio optimization problem\nwith $m \\geq 2$ assets when the expected logarithmic growth (ELG) rate of\nwealth is used as the performance metric. With the aid of the notion called\ndominant asset, it is known that the optimal ELG level is achieved by investing\nall available funds on that asset. However, such an \"all-in\" strategy is\narguably too risky to implement in practice. Motivated by this issue, we study\nthe case where the portfolio weights are chosen in a rather ad-hoc manner and a\nbuy-and-hold strategy is subsequently used. Then we show that, if the\nunderlying portfolio contains a dominant asset, buy and hold on that specific\nasset is asymptotically log-optimal with a sublinear rate of convergence. This\nresult also extends to the scenario where a trader either does not have a\nprobabilistic model for the returns or does not trust a model obtained from\nhistorical data. To be more specific, we show that if a market contains a\ndominant asset, buy and hold a market portfolio involving nonzero weights for\neach asset is asymptotically log-optimal. Additionally, this paper also\nincludes a conjecture regarding the property called high-frequency maximality.\nThat is, in the absence of transaction costs, high-frequency rebalancing is\nunbeatable in the ELG sense. Support for the conjecture, involving a lemma for\na weak version of the conjecture, is provided. This conjecture, if true,\nenables us to improve the log-optimality result obtained previously. Finally, a\nresult that indicates a way regarding an issue about when should one to\nrebalance their portfolio if needed, is also provided. Examples, some involving\nsimulations with historical data, are also provided along the way to illustrate\nthe~theory.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:01:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hsieh", "Chung-Han", ""]]}, {"id": "2103.05455", "submitter": "Jack Gindi", "authors": "Nicholas Moehle, Jack Gindi, Stephen Boyd, Mykel Kochenderfer", "title": "Portfolio Construction as Linearly Constrained Separable Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mean-variance portfolio optimization problems often involve separable\nnonconvex terms, including penalties on capital gains, integer share\nconstraints, and minimum position and trade sizes. We propose a heuristic\nalgorithm for this problem based on the alternating direction method of\nmultipliers (ADMM). This method allows for solve times in tens to hundreds of\nmilliseconds with around 1000 securities and 100 risk factors. We also obtain a\nbound on the achievable performance. Our heuristic and bound are both derived\nfrom similar results for other optimization problems with a separable objective\nand affine equality constraints. We discuss a concrete implementation in the\ncase where the separable terms in the objective are piecewise-quadratic, and we\ndemonstrate their effectiveness empirically in realistic tax-aware portfolio\nconstruction problems.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:42:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Moehle", "Nicholas", ""], ["Gindi", "Jack", ""], ["Boyd", "Stephen", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "2103.05777", "submitter": "Nicole B\\\"auerle", "authors": "Nicole B\\\"auerle and Gregor Leimcke", "title": "Bayesian optimal investment and reinsurance with dependent financial and\n  insurance risks", "comments": "arXiv admin note: text overlap with arXiv:2001.11301", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major events like natural catastrophes or the COVID-19 crisis have impact\nboth on the financial market and on claim arrival intensities and claim sizes\nof insurers. Thus, when optimal investment and reinsurance strategies have to\nbe determined it is important to consider models which reflect this dependence.\nIn this paper we make a proposal how to generate dependence between the\nfinancial market and claim sizes in times of crisis and determine via a\nstochastic control approach an optimal investment and reinsurance strategy\nwhich maximizes the expected exponential utility of terminal wealth. Moreover,\nwe also allow that the claim size distribution may be learned in the model. We\ngive comparisons and bounds on the optimal strategy using simple models. What\nturns out to be very surprising is that numerical results indicate that even a\nminimal dependence which is created in this model has a huge impact on the\ncontrol in the sense that the insurer is much more prudent then.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2021 08:35:53 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["B\u00e4uerle", "Nicole", ""], ["Leimcke", "Gregor", ""]]}, {"id": "2103.05880", "submitter": "Sakae Oya", "authors": "Sakae Oya", "title": "A Bayesian Graphical Approach for Large-Scale Portfolio Management with\n  Fewer Historical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing a large-scale portfolio with many assets is one of the most\nchallenging tasks in the field of finance. It is partly because estimation of\neither covariance or precision matrix of asset returns tends to be unstable or\neven infeasible when the number of assets $p$ exceeds the number of\nobservations $n$. For this reason, most of the previous studies on portfolio\nmanagement have focused on the case of $p < n$. To deal with the case of $p >\nn$, we propose to use a new Bayesian framework based on adaptive graphical\nLASSO for estimating the precision matrix of asset returns in a large-scale\nportfolio. Unlike the previous studies on graphical LASSO in the literature,\nour approach utilizes a Bayesian estimation method for the precision matrix\nproposed by Oya and Nakatsuma (2020) so that the positive definiteness of the\nprecision matrix should be always guaranteed. As an empirical application, we\nconstruct the global minimum variance portfolio of $p=100$ for various values\nof $n$ with the proposed approach as well as the non-Bayesian graphical LASSO\napproach, and compare their out-of-sample performance with the equal weight\nportfolio as the benchmark. In this comparison, the proposed approach produces\nmore stable results than the non-Bayesian approach in terms of Sharpe ratio,\nportfolio composition and turnover. Furthermore, the proposed approach succeeds\nin estimating the precision matrix even if $n$ is much smaller than $p$ and the\nnon-Bayesian approach fails to do so.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 05:52:28 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Oya", "Sakae", ""]]}, {"id": "2103.09987", "submitter": "Yu Man Tam", "authors": "Raymond C. W. Leung and Yu-Man Tam", "title": "Statistical Arbitrage Risk Premium by Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How to hedge factor risks without knowing the identities of the factors? We\nfirst prove a general theoretical result: even if the exact set of factors\ncannot be identified, any risky asset can use some portfolio of similar peer\nassets to hedge against its own factor exposures. A long position of a risky\nasset and a short position of a \"replicate portfolio\" of its peers represent\nthat asset's factor residual risk. We coin the expected return of an asset's\nfactor residual risk as its Statistical Arbitrage Risk Premium (SARP). The\nchallenge in empirically estimating SARP is finding the peers for each asset\nand constructing the replicate portfolios. We use the elastic-net, a machine\nlearning method, to project each stock's past returns onto that of every other\nstock. The resulting high-dimensional but sparse projection vector serves as\ninvestment weights in constructing the stocks' replicate portfolios. We say a\nstock has high (low) Statistical Arbitrage Risk (SAR) if it has low (high)\nR-squared with its peers. The key finding is that \"unique\" stocks have both a\nhigher SARP and higher excess returns than \"ubiquitous\" stocks: in the\ncross-section, high SAR stocks have a monthly SARP (monthly excess returns)\nthat is 1.101% (0.710%) greater than low SAR stocks. The average SAR across all\nstocks is countercyclical. Our results are robust to controlling for various\nknown priced factors and characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:18:52 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Leung", "Raymond C. W.", ""], ["Tam", "Yu-Man", ""]]}, {"id": "2103.10813", "submitter": "Ayse Sinem Uysal", "authors": "Xiaoyue Li, A. Sinem Uysal, John M. Mulvey", "title": "Multi-Period Portfolio Optimization using Model Predictive Control with\n  Mean-Variance and Risk Parity Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We employ model predictive control for a multi-period portfolio optimization\nproblem. In addition to the mean-variance objective, we construct a portfolio\nwhose allocation is given by model predictive control with a risk-parity\nobjective, and provide a successive convex program algorithm that provides 30\ntimes faster and robust solutions in the experiments. Computational results on\nthe multi-asset universe show that multi-period models perform better than\ntheir single period counterparts in out-of-sample period, 2006-2020. The\nout-of-sample risk-adjusted performance of both mean-variance and risk-parity\nformulations beat the fix-mix benchmark, and achieve Sharpe ratio of 0.64 and\n0.97, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:58:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Xiaoyue", ""], ["Uysal", "A. Sinem", ""], ["Mulvey", "John M.", ""]]}, {"id": "2103.10925", "submitter": "Steven Campbell", "authors": "Steven Campbell, Ting-Kam Leonard Wong", "title": "Functional portfolio optimization in stochastic portfolio theory", "comments": "37 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a concrete and fully implementable approach to the\noptimization of functionally generated portfolios in stochastic portfolio\ntheory. The main idea is to optimize over a family of rank-based portfolios\nparameterized by an exponentially concave function on the unit interval. This\nchoice can be motivated by the long term stability of the capital distribution\nobserved in large equity markets, and allows us to circumvent the curse of\ndimensionality. The resulting optimization problem, which is convex, is\nflexible as various regularizations and constraints can be imposed on the\ngenerating function. We prove that the optimization problem is well-posed and\nprovide a stability estimate in terms of a Wasserstein metric of the input\nmeasure. We then give a careful treatment of its discretization and the\noptimization algorithm. Finally, we present empirical examples using CRSP data\nfrom the US stock market.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:45:29 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 18:48:59 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Campbell", "Steven", ""], ["Wong", "Ting-Kam Leonard", ""]]}, {"id": "2103.10958", "submitter": "Kerstin D\\\"achert", "authors": "Kerstin D\\\"achert, Ria Grindel, Elisabeth Leoff, Jonas Mahnkopp,\n  Florian Schirra and J\\\"org Wenzel", "title": "Multicriteria asset allocation in practice", "comments": "24 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-fin.PM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we consider the strategic asset allocation of an insurance\ncompany. This task can be seen as a special case of portfolio optimization. In\nthe 1950s, Markowitz proposed to formulate portfolio optimization as a\nbicriteria optimization problem considering risk and return as objectives.\nHowever, recent developments in the field of insurance require four and more\nobjectives to be considered, among them the so-called solvency ratio that stems\nfrom the Solvency II directive of the European Union issued in 2009. Moreover,\nthe distance to the current portfolio plays an important role. While literature\non portfolio optimization with three objectives is already scarce, applications\nwith four and more objectives have not yet been solved so far by\nmulti-objective approaches based on scalarizations. However, recent algorithmic\nimprovements in the field of exact multi-objective methods allow the\nincorporation of many objectives and the generation of well-spread\nrepresentations within few iterations. We describe the implementation of such\nan algorithm for a strategic asset allocation with four objective functions and\ndemonstrate its usefulness for the practitioner. Our approach is in operative\nuse in a German insurance company. Our partners report a significant\nimprovement in their decision making process since, due to the proper\nintegration of the new objectives, the software proposes portfolios of much\nbetter quality than before within short running time.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 15:42:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["D\u00e4chert", "Kerstin", ""], ["Grindel", "Ria", ""], ["Leoff", "Elisabeth", ""], ["Mahnkopp", "Jonas", ""], ["Schirra", "Florian", ""], ["Wenzel", "J\u00f6rg", ""]]}, {"id": "2103.10989", "submitter": "Fouad Marri", "authors": "Fouad Marri and Khouzeima Moutanabbir", "title": "Risk aggregation and capital allocation using a new generalized\n  Archimedean copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.PM q-fin.PR q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address risk aggregation and capital allocation problems in\nthe presence of dependence between risks. The dependence structure is defined\nby a mixed Bernstein copula which represents a generalization of the well-known\nArchimedean copulas. Using this new copula, the probability density function\nand the cumulative distribution function of the aggregate risk are obtained.\nThen, closed-form expressions for basic risk measures, such as tail\nvalue-at-risk(TVaR) and TVaR-based allocations, are derived.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:06:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Marri", "Fouad", ""], ["Moutanabbir", "Khouzeima", ""]]}, {"id": "2103.11455", "submitter": "Huanming Zhang", "authors": "Huanming Zhang, Zhengyong Jiang, Jionglong Su", "title": "A Deep Deterministic Policy Gradient-based Strategy for Stocks Portfolio\n  Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the improvement of computer performance and the development of\nGPU-accelerated technology, trading with machine learning algorithms has\nattracted the attention of many researchers and practitioners. In this\nresearch, we propose a novel portfolio management strategy based on the\nframework of Deep Deterministic Policy Gradient, a policy-based reinforcement\nlearning framework, and compare its performance to that of other trading\nstrategies. In our framework, two Long Short-Term Memory neural networks and\ntwo fully connected neural networks are constructed. We also investigate the\nperformance of our strategy with and without transaction costs. Experimentally,\nwe choose eight US stocks consisting of four low-volatility stocks and four\nhigh-volatility stocks. We compare the compound annual return rate of our\nstrategy against seven other strategies, e.g., Uniform Buy and Hold,\nExponential Gradient and Universal Portfolios. In our case, the compound annual\nreturn rate is 14.12%, outperforming all other strategies. Furthermore, in\nterms of Sharpe Ratio (0.5988), our strategy is nearly 33% higher than that of\nthe second-best performing strategy.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 18:17:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Huanming", ""], ["Jiang", "Zhengyong", ""], ["Su", "Jionglong", ""]]}, {"id": "2103.12345", "submitter": "Yijian Chuan", "authors": "Yijian Chuan, Chaoyi Zhao, Zhenrui He, and Lan Wu", "title": "The Success of AdaBoost and Its Application in Portfolio Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approach to explain why AdaBoost is a successful\nclassifier. By introducing a measure of the influence of the noise points (ION)\nin the training data for the binary classification problem, we prove that there\nis a strong connection between the ION and the test error. We further identify\nthat the ION of AdaBoost decreases as the iteration number or the complexity of\nthe base learners increases. We confirm that it is impossible to obtain a\nconsistent classifier without deep trees as the base learners of AdaBoost in\nsome complicated situations. We apply AdaBoost in portfolio management via\nempirical studies in the Chinese market, which corroborates our theoretical\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 06:41:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chuan", "Yijian", ""], ["Zhao", "Chaoyi", ""], ["He", "Zhenrui", ""], ["Wu", "Lan", ""]]}, {"id": "2103.13806", "submitter": "Alireza Ghahtarani", "authors": "Alireza Ghahtarani, Ahmed Saif, Alireza Ghasemi", "title": "Robust Portfolio Selection Problems: A Comprehensive Review", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we provide a comprehensive review of recent advances in robust\nportfolio selection problems and their extensions, from both operational\nresearch and financial perspectives. A multi-dimensional classification of the\nmodels and methods proposed in the literature is presented, based on the types\nof financial problems, uncertainty sets, robust optimization approaches, and\nmathematical formulations. Several open questions and potential future research\ndirections are identified.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:48:55 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ghahtarani", "Alireza", ""], ["Saif", "Ahmed", ""], ["Ghasemi", "Alireza", ""]]}, {"id": "2103.14506", "submitter": "Wenpin Tang", "authors": "Wenpin Tang and Xiao Xu and Xun Yu Zhou", "title": "Asset Selection via Correlation Blockmodel Clustering", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.CP q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We aim to cluster financial assets in order to identify a small set of stocks\nto approximate the level of diversification of the whole universe of stocks. We\ndevelop a data-driven approach to clustering based on a correlation blockmodel\nin which assets in the same cluster have the same correlations with all other\nassets. We devise an algorithm to detect the clusters, with a theoretical\nanalysis and a practical guidance. Finally, we conduct an empirical analysis to\nattest the performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:04:22 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tang", "Wenpin", ""], ["Xu", "Xiao", ""], ["Zhou", "Xun Yu", ""]]}, {"id": "2103.15232", "submitter": "Pier Francesco Procacci", "authors": "Pier Francesco Procacci and Tomaso Aste", "title": "Portfolio Optimization with Sparse Multivariate Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Portfolio optimization approaches inevitably rely on multivariate modeling of\nmarkets and the economy. In this paper, we address three sources of error\nrelated to the modeling of these complex systems: 1. oversimplifying\nhypothesis; 2. uncertainties resulting from parameters' sampling error; 3.\nintrinsic non-stationarity of these systems. For what concerns point 1. we\npropose a L0-norm sparse elliptical modeling and show that sparsification is\neffective. The effects of points 2. and 3. are quantifified by studying the\nmodels' likelihood in- and out-of-sample for parameters estimated over train\nsets of different lengths. We show that models with larger off-sample\nlikelihoods lead to better performing portfolios up to when two to three years\nof daily observations are included in the train set. For larger train sets, we\nfound that portfolio performances deteriorate and detach from the models'\nlikelihood, highlighting the role of non-stationarity. We further investigate\nthis phenomenon by studying the out-of-sample likelihood of individual\nobservations showing that the system changes significantly through time. Larger\nestimation windows lead to stable likelihood in the long run, but at the cost\nof lower likelihood in the short-term: the `optimal' fit in finance needs to be\ndefined in terms of the holding period. Lastly, we show that sparse models\noutperform full-models in that they deliver higher out of sample likelihood,\nlower realized portfolio volatility and improved portfolios' stability,\navoiding typical pitfalls of the Mean-Variance optimization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 22:05:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Procacci", "Pier Francesco", ""], ["Aste", "Tomaso", ""]]}, {"id": "2103.16451", "submitter": "Viet Anh Nguyen", "authors": "Viet Anh Nguyen, Fan Zhang, Jose Blanchet, Erick Delage, Yinyu Ye", "title": "Robustifying Conditional Portfolio Decisions via Optimal Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.OC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a data-driven portfolio selection model that integrates side\ninformation, conditional estimation and robustness using the framework of\ndistributionally robust optimization. Conditioning on the observed side\ninformation, the portfolio manager solves an allocation problem that minimizes\nthe worst-case conditional risk-return trade-off, subject to all possible\nperturbations of the covariate-return probability distribution in an optimal\ntransport ambiguity set. Despite the non-linearity of the objective function in\nthe probability measure, we show that the distributionally robust portfolio\nallocation with side information problem can be reformulated as a\nfinite-dimensional optimization problem. If portfolio decisions are made based\non either the mean-variance or the mean-Conditional Value-at-Risk criterion,\nthe resulting reformulation can be further simplified to second-order or\nsemi-definite cone programs. Empirical studies in the US and Chinese equity\nmarkets demonstrate the advantage of our integrative framework against other\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:56:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Nguyen", "Viet Anh", ""], ["Zhang", "Fan", ""], ["Blanchet", "Jose", ""], ["Delage", "Erick", ""], ["Ye", "Yinyu", ""]]}, {"id": "2103.16800", "submitter": "Yilun Song", "authors": "Lin He, Zongxia Liang, Yilun Song, Qi Ye", "title": "Optimal Retirement Time and Consumption with the Variation in Habitual\n  Persistence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper,we study the individual's optimal retirement time and optimal\nconsumption under habitual persistence. Because the individual feels equally\nsatisfied with a lower habitual level and is more reluctant to change the\nhabitual level after retirement, we assume that both the level and the\nsensitivity of the habitual consumption decline at the time of retirement. We\nestablish the concise form of the habitual evolutions, and obtain the optimal\nretirement time and consumption policy based on martingale and duality methods.\nThe optimal consumption experiences a sharp decline at retirement, but the\nexcess consumption raises because of the reduced sensitivity of the habitual\nlevel. This result contributes to explain the \"retirement consumption puzzle\".\nParticularly, the optimal retirement and consumption policies are balanced\nbetween the wealth effect and the habitual effect. Larger wealth increases\nconsumption, and larger growth inertia (sensitivity) of the habitual level\ndecreases consumption and brings forward the retirement time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 04:26:22 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["He", "Lin", ""], ["Liang", "Zongxia", ""], ["Song", "Yilun", ""], ["Ye", "Qi", ""]]}]