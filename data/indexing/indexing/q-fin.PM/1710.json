[{"id": "1710.00431", "submitter": "Zachariah Peterson", "authors": "Zachariah Peterson", "title": "Kelly's Criterion in Portfolio Optimization: A Decoupled Problem", "comments": "16 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Kelly's Criterion is well known among gamblers and investors as a method for\nmaximizing the returns one would expect to observe over long periods of betting\nor investing. These ideas are conspicuously absent from portfolio optimization\nproblems in the financial and automation literature. This paper will show how\nKelly's Criterion can be incorporated into standard portfolio optimization\nmodels. The model developed here combines risk and return into a single\nobjective function by incorporating a risk parameter. This model is then solved\nfor a portfolio of 10 stocks from a major stock exchange using a differential\nevolution algorithm. Monte Carlo calculations are used to verify the accuracy\nof the results obtained from differential evolution. The results show that\nevolutionary algorithms can be successfully applied to solve a portfolio\noptimization problem where returns are calculated by applying Kelly's Criterion\nto each of the assets in the portfolio.\n", "versions": [{"version": "v1", "created": "Sun, 1 Oct 2017 23:03:28 GMT"}, {"version": "v2", "created": "Sat, 17 Feb 2018 22:17:15 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Peterson", "Zachariah", ""]]}, {"id": "1710.01503", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh and B. Ross Barmish", "title": "On Drawdown-Modulated Feedback Control in Stock Trading", "comments": "Proc. 20th IFAC World Congress (IFAC WC 2017), Toulouse, France, July\n  9-14, 2017 (accepted)", "journal-ref": "IFAC-PapersOnLine Volume 50, Issue 1, 2017", "doi": "10.1016/j.ifacol.2017.08.167", "report-no": null, "categories": "math.OC q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control of drawdown, that is, the control of the drops in wealth over time\nfrom peaks to subsequent lows, is of great concern from a risk management\nperspective. With this motivation in mind, the focal point of this paper is to\naddress the drawdown issue in a stock trading context. Although our analysis\ncan be carried out without reference to control theory, to make the work\naccessible to this community, we use the language of feedback systems. The\ntakeoff point for the results to follow, which we call the Drawdown Modulation\nLemma, characterizes any investment which guarantees that the percentage\ndrawdown is no greater than a prespecified level with probability one. With the\naid of this lemma, we introduce a new scheme which we call the\ndrawdown-modulated feedback control. To illustrate the power of the theory, we\nconsider a drawdown-constrained version of the well-known Kelly Optimization\nProblem which involves maximizing the expected logarithmic growth of the\ntrader's account value. As the drawdown parameter dmax in our new formulation\ntends to one, we recover existing results as a special case. This new theory\nleads to an optimal investment strategy whose application is illustrated via an\nexample with historical stock-price data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 08:24:28 GMT"}], "update_date": "2017-10-20", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""]]}, {"id": "1710.01786", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh, B. Ross Barmish, and John A. Gubner", "title": "Kelly Betting Can Be Too Conservative", "comments": "Accepted in 2016 IEEE 55th Conference on Decision and Control (CDC)", "journal-ref": "Proceedings of the IEEE Conference on Decision and Control (CDC),\n  pp .3695-3701, 2016", "doi": "10.1109/CDC.2016.7798825", "report-no": null, "categories": "q-fin.PM math.OC q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kelly betting is a prescription for optimal resource allocation among a set\nof gambles which are typically repeated in an independent and identically\ndistributed manner. In this setting, there is a large body of literature which\nincludes arguments that the theory often leads to bets which are \"too\naggressive\" with respect to various risk metrics. To remedy this problem, many\npapers include prescriptions for scaling down the bet size. Such schemes are\nreferred to as Fractional Kelly Betting. In this paper, we take the opposite\ntack. That is, we show that in many cases, the theoretical Kelly-based results\nmay lead to bets which are \"too conservative\" rather than too aggressive. To\nmake this argument, we consider a random vector X with its assumed probability\ndistribution and draw m samples to obtain an empirically-derived counterpart\nXhat. Subsequently, we derive and compare the resulting Kelly bets for both X\nand Xhat with consideration of sample size m as part of the analysis. This\nleads to identification of many cases which have the following salient feature:\nThe resulting bet size using the true theoretical distribution for X is much\nsmaller than that for Xhat. If instead the bet is based on empirical data,\n\"golden\" opportunities are identified which are essentially rejected when the\npurely theoretical model is used. To formalize these ideas, we provide a result\nwhich we call the Restricted Betting Theorem. An extreme case of the theorem is\nobtained when X has unbounded support. In this situation, using X, the Kelly\ntheory can lead to no betting at all.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:01:54 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""], ["Gubner", "John A.", ""]]}, {"id": "1710.01787", "submitter": "Chung-Han Hsieh", "authors": "Chung-Han Hsieh and B. Ross Barmish", "title": "On Kelly Betting: Some Limitations", "comments": "Accepted in 53rd Annual Allerton Conference on Communication,\n  Control, and Computing, 2015", "journal-ref": "Proceedings of the Annual Allerton Conference on Communication,\n  Control, and Computing, pp.165-172, 2015", "doi": "10.1109/ALLERTON.2015.7447000", "report-no": null, "categories": "math.OC q-fin.PM q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focal point of this paper is the so-called Kelly Criterion, a\nprescription for optimal resource allocation among a set of gambles which are\nrepeated over time. The criterion calls for maximization of the expected value\nof the logarithmic growth of wealth. While significant literature exists\nproviding the rationale for such an optimization, this paper concentrates on\nthe limitations of the Kelly-based theory. To this end, we fill a void in\npublished results by providing specific examples quantifying what difficulties\nare encountered when Taylor-style approximations are used and when wealth\ndrawdowns are considered. For the case of drawdown, we describe some research\ndirections which we feel are promising for improvement of the theory.\n", "versions": [{"version": "v1", "created": "Wed, 4 Oct 2017 20:05:32 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Hsieh", "Chung-Han", ""], ["Barmish", "B. Ross", ""]]}, {"id": "1710.02435", "submitter": "Philipp Johannes Kremer", "authors": "Philipp J. Kremer, Sangkyun Lee, Malgorzata Bogdan, Sandra Paterlini", "title": "Sparse Portfolio Selection via the sorted $\\ell_{1}$-Norm", "comments": "41 pages", "journal-ref": "Journal of Banking & Finance, Volume 110, January 2020, 105687", "doi": "10.1016/j.jbankfin.2019.105687", "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a financial portfolio optimization framework that allows us to\nautomatically select the relevant assets and estimate their weights by relying\non a sorted $\\ell_1$-Norm penalization, henceforth SLOPE. Our approach is able\nto group constituents with similar correlation properties, and with the same\nunderlying risk factor exposures. We show that by varying the intensity of the\npenalty, SLOPE can span the entire set of optimal portfolios on the\nrisk-diversification frontier, from minimum variance to the equally weighted.\nTo solve the optimization problem, we develop a new efficient algorithm, based\non the Alternating Direction Method of Multipliers. Our empirical analysis\nshows that SLOPE yields optimal portfolios with good out-of-sample risk and\nreturn performance properties, by reducing the overall turnover through more\nstable asset weight estimates. Moreover, using the automatic grouping property\nof SLOPE, new portfolio strategies, such as SLOPE-MV, can be developed to\nexploit the data-driven detected similarities across assets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Oct 2017 14:45:44 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Kremer", "Philipp J.", ""], ["Lee", "Sangkyun", ""], ["Bogdan", "Malgorzata", ""], ["Paterlini", "Sandra", ""]]}, {"id": "1710.03267", "submitter": "Abhishek Mohan", "authors": "Abhishek Mohan, Agnibho Roy", "title": "A Strategic Investment Framework for Biotechnology Markets via Dynamic\n  Asset Allocation and Class Diversification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an innovative investment framework incorporating\nasset allocation and class diversification oriented specifically for the\nbiotechnology industry. With growing interests and capitalization in multiple\nbiotech markets, investors require a more dynamic method of managing their\nassets within individual portfolios for optimal return efficiency. By selecting\na single firm representative of identified industry trends, analyzing financial\nmetrics relevant to the suggested approaches, and assessing financial health,\nwe developed an adaptable investment methodology. We also performed analyses of\nindustrial viability and investigated the implications of the selected\nstrategies, with which we were able to optimize our framework for versatile\napplication within specialized biotech markets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Oct 2017 19:14:36 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Mohan", "Abhishek", ""], ["Roy", "Agnibho", ""]]}, {"id": "1710.04579", "submitter": "Stanislaus  Maier-Paape", "authors": "Stanislaus Maier-Paape and Qiji Jim Zhu", "title": "A General Framework for Portfolio Theory. Part I: theory and various\n  models", "comments": null, "journal-ref": "Risks 2018, 6(2), 53", "doi": "10.3390/risks6020053", "report-no": null, "categories": "q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utility and risk are two often competing measurements on the investment\nsuccess. We show that efficient trade-off between these two measurements for\ninvestment portfolios happens, in general, on a convex curve in the two\ndimensional space of utility and risk. This is a rather general pattern. The\nmodern portfolio theory of Markowitz [H. Markowitz, Portfolio Selection, 1959]\nand its natural generalization, the capital market pricing model, [W. F.\nSharpe, Mutual fund performance , 1966] are special cases of our general\nframework when the risk measure is taken to be the standard deviation and the\nutility function is the identity mapping. Using our general framework, we also\nrecover the results in [R. T. Rockafellar, S. Uryasev and M. Zabarankin, Master\nfunds in portfolio analysis with general deviation measures, 2006] that extends\nthe capital market pricing model to allow for the use of more general deviation\nmeasures. This generalized capital asset pricing model also applies to e.g.\nwhen an approximation of the maximum drawdown is considered as a risk measure.\nFurthermore, the consideration of a general utility function allows to go\nbeyond the \"additive\" performance measure to a \"multiplicative\" one of\ncumulative returns by using the log utility. As a result, the growth optimal\nportfolio theory [J. Lintner, The valuation of risk assets and the selection of\nrisky investments in stock portfolios and capital budgets, 1965] and the\nleverage space portfolio theory [R. Vince, The Leverage Space Trading Model,\n2009] can also be understood under our general framework. Thus, this general\nframework allows a unification of several important existing portfolio theories\nand goes much beyond.\n", "versions": [{"version": "v1", "created": "Thu, 12 Oct 2017 15:50:55 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Maier-Paape", "Stanislaus", ""], ["Zhu", "Qiji Jim", ""]]}, {"id": "1710.04818", "submitter": "Stanislaus  Maier-Paape", "authors": "Stanislaus Maier-Paape and Qiji Jim Zhu", "title": "A General Framework for Portfolio Theory. Part II: drawdown risk\n  measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide several examples of convex risk measures\nnecessary for the application of the general framework for portfolio theory of\nMaier-Paape and Zhu, presented in Part I of this series (arXiv:1710.04579\n[q-fin.PM]). As alternative to classical portfolio risk measures such as the\nstandard deviation we in particular construct risk measures related to the\ncurrent drawdown of the portfolio equity. Combined with the results of Part I\n(arXiv:1710.04579 [q-fin.PM]), this allows us to calculate efficient portfolios\nbased on a drawdown risk measure constraint.\n", "versions": [{"version": "v1", "created": "Fri, 13 Oct 2017 06:20:49 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Maier-Paape", "Stanislaus", ""], ["Zhu", "Qiji Jim", ""]]}, {"id": "1710.06350", "submitter": "Ilija Zovko", "authors": "Ilija I. Zovko", "title": "Navigating dark liquidity (How Fisher catches Poisson in the Dark)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reduce signalling, traders may resort to limiting access to dark\nvenues and imposing limits on minimum fill sizes they are willing to trade.\nHowever, doing this also restricts the liquidity available to the trader since\nan ever increasing quantity of orders are traded by algos in clips. An\nalternative is to attempt to monitor signalling in real time and dynamically\nmake adjustments to the dark liquidity accessed.\n  In practice, price slippage against the order is commonly taken as an\nindication of signalling. However, estimating slippage is difficult and\nrequires a large number of fills to reliably detect it. Ultimately, even if\ndetected, it fails to capture an important element of causality between dark\nfills and lit prints - a signature of information leakage. In the extreme, this\ncan lead to scaling back trading at a time when slippage is caused by a\ncompeting trader consuming liquidity, and the appropriate action would be to\nscale trading up -- not down -- in order to capture good prices.\n  In this paper we describe a methodology aimed to address this dichotomy of\ntrading objectives, allowing to maximally capture available liquidity while at\nthe same time protecting the trader from excessive signalling. The method is\ndesigned to profile dark liquidity in a dynamic fashion, on a per fill basis,\nin contrast to historical venue analyses based on estimated slippage. This\nallows for a dynamic and real-time control of the desired liquidity exposure.\n", "versions": [{"version": "v1", "created": "Tue, 17 Oct 2017 15:47:02 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Zovko", "Ilija I.", ""]]}]