[{"id": "1804.00003", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Alexander Sidorenko, David J. Thomson", "title": "Spectral Estimation of Plasma Fluctuations I: Comparison of Methods", "comments": "Missing Figures", "journal-ref": "Physics of Plasmas, Volume 1, Issue 3, March 1994, pp.485-500", "doi": "10.1063/1.870794", "report-no": null, "categories": "stat.AP eess.AS eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative root mean squared errors (RMSE) of nonparametric methods for\nspectral estimation is compared for microwave scattering data of plasma\nfluctuations. These methods reduce the variance of the periodogram estimate by\naveraging the spectrum over a frequency bandwidth. As the bandwidth increases,\nthe variance decreases, but the bias error increases. The plasma spectra vary\nby over four orders of magnitude, and therefore, using a spectral window is\nnecessary. We compare the smoothed tapered periodogram with the adaptive\nmultiple taper methods and hybrid methods. We find that a hybrid method, which\nuses four orthogonal tapers and then applies a kernel smoother, performs best.\nFor 300 point data segments, even an optimized smoothed tapered periodogram has\na 24 \\% larger relative RMSE than the hybrid method. We present two new\nadaptive multi-taper weightings which outperform Thomson's original adaptive\nweighting.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 19:54:45 GMT"}], "update_date": "2019-11-15", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "Alexander", ""], ["Thomson", "David J.", ""]]}, {"id": "1804.00049", "submitter": "Martin Dyrba", "authors": "Martin Dyrba, Reza Mohammadi, Michel J. Grothe, Thomas Kirste, Stefan\n  J. Teipel", "title": "Gaussian graphical models reveal inter-modal and inter-regional\n  conditional dependencies of brain alterations in Alzheimer's disease", "comments": "24 pages, 9 figures, 2 tables, supporting material", "journal-ref": null, "doi": "10.3389/fnagi.2020.00099", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) is characterized by a sequence of pathological\nchanges, which are commonly assessed in vivo using MRI and PET. Currently, the\nmost approaches to analyze statistical associations between brain regions rely\non Pearson correlation. However, these are prone to spurious correlations\narising from uninformative shared variance. Notably, there are no appropriate\nmultivariate statistical models available that can easily integrate dozens of\nvariables derived from such data, being able to use the additional information\nprovided from the combination of data sources. Gaussian graphical models (GGMs)\ncan estimate the conditional dependency from given data, which is expected to\nreflect the underlying causal relationships. We applied GGMs to assess\nmultimodal regional brain alterations in AD. We obtained data from N=972\nsubjects from the Alzheimer's Disease Neuroimaging Initiative. The mean amyloid\nload (AV45-PET), glucose metabolism (FDG-PET), and gray matter volume (MRI)\nwere calculated. GGMs were estimated using a Bayesian framework for the\ncombined multimodal data to obtain conditional dependency networks. Conditional\ndependency matrices were much sparser (10% density) than Pearson correlation\nmatrices (50% density). Within modalities, conditional dependency networks\nyielded clusters connecting anatomically adjacent regions. For associations\nbetween different modalities, only few region-specific connections remained.\nGraph-theoretical network statistics were significantly altered between groups,\nwith a biphasic u-shape trajectory. GGMs removed shared variance among\nmultimodal measures of regional brain alterations in MCI and AD, and yielded\nsparser matrices compared to Pearson correlation networks. Therefore, GGMs may\nbe used as alternative to thresholding-approaches typically applied to\ncorrelation networks to obtain the most informative relations between\nvariables.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 20:33:49 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2018 14:24:18 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 16:27:51 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Dyrba", "Martin", ""], ["Mohammadi", "Reza", ""], ["Grothe", "Michel J.", ""], ["Kirste", "Thomas", ""], ["Teipel", "Stefan J.", ""]]}, {"id": "1804.00160", "submitter": "Abhik Ghosh PhD", "authors": "Ayanendranath Basu, Abhik Ghosh, Abhijit Mandal, Nirian Martin,\n  Leandro Pardo", "title": "Robust Wald-type test in GLM with random design based on minimum density\n  power divergence estimators", "comments": "Pre-print, Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of robust inference under the generalized linear\nmodel (GLM) with stochastic covariates. We derive the properties of the minimum\ndensity power divergence estimator of the parameters in GLM with random design\nand use this estimator to propose robust Wald-type tests for testing any\ngeneral composite null hypothesis about the GLM. The asymptotic and robustness\nproperties of the proposed tests are also examined for the GLM with random\ndesign. Application of the proposed robust inference procedures to the popular\nPoisson regression model for analyzing count data is discussed in detail both\ntheoretically and numerically through simulation studies and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 11:24:35 GMT"}, {"version": "v2", "created": "Sun, 16 Jun 2019 21:23:07 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2020 17:49:58 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1804.00191", "submitter": "Jean-Philippe Ovarlez", "authors": "Emmanuelle Jay, Eug\\'enie Terreaux, Jean-Philippe Ovarlez and\n  Fr\\'ed\\'eric Pascal", "title": "Improving Portfolios Global Performance with Robust Covariance Matrix\n  Estimation: Application to the Maximum Variety Portfolio", "comments": "Submitted to 26th European Signal Processing Conference (EUSIPCO\n  2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents how the most recent improvements made on covariance\nmatrix estimation and model order selection can be applied to the portfolio\noptimisation problem. The particular case of the Maximum Variety Portfolio is\ntreated but the same improvements apply also in the other optimisation problems\nsuch as the Minimum Variance Portfolio. We assume that the most important\ninformation (or the latent factors) are embedded in correlated Elliptical\nSymmetric noise extending classical Gaussian assumptions. We propose here to\nfocus on a recent method of model order selection allowing to efficiently\nestimate the subspace of main factors describing the market. This non-standard\nmodel order selection problem is solved through Random Matrix Theory and robust\ncovariance matrix estimation. The proposed procedure will be explained through\nsynthetic data and be applied and compared with standard techniques on real\nmarket data showing promising improvements.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2018 16:32:31 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Jay", "Emmanuelle", ""], ["Terreaux", "Eug\u00e9nie", ""], ["Ovarlez", "Jean-Philippe", ""], ["Pascal", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1804.00240", "submitter": "Guy Kelman", "authors": "Guy Kelman, Eran Manes, Marco Lamieri, David Bre\\'e", "title": "Missing Data as Part of the Social Behavior in Real-World Financial\n  Complex Systems", "comments": "17 pages, 12 figures", "journal-ref": "Advances in Complex Systems, 25(1), 1850002 (2017)", "doi": "10.1142/S0219525918500029", "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world networks are known to exhibit facts that counter our\nknowledge prescribed by the theories on network creation and communication\npatterns. A common prerequisite in network analysis is that information on\nnodes and links will be complete because network topologies are extremely\nsensitive to missing information of this kind. Therefore, many real-world\nnetworks that fail to meet this criterion under random sampling may be\ndiscarded.\n  In this paper we offer a framework for interpreting the missing observations\nin network data under the hypothesis that these observations are not missing at\nrandom. We demonstrate the methodology with a case study of a financial trade\nnetwork, where the awareness of agents to the data collection procedure by a\nself-interested observer may result in strategic revealing or withholding of\ninformation. The non-random missingness has been overlooked despite the\npossibility of this being an important feature of the processes by which the\nnetwork is generated. The analysis demonstrates that strategic information\nwithholding may be a valid general phenomenon in complex systems. The evidence\nis sufficient to support the existence of an influential observer and to offer\na compelling dynamic mechanism for the creation of the network.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 01:31:25 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kelman", "Guy", ""], ["Manes", "Eran", ""], ["Lamieri", "Marco", ""], ["Bre\u00e9", "David", ""]]}, {"id": "1804.00327", "submitter": "Samuel Scarpino", "authors": "Samuel V. Scarpino, James G. Scott, Rosalind M. Eggo, Bruce Clements,\n  Nedialko B. Dimitrov, Lauren Ancel Meyers", "title": "Socioeconomic bias in influenza surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals in low socioeconomic brackets are considered at-risk for\ndeveloping influenza-related complications and often exhibit higher than\naverage influenza-related hospitalization rates. This disparity has been\nattributed to various factors, including restricted access to preventative and\ntherapeutic health care, limited sick leave, and household structure. Adequate\ninfluenza surveillance in these at-risk populations is a critical precursor to\naccurate risk assessments and effective intervention. However, the United\nStates of America's primary national influenza surveillance system (ILINet)\nmonitors outpatient healthcare providers, which may be largely inaccessible to\nlower socioeconomic populations. Recent initiatives to incorporate\ninternet-source and hospital electronic medical records data into surveillance\nsystems seek to improve the timeliness, coverage, and accuracy of outbreak\ndetection and situational awareness. Here, we use a flexible statistical\nframework for integrating multiple surveillance data sources to evaluate the\nadequacy of traditional (ILINet) and next generation (BioSense 2.0 and Google\nFlu Trends) data for situational awareness of influenza across poverty levels.\nWe find that zip codes in the highest poverty quartile are a critical\nblind-spot for ILINet that the integration of next generation data fails to\nameliorate.\n", "versions": [{"version": "v1", "created": "Sun, 1 Apr 2018 18:05:13 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Scarpino", "Samuel V.", ""], ["Scott", "James G.", ""], ["Eggo", "Rosalind M.", ""], ["Clements", "Bruce", ""], ["Dimitrov", "Nedialko B.", ""], ["Meyers", "Lauren Ancel", ""]]}, {"id": "1804.00636", "submitter": "Dionysios Kalogerias", "authors": "Dionysios S. Kalogerias, Warren B. Powell", "title": "Recursive Optimization of Convex Risk Measures: Mean-Semideviation\n  Models", "comments": "90 pages, 3 figures. Update: Substantial revision of the technical\n  content, with an additional fully detailed analysis in regard to the rate of\n  convergence of the MESSAGEp algorithm. NOTE: Please open in browser to see\n  the math in the abstract!", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop recursive, data-driven, stochastic subgradient methods for\noptimizing a new, versatile, and application-driven class of convex risk\nmeasures, termed here as mean-semideviations, strictly generalizing the\nwell-known and popular mean-upper-semideviation. We introduce the MESSAGEp\nalgorithm, which is an efficient compositional subgradient procedure for\niteratively solving convex mean-semideviation risk-averse problems to\noptimality. We analyze the asymptotic behavior of the MESSAGEp algorithm under\na flexible and structure-exploiting set of problem assumptions. In particular:\n1) Under appropriate stepsize rules, we establish pathwise convergence of the\nMESSAGEp algorithm in a strong technical sense, confirming its asymptotic\nconsistency. 2) Assuming a strongly convex cost, we show that, for fixed\nsemideviation order $p>1$ and for $\\epsilon\\in\\left[0,1\\right)$, the MESSAGEp\nalgorithm achieves a squared-${\\cal L}_{2}$ solution suboptimality rate of the\norder of ${\\cal O}(n^{-\\left(1-\\epsilon\\right)/2})$ iterations, where, for\n$\\epsilon>0$, pathwise convergence is simultaneously guaranteed. This result\nestablishes a rate of order arbitrarily close to ${\\cal O}(n^{-1/2})$, while\nensuring strongly stable pathwise operation. For $p\\equiv1$, the rate order\nimproves to ${\\cal O}(n^{-2/3})$, which also suffices for pathwise convergence,\nand matches previous results. 3) Likewise, in the general case of a convex\ncost, we show that, for any $\\epsilon\\in\\left[0,1\\right)$, the MESSAGEp\nalgorithm with iterate smoothing achieves an ${\\cal L}_{1}$ objective\nsuboptimality rate of the order of ${\\cal\nO}(n^{-\\left(1-\\epsilon\\right)/\\left(4\\bf{1}_{\\left\\{ p>1\\right\\} }+4\\right)})$\niterations. This result provides maximal rates of ${\\cal O}(n^{-1/4})$, if\n$p\\equiv1$, and ${\\cal O}(n^{-1/8})$, if $p>1$, matching the state of the art,\nas well.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 17:27:52 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 02:36:03 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 03:15:25 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 00:47:36 GMT"}, {"version": "v5", "created": "Mon, 29 Oct 2018 15:49:58 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kalogerias", "Dionysios S.", ""], ["Powell", "Warren B.", ""]]}, {"id": "1804.00735", "submitter": "Yan Wang", "authors": "Yan Wang, Nathan Palmer, Qian Di, Joel Schwartz, Isaac Kohane, Tianxi\n  Cai", "title": "A Fast Divide-and-Conquer Sparse Cox Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a computationally and statistically efficient divide-and-conquer\n(DAC) algorithm to fit sparse Cox regression to massive datasets where the\nsample size $n_0$ is exceedingly large and the covariate dimension $p$ is not\nsmall but $n_0\\gg p$. The proposed algorithm achieves computational efficiency\nthrough a one-step linear approximation followed by a least square\napproximation to the partial likelihood (PL). These sequences of linearization\nenable us to maximize the PL with only a small subset and perform penalized\nestimation via a fast approximation to the PL. The algorithm is applicable for\nthe analysis of both time-independent and time-dependent survival data.\nSimulations suggest that the proposed DAC algorithm substantially outperforms\nthe full sample-based estimators and the existing DAC algorithm with respect to\nthe computational speed, while it achieves similar statistical efficiency as\nthe full sample-based estimators. The proposed algorithm was applied to an\nextraordinarily large time-independent survival dataset and an extraordinarily\nlarge time-dependent survival dataset for the prediction of heart\nfailure-specific readmission within 30 days among Medicare heart failure\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 21:25:59 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Wang", "Yan", ""], ["Palmer", "Nathan", ""], ["Di", "Qian", ""], ["Schwartz", "Joel", ""], ["Kohane", "Isaac", ""], ["Cai", "Tianxi", ""]]}, {"id": "1804.00760", "submitter": "Javier Orlando Neira Rueda", "authors": "Javier Neira Rueda, Andres Carrion Garcia", "title": "Process Control with Highly Left Censored Data", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to monitor industrial processes, detecting changes in process\nparameters in order to promptly correct problems that may arise, generates a\nparticular area of interest. This is particularly critical and complex when the\nmeasured value falls below the sensitivity limits of the measuring system or\nbelow detection limits, causing much of their observations are incomplete. Such\nobservations to be called incomplete observations or left censored data. With a\nhigh level of censorship, for example greater than 70%, the application of\ntraditional methods for monitoring processes is not appropriate. It is required\nto use appropriate data analysis statistical techniques, to assess the actual\nstate of the process at any time. This paper proposes a way to estimate process\nparameters in such cases and presents the corresponding control chart, from an\nalgorithm that is also presented.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2018 23:28:08 GMT"}, {"version": "v2", "created": "Sun, 5 May 2019 20:01:05 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Rueda", "Javier Neira", ""], ["Garcia", "Andres Carrion", ""]]}, {"id": "1804.01182", "submitter": "Teng Huang", "authors": "Teng Huang, David Bergman, and Ram Gopal", "title": "Predictive and Prescriptive Analytics for Location Selection of Add-on\n  Retail Products", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study an analytical approach to selecting expansion\nlocations for retailers selling add-on products whose demand is derived from\nthe demand of another base product. Demand for the add-on product is realized\nonly as a supplement to the demand of the base product. In our context, either\nof the two products could be subject to spatial autocorrelation where demand at\na given location is impacted by demand at other locations. Using data from an\nindustrial partner selling add-on products, we build predictive models for\nunderstanding the derived demand of the add-on product and establish an\noptimization framework for automating expansion decisions to maximize expected\nsales. Interestingly, spatial autocorrelation and the complexity of the\npredictive model impact the complexity and the structure of the prescriptive\noptimization model. Our results indicate that the models formulated are highly\neffective in predicting add-on product sales, and that using the optimization\nframework built on the predictive model can result in substantial increases in\nexpected sales over baseline policies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:25:48 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Huang", "Teng", ""], ["Bergman", "David", ""], ["Gopal", "Ram", ""]]}, {"id": "1804.01185", "submitter": "Seojin Bang", "authors": "Haim Bar and Seojin Bang", "title": "A Mixture Model to Detect Edges in Sparse Co-expression Graphs", "comments": null, "journal-ref": "PLoS ONE 16(2): (2021) e0246945", "doi": "10.1371/journal.pone.0246945", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the early days of gene expression data, researchers have focused on\ngene-level analysis, and particularly on finding differentially expressed\ngenes. This usually involved making a simplifying assumption that genes are\nindependent, which made likelihood derivations feasible and allowed for\nrelatively simple implementations. In recent years, the scope has expanded to\ninclude pathway and `gene set' analysis in an attempt to understand the\nrelationships between genes. We develop a method to recover a gene network's\nstructure from co-expression data, measured in terms of normalized Pearson's\ncorrelation coefficients between gene pairs. We treat these co-expression\nmeasurements as weights in the complete graph in which nodes correspond to\ngenes. To decide which edges exist in the gene network, we fit a\nthree-component mixture model such that the observed weights of `null edges'\nfollow a normal distribution with mean 0, and the non-null edges follow a\nmixture of two lognormal distributions, one for positively- and one for\nnegatively-correlated pairs. We show that this so-called L2N mixture model\noutperforms other methods in terms of power to detect edges, and it allows us\nto control the false discovery rate. Importantly, the method makes no\nassumptions about the true network structure.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:36:57 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 19:10:37 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 20:23:18 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim", ""], ["Bang", "Seojin", ""]]}, {"id": "1804.01187", "submitter": "Amir Nikooienejad", "authors": "Amir Nikooienejad and Valen E. Johnson", "title": "On the Existence of Uniformly Most Powerful Bayesian Tests With\n  Application to Non-Central Chi-Squared Tests", "comments": "16 pages, 3 figures", "journal-ref": "Bayesian Analysis (2020)", "doi": "10.1214/19-ba1194", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniformly most powerful Bayesian tests (UMPBT's) are an objective class of\nBayesian hypothesis tests that can be considered the Bayesian counterpart of\nclassical uniformly most powerful tests. Because the rejection regions of\nUMPBT's can be matched to the rejection regions of classical uniformly most\npowerful tests (UMPTs), UMPBT's provide a mechanism for calibrating Bayesian\nevidence thresholds, Bayes factors, classical significance levels and p-values.\nThe purpose of this article is to expand the application of UMPBT's outside the\nclass of exponential family models. Specifically, we introduce sufficient\nconditions for the existence of UMPBT's and propose a unified approach for\ntheir derivation. An important application of our methodology is the extension\nof UMPBT's to testing whether the non-centrality parameter of a chi-squared\ndistribution is zero. The resulting tests have broad applicability, providing\ndefault alternative hypotheses to compute Bayes factors in, for example,\nPearson's chi-squared test for goodness-of-fit, tests of independence in\ncontingency tables, and likelihood ratio, score and Wald tests.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 22:39:57 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 02:54:08 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Nikooienejad", "Amir", ""], ["Johnson", "Valen E.", ""]]}, {"id": "1804.01203", "submitter": "Min Xu", "authors": "Yixiu Zhao, Xiangrui Zeng, Qiang Guo, Min Xu", "title": "An integration of fast alignment and maximum-likelihood methods for\n  electron subtomogram averaging and classification", "comments": "17 pages", "journal-ref": "Intelligent Systems for Molecular Biology (ISMB) 2018,\n  Bioinformatics", "doi": "10.1093/bioinformatics/bty267", "report-no": null, "categories": "q-bio.QM cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Cellular Electron CryoTomography (CECT) is an emerging 3D imaging\ntechnique that visualizes subcellular organization of single cells at\nsubmolecular resolution and in near-native state. CECT captures large numbers\nof macromolecular complexes of highly diverse structures and abundances.\nHowever, the structural complexity and imaging limits complicate the systematic\nde novo structural recovery and recognition of these macromolecular complexes.\nEfficient and accurate reference-free subtomogram averaging and classification\nrepresent the most critical tasks for such analysis. Existing subtomogram\nalignment based methods are prone to the missing wedge effects and low\nsignal-to-noise ratio (SNR). Moreover, existing maximum-likelihood based\nmethods rely on integration operations, which are in principle computationally\ninfeasible for accurate calculation.\n  Results: Built on existing works, we propose an integrated method, Fast\nAlignment Maximum Likelihood method (FAML), which uses fast subtomogram\nalignment to sample sub-optimal rigid transformations. The transformations are\nthen used to approximate integrals for maximum-likelihood update of subtomogram\naverages through expectation-maximization algorithm. Our tests on simulated and\nexperimental subtomograms showed that, compared to our previously developed\nfast alignment method (FA), FAML is significantly more robust to noise and\nmissing wedge effects with moderate increases of computation cost.Besides, FAML\nperforms well with significantly fewer input subtomograms when the FA method\nfails. Therefore, FAML can serve as a key component for improved construction\nof initial structural models from macromolecules captured by CECT.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 01:16:20 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Zhao", "Yixiu", ""], ["Zeng", "Xiangrui", ""], ["Guo", "Qiang", ""], ["Xu", "Min", ""]]}, {"id": "1804.01269", "submitter": "Debasis Kundu Professor", "authors": "Rhythm Grover and Debasis Kundu and Amit Mitra", "title": "On approximate least squares estimators of parameters on one-dimensional\n  chirp signal", "comments": "Going to appear in Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chirp signals are quite common in many natural and man-made systems like\naudio signals, sonar, radar etc. Estimation of the unknown parameters of a\nsignal is a fundamental problem in statistical signal processing. Recently,\nKundu and Nandi \\cite{2008} studied the asymptotic properties of least squares\nestimators of the unknown parameters of a simple chirp signal model under the\nassumption of stationary noise. In this paper, we propose periodogram-type\nestimators called the approximate least squares estimators to estimate the\nunknown parameters and study the asymptotic properties of these estimators\nunder the same error assumptions. It is observed that the approximate least\nsquares estimators are strongly consistent and asymptotically equivalent to the\nleast squares estimators. Similar to the periodogram estimators, these\nestimators can also be used as initial guesses to find the least squares\nestimators of the unknown parameters. We perform some numerical simulations to\nsee the performance of the proposed estimators and compare them with the least\nsquares estimators and the estimators proposed by Lahiri et al., \\cite{2013}.\nWe have analysed two real data sets for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 07:30:05 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Grover", "Rhythm", ""], ["Kundu", "Debasis", ""], ["Mitra", "Amit", ""]]}, {"id": "1804.01367", "submitter": "Juraj Hledik", "authors": "Juraj Hledik and Riccardo Rastelli", "title": "A dynamic network model to measure exposure diversification in the\n  Austrian interbank market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical model for weighted temporal networks capable of\nmeasuring the level of heterogeneity in a financial system. Our model focuses\non the level of diversification of financial institutions; that is, whether\nthey are more inclined to distribute their assets equally among partners, or if\nthey rather concentrate their commitment towards a limited number of\ninstitutions. Crucially, a Markov property is introduced to capture time\ndependencies and to make our measures comparable across time. We apply the\nmodel on an original dataset of Austrian interbank exposures. The temporal span\nencompasses the onset and development of the financial crisis in 2008 as well\nas the beginnings of European sovereign debt crisis in 2011. Our analysis\nhighlights an overall increasing trend for network homogeneity, whereby core\nbanks have a tendency to distribute their market exposures more equally across\ntheir partners.\n", "versions": [{"version": "v1", "created": "Wed, 4 Apr 2018 12:33:37 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 09:05:45 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Hledik", "Juraj", ""], ["Rastelli", "Riccardo", ""]]}, {"id": "1804.01777", "submitter": "Yuan Zeng", "authors": "Yuan Zeng, Miao Luo, Yuzhong Liu", "title": "Future Energy Consumption Prediction Based on Grey Forecast Model", "comments": "25 pages,21 figurea", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We use grey forecast model to predict the future energy consumption of four\nstates in the U.S, and make some improvments to the model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 11:01:05 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Zeng", "Yuan", ""], ["Luo", "Miao", ""], ["Liu", "Yuzhong", ""]]}, {"id": "1804.01807", "submitter": "Sean van der Merwe", "authors": "Sean van der Merwe, Darren Steven, Martinette Pretorius", "title": "Bayesian Extreme Value Analysis of Stock Exchange Data", "comments": "11 pages, 5 figures, presented at the 2013 conference of the\n  Economics Society of South Africa", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Solvency II Directive and Solvency Assessment and Management (the South\nAfrican equivalent) give a Solvency Capital Requirement which is based on a\n99.5% Value-at-Risk (VaR) calculation. This calculation involves aggregating\nindividual risks. When considering log returns of financial instruments,\nespecially with share prices, there are extreme losses that are observed from\ntime to time that often do not fit whatever model is proposed for the regular\ntrading behaviour. The problem of accurately modelling these extreme losses is\naddressed, which, in turn, assists with the calculation of tail probabilities\nsuch as the 99.5% VaR. The focus is on the fitting of the Generalized Pareto\nDistribution (GPD) beyond a threshold. We show how objective Bayes methods can\nimprove parameter estimation and the calculation of risk measures. Lastly we\nconsider the choice of threshold. All aspects are illustrated using share\nlosses on the Johannesburg Stock Exchange (JSE).\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 12:33:40 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["van der Merwe", "Sean", ""], ["Steven", "Darren", ""], ["Pretorius", "Martinette", ""]]}, {"id": "1804.01809", "submitter": "Sean van der Merwe", "authors": "Sean van der Merwe", "title": "Time Series Analysis of the Southern Oscillation Index using Bayesian\n  Additive Regression Trees", "comments": "9 pages. Part of a department collection from 2009", "journal-ref": null, "doi": null, "report-no": "UFS Department of Mathematical Statistics and Actuarial Science 396", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian additive regression trees (BART) is a regression technique developed\nby Chipman et al. (2008). Its usefulness in standard regression settings has\nbeen clearly demonstrated, but it has not been applied to time series analysis\nas yet. We discuss the difficulties in applying this technique to time series\nanalysis and demonstrate its superior predictive capabilities in the case of a\nwell know time series: the Southern Oscillation Index.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 12:39:24 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["van der Merwe", "Sean", ""]]}, {"id": "1804.01955", "submitter": "Przemyslaw Biecek", "authors": "Mateusz Staniak and Przemyslaw Biecek", "title": "Explanations of model predictions with live and breakDown packages", "comments": null, "journal-ref": "The R Journal (2018), 10 (2) p. 395-409", "doi": "10.32614/RJ-2018-072", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex models are commonly used in predictive modeling. In this paper we\npresent R packages that can be used to explain predictions from complex black\nbox models and attribute parts of these predictions to input features. We\nintroduce two new approaches and corresponding packages for such attribution,\nnamely live and breakDown. We also compare their results with existing\nimplementations of state of the art solutions, namely lime that implements\nLocally Interpretable Model-agnostic Explanations and ShapleyR that implements\nShapley values.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:05:15 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 19:49:35 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Staniak", "Mateusz", ""], ["Biecek", "Przemyslaw", ""]]}, {"id": "1804.01957", "submitter": "Hon Keung Tony Ng", "authors": "Indranil Ghosh, Hon Keung Tony Ng", "title": "A Class of Skewed Distributions with Applications in Environmental Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In environmental studies, many data are typically skewed and it is desired to\nhave a flexible statistical model for this kind of data. In this paper, we\nstudy a class of skewed distributions by invoking arguments as described by\nFerreira and Steel (2006, Journal of the American Statistical Association, 101:\n823--829). In particular, we consider using the logistic kernel to derive a\nclass of univariate distribution called the truncated-logistic skew symmetric\n(TLSS) distribution. We provide some structural properties of the proposed\ndistribution and develop the statistical inference for the TLSS distribution. A\nsimulation study is conducted to investigate the efficacy of the maximum\nlikelihood method. For illustrative purposes, two real data sets from\nenvironmental studies are used to exhibit the applicability of such a model.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2018 17:11:19 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Ghosh", "Indranil", ""], ["Ng", "Hon Keung Tony", ""]]}, {"id": "1804.02084", "submitter": "Dimitrios Berberidis", "authors": "Donghoon Lee, Dimitris Berberidis, Georgios B. Giannakis", "title": "Adaptive Bayesian Radio Tomography", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2899806", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio tomographic imaging (RTI) is an emerging technology to locate physical\nobjects in a geographical area covered by wireless networks. From the\nattenuation measurements collected at spatially distributed sensors, radio\ntomography capitalizes on spatial loss fields (SLFs) measuring the absorption\nof radio frequency waves at each location along the propagation path. These\nSLFs can be utilized for interference management in wireless communication\nnetworks, environmental monitoring, and survivor localization after natural\ndisaster such as earthquakes. Key to success of RTI is to model accurately the\nshadowing effects as the bi-dimensional integral of the SLF scaled by a weight\nfunction, which is estimated using regularized regression. However, the\nexisting approaches are less effective when the propagation environment is\nheterogeneous. To cope with this, the present work introduces a piecewise\nhomogeneous SLF governed by a hidden Markov random field (MRF) model. Efficient\nand tractable SLF estimators are developed by leveraging Markov chain Monte\nCarlo (MCMC) techniques. Furthermore, an uncertainty sampling method is\ndeveloped to adaptively collect informative measurements in estimating the SLF.\nNumerical tests using synthetic and real datasets demonstrate capabilities of\nthe proposed algorithm for radio tomography and channel-gain estimation.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:01:44 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Lee", "Donghoon", ""], ["Berberidis", "Dimitris", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1804.02090", "submitter": "Maria DeYoreo", "authors": "Carolyn Rutter, Jonathan Ozik, Maria DeYoreo, Nicholson Collier", "title": "Microsimulation Model Calibration using Incremental Mixture Approximate\n  Bayesian Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microsimulation models (MSMs) are used to predict population-level effects of\nhealth care policies by simulating individual-level outcomes. Simulated\noutcomes are governed by unknown parameters that are chosen so that the model\naccurately predicts specific targets, a process referred to as model\ncalibration. Calibration targets can come from randomized controlled trials,\nobservational studies, and expert opinion, and are typically summary\nstatistics. A well calibrated model can reproduce a wide range of targets. MSM\ncalibration generally involves searching a high dimensional parameter space and\npredicting many targets through model simulation. This requires efficient\nmethods for exploring the parameter space and sufficient computational\nresources. We develop Incremental Mixture Approximate Bayesian Computation\n(IMABC) as a method for MSM calibration and implement it via a high-performance\ncomputing workflow, which provides the necessary computational scale. IMABC\nbegins with a rejection-based approximate Bayesian computation (ABC) step,\ndrawing a sample of parameters from the prior distribution and simulating\ncalibration targets. Next, the sample is iteratively updated by drawing\nadditional points from a mixture of multivariate normal distributions, centered\nat the points that yield simulated targets that are near observed targets.\nPosterior estimates are obtained by weighting sampled parameter vectors to\naccount for the adaptive sampling scheme. We demonstrate IMABC by calibrating a\nMSM for the natural history of colorectal cancer to obtain simulated draws from\nthe joint posterior distribution of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 00:31:11 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 20:10:36 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 16:40:48 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Rutter", "Carolyn", ""], ["Ozik", "Jonathan", ""], ["DeYoreo", "Maria", ""], ["Collier", "Nicholson", ""]]}, {"id": "1804.02097", "submitter": "Luwan Zhang", "authors": "Luwan Zhang, Katherine Liao, Issac Kohane, Tianxi Cai", "title": "Multi-view Banded Spectral Clustering with Application to ICD9\n  Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent development in methodology, community detection remains a\nchallenging problem. Existing literature largely focuses on the standard\nsetting where a network is learned using an observed adjacency matrix from a\nsingle data source. Constructing a shared network from multiple data sources is\nmore challenging due to the heterogeneity across populations. Additionally, no\nexisting method leverages the prior distance knowledge available in many\ndomains to help the discovery of the network structure. To bridge this gap, in\nthis paper we propose a novel spectral clustering method that optimally\ncombines multiple data sources while leveraging the prior distance knowledge.\nThe proposed method combines a banding step guided by the distance knowledge\nwith a subsequent weighting step to maximize consensus across multiple sources.\nIts statistical performance is thoroughly studied under a multi-view stochastic\nblock model. We also provide a simple yet optimal rule of choosing weights in\npractice. The efficacy and robustness of the method is fully demonstrated\nthrough extensive simulations. Finally, we apply the method to cluster the\nInternational classification of diseases, ninth revision (ICD9), codes and\nyield a very insightful clustering structure by integrating information from a\nlarge claim database and two healthcare systems.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 01:02:25 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 15:03:05 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Zhang", "Luwan", ""], ["Liao", "Katherine", ""], ["Kohane", "Issac", ""], ["Cai", "Tianxi", ""]]}, {"id": "1804.02334", "submitter": "Grigorios Papageorgiou", "authors": "Grigorios Papageorgiou, Mostafa M. Mokhles, Johanna J. M. Takkenberg\n  and Dimitris Rizopoulos", "title": "Individualized Dynamic Prediction of Survival under Time-Varying\n  Treatment Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often in follow-up studies intermediate events occur in some patients, such\nas reinterventions or adverse events. These intermediate events directly affect\nthe shapes of their longitudinal profiles. Our work is motivated by two studies\nin which such intermediate events have been recorded during follow-up. The\nfirst study concerns Congenital Heart Diseased patients who were followed-up\nechocardiographically, with several patients undergoing reintervention. The\nsecond study concerns patients who participated in the SPRINT study and\nexperienced adverse events during follow-up. We are interested in the change of\nthe longitudinal profiles after the occurrence of the intermediate event and in\nutilizing this information to improve the accuracy of the dynamic prediction\nfor their risk.\n  To achieve this, we propose a flexible joint modeling framework for the\nlongitudinal and survival data that includes the intermediate event as a\ntime-varying binary covariate in both the longitudinal and survival submodels.\nWe consider a set of joint models that postulate different effects of the\nintermediate event in the longitudinal profile and the risk of the clinical\nendpoint, with different formulations for their association while allowing its\nparametrization to change after the occurrence of the intermediate event. Based\non these models we derive dynamic predictions of conditional survival\nprobabilities which are adaptive to different scenarios with respect to the\noccurrence of the intermediate event. We evaluate the accuracy of these\npredictions with a simulation study using the time-dependent area under the\nreceiver operating characteristic curve and the expected prediction error\nadjusted to our setting. The results suggest that accounting for the changes in\nthe longitudinal profiles and the instantaneous risk for the clinical endpoint\nis important, and improves the accuracy of the dynamic predictions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 16:03:14 GMT"}], "update_date": "2018-04-09", "authors_parsed": [["Papageorgiou", "Grigorios", ""], ["Mokhles", "Mostafa M.", ""], ["Takkenberg", "Johanna J. M.", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1804.02608", "submitter": "Tauhid Zaman", "authors": "Fanyu Que, Krishnan Rajagopalan, Tauhid Zaman", "title": "Penetrating a Social Network: The Follow-back Problem", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern threats have emerged from the prevalence of social networks. Hostile\nactors, such as extremist groups or foreign governments, utilize these networks\nto run propaganda campaigns with different aims. For extremists, these\ncampaigns are designed for recruiting new members or inciting violence. For\nforeign governments, the aim may be to create instability in rival nations.\nProper social network counter-measures are needed to combat these threats. Here\nwe present one important counter-measure: penetrating social networks. This\nmeans making target users connect with or follow agents deployed in the social\nnetwork. Once such connections are established with the targets, the agents can\ninfluence them by sharing content which counters the influence campaign. In\nthis work we study how to penetrate a social network, which we call the\nfollow-back problem. The goal here is to find a policy that maximizes the\nnumber of targets that follow the agent.\n  We conduct an empirical study to understand what behavioral and network\nfeatures affect the probability of a target following an agent. We find that\nthe degree of the target and the size of the mutual neighborhood of the agent\nand target in the network affect this probability. Based on our empirical\nfindings, we then propose a model for targets following an agent. Using this\nmodel, we solve the follow-back problem exactly on directed acyclic graphs and\nderive a closed form expression for the expected number of follows an agent\nreceives under the optimal policy. We then formulate the follow-back problem on\nan arbitrary graph as an integer program. To evaluate our integer program based\npolicies, we conduct simulations on real social network topologies in Twitter.\nWe find that our polices result in more effective network penetration, with\nsignificant increases in the expected number of targets that follow the agent.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 00:58:51 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Que", "Fanyu", ""], ["Rajagopalan", "Krishnan", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1804.02737", "submitter": "Jacob Rhyne", "authors": "Jacob Rhyne, Jung-Ying Tzeng, Teng Zhang, and X. Jessie Jeng", "title": "eQTL Mapping via Effective SNP Ranking and Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide eQTL mapping explores the relationship between gene expression\nvalues and DNA variants to understand genetic causes of human disease. Due to\nthe large number of genes and DNA variants that need to be assessed\nsimultaneously, current methods for eQTL mapping often suffer from low\ndetection power, especially for identifying trans-eQTLs. In this paper, we\npropose a new method that utilizes advanced techniques in large-scale signal\ndetection to pursue the structure of eQTL data and improve the power for eQTL\nmapping. The new method greatly reduces the burden of joint modeling by\ndeveloping a new ranking and screening strategy based on the higher criticism\nstatistic. Numerical results in simulation studies demonstrate the superior\nperformance of our method in detecting true eQTLs with reduced computational\nexpense. The proposed method is also evaluated in HapMap eQTL data analysis and\nthe results are compared to a database of known eQTLs.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 18:48:18 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Rhyne", "Jacob", ""], ["Tzeng", "Jung-Ying", ""], ["Zhang", "Teng", ""], ["Jeng", "X. Jessie", ""]]}, {"id": "1804.02742", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta and Zacharias Faidon Brotzakis and Antonietta Mira", "title": "Bayesian Calibration of Force-fields from Experimental Data: TIP4P Water", "comments": null, "journal-ref": null, "doi": "10.1063/1.5030950", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Molecular dynamics (MD) simulations give access to equilibrium structures and\ndynamic properties given an ergodic sampling and an accurate force-field. The\nforce-field parameters are calibrated to reproduce properties measured by\nexperiments or simulations. The main contribution of this paper is an\napproximate Bayesian framework for the calibration and uncertainty\nquantification of the force-field parameters, without assuming parameter\nuncertainty to be Gaussian. To this aim, since the likelihood function of the\nMD simulation models are intractable in absence of Gaussianity assumption, we\nuse a likelihood-free inference scheme known as approximate Bayesian\ncomputation (ABC) and propose an adaptive population Monte Carlo ABC algorithm,\nwhich is illustrated to converge faster and scales better than previously used\nABCsubsim algorithm for calibration of force-field of a helium system. The\nsecond contribution is the adaptation of ABC algorithms for High Performance\nComputing to MD simulation within the Python ecosystem ABCpy. We illustrate the\nperformance of the developed methodology to learn posterior distribution and\nBayesian estimates of Lennard-Jones force-field parameters of helium and TIP4P\nsystem of water implemented both for simulated and experimental datasets\ncollected using Neutron and X-ray diffraction. For simulated data, the Bayesian\nestimate is in close agreement with the true parameter value used to generate\nthe dataset. For experimental as well as for simulated data, the Bayesian\nposterior distribution shows a strong correlation pattern between the\nforce-field parameters. Providing an estimate of the entire posterior\ndistribution, our methodology also allows us to perform uncertainty\nquantification of model prediction. This research opens up the possibility to\nrigorously calibrate force-fields from available experimental datasets of any\nstructural and dynamic property.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 19:22:06 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2018 11:19:28 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Brotzakis", "Zacharias Faidon", ""], ["Mira", "Antonietta", ""]]}, {"id": "1804.02773", "submitter": "Attila Varga", "authors": "Attila Varga", "title": "Novelty and Foreseeing Research Trends; The Case of Astrophysics and\n  Astronomy", "comments": null, "journal-ref": null, "doi": "10.3847/1538-4365/aab765", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics based on reference lists of research articles or on keywords have\nbeen used to predict citation impact. The concept behind such metrics is that\noriginal ideas stem from the reconfiguration of the structure of past\nknowledge, and therefore atypical combinations in the reference lists,\nkeywords, or classification codes indicate future high impact research. The\ncurrent paper serves as an introduction to this line of research for\nastronomers and also addresses some methodological questions of this field of\ninnovation studies. It is still not clear if the choice of particular indexes,\nsuch as references to journals, articles, or specific bibliometric\nclassification codes would affect the relationship between atypical\ncombinations and citation impact. To understand more aspects of the innovation\nprocess, a new metric has been devised to measure to what extent researchers\nare able to anticipate the changing combinatorial trends of the future. Results\nshow that the variant of the latter anticipation scores that is based on paper\ncombinations is a good predictor of future citation impact of scholarly works.\nThe study also shows that the effect of tested indexes vary with the\naggregation level that was used to construct them. A detailed analysis of\ncombinatorial novelty in the field reveals that certain sub-fields of astronomy\nand astrophysics have different roles in the reconfiguration in past knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 8 Apr 2018 23:03:01 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Varga", "Attila", ""]]}, {"id": "1804.02955", "submitter": "Stephen Haben", "authors": "Stephen Haben, Georgios Giasemidis, Florian Ziel and Siddharth Arora", "title": "Short Term Load Forecasts of Low Voltage Demand and the Effects of\n  Weather", "comments": null, "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1469-1484", "doi": "10.1016/j.ijforecast.2018.10.007", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short term load forecasts will play a key role in the implementation of smart\nelectricity grids. They are required to optimise a wide range of potential\nnetwork solutions on the low voltage (LV) grid, including integrating low\ncarbon technologies (such as photovoltaics) and utilising battery storage\ndevices. Despite the need for accurate LV level load forecasts, previous\nstudies have mostly focused on forecasting at the individual household or\nbuilding level using data from smart meters. In this study we provide detailed\nanalysis of a variety of methods in terms of both point and probabilistic\nforecasting accuracy using data from 100 real LV feeders. Moreover, we\ninvestigate the effect of temperature (both actual and forecasts) on the\naccuracy of load forecasts. We present some important results on the drivers of\nLV forecasting accuracy that are crucial for the management of LV networks,\nalong with an empirical comparison of forecast measures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 13:44:18 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Haben", "Stephen", ""], ["Giasemidis", "Georgios", ""], ["Ziel", "Florian", ""], ["Arora", "Siddharth", ""]]}, {"id": "1804.03077", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "A plug-in approach to maximising precision at the top and recall at the\n  top", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For information retrieval and binary classification, we show that precision\nat the top (or precision at k) and recall at the top (or recall at k) are\nmaximised by thresholding the posterior probability of the positive class. This\nfinding is a consequence of a result on constrained minimisation of the\ncost-sensitive expected classification error which generalises an earlier\nrelated result from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 16:10:45 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1804.03185", "submitter": "Anders Eklund", "authors": "Anders Eklund, Hans Knutsson, Thomas E Nichols", "title": "Cluster Failure Revisited: Impact of First Level Design and Data Quality\n  on Cluster False Positive Rates", "comments": null, "journal-ref": "Human Brain Mapping, 2018", "doi": "10.1002/hbm.24350", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Methodological research rarely generates a broad interest, yet our work on\nthe validity of cluster inference methods for functional magnetic resonance\nimaging (fMRI) created intense discussion on both the minutia of our approach\nand its implications for the discipline. In the present work, we take on\nvarious critiques of our work and further explore the limitations of our\noriginal work. We address issues about the particular event-related designs we\nused, considering multiple event types and randomisation of events between\nsubjects. We consider the lack of validity found with one-sample permutation\n(sign flipping) tests, investigating a number of approaches to improve the\nfalse positive control of this widely used procedure. We found that the\ncombination of a two-sided test and cleaning the data using ICA FIX resulted in\nnominal false positive rates for all datasets, meaning that data cleaning is\nnot only important for resting state fMRI, but also for task fMRI. Finally, we\ndiscuss the implications of our work on the fMRI literature as a whole,\nestimating that at least 10% of the fMRI studies have used the most problematic\ncluster inference method (P = 0.01 cluster defining threshold), and how\nindividual studies can be interpreted in light of our findings. These\nadditional results underscore our original conclusions, on the importance of\ndata sharing and thorough evaluation of statistical methods on realistic null\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 19:00:40 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 04:09:54 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Eklund", "Anders", ""], ["Knutsson", "Hans", ""], ["Nichols", "Thomas E", ""]]}, {"id": "1804.03521", "submitter": "Pierre Pinson", "authors": "Etienne Sorin, Lucien Bobo, Pierre Pinson", "title": "Consensus-based approach to peer-to-peer electricity markets with\n  product differentiation", "comments": "Accepted for publication in IEEE Transactions on Power Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the sustained deployment of distributed generation capacities and the\nmore proactive role of consumers, power systems and their operation are\ndrifting away from a conventional top-down hierarchical structure. Electricity\nmarket structures, however, have not yet embraced that evolution. Respecting\nthe high-dimensional, distributed and dynamic nature of modern power systems\nwould translate to designing peer-to-peer markets or, at least, to using such\nan underlying decentralized structure to enable a bottom-up approach to future\nelectricity markets. A peer-to-peer market structure based on a Multi-Bilateral\nEconomic Dispatch (MBED) formulation is introduced, allowing for\nmulti-bilateral trading with product differentiation, for instance based on\nconsumer preferences. A Relaxed Consensus+Innovation (RCI) approach is\ndescribed to solve the MBED in fully decentralized manner. A set of realistic\ncase studies and their analysis allow us showing that such peer-to-peer market\nstructures can effectively yield market outcomes that are different from\ncentralized market structures and optimal in terms of respecting consumers\npreferences while maximizing social welfare. Additionally, the RCI solving\napproach allows for a fully decentralized market clearing which converges with\na negligible optimality gap, with a limited amount of information being shared.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2018 16:28:37 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 17:07:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Sorin", "Etienne", ""], ["Bobo", "Lucien", ""], ["Pinson", "Pierre", ""]]}, {"id": "1804.03707", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Yi Huang, and Ishanu Chattopadhyay", "title": "A Tamper-Free Semi-Universal Communication System for Deletion Channels", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reliable communication between two legitimate\nparties over deletion channels under an active eavesdropping (aka jamming)\nadversarial model. To this goal, we develop a theoretical framework based on\nprobabilistic finite-state automata to define novel encoding and decoding\nschemes that ensure small error probability in both message decoding as well as\ntamper detecting. We then experimentally verify the reliability and\ntamper-detection property of our scheme.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2018 16:13:33 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Huang", "Yi", ""], ["Chattopadhyay", "Ishanu", ""]]}, {"id": "1804.03981", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Compressive Regularized Discriminant Analysis of High-Dimensional Data\n  with Applications to Microarray Studies", "comments": "5 pages, 2018 IEEE International Conference on Acoustics, Speech and\n  Signal Processing 15-20 April 2018 | Calgary, Alberta, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a modification of linear discriminant analysis, referred to as\ncompressive regularized discriminant analysis (CRDA), for analysis of\nhigh-dimensional datasets. CRDA is specially designed for feature elimination\npurpose and can be used as gene selection method in microarray studies. CRDA\nlends ideas from $\\ell_{q,1}$ norm minimization algorithms in the multiple\nmeasurement vectors (MMV) model and utilizes joint-sparsity promoting hard\nthresholding for feature elimination. A regularization of the sample covariance\nmatrix is also needed as we consider the challenging scenario where the number\nof features (variables) is comparable or exceeding the sample size of the\ntraining dataset. A simulation study and four examples of real-life microarray\ndatasets evaluate the performances of CRDA based classifiers. Overall, the\nproposed method gives fewer misclassification errors than its competitors,\nwhile at the same time achieving accurate feature selection.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 13:48:56 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1804.04249", "submitter": "Bowei Xi", "authors": "Lin-Yang Cheng, Bowei Xi", "title": "A Likelihood Ratio Approach for Precise Discovery of Truly Relevant\n  Protein Markers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of biomarker discovery is typically lengthy and costly, involving\nthe phases of discovery, qualification, verification, and validation before\nclinical evaluation. Being able to efficiently identify the truly relevant\nmarkers in discovery studies can significantly simplify the process. However,\nin discovery studies the sample size is typically small while the number of\nmarkers being explored is much larger. Hence discovery studies suffer from\nsparsity and high dimensionality issues. Currently the state-of-the-art methods\neither find too many false positives or fail to identify many truly relevant\nmarkers. In this paper we develop a likelihood ratio-based approach and aim for\naccurately finding the truly relevant protein markers in discovery studies. Our\nmethod fits especially well with discovery studies because they are mostly\nbalanced design due to the fact that experiments are limited and controlled.\nOur approach is based on the observation that the underlying distributions of\nexpression profiles are unimodal for those irrelevant plain markers. Our method\nhas asymptotic chi-square null distribution which facilitates the efficient\ncontrol of false discovery rate. We then evaluate our method using both\nsimulated and real experimental data. In all the experiments our method is\nhighly effective to discover the set of truly relevant markers, leading to\naccurate biomarker identifications with high sensitivity and low empirical\nfalse discovery rate.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2018 22:34:02 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Cheng", "Lin-Yang", ""], ["Xi", "Bowei", ""]]}, {"id": "1804.04541", "submitter": "Matei Tene", "authors": "Matei Tene, Dana E. Stuparu, Dorota Kurowicka, Ghada Y. El Serafy", "title": "A copula-based sensitivity analysis method and its application to a\n  North Sea sediment transport model", "comments": null, "journal-ref": "Environmental Modelling & Software, Volume 104, June 2018, Pages\n  1-12", "doi": "10.1016/j.envsoft.2018.03.002", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a novel sensitivity analysis method, able to handle\ndependency relationships between model parameters. The starting point is the\npopular Morris (1991) algorithm, which was initially devised under the\nassumption of parameter independence. This important limitation is tackled by\nallowing the user to incorporate dependency information through a copula. The\nset of model runs obtained using latin hypercube sampling, are then used for\nderiving appropriate sensitivity measures.\n  Delft3D-WAQ (Deltares, 2010) is a sediment transport model with strong\ncorrelations between input parameters. Despite this, the parameter ranking\nobtained with the newly proposed method is in accordance with the knowledge\nobtained from expert judgment. However, under the same conditions, the classic\nMorris method elicits its results from model runs which break the assumptions\nof the underlying physical processes. This leads to the conclusion that the\nproposed extension is superior to the classic Morris algorithm and can\naccommodate a wide range of use cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 12:10:02 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Tene", "Matei", ""], ["Stuparu", "Dana E.", ""], ["Kurowicka", "Dorota", ""], ["Serafy", "Ghada Y. El", ""]]}, {"id": "1804.04568", "submitter": "Juliano Neves", "authors": "Juliano C. S. Neves", "title": "A fuzzy process of individuation", "comments": "12 pages, 1 table. V2 with minor changes. Published in The Journal of\n  Mathematical Sociology", "journal-ref": "J. Math. Sociol. 44 (2), 90-98 (2020)", "doi": "10.1080/0022250X.2019.1652908", "report-no": null, "categories": "physics.soc-ph math.HO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that an aspect of the process of individuation may be thought of\nas a fuzzy set. The process of individuation has been interpreted as a\ntwo-valued problem in the history of philosophy. In this work, I intend to show\nthat such a process in its psychosocial aspect is better understood in terms of\na fuzzy set, characterized by a continuum membership function. According to\nthis perspective, species and their members present different degrees of\nindividuation. Such degrees are measured from the membership function of the\npsychosocial process of individuation. Thus, a social analysis is suggested by\nusing this approach in human societies.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 15:25:15 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 17:15:22 GMT"}], "update_date": "2020-03-11", "authors_parsed": [["Neves", "Juliano C. S.", ""]]}, {"id": "1804.04586", "submitter": "Yili Hong", "authors": "Caleb King and Zhibing Xu and I-Chen Lee and Yili Hong", "title": "Reliability Analysis of Polymeric Materials", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polymeric materials are widely used in many applications and are especially\nuseful when combined with other polymers to make polymer composites. The\nappealing features of these materials come from their having comparable levels\nof strength and endurance to what one would find in metal alloys while being\nmore lightweight and economical. However, these materials are still susceptible\nto degradation over time and so it is of great importance to manufacturers to\nassess their product's lifetime. Because these materials are meant to last over\na span of several years or even decades, accelerated testing is often the\nmethod of choice in assessing product lifetimes in a more feasible time frame.\nIn this article, a brief introduction is given to the methods of accelerated\ntesting and analysis used with polymer materials. Special attention is given to\ndegradation testing and modeling due to the growing popularity of these\ntechniques along with a brief discussion of fatigue testing. References are\nprovided for further reading in each of these areas.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 12:45:35 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["King", "Caleb", ""], ["Xu", "Zhibing", ""], ["Lee", "I-Chen", ""], ["Hong", "Yili", ""]]}, {"id": "1804.04587", "submitter": "William Artman", "authors": "William J. Artman, Inbal Nahum-Shani, Tianshuang Wu, James R. McKay,\n  Ashkan Ertefaie", "title": "Power Analysis in a SMART Design: Sample Size Estimation for Determining\n  the Best Dynamic Treatment Regime", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential, multiple assignment, randomized trial (SMART) designs have become\nincreasingly popular in the field of precision medicine by providing a means\nfor comparing sequences of treatments tailored to the individual patient, i.e.,\ndynamic treatment regime (DTR). The construction of evidence-based DTRs\npromises a replacement to adhoc one-size-fits-all decisions pervasive in\npatient care. However, there are substantial statistical challenges in sizing\nSMART designs due to the complex correlation structure between the DTRs\nembedded in the design. Since the primary goal of SMARTs is the construction of\nan optimal DTR, investigators are interested in sizing SMARTs based on the\nability to screen out DTRs inferior to the optimal DTR by a given amount which\ncannot be done using existing methods. In this paper, we fill this gap by\ndeveloping a rigorous power analysis framework that leverages multiple\ncomparisons with the best methodology. Our method employs Monte Carlo\nsimulation in order to compute the minimum number of individuals to enroll in\nan arbitrary SMART. We will evaluate our method through extensive simulation\nstudies. We will illustrate our method by retrospectively computing the power\nin the Extending Treatment Effectiveness of Naltrexone SMART study.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 18:17:06 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Artman", "William J.", ""], ["Nahum-Shani", "Inbal", ""], ["Wu", "Tianshuang", ""], ["McKay", "James R.", ""], ["Ertefaie", "Ashkan", ""]]}, {"id": "1804.04588", "submitter": "Rapha\\\"el Huser", "authors": "Sabrina Vettori, Rapha\\\"el Huser and Marc G. Genton", "title": "Bayesian Modeling of Air Pollution Extremes Using Nested Multivariate\n  Max-Stable Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capturing the potentially strong dependence among the peak concentrations of\nmultiple air pollutants across a spatial region is crucial for assessing the\nrelated public health risks. In order to investigate the multivariate spatial\ndependence properties of air pollution extremes, we introduce a new class of\nmultivariate max-stable processes. Our proposed model admits a hierarchical\ntree-based formulation, in which the data are conditionally independent given\nsome latent nested $\\alpha$-stable random factors. The hierarchical structure\nfacilitates Bayesian inference and offers a convenient and interpretable\ncharacterization. We fit this nested multivariate max-stable model to the\nmaxima of air pollution concentrations and temperatures recorded at a number of\nsites in the Los Angeles area, showing that the proposed model succeeds in\ncapturing their complex tail dependence structure.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 14:35:59 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Vettori", "Sabrina", ""], ["Huser", "Rapha\u00ebl", ""], ["Genton", "Marc G.", ""]]}, {"id": "1804.04590", "submitter": "Fatemeh Nasiri", "authors": "Fatemeh Nasiri, Oscar Acosta-Tamayo", "title": "Mixed-Effect Modeling for Longitudinal Prediction of Cancer Tumor", "comments": "arXiv admin note: substantial text overlap with arXiv:1803.04241", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a mixed-effect modeling scheme is proposed to construct a\npredictor for different features of cancer tumor. For this purpose, a set of\nfeatures is extracted from two groups of patients with the same type of cancer\nbut with two medical outcome: 1) survived and 2) passed away. The goal is to\nbuild different models for the two groups, where in each group,\npatient-specified behavior of individuals can be characterized. These models\nare then used as predictors to forecast future state of patients with a given\nhistory or initial state. To this end, a leave-on-out cross validation method\nis used to measure the prediction accuracy of each patient-specified model.\nExperiments show that compared to fixed-effect modeling (regression),\nmixed-effect modeling has a superior performance on some of the extracted\nfeatures and similar or worse performance on the others.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 14:57:36 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Nasiri", "Fatemeh", ""], ["Acosta-Tamayo", "Oscar", ""]]}, {"id": "1804.04678", "submitter": "Leonardo Bastos", "authors": "Leonardo S Bastos and Natalia S Paiva and Francisco I Bastos and\n  Daniel A M Villela", "title": "Fast approaches for Bayesian estimation of size of hard-to-reach\n  populations using Network Scale-up", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Network scale-up method is commonly used to overcome difficulties in\nestimating the size of hard-to-reach populations. The method uses indirect\ninformation based on social network of each participant taken from the general\npopulation, but in some applications a fast computational approach would be\nhighly recommended. We propose a Gibbs sampling method and a Monte Carlo\napproach to sample from the random degree model. We applied the abovementioned\nanalytical strategies to previous data on heavy drug users from Curitiba,\nBrazil.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2018 18:12:08 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Bastos", "Leonardo S", ""], ["Paiva", "Natalia S", ""], ["Bastos", "Francisco I", ""], ["Villela", "Daniel A M", ""]]}, {"id": "1804.04954", "submitter": "Sina Dabiri", "authors": "Sina Dabiri, Montasir Abbas", "title": "Evaluation of the Gradient Boosting of Regression Trees Method on\n  Estimating the Car Following Behavior", "comments": "17 pages, 4 figures, 1 table", "journal-ref": "Transportation Research Record. 2018", "doi": "10.1177/0361198118772689", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Car-following models, as the essential part of traffic microscopic\nsimulations, have been utilized to analyze and estimate longitudinal drivers'\nbehavior since sixty years ago. The conventional car following models use\nmathematical formulas to replicate human behavior in the car-following\nphenomenon. Incapability of these approaches to capturing the complex\ninteractions between vehicles calls for deploying advanced learning frameworks\nto consider the more detailed behavior of drivers. In this study, we apply the\nGradient Boosting of Regression Tree (GBRT) algorithm to the vehicle trajectory\ndata sets, which have been collected through the Next Generation Simulation\nprogram, so as to develop a new car-following model. First, the regularization\nparameters of the proposed method are tuned using the cross-validation\ntechnique and the sensitivity analysis. Afterward, the prediction performance\nof the GBRT is compared to the world-famous GHR model, when both models have\nbeen trained on the same data sets. The estimation results of the models on the\nunseen records indicate the superiority of the GBRT algorithm in capturing the\nmotion characteristics of two successive vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:03:38 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dabiri", "Sina", ""], ["Abbas", "Montasir", ""]]}, {"id": "1804.04960", "submitter": "Sina Dabiri", "authors": "Sina Dabiri, Kianoush Kompany, Montasir Abbas", "title": "Introducing a Cost-Effective Approach for Improving the Arterial Traffic\n  Performance Operating Under the Semi-Actuated Coordinated Signal Control", "comments": "17 pages, 4 figures, 3 tables", "journal-ref": "Transportation Research Record. 2018", "doi": "10.1177/0361198118772691", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The semi-actuated coordinated operation mode is a type of signal control\nwhere minor approaches are placed with detectors to develop actuated phasing\nwhile major movements are coordinated without using detection systems. The\nobjective of this study is to propose a cost-effective approach for reducing\ndelay in the semi-actuated coordinated signal operation without incurring any\nextra costs in terms of installing new detectors or developing adaptive\ncontroller systems. We propose a simple approach for further enhancing a\npre-optimized timing plan. In this method, the green splits of non-coordinated\nphases are multiplied by a factor greater than one. In the meantime, the amount\nof green time added to the non-coordinated phases is subtracted from the\ncoordinated phases to keep the cycle length constant. Thus, if the traffic\ndemand on the side streets exceeds the expected traffic flow, the added time in\nthe non-coordinated phase enables the non-coordinated phases to accommodate the\nadditional traffic demand. A regression analysis is implemented so as to\nidentify the optimal value of the mentioned factor, called Actuated Factor\n(ActF). The response variable is the average delay reduction (seconds/vehicle)\nof the simulation runs under the proposed signal timing plan compared to the\nsimulation runs under the pre-optimized timing plan, obtained through the\nmacroscopic signal optimization tools. External traffic movements, left-turn\npercentage, and ActF are the explanatory variables in the model. Results reveal\nthat the ActF is the only significant variable with the optimal value of 1.15\nthat is applicable for a wide range of traffic volumes.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 14:18:56 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dabiri", "Sina", ""], ["Kompany", "Kianoush", ""], ["Abbas", "Montasir", ""]]}, {"id": "1804.05015", "submitter": "Antoine Mazieres", "authors": "Antoine Mazi\\`eres and Camille Roth", "title": "Large-scale diversity estimation through surname origin inference", "comments": null, "journal-ref": "Bulletin of Sociological Methodology/Bulletin de M\\'ethodologie\n  Sociologique 2018, 139(1):59-73", "doi": "10.1177/0759106318778828", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The study of surnames as both linguistic and geographical markers of the past\nhas proven valuable in several research fields spanning from biology and\ngenetics to demography and social mobility. This article builds upon the\nexisting literature to conceive and develop a surname origin classifier based\non a data-driven typology. This enables us to explore a methodology to describe\nlarge-scale estimates of the relative diversity of social groups, especially\nwhen such data is scarcely available. We subsequently analyze the\nrepresentativeness of surname origins for 15 socio-professional groups in\nFrance.\n", "versions": [{"version": "v1", "created": "Fri, 13 Apr 2018 16:35:51 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 20:50:57 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Mazi\u00e8res", "Antoine", ""], ["Roth", "Camille", ""]]}, {"id": "1804.05274", "submitter": "Gabriela Nane", "authors": "Tina Nane and Kasper Kooijman", "title": "A bootstrap analysis for finite populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap methods are increasingly accepted as one of the common approaches\nin constructing confidence intervals in bibliometric studies. Typical bootstrap\nmethods assume that the statistical population is infinite. When the\nstatistical population is finite, a correction needs to be applied in computing\nthe estimated variance of the estimators and thus constructing confidence\nintervals. We investigate the effect of overlooking the finiteness assumption\nof the statistical population using a dataset containing all articles in Web of\nScience (WoS) for Delft University of Technology from 2006 until 2009. We\nregard the data as our finite statistical population and consider simple random\nsamples of various sizes. Standard bootstrap methods are firstly employed in\naccounting for the variability of the estimates, as well as constructing the\nconfidence intervals. The results unveil two issues, namely that the\nvariability in the estimates does not decrease to zero as the sample size\napproaches the population size and that the confidence intervals are not valid.\nBoth issues are addressed when accounting for a finite population correction in\nthe bootstrap methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2018 20:53:35 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Nane", "Tina", ""], ["Kooijman", "Kasper", ""]]}, {"id": "1804.05378", "submitter": "Muxuan Liang", "authors": "Muxuan Liang, Ye Ting, Haoda Fu", "title": "Estimating Individualized Optimal Combination Therapies through Outcome\n  Weighted Deep Learning Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in drug development, multiple treatments are available\nfor a single disease. Patients can often benefit from taking multiple\ntreatments simultaneously. For example, patients in Clinical Practice Research\nDatalink (CPRD) with chronic diseases such as type 2 diabetes can receive\nmultiple treatments simultaneously. Therefore, it is important to estimate what\ncombination therapy from which patients can benefit the most. However, to\nrecommend the best treatment combination is not a single-label but a\nmulti-label classification problem. In this paper, we propose a novel outcome\nweighted deep learning algorithm to estimate individualized optimal combination\ntherapy. The fisher consistency of the proposed loss function under certain\nconditions is also provided. In addition, we extend our method to a family of\nloss functions, which allows adaptive changes based on treatment interactions.\nWe demonstrate the performance of our methods through simulations and real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 16:40:03 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Liang", "Muxuan", ""], ["Ting", "Ye", ""], ["Fu", "Haoda", ""]]}, {"id": "1804.05430", "submitter": "Young-Geun Choi", "authors": "Young-Geun Choi, Lawrence P. Hanrahan, Derek Norton and Ying-Qi Zhao", "title": "Simultaneous disease mapping and hot spot detection with application to\n  childhood obesity surveillance from electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records (EHRs) have become a platform for data-driven\nsurveillance on a granular level in recent years. In this paper, we make use of\nEHRs for early prevention of childhood obesity. The proposed method\nsimultaneously provides smooth disease mapping and outlier information for\nobesity prevalence, which are useful for raising public awareness and\nfacilitating targeted intervention. More precisely, we consider a penalized\nmultilevel generalized linear model. We decompose regional contribution into\nsmooth and sparse signals, which are automatically identified by a combination\nof fusion and sparse penalties imposed on the likelihood function. In addition,\nwe weigh the proposed likelihood to account for the missingness and potential\nnon-representativeness arising from the EHR data. We develop a novel\nalternating minimization algorithm, which is computationally efficient, easy to\nimplement, and guarantees convergence. Simulation studies demonstrate superior\nperformance of the proposed method. Finally, we apply our method to the\nUniversity of Wisconsin Population Health Information Exchange database.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2018 21:17:20 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 15:29:54 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Choi", "Young-Geun", ""], ["Hanrahan", "Lawrence P.", ""], ["Norton", "Derek", ""], ["Zhao", "Ying-Qi", ""]]}, {"id": "1804.05803", "submitter": "Wenxin Jiang", "authors": "Wenxin Jiang, Gary King, Allen Schmaltz, and Martin A. Tanner", "title": "Ecological Regression with Partial Identification", "comments": null, "journal-ref": "Polit. Anal. 28 (2020) 65-86", "doi": "10.1017/pan.2019.19", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological inference (EI) is the process of learning about individual\nbehavior from aggregate data. We study a partially identified linear contextual\neffects model for EI and describe how to estimate the district level parameter\naveraging over many precincts in the presence of the non-identified parameter\nof the contextual effect. This may be regarded as a first attempt in this\nvenerable literature to limit the scope of the key form of non-identifiability\nin EI. To study the operating characteristics of our model, we have amassed the\nlargest collection of data with known ground truth ever applied to evaluate\nsolutions to the EI problem. We collect and study 459 datasets from a variety\nof fields including public health, political science, and sociology. The\ndatasets contain a total of 2,370,854 geographic units (e.g., precincts), with\nan average of 5,165 geographic units per dataset. Our replication data are\npublicly available via the Harvard Dataverse (Jiang et al. 2018) and may serve\nas a useful resource for future researchers. For all real data sets in our\ncollection that fit our proposed rules, our approach reduces the width of the\nDuncan and Davis (1953) deterministic bound, on average, by about 45\\%, while\nstill capturing the true district level parameter in excess of 97\\% of the\ntime. .\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 17:24:17 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 20:47:29 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Jiang", "Wenxin", ""], ["King", "Gary", ""], ["Schmaltz", "Allen", ""], ["Tanner", "Martin A.", ""]]}, {"id": "1804.05882", "submitter": "Gregory Matthews", "authors": "Hunyong Cho, Gregory J. Matthews, and Ofer Harel", "title": "Confidence intervals for the area under the receiver operating\n  characteristic curve in the presence of ignorable missing data", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Receiver operating characteristic (ROC) curves are widely used as a measure\nof accuracy of diagnostic tests and can be summarized using the area under the\nROC curve (AUC). Often, it is useful to construct a confidence intervals for\nthe AUC, however, since there are a number of different proposed methods to\nmeasure variance of the AUC, there are thus many different resulting methods\nfor constructing these intervals. In this manuscript, we compare different\nmethods of constructing Wald-type confidence interval in the presence of\nmissing data where the missingness mechanism is ignorable. We find that\nconstructing confidence intervals using multiple imputation (MI) based on\nlogistic regression (LR) gives the most robust coverage probability and the\nchoice of CI method is less important. However, when missingness rate is less\nsevere (e.g. less than 70%), we recommend using Newcombe's Wald method for\nconstructing confidence intervals along with multiple imputation using\npredictive mean matching (PMM).\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2018 18:28:00 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Cho", "Hunyong", ""], ["Matthews", "Gregory J.", ""], ["Harel", "Ofer", ""]]}, {"id": "1804.06022", "submitter": "Matthew Battifarano", "authors": "Matthew Battifarano, David DeSmet, Achyuth Madabhushi, Parth Nabar", "title": "Predicting Future Machine Failure from Machine State Using Logistic\n  Regression", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately predicting machine failures in advance can decrease maintenance\ncost and help allocate maintenance resources more efficiently. Logistic\nregression was applied to predict machine state 24 hours in the future given\nthe current machine state.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 03:03:06 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Battifarano", "Matthew", ""], ["DeSmet", "David", ""], ["Madabhushi", "Achyuth", ""], ["Nabar", "Parth", ""]]}, {"id": "1804.06033", "submitter": "Alex Kalmikov", "authors": "Alexander G. Kalmikov and Patrick Heimbach", "title": "On Barotropic Mechanisms of Uncertainty Propagation in Estimation of\n  Drake Passage Transport", "comments": "Submitted to Monthly Weather Review on Sep-28-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph physics.comp-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty in estimation of Drake Passage transport is analyzed in a\nHessian-based uncertainty quantification (UQ) framework. The approach extends\nthe adjoint-based ocean state estimation method to provide formal error bounds\nfunctionality. Mechanisms of uncertainty propagation in an idealized barotropic\nmodel of the Antarctic Circumpolar Current are identified by analysis of\nHessian and Jacobian derivative operators, generated via algorithmic\ndifferentiation (AD) of the MIT ocean general circulation model (MITgcm).\nInverse and forward uncertainty propagation mechanisms are identified,\nprojecting uncertainty between observation, control and state variable domains.\nTime resolving analysis of uncertainty propagation captures the dynamics of\nuncertainty evolution and reveals transient and stationary uncertainty regimes.\nThe UQ system resolves also the dynamical coupling of uncertainty across\ndifferent physical fields, as represented by the off-diagonal posterior\ncovariance structure. The spatial patterns of posterior uncertainty reduction\nand their temporal evolution are explained in terms of barotropic ocean\ndynamics. Global uncertainty teleconnection mechanisms are associated with\nbarotropic wave propagation. Uncertainty coupling via data assimilation is\ndemonstrated to dominate the reduction of Drake Passage transport uncertainty,\nhighlighting the importance of correlation between different oceanic variables\non the large scale.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 03:37:52 GMT"}, {"version": "v2", "created": "Wed, 18 Apr 2018 02:59:39 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Kalmikov", "Alexander G.", ""], ["Heimbach", "Patrick", ""]]}, {"id": "1804.06285", "submitter": "Zhe Sha", "authors": "Zhe Sha, Jonathan Rougier, Maike Schumacher and Jonathan Bamber", "title": "Bayesian model-data synthesis with an application to global\n  Glacio-Isostatic Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for updating large scale geospatial processes using\na model-data synthesis method based on Bayesian hierarchical modelling. Two\nmajor challenges come from updating large-scale Gaussian process and modelling\nnon-stationarity. To address the first, we adopt the SPDE approach that uses a\nsparse Gaussian Markov random fields (GMRF) approximation to reduce the\ncomputational cost and implement the Bayesian inference by using the INLA\nmethod. For non-stationary global processes, we propose two general models that\naccommodate commonly-seen geospatial problems. Finally, we show an example of\nupdating an estimate of global glacial isostatic adjustment (GIA) using GPS\nmeasurements.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 14:24:23 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 12:43:15 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Sha", "Zhe", ""], ["Rougier", "Jonathan", ""], ["Schumacher", "Maike", ""], ["Bamber", "Jonathan", ""]]}, {"id": "1804.06316", "submitter": "Pierangelo Lombardo", "authors": "Pierangelo Lombardo, Salvatore Saeli, Federica Bisio, Davide Bernardi,\n  and Danilo Massa", "title": "Fast Flux Service Network Detection via Data Mining on Passive DNS\n  Traffic", "comments": "This is a pre-print of an article published in the proceedings of\n  21st International Conference, ISC 2018, Guildford, UK, September 9-12, 2018.\n  The final authenticated version is available online at:\n  https://doi.org/10.1007/978-3-319-99136-8_25", "journal-ref": null, "doi": "10.1007/978-3-319-99136-8_25", "report-no": null, "categories": "cs.CR cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, the use of fast flux technique has become established as\na common practice to organise botnets in Fast Flux Service Networks (FFSNs),\nwhich are platforms able to sustain illegal online services with very high\navailability. In this paper, we report on an effective fast flux detection\nalgorithm based on the passive analysis of the Domain Name System (DNS) traffic\nof a corporate network. The proposed method is based on the near-real-time\nidentification of different metrics that measure a wide range of fast flux key\nfeatures; the metrics are combined via a simple but effective mathematical and\ndata mining approach. The proposed solution has been evaluated in a one-month\nexperiment over an enterprise network, with the injection of pcaps associated\nwith different malware campaigns, that leverage FFSNs and cover a wide variety\nof attack scenarios. An in-depth analysis of a list of fast flux domains\nconfirmed the reliability of the metrics used in the proposed algorithm and\nallowed for the identification of many IPs that turned out to be part of two\nnotorious FFSNs, namely Dark Cloud and SandiFlux, to the description of which\nwe therefore contribute. All the fast flux domains were detected with a very\nlow false positive rate; a comparison of performance indicators with previous\nworks show a remarkable improvement.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 15:26:06 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 15:25:37 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 09:26:37 GMT"}, {"version": "v4", "created": "Fri, 7 Sep 2018 12:51:47 GMT"}, {"version": "v5", "created": "Tue, 18 Sep 2018 12:45:07 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Lombardo", "Pierangelo", ""], ["Saeli", "Salvatore", ""], ["Bisio", "Federica", ""], ["Bernardi", "Davide", ""], ["Massa", "Danilo", ""]]}, {"id": "1804.06327", "submitter": "Andrew White", "authors": "Rainier Barrett, Shaoyi Jiang, Andrew D White", "title": "Classifying Antimicrobial and Multifunctional Peptides with Bayesian\n  Network Models", "comments": "19 pages, 7 figures, 1 table, supporting information included", "journal-ref": "Peptide Science, Volume 110, Issue 4, 2018", "doi": "10.1002/pep2.24079", "report-no": null, "categories": "stat.AP q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network models are finding success in characterizing\nenzyme-catalyzed reactions, slow conformational changes, predicting enzyme\ninhibition, and genomics. In this work, we apply them to statistical modeling\nof peptides by simultaneously identifying amino acid sequence motifs and using\na motif-based model to clarify the role motifs may play in antimicrobial\nactivity. We construct models of increasing sophistication, demonstrating how\nchemical knowledge of a peptide system may be embedded without requiring new\nderivation of model fitting equations after changing model structure. These\nmodels are used to construct classifiers with good performance (94% accuracy,\nMatthews correlation coefficient of 0.87) at predicting antimicrobial activity\nin peptides, while at the same time being built of interpretable parameters. We\ndemonstrate use of these models to identify peptides that are potentially both\nantimicrobial and antifouling, and show that the background distribution of\namino acids could play a greater role in activity than sequence motifs do. This\nprovides an advancement in the type of peptide activity modeling that can be\ndone and the ease in which models can be constructed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 15:40:00 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Barrett", "Rainier", ""], ["Jiang", "Shaoyi", ""], ["White", "Andrew D", ""]]}, {"id": "1804.06434", "submitter": "Jordan Dworkin", "authors": "Jordan D. Dworkin, Russell T. Shinohara, Danielle S. Bassett", "title": "The emergent integrated network structure of scientific research", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0216146", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The practice of scientific research is often thought of as individuals and\nsmall teams striving for disciplinary advances. Yet as a whole, this endeavor\nmore closely resembles a complex system of natural computation, in which\ninformation is obtained, generated, and disseminated more effectively than\nwould be possible by individuals acting in isolation. Currently, the structure\nof this integrated and innovative landscape of scientific ideas is not well\nunderstood. Here we use tools from network science to map the landscape of\ninterconnected research topics covered in the multidisciplinary journal PNAS\nsince 2000. We construct networks in which nodes represent topics of study and\nedges give the degree to which topics occur in the same papers. The network\ndisplays small-world architecture, with dense connectivity within scientific\nclusters and sparse connectivity between clusters. Notably, clusters tend not\nto align with assigned article classifications, but instead contain topics from\nvarious disciplines. Using a temporal graph, we find that small-worldness has\nincreased over time, suggesting growing efficiency and integration of ideas.\nFinally, we define a novel measure of interdisciplinarity, which is positively\nassociated with PNAS's impact factor. Broadly, this work suggests that complex\nand dynamic patterns of knowledge emerge from scientific research, and that\nstructures reflecting intellectual integration may be beneficial for obtaining\nscientific insight.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 18:54:56 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Dworkin", "Jordan D.", ""], ["Shinohara", "Russell T.", ""], ["Bassett", "Danielle S.", ""]]}, {"id": "1804.06466", "submitter": "Marco Pollo Almeida", "authors": "Marco Pollo, Vera Tomazella, Gustavo Gilardoni, Pedro L. Ramos, Marcio\n  J. Nicola, Francisco Louzada", "title": "Objective Bayesian Inference for Repairable System Subject to Competing\n  Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks models for a repairable system subject to several failure\nmodes are discussed. Under minimal repair, it is assumed that each failure mode\nhas a power law intensity. An orthogonal reparametrization is used to obtain an\nobjective Bayesian prior which is invariant under relabelling of the failure\nmodes. The resulting posterior is a product of gamma distributions and has\nappealing properties: one-to-one invariance, consistent marginalization and\nconsistent sampling properties. Moreover, the resulting Bayes estimators have\nclosed-form expressions and are naturally unbiased for all the parameters of\nthe model. The methodology is applied in the analysis of (i) a previously\nunpublished dataset about recurrent failure history of a sugarcane harvester\nand (ii) records of automotive warranty claims introduced in [1]. A simulation\nstudy was carried out to study the efficiency of the methods proposed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 20:56:54 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Pollo", "Marco", ""], ["Tomazella", "Vera", ""], ["Gilardoni", "Gustavo", ""], ["Ramos", "Pedro L.", ""], ["Nicola", "Marcio J.", ""], ["Louzada", "Francisco", ""]]}, {"id": "1804.06469", "submitter": "Jonah Bernhard", "authors": "Jonah E. Bernhard", "title": "Bayesian parameter estimation for relativistic heavy-ion collisions", "comments": "Ph.D. dissertation; 198 pages, 64 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "nucl-th hep-ph nucl-ex stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I develop and apply a Bayesian method for quantitatively estimating\nproperties of the quark-gluon plasma (QGP), an extremely hot and dense state of\nfluid-like matter created in relativistic heavy-ion collisions.\n  The QGP cannot be directly observed -- it is extraordinarily tiny and\nephemeral, about $10^{-14}$ meters in size and living $10^{-23}$ seconds before\nfreezing into discrete particles -- but it can be indirectly characterized by\nmatching the output of a computational collision model to experimental\nobservations. The model, which takes the QGP properties of interest as input\nparameters, is calibrated to fit the experimental data, thereby extracting a\nposterior probability distribution for the parameters.\n  In this dissertation, I construct a specific computational model of heavy-ion\ncollisions and formulate the Bayesian parameter estimation method, which is\nbased on general statistical techniques. I then apply these tools to estimate\nfundamental QGP properties, including its key transport coefficients and\ncharacteristics of the initial state of heavy-ion collisions.\n  Perhaps most notably, I report the most precise estimate to date of the\ntemperature-dependent specific shear viscosity $\\eta/s$, the measurement of\nwhich is a primary goal of heavy-ion physics. The estimated minimum value is\n$\\eta/s = 0.085_{-0.025}^{+0.026}$ (posterior median and 90% uncertainty),\nremarkably close to the conjectured lower bound of $1/4\\pi \\simeq 0.08$. The\nanalysis also shows that $\\eta/s$ likely increases slowly as a function of\ntemperature.\n  Other estimated quantities include the temperature-dependent bulk viscosity\n$\\zeta/s$, the scaling of initial state entropy deposition, and the duration of\nthe pre-equilibrium stage that precedes QGP formation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2018 21:13:54 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Bernhard", "Jonah E.", ""]]}, {"id": "1804.06806", "submitter": "Lianfen Qian", "authors": "Eric Golinko and Lianfen Qian", "title": "A Min.Max Algorithm for Spline Based Modeling of Violent Crime Rates in\n  USA", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on modeling violent crime rates against population over\nthe years 1960-2014 for the United States via cubic spline based method. We\npropose a new min/max algorithm on knots detection and estimation for cubic\nspline regression. We employ least squares estimation to find potential\nregression coefficients based upon the cubic spline model and the knots chosen\nby the min/max algorithm. We then utilize the best subsets regression method to\naid in model selection in which we find the minimum value of the Bayesian\nInformation Criteria. Finally, we report the $R_{adj}^{2}$ as a measure of\noverall goodness-of-fit of our selected model. Among the fifty states and\nWashington D.C., we have found 42 out of 51 with $R_{adj}^{2}$ value that was\ngreater than $90\\%$. We also present an overall model for the United States as\na whole. Our method can serve as a unified model for violent crime rate over\nfuture years.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 16:43:07 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Golinko", "Eric", ""], ["Qian", "Lianfen", ""]]}, {"id": "1804.06883", "submitter": "Wenyi Wang", "authors": "Seung Jun Shin, Jialu Li, Jing Ning, Jasmina Bojadzieva, Louise C.\n  Strong, Wenyi Wang", "title": "Bayesian estimation of a semiparametric recurrent event model with\n  applications to the penetrance estimation of multiple primary cancers in\n  Li-Fraumeni Syndrome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common phenomenon in cancer syndromes is for an individual to have multiple\nprimary cancers at different sites during his/her lifetime. Patients with\nLi-Fraumeni syndrome (LFS), a rare pediatric cancer syndrome mainly caused by\ngermline TP53 mutations, are known to have a higher probability of developing a\nsecond primary cancer than those with other cancer syndromes. In this context,\nit is desirable to model the development of multiple primary cancers to enable\nbetter clinical management of LFS. Here, we propose a Bayesian recurrent event\nmodel based on a non-homogeneous Poisson process in order to obtain penetrance\nestimates for multiple primary cancers related to LFS. We employed a\nfamily-wise likelihood that facilitates using genetic information inherited\nthrough the family pedigree and properly adjusted for the ascertainment bias\nthat was inevitable in studies of rare diseases by using an inverse probability\nweighting scheme. We applied the proposed method to data on LFS, using a family\ncohort collected through pediatric sarcoma patients at MD Anderson Cancer\nCenter from 1944 to 1982. Both internal and external validation studies showed\nthat the proposed model provides reliable penetrance estimates for multiple\nprimary cancers in LFS, which, to the best of our knowledge, have not been\nreported in the LFS literature.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2018 19:27:46 GMT"}, {"version": "v2", "created": "Wed, 12 Sep 2018 20:59:11 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 18:03:31 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Shin", "Seung Jun", ""], ["Li", "Jialu", ""], ["Ning", "Jing", ""], ["Bojadzieva", "Jasmina", ""], ["Strong", "Louise C.", ""], ["Wang", "Wenyi", ""]]}, {"id": "1804.06912", "submitter": "Gabriele Tolomei", "authors": "Gabriele Tolomei, Mounia Lalmas, Ayman Farahat, Andrew Haines", "title": "You Must Have Clicked on this Ad by Mistake! Data-Driven Identification\n  of Accidental Clicks on Mobile Ads with Applications to Advertiser Cost\n  Discounting and Click-Through Rate Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the cost per click (CPC) pricing model, an advertiser pays an ad network\nonly when a user clicks on an ad; in turn, the ad network gives a share of that\nrevenue to the publisher where the ad was impressed. Still, advertisers may be\nunsatisfied with ad networks charging them for \"valueless\" clicks, or so-called\naccidental clicks. [...] Charging advertisers for such clicks is detrimental in\nthe long term as the advertiser may decide to run their campaigns on other ad\nnetworks. In addition, machine-learned click models trained to predict which ad\nwill bring the highest revenue may overestimate an ad click-through rate, and\nas a consequence negatively impacting revenue for both the ad network and the\npublisher. In this work, we propose a data-driven method to detect accidental\nclicks from the perspective of the ad network. We collect observations of time\nspent by users on a large set of ad landing pages - i.e., dwell time. We notice\nthat the majority of per-ad distributions of dwell time fit to a mixture of\ndistributions, where each component may correspond to a particular type of\nclicks, the first one being accidental. We then estimate dwell time thresholds\nof accidental clicks from that component. Using our method to identify\naccidental clicks, we then propose a technique that smoothly discounts the\nadvertiser's cost of accidental clicks at billing time. Experiments conducted\non a large dataset of ads served on Yahoo mobile apps confirm that our\nthresholds are stable over time, and revenue loss in the short term is\nmarginal. We also compare the performance of an existing machine-learned click\nmodel trained on all ad clicks with that of the same model trained only on\nnon-accidental clicks. There, we observe an increase in both ad click-through\nrate (+3.9%) and revenue (+0.2%) on ads served by the Yahoo Gemini network when\nusing the latter. [...]\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2018 18:00:06 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Tolomei", "Gabriele", ""], ["Lalmas", "Mounia", ""], ["Farahat", "Ayman", ""], ["Haines", "Andrew", ""]]}, {"id": "1804.07079", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel and Norbert Hilger", "title": "On optimal allocation of treatment/condition variance in principal\n  component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The allocation of a (treatment) condition-effect on the wrong principal\ncomponent (misallocation of variance) in principal component analysis (PCA) has\nbeen addressed in research on event-related potentials of the\nelectroencephalogram. However, the correct allocation of condition-effects on\nPCA components might be relevant in several domains of research. The present\npaper investigates whether different loading patterns at each condition-level\nare a basis for an optimal allocation of between-condition variance on\nprincipal components. It turns out that a similar loading shape at each\ncondition-level is a necessary condition for an optimal allocation of\nbetween-condition variance, whereas a similar loading magnitude is not\nnecessary.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 10:52:28 GMT"}], "update_date": "2018-04-20", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Hilger", "Norbert", ""]]}, {"id": "1804.07091", "submitter": "Bj\\\"orn Barz", "authors": "Bj\\\"orn Barz, Erik Rodner, Yanira Guanche Garcia, Joachim Denzler", "title": "Detecting Regions of Maximal Divergence for Spatio-Temporal Anomaly\n  Detection", "comments": "Accepted by TPAMI. Examples and code:\n  https://cvjena.github.io/libmaxdiv/", "journal-ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  vol. 41, no. 5, pp. 1088-1101, 1 May 2019", "doi": "10.1109/TPAMI.2018.2823766", "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic detection of anomalies in space- and time-varying measurements is\nan important tool in several fields, e.g., fraud detection, climate analysis,\nor healthcare monitoring. We present an algorithm for detecting anomalous\nregions in multivariate spatio-temporal time-series, which allows for spotting\nthe interesting parts in large amounts of data, including video and text data.\nIn opposition to existing techniques for detecting isolated anomalous data\npoints, we propose the \"Maximally Divergent Intervals\" (MDI) framework for\nunsupervised detection of coherent spatial regions and time intervals\ncharacterized by a high Kullback-Leibler divergence compared with all other\ndata given. In this regard, we define an unbiased Kullback-Leibler divergence\nthat allows for ranking regions of different size and show how to enable the\nalgorithm to run on large-scale data sets in reasonable time using an interval\nproposal technique. Experiments on both synthetic and real data from various\ndomains, such as climate analysis, video surveillance, and text forensics,\ndemonstrate that our method is widely applicable and a valuable tool for\nfinding interesting events in different types of data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 11:23:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 07:23:39 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Barz", "Bj\u00f6rn", ""], ["Rodner", "Erik", ""], ["Garcia", "Yanira Guanche", ""], ["Denzler", "Joachim", ""]]}, {"id": "1804.07349", "submitter": "Rogelio Nazar", "authors": "Rogelio Nazar", "title": "Invitaci\\'on al estudio estad\\'istico del lenguaje", "comments": "in Spanish. Este texto es el contenido de una conferencia dictada por\n  el autor en la Universidad de Barcelona en el a\\~no 2009. La traducci\\'on al\n  castellano es de Abril de 2018. La versi\\'on original en catal\\'an fue\n  publicado en Jaume Mart\\'i y Marina Salse (coord.) La terminolog\\'ia y la\n  documentaci\\'on: relaciones y sinergias Barcelona: Instituto de Estudios\n  Catalanes, 2010, p. 47-73", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invitation to the statistical study of language: The topic of this\npresentation is the interdisciplinary nexus between linguistics and statistics.\nIt targets linguists, for whom it may have a theoretical interest, or\nprofessionals that work with language, for whom it may have a practical\ninterest. It focuses on the concept of the combinatory probability of words\nfrom three different perspectives: a) the studies of association between the\nunits that are combined, b) the distribution of this combination of units in\nthe corpus, and finally c) the ways of measuring similarity between units\naccording to the combination possibilities. All these topics are addressed in a\nstrictly theoretical fashion and are illustrated by examples of practical\napplication in terminology and in documentation. The objective is to\ndemonstrate that the use of statistical tools in these fields is a necessary\ncomplement to the researcher's intuition.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 19:37:53 GMT"}], "update_date": "2018-04-23", "authors_parsed": [["Nazar", "Rogelio", ""]]}, {"id": "1804.07371", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao, Yang Chen, Jingshu Wang, Dylan S. Small", "title": "Powerful genome-wide design and robust statistical inference in\n  two-sample summary-data Mendelian randomization", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample summary-data Mendelian randomization (MR) has become a popular\nresearch design to estimate the causal effect of risk exposures. With the\nsample size of GWAS continuing to increase, it is now possible to utilize\ngenetic instruments that are only weakly associated with the exposure. To\nmaximize the statistical power of MR, we propose a genome-wide design where\nmore than a thousand genetic instruments are used. For the statistical\nanalysis, we use an empirical partially Bayes approach where instruments are\nweighted according to their strength, thus weak instruments bring less\nvariation to the estimator. The estimator is highly efficient with many weak\ngenetic instruments and is robust to balanced and/or sparse pleiotropy. We\napply our method to estimate the causal effect of body mass index (BMI) and\nmajor blood lipids on cardiovascular disease outcomes and obtain substantially\nshorter confidence intervals. Some new and statistically significant findings\nare: the estimated causal odds ratio of BMI on ischemic stroke is 1.19 (95% CI:\n1.07--1.32, p-value < 0.001); the estimated causal odds ratio of high-density\nlipoprotein cholesterol (HDL-C) on coronary artery disease (CAD) is 0.78 (95%\nCI 0.73--0.84, p-value < 0.001). However, the estimated effect of HDL-C becomes\nsubstantially smaller and statistically non-significant when we only use the\nstrong instruments. By employing a genome-wide design and robust statistical\nmethods, the statistical power of MR studies can be greatly improved. Our\nempirical results suggest that, even though the relationship between HDL-C and\nCAD appears to be highly heterogeneous, it may be too soon to completely\ndismiss the HDL hypothesis.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 20:36:57 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 17:57:01 GMT"}, {"version": "v3", "created": "Fri, 16 Nov 2018 21:20:48 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Chen", "Yang", ""], ["Wang", "Jingshu", ""], ["Small", "Dylan S.", ""]]}, {"id": "1804.07549", "submitter": "Anhad Sandhu", "authors": "Anhadjeet Sandhu, Anne Reinarz, Timothy Dodwell", "title": "A Bayesian Framework for Assessing the Strength Distribution of\n  Composite Structures with Random Defects", "comments": "21 pages, 12 figures", "journal-ref": "Composite Structures, 2018", "doi": "10.1016/j.compstruct.2018.08.074", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel stochastic framework to quantify the knock down\nin strength from out-of-plane wrinkles at the coupon level. The key innovation\nis a Markov Chain Monte Carlo algorithm which rigorously derives the stochastic\ndistribution of wrinkle defects directly informed from image data of defects.\nThe approach significantly reduces uncertainty in the parameterization of\nstochastic numerical studies on the effects of defects. To demonstrate our\nmethodology, we present an original stochastic study to determine the\ndistribution of strength of corner bend samples with random out-plane wrinkle\ndefects. The defects are parameterized by stochastic random fields defined\nusing Karhunen-Lo\\'{e}ve (KL) modes. The distribution of KL coefficients are\ninferred from misalignment data extracted from B-Scan data using a modified\nversion of Multiple Field Image Analysis. The strength distribution is\nestimated, by embedding wrinkles into high fidelity FE simulations using the\nhigh performance toolbox 'dune-composites' from which we observe severe\nknockdowns of $74\\%$ with a probability of $1/200$. Supported by the literature\nour results highlight the strong correlation between maximum misalignment and\nknockdown in coupon strength. This observations allows us to define a surrogate\nmodel providing fast assessment of predicted strength informed from stochastic\nsimulations utilizing both observed wrinkle data and high fidelity finite\nelement models.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 11:03:05 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Sandhu", "Anhadjeet", ""], ["Reinarz", "Anne", ""], ["Dodwell", "Timothy", ""]]}, {"id": "1804.07648", "submitter": "Abhishek Shah", "authors": "Abhishek Shah, Mohamad El Gharamti and Laurent Bertino", "title": "Assimilation of semi-qualitative observations with a stochastic Ensemble\n  Kalman Filter", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3381", "report-no": null, "categories": "math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman filter assumes the observations to be Gaussian random\nvariables with a pre-specified mean and variance. In practice, observations may\nalso have detection limits, for instance when a gauge has a minimum or maximum\nvalue. In such cases most data assimilation schemes discard out-of-range\nvalues, treating them as \"not a number\", at a loss of possibly useful\nqualitative information.\n  The current work focuses on the development of a data assimilation scheme\nthat tackles observations with a detection limit. We present the Ensemble\nKalman Filter Semi-Qualitative (EnKF-SQ) and test its performance against the\nPartial Deterministic Ensemble Kalman Filter (PDEnKF) of Borup et al. (2015).\nBoth are designed to explicitly assimilate the out-of-range observations: the\nout-of-range values are qualitative by nature (inequalities), but one can\npostulate a probability distribution for them and then update the ensemble\nmembers accordingly. The EnKF-SQ is tested within the framework of twin\nexperiments, using both linear and non-linear toy models. Different sensitivity\nexperiments are conducted to assess the influence of the ensemble size,\nobservation detection limit and a number of observations on the performance of\nthe filter. Our numerical results show that assimilating qualitative\nobservations using the proposed scheme improves the overall forecast mean,\nmaking it viable for testing on more realistic applications such as sea-ice\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 14:45:12 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Shah", "Abhishek", ""], ["Gharamti", "Mohamad El", ""], ["Bertino", "Laurent", ""]]}, {"id": "1804.07683", "submitter": "Pantelis Samartsidis", "authors": "Pantelis Samartsidis, Shaun R. Seaman, Anne M. Presanis, Matthew\n  Hickman, Daniela De Angelis", "title": "Assessing the causal effect of binary interventions from observational\n  panel data with few treated units", "comments": null, "journal-ref": "Statistical Science, Volume 34, Number 3 (2019), 486-503", "doi": "10.1214/19-STS713", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often challenged with assessing the impact of an intervention\non an outcome of interest in situations where the intervention is\nnon-randomised, the intervention is only applied to one or few units, the\nintervention is binary, and outcome measurements are available at multiple time\npoints. In this paper, we review existing methods for causal inference in these\nsituations. We detail the assumptions underlying each method, emphasize\nconnections between the different approaches and provide guidelines regarding\ntheir practical implementation. Several open problems are identified thus\nhighlighting the need for future research.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 15:43:08 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 18:12:41 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Samartsidis", "Pantelis", ""], ["Seaman", "Shaun R.", ""], ["Presanis", "Anne M.", ""], ["Hickman", "Matthew", ""], ["De Angelis", "Daniela", ""]]}, {"id": "1804.07820", "submitter": "Pierre Naz\\'e", "authors": "Pierre Naz\\'e", "title": "From Weakly Chaotic Dynamics to Deterministic Subdiffusion via Copula\n  Modeling", "comments": "16 pages, 7 figures", "journal-ref": "J. Stat. Phys. 171(3), 434-448 (2018)", "doi": "10.1007/s10955-018-1999-8", "report-no": null, "categories": "physics.data-an nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula modeling consists in finding a probabilistic distribution, called\ncopula, whereby its coupling with the marginal distributions of a set of random\nvariables produces their joint distribution. The present work aims to use this\ntechnique to connect the statistical distributions of weakly chaotic dynamics\nand deterministic subdiffusion. More precisely, we decompose the jumps\ndistribution of Geisel-Thomae map into a bivariate one and determine the\nmarginal and copula distributions respectively by infinite ergodic theory and\nstatistical inference techniques. We verify therefore that the characteristic\ntail distribution of subdiffusion is an extreme value copula coupling\nMittag-Leffler distributions. We also present a method to calculate the exact\ncopula and joint distributions in the case where weakly chaotic dynamics and\ndeterministic subdiffusion statistical distributions are already known.\nNumerical simulations and consistency with the dynamical aspects of the map\nsupport our results.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2018 17:16:38 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Naz\u00e9", "Pierre", ""]]}, {"id": "1804.07864", "submitter": "Jens Wilting", "authors": "Jens Wilting and Viola Priesemann", "title": "Between perfectly critical and fully irregular: a reverberating model\n  captures and predicts cortical spike propagation", "comments": "27 pages + supplementary information and supplementary figures", "journal-ref": "Cerebral Cortex (2019)", "doi": "10.1093/cercor/bhz049", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge about the collective dynamics of cortical spiking is very\ninformative about the underlying coding principles. However, even most basic\nproperties are not known with certainty, because their assessment is hampered\nby spatial subsampling, i.e. the limitation that only a tiny fraction of all\nneurons can be recorded simultaneously with millisecond precision. Building on\na novel, subsampling-invariant estimator, we fit and carefully validate a\nminimal model for cortical spike propagation. The model interpolates between\ntwo prominent states: asynchronous and critical. We find neither of them in\ncortical spike recordings across various species, but instead identify a narrow\nreverberating regime. This approach enables us to predict yet unknown\nproperties from very short recordings and for every circuit individually,\nincluding responses to minimal perturbations, intrinsic network timescales, and\nthe strength of external input compared to recurrent activation - thereby\ninforming about the underlying coding principles for each circuit, area, state\nand task.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2018 23:46:10 GMT"}, {"version": "v2", "created": "Tue, 12 Mar 2019 17:50:23 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Wilting", "Jens", ""], ["Priesemann", "Viola", ""]]}, {"id": "1804.08055", "submitter": "Samrachana Adhikari", "authors": "Samrachana Adhikari, Sherri Rose, Sharon-Lise Normand", "title": "Nonparametric Bayesian Instrumental Variable Analysis: Evaluating\n  Heterogeneous Effects of Coronary Arterial Access Site Strategies", "comments": "11 tables, 5 figures", "journal-ref": "Journal of the American Statistical Association (2020)", "doi": "10.1080/01621459.2019.1688663", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percutaneous coronary interventions (PCIs) are nonsurgical procedures to open\nblocked blood vessels to the heart, frequently using a catheter to place a\nstent. The catheter can be inserted into the blood vessels using an artery in\nthe groin or an artery in the wrist. Because clinical trials have indicated\nthat access via the wrist may result in fewer post procedure complications,\nshortening the length of stay, and ultimately cost less than groin access,\nadoption of access via the wrist has been encouraged. However, patients treated\nin usual care are likely to differ from those participating in clinical trials,\nand there is reason to believe that the effectiveness of wrist access may\ndiffer between males and females. Moreover, the choice of artery access\nstrategy is likely to be influenced by patient or physician unmeasured factors.\nTo study the effectiveness of the two artery access site strategies on\nhospitalization charges, we use data from a state-mandated clinical registry\nincluding 7,963 patients undergoing PCI. A hierarchical Bayesian\nlikelihood-based instrumental variable analysis under a latent index modeling\nframework is introduced to jointly model outcomes and treatment status. Our\napproach accounts for unobserved heterogeneity via a latent factor structure,\nand permits nonparametric error distributions with Dirichlet process mixture\nmodels. Our results demonstrate that artery access in the wrist reduces\nhospitalization charges compared to access in the groin, with higher mean\nreduction for male patients.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 01:59:19 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 21:07:44 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 03:46:30 GMT"}, {"version": "v4", "created": "Mon, 4 Nov 2019 01:56:18 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Adhikari", "Samrachana", ""], ["Rose", "Sherri", ""], ["Normand", "Sharon-Lise", ""]]}, {"id": "1804.08154", "submitter": "Yo Joong Choe", "authors": "Yo Joong Choe, Sivaraman Balakrishnan, Aarti Singh, Jean M. Vettel,\n  Timothy Verstynen", "title": "Local White Matter Architecture Defines Functional Brain Dynamics", "comments": "Accepted to the 2018 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large bundles of myelinated axons, called white matter, anatomically connect\ndisparate brain regions together and compose the structural core of the human\nconnectome. We recently proposed a method of measuring the local integrity\nalong the length of each white matter fascicle, termed the local connectome. If\ncommunication efficiency is fundamentally constrained by the integrity along\nthe entire length of a white matter bundle, then variability in the functional\ndynamics of brain networks should be associated with variability in the local\nconnectome. We test this prediction using two statistical approaches that are\ncapable of handling the high dimensionality of data. First, by performing\nstatistical inference on distance-based correlations, we show that similarity\nin the local connectome between individuals is significantly correlated with\nsimilarity in their patterns of functional connectivity. Second, by employing\nvariable selection using sparse canonical correlation analysis and\ncross-validation, we show that segments of the local connectome are predictive\nof certain patterns of functional brain dynamics. These results are consistent\nwith the hypothesis that structural variability along axon bundles constrains\ncommunication between disparate brain regions.\n", "versions": [{"version": "v1", "created": "Sun, 22 Apr 2018 18:46:08 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 12:33:36 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Choe", "Yo Joong", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""], ["Vettel", "Jean M.", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1804.08218", "submitter": "Michael Smith", "authors": "Michael Stanley Smith and Thomas S. Shively", "title": "Econometric Modeling of Regional Electricity Spot Prices in the\n  Australian Market", "comments": "Key Words: Bayesian Monotonic Function Estimation, Intraday\n  Electricity Prices, Copula Time Series Model. JEL: C11, C14, C32, C53", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wholesale electricity markets are increasingly integrated via high voltage\ninterconnectors, and inter-regional trade in electricity is growing. To model\nthis, we consider a spatial equilibrium model of price formation, where\nconstraints on inter-regional flows result in three distinct equilibria in\nprices. We use this to motivate an econometric model for the distribution of\nobserved electricity spot prices that captures many of their unique empirical\ncharacteristics. The econometric model features supply and inter-regional trade\ncost functions, which are estimated using Bayesian monotonic regression\nsmoothing methodology. A copula multivariate time series model is employed to\ncapture additional dependence -- both cross-sectional and serial-- in regional\nprices. The marginal distributions are nonparametric, with means given by the\nregression means. The model has the advantage of preserving the heavy\nright-hand tail in the predictive densities of price. We fit the model to\nhalf-hourly spot price data in the five interconnected regions of the\nAustralian national electricity market. The fitted model is then used to\nmeasure how both supply and price shocks in one region are transmitted to the\ndistribution of prices in all regions in subsequent periods. Finally, to\nvalidate our econometric model, we show that prices forecast using the proposed\nmodel compare favorably with those from some benchmark alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 01:52:35 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Smith", "Michael Stanley", ""], ["Shively", "Thomas S.", ""]]}, {"id": "1804.08298", "submitter": "Mehdi Shafiei", "authors": "Mehdi Shafiei, Gerard Ledwich, Ghavameddin Nourbakhsh, Ali Arefi,\n  Houman Pezeshki", "title": "Layered Based Augmented Complex Kalman Filter for Fast Forecasting-Aided\n  State Estimation of Distribution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of renewable resources, distribution networks have become\nextremely complex to monitor, operate and control. Furthermore, for the real\ntime applications, active distribution networks require fast real time\ndistribution state estimation (DSE). Forecasting aided state estimator (FASE),\ndeploys measured data in consecutive time samples to refine the state estimate.\nAlthough most of the DSE algorithms deal with real and imaginary parts of\ndistribution networks states independently, we propose a non iterative complex\nDSE algorithm based on augmented complex Kalman filter (ACKF) which considers\nthe states as complex values. In case of real time DSE and in presence of a\nlarge number of customer loads in the system, employing DSEs in one single\nestimation layer is not computationally efficient. Consequently, our proposed\nmethod performs in several estimation layers hierarchically as a Multi layer\nDSE using ACKF (DSEMACKF). In the proposed method, a distribution network can\nbe divided into one main area and several subareas. The aggregated loads in\neach subarea act like a big customer load in the main area. Load aggregation\nresults in a lower variability and higher cross correlation. This increases the\naccuracy of the estimated states. Additionally, the proposed method is\nformulated to include unbalanced loads in low voltage (LV) distribution\nnetwork.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 09:05:52 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Shafiei", "Mehdi", ""], ["Ledwich", "Gerard", ""], ["Nourbakhsh", "Ghavameddin", ""], ["Arefi", "Ali", ""], ["Pezeshki", "Houman", ""]]}, {"id": "1804.08345", "submitter": "Yesim Guney", "authors": "Ye\\c{s}im G\\\"uney, \\c{S}enay \\\"Ozdemir, Yetkin Tua\\c{c} and Olcay\n  Arslan", "title": "Optimal B-Robust Estimation for the Parameters of Marshall-Olkin\n  Extended Burr XII Distribution and Application for Modeling Data from\n  Pharmacokinetics Study", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marshall-Olkin Extended Burr XII (MOEBXII) distribution family, which is a\ngeneralization of Burr XII distribution proposed by Al-Saiari et al. [1] , is a\nflexible distribution that can be used in many fields such as actuarial\nscience, economics, life testing, reliability and failure time modeling. The\nparameters of the MOEBXII distribution are usually estimated by the maximum\nlikelihood (ML) and least squares (LS) estimation methods. However, these\nestimators are not robust to the outliers which are often encountered in\npractice. There are two main purposes of this paper. The first one is to find\nthe robust estimators for the parameters of the MOEBXII distribution. The\nsecond one is to use this distribution for modeling data from pharmacokinetics\nstudy. To obtain the robust estimators we use the optimal B robust estimator\nproposed by Hampel et al. [2]. We provide a simulation study to show the\nperformance of the proposed estimators for the ML, LS and robust M estimators.\nWe also give a real data example to illustrate the modeling capacity of the\nMOEBXII distribution for data from pharmacokinetics study.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 11:28:37 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["G\u00fcney", "Ye\u015fim", ""], ["\u00d6zdemir", "\u015eenay", ""], ["Tua\u00e7", "Yetkin", ""], ["Arslan", "Olcay", ""]]}, {"id": "1804.08570", "submitter": "Antonio P. Ramos", "authors": "Antonio P. Ramos and Robert E. Weiss", "title": "Measuring Within and Between Group Inequality in Early-Life Mortality\n  Over Time: A Bayesian Approach with Application to India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies on inequality in infant and child mortality compare average\nmortality rates between large groups of births, for example, comparing births\nfrom different countries, income groups, ethnicities, or different times. These\nstudies do not measure within-group disparities. The few studies that have\nmeasured within-group variability in infant and child mortality have used tools\nfrom the income inequality literature, such as Gini indices. We show that the\nlatter are inappropriate for infant and child mortality. We develop novel tools\nthat are appropriate for analyzing infant and child mortality inequality,\nincluding inequality measures, covariate adjustments, and ANOVA methods. We\nillustrate how to handle uncertainty about complex inference targets, including\nensembles of probabilities and kernel density estimates. We illustrate our\nmethodology using a large data set from India, where we estimate infant and\nchild mortality risk for over 400,000 births using a Bayesian hierarchical\nmodel. We show that most of the variance in mortality risk exists within groups\nof births, not between them, and thus that within-group mortality needs to be\ntaken into account when assessing inequality in infant and child mortality. Our\napproach has broad applicability to many health indicators.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 17:10:26 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2019 22:31:41 GMT"}], "update_date": "2019-12-23", "authors_parsed": [["Ramos", "Antonio P.", ""], ["Weiss", "Robert E.", ""]]}, {"id": "1804.08698", "submitter": "Tanujit Chakraborty", "authors": "Tanujit Chakraborty, Ashis Kumar Chakraborty, Swarup Chattopadhyay", "title": "A novel distribution-free hybrid regression model for manufacturing\n  process efficiency improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by a particular problem of a modern paper\nmanufacturing industry, in which maximum efficiency of the fiber-filler\nrecovery process is desired. A lot of unwanted materials along with valuable\nfibers and fillers come out as a by-product of the paper manufacturing process\nand mostly goes as waste. The job of an efficient Krofta supracell is to\nseparate the unwanted materials from the valuable ones so that fibers and\nfillers can be collected from the waste materials and reused in the\nmanufacturing process. The efficiency of Krofta depends on several crucial\nprocess parameters and monitoring them is a difficult proposition. To solve\nthis problem, we propose a novel hybridization of regression trees (RT) and\nartificial neural networks (ANN), hybrid RT-ANN model, to solve the problem of\nlow recovery percentage of the supracell. This model is used to achieve the\ngoal of improving supracell efficiency, viz., gain in percentage recovery. In\naddition, theoretical results for the universal consistency of the proposed\nmodel are given with the optimal value of a vital model parameter. Experimental\nfindings show that the proposed hybrid RT-ANN model achieves higher accuracy in\npredicting Krofta recovery percentage than other conventional regression models\nfor solving the Krofta efficiency problem. This work will help the paper\nmanufacturing company to become environmentally friendly with minimal\necological damage and improved waste recovery.\n", "versions": [{"version": "v1", "created": "Mon, 23 Apr 2018 20:03:55 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 09:27:52 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Chakraborty", "Ashis Kumar", ""], ["Chattopadhyay", "Swarup", ""]]}, {"id": "1804.08796", "submitter": "Theja Tulabandhula", "authors": "Mehrnaz Amjadi and Theja Tulabandhula", "title": "Block-Structure Based Time-Series Models For Graph Sequences", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the computational and statistical trade-off for modeling single\ngraphs, for instance, using block models is relatively well understood,\nextending such results to sequences of graphs has proven to be difficult. In\nthis work, we take a step in this direction by proposing two models for graph\nsequences that capture: (a) link persistence between nodes across time, and (b)\ncommunity persistence of each node across time. In the first model, we assume\nthat the latent community of each node does not change over time, and in the\nsecond model we relax this assumption suitably. For both of these proposed\nmodels, we provide statistically and computationally efficient inference\nalgorithms, whose unique feature is that they leverage community detection\nmethods that work on single graphs. We also provide experimental results\nvalidating the suitability of our models and methods on synthetic and real\ninstances.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 01:14:16 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 16:34:31 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Amjadi", "Mehrnaz", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1804.08807", "submitter": "Michael Bertolacci", "authors": "Michael Bertolacci, Edward Cripps, Ori Rosen, Sally Cripps", "title": "A comparison of methods for modeling marginal non-zero daily rainfall\n  across the Australian continent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naveau et al. (2016) have recently developed a class of methods, based on\nextreme-value theory (EVT), for capturing low, moderate, and heavy rainfall\nsimultaneously, without the need to choose a threshold typical to EVT methods.\nWe analyse the performance of Naveau et al.'s methods, along with mixtures of\ngamma distributions, by fitting them to marginal non-zero rainfall from 16,968\nsites spanning the Australian continent and which represent a wide variety of\nrainfall patterns. Performance is assessed by the distribution across sites of\nthe log ratios of each method's estimated quantiles and the empirical\nquantiles. We do so for quantiles corresponding to low, moderate, and heavy\nrainfall. Under this metric, mixtures of three and four gamma distributions\noutperform Naveau et al's methods for small and moderate rainfall, and provide\nequivalent fits for heavy rainfall.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 01:53:46 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Bertolacci", "Michael", ""], ["Cripps", "Edward", ""], ["Rosen", "Ori", ""], ["Cripps", "Sally", ""]]}, {"id": "1804.08830", "submitter": "Theresa Gebert", "authors": "Theresa Gebert, Shuli Jiang, Jiaxian Sheng", "title": "Characterizing Allegheny County Opioid Overdoses with an Interactive\n  Data Explorer and Synthetic Prediction Tool", "comments": "10 pages, 7 figures, HackAuton 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The United States has an opioid epidemic, and Pennsylvania's Allegheny County\nis among the worst. This motivates a deeper exploration of what characterizes\nthe epidemic, such as what are risk factors for people who ultimately overdose\nand die due to opioids. We show that some interesting trends and factors can be\nidentified from openly available autopsy data, and demonstrate the power of\nbuilding an interactive data exploration tool for policy makers. However, there\nis still a pressing need to incorporate further demographic factors. We show\nthis by using synthetic Electronic Medical Record (EMR) data to simulate the\npredictive power of random forests and neural networks when given additional\nloosely correlated features. In addition, we give examples of useful feature\nextraction that enable model enhancement without sacrificing privacy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 03:36:44 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Gebert", "Theresa", ""], ["Jiang", "Shuli", ""], ["Sheng", "Jiaxian", ""]]}, {"id": "1804.08942", "submitter": "Hua Cheng M.D.", "authors": "Cheng Hua", "title": "Internal relation between Personality trait Statistical outcomes among\n  Junior College Divers and their performance", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Personality trait can predict divers' behavioral performance\nunderwater. However, we know very little about the innate personality of the\njunior college diving students. To gain a better insight of personality\ncharacteristics of them, we carried out a personality survey base on Eysenck\nquestionnaire. Method: 93 college diving students participated in this survey\nand totally 74 valid questionnaires recovered. Four dimensions were rating by\nthe self-report scale of 85 questions. Descriptive analysis, T test and\nvariance analyses are processing by SPSS20.0. Results: Statistical results\nindicated that college divers are more extraverted (t=10.838, p=0.000), more\nneurotic (t=2.747, p=0.008) and unlikely psychoticism (t=-1.332, p=0.187).\nDifferences were found in Gender only in Liar scale score (F=7.025, p=0.010).\nConclusion: These outcomes suggested that parts of the character of the college\ndivers' are suitable for diving activities. And emotional control training is\nneeded in the curriculum setting for them in the process of safe diving.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 10:15:04 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Hua", "Cheng", ""]]}, {"id": "1804.08962", "submitter": "Elsa Cazelles", "authors": "J\\'er\\'emie Bigot and Elsa Cazelles and Nicolas Papadakis", "title": "Data-driven regularization of Wasserstein barycenters with an\n  application to multivariate density registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to simultaneously align and smooth data in the form of\nmultiple point clouds sampled from unknown densities with support in a\nd-dimensional Euclidean space. This work is motivated by applications in\nbioinformatics where researchers aim to automatically homogenize large datasets\nto compare and analyze characteristics within a same cell population.\nInconveniently, the information acquired is most certainly noisy due to\nmis-alignment caused by technical variations of the environment. To overcome\nthis problem, we propose to register multiple point clouds by using the notion\nof regularized barycenters (or Fr\\'{e}chet mean) of a set of probability\nmeasures with respect to the Wasserstein metric. A first approach consists in\npenalizing a Wasserstein barycenter with a convex functional as recently\nproposed in Bigot and al. (2018). A second strategy is to transform the\nWasserstein metric itself into an entropy regularized transportation cost\nbetween probability measures as introduced in Cuturi (2013). The main\ncontribution of this work is to propose data-driven choices for the\nregularization parameters involved in each approach using the\nGoldenshluger-Lepski's principle. Simulated data sampled from Gaussian mixtures\nare used to illustrate each method, and an application to the analysis of flow\ncytometry data is finally proposed. This way of choosing of the regularization\nparameter for the Sinkhorn barycenter is also analyzed through the prism of an\noracle inequality that relates the error made by such data-driven estimators to\nthe one of an ideal estimator.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 11:34:06 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 09:17:15 GMT"}, {"version": "v3", "created": "Sat, 4 May 2019 15:26:17 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 13:48:40 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Cazelles", "Elsa", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1804.09062", "submitter": "Manoj Gopalkrishnan", "authors": "Muppirala Viswa Virinchi, Abhishek Behera, Manoj Gopalkrishnan", "title": "A reaction network scheme which implements the EM algorithm", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.IT cs.SY math.IT q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A detailed algorithmic explanation is required for how a network of chemical\nreactions can generate the sophisticated behavior displayed by living cells.\nThough several previous works have shown that reaction networks are\ncomputationally universal and can in principle implement any algorithm, there\nis scope for constructions that map well onto biological reality, make\nefficient use of the computational potential of the native dynamics of reaction\nnetworks, and make contact with statistical mechanics. We describe a new\nreaction network scheme for solving a large class of statistical problems\nincluding the problem of how a cell would infer its environment from\nreceptor-ligand bindings. Specifically we show how reaction networks can\nimplement information projection, and consequently a generalized\nExpectation-Maximization algorithm, to solve maximum likelihood estimation\nproblems in partially-observed exponential families on categorical data. Our\nscheme can be thought of as an algorithmic interpretation of E. T. Jaynes's\nvision of statistical mechanics as statistical inference.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 14:20:24 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Virinchi", "Muppirala Viswa", ""], ["Behera", "Abhishek", ""], ["Gopalkrishnan", "Manoj", ""]]}, {"id": "1804.09088", "submitter": "Evangelos Papalexakis", "authors": "Gisel Bastidas Guacho, Sara Abdali, Neil Shah, Evangelos E.\n  Papalexakis", "title": "Semi-supervised Content-based Detection of Misinformation via Tensor\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fake news may be intentionally created to promote economic, political and\nsocial interests, and can lead to negative impacts on humans beliefs and\ndecisions. Hence, detection of fake news is an emerging problem that has become\nextremely prevalent during the last few years. Most existing works on this\ntopic focus on manual feature extraction and supervised classification models\nleveraging a large number of labeled (fake or real) articles. In contrast, we\nfocus on content-based detection of fake news articles, while assuming that we\nhave a small amount of labels, made available by manual fact-checkers or\nautomated sources. We argue this is a more realistic setting in the presence of\nmassive amounts of content, most of which cannot be easily factchecked. To that\nend, we represent collections of news articles as multi-dimensional tensors,\nleverage tensor decomposition to derive concise article embeddings that capture\nspatial/contextual information about each news article, and use those\nembeddings to create an article-by-article graph on which we propagate limited\nlabels. Results on three real-world datasets show that our method performs on\npar or better than existing models that are fully supervised, in that we\nachieve better detection accuracy using fewer labels. In particular, our\nproposed method achieves 75.43% of accuracy using only 30% of labels of a\npublic dataset while an SVM-based classifier achieved 67.43%. Furthermore, our\nmethod achieves 70.92% of accuracy in a large dataset using only 2% of labels.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 15:13:51 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Guacho", "Gisel Bastidas", ""], ["Abdali", "Sara", ""], ["Shah", "Neil", ""], ["Papalexakis", "Evangelos E.", ""]]}, {"id": "1804.09129", "submitter": "David Pastor-Escuredo", "authors": "David Pastor-Escuredo, Yolanda Torres, Maria Martinez, Pedro J.\n  Zufiria", "title": "Floods impact dynamics quantified from big data sources", "comments": "Submitted to the UN Data For Climate Action Challenge being Finalist\n  with Honorable Mention http://www.dataforclimateaction.org/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural disasters affect hundreds of millions of people worldwide every year.\nEarly warning, humanitarian response and recovery mechanisms can be improved by\nusing big data sources. Measuring the different dimensions of the impact of\nnatural disasters is critical for designing policies and building up\nresilience. Detailed quantification of the movement and behaviours of affected\npopulations requires the use of high granularity data that entails privacy\nrisks. Leveraging all this data is costly and has to be done ensuring privacy\nand security of large amounts of data. Proxies based on social media and data\naggregates would streamline this process by providing evidences and narrowing\nrequirements. We propose a framework that integrates environmental data, social\nmedia, remote sensing, digital topography and mobile phone data to understand\ndifferent types of floods and how data can provide insights useful for managing\nhumanitarian action and recovery plans. Thus, data is dynamically requested\nupon data-based indicators forming a multi-granularity and multi-access data\npipeline. We present a composed study of three cases to show potential\nvariability in the natures of floodings,as well as the impact and applicability\nof data sources. Critical heterogeneity of the available data in the different\ncases has to be addressed in order to design systematic approaches based on\ndata. The proposed framework establishes the foundation to relate the physical\nand socio-economical impacts of floods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 16:45:27 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Pastor-Escuredo", "David", ""], ["Torres", "Yolanda", ""], ["Martinez", "Maria", ""], ["Zufiria", "Pedro J.", ""]]}, {"id": "1804.09135", "submitter": "Nicolas Haverkamp", "authors": "Nicolas Haverkamp and Andr\\'e Beauducel", "title": "Differences of Type I error rates for ANOVA and Multilevel-Linear-Models\n  using SAS and SPSS for repeated measures designs", "comments": "32 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To derive recommendations on how to analyze longitudinal data, we examined\nType I error rates of Multilevel Linear Models (MLM) and repeated measures\nAnalysis of Variance (rANOVA) using SAS and SPSS.We performed a simulation with\nthe following specifications: To explore the effects of high numbers of\nmeasurement occasions and small sample sizes on Type I error, measurement\noccasions of m = 9 and 12 were investigated as well as sample sizes of n = 15,\n20, 25 and 30. Effects of non-sphericity in the population on Type I error were\nalso inspected: 5,000 random samples were drawn from two populations containing\nneither a within-subject nor a between-group effect. They were analyzed\nincluding the most common options to correct rANOVA and MLM-results: The\nHuynh-Feldt-correction for rANOVA (rANOVA-HF) and the Kenward-Roger-correction\nfor MLM (MLM-KR), which could help to correct progressive bias of MLM with an\nunstructured covariance matrix (MLM-UN). Moreover, uncorrected rANOVA and MLM\nassuming a compound symmetry covariance structure (MLM-CS) were also taken into\naccount. The results showed a progressive bias for MLM-UN for small samples\nwhich was stronger in SPSS than in SAS. Moreover, an appropriate bias\ncorrection for Type I error via rANOVA-HF and an insufficient correction by\nMLM-UN-KR for n < 30 were found. These findings suggest MLM-CS or rANOVA if\nsphericity holds and a correction of a violation via rANOVA-HF. If an analysis\nrequires MLM, SPSS yields more accurate Type I error rates for MLM-CS and SAS\nyields more accurate Type I error rates for MLM-UN.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 16:55:38 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 11:28:26 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 12:42:20 GMT"}, {"version": "v4", "created": "Mon, 5 Nov 2018 14:01:00 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Haverkamp", "Nicolas", ""], ["Beauducel", "Andr\u00e9", ""]]}, {"id": "1804.09233", "submitter": "Marie Courbariaux", "authors": "Marie Courbariaux, Pierre Barbillon, Luc Perreault and \\'Eric Parent", "title": "Post-processing multi-ensemble temperature and precipitation forecasts\n  through an Exchangeable Gamma Normal model and its Tobit extension", "comments": null, "journal-ref": "Journal of Agricultural, Biological and Environmental Statistics,\n  2019, p. 1-37", "doi": "10.1007/s13253-019-00358-2", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meteorological ensembles are a collection of scenarios for future weather\ndelivered by a meteorological center. Such ensembles form the main source of\nvaluable information for probabilistic forecasting which aims at producing a\npredictive probability distribution of the quantity of interest instead of a\nsingle best guess estimate. Unfortunately, ensembles cannot generally be\nconsidered as a sample from such a predictive probability distribution without\na preliminary post-processing treatment to calibrate the ensemble. Two main\nfamilies of post-processing methods, either competing such as BMA or\ncollaborative such as EMOS, can be found in the literature. This paper proposes\na mixed effect model belonging to the collaborative family. The structure of\nthe model is based on the hypothesis of invariance under the relabelling of the\nensemble members. Its interesting specificities are as follows: 1)\nexchangeability, which contributes to parsimony, with a latent pivot variable\nsynthesizing the essential meteorological features of the ensembles, 2) a\nmulti-ensemble implementation, allowing to take advantage of various\ninformation so as to increase the sharpness of the forecasting procedure. Focus\nis cast onto Normal statistical structures, first with a direct application for\ntemperatures, then with its Tobit extension for precipitation. Inference is\nperformed by EM algorithms with recourse made to stochastic conditional\nsimulations in the precipitation case. After checking its good behavior on\nartificial data, the proposed post-processing technique is applied to\ntemperature and precipitation ensemble forecasts produced over five river\nbasins managed by Hydro-Qu$\\'e$bec. These ensemble forecasts were extracted\nfrom the THORPEX Interactive Grand Global Ensemble (TIGGE) database. The\nresults indicate that post-processed ensemble are calibrated and generally\nsharper than the raw ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:02:53 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 20:52:48 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Courbariaux", "Marie", ""], ["Barbillon", "Pierre", ""], ["Perreault", "Luc", ""], ["Parent", "\u00c9ric", ""]]}, {"id": "1804.09253", "submitter": "Kevin Kuo", "authors": "Kevin Kuo", "title": "DeepTriangle: A Deep Learning Approach to Loss Reserving", "comments": "Published version available at https://www.mdpi.com/2227-9091/7/3/97", "journal-ref": "Risks 2019, 7(3), 97", "doi": "10.3390/risks7030097", "report-no": null, "categories": "stat.AP cs.LG q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for loss reserving based on deep neural networks.\nThe approach allows for joint modeling of paid losses and claims outstanding,\nand incorporation of heterogeneous inputs. We validate the models on loss\nreserving data across lines of business, and show that they improve on the\npredictive accuracy of existing stochastic methods. The models require minimal\nfeature engineering and expert input, and can be automated to produce forecasts\nmore frequently than manual workflows.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 20:47:04 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 20:35:59 GMT"}, {"version": "v3", "created": "Sun, 17 Mar 2019 19:04:15 GMT"}, {"version": "v4", "created": "Mon, 16 Sep 2019 17:12:04 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kuo", "Kevin", ""]]}, {"id": "1804.09281", "submitter": "Lu Wang", "authors": "Lu Wang, Ying Huang, Alexander R Luedtke", "title": "Test for Incremental Value of New Biomarkers Based on OR Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In early detection of disease, a single biomarker often has inadequate\nclassification performance, making it important to identify new biomarkers to\ncombine with the existing marker for improved performance. A biologically\nnatural method to combine biomarkers is to use logic rules, e.g. the OR/AND\nrules. In our motivating example of early detection of pancreatic cancer, the\nestablished biomarker CA19-9 is only present in a subclass of cancer; it is of\ninterest to identify new biomarkers present in the other subclasses and declare\ndisease when either marker is positive. While there has been research on\ndeveloping biomarker combinations using the OR/AND rules, the inference\nregarding the incremental value of the new marker within this framework is\nlacking and challenging due to a statistical non-regularity. In this paper, we\naim to answer the inferential question of whether combining the new biomarker\nachieves better classification performance than using the existing biomarker\nalone, based on a nonparametrically estimated OR rule that maximizes the\nweighted average of sensitivity and specificity. We propose and compare various\nprocedures for testing the incremental value of the new biomarker and\nconstructing its confidence interval, using bootstrap, cross-validation, and a\nnovel fuzzy p-value-based technique. We compare the performance of different\nmethods via extensive simulation studies and apply them to the pancreatic\ncancer example.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 22:33:22 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Wang", "Lu", ""], ["Huang", "Ying", ""], ["Luedtke", "Alexander R", ""]]}, {"id": "1804.09302", "submitter": "Cheng Yong Tang", "authors": "Miao Yuan, Cheng Yong Tang, Yili Hong, Jian Yang", "title": "Disentangling and Assessing Uncertainties in Multiperiod Corporate\n  Default Risk Predictions", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the corporate default risk is broadly important in economics and\nfinance. Quantitative methods have been developed to predictively assess future\ncorporate default probabilities. However, as a more difficult yet crucial\nproblem, evaluating the uncertainties associated with the default predictions\nremains little explored. In this paper, we attempt to fill this blank by\ndeveloping a procedure for quantifying the level of associated uncertainties\nupon carefully disentangling multiple contributing sources. Our framework\neffectively incorporates broad information from historical default data,\ncorporates' financial records, and macroeconomic conditions by a)\ncharacterizing the default mechanism, and b) capturing the future dynamics of\nvarious features contributing to the default mechanism. Our procedure overcomes\nthe major challenges in this large scale statistical inference problem and\nmakes it practically feasible by using parsimonious models, innovative methods,\nand modern computational facilities. By predicting the marketwide total number\nof defaults and assessing the associated uncertainties, our method can also be\napplied for evaluating the aggregated market credit risk level. Upon analyzing\na US market data set, we demonstrate that the level of uncertainties associated\nwith default risk assessments is indeed substantial. More informatively, we\nalso find that the level of uncertainties associated with the default risk\npredictions is correlated with the level of default risks, indicating potential\nfor new scopes in practical applications including improving the accuracy of\ndefault risk assessments.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 00:46:49 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yuan", "Miao", ""], ["Tang", "Cheng Yong", ""], ["Hong", "Yili", ""], ["Yang", "Jian", ""]]}, {"id": "1804.09495", "submitter": "Dmitry Kobak", "authors": "Dmitry Kobak, Sergey Shpilkin, Maxim S. Pshenichnikov", "title": "Putin's peaks: Russian election data revisited", "comments": "To appear in Significance magazine", "journal-ref": "Significance 15 (3), 2018, 8-9", "doi": "10.1111/j.1740-9713.2018.01141.x", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the anomalous prevalence of integer percentages in the last\nparliamentary (2016) and presidential (2018) Russian elections. We show how\nthis anomaly in Russian federal elections has evolved since 2000.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2018 11:59:53 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Kobak", "Dmitry", ""], ["Shpilkin", "Sergey", ""], ["Pshenichnikov", "Maxim S.", ""]]}, {"id": "1804.10163", "submitter": "Evgeny Burnaev", "authors": "Alexander Bernstein, Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana\n  Sushchinskaya, Maxim Sharaev, Alexander Andreev, Alexey Artemov, Renat\n  Akzhigitov", "title": "Machine Learning pipeline for discovering neuroimaging-based biomarkers\n  in neurology and psychiatry", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a problem of diagnostic pattern recognition/classification from\nneuroimaging data. We propose a common data analysis pipeline for\nneuroimaging-based diagnostic classification problems using various ML\nalgorithms and processing toolboxes for brain imaging. We illustrate the\npipeline application by discovering new biomarkers for diagnostics of epilepsy\nand depression based on clinical and MRI/fMRI data for patients and healthy\nvolunteers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:42:38 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Kondratyeva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Sharaev", "Maxim", ""], ["Andreev", "Alexander", ""], ["Artemov", "Alexey", ""], ["Akzhigitov", "Renat", ""]]}, {"id": "1804.10167", "submitter": "Evgeny Burnaev", "authors": "Maxim Sharaev, Alexander Andreev, Alexey Artemov, Alexander Bernstein,\n  Evgeny Burnaev, Ekaterina Kondratyeva, Svetlana Sushchinskaya, Renat\n  Akzhigitov", "title": "fMRI: preprocessing, classification and pattern recognition", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning continues to gain momentum in the neuroscience community,\nwe witness the emergence of novel applications such as diagnostics,\ncharacterization, and treatment outcome prediction for psychiatric and\nneurological disorders, for instance, epilepsy and depression. Systematic\nresearch into these mental disorders increasingly involves drawing clinical\nconclusions on the basis of data-driven approaches; to this end, structural and\nfunctional neuroimaging serve as key source modalities. Identification of\ninformative neuroimaging markers requires establishing a comprehensive\npreparation pipeline for data which may be severely corrupted by artifactual\nsignal fluctuations. In this work, we review a large body of literature to\nprovide ample evidence for the advantages of pattern recognition approaches in\nclinical applications, overview advanced graph-based pattern recognition\napproaches, and propose a noise-aware neuroimaging data processing pipeline. To\ndemonstrate the effectiveness of our approach, we provide results from a pilot\nstudy, which show a significant improvement in classification accuracy,\nindicating a promising research direction.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 16:48:52 GMT"}], "update_date": "2018-04-27", "authors_parsed": [["Sharaev", "Maxim", ""], ["Andreev", "Alexander", ""], ["Artemov", "Alexey", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Kondratyeva", "Ekaterina", ""], ["Sushchinskaya", "Svetlana", ""], ["Akzhigitov", "Renat", ""]]}, {"id": "1804.10308", "submitter": "Zekun Xu", "authors": "Zekun Xu, Ye Liu", "title": "A Regularized Vector Autoregressive Hidden Semi-Markov Model, with\n  Application to Multivariate Financial Data", "comments": "6 pages, 3 figures", "journal-ref": "The International FLAIRS Conference Proceedings, Vol 34 No 1,\n  (2021)", "doi": "10.32473/flairs.v34i1.128424", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regularized vector autoregressive hidden semi-Markov model is developed to\nanalyze multivariate financial time series with switching data generating\nregimes. Furthermore, an augmented EM algorithm is proposed for parameter\nestimation by embedding regularized estimators for the state-dependent\ncovariance matrices and autoregression matrices in the M-step. The performance\nof the proposed regularized estimators is evaluated both in the simulation\nexperiments and on the New York Stock Exchange financial portfolio data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 23:42:24 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 05:00:35 GMT"}, {"version": "v3", "created": "Tue, 18 Feb 2020 03:04:54 GMT"}, {"version": "v4", "created": "Wed, 12 May 2021 18:46:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Xu", "Zekun", ""], ["Liu", "Ye", ""]]}, {"id": "1804.10454", "submitter": "Andreas Meinel", "authors": "Andreas Meinel, Henrich Kolkhorst, Michael Tangermann", "title": "Mining within-trial oscillatory brain dynamics to address the\n  variability of optimized spatial filters", "comments": null, "journal-ref": null, "doi": "10.1109/TNSRE.2019.2894914", "report-no": null, "categories": "eess.SP cs.LG q-bio.NC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven spatial filtering algorithms optimize scores such as the contrast\nbetween two conditions to extract oscillatory brain signal components. Most\nmachine learning approaches for filter estimation, however, disregard\nwithin-trial temporal dynamics and are extremely sensitive to changes in\ntraining data and involved hyperparameters. This leads to highly variable\nsolutions and impedes the selection of a suitable candidate for,\ne.g.,~neurotechnological applications. Fostering component introspection, we\npropose to embrace this variability by condensing the functional signatures of\na large set of oscillatory components into homogeneous clusters, each\nrepresenting specific within-trial envelope dynamics.\n  The proposed method is exemplified by and evaluated on a complex hand force\ntask with a rich within-trial structure. Based on electroencephalography data\nof 18 healthy subjects, we found that the components' distinct temporal\nenvelope dynamics are highly subject-specific. On average, we obtained seven\nclusters per subject, which were strictly confined regarding their underlying\nfrequency bands. As the analysis method is not limited to a specific spatial\nfiltering algorithm, it could be utilized for a wide range of\nneurotechnological applications, e.g., to select and monitor functionally\nrelevant features for brain-computer interface protocols in stroke\nrehabilitation.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 11:56:04 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 11:39:50 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Meinel", "Andreas", ""], ["Kolkhorst", "Henrich", ""], ["Tangermann", "Michael", ""]]}, {"id": "1804.10527", "submitter": "Nazih Benoumechiara", "authors": "Nazih Benoumechiara (LPSM, EDF R and D PRISME), Bertrand Michel\n  (LMJL), Philippe Saint-Pierre (IMT), Nicolas Bousquet (LPSM)", "title": "Detecting and modeling worst-case dependence structures between random\n  inputs of computational reliability models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain information on input parameters of reliability models is usually\nmodeled by considering these parameters as random, and described by marginal\ndistributions and a dependence structure of these variables. In numerous\nreal-world applications, while information is mainly provided by marginal\ndistributions, typically from samples , little is really known on the\ndependence structure itself. Faced with this problem of incomplete or missing\ninformation, risk studies are often conducted by considering independence of\ninput variables, at the risk of including irrelevant situations. This approach\nis especially used when reliability functions are considered as black-box\ncomputational models. Such analyses remain weakened in absence of in-depth\nmodel exploration, at the possible price of a strong risk misestimation.\nConsidering the frequent case where the reliability output is a quantile, this\narticle provides a methodology to improve risk assessment, by exploring a set\nof pessimistic dependencies using a copula-based strategy. In dimension greater\nthan two, a greedy algorithm is provided to build input regular vine copulas\nreaching a minimum quantile to which a reliability admissible limit value can\nbe compared, by selecting pairwise components of sensitive influence on the\nresult. The strategy is tested over toy models and a real industrial\ncase-study. The results highlight that current approaches can provide\nnon-conservative results, and that a nontrivial dependence structure can be\nexhibited to define a worst-case scenario.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 14:15:42 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Benoumechiara", "Nazih", "", "LPSM, EDF R and D PRISME"], ["Michel", "Bertrand", "", "LMJL"], ["Saint-Pierre", "Philippe", "", "IMT"], ["Bousquet", "Nicolas", "", "LPSM"]]}, {"id": "1804.10636", "submitter": "Milana Gataric", "authors": "Milana Gataric, George S. D. Gordon, Francesco Renna, Alberto Gil C.\n  P. Ramos, Maria P. Alcolea, Sarah E. Bohndiek", "title": "Reconstruction of optical vector-fields with applications in endoscopic\n  imaging", "comments": null, "journal-ref": null, "doi": "10.1109/TMI.2018.2875875", "report-no": null, "categories": "math.NA eess.IV physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for the reconstruction of the amplitude, phase and\npolarisation of an optical vector-field using calibration measurements acquired\nby an imaging device with an unknown linear transformation. By incorporating\neffective regularisation terms, this new approach is able to recover an optical\nvector-field with respect to an arbitrary representation system, which may be\ndifferent from the one used in calibration. In particular, it enables the\nrecovery of an optical vector-field with respect to a Fourier basis, which is\nshown to yield indicative features of increased scattering associated with\ntissue abnormalities. We demonstrate the effectiveness of our approach using\nsynthetic holographic images as well as biological tissue samples in an\nexperimental setting where measurements of an optical vector-field are acquired\nby a fibre endoscope, and observe that indeed the recovered Fourier\ncoefficients are useful in distinguishing healthy tissues from lesions in early\nstages of oesophageal cancer.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2018 18:26:54 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2018 10:41:37 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Gataric", "Milana", ""], ["Gordon", "George S. D.", ""], ["Renna", "Francesco", ""], ["Ramos", "Alberto Gil C. P.", ""], ["Alcolea", "Maria P.", ""], ["Bohndiek", "Sarah E.", ""]]}, {"id": "1804.11005", "submitter": "Sahil Garg", "authors": "Emilia M. Wysocka, Valery Dzutsati, Tirthankar Bandyopadhyay, Laura\n  Condon, Sahil Garg", "title": "Building Models for Biopathway Dynamics Using Intrinsic Dimensionality\n  Analysis", "comments": "Presented in Santa Fe Complex Systems Summer School (CSSS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.IT math.IT q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task for many if not all the scientific domains is efficient\nknowledge integration, testing and codification. It is often solved with model\nconstruction in a controllable computational environment. In spite of that, the\nthroughput of in-silico simulation-based observations become similarly\nintractable for thorough analysis. This is especially the case in molecular\nbiology, which served as a subject for this study. In this project, we aimed to\ntest some approaches developed to deal with the curse of dimensionality. Among\nthese we found dimension reduction techniques especially appealing. They can be\nused to identify irrelevant variability and help to understand critical\nprocesses underlying high-dimensional datasets. Additionally, we subjected our\ndata sets to nonlinear time series analysis, as those are well established\nmethods for results comparison. To investigate the usefulness of dimension\nreduction methods, we decided to base our study on a concrete sample set. The\nexample was taken from the domain of systems biology concerning dynamic\nevolution of sub-cellular signaling. Particularly, the dataset relates to the\nyeast pheromone pathway and is studied in-silico with a stochastic model. The\nmodel reconstructs signal propagation stimulated by a mating pheromone. In the\npaper, we elaborate on the reason of multidimensional analysis problem in the\ncontext of molecular signaling, and next, we introduce the model of choice,\nsimulation details and obtained time series dynamics. A description of used\nmethods followed by a discussion of results and their biological interpretation\nfinalize the paper.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2018 23:40:44 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 13:13:15 GMT"}, {"version": "v3", "created": "Sat, 3 Nov 2018 03:33:57 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Wysocka", "Emilia M.", ""], ["Dzutsati", "Valery", ""], ["Bandyopadhyay", "Tirthankar", ""], ["Condon", "Laura", ""], ["Garg", "Sahil", ""]]}, {"id": "1804.11087", "submitter": "Genevieve Robin", "authors": "Fran\\c{c}ois Husson (IRMAR), Julie Josse (CMAP, XPOP), Balasubramanian\n  Narasimhan, Genevi\\`eve Robin (XPOP, CMAP)", "title": "Imputation of mixed data with multilevel singular value decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of large data sets offers new opportunities to better\nunderstand many processes. Yet, data accumulation often implies relaxing\nacquisition procedures or compounding diverse sources. As a consequence, such\ndata sets often contain mixed data, i.e. both quantitative and qualitative and\nmany missing values. Furthermore, aggregated data present a natural\n\\textit{multilevel} structure, where individuals or samples are nested within\ndifferent sites, such as countries or hospitals. Imputation of multilevel data\nhas therefore drawn some attention recently, but current solutions are not\ndesigned to handle mixed data, and suffer from important drawbacks such as\ntheir computational cost. In this article, we propose a single imputation\nmethod for multilevel data, which can be used to complete either quantitative,\ncategorical or mixed data. The method is based on multilevel singular value\ndecomposition (SVD), which consists in decomposing the variability of the data\ninto two components, the between and within groups variability, and performing\nSVD on both parts. We show on a simulation study that in comparison to\ncompetitors, the method has the great advantages of handling data sets of\nvarious size, and being computationally faster. Furthermore, it is the first so\nfar to handle mixed data. We apply the method to impute a medical data set\nresulting from the aggregation of several data sets coming from different\nhospitals. This application falls in the framework of a larger project on\nTrauma patients. To overcome obstacles associated to the aggregation of medical\ndata, we turn to distributed computation. The method is implemented in an R\npackage.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 09:07:13 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Husson", "Fran\u00e7ois", "", "IRMAR"], ["Josse", "Julie", "", "CMAP, XPOP"], ["Narasimhan", "Balasubramanian", "", "XPOP, CMAP"], ["Robin", "Genevi\u00e8ve", "", "XPOP, CMAP"]]}, {"id": "1804.11205", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu and Vahid Nekoukhou", "title": "On Bivariate Discrete Weibull Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Lee and Cha (2015, `On two generalized classes of discrete\nbivariate distributions', {\\it American Statistician}, 221 - 230) proposed two\ngeneral classes of discrete bivariate distributions. They have discussed some\ngeneral properties and some specific cases of their proposed distributions. In\nthis paper we have considered one model, namely bivariate discrete Weibull\ndistribution, which has not been considered in the literature yet. The proposed\nbivariate discrete Weibull distribution is a discrete analogue of the\nMarshall-Olkin bivariate Weibull distribution. We study various properties of\nthe proposed distribution and discuss its interesting physical interpretations.\nThe proposed model has four parameters, and because of that it is a very\nflexible distribution. The maximum likelihood estimators of the parameters\ncannot be obtained in closed forms, and we have proposed a very efficient\nnested EM algorithm which works quite well for discrete data. We have also\nproposed augmented Gibbs sampling procedure to compute Bayes estimates of the\nunknown parameters based on a very flexible set of priors. Two data sets have\nbeen analyzed to show how the proposed model and the method work in practice.\nWe will see that the performances are quite satisfactory. Finally, we conclude\nthe paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 18:50:07 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Kundu", "Debasis", ""], ["Nekoukhou", "Vahid", ""]]}, {"id": "1804.11224", "submitter": "Craig Wang", "authors": "Craig Wang and Reinhard Furrer", "title": "eggCounts: a Bayesian hierarchical toolkit to model faecal egg count\n  reductions", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a vignette for the R package eggCounts version 2.0. The package\nimplements a suite of Bayesian hierarchical models dealing with faecal egg\ncount reductions. The models are designed for a variety of practical\nsituations, including individual treatment efficacy, zero inflation, small\nsample size (less than 10) and potential outliers. The functions are intuitive\nto use and their output are easy to interpret, such that users are protected\nfrom being exposed to complex Bayesian hierarchical modelling tasks. In\naddition, the package includes plotting functions to display data and results\nin a visually appealing manner. The models are implemented in Stan modelling\nlanguage, which provides efficient sampling technique to obtain posterior\nsamples. This vignette briefly introduces different models, and provides a\nshort walk-through analysis with example data.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 14:16:27 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Wang", "Craig", ""], ["Furrer", "Reinhard", ""]]}]