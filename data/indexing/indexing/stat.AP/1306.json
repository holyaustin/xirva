[{"id": "1306.0071", "submitter": "Ulvi Yurtsever", "authors": "Caren Marzban, Raju Viswanathan, Ulvi Yurtsever", "title": "Life After Earth", "comments": "pdfLaTeX, 9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent study reported that there is evidence life may have originated prior\nto the formation of the Earth. That conclusion was based on a regression\nanalysis of a certain data set involving evolution of functional genome size\nacross major phyla. Here it is shown that if measurement errors and\n\"confidence\" intervals are taken into account, then the regression analysis of\nthe same data set leads to conclusions that allow for life to have appeared\nafter the formation of the Earth.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2013 03:52:10 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Marzban", "Caren", ""], ["Viswanathan", "Raju", ""], ["Yurtsever", "Ulvi", ""]]}, {"id": "1306.0267", "submitter": "Minh Tang", "authors": "Heng Wang and Minh Tang and Youngser Park and Carey E. Priebe", "title": "Locality statistics for anomaly detection in time series of graphs", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to detect change-points in a dynamic network or a time series of\ngraphs is an increasingly important task in many applications of the emerging\ndiscipline of graph signal processing. This paper formulates change-point\ndetection as a hypothesis testing problem in terms of a generative latent\nposition model, focusing on the special case of the Stochastic Block Model time\nseries. We analyze two classes of scan statistics, based on distinct underlying\nlocality statistics presented in the literature. Our main contribution is the\nderivation of the limiting distributions and power characteristics of the\ncompeting scan statistics. Performance is compared theoretically, on synthetic\ndata, and on the Enron email corpus. We demonstrate that both statistics are\nadmissible in one simple setting, while one of the statistics is inadmissible a\nsecond setting.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 01:07:53 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2013 16:53:22 GMT"}], "update_date": "2013-09-24", "authors_parsed": [["Wang", "Heng", ""], ["Tang", "Minh", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1306.0331", "submitter": "Ammar Mesloub", "authors": "Ammar Mesloub, Karim Abeb-Meraim and Adel Belouchrani", "title": "A new algorithm for complex non orthogonal joint diagonalization based\n  on Shear and Givens rotations", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2303947", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new algorithm to approximate non orthogonal joint\ndiagonalization (NOJD) of a set of complex matrices. This algorithm is based on\nthe Frobenius norm formulation of the JD problem and takes advantage from\ncombining Givens and Shear rotations to attempt the approximate joint\ndiagonalization (JD). It represents a non trivial generalization of the JDi\n(Joint Diagonalization) algorithm (Souloumiac 2009) to the complex case. The\nJDi is first slightly modified then generalized to the CJDi (i.e. Complex JDi)\nusing complex to real matrix transformation. Also, since several methods exist\nalready in the literature, we propose herein a brief overview of existing NOJD\nalgorithms then we provide an extensive comparative study to illustrate the\neffectiveness and stability of the CJDi w.r.t. various system parameters and\napplication contexts.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 09:00:28 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Mesloub", "Ammar", ""], ["Abeb-Meraim", "Karim", ""], ["Belouchrani", "Adel", ""]]}, {"id": "1306.0408", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Non-stationary Spatial Modelling with Applications to Spatial Prediction\n  of Precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-stationary spatial Gaussian random field (GRF) is described as the\nsolution of an inhomogeneous stochastic partial differential equation (SPDE),\nwhere the covariance structure of the GRF is controlled by the coefficients in\nthe SPDE. This allows for a flexible way to vary the covariance structure,\nwhere intuition about the resulting structure can be gained from the local\nbehaviour of the differential equation. Additionally, computations can be done\nwith computationally convenient Gaussian Markov random fields which approximate\nthe true GRFs. The model is applied to a dataset of annual precipitation in the\nconterminous US. The non-stationary model performs better than a stationary\nmodel measured with both CRPS and the logarithmic scoring rule.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 13:59:31 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1306.0413", "submitter": "Isabella Gollini", "authors": "Isabella Gollini, Binbin Lu, Martin Charlton, Christopher Brunsdon,\n  and Paul Harris", "title": "GWmodel: an R Package for Exploring Spatial Heterogeneity using\n  Geographically Weighted Models", "comments": "43 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial statistics is a growing discipline providing important analytical\ntechniques in a wide range of disciplines in the natural and social sciences.\nIn the R package GWmodel, we introduce techniques from a particular branch of\nspatial statistics, termed geographically weighted (GW) models. GW models suit\nsituations when data are not described well by some global model, but where\nthere are spatial regions where a suitably localised calibration provides a\nbetter description. The approach uses a moving window weighting technique,\nwhere localised models are found at target locations. Outputs are mapped to\nprovide a useful exploratory tool into the nature of the data spatial\nheterogeneity. GWmodel includes: GW summary statistics, GW principal components\nanalysis, GW regression, GW regression with a local ridge compensation, and GW\nregression for prediction; some of which are provided in basic and robust\nforms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2013 14:02:24 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 09:07:56 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Gollini", "Isabella", ""], ["Lu", "Binbin", ""], ["Charlton", "Martin", ""], ["Brunsdon", "Christopher", ""], ["Harris", "Paul", ""]]}, {"id": "1306.0938", "submitter": "Laszlo Korsos", "authors": "Laszlo F. Korsos", "title": "The Dirichlet Portfolio Model: Uncovering the Hidden Composition of\n  Hedge Fund Investments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hedge funds have long been viewed as a veritable \"black box\" of investing\nsince outsiders may never view the exact composition of portfolio holdings.\nTherefore, the ability to estimate an informative set of asset weights is\nhighly desirable for analysis. We present a compositional state space model for\nestimation of an investment portfolio's unobserved asset allocation weightings\non a set of candidate assets when the only observed information is the time\nseries of portfolio returns and the candidate asset returns. In this paper, we\nexhibit both sequential Monte Carlo numerical and conditionally Normal\nanalytical approaches to solve for estimates of the unobserved asset weight\ntime series. This methodology is motivated by the estimation of monthly asset\nclass weights on the aggregate hedge fund industry from 1996 to 2012.\nFurthermore, we show how to implement the results as predictive investment\nweightings in order to construct hedge fund replicating portfolios.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2013 22:40:00 GMT"}], "update_date": "2013-06-06", "authors_parsed": [["Korsos", "Laszlo F.", ""]]}, {"id": "1306.1110", "submitter": "Nicolas Oteiza Aguirre", "authors": "Carlos E. Laciana, Nicolas Oteiza Aguirre", "title": "An agent based multi-optional model for the diffusion of innovations", "comments": "This article was accepted for publication at Physica A. Publication\n  Reference: PHYSA-13-139R1", "journal-ref": null, "doi": "10.1016/j.physa.2013.09.046", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for the diffusion of several products competing in a\ncommon market based on the generalization of the Ising model of statiscal\nmechanics (Potts model). Using an agent based implementation, we analyze two\nproblems: (i) a three options case, i.e. to adopt a product A, a product B, or\nnon-adoption and (ii) a four options case, i.e. the adoption of product A,\nproduct B, both, or none. In the first case we analyze a launching strategy for\none of the two products, which delays its launching with the objective of\ncompeting with improvements. Market shares reached by each product are then\nestimated at market saturation. Finally, simulations are carried out with\nvarying degrees of social network topology, uncertainty, and population\nhomogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2013 14:12:55 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2013 15:29:42 GMT"}, {"version": "v3", "created": "Thu, 26 Sep 2013 19:14:11 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2013 12:56:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Laciana", "Carlos E.", ""], ["Aguirre", "Nicolas Oteiza", ""]]}, {"id": "1306.1271", "submitter": "Kevin Xu", "authors": "Kevin S. Xu", "title": "Predictability of social interactions", "comments": "Extended abstract selected as the winner of the 2013 International\n  Conference on Social Computing, Behavioral-Cultural Modeling, and Prediction\n  (SBP) Challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict social interactions between people has profound\napplications including targeted marketing and prediction of information\ndiffusion and disease propagation. Previous work has shown that the location of\nan individual at any given time is highly predictable. This study examines the\npredictability of social interactions between people to determine whether\ninteraction patterns are similarly predictable. I find that the locations and\ntimes of interactions for an individual are highly predictable; however, the\nother person the individual interacts with is less predictable. Furthermore, I\nshow that knowledge of the locations and times of interactions has almost no\neffect on the predictability of the other person. Finally I demonstrate that a\nsimple Markov chain model is able to achieve close to the upper bound in terms\nof predicting the next person with whom a given individual will interact.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2013 00:40:55 GMT"}], "update_date": "2013-06-07", "authors_parsed": [["Xu", "Kevin S.", ""]]}, {"id": "1306.1781", "submitter": "Panagiotis Nanos", "authors": "Panagiotis Nanos and Christian Schluter", "title": "The Composition of Wage Differentials between Migrants and Natives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We consider the role of unobservables, such as differences in search\nfrictions, reservation wages, and productivities for the explanation of wage\ndifferentials between migrants and natives. We disentangle these by estimating\nan empirical general equilibrium search model with on-the-job search due to\nBontemps, Robin, and van den Berg (1999) on segments of the labour market\ndefined by occupation, age, and nationality using a large scale German\nadministrative dataset.\n  The native-migrant wage differential is then decomposed into several parts,\nand we focus especially on the component that we label \"migrant effect\", being\nthe difference in wage offers between natives and migrants in the same\noccupation-age segment in firms of the same productivity. Counterfactual\ndecompositions of wage differentials allow us to identify and quantify their\ndrivers, thus explaining within a common framework what is often labelled the\nunexplained wage gap.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2013 17:21:39 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2013 19:39:35 GMT"}, {"version": "v3", "created": "Sun, 20 Oct 2013 13:48:24 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Nanos", "Panagiotis", ""], ["Schluter", "Christian", ""]]}, {"id": "1306.1904", "submitter": "Chris Oates", "authors": "Chris J Oates, Bryan T Hennessy, Yiling Lu, Gordon B Mills and Sach\n  Mukherjee", "title": "Network Inference Using Steady State Data and Goldbeter-Koshland\n  Kinetics", "comments": "Preprint", "journal-ref": "Bioinformatics (2012) 28(18):2342-2348", "doi": "10.1093/bioinformatics/bts459", "report-no": null, "categories": "stat.AP q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network inference approaches are widely used to shed light on regulatory\ninterplay between molecular players such as genes and proteins. Biochemical\nprocesses underlying networks of interest (e.g. gene regulatory or protein\nsignalling networks) are generally nonlinear. In many settings, knowledge is\navailable concerning relevant chemical kinetics. However, existing network\ninference methods for continuous, steady-state data are typically rooted in\nstatistical formulations, which do not exploit chemical kinetics to guide\ninference. Herein, we present an approach to network inference for steady-state\ndata that is rooted in non-linear descriptions of biochemical mechanism. We use\nequilibrium analysis of chemical kinetics to obtain functional forms that are\nin turn used to infer networks using steady-state data. The approach we propose\nis directly applicable to conventional steady-state gene expression or\nproteomic data and does not require knowledge of either network topology or any\nkinetic parameters. We illustrate the approach in the context of protein\nphosphorylation networks, using data simulated from a recent mechanistic model\nand proteomic data from cancer cell lines. In the former, the true network is\nknown and used for assessment, whereas in the latter, results are compared\nagainst known biochemistry. We find that the proposed methodology is more\neffective at estimating network topology than methods based on linear models.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 10:43:23 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Oates", "Chris J", ""], ["Hennessy", "Bryan T", ""], ["Lu", "Yiling", ""], ["Mills", "Gordon B", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1306.1927", "submitter": "Been Kim", "authors": "Been Kim and Cynthia Rudin", "title": "Learning About Meetings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most people participate in meetings almost every day, multiple times a day.\nThe study of meetings is important, but also challenging, as it requires an\nunderstanding of social signals and complex interpersonal dynamics. Our aim\nthis work is to use a data-driven approach to the science of meetings. We\nprovide tentative evidence that: i) it is possible to automatically detect when\nduring the meeting a key decision is taking place, from analyzing only the\nlocal dialogue acts, ii) there are common patterns in the way social dialogue\nacts are interspersed throughout a meeting, iii) at the time key decisions are\nmade, the amount of time left in the meeting can be predicted from the amount\nof time that has passed, iv) it is often possible to predict whether a proposal\nduring a meeting will be accepted or rejected based entirely on the language\n(the set of persuasive words) used by the speaker.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jun 2013 14:59:52 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Kim", "Been", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1306.2060", "submitter": "Eric Rowland", "authors": "Lara Pudwell and Eric Rowland", "title": "What's in YOUR wallet?", "comments": "10 pages; final version", "journal-ref": "The Mathematical Intelligencer 37 (2015) 54-60", "doi": "10.1007/s00283-015-9570-9", "report-no": null, "categories": "math.HO cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use Markov chains and numerical linear algebra -- and several CPU hours --\nto determine the expected number of coins in a person's possession under\ncertain conditions. We identify the spending strategy that results in the\nminimum possible expected number of coins, and we consider two other strategies\nthat are more realistic.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 20:47:25 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 10:27:01 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 00:21:31 GMT"}], "update_date": "2016-05-16", "authors_parsed": [["Pudwell", "Lara", ""], ["Rowland", "Eric", ""]]}, {"id": "1306.2062", "submitter": "Burcu Aydin", "authors": "Burcu Ayd{\\i}n and J.S. Marron", "title": "Analyzing Collaborative Forecast and Response Networks", "comments": "29 pages, 6 figures, 6 supplementary files", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative forecasting involves exchanging information on how much of an\nitem will be needed by a buyer and how much can be supplied by a seller or\nmanufacturer in a supply chain. This exchange allows parties to plan their\noperations based on the needs and limitations of their supply chain partner.\nThe success of this system critically depends on the healthy flow of\ninformation. This paper focuses on methods to easily analyze and visualize this\nprocess. To understand how the information travels on this network and how\nparties react to new information from their partners, this paper proposes a\nGaussian Graphical Model based method, and finds certain inefficiencies in the\nsystem. To simplify and better understand the update structure, a Continuum\nCanonical Correlation based method is proposed. The analytical tools introduced\nin this article are implemented as a part of a forecasting solution software\ndeveloped to aid the forecasting practice of a large company.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2013 21:52:28 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Ayd\u0131n", "Burcu", ""], ["Marron", "J. S.", ""]]}, {"id": "1306.2094", "submitter": "Si-Chi Chin Si-Chi Chin", "authors": "Kiyana Zolfaghar, Nele Verbiest, Jayshree Agarwal, Naren Meadem,\n  Si-Chi Chin, Senjuti Basu Roy, Ankur Teredesai, David Hazel, Paul Amoroso,\n  Lester Reed", "title": "Predicting Risk-of-Readmission for Congestive Heart Failure Patients: A\n  Multi-Layer Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitigating risk-of-readmission of Congestive Heart Failure (CHF) patients\nwithin 30 days of discharge is important because such readmissions are not only\nexpensive but also critical indicator of provider care and quality of\ntreatment. Accurately predicting the risk-of-readmission may allow hospitals to\nidentify high-risk patients and eventually improve quality of care by\nidentifying factors that contribute to such readmissions in many scenarios. In\nthis paper, we investigate the problem of predicting risk-of-readmission as a\nsupervised learning problem, using a multi-layer classification approach.\nEarlier contributions inadequately attempted to assess a risk value for 30 day\nreadmission by building a direct predictive model as opposed to our approach.\nWe first split the problem into various stages, (a) at risk in general (b) risk\nwithin 60 days (c) risk within 30 days, and then build suitable classifiers for\neach stage, thereby increasing the ability to accurately predict the risk using\nmultiple layers of decision. The advantage of our approach is that we can use\ndifferent classification models for the subtasks that are more suited for the\nrespective problems. Moreover, each of the subtasks can be solved using\ndifferent features and training data leading to a highly confident diagnosis or\nrisk compared to a one-shot single layer approach. An experimental evaluation\non actual hospital patient record data from Multicare Health Systems shows that\nour model is significantly better at predicting risk-of-readmission of CHF\npatients within 30 days after discharge compared to prior attempts.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 03:18:25 GMT"}], "update_date": "2013-06-11", "authors_parsed": [["Zolfaghar", "Kiyana", ""], ["Verbiest", "Nele", ""], ["Agarwal", "Jayshree", ""], ["Meadem", "Naren", ""], ["Chin", "Si-Chi", ""], ["Roy", "Senjuti Basu", ""], ["Teredesai", "Ankur", ""], ["Hazel", "David", ""], ["Amoroso", "Paul", ""], ["Reed", "Lester", ""]]}, {"id": "1306.2116", "submitter": "Francoise Pene", "authors": "Pierre Ailliot (LM), Francoise Pene (LM)", "title": "Consistency of the maximum likelihood estimate for Non-homogeneous\n  Markov-switching models", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many nonlinear time series models have been proposed in the last decades.\nAmong them, the models with regime switchings provide a class of versatile and\ninterpretable models which have received a particular attention in the\nliterature. In this paper, we consider a large family of such models which\ngeneralize the well known Markov-switching AutoRegressive (MS-AR) by allowing\nnon-homogeneous switching and encompass Threshold AutoRegressive (TAR) models.\nWe prove various theoretical results related to the stability of these models\nand the asymptotic properties of the Maximum Likelihood Estimates (MLE). The\nability of the model to catch complex nonlinearities is then illustrated on\nvarious time series.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 07:22:16 GMT"}, {"version": "v2", "created": "Sun, 18 May 2014 19:16:54 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Ailliot", "Pierre", "", "LM"], ["Pene", "Francoise", "", "LM"]]}, {"id": "1306.2250", "submitter": "Zhijian Wang", "authors": "Zhijian Wang", "title": "Cyclic motions in Dekel-Scotchmer Game Experiments", "comments": "7 Page, 3 Figure; keywords: experimental economics; angular momentum;\n  period by period transition; social motion; stochastic averaging method;\n  tumbling cycle", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TASP (Time Average Shapley Polygon, Bena{\\=\\i}m, Hofbauer and Hopkins,\n\\emph{Journal of Economic Theory}, 2009), as a novel evolutionary dynamics\nmodel, predicts that a game could converge to cycles instead of fix points\n(Nash equilibria). To verify TASP theory, using the four strategy\nDekel-Scotchmer games (Dekel and Scotchmer, \\emph{Journal of Economic Theory},\n1992), four experiments were conducted (Cason, Friedman and Hopkins,\n\\emph{Journal of Economic Theory}, 2010), in which, however, reported no\nevidence of cycles (Cason, Friedman and Hopkins, \\emph{The Review of Economic\nStudies}, 2013). We reanalysis the four experiment data by testing the\nstochastic averaging of angular momentum in period-by-period transitions of the\nsocial state. We find, the existence of persistent cycles in Dekel-Scotchmer\ngame can be confirmed. On the cycles, the predictions from evolutionary models\nhad been supported by the four experiments.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2013 17:05:32 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 07:38:10 GMT"}], "update_date": "2013-08-26", "authors_parsed": [["Wang", "Zhijian", ""]]}, {"id": "1306.2581", "submitter": "Eleftherios Kofidis", "authors": "Eleftherios Kofidis", "title": "Preamble-based Channel Estimation in FBMC/OQAM Systems: A Time-Domain\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filter bank-based multicarrier (FBMC) systems based on offset QAM (FBMC/OQAM)\nhave recently attracted increased interest in several applications due to their\nenhanced flexibility, higher spectral efficiency, and better spectral\ncontainment compared to conventional OFDM. They suffer, however, from an\ninter-carrier/inter-symbol interference that complicates signal processing\ntasks such as channel estimation. Most of the methods reported thus far rely on\nthe assumption of (almost) flat subchannels to more easily tackle this problem,\naddressing it in a way similar to OFDM. However, this assumption may be often\nquite inaccurate, due to the high freq. selectivity of the channel and/or the\nsmall number of subcarriers employed to cope with frequency dispersion in fast\nfading. In such cases, severe error floors are exhibited at medium to high SNR\nvalues, which cancel the advantage of FBMC over OFDM. Moreover, the existing\nmethods provide estimates of the subchannel responses, most commonly in the\nfrequency domain. The goal of this paper is to revisit this problem through an\nalternative formulation that focuses on the estimation of the channel impulse\nresponse itself and makes no assumption on the degree of frequency selectivity\nof the subchannels. The possible gains in estimation performance offered by\nsuch an approach are investigated through the design of optimal (in the MSE\nsense) preambles, of both the full and sparse types, and of the smallest\npossible duration of only one pilot FBMC symbol. Existing designs for flat\nsubchannels are then shown to result as special cases. Longer preambles,\nconsisting of two consecutive pilot FBMC symbols, are also analyzed. The\nsimulation results demonstrate significant improvements from the proposed\napproach for both mildly and highly frequency selective channels. Most notably,\nno error floors appear anymore over a quite wide range of SNR values.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2013 17:01:01 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 08:12:18 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Kofidis", "Eleftherios", ""]]}, {"id": "1306.3110", "submitter": "Remy Chicheportiche", "authors": "R\\'emy Chicheportiche and Jean-Philippe Bouchaud", "title": "Some applications of first-passage ideas to finance", "comments": "30 pages. To appear in the special volume \"First-Passage Phenomena\n  and Their Applications\", Eds. R. Metzler, G. Oshanin, S. Redner. World\n  Scientific (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in finance are related to first passage times. Among all of\nthem, we chose three on which we contributed personally. Our first example\nrelates Kolmogorov-Smirnov like goodness-of-fit tests, modified in such a way\nthat tail events and core events contribute equally to the test (in the\nstandard Kolmogorov-Smirnov, the tails contribute very little to the measure of\ngoodness-of-fit). We show that this problem can be mapped onto that of a random\nwalk inside moving walls. The second example is the optimal time to sell an\nasset (modelled as a random walk with drift) such that the sell time is as\nclose as possible to the time at which the asset reaches its maximum value. The\nlast example concerns optimal trading in the presence of transaction costs. In\nthis case, the optimal strategy is to wait until the predictor reaches (plus or\nminus) a threshold value before buying or selling. The value of this threshold\nis found by mapping the problem onto that of a random walk between two walls.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 14:03:55 GMT"}], "update_date": "2013-06-14", "authors_parsed": [["Chicheportiche", "R\u00e9my", ""], ["Bouchaud", "Jean-Philippe", ""]]}, {"id": "1306.3133", "submitter": "Piotr Sapiezynski", "authors": "Jakob Eg Larsen, Piotr Sapiezynski, Arkadiusz Stopczynski, Morten\n  Moerup, Rasmus Theodorsen", "title": "Crowds, Bluetooth, and Rock-n-Roll. Understanding Music Festival\n  Participant Behavior", "comments": "Presented at Sunbelt 2013 in Hamburg on May, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a study of sensing and analyzing an offline social\nnetwork of participants at a large-scale music festival (8 days, 130,000+\nparticipants). We place 33 fixed-location Bluetooth scanners in strategic spots\naround the festival area to discover Bluetooth-enabled mobile phones carried by\nthe participants, and thus collect spatio-temporal traces of their mobility and\ninteractions. We subsequently analyze the data on two levels. On the micro\nlevel, we run a community detection algorithm to reveal a variety of groups the\nfestival participants form. On the macro level, we employ an Infinite\nRelational Model (IRM) in order to recover the structure of the social network\nrelated to participants' music preferences. The obtained structure in the form\nof clusters of concerts and participants is then interpreted using\nmeta-information about music genres, band origins, stages, and dates of\nperformances. We show that most of the concerts clusters can be described by\none or more of the meta-features, effectively revealing preferences of\nparticipants (e.g. a cluster of US bands) and discuss the significance of the\nfindings and the potential and limitations of the used method. Finally, we\ndiscuss the possibility of employing the described method and techniques for\ncreating user-oriented applications and extending the sensing capabilities\nduring large-scale events by introducing user involvement.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 15:08:22 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2013 08:39:32 GMT"}], "update_date": "2013-06-17", "authors_parsed": [["Larsen", "Jakob Eg", ""], ["Sapiezynski", "Piotr", ""], ["Stopczynski", "Arkadiusz", ""], ["Moerup", "Morten", ""], ["Theodorsen", "Rasmus", ""]]}, {"id": "1306.3172", "submitter": "Tiancheng Li", "authors": "Tiancheng Li, Shudong Sun, Tariq Pervez Sattar", "title": "Adapting sample size in particle filters through KLD-resampling", "comments": "short letter of 2 pages, a Finishing Touch of appling KLD measure for\n  sample size adaption for particle filters. In Electronics Letters 2013", "journal-ref": "Electronics Letters,Volume: 49, Issue: 12, Pages 740-742 (2013)", "doi": "10.1049/el.2013.0233", "report-no": null, "categories": "stat.AP cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter provides an adaptive resampling method. It determines the number\nof particles to resample so that the Kullback-Leibler distance (KLD) between\ndistribution of particles before resampling and after resampling does not\nexceed a pre-specified error bound. The basis of the method is the same as\nFox's KLD-sampling but implemented differently. The KLD-sampling assumes that\nsamples are coming from the true posterior distribution and ignores any\nmismatch between the true and the proposal distribution. In contrast, we\nincorporate the KLD measure into the resampling in which the distribution of\ninterest is just the posterior distribution. That is to say, for sample size\nadjustment, it is more theoretically rigorous and practically flexible to\nmeasure the fit of the distribution represented by weighted particles based on\nKLD during resampling than in sampling. Simulations of target tracking\ndemonstrate the efficiency of our method.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2013 17:20:31 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Li", "Tiancheng", ""], ["Sun", "Shudong", ""], ["Sattar", "Tariq Pervez", ""]]}, {"id": "1306.3531", "submitter": "Argyn Kuketayev", "authors": "Argyn Kuketayev", "title": "The convergence of regional house prices in the USA in the context of\n  the stress testing of financial institutions", "comments": "38 pages, 7 tables, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  I studied the convergence of regional house prices to national prices in USA\nby analyzing time-series of house price indices of 9 Census Divisions. I found\nthe evidence of the convergence in some parts of the country using asymmetric\nunit root tests. The fact that the evidence of the convergence is not present\nin large parts of the country raises an issue of execution and interpretation\nof results of Federal Reserve Bank's annual stress testing of the US banking\nsystem.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 23:57:54 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Kuketayev", "Argyn", ""]]}, {"id": "1306.3636", "submitter": "Guillemette Marot", "authors": "Andrea Rau (GABI), Guillemette Marot (INRIA Lille - Nord Europe,\n  CERIM), Florence Jaffr\\'ezic (GABI)", "title": "Differential meta-analysis of RNA-seq data from multiple studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput sequencing is now regularly used for studies of the\ntranscriptome (RNA-seq), particularly for comparisons among experimental\nconditions. For the time being, a limited number of biological replicates are\ntypically considered in such experiments, leading to low detection power for\ndifferential expression. As their cost continues to decrease, it is likely that\nadditional follow-up studies will be conducted to re-address the same\nbiological question. We demonstrate how p-value combination techniques\npreviously used for microarray meta-analyses can be used for the differential\nanalysis of RNA-seq data from multiple related studies. These techniques are\ncompared to a negative binomial generalized linear model (GLM) including a\nfixed study effect on simulated data and real data on human melanoma cell\nlines. The GLM with fixed study effect performed well for low inter-study\nvariation and small numbers of studies, but was outperformed by the\nmeta-analysis methods for moderate to large inter-study variability and larger\nnumbers of studies. To conclude, the p-value combination techniques illustrated\nhere are a valuable tool to perform differential meta-analyses of RNA-seq data\nby appropriately accounting for biological and technical variability within\nstudies as well as additional study-specific effects. An R package metaRNASeq\nis available on the R Forge.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 07:45:21 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Rau", "Andrea", "", "GABI"], ["Marot", "Guillemette", "", "INRIA Lille - Nord Europe,\n  CERIM"], ["Jaffr\u00e9zic", "Florence", "", "GABI"]]}, {"id": "1306.3656", "submitter": "Alberto Hernando", "authors": "A. Hernando, R. Hernando and A. Plastino", "title": "Space-time correlations in urban sprawl", "comments": null, "journal-ref": "Partially published in J. R. Soc. Interface 6 February 2014 vol.\n  11 no. 91 20130930", "doi": "10.1098/rsif.2013.0930", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Understanding demographic and migrational patterns constitutes a great\nchallenge. Millions of individual decisions, motivated by economic, political,\ndemographic, rational, and/or emotional reasons underlie the high complexity of\ndemographic dynamics. Significant advances in quantitatively understanding such\ncomplexity have been registered in recent years, as those involving the growth\nof cities [Bettencourt LMA, Lobo J, Helbing D, Kuehnert C, West GB (2007)\nGrowth,. Innovation, Scaling, and the Pace of Life in Cities, Proc Natl Acad\nSci USA 104 (17) 7301-7306] but many fundamental issues still defy\ncomprehension. We present here compelling empirical evidence of a high level of\nregularity regarding time and spatial correlations in urban sprawl, unraveling\npatterns about the inertia in the growth of cities and their interaction with\neach other. By using one of the world's most exhaustive extant demographic data\nbasis ---that of the Spanish Government's Institute INE, with records covering\n111 years and (in 2011) 45 million people, distributed amongst more than 8000\npopulation nuclei--- we show that the inertia of city growth has a\ncharacteristic time of 15 years, and its interaction with the growth of other\ncities has a characteristic distance of 70 km. Distance is shown to be the main\nfactor that entangles two cities (a 60% of total correlations). We present a\nmathematical model for population flows that i) reproduces all these\nregularities and ii) can be used to predict the population-evolution of cities.\nThe power of our current social theories is thereby enhanced.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2013 12:58:34 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Hernando", "A.", ""], ["Hernando", "R.", ""], ["Plastino", "A.", ""]]}, {"id": "1306.3762", "submitter": "Alexander Kushpel", "authors": "Alexander Kushpel and Jeremy Levesley", "title": "L\\'{e}vy driven models and derivative pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general method for derivative pricing. This approach has its\nroots in Shannon's Information Theory. The notion of $\\lambda$-analyticity of\nL\\'{e}vy models is introduced on the basis of which new representations of the\npricing integral are obtained. It is shown that popular in applications\nL\\'{e}vy models are $\\lambda$-analytic. We apply these results to derive a\ngeneral algorithm for pricing of European call options.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 08:07:09 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Kushpel", "Alexander", ""], ["Levesley", "Jeremy", ""]]}, {"id": "1306.3768", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "\\v{S}\\'arka Hudecov\\'a and Michal Pe\\v{s}ta", "title": "Modeling Dependencies in Claims Reserving with GEE", "comments": "Submitted on April 9, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to the claims reserving problem is based on generalized\nlinear models (GLM). Within this framework, the claims in different origin and\ndevelopment years are assumed to be independent variables. If this assumption\nis violated, the classical techniques may provide incorrect predictions of the\nclaims reserves or even misleading estimates of the prediction error.\n  In this article, the application of generalized estimating equations (GEE)\nfor estimation of the claims reserves is shown. Claim triangles are handled as\npanel data, where claim amounts within the same accident year are dependent.\nSince the GEE allow to incorporate dependencies, various correlation structures\nare introduced and some practical recommendations are given.\n  Model selection criteria within the GEE reserving method are proposed.\nMoreover, an estimate for the mean square error of prediction for the claims\nreserves is derived in a nonstandard way and its advantages are discussed. Real\ndata examples are provided as an illustration of the potential benefits of the\npresented approach.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 08:40:34 GMT"}], "update_date": "2013-06-18", "authors_parsed": [["Hudecov\u00e1", "\u0160\u00e1rka", ""], ["Pe\u0161ta", "Michal", ""]]}, {"id": "1306.3927", "submitter": "Ciro D'Urso", "authors": "C. D'Urso", "title": "Cost Effectiveness Statistic: A Proposal To Take Into Account The\n  Patient Stratification Factors", "comments": "6 pages, 3 figures. Theoretical proposal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The solution here proposed can be used to conduct economic analysis in\nrandomized clinical trials. It is based on a statistical approach and aims at\ncalculating a revised version of the incremental costeffective ratio (ICER) in\norder to take into account the key factors that can influence the choice of\ntherapy causing confounding by indication. Let us take as an example a new\ntherapy to treat cancer being compared to an existing therapy with\neffectiveness taken as time to death. A challenging problem is that the ICER is\ndefined in terms of means over the entire treatment groups. It makes no\nprovision for stratification by groups of patients with differing risk of\ndeath. For example, for a fair and unbiased analysis, one would desire to\ncompare time to death in groups with similar life expectancy which would be\nimpacted by factors such as age, gender, disease severity, etc. The method we\ndecided to apply is borrowed by cluster analysis and aims at (i) discard any\noutliers in the set under analysis that may arise, (ii) identify groups (i.e.\nclusters) of patients with \"similar\" key factors.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2013 07:42:14 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2013 07:12:56 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["D'Urso", "C.", ""]]}, {"id": "1306.4036", "submitter": "Venkata Sriram Siddhardh (Sid) Nadendla", "authors": "V. Sriram Siddhardh (Sid) Nadendla, Yunghsiang S. Han, Pramod K.\n  Varshney", "title": "Distributed Inference with M-ary Quantized Data in the Presence of\n  Byzantine Attacks", "comments": "15 pages, 8 figures, 1 table, Revision submitted to IEEE Transactions\n  on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of distributed inference with M-ary quantized data at the sensors\nis investigated in the presence of Byzantine attacks. We assume that the\nattacker does not have knowledge about either the true state of the phenomenon\nof interest, or the quantization thresholds used at the sensors. Therefore, the\nByzantine nodes attack the inference network by modifying modifying the symbol\ncorresponding to the quantized data to one of the other M symbols in the\nquantization alphabet-set and transmitting the false symbol to the fusion\ncenter (FC). In this paper, we find the optimal Byzantine attack that blinds\nany distributed inference network. As the quantization alphabet size increases,\na tremendous improvement in the security performance of the distributed\ninference network is observed.\n  We also investigate the problem of distributed inference in the presence of\nresource-constrained Byzantine attacks. In particular, we focus our attention\non two problems: distributed detection and distributed estimation, when the\nByzantine attacker employs a highly-symmetric attack. For both the problems, we\nfind the optimal attack strategies employed by the attacker to maximally\ndegrade the performance of the inference network. A reputation-based scheme for\nidentifying malicious nodes is also presented as the network's strategy to\nmitigate the impact of Byzantine threats on the inference performance of the\ndistributed sensor network.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2013 22:39:51 GMT"}, {"version": "v2", "created": "Tue, 14 Jan 2014 14:34:09 GMT"}], "update_date": "2014-01-15", "authors_parsed": [["Siddhardh", "V. Sriram", "", "Sid"], ["Nadendla", "", ""], ["Han", "Yunghsiang S.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1306.4052", "submitter": "Aditya Vempaty", "authors": "Aditya Vempaty, Yunghsiang S. Han and Pramod K. Varshney", "title": "Target Localization in Wireless Sensor Networks using Error Correcting\n  Codes", "comments": "16 pages, 13 figures, to appear in IEEE Transactions on Information\n  Theory", "journal-ref": null, "doi": "10.1109/TIT.2013.2289859", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the task of target localization using quantized\ndata in Wireless Sensor Networks (WSNs). We propose an energy efficient\nlocalization scheme by modeling it as an iterative classification problem. We\ndesign coding based iterative approaches for target localization where at every\niteration, the Fusion Center (FC) solves an M-ary hypothesis testing problem\nand decides the Region of Interest (ROI) for the next iteration. The coding\nbased iterative approach works well even in the presence of Byzantine\n(malicious) sensors in the network. We further consider the effect of non-ideal\nchannels. We suggest the use of soft-decision decoding to compensate for the\nloss due to the presence of fading channels between the local sensors and the\nFC. We evaluate the performance of the proposed schemes in terms of the\nByzantine fault tolerance capability and probability of detection of the target\nregion. We also present performance bounds which help us in designing the\nsystem. We provide asymptotic analysis of the proposed schemes and show that\nthe schemes achieve perfect region detection irrespective of the noise variance\nwhen the number of sensors tends to infinity. Our numerical results show that\nthe proposed schemes provide a similar performance in terms of Mean Square\nError (MSE) as compared to the traditional Maximum Likelihood Estimation (MLE)\nbut are computationally much more efficient and are resilient to errors due to\nByzantines and non-ideal channels.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2013 00:46:36 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2013 04:36:17 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Vempaty", "Aditya", ""], ["Han", "Yunghsiang S.", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1306.4454", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann, Loet Leydesdorff, and Jian Wang", "title": "Which percentile-based approach should be preferred for calculating\n  normalized citation impact values? An empirical comparison of five approaches\n  including a newly developed citation-rank approach (P100)", "comments": "Accepted for publication in the Journal of Informetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Percentile-based approaches have been proposed as a non-parametric\nalternative to parametric central-tendency statistics to normalize observed\ncitation counts. Percentiles are based on an ordered set of citation counts in\na reference set, whereby the fraction of papers at or below the citation counts\nof a focal paper is used as an indicator for its relative citation impact in\nthe set. In this study, we pursue two related objectives: (1) although\ndifferent percentile-based approaches have been developed, an approach is\nhitherto missing that satisfies a number of criteria such as scaling of the\npercentile ranks from zero (all other papers perform better) to 100 (all other\npapers perform worse), and solving the problem with tied citation ranks\nunambiguously. We introduce a new citation-rank approach having these\nproperties, namely P100. (2) We compare the reliability of P100 empirically\nwith other percentile-based approaches, such as the approaches developed by the\nSCImago group, the Centre for Science and Technology Studies (CWTS), and\nThomson Reuters (InCites), using all papers published in 1980 in Thomson\nReuters Web of Science (WoS). How accurately can the different approaches\npredict the long-term citation impact in 2010 (in year 31) using citation\nimpact measured in previous time windows (years 1 to 30)? The comparison of the\napproaches shows that the method used by InCites overestimates citation impact\n(because of using the highest percentile rank when papers are assigned to more\nthan a single subject category) whereas the SCImago indicator shows higher\npower in predicting the long-term citation impact on the basis of citation\nrates in early years. Since the results show a disadvantage in this predictive\nability for P100 against the other approaches, there is still room for further\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 08:43:02 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2013 12:18:02 GMT"}], "update_date": "2013-09-18", "authors_parsed": [["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""], ["Wang", "Jian", ""]]}, {"id": "1306.4469", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "An empirical study to check the accuracy of approximating averages of\n  ratios using ratios of averages", "comments": "3 tables, 3 figures", "journal-ref": "Journal of Informetrics, 2013, 7(4), pp. 909-913", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a number of researchers a number of publications for each author is\nsimulated using the zeta distribution and then for each publication a number of\ncitations per publication simulated. Bootstrap confidence intervals indicate\nthat the difference between the average of ratios and the ratio of averages are\nnot significant, and there are no significant differences in the distributions\nin realistic problems when using the two-sample Kolmogorov-Smirnov test to\ncompare distributions. It was found that the log-logistic distribution which is\na general form for the ratio of two correlated Pareto random variables, give a\ngood fit to the estimated ratios.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 09:40:34 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2013 11:20:18 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1306.4529", "submitter": "Michal Pe\\v{s}ta PhD", "authors": "Michal Pe\\v{s}ta and Ostap Okhrin", "title": "Conditional Least Squares and Copulae in Claims Reserving for a Single\n  Line of Business", "comments": "Submitted on June 19, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals in non-life insurance is to estimate the claims reserve\ndistribution. A generalized time series model, that allows for modeling the\nconditional mean and variance of the claim amounts, is proposed for the claims\ndevelopment. On contrary to the classical stochastic reserving techniques, the\nnumber of model parameters does not depend on the number of development\nperiods, which leads to a more precise forecasting.\n  Moreover, the time series innovations for the consecutive claims are not\nconsidered to be independent anymore. Conditional least squares are used for\nmodel parameter estimation and consistency of such estimate is proved. Copula\napproach is used for modeling the dependence structure, which improves the\nprecision of the reserve distribution estimate as well.\n  Real data examples are provided as an illustration of the potential benefits\nof the presented approach.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 13:05:45 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Pe\u0161ta", "Michal", ""], ["Okhrin", "Ostap", ""]]}, {"id": "1306.4615", "submitter": "Soo-Heang Eo", "authors": "Soo-Heang Eo, Hyo Jeong Kang, Seung-Mo Hong, HyungJun Cho", "title": "K-Adaptive Partitioning for Survival Data, with an Application to Cancer\n  Staging", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In medical research, it is often needed to obtain subgroups with\nheterogeneous survivals, which have been predicted from a prognostic factor.\nFor this purpose, a binary split has often been used once or recursively;\nhowever, binary partitioning may not provide an optimal set of well separated\nsubgroups. We propose a multi-way partitioning algorithm, which divides the\ndata into K heterogeneous subgroups based on the information from a prognostic\nfactor. The resulting subgroups show significant differences in survival. Such\na multi-way partition is found by maximizing the minimum of the subgroup\npairwise test statistics. An optimal number of subgroups is determined by a\npermutation test. Our developed algorithm is compared with two binary recursive\npartitioning algorithms. In addition, its usefulness is demonstrated with a\nreal data of colorectal cancer cases from the Surveillance Epidemiology and End\nResults program. We have implemented our algorithm into an R package maps,\nwhich is freely available in the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2013 16:57:06 GMT"}, {"version": "v2", "created": "Mon, 17 Mar 2014 11:10:13 GMT"}, {"version": "v3", "created": "Sat, 1 Nov 2014 12:51:16 GMT"}], "update_date": "2014-11-04", "authors_parsed": [["Eo", "Soo-Heang", ""], ["Kang", "Hyo Jeong", ""], ["Hong", "Seung-Mo", ""], ["Cho", "HyungJun", ""]]}, {"id": "1306.5006", "submitter": "Antonio Punzo", "authors": "Luca Bagnato and Lucio De Capitani and Antonio Punzo", "title": "Improving the autodependogram using the Kulback-Leibler divergence", "comments": "We have decided to withdraw the paper due to a crucial error in\n  equation (9), that is in the definition of the p-value. This invalidates the\n  results reported into the manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autodependogram is a graphical device recently proposed in the literature\nto analyze autodependencies. It is defined computing the classical Pearson\nchi-square statistics of independence at various lags in order to point out the\npresence lag-depedencies. This paper proposes an improvement of this diagram\nobtained by substituting the chi-square statistics with an estimator of the\nKulback-Leibler divergence between the bivariate density of two delayed\nvariables and the product of their marginal distributions. A simulation study,\non well-established time series models, shows that this new autodependogram is\nmore powerful than the previous one. An application to financial data is also\nshown.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2013 21:43:18 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 13:48:13 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 10:33:14 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Bagnato", "Luca", ""], ["De Capitani", "Lucio", ""], ["Punzo", "Antonio", ""]]}, {"id": "1306.5158", "submitter": "Uwe Aickelin", "authors": "Galina Sherman, Peer-Olaf Siebers, Uwe Aickelin, David Menachof", "title": "Scenario Analysis, Decision Trees and Simulation for Cost Benefit\n  Analysis of the Cargo Screening Process", "comments": "International Workshop of Applied Modelling and Simulation (WAMS),\n  5-7 May, Buizos, Brasil, 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present our ideas for conducting a cost benefit analysis by\nusing three different methods: scenario analysis, decision trees and\nsimulation. Then we introduce our case study and examine these methods in a\nreal world situation. We show how these tools can be used and what the results\nare for each of them. Our aim is to conduct a comparison of these different\nprobabilistic methods of estimating costs for port security risk assessment\nstudies. Methodologically, we are trying to understand the limits of all the\ntools mentioned above by focusing on rare events.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 14:47:02 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Sherman", "Galina", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Menachof", "David", ""]]}, {"id": "1306.5186", "submitter": "Bertrand Roehner", "authors": "Bertrand M. Roehner", "title": "Incidence of the Bertillon and Gompertz effects on the outcome of\n  clinical trials", "comments": "9 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.physa.2014.07.059", "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accounts of medical trials provide very detailed information about the\npatients' health conditions. In contrast, only minimal data are usually given\nabout demographic factors. Yet, some of these factors can have a notable impact\non the overall death rate, thereby changing the outcome and conclusions of the\ntrial. This paper focuses on two of these variables. The first is marital\nstatus; this effect, which will be referred to as the Bertillon effect, may\nchange death rates by over 100%. The second is the age of the oldest patients;\nbecause of the exponential nature of Gompertz's law, changes in the\ndistribution of ages in the oldest age group can have dramatic consequences on\nthe overall number of deaths. It will be seen that randomization alone can\nhardly take care of these problems. Appropriate remedies are easy to formulate\nhowever. First, the marital status of patients as well as the age distribution\nof those over 65 should be documented for both study groups. Then, thanks to\nthese data and based on the Bertillon and Gompertz laws, it will become\npossible to perform appropriate corrections. Such corrections will notably\nimprove the reliability and accuracy of the trial's conclusions, especially for\ntrials which include a large proportion of elderly subjects.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 16:09:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Roehner", "Bertrand M.", ""]]}, {"id": "1306.5262", "submitter": "Fabrizio De Vico Fallani", "authors": "Fabrizio De Vico Fallani, Floriana Pichiorri, Giovanni Morone, Marco\n  Molinari, Fabio Babiloni, Febo Cincotti, Donatella Mattia", "title": "Multiscale Topological Properties Of Functional Brain Networks During\n  Motor Imagery After Stroke", "comments": "Neuroimage, accepted manuscript (unedited version) available online\n  19-June-2013", "journal-ref": null, "doi": "10.1016/j.neuroimage.2013.06.039", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, network analyses have been used to evaluate brain\nreorganization following stroke. However, many studies have often focused on\nsingle topological scales, leading to an incomplete model of how focal brain\nlesions affect multiple network properties simultaneously and how changes on\nsmaller scales influence those on larger scales. In an EEG-based experiment on\nthe performance of hand motor imagery (MI) in 20 patients with unilateral\nstroke, we observed that the anatomic lesion affects the functional brain\nnetwork on multiple levels. In the beta (13-30 Hz) frequency band, the MI of\nthe affected hand (Ahand) elicited a significantly lower smallworldness and\nlocal efficiency (Eloc) versus the unaffected hand (Uhand). Notably, the\nabnormal reduction in Eloc significantly depended on the increase in\ninterhemispheric connectivity, which was in turn determined primarily by the\nrise in regional connectivity in the parieto-occipital sites of the affected\nhemisphere. Further, in contrast to the Uhand MI, in which significantly high\nconnectivity was observed for the contralateral sensorimotor regions of the\nunaffected hemisphere, the regions that increased in connection during the\nAhand MI lay in the frontal and parietal regions of the contralaterally\naffected hemisphere. Finally, the overall sensorimotor function of our\npatients, as measured by Fugl-Meyer Assessment (FMA) index, was significantly\npredicted by the connectivity of their affected hemisphere. These results\nincrease our understanding of stroke-induced alterations in functional brain\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2013 22:02:49 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Fallani", "Fabrizio De Vico", ""], ["Pichiorri", "Floriana", ""], ["Morone", "Giovanni", ""], ["Molinari", "Marco", ""], ["Babiloni", "Fabio", ""], ["Cincotti", "Febo", ""], ["Mattia", "Donatella", ""]]}, {"id": "1306.5374", "submitter": "AbdoulAhad Validi", "authors": "AbdoulAhad Validi", "title": "Low-Rank Separated Representation Surrogates of High-Dimensional\n  Stochastic Functions: Application in Bayesian Inference", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2013.12.024", "report-no": null, "categories": "physics.data-an math-ph math.MP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study introduces a non-intrusive approach in the context of low-rank\nseparated representation to construct a surrogate of high-dimensional\nstochastic functions, e.g., PDEs/ODEs, in order to decrease the computational\ncost of Markov Chain Monte Carlo simulations in Bayesian inference. The\nsurrogate model is constructed via a regularized alternative least-square\nregression with Tikhonov regularization using a roughening matrix computing the\ngradient of the solution, in conjunction with a perturbation-based error\nindicator to detect optimal model complexities. The model approximates a vector\nof a continuous solution at discrete values of a physical variable. The\nrequired number of random realizations to achieve a successful approximation\nlinearly depends on the function dimensionality. The computational cost of the\nmodel construction is quadratic in the number of random inputs, which\npotentially tackles the curse of dimensionality in high-dimensional stochastic\nfunctions. Furthermore, this vector valued separated representation-based\nmodel, in comparison to the available scalar-valued case, leads to a\nsignificant reduction in the cost of approximation by an order of magnitude\nequal to the vector size. The performance of the method is studied through its\napplication to three numerical examples including a 41-dimensional elliptic PDE\nand a 21-dimensional cavity flow.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 05:09:05 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Validi", "AbdoulAhad", ""]]}, {"id": "1306.5417", "submitter": "Eyal Neuman", "authors": "Ilya Gertsbakh, Eyal Neuman, Radislav Vaisman", "title": "Monte Carlo for estimating exponential convolution", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we study the numerical stability problem that may take place\nwhen calculating the cumulative distribution function of the {\\it\nHypoexponential} random variable. This computation is extensively used during\nthe execution of Monte Carlo network reliability estimation algorithms. In\nspite of the fact that analytical formulas are available, they can be unstable\nin practice. This instability occurs frequently when estimating very small\nfailure probabilities $(10^{-30}-10^{-40})$ that can happen for example while\nestimating the unreliability of telecommunication systems. In order to address\nthis problem, we propose a simple unbiased estimation algorithm that is capable\nof handling a large number of variables. We show that the proposed estimator\nhas a bounded relative error and that it compares favorably with other existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jun 2013 14:29:37 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2013 05:18:05 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Gertsbakh", "Ilya", ""], ["Neuman", "Eyal", ""], ["Vaisman", "Radislav", ""]]}, {"id": "1306.5524", "submitter": "Haochang Shou", "authors": "Haochang Shou, Russell T. Shinohara, Han Liu, Daniel S. Reich and\n  Ciprian M. Crainiceanu", "title": "Soft Null Hypotheses: A Case Study of Image Enhancement Detection in\n  Brain Lesions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by a study of a population of multiple sclerosis (MS)\npatients using dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI)\nto identify active brain lesions. At each visit, a contrast agent is\nadministered intravenously to a subject and a series of images is acquired to\nreveal the location and activity of MS lesions within the brain. Our goal is to\nidentify and quantify lesion enhancement location at the subject level and\nlesion enhancement patterns at the population level. With this example, we aim\nto address the difficult problem of transforming a qualitative scientific null\nhypothesis, such as \"this voxel does not enhance\", to a well-defined and\nnumerically testable null hypothesis based on existing data. We call the\nprocedure \"soft null hypothesis\" testing as opposed to the standard \"hard null\nhypothesis\" testing. This problem is fundamentally different from: 1) testing\nwhen a quantitative null hypothesis is given; 2) clustering using a mixture\ndistribution; or 3) identifying a reasonable threshold with a parametric null\nassumption. We analyze a total of 20 subjects scanned at 63 visits (~30Gb), the\nlargest population of such clinical brain images.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 07:26:23 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Shou", "Haochang", ""], ["Shinohara", "Russell T.", ""], ["Liu", "Han", ""], ["Reich", "Daniel S.", ""], ["Crainiceanu", "Ciprian M.", ""]]}, {"id": "1306.5630", "submitter": "Thomas Toulias Dr.", "authors": "Christos P. Kitsos, Nikolaos K. Tavoularis, Thomas L. Toulias, George\n  Lolas", "title": "Statistical Approaches for Modelling Cancer Bioassays", "comments": "30 pages, 14 figures, ICCRA3 Proceedings (2009), C.P. Kitsos, C.\n  Caroni (Eds.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the possible ways to analyse the data, adopting a matrix\nnotation, so often used in Bioassays. The paper also reviews the Multistage\nModels (MM). The MM class of models is applied for extrapolation, to the region\nof Low-Dose. The effect of covariates in experimental carcinogenesis is\nintroduced and the relative efficiency is evaluated. Certainly the discussed\ncase was refereed to uncorrelated covariates and therefore an open problem\nmight be the multicollinear predictive covariates.Various nonlinear models are\ndiscussed, giving more emphasis on the Michaelis-Menten and the Fisher's\ninformation for them is discussed\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 13:59:49 GMT"}], "update_date": "2013-06-25", "authors_parsed": [["Kitsos", "Christos P.", ""], ["Tavoularis", "Nikolaos K.", ""], ["Toulias", "Thomas L.", ""], ["Lolas", "George", ""]]}, {"id": "1306.5793", "submitter": "Michael Kallitsis", "authors": "Michael Kallitsis, Stilian Stoev, George Michailidis", "title": "A State-Space Approach for Optimal Traffic Monitoring via Network Flow\n  Sampling", "comments": "preliminary work, short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.NI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustness and integrity of IP networks require efficient tools for\ntraffic monitoring and analysis, which scale well with traffic volume and\nnetwork size. We address the problem of optimal large-scale flow monitoring of\ncomputer networks under resource constraints. We propose a stochastic\noptimization framework where traffic measurements are done by exploiting the\nspatial (across network links) and temporal relationship of traffic flows.\nSpecifically, given the network topology, the state-space characterization of\nnetwork flows and sampling constraints at each monitoring station, we seek an\noptimal packet sampling strategy that yields the best traffic volume estimation\nfor all flows of the network. The optimal sampling design is the result of a\nconcave minimization problem; then, Kalman filtering is employed to yield a\nsequence of traffic estimates for each network flow. We evaluate our algorithm\nusing real-world Internet2 data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2013 21:58:30 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Kallitsis", "Michael", ""], ["Stoev", "Stilian", ""], ["Michailidis", "George", ""]]}, {"id": "1306.5993", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly, Jeffrey J. Early", "title": "Frequency-Domain Stochastic Modeling of Stationary Bivariate or\n  Complex-Valued Signals", "comments": "To appear in IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, 2017", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are three equivalent ways of representing two jointly observed\nreal-valued signals: as a bivariate vector signal, as a single complex-valued\nsignal, or as two analytic signals known as the rotary components. Each\nrepresentation has unique advantages depending on the system of interest and\nthe application goals. In this paper we provide a joint framework for all three\nrepresentations in the context of frequency-domain stochastic modeling. This\nframework allows us to extend many established statistical procedures for\nbivariate vector time series to complex-valued and rotary representations.\nThese include procedures for parametrically modeling signal coherence,\nestimating model parameters using the Whittle likelihood, performing\nsemi-parametric modeling, and choosing between classes of nested models using\nmodel choice. We also provide a new method of testing for impropriety in\ncomplex-valued signals, which tests for noncircular or anisotropic second-order\nstatistical structure when the signal is represented in the complex plane.\nFinally, we demonstrate the usefulness of our methodology in capturing the\nanisotropic structure of signals observed from fluid dynamic simulations of\nturbulence.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 15:15:06 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2015 06:59:25 GMT"}, {"version": "v3", "created": "Sun, 22 May 2016 00:48:38 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 13:04:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "1306.5995", "submitter": "Cinzia Carota", "authors": "Cinzia Carota, Maurizio Filippone, Roberto Leombruni, Silvia Polettini", "title": "Bayesian nonparametric disclosure risk estimation via mixed effects\n  log-linear models", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS807 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 525-546", "doi": "10.1214/15-AOAS807", "report-no": "IMS-AOAS-AOAS807", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical agencies and other institutions collect data under the promise to\nprotect the confidentiality of respondents. When releasing microdata samples,\nthe risk that records can be identified must be assessed. To this aim, a widely\nadopted approach is to isolate categorical variables key to the identification\nand analyze multi-way contingency tables of such variables. Common disclosure\nrisk measures focus on sample unique cells in these tables and adopt parametric\nlog-linear models as the standard statistical tools for the problem. Such\nmodels often have to deal with large and extremely sparse tables that pose a\nnumber of challenges to risk estimation. This paper proposes to overcome these\nproblems by studying nonparametric alternatives based on Dirichlet process\nrandom effects. The main finding is that the inclusion of such random effects\nallows us to reduce considerably the number of fixed effects required to\nachieve reliable risk estimates. This is studied on applications to real data,\nsuggesting, in particular, that our mixed models with main effects only produce\nroughly equivalent estimates compared to the all two-way interactions models,\nand are effective in defusing potential shortcomings of traditional log-linear\nmodels. This paper adopts a fully Bayesian approach that accounts for all\nsources of uncertainty, including that about the population frequencies, and\nsupplies unconditional (posterior) variances and credible intervals.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 15:19:53 GMT"}, {"version": "v2", "created": "Sun, 12 Oct 2014 21:57:08 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2015 07:49:22 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Carota", "Cinzia", ""], ["Filippone", "Maurizio", ""], ["Leombruni", "Roberto", ""], ["Polettini", "Silvia", ""]]}, {"id": "1306.6103", "submitter": "Babak Shahbaba", "authors": "Babak Shahbaba, Bo Zhou, Shiwei Lan, Hernando Ombao, David Moorman,\n  and Sam Behseta", "title": "A Semiparametric Bayesian Model for Detecting Synchrony Among Multiple\n  Neurons", "comments": null, "journal-ref": "Neural Computation, September 2014, Vol. 26, No. 9, Pages\n  2025-2051", "doi": "10.1162/NECO_a_00631", "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable semiparametric Bayesian model to capture dependencies\namong multiple neurons by detecting their co-firing (possibly with some lag\ntime) patterns over time. After discretizing time so there is at most one spike\nat each interval, the resulting sequence of 1's (spike) and 0's (silence) for\neach neuron is modeled using the logistic function of a continuous latent\nvariable with a Gaussian process prior. For multiple neurons, the corresponding\nmarginal distributions are coupled to their joint probability distribution\nusing a parametric copula model. The advantages of our approach are as follows:\nthe nonparametric component (i.e., the Gaussian process model) provides a\nflexible framework for modeling the underlying firing rates; the parametric\ncomponent (i.e., the copula model) allows us to make inference regarding both\ncontemporaneous and lagged relationships among neurons; using the copula model,\nwe construct multivariate probabilistic models by separating the modeling of\nunivariate marginal distributions from the modeling of dependence structure\namong variables; our method is easy to implement using a computationally\nefficient sampling algorithm that can be easily extended to high dimensional\nproblems. Using simulated data, we show that our approach could correctly\ncapture temporal dependencies in firing rates and identify synchronous neurons.\nWe also apply our model to spike train data obtained from prefrontal cortical\nareas in rat's brain.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2013 23:15:23 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 22:01:43 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Shahbaba", "Babak", ""], ["Zhou", "Bo", ""], ["Lan", "Shiwei", ""], ["Ombao", "Hernando", ""], ["Moorman", "David", ""], ["Behseta", "Sam", ""]]}, {"id": "1306.6111", "submitter": "David Darmon", "authors": "David Darmon, Jared Sylvester, Michelle Girvan, William Rand", "title": "Understanding the Predictive Power of Computational Mechanics and Echo\n  State Networks in Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a large amount of interest in understanding users of social media in\norder to predict their behavior in this space. Despite this interest, user\npredictability in social media is not well-understood. To examine this\nquestion, we consider a network of fifteen thousand users on Twitter over a\nseven week period. We apply two contrasting modeling paradigms: computational\nmechanics and echo state networks. Both methods attempt to model the behavior\nof users on the basis of their past behavior. We demonstrate that the behavior\nof users on Twitter can be well-modeled as processes with self-feedback. We\nfind that the two modeling approaches perform very similarly for most users,\nbut that they differ in performance on a small subset of the users. By\nexploring the properties of these performance-differentiated users, we\nhighlight the challenges faced in applying predictive models to dynamic social\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 00:58:39 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 20:13:27 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Darmon", "David", ""], ["Sylvester", "Jared", ""], ["Girvan", "Michelle", ""], ["Rand", "William", ""]]}, {"id": "1306.6137", "submitter": "Prathamesh Muzumdar", "authors": "Prathamesh Muzumdar", "title": "Effects Of Zoning On Housing Option Value", "comments": "Regression analysis; Hedonic Model; zoning patterns; physical value;\n  assessed value; multicollinearity", "journal-ref": "Journal of Business & Economics Research (2011), Volume 9, Number\n  5, 41- 48", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research explores the subject of zoning effect on price value of a house\nin a certain designated zone. Zoning is defined as a land under planned use.\nThis results in price value of the land. As the area of the land decreases an\nincrease in price occurs. This research does not specify the magnitude of a\ncertain unit by which a housing price will increase or decrease but it\nestimates the housing option value. This research helps the policy makers to\nimprove long term policy decisions. This research will showcase zoning effects\non housing option value for the specific zones in the city of Normal. The\nobservations are taken for different parcels of total neighborhoods in the city\nof Normal. This research will use hedonic model for conducting a regression\nanalysis on the subject. The hypothesis taken into consideration before\nstarting the research is to prove that more 50 percent variation in the housing\nprice is affected by the zoning characteristics. This research will try to\nprove the hypothesis by comparing the assessed values with the predicted values\nfrom the model.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 05:43:46 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Muzumdar", "Prathamesh", ""]]}, {"id": "1306.6281", "submitter": "Zachary Harmany", "authors": "Zachary T. Harmany, Roummel F. Marcia, Rebecca M. Willett", "title": "Compressive Coded Aperture Keyed Exposure Imaging with Optical Flow\n  Reconstruction", "comments": "13 pages, 4 figures, Submitted to IEEE Transactions on Image\n  Processing. arXiv admin note: substantial text overlap with arXiv:1111.7247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a coded aperture and keyed exposure approach to\ncompressive video measurement which admits a small physical platform, high\nphoton efficiency, high temporal resolution, and fast reconstruction\nalgorithms. The proposed projections satisfy the Restricted Isometry Property\n(RIP), and hence compressed sensing theory provides theoretical guarantees on\nthe video reconstruction quality. Moreover, the projections can be easily\nimplemented using existing optical elements such as spatial light modulators\n(SLMs). We extend these coded mask designs to novel dual-scale masks (DSMs)\nwhich enable the recovery of a coarse-resolution estimate of the scene with\nnegligible computational cost. We develop fast numerical algorithms which\nutilize both temporal correlations and optical flow in the video sequence as\nwell as the innovative structure of the projections. Our numerical experiments\ndemonstrate the efficacy of the proposed approach on short-wave infrared data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2013 15:53:55 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Harmany", "Zachary T.", ""], ["Marcia", "Roummel F.", ""], ["Willett", "Rebecca M.", ""]]}, {"id": "1306.6479", "submitter": "Dimitris Rizopoulos", "authors": "Dimitris Rizopoulos, Magdalena Murawska, Eleni-Rosalina Andrinopoulou,\n  Geert Molenberghs, Johanna J.M. Takkenberg and Emmanuel Lesaffre", "title": "Dynamic Predictions with Time-Dependent Covariates in Survival Analysis\n  using Joint Modeling and Landmarking", "comments": "34 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:1303.2797", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key question in clinical practice is accurate prediction of patient\nprognosis. To this end, nowadays, physicians have at their disposal a variety\nof tests and biomarkers to aid them in optimizing medical care. These tests are\noften performed on a regular basis in order to closely follow the progression\nof the disease. In this setting it is of medical interest to optimally utilize\nthe recorded information and provide medically-relevant summary measures, such\nas survival probabilities, that will aid in decision making. In this work we\npresent and compare two statistical techniques that provide dynamically-updated\nestimates of survival probabilities, namely landmark analysis and joint models\nfor longitudinal and time-to-event data. Special attention is given to the\nfunctional form linking the longitudinal and event time processes, and to\nmeasures of discrimination and calibration in the context of dynamic\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 12:35:38 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Rizopoulos", "Dimitris", ""], ["Murawska", "Magdalena", ""], ["Andrinopoulou", "Eleni-Rosalina", ""], ["Molenberghs", "Geert", ""], ["Takkenberg", "Johanna J. M.", ""], ["Lesaffre", "Emmanuel", ""]]}, {"id": "1306.6510", "submitter": "Yipeng Liu Dr.", "authors": "Yipeng Liu, Maarten De Vos, Ivan Gligorijevic, Vladimir Matic, Yuqian\n  Li, and Sabine Van Huffel", "title": "Multi-Structural Signal Recovery for Biomedical Compressive Sensing", "comments": "29 pages, 20 figures, accepted by IEEE Transactions on Biomedical\n  Engineering. Online first version:\n  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6519288&tag=1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing has shown significant promise in biomedical fields. It\nreconstructs a signal from sub-Nyquist random linear measurements. Classical\nmethods only exploit the sparsity in one domain. A lot of biomedical signals\nhave additional structures, such as multi-sparsity in different domains,\npiecewise smoothness, low rank, etc. We propose a framework to exploit all the\navailable structure information. A new convex programming problem is generated\nwith multiple convex structure-inducing constraints and the linear measurement\nfitting constraint. With additional a priori information for solving the\nunderdetermined system, the signal recovery performance can be improved. In\nnumerical experiments, we compare the proposed method with classical methods.\nBoth simulated data and real-life biomedical data are used. Results show that\nthe newly proposed method achieves better reconstruction accuracy performance\nin term of both L1 and L2 errors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 14:26:44 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Liu", "Yipeng", ""], ["De Vos", "Maarten", ""], ["Gligorijevic", "Ivan", ""], ["Matic", "Vladimir", ""], ["Li", "Yuqian", ""], ["Van Huffel", "Sabine", ""]]}, {"id": "1306.6677", "submitter": "Berk Ustun Berk Ustun", "authors": "Berk Ustun, Stefano Trac\\`a, Cynthia Rudin", "title": "Supersparse Linear Integer Models for Interpretable Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring systems are classification models that only require users to add,\nsubtract and multiply a few meaningful numbers to make a prediction. These\nmodels are often used because they are practical and interpretable. In this\npaper, we introduce an off-the-shelf tool to create scoring systems that both\naccurate and interpretable, known as a Supersparse Linear Integer Model (SLIM).\nSLIM is a discrete optimization problem that minimizes the 0-1 loss to\nencourage a high level of accuracy, regularizes the L0-norm to encourage a high\nlevel of sparsity, and constrains coefficients to a set of interpretable\nvalues. We illustrate the practical and interpretable nature of SLIM scoring\nsystems through applications in medicine and criminology, and show that they\nare are accurate and sparse in comparison to state-of-the-art classification\nmodels using numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2013 22:42:01 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 05:08:21 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2013 03:02:27 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2013 04:11:29 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2013 00:38:21 GMT"}, {"version": "v6", "created": "Fri, 11 Apr 2014 03:16:54 GMT"}], "update_date": "2014-04-14", "authors_parsed": [["Ustun", "Berk", ""], ["Trac\u00e0", "Stefano", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1306.6703", "submitter": "Shinsuke Koyama", "authors": "Shinsuke Koyama", "title": "Temporal fluctuation scaling in nonstationary counting processes", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fluctuation scaling law has universally been observed in a wide variety\nof phenomena. For counting processes describing the number of events occurred\nduring time intervals, it is expressed as a power function relationship between\nthe variance and the mean of the event count per unit time, the characteristic\nexponent of which is obtained theoretically in the limit of long duration of\ncounting windows. Here I show that the scaling law effectively appears even in\na short timescale in which only a few events occur. Consequently, the counting\nstatistics of nonstationary event sequences are shown to exhibit the scaling\nlaw as well as the dynamics at temporal resolution of this timescale. I also\npropose a method to extract in a systematic manner the characteristic scaling\nexponent from nonstationary data.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2013 03:05:26 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Koyama", "Shinsuke", ""]]}]