[{"id": "2105.00031", "submitter": "Diego Nascimento", "authors": "Diego C Nascimento, Pedro Luiz Ramos, David Elal-Olivero, Milton\n  Cortes-Araya, Francisco Louzada", "title": "Generalizing the normality: a novel towards different estimation methods\n  for skewed information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normality is the most often mathematical supposition used in data modeling.\nNonetheless, even based on the law of large numbers (LLN), normality is a\nstrong presumption given that the presence of asymmetry and multi-modality in\nreal-world problems is expected. Thus, a flexible modification in the Normal\ndistribution proposed by Elal-Olivero [12] adds a skewness parameter, called\nAlpha-skew Normal (ASN) distribution, enabling bimodality and fat-tail, if\nneeded, although sometimes not trivial to estimate this third parameter\n(regardless of the location and scale). This work analyzed seven different\nstatistical inferential methods towards the ASNdistribution on synthetic data\nand historical data of water flux from 21 rivers (channels) in the Atacama\nregion. Moreover, the contribution of this paper is related to the probability\nestimation surrounding the rivers' flux level in Copiapo city neighborhood, the\nmost important economic city of the third Chilean region, and known to be\nlocated in one of the driest areas on Earth, besides the North and the South\nPole\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:19:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nascimento", "Diego C", ""], ["Ramos", "Pedro Luiz", ""], ["Elal-Olivero", "David", ""], ["Cortes-Araya", "Milton", ""], ["Louzada", "Francisco", ""]]}, {"id": "2105.00211", "submitter": "Emmanuel Ramasso", "authors": "Pablo Juesas, Emmanuel Ramasso, S\\'ebastien Drujont, Vincent Placet", "title": "Autoregressive Hidden Markov Models with partial knowledge on latent\n  space applied to aero-engines prognostics", "comments": null, "journal-ref": "European Conference of the PHM Society 2016, selected for extended\n  version in IJPHM", "doi": "10.36001/phme.2016.v3i1.1642", "report-no": "hal-02131233", "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [This paper was initially published in PHME conference in 2016, selected for\nfurther publication in International Journal of Prognostics and Health\nManagement.]\n  This paper describes an Autoregressive Partially-hidden Markov model (ARPHMM)\nfor fault detection and prognostics of equipments based on sensors' data. It is\na particular dynamic Bayesian network that allows to represent the dynamics of\na system by means of a Hidden Markov Model (HMM) and an autoregressive (AR)\nprocess. The Markov chain assumes that the system is switching back and forth\nbetween internal states while the AR process ensures a temporal coherence on\nsensor measurements. A sound learning procedure of standard ARHMM based on\nmaximum likelihood allows to iteratively estimate all parameters\nsimultaneously. This paper suggests a modification of the learning procedure\nconsidering that one may have prior knowledge about the structure which becomes\npartially hidden. The integration of the prior is based on the Theory of\nWeighted Distributions which is compatible with the Expectation-Maximization\nalgorithm in the sense that the convergence properties are still satisfied. We\nshow how to apply this model to estimate the remaining useful life based on\nhealth indicators. The autoregressive parameters can indeed be used for\nprediction while the latent structure can be used to get information about the\ndegradation level. The interest of the proposed method for prognostics and\nhealth assessment is demonstrated on CMAPSS datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 10:23:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Juesas", "Pablo", ""], ["Ramasso", "Emmanuel", ""], ["Drujont", "S\u00e9bastien", ""], ["Placet", "Vincent", ""]]}, {"id": "2105.00224", "submitter": "Debasis Kundu Professor", "authors": "Debashis Samanta and Debasis Kundu", "title": "Bayesian Inference of a Dependent Competing Risk Data", "comments": "26 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of competing risks data plays an important role in the lifetime data\nanalysis. Recently Feizjavadian and Hashemi (Computational Statistics and Data\nAnalysis, vol. 82, 19-34, 2015) provided a classical inference of a competing\nrisks data set using four-parameter Marshall-Olkin bivariate Weibull\ndistribution when the failure of an unit at a particular time point can happen\ndue to more than one cause. The aim of this paper is to provide the Bayesian\nanalysis of the same model based on a very flexible Gamma-Dirichlet prior on\nthe scale parameters. It is observed that the Bayesian inference has certain\nadvantages over the classical inference in this case. We provide the Bayes\nestimates of the unknown parameters and the associated highest posterior\ndensity credible intervals based on Gibbs sampling technique. We further\nconsider the Bayesian inference of the model parameters assuming partially\nordered Gamma-Dirichlet prior on the scale parameters when one cause is more\nsevere than the other cause. We have extended the results for different\ncensoring schemes also.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:39:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Samanta", "Debashis", ""], ["Kundu", "Debasis", ""]]}, {"id": "2105.00230", "submitter": "Avik Kumar Das", "authors": "Avik Kumar Das, Chrisopher K. Y. Leung, and Kai Tai Wan", "title": "Application of Deep Convolutional Neural Networks for automated and\n  rapid identification and characterization of thin cracks in SHCCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Previous research has showcased that the characterization of surface cracks\nis one of the key steps towards understanding the durability of strain\nhardening cementitious composites (SHCCs). Under laboratory conditions, surface\ncrack statistics can be obtained from images of specimen surfaces through\nmanual inspection or image processing techniques. Since these techniques\nrequire optimal lighting conditions, proper surface treatment, and prior\n(manual) selection of the correct region for proper inference, they are\nstrenuous and time-consuming. Through this work, we explored and tailored deep\nconvolutional networks (DCCNs) for the rapid characterization of cracks in SHCC\nfrom various kinds of photographs. The results from the controlled study\nsuggest that the inference ability of the tailored DCCN (TDCNN) is quite good,\nresilient against epistemic uncertainty, and tunable for completely independent\nbut adverse observations. From the crack pattern computed using TDCCN, average\ncrack width (ACW) and crack density (CD) can be calculated to facilitate\ndurability design and conditional assessment in a practical environment.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:50:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Das", "Avik Kumar", ""], ["Leung", "Chrisopher K. Y.", ""], ["Wan", "Kai Tai", ""]]}, {"id": "2105.00458", "submitter": "Angelo Mele", "authors": "Shweta Gaonkar and Angelo Mele", "title": "A model of inter-organizational network formation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How do inter-organizational networks emerge? Accounting for interdependence\namong ties while studying tie formation is one of the key challenges in this\narea of research. We address this challenge using an equilibrium framework\nwhere firms' decisions to form links with other firms are modeled as a\nstrategic game. In this game, firms weigh the costs and benefits of\nestablishing a relationship with other firms and form ties if their net payoffs\nare positive. We characterize the equilibrium networks as exponential random\ngraphs (ERGM), and we estimate the firms' payoffs using a Bayesian approach. To\ndemonstrate the usefulness of our approach, we apply the framework to a\nco-investment network of venture capital firms in the medical device industry.\nThe equilibrium framework allows researchers to draw economic interpretation\nfrom parameter estimates of the ERGM Model. We learn that firms rely on their\njoint partners (transitivity) and prefer to form ties with firms similar to\nthemselves (homophily). These results hold after controlling for the\ninterdependence among ties. Another, critical advantage of a structural\napproach is that it allows us to simulate the effects of economic shocks or\npolicy counterfactuals. We test two such policy shocks, namely, firm entry and\nregulatory change. We show how new firms' entry or a regulatory shock of\nminimum capital requirements increase the co-investment network's density and\nclustering.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 12:30:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gaonkar", "Shweta", ""], ["Mele", "Angelo", ""]]}, {"id": "2105.00489", "submitter": "Aba Diop", "authors": "Aba Diop, El Hadji Deme", "title": "Parametric bootstrapping in a generalized extreme value regression model\n  for binary response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized extreme value (GEV) regression is often more adapted when we\ninvestigate a relationship between a binary response variable $Y$ which\nrepresents a rare event and potentiel predictors $\\mathbf{X}$. In particular,\nwe use the quantile function of the GEV distribution as link function.\nBootstrapping assigns measures of accuracy (bias, variance, confidence\nintervals, prediction error, test of hypothesis) to sample estimates. This\ntechnique allows estimation of the sampling distribution of almost any\nstatistic using random sampling methods. Bootstrapping estimates the properties\nof an estimator by measuring those properties when sampling from an\napproximating distribution. In this paper, we fitted the generalized extreme\nvalue regression model, then we performed parametric bootstrap method for\ntesting hupthesis, estimating confidence interval of parameters for generalized\nextreme value regression model and a real data application.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:43:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Diop", "Aba", ""], ["Deme", "El Hadji", ""]]}, {"id": "2105.00600", "submitter": "Mehala Balamurali", "authors": "Mehala Balamurali", "title": "A Bayesian Method for Estimating Uncertainty in Excavated Material", "comments": "Uncertainty in the excavated material, Gaussian Mixture Model, Moment\n  matching, Uncertainty tracking, 17 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to probabilistically quantify the moments (mean\nand variance) of excavated material during excavation by aggregating the prior\nmoments of the grade blocks around the given bucket dig location. By modelling\nthe moments as random probability density functions (pdf) at sampled locations,\na formulation of the sums of Gaussian based uncertainty estimation is presented\nthat jointly estimates the location pdfs, as well as the prior values for\nuncertainty coming from ore body knowledge (obk) sub block models. The moments\ncalculated at each random location is a single Gaussian and they are the\ncomponents of Gaussian mixture distribution. The overall uncertainty of the\nexcavated material at the given bucket location is represented by the Gaussian\nMixture Model (GMM) and therefore moment matching method is proposed to\nestimate the moments of the reduced GMM. The method was tested in a region at a\nPilbara iron ore deposit situated in the Brockman Iron Formation of the\nHamersley Province, Western Australia, and suggests a frame work to quantify\nthe uncertainty in the excavated material that hasn't been studied anywhere in\nthe literature yet.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 02:07:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Balamurali", "Mehala", ""]]}, {"id": "2105.00620", "submitter": "Siawpeng Er", "authors": "Siawpeng Er, Shihao Yang, Tuo Zhao", "title": "COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The global spread of COVID-19, the disease caused by the novel coronavirus\nSARS-CoV-2, has cast a significant threat to mankind. As the COVID-19 situation\ncontinues to evolve, predicting localized disease severity is crucial for\nadvanced resource allocation. This paper proposes a method named COURAGE\n(COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of\n2-week-ahead COVID-19 related deaths for each county in the United States,\nleveraging modern deep learning techniques. Specifically, our method adopts a\nself-attention model from Natural Language Processing, known as the transformer\nmodel, to capture both short-term and long-term dependencies within the time\nseries while enjoying computational efficiency. Our model fully utilizes\npublicly available information of COVID-19 related confirmed cases, deaths,\ncommunity mobility trends and demographic information, and can produce\nstate-level prediction as an aggregation of the corresponding county-level\npredictions. Our numerical experiments demonstrate that our model achieves the\nstate-of-the-art performance among the publicly available benchmark models.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 04:00:59 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 02:50:06 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Er", "Siawpeng", ""], ["Yang", "Shihao", ""], ["Zhao", "Tuo", ""]]}, {"id": "2105.00700", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Pedro Puig and Albert Navarro", "title": "Analysis of zero inflated dichotomous variables from a Bayesian\n  perspective: Application to occupational health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a new methodology to fit zero inflated Bernoulli data from\na Bayesian approach, able to distinguish between two potential sources of zeros\n(structurals and non-structurals). Its usage is illustrated by means of a real\nexample from the field of occupational health as the phenomenon of sickness\npresenteeism, in which it is reasonable to think that some individuals will\nnever be at risk of suffering it because they have not been sick in the period\nof study (structural zeros). Without separating structural and non-structural\nzeros one would one would be studying jointly the general health status and the\npresenteeism itself, and therefore obtaining potentially biased estimates as\nthe phenomenon is being implicitly underestimated by diluting it into the\ngeneral health status. The proposed methodology performance has been evaluated\nthrough a comprehensive simulation study, and it has been compiled as an R\npackage freely available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:13:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Puig", "Pedro", ""], ["Navarro", "Albert", ""]]}, {"id": "2105.00773", "submitter": "Michael Hughes", "authors": "Gian Marco Visani, Alexandra Hope Lee, Cuong Nguyen, David M. Kent,\n  John B. Wong, Joshua T. Cohen, and Michael C. Hughes", "title": "Approximate Bayesian Computation for an Explicit-Duration Hidden Markov\n  Model of COVID-19 Hospital Trajectories", "comments": "To appear in the Proceedings of the Machine Learning for Healthcare\n  (MLHC) conference, 2021. 20 pages, 7 figures and 1 table. 26 additional pages\n  of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem of modeling constrained hospital resources in the\nmidst of the COVID-19 pandemic in order to inform decision-makers of future\ndemand and assess the societal value of possible interventions. For broad\napplicability, we focus on the common yet challenging scenario where\npatient-level data for a region of interest are not available. Instead, given\ndaily admissions counts, we model aggregated counts of observed resource use,\nsuch as the number of patients in the general ward, in the intensive care unit,\nor on a ventilator. In order to explain how individual patient trajectories\nproduce these counts, we propose an aggregate count explicit-duration hidden\nMarkov model, nicknamed the ACED-HMM, with an interpretable, compact\nparameterization. We develop an Approximate Bayesian Computation approach that\ndraws samples from the posterior distribution over the model's transition and\nduration parameters given aggregate counts from a specific location, thus\nadapting the model to a region or individual hospital site of interest. Samples\nfrom this posterior can then be used to produce future forecasts of any counts\nof interest. Using data from the United States and the United Kingdom, we show\nour mechanistic approach provides competitive probabilistic forecasts for the\nfuture even as the dynamics of the pandemic shift. Furthermore, we show how our\nmodel provides insight about recovery probabilities or length of stay\ndistributions, and we suggest its potential to answer challenging what-if\nquestions about the societal value of possible interventions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:32:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 15:51:01 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Visani", "Gian Marco", ""], ["Lee", "Alexandra Hope", ""], ["Nguyen", "Cuong", ""], ["Kent", "David M.", ""], ["Wong", "John B.", ""], ["Cohen", "Joshua T.", ""], ["Hughes", "Michael C.", ""]]}, {"id": "2105.00866", "submitter": "Lin Zhang", "authors": "Zhiwei Xing, Lin Zhang, Huan Xia, Qian Luo, and Zhao-xin Chen", "title": "Causal Discovery of Flight Service Process Based on Event Sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of the civil aviation industry has continuously increased the\nrequirements for the efficiency of airport ground support services. In the\nexisting ground support research, there has not yet been a process model that\ndirectly obtains support from the ground support log to study the causal\nrelationship between service nodes and flight delays. Most ground support\nstudies mainly use machine learning methods to predict flight delays, and the\nflight support model they are based on is an ideal model. The study did not\nconduct an in-depth study of the causal mechanism behind the ground support\nlink and did not reveal the true cause of flight delays. Therefore, there is a\ncertain deviation in the prediction of flight delays by machine learning, and\nthere is a certain deviation between the ideal model based on the research and\nthe actual service process. Therefore, it is of practical significance to\nobtain the process model from the guarantee log and analyze its causality.\nHowever, the existing process causal factor discovery methods only do certain\nresearch when the assumption of causal sufficiency is established and does not\nconsider the existence of latent variables. Therefore, this article proposes a\nframework to realize the discovery of process causal factors without assuming\ncausal sufficiency. The optimized fuzzy mining process model is used as the\nservice benchmark model, and the local causal discovery algorithm is used to\ndiscover the causal factors. Under this framework, this paper proposes a new\nMarkov blanket discovery algorithm that does not assume causal sufficiency to\ndiscover causal factors and uses benchmark data sets for testing. Finally, the\nactual flight service data is used.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:59:44 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Xing", "Zhiwei", ""], ["Zhang", "Lin", ""], ["Xia", "Huan", ""], ["Luo", "Qian", ""], ["Chen", "Zhao-xin", ""]]}, {"id": "2105.00890", "submitter": "Guilherme Oliveira Lopes de", "authors": "Guilherme Lopes de Oliveira, Rosangela Helena Loschi", "title": "Estimation of underreporting in Brazilian tuberculosis data, 2012-2014", "comments": "12 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Analysis of burden of underregistration in tuberculosis data in Brazil, from\n2012 to 2014. Approches of Oliveira et al. (2020) and Stoner et al. (2019) are\napplied. The main focus is to illustrated how the approach of Oliveira et al.\n(2020) can be applied when the clustering structure is not previously\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:19:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["de Oliveira", "Guilherme Lopes", ""], ["Loschi", "Rosangela Helena", ""]]}, {"id": "2105.00909", "submitter": "Udit Bhatia", "authors": "Surender V Raj, Udit Bhatia, Manish Kumar", "title": "Cyclone preparedness strategies for regional power transmission systems\n  in data-scarce coastal regions of India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the frequency and intensity of tropical cyclones, and the degree of\nurbanization increase, a systematic strengthening of power transmission\nnetworks in the coastal regions becomes imperative. An effective strategy for\nthe same can be to strengthen select transmission towers, which requires\nconsideration of network at holistic scale and its orientation relative to the\ncoastline, the fragility of towers, and properties of cyclones. Since necessary\ninformation is often missing, actionable frameworks for prioritization remain\nelusive. Based on publicly available data, we assess efficacies of strategic\ninterventions in the network that serves 40 million people. After evaluating 72\nstrategies for prioritization, we find that strategies that consider rather\nsimplistic properties of the network and its orientation with respect to the\ncoastline work much better than those based purely on the network's properties,\nin spite of minor variations in fragilities of towers. This integrated approach\nopens avenues for actionable engineering and policy interventions in\nresource-constrained and data-deprived settings.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:46:04 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Raj", "Surender V", ""], ["Bhatia", "Udit", ""], ["Kumar", "Manish", ""]]}, {"id": "2105.01124", "submitter": "Ting Ye", "authors": "Ting Ye and Dylan S. Small", "title": "Combining Broad and Narrow Case Definitions in Matched Case-Control\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a matched case-control study, cases are compared to noncases, who are\nsimilar in observed covariates, in terms of their retrospective exposure to a\ntreatment to assess the impact of the treatment on the outcome. In the absence\nof a gold standard case definition, there is often a broad case definition and\na narrow case definition. The broad case definition offers a larger sample size\nof cases but the narrow case definition may offer a larger effect size.\nRestricting to the narrow case definition may introduce selection bias because\nthe treatment may affect the type of a case a subject is. In this article, we\npropose a new sensitivity analysis framework for combining broad and narrow\ncase definitions in matched case-control studies, that considers the unmeasured\nconfounding bias and selection bias simultaneously. We develop a valid\nrandomization-based testing procedure using only the narrow case matched sets\nwhen the effect of the unmeasured confounder on receiving treatment and the\neffect of the treatment on case definition among the always-cases are\ncontrolled by sensitivity parameters. We then use the Bonferroni method to\ncombine the testing procedures using the broad and narrow case definitions. We\nalso study comprehensively the proposed testing procedures' sensitivity to\nunmeasured biases using the design sensitivity and extensive power analyses.\nOur method is applied to study whether having firearms at home increases\nsuicide risk.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:53:22 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ye", "Ting", ""], ["Small", "Dylan S.", ""]]}, {"id": "2105.01150", "submitter": "Shadi Shahsavari", "authors": "Pavan Holur, Shadi Shahsavari, Ehsan Ebrahimzadeh, Timothy R.\n  Tangherlini, Vwani Roychowdhury", "title": "Modeling Social Readers: Novel Tools for Addressing Reception from\n  Online Book Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Readers' responses to literature have received scant attention in\ncomputational literary studies. The rise of social media offers an opportunity\nto capture a segment of these responses while data-driven analysis of these\nresponses can provide new critical insight into how people \"read\". Posts\ndiscussing an individual book on Goodreads, a social media platform that hosts\nuser discussions of popular literature, are referred to as \"reviews\", and\nconsist of plot summaries, opinions, quotes, or some mixture of these. Since\nthese reviews are written by readers, computationally modeling them allows one\nto discover the overall non-professional discussion space about a work,\nincluding an aggregated summary of the work's plot, an implicit ranking of the\nimportance of events, and the readers' impressions of main characters. We\ndevelop a pipeline of interlocking computational tools to extract a\nrepresentation of this reader generated shared narrative model. Using a corpus\nof reviews of five popular novels, we discover the readers' distillation of the\nmain storylines in a novel, their understanding of the relative importance of\ncharacters, as well as the readers' varying impressions of these characters. In\nso doing, we make three important contributions to the study of infinite\nvocabulary networks: (i) an automatically derived narrative network that\nincludes meta-actants; (ii) a new sequencing algorithm, REV2SEQ, that generates\na consensus sequence of events based on partial trajectories aggregated from\nthe reviews; and (iii) a new \"impressions\" algorithm, SENT2IMP, that provides\nfiner, non-trivial and multi-modal insight into readers' opinions of\ncharacters.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:10:14 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 17:36:01 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Holur", "Pavan", ""], ["Shahsavari", "Shadi", ""], ["Ebrahimzadeh", "Ehsan", ""], ["Tangherlini", "Timothy R.", ""], ["Roychowdhury", "Vwani", ""]]}, {"id": "2105.01184", "submitter": "Peng Ding", "authors": "Anqi Zhao and Peng Ding", "title": "Reconciling design-based and model-based causal inferences for\n  split-plot experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The split-plot design assigns different interventions at the whole-plot and\nsub-plot levels, respectively, and induces a group structure on the final\ntreatment assignments. A common strategy is to use the OLS fit of the outcome\non the treatment indicators coupled with the robust standard errors clustered\nat the whole-plot level. It does not give consistent estimator for the causal\neffects of interest when the whole-plot sizes vary. Another common strategy is\nto fit the linear mixed-effects model of the outcome with Normal random effects\nand errors. It is a purely model-based approach and can be sensitive to\nviolations of parametric assumptions. In contrast, the design-based inference\nassumes no outcome models and relies solely on the controllable randomization\nmechanism determined by the physical experiment. We first extend the existing\ndesign-based inference based on the {\\htf} estimator to the Hajek estimator,\nand establish the finite-population central limit theorem for both under\nsplit-plot randomization. We then reconcile the results with those under the\nmodel-based approach, and propose two regression strategies, namely (i) the WLS\nfit of the unit-level data based on the inverse probability weighting and (ii)\nthe OLS fit of the aggregate data based on whole-plot total outcomes, to\nreproduce the Hajek and {\\htf} estimators from least squares, respectively.\nThis, together with the asymptotic conservativeness of the corresponding\ncluster-robust covariances for estimating the true design-based covariances as\nwe establish in the process, justifies the validity of regression-based\nestimators for design-based inference. In light of the flexibility of\nregression formulation with covariate adjustment, we further extend the theory\nto the case with covariates and demonstrate the efficiency gain by\nregression-based covariate adjustment via both asymptotic theory and\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:38:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2105.01200", "submitter": "Mengyang Gu", "authors": "Mengyang Gu, Yimin Luo, Yue He, Matthew E. Helgeson and Megan T.\n  Valentine", "title": "Uncertainty quantification and estimation in differential dynamic\n  microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.soft physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential dynamic microscopy (DDM) is a form of video image analysis that\ncombines the sensitivity of scattering and the direct visualization benefits of\nmicroscopy. DDM is broadly useful in determining dynamical properties including\nthe intermediate scattering function for many spatiotemporally correlated\nsystems. Despite its straightforward analysis, DDM has not been fully adopted\nas a routine characterization tool, largely due to computational cost and lack\nof algorithmic robustness. We present a comprehensive statistical framework\nthat aims at quantifying error, reducing the computational order and enhancing\nthe robustness of DDM analysis. We quantify the error, and propagate an\nindependent noise term to derive a closed-form expression of the expected value\nand variance of the observed image structure function. Significantly, we\npropose an unbiased estimator of the mean of the noise in the observed image\nstructure function, which can be determined experimentally and significantly\nimproves the accuracy of applications of DDM. Furthermore, through use of\nGaussian Process Regression (GPR), we find that predictive samples of the image\nstructure function require only around 1% of the Fourier Transforms of the\nobserved quantities. This vastly reduces computational cost, while preserving\ninformation of the quantities of interest, such as quantiles of the image\nscattering function, for subsequent analysis. The approach, which we call DDM\nwith Uncertainty Quantification (DDM-UQ), is validated using both simulations\nand experiments with respect to accuracy and computational efficiency, as\ncompared with conventional DDM and multiple particle tracking. Overall, we\npropose that DDM-UQ lays the foundation for important new applications of DDM,\nas well as to high-throughput characterization.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 22:39:09 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Gu", "Mengyang", ""], ["Luo", "Yimin", ""], ["He", "Yue", ""], ["Helgeson", "Matthew E.", ""], ["Valentine", "Megan T.", ""]]}, {"id": "2105.01226", "submitter": "Vincent Chin", "authors": "Vincent Chin, Adam Beavan, Job Fransen, Jan Mayer, Robert Kohn, Louise\n  M. Ryan, Scott A. Sisson", "title": "Modelling age-related changes in executive functions of soccer players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The widespread popularity of soccer across the globe has turned it into a\nmulti-billion dollar industry. As a result, most professional clubs actively\nengage in talent identification and development programmes. Contemporary\nresearch has generally supported the use of executive functions - a class of\nneuropsychological processes responsible for cognitive behaviours - in\npredicting a soccer player's future success. However, studies on the\ndevelopmental evolution of executive functions have yielded differing results\nin their structural form (such as inverted U-shapes, or otherwise). This\narticle presents the first analysis of changes in the domain-generic and\ndomain-specific executive functions based on longitudinal data measured on\nelite German soccer players. Results obtained from a latent variable model show\nthat these executive functions experience noticeable growth from late childhood\nuntil pre-adolescence, but remain fairly stable in later growth stages. As a\nconsequence, our results suggest that the role of executive functions in\nfacilitating talent identification may have been overly emphasised.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 00:33:08 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Chin", "Vincent", ""], ["Beavan", "Adam", ""], ["Fransen", "Job", ""], ["Mayer", "Jan", ""], ["Kohn", "Robert", ""], ["Ryan", "Louise M.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "2105.01330", "submitter": "Guillaume Chauvet", "authors": "Marie-Astrid Metten (Irset), Nathalie Costet (Irset), J.-F. Viel,\n  Guillaume Chauvet (IRMAR)", "title": "Reflection on modern methods: a note on variance estimation when using\n  inverse probability weighting to handle attrition in cohort studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse probability weighting (IPW) method is used to handle attrition in\nassociation analyses derived from cohort studies. It consists in weighting the\nrespondents at a given follow-up by their inverse probability to participate.\nWeights are estimated first and then used in a weighted association model. When\nthe IPW method is used, instead of using a so-called na{\\\"i}ve variance\nestimator, the literature recommends using a robust variance estimator.\nHowever, the latter may overestimate the variance because the weights are\nconsidered known rather than estimated. In this note, we develop, by a\nlinearization technique, an estimator accounting for the weight estimation\nphase and explain how it differs from na{\\\"i}ve and robust variance estimators.\nWe compare the three variance estimators through simulations under several MAR\nand MNAR scenarios. We found that both the robust and linearized variance\nestimators were approximately unbiased, even in MNAR scenarios. The naive\nvariance estimator severely underestimated the variance. We encourage\nresearchers to be careful with variance estimation when using the IPW method,\navoiding na{\\\"i}ve estimator and opting for a robust or linearized estimator. R\nand SAS codes are provided to implement them in their own studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:25:53 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Metten", "Marie-Astrid", "", "Irset"], ["Costet", "Nathalie", "", "Irset"], ["Viel", "J. -F.", "", "IRMAR"], ["Chauvet", "Guillaume", "", "IRMAR"]]}, {"id": "2105.01460", "submitter": "Harrison Zhu", "authors": "Harrison Zhu, Adam Howes, Owen van Eer, Maxime Rischard, Dino\n  Sejdinovic, Seth Flaxman", "title": "Multi-resolution Spatial Regression for Aggregated Data with an\n  Application to Crop Yield Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new methodology for spatial regression of aggregated outputs on\nmulti-resolution covariates. Such problems often occur with spatial data, for\nexample in crop yield prediction, where the output is spatially-aggregated over\nan area and the covariates may be observed at multiple resolutions. Building\nupon previous work on aggregated output regression, we propose a regression\nframework to synthesise the effects of the covariates at different resolutions\non the output and provide uncertainty estimation. We show that, for a crop\nyield prediction problem, our approach is more scalable, via variational\ninference, than existing multi-resolution regression models. We also show that\nour framework yields good predictive performance, compared to existing\nmulti-resolution crop yield models, whilst being able to provide estimation of\nthe underlying spatial effects.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:41:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhu", "Harrison", ""], ["Howes", "Adam", ""], ["van Eer", "Owen", ""], ["Rischard", "Maxime", ""], ["Sejdinovic", "Dino", ""], ["Flaxman", "Seth", ""]]}, {"id": "2105.01728", "submitter": "Sabina Tomkins", "authors": "Sabina Tomkins, Keniel Yao, Johann Gaebler, Tobias Konitzer, David\n  Rothschild, Marc Meredith, Sharad Goel", "title": "Blocks as geographic discontinuities: The effect of polling place\n  assignment on voting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A potential voter must incur a number of costs in order to successfully cast\nan in-person ballot, including the costs associated with identifying and\ntraveling to a polling place. In order to investigate how these costs affect\nvoting behavior, we introduce two quasi-experimental designs that can be used\nto study how the political participation of registered voters is affected by\ndifferences in the relative distance that registrants must travel to their\nassigned Election Day polling place and whether their polling place remains at\nthe same location as in a previous election. Our designs make comparisons of\nregistrants who live on the same residential block, but are assigned to vote at\ndifferent polling places. We find that living farther from a polling place and\nbeing assigned to a new polling place reduce in-person Election Day voting, but\nthat registrants largely offset for this by casting more early in-person and\nmail ballots.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 19:50:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tomkins", "Sabina", ""], ["Yao", "Keniel", ""], ["Gaebler", "Johann", ""], ["Konitzer", "Tobias", ""], ["Rothschild", "David", ""], ["Meredith", "Marc", ""], ["Goel", "Sharad", ""]]}, {"id": "2105.01733", "submitter": "Bart J. A. Mertens", "authors": "Bart J. A. Mertens", "title": "Calibration of prediction rules for life-time outcomes using prognostic\n  Cox regression survival models and multiple imputations to account for\n  missing predictor data with cross-validatory assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we expand the methodology presented in Mertens et. al (2020,\nBiometrical Journal) to the study of life-time (survival) outcome which is\nsubject to censoring and when imputation is used to account for missing values.\nWe consider the problem where missing values can occur in both the calibration\ndata as well as newly - to-be-predicted - observations (validation). We focus\non the Cox model. Methods are described to combine imputation with predictive\ncalibration in survival modeling subject to censoring. Application to\ncross-validation is discussed. We demonstrate how conclusions broadly confirm\nthe first paper which restricted to the study of binary outcomes only.\nSpecifically prediction-averaging appears to have superior statistical\nproperties, especially smaller predictive variation, as opposed to a direct\napplication of Rubin's rules. Distinct methods for dealing with the baseline\nhazards are discussed when using Rubin's rules-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:10:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mertens", "Bart J. A.", ""]]}, {"id": "2105.01737", "submitter": "Alexey Shutov", "authors": "A. A. Kaygorodtseva, A. V. Shutov", "title": "Inspection of ratcheting models for pathological error sensitivity and\n  overparametrization", "comments": "20 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cond-mat.mtrl-sci stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate analysis of plastic strain accumulation under stress-controlled\ncyclic loading is vital for numerous engineering applications. Typically,\nmodels of plastic ratcheting are calibrated against available experimental\ndata. Since actual experiments are not exactly accurate, one should check the\nidentification protocols for pathological dependencies on experimental errors.\nIn this paper, a step-by-step algorithm is presented to estimate the\nsensitivities of identified material parameters. As a part of the sensitivity\nanalysis method, a new mechanics-based metric in the space of material\nparameters is proposed especially for ratcheting-related applications. The\nsensitivity of material parameters to experimental errors is estimated, based\non this metric. For demonstration purposes, the accumulation of irreversible\nstrain in the titanium alloy VT6 (Russian analog of Ti-6Al-4V) is analysed.\nThree types of phenomenological models of plastic ratcheting are considered.\nThey are the Armstrong-Frederick model as well as the first and the second\nOhno-Wang models. Based on real data, a new rule of isotropic hardening is\nproposed for greater accuracy of simulation. The ability of the sensitivity\nanalysis to determine reliable and unreliable parameters is demonstrated. The\nplausibility of the new method is checked by alternative approaches, like the\nconsideration of correlation matrices and validation of identified parameters\non \"unseen\" data. A relation between pathological error sensitivity and\noverparametrization is established.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:06:19 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Kaygorodtseva", "A. A.", ""], ["Shutov", "A. V.", ""]]}, {"id": "2105.01807", "submitter": "Teresa Portone", "authors": "Teresa Portone, Robert D. Moser", "title": "Bayesian inference of an uncertain generalized diffusion operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines a novel Bayesian inverse problem to infer an\ninfinite-dimensional uncertain operator appearing in a differential equation,\nwhose action on an observable state variable affects its dynamics. The operator\nis parametrized using its eigendecomposition, which enables prior information\nto be incorporated into its formulation. The Bayesian inverse problem is\ndefined in terms of an uncertain, generalized diffusion operator appearing in\nan evolution equation for a contaminant's transport through a heterogeneous\nporous medium. Limited observations of the state variable's evolution are used\nas data for inference, and the dependence on the solution of the inverse\nproblem is studied as a function of the frequency of observations, as well as\non whether or not the data is collected as a spatial or time series.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 00:32:28 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Portone", "Teresa", ""], ["Moser", "Robert D.", ""]]}, {"id": "2105.02030", "submitter": "Christian Bartels", "authors": "Christian Bartels and Thomas Dumortier", "title": "Inverse probability of censoring weighting for visual predictive checks\n  of time-to-event models with time-varying covariates", "comments": null, "journal-ref": "Pharmaceutical Statistics (2021) 1-10", "doi": "10.1002/pst.2124", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When constructing models to summarize clinical data to be used for\nsimulations, it is good practice to evaluate the models for their capacity to\nreproduce the data. This can be done by means of Visual Predictive Checks\n(VPC), which consist of (1) several reproductions of the original study by\nsimulation from the model under evaluation, (2) calculating estimates of\ninterest for each simulated study and (3) comparing the distribution of those\nestimates with the estimate from the original study. This procedure is a\ngeneric method that is straightforward to apply, in general. Here we consider\nthe application of the method to time to event data and consider the special\ncase when a time-varying covariate is not known or cannot be approximated after\nevent time. In this case, simulations cannot be conducted beyond the end of the\nfollow-up time (event or censoring time) in the original study. Thus, the\nsimulations must be censored at the end of the follow-up time. Since this\ncensoring is not random, the standard KM estimates from the simulated studies\nand the resulting VPC will be biased. We propose to use inverse probability of\ncensoring weighting (IPoC) method to correct the KM estimator for the simulated\nstudies and obtain unbiased VPCs. For analyzing the Cantos study, the IPoC\nweighting as described here proved valuable and enabled the generation of VPCs\nto qualify PKPD models for simulations. Here, we use a generated data set,\nwhich allows illustration of the different situations and evaluation against\nthe known truth.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:59:30 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bartels", "Christian", ""], ["Dumortier", "Thomas", ""]]}, {"id": "2105.02140", "submitter": "Luiza Piancastelli", "authors": "Luiza Piancastelli, Nial Friel, Julie Vercelloni, Kerrie Mengersen,\n  Antonietta Mira", "title": "A Bayesian latent allocation model for clustering compositional data\n  with application to the Great Barrier Reef", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative abundance is a common metric to estimate the composition of species\nin ecological surveys reflecting patterns of commonness and rarity of\nbiological assemblages. Measurements of coral reef compositions formed by four\ncommunities along Australia's Great Barrier Reef (GBR) gathered between 2012\nand 2017 are the focus of this paper. We undertake the task of finding clusters\nof transect locations with similar community composition and investigate\nchanges in clustering dynamics over time. During these years, an unprecedented\nsequence of extreme weather events (cyclones and coral bleaching) impacted the\n58 surveyed locations. The dependence between constituent parts of a\ncomposition presents a challenge for existing multivariate clustering\napproaches. In this paper, we introduce a finite mixture of Dirichlet\ndistributions with group-specific parameters, where cluster memberships are\ndictated by unobserved latent variables. The inference is carried in a Bayesian\nframework, where MCMC strategies are outlined to sample from the posterior\nmodel. Simulation studies are presented to illustrate the performance of the\nmodel in a controlled setting. The application of the model to the 2012 coral\nreef data reveals that clusters were spatially distributed in similar ways\nacross reefs which indicates a potential influence of wave exposure at the\norigin of coral reef community composition. The number of clusters estimated by\nthe model decreased from four in 2012 to two from 2014 until 2017. Posterior\nprobabilities of transect allocations to the same cluster substantially\nincrease through time showing a potential homogenization of community\ncomposition across the whole GBR. The Bayesian model highlights the diversity\nof coral reef community composition within a coral reef and rapid changes\nacross large spatial scales that may contribute to undermining the future of\nthe GBR's biodiversity.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:47:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Piancastelli", "Luiza", ""], ["Friel", "Nial", ""], ["Vercelloni", "Julie", ""], ["Mengersen", "Kerrie", ""], ["Mira", "Antonietta", ""]]}, {"id": "2105.02379", "submitter": "Jose R. Zubizarreta", "authors": "Jose R. Zubizarreta, Yige Li, Nancy L. Keating, Mary Beth Landrum", "title": "Targeted Quality Measurement of Health Care Providers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the problem of measuring the quality of cancer care delivered by\nproviders across the US, we present a framework for institutional quality\nmeasurement which addresses the heterogeneity of the public they serve. For\nthis, we conceptualize the task of quality measurement as a causal inference\nproblem, decoupling estimands from estimators, making explicit the assumptions\nneeded for identification, and helping to target flexible covariate profiles\nthat can represent specific populations of interest. We propose methods for\nlayered case-mix adjustments that combine weighting and regression modeling\napproaches in a sequential manner in order to reduce model extrapolation and\nallow for provider effect modification. We evaluate these methods in an\nextensive simulation study and highlight the practical utility of weighting\nmethods that warn the investigator when case-mix adjustments are infeasible\nwithout some form of extrapolation that goes beyond the support of the data.\nSpecifically, our constrained optimization approach to weighting constitutes a\ndiagnostic of sparse or null data for a given provider relative to the target\nprofiles. In our study of cancer care outcomes, we assess the performance of\noncology practices for different profiles that correspond to different types of\npatients that may receive cancer care. We describe how the proposed methods may\nbe particularly important for high-stakes quality measurement, such as public\nreporting or performance-based payments. They may also be important for\nindividual patients seeking practices that provide high-quality care to\npatients like them. Our approach applies to other settings besides health care,\nincluding business and education, where instead of cancer practices, we have\ncompanies and schools.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 00:12:58 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zubizarreta", "Jose R.", ""], ["Li", "Yige", ""], ["Keating", "Nancy L.", ""], ["Landrum", "Mary Beth", ""]]}, {"id": "2105.02381", "submitter": "Max Rubinstein", "authors": "Max Rubinstein and Amelia Haviland", "title": "The Effect of Medicaid Expansion on Non-Elderly Adult Uninsurance Rates\n  Among States that did not Expand Medicaid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We estimate the effect of Medicaid expansion on the adult uninsurance rate in\nstates that did not expand Medicaid in 2014. Using data from the American\nCommunity Survey (ACS), we estimate this effect - the treatment effect on the\ncontrols (ETC) - by re-weighting expansion regions to approximately balance the\ncovariates from non-expansion regions using an extension of the stable\nbalancing weights objective function (Zubizaretta (2015)). We contribute to the\nbalancing weights literature by accounting for hierarchical data structure and\ncovariate measurement error when calculating our weights, and to the synthetic\ncontrols literature (see, e.g. Abadie et al 2010) by outlining a set of\nassumptions that identifies the ETC using time-series cross-sectional data. We\nestimate that Medicaid expansion would have changed the uninsurance rate by\n-2.33 (-3.49, -1.16) percentage points. These results are smaller in absolute\nmagnitude than existing estimates of the treatment effect on the treated (ETT),\nthough may not be directly comparable due to the study design, target\npopulation, and level of analysis. Regardless, we caution against making\ninferences about the ETC using estimates of the ETT, and emphasize the need to\ndirectly estimate the appropriate counterfactual when they are the quantity of\ninterest.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 00:32:14 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 15:05:13 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Rubinstein", "Max", ""], ["Haviland", "Amelia", ""]]}, {"id": "2105.02488", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Anders M. Fjell, Kristine B. Walhovd", "title": "Longitudinal modeling of age-dependent latent traits with generalized\n  additive latent and mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present generalized additive latent and mixed models (GALAMMs) for\nanalysis of clustered data with latent and observed variables depending\nsmoothly on observed variables. A profile likelihood algorithm is proposed, and\nwe derive asymptotic standard errors of both smooth and parametric terms. The\nwork was motivated by applications in cognitive neuroscience, and we show how\nGALAMMs can successfully model the complex lifespan trajectory of latent\nepisodic memory, along with a discrepant trajectory of working memory, as well\nas the effect of latent socioeconomic status on hippocampal development.\nSimulation experiments suggest that model estimates are accurate even with\nmoderate sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:42:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Fjell", "Anders M.", ""], ["Walhovd", "Kristine B.", ""]]}, {"id": "2105.02526", "submitter": "Sevvandi Kandanaarachchi", "authors": "Sevvandi Kandanaarachchi, Hideya Ochiai, Asha Rao", "title": "Honeyboost: Boosting honeypot performance with data fusion and anomaly\n  detection", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With cyber incidents and data breaches becoming increasingly common, being\nable to predict a cyberattack has never been more crucial. Network Anomaly\nDetection Systems (NADS) ability to identify unusual behavior makes them useful\nin predicting such attacks. In this paper, we introduce a novel framework to\nenhance the performance of honeypot aided NADS. We use a hybrid of two\napproaches: horizontal and vertical. The horizontal approach constructs a time\nseries from the communications of each node, with node-level features\nencapsulating their behavior over time. The vertical approach finds anomalies\nin each protocol space. To the best of our knowledge, this is the first time\nnode-level features have been used in honeypot aided NADS. Furthermore, using\nextreme value theory, anomaly detection with low false positive rates is\npossible. Experimental results indicate the efficacy of our framework in\nidentifying suspicious activities of nodes from node-level features, often\nbefore the honeypot does.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:59:44 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Kandanaarachchi", "Sevvandi", ""], ["Ochiai", "Hideya", ""], ["Rao", "Asha", ""]]}, {"id": "2105.02625", "submitter": "Boyla Mainsah", "authors": "Anish Karpurapu, Adam Krekorian, Ye Tian, Leslie M. Collins, Ravi\n  Karra, Aaron Franklin and Boyla O. Mainsah", "title": "Evaluating the Effect of Longitudinal Dose and INR Data on Maintenance\n  Warfarin Dose Predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Warfarin, a commonly prescribed drug to prevent blood clots, has a highly\nvariable individual response. Determining a maintenance warfarin dose that\nachieves a therapeutic blood clotting time, as measured by the international\nnormalized ratio (INR), is crucial in preventing complications. Machine\nlearning algorithms are increasingly being used for warfarin dosing; usually,\nan initial dose is predicted with clinical and genotype factors, and this dose\nis revised after a few days based on previous doses and current INR. Since a\nsequence of prior doses and INR better capture the variability in individual\nwarfarin response, we hypothesized that longitudinal dose response data will\nimprove maintenance dose predictions. To test this hypothesis, we analyzed a\ndataset from the COAG warfarin dosing study, which includes clinical data,\nwarfarin doses and INR measurements over the study period, and maintenance dose\nwhen therapeutic INR was achieved. Various machine learning regression models\nto predict maintenance warfarin dose were trained with clinical factors and\ndosing history and INR data as features. Overall, dose revision algorithms with\na single dose and INR achieved comparable performance as the baseline dose\nrevision algorithm. In contrast, dose revision algorithms with longitudinal\ndose and INR data provided maintenance dose predictions that were statistically\nsignificantly much closer to the true maintenance dose. Focusing on the best\nperforming model, gradient boosting (GB), the proportion of ideal estimated\ndose, i.e., defined as within $\\pm$20% of the true dose, increased from the\nbaseline (54.92%) to the GB model with the single (63.11%) and longitudinal\n(75.41%) INR. More accurate maintenance dose predictions with longitudinal dose\nresponse data can potentially achieve therapeutic INR faster, reduce\ndrug-related complications and improve patient outcomes with warfarin therapy.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 13:01:42 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Karpurapu", "Anish", ""], ["Krekorian", "Adam", ""], ["Tian", "Ye", ""], ["Collins", "Leslie M.", ""], ["Karra", "Ravi", ""], ["Franklin", "Aaron", ""], ["Mainsah", "Boyla O.", ""]]}, {"id": "2105.02757", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph, Catherine Gimbrone, Ellicott C. Matthay, Ivan Diaz,\n  Corey S. Davis, John R. Pamplin II, Katherine Keyes, Magdalena Cerda", "title": "When effects cannot be estimated: redefining estimands to understand the\n  effects of naloxone access laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: All states in the US have enacted at least some naloxone access\nlaws (NALs) in an effort to reduce opioid overdose lethality. Previous\nevaluations found NALs increased naloxone dispensing but showed mixed results\nin terms of opioid overdose mortality. One reason for mixed results could be\nfailure to address violations of the positivity assumption caused by the\nco-occurrence of NAL enactment with enactment of related laws, ultimately\nresulting in bias, increased variance, and low statistical power.\n  Methods: We reformulated the research question to alleviate some challenges\nrelated to law co-occurrence. Because NAL enactment was closely correlated with\nGood Samaritan Law (GSL) enactment, we bundled NAL with GSL, and estimated the\nhypothetical associations of enacting NAL/GSL up to 2 years earlier (an amount\nsupported by the observed data) on naloxone dispensation and opioid overdose\nmortality.\n  Results: We estimated that such a shift in NAL/GSL duration would have been\nassociated with increased naloxone dispensations (0.28 dispensations/100,000\npeople (95% CI: 0.18-0.38) in 2013 among early NAL/GSL enactors; 47.58 (95% CI:\n28.40-66.76) in 2018 among late enactors). We estimated that such a shift would\nhave been associated with increased opioid overdose mortality (1.88\ndeaths/100,000 people (95% CI: 1.03-2.72) in 2013; 2.06 (95% CI: 0.92-3.21) in\n2018).\n  Conclusions: Consistent with prior research, increased duration of NAL/GSL\nenactment was associated with increased naloxone dispensing. Contrary to\nexpectation, we did not find a protective association with opioid overdose\nmorality, though residual bias due to unobserved confounding and interference\nlikely remain.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 15:32:14 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Gimbrone", "Catherine", ""], ["Matthay", "Ellicott C.", ""], ["Diaz", "Ivan", ""], ["Davis", "Corey S.", ""], ["Pamplin", "John R.", "II"], ["Keyes", "Katherine", ""], ["Cerda", "Magdalena", ""]]}, {"id": "2105.02869", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Pamela Ventola, and James S. Duncan", "title": "Estimating Reproducible Functional Networks Associated with Task\n  Dynamics using Unsupervised LSTMs", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2020", "journal-ref": "2020 IEEE 17th International Symposium on Biomedical Imaging\n  (ISBI), 2020, p. 1395-1398", "doi": "10.1109/ISBI45749.2020.9098377", "report-no": null, "categories": "q-bio.QM cs.LG eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating more reproducible functional networks that\nare more strongly associated with dynamic task activity by using recurrent\nneural networks with long short term memory (LSTMs). The LSTM model is trained\nin an unsupervised manner to learn to generate the functional magnetic\nresonance imaging (fMRI) time-series data in regions of interest. The learned\nfunctional networks can then be used for further analysis, e.g., correlation\nanalysis to determine functional networks that are strongly associated with an\nfMRI task paradigm. We test our approach and compare to other methods for\ndecomposing functional networks from fMRI activity on 2 related but separate\ndatasets that employ a biological motion perception task. We demonstrate that\nthe functional networks learned by the LSTM model are more strongly associated\nwith the task activity and dynamics compared to other approaches. Furthermore,\nthe patterns of network association are more closely replicated across subjects\nwithin the same dataset as well as across datasets. More reproducible\nfunctional networks are essential for better characterizing the neural\ncorrelates of a target task.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:53:22 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Ventola", "Pamela", ""], ["Duncan", "James S.", ""]]}, {"id": "2105.02874", "submitter": "Nicha Dvornek", "authors": "Shiyu Wang and Nicha C. Dvornek", "title": "A Metamodel Structure For Regression Analysis: Application To Prediction\n  Of Autism Spectrum Disorder Severity", "comments": "IEEE International Symposium on Biomedical Imaging (ISBI) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional regression models do not generalize well when learning from small\nand noisy datasets. Here we propose a novel metamodel structure to improve the\nregression result. The metamodel is composed of multiple classification base\nmodels and a regression model built upon the base models. We test this\nstructure on the prediction of autism spectrum disorder (ASD) severity as\nmeasured by the ADOS communication (ADOS COMM) score from resting-state fMRI\ndata, using a variety of base models. The metamodel outperforms traditional\nregression models as measured by the Pearson correlation coefficient between\ntrue and predicted scores and stability. In addition, we found that the\nmetamodel is more flexible and more generalizable.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 17:58:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Wang", "Shiyu", ""], ["Dvornek", "Nicha C.", ""]]}, {"id": "2105.02971", "submitter": "Matthew Bonas", "authors": "Matthew Bonas and Stefano Castruccio", "title": "Calibration of Spatial Forecasts from Citizen Science Urban Air\n  Pollution Data with Sparse Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With their continued increase in coverage and quality, data collected from\npersonal air quality monitors has become an increasingly valuable tool to\ncomplement existing public health monitoring system over urban areas. However,\nthe potential of using such `citizen science data' for automatic early warning\nsystems is hampered by the lack of models able to capture the high-resolution,\nnonlinear spatio-temporal features stemming from local emission sources such as\ntraffic, residential heating and commercial activities. In this work, we\npropose a machine learning approach to forecast high-frequency spatial fields\nwhich has two distinctive advantages from standard neural network methods in\ntime: 1) sparsity of the neural network via a spike-and-slab prior, and 2) a\nsmall parametric space. The introduction of stochastic neural networks\ngenerates additional uncertainty, and in this work we propose a fast approach\nfor forecast calibration, both marginal and spatial. We focus on assessing\nexposure to urban air pollution in San Francisco, and our results suggest an\nimprovement of 35.7% in the mean squared error over standard time series\napproach with a calibrated forecast for up to 5 days.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 21:20:37 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Bonas", "Matthew", ""], ["Castruccio", "Stefano", ""]]}, {"id": "2105.03022", "submitter": "Shirin Golchi", "authors": "Shirin Golchi", "title": "Estimating the Design Operating Characteristics in Clinical Trials with\n  the Ordinal Scale Disease Progression Endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian adaptive designs have gained popularity in all phases of clinical\ntrials in the recent years. The COVID-19 pandemic, however, has brought these\ndesigns to the centre stage. The need for establishing evidence for the\neffectiveness of vaccines, therapeutic treatments and policies that could\nresolve or control the crisis has resulted in development of efficient designs\nfor clinical trials that can be concluded with smaller sample sizes in a\nshorter time period. Design of Bayesian adaptive trials, however, requires\nextensive simulation studies that is considered a disadvantage in\ntime-sensitive settings such as the pandemic. In this paper, we propose a set\nof methods for efficient estimation and uncertainty quantification for the\ndesign operating characteristics of Bayesian adaptive trials. The proposed\napproach is tailored to address design of clinical trials with the ordinal\ndisease progression scale endpoint but can be used generally in the clinical\ntrials context where design operating characteristics cannot be obtained\nanalytically.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 01:17:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Golchi", "Shirin", ""]]}, {"id": "2105.03197", "submitter": "Abdul-Karim Iddrisu", "authors": "Abdul-Karim Iddrisu, Abukari Alhassan", "title": "Primary analysis method for incomplete CD4 count data from IMPI trial\n  and other trials with similar setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The National Research Council panel on prevention and treatment of missing\ndata in clinical trials recommends that primary analysis methods are carefully\nselected before appropriate sensitivity analysis methods can be chosen. In this\npaper, we recommend an appropriate primary analysis method for handling CD4\ncount data from the IMPI trial and trials with similar settings. The estimand\nof interest in the IMPI trial is the effectiveness estimand hypothesis. We\ndiscussed, compared, and contrasted results from complete case analysis and\nsimple imputation methods, with the direct-likelihood and multiple imputation\nmethods. The simple imputation methods produced biased estimates of treatment\neffect. However, the maximum likelihood and the multiple imputation methods\nproduced consistent estimates of treatment effect. The maximum likelihood or\nthe multiple imputation approaches produced unbiased and consistent estimates.\nTherefore, either the maximum likelihood or the multiple imputation methods,\nunder the assumption that the data are missing at random can be considered as\nprimary analysis method when one is considering sensitivity analysis to dropout\nusing the CD4 count data from the IMPI trial and other trials with similar\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 12:09:58 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Iddrisu", "Abdul-Karim", ""], ["Alhassan", "Abukari", ""]]}, {"id": "2105.03269", "submitter": "Stephen Johnson", "authors": "Stephen Richard Johnson, Sarah Elizabeth Heaps, Kevin James Wilson and\n  Darren James Wilkinson", "title": "Bayesian spatio-temporal model for high-resolution short-term\n  forecasting of precipitation fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With extreme weather events becoming more common, the risk posed by surface\nwater flooding is ever increasing. In this work we propose a model, and\nassociated Bayesian inference scheme, for generating probabilistic\n(high-resolution short-term) forecasts of localised precipitation. The\nparametrisation of our underlying hierarchical dynamic spatio-temporal model is\nmotivated by a forward-time, centred-space finite difference solution to a\ncollection of stochastic partial differential equations, where the main driving\nforces are advection and diffusion. Observations from both weather radar and\nground based rain gauges provide information from which we can learn about the\nlikely values of the (latent) precipitation field in addition to other unknown\nmodel parameters. Working in the Bayesian paradigm provides a coherent\nframework for capturing uncertainty both in the underlying model parameters and\nalso in our forecasts. Further, appealing to simulation based (MCMC) sampling\nyields a straightforward solution to handling zeros, treated as censored\nobservations, via data augmentation. Both the underlying state and the\nobservations are of moderately large dimension ($\\mathcal{O}(10^4)$ and\n$\\mathcal{O}(10^3)$ respectively) and this renders standard inference\napproaches computationally infeasible. Our solution is to embed the ensemble\nKalman smoother within a Gibbs sampling scheme to facilitate approximate\nBayesian inference in reasonable time. Both the methodology and the\neffectiveness of our posterior sampling scheme are demonstrated via simulation\nstudies and also by a case study of real data from the Urban Observatory\nproject based in Newcastle upon Tyne, UK.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:48:33 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Johnson", "Stephen Richard", ""], ["Heaps", "Sarah Elizabeth", ""], ["Wilson", "Kevin James", ""], ["Wilkinson", "Darren James", ""]]}, {"id": "2105.03275", "submitter": "Prateek Bansal", "authors": "Subodh Dubey, Oded Cats, Serge Hoogendoorn and Prateek Bansal", "title": "A Non-Compensatory Random Utility Choice Model based on Choquet Integral", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a random utility maximisation (RUM) based discrete choice model to\nsimultaneously consider three behavioural aspects of the decision-maker - i)\nevaluation of each attribute (e.g., constant marginal utility beyond attribute\nthresholds), ii) aggregation of attributes in the systematic part of the\nindirect utility, and iii) flexible substitution patterns between alternatives.\nCorresponding to each aspect, we use i) fuzzy membership functions to capture\nhow the decision-maker evaluates a range of attributes, ii) replace the\nweighted-sum aggregation function in the indirect utility with the Choquet\nintegral (CI) to capture non-linear interactions between attributes, and iii)\nadopt multinomial probit (MNP) choice probability kernel to represent flexible\nsubstitution patterns between alternatives. We estimate the proposed model\nusing a constrained maximum likelihood estimator. A comprehensive Monte Carlo\nstudy is performed to establish the statistical properties of the estimator and\ndemonstrate the superiority of the proposed model over the traditional MNP\nmodel with weighted sum indirect utility in terms of goodness of fit and\nrecovery of marginal effects. Note that the proposed CI-based specification\nprovides complementarity between pairs of attributes coupled with their\nindividual importance ranking as a by-product of the estimation. This\ninformation could potentially help policymakers in making policies to improve\nthe preference level for an alternative. The advantages of considering\nattribute cut-off based non-compensatory behaviour and a flexible aggregation\nfunction are illustrated in an empirical application to understand New Yorkers'\npreferences towards mobility-on-demand services.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 14:03:23 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Dubey", "Subodh", ""], ["Cats", "Oded", ""], ["Hoogendoorn", "Serge", ""], ["Bansal", "Prateek", ""]]}, {"id": "2105.03309", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i", "title": "Estimating latent linear correlations from fuzzy frequency tables", "comments": "27 pages, 5 figures, 9 tables, 2 supplementary figures, 2\n  supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research concerns the estimation of latent linear or polychoric\ncorrelations from fuzzy frequency tables. Fuzzy counts are of particular\ninterest to many disciplines including social and behavioral sciences, and are\nespecially relevant when observed data are classified using fuzzy categories -\nas for socio-economic studies, clinical evaluations, content analysis,\ninter-rater reliability analysis - or when imprecise observations are\nclassified into either precise or imprecise categories - as for the analysis of\nratings data or fuzzy coded variables. In these cases, the space of count\nmatrices is no longer defined over naturals and, consequently, the polychoric\nestimator cannot be used to accurately estimate latent linear correlations. The\naim of this contribution is twofold. First, we illustrate a computational\nprocedure based on generalized natural numbers for computing fuzzy frequencies.\nSecond, we reformulate the problem of estimating latent linear correlations\nfrom fuzzy counts in the context of Expectation-Maximization based maximum\nlikelihood estimation. A simulation study and two applications are used to\ninvestigate the characteristics of the proposed method. Overall, the results\nshow that the fuzzy EM-based polychoric estimator is more efficient to deal\nwith imprecise count data as opposed to standard polychoric estimators that may\nbe used in this context.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:57 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""]]}, {"id": "2105.03454", "submitter": "Boyu Ren", "authors": "Boyu Ren, Xiao Wu, Danielle Braun, Natesh Pillai, Francesca Dominici", "title": "Bayesian Modeling for Exposure Response Curve via Gaussian Processes:\n  Causal Effects of Exposure to Air Pollution on Health Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by environmental health research on air pollution, we address the\nchallenge of estimation and uncertainty quantification of causal\nexposure-response function (CERF). The CERF describes the relationship between\na continuously varying exposure (or treatment) and its causal effect on a\noutcome. We propose a new Bayesian approach that relies on a Gaussian process\n(GP) model to estimate the CERF. We parametrize the covariance (kernel)\nfunction of the GP to mimic matching via a Generalized Propensity Score (GPS).\nThe tuning parameters of the matching function are chosen to optimize covariate\nbalance. Our approach achieves automatic uncertainty evaluation of the CERF\nwith high computational efficiency, enables change point detection through\ninference on derivatives of the CERF, and yields the desired separation of\ndesign and analysis phases for causal estimation. We provide theoretical\nresults showing the correspondence between our Bayesian GP framework and\ntraditional approaches in causal inference for estimating causal effects of a\ncontinuous exposure. We apply the methods to 520,711 ZIP-code-level\nobservations to estimate the causal effect of long-term exposures to PM2.5 on\nall-cause mortality among Medicare enrollees in the United States.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:06:46 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:18:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ren", "Boyu", ""], ["Wu", "Xiao", ""], ["Braun", "Danielle", ""], ["Pillai", "Natesh", ""], ["Dominici", "Francesca", ""]]}, {"id": "2105.03478", "submitter": "Soichiro Yamauchi", "authors": "Matthew Blackwell, Soichiro Yamauchi", "title": "Adjusting for Unmeasured Confounding in Marginal Structural Models with\n  Propensity-Score Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Marginal structural models are a popular tool for investigating the effects\nof time-varying treatments, but they require an assumption of no unobserved\nconfounders between the treatment and outcome. With observational data, this\nassumption may be difficult to maintain, and in studies with panel data, many\nresearchers use fixed effects models to purge the data of time-constant\nunmeasured confounding. Unfortunately, traditional linear fixed effects models\nare not suitable for estimating the effects of time-varying treatments, since\nthey can only estimate lagged effects under implausible assumptions. To resolve\nthis tension, we a propose a novel inverse probability of treatment weighting\nestimator with propensity-score fixed effects to adjust for time-constant\nunmeasured confounding in marginal structural models of fixed-length treatment\nhistories. We show that these estimators are consistent and asymptotically\nnormal when the number of units and time periods grow at a similar rate. Unlike\ntraditional fixed effect models, this approach works even when the outcome is\nonly measured at a single point in time as is common in marginal structural\nmodels. We apply these methods to estimating the effect of negative advertising\non the electoral success of candidates for statewide offices in the United\nStates.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:43:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:29:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Blackwell", "Matthew", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2105.03493", "submitter": "Forrest Crawford", "authors": "Xiaoxuan Cai, Eben Kenah, and Forrest W. Crawford", "title": "Causal identification of infectious disease intervention effects in a\n  clustered population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal identification of treatment effects for infectious disease outcomes in\ninterconnected populations is challenging because infection outcomes may be\ntransmissible to others, and treatment given to one individual may affect\nothers' outcomes. Contagion, or transmissibility of outcomes, complicates\nstandard conceptions of treatment interference in which an intervention\ndelivered to one individual can affect outcomes of others. Several statistical\nframeworks have been proposed to measure causal treatment effects in this\nsetting, including structural transmission models, mediation-based partnership\nmodels, and randomized trial designs. However, existing estimands for\ninfectious disease intervention effects are of limited conceptual usefulness:\nSome are parameters in a structural model whose causal interpretation is\nunclear, others are causal effects defined only in a restricted two-person\nsetting, and still others are nonparametric estimands that arise naturally in\nthe context of a randomized trial but may not measure any biologically\nmeaningful effect. In this paper, we describe a unifying formalism for defining\nnonparametric structural causal estimands and an identification strategy for\nlearning about infectious disease intervention effects in clusters of\ninteracting individuals when infection times are observed. The estimands\ngeneralize existing quantities and provide a framework for causal\nidentification in randomized and observational studies, including situations\nwhere only binary infection outcomes are observed. A semiparametric class of\npairwise Cox-type transmission hazard models is used to facilitate statistical\ninference in finite samples. A comprehensive simulation study compares existing\nand proposed estimands under a variety of randomized and observational vaccine\ntrial designs.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:25:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cai", "Xiaoxuan", ""], ["Kenah", "Eben", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "2105.03497", "submitter": "Derek Chang", "authors": "Derek Chang, Kerry Emanuel, Saurabh Amin", "title": "Probabilistic Modeling of Hurricane Wind-Induced Damage in\n  Infrastructure Systems", "comments": "Submitted to Risk Analysis, 21 pages (main text), 17 figures,\n  includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a modeling approach for probabilistic estimation of\nhurricane wind-induced damage to infrastructural assets. In our approach, we\nemploy a Nonhomogeneous Poisson Process (NHPP) model for estimating\nspatially-varying probability distributions of damage as a function of\nhurricane wind field velocities. Specifically, we consider a physically-based,\nquadratic NHPP model for failures of overhead assets in electricity\ndistribution systems. The wind field velocities are provided by Forecasts of\nHurricanes using Large-Ensemble Outputs (FHLO), a framework for generating\nprobabilistic hurricane forecasts. We use FHLO in conjunction with the NHPP\nmodel, such that the hurricane forecast uncertainties represented by FHLO are\naccounted for in estimating the probability distributions of damage.\nFurthermore, we evaluate the spatial variability and extent of hurricane damage\nunder key wind field parameters (intensity, size, and asymmetries). By applying\nour approach to prediction of power outages (loss-of-service) in northwestern\nFlorida due to Hurricane Michael (2018), we demonstrate a statistically\nsignificant relationship between outage rate and failure rate. Finally, we\nformulate parametric models that relate total damage and financial losses to\nthe hurricane parameters of intensity and size. Overall, this paper's findings\nsuggest that our approach is well-suited to jointly account for spatial\nvariability and forecast uncertainty in the damage estimates, and is readily\napplicable to prediction of system loss-of-service due to the damage.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:36:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chang", "Derek", ""], ["Emanuel", "Kerry", ""], ["Amin", "Saurabh", ""]]}, {"id": "2105.03508", "submitter": "Heejong Bong", "authors": "Heejong Bong (1), Val\\'erie Ventura (1), Eric A. Yttri (3, 4 and 5),\n  Matthew A. Smith (4 and 5) and Robert E. Kass (1, 2 and 5) ((1) Department of\n  Statistics and Data Sciences, Carnegie Mellon University, (2) Machine\n  Learning Department, Carnegie Mellon University, (3) Department of Biological\n  Sciences, Carnegie Mellon University, (4) Department of Biomedical\n  Engineering, Carnegie Mellon University, (5) Neuroscience Institute, Carnegie\n  Mellon University)", "title": "Latent Cross-population Dynamic Time-series Analysis of High-dimensional\n  Neural Recordings", "comments": "19 pages, 9 figures, submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important problem in analysis of neural data is to characterize\ninteractions across brain regions from high-dimensional multiple-electrode\nrecordings during a behavioral experiment. Lead-lag effects indicate possible\ndirectional flows of neural information, but they are often transient,\nappearing during short intervals of time. Such non-stationary interactions can\nbe difficult to identify, but they can be found by taking advantage of the\nreplication structure inherent to many neurophysiological experiments. To\ndescribe non-stationary interactions between replicated pairs of\nhigh-dimensional time series, we developed a method of estimating latent,\nnon-stationary cross-correlation. Our approach begins with an extension of\nprobabilistic CCA to the time series setting, which provides a model-based\ninterpretation of multiset CCA. Because the covariance matrix describing\nnon-stationary dependence is high-dimensional, we assume sparsity of\ncross-correlations within a range of possible interesting lead-lag effects. We\nshow that the method can perform well in realistic settings and we apply it to\n192 simultaneous local field potential (LFP) recordings from prefrontal cortex\n(PFC) and visual cortex (area V4) during a visual memory task. We find lead-lag\nrelationships that are highly plausible, being consistent with related results\nin the literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:13:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bong", "Heejong", "", "3, 4 and 5"], ["Ventura", "Val\u00e9rie", "", "3, 4 and 5"], ["Yttri", "Eric A.", "", "3, 4 and 5"], ["Smith", "Matthew A.", "", "4 and 5"], ["Kass", "Robert E.", "", "1, 2 and 5"]]}, {"id": "2105.03512", "submitter": "Amanda Stathopoulos", "authors": "Jason Soria, Amanda Stathopoulos", "title": "Investigating Socio-spatial Differences between Solo Ridehailing and\n  Pooled Rides in Diverse Communities", "comments": "Submitted to journal of Transport Geography", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformative mobility services present both considerable opportunities and\nchallenges for urban mobility systems. Increasing attention is being paid to\nridehailing platforms and connections between demand and continuous innovation\nin service features; one of these features is dynamic ride-pooling. To\ndisentangle how ridehailing impacts existing transportation networks and its\nability to support economic vitality and community livability it is essential\nto consider the distribution of demand across diverse communities. In this\npaper we expand the literature on ridehailing demand by exploring community\nvariation and spatial dependence in ridehailing use. Specifically, we\ninvestigate the diffusion and role of solo requests versus ride-pooling to shed\nlight on how different mobility services, with different environmental and\naccessibility implications, are used by diverse communities. This paper employs\na Social Disadvantage Index, Transit Access Analysis, and a Spatial Durbin\nModel to investigate the influence of both local and spatial spillover effects\non the demand for shared and solo ridehailing. The analysis of 127 million\nridehailing rides, of which 15% are pooled, confirms the presence of spatial\neffects. Results indicate that density and vibrancy variables have analogue\neffects, both direct and indirect, on demand for solo vs pooled rides. Instead,\nour analysis reveals significant contrasting effects for socio-economic\ndisadvantage, which is positively correlated with ride-pooling and negatively\nwith solo rides. Additionally, we find that higher rail transit access is\nassociated with higher demand for both solo and pooled ridehailing along with\nsubstantial spatial spillovers. We discuss implications for policy, operations\nand research related to the novel insight on how pooled ridesourcing relate to\ngeography, living conditions, and transit interactions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:20:15 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Soria", "Jason", ""], ["Stathopoulos", "Amanda", ""]]}, {"id": "2105.03529", "submitter": "Adam Sales", "authors": "Johann A. Gagnon-Bartsch, Adam C. Sales, Edward Wu, Anthony F.\n  Botelho, John A. Erickson, Luke W. Miratrix, Neil T. Heffernan", "title": "Precise Unbiased Estimation in Randomized Experiments using Auxiliary\n  Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized controlled trials (RCTs) are increasingly prevalent in education\nresearch, and are often regarded as a gold standard of causal inference. Two\nmain virtues of randomized experiments are that they (1) do not suffer from\nconfounding, thereby allowing for an unbiased estimate of an intervention's\ncausal impact, and (2) allow for design-based inference, meaning that the\nphysical act of randomization largely justifies the statistical assumptions\nmade. However, RCT sample sizes are often small, leading to low precision; in\nmany cases RCT estimates may be too imprecise to guide policy or inform\nscience. Observational studies, by contrast, have strengths and weaknesses\ncomplementary to those of RCTs. Observational studies typically offer much\nlarger sample sizes, but may suffer confounding. In many contexts, experimental\nand observational data exist side by side, allowing the possibility of\nintegrating \"big observational data\" with \"small but high-quality experimental\ndata\" to get the best of both. Such approaches hold particular promise in the\nfield of education, where RCT sample sizes are often small due to cost\nconstraints, but automatic collection of observational data, such as in\ncomputerized educational technology applications, or in state longitudinal data\nsystems (SLDS) with administrative data on hundreds of thousand of students,\nhas made rich, high-dimensional observational data widely available. We outline\nan approach that allows one to employ machine learning algorithms to learn from\nthe observational data, and use the resulting models to improve precision in\nrandomized experiments. Importantly, there is no requirement that the machine\nlearning models are \"correct\" in any sense, and the final experimental results\nare guaranteed to be exactly unbiased. Thus, there is no danger of confounding\nbiases in the observational data leaking into the experiment.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 22:38:45 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Gagnon-Bartsch", "Johann A.", ""], ["Sales", "Adam C.", ""], ["Wu", "Edward", ""], ["Botelho", "Anthony F.", ""], ["Erickson", "John A.", ""], ["Miratrix", "Luke W.", ""], ["Heffernan", "Neil T.", ""]]}, {"id": "2105.03651", "submitter": "Anirban Mondal", "authors": "Anirban Mondal and Bani Mallick", "title": "Spline-Based Bayesian Emulators for Large Scale Spatial Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian approach to nonlinear inverse problems is considered where the\nunknown quantity (input) is a random spatial field. The forward model is\ncomplex and non-linear, therefore computationally expensive. An emulator-based\nmethodology is developed, where the Bayesian multivariate adaptive regression\nsplines (BMARS) are used to model the function that maps the inputs to the\noutputs. Discrete cosine transformation (DCT) is used for dimension reduction\nof the input spatial field. The posterior sampling is carried out using\ntrans-dimensional Markov Chain Monte Carlo (MCMC) methods. Numerical results\nare presented by analyzing simulated as well as real data on hydrocarbon\nreservoir characterization.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 09:28:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Mondal", "Anirban", ""], ["Mallick", "Bani", ""]]}, {"id": "2105.03699", "submitter": "Bruno Santos", "authors": "Agatha Rodrigues, Patrick Borges and Bruno Santos", "title": "A defective cure rate quantile regression model for male breast cancer\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we particularly address the problem of assessing the impact\nof clinical stage and age on the specific survival times of men with breast\ncancer when cure is a possibility, where there is also the interest of\nexplaining this impact on different quantiles of the survival times. To this\nend, we developed a quantile regression model for survival data in the presence\nof long-term survivors based on the generalized distribution of Gompertz in a\ndefective version, which is conveniently reparametrized in terms of the q-th\nquantile and then linked to covariates via a logarithm link function. This\nproposal allows us to obtain how each variable affects the survival times in\ndifferent quantiles. In addition, we are able to study the effects of\ncovariates on the cure rate as well. We consider Markov Chain Monte Carlo\n(MCMC) methods to develop a Bayesian analysis in the proposed model and we\nevaluate its performance through a Monte Carlo simulation study. Finally, we\nillustrate the advantages of our model in a data set about male breast cancer\nfrom Brazil.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 13:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rodrigues", "Agatha", ""], ["Borges", "Patrick", ""], ["Santos", "Bruno", ""]]}, {"id": "2105.03806", "submitter": "Helton Saulo", "authors": "Dan\\'ubia R. Cunha, Jose A. Divino and Helton Saulo", "title": "The zero-adjusted log-symmetric quantile regression model applied to\n  extramarital affairs data", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a zero-adjusted log-symmetric quantile regression\nmodel. Initially, we introduce zero-adjusted log-symmetric distributions, which\nallow for the accommodation of zeros. The estimation of the parameters is\napproached by the maximum likelihood method and a Monte Carlo simulation is\nperformed to evaluate the estimates. Finally, we illustrate the proposed\nmethodology with the use of a real extramarital affairs data set.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:01:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cunha", "Dan\u00fabia R.", ""], ["Divino", "Jose A.", ""], ["Saulo", "Helton", ""]]}, {"id": "2105.03993", "submitter": "Xiaoquan Wen", "authors": "Yi Zhao and Xiaoquan Wen", "title": "Statistical Assessment of Replicability via Bayesian Model Criticism", "comments": "45 pages, 9 figures, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessment of replicability is critical to ensure the quality and rigor of\nscientific research. In this paper, we discuss inference and modeling\nprinciples for replicability assessment. Targeting distinct application\nscenarios, we propose two types of Bayesian model criticism approaches to\nidentify potentially irreproducible results in scientific experiments. They are\nmotivated by established Bayesian prior and posterior predictive model-checking\nprocedures and generalize many existing replicability assessment methods.\nFinally, we discuss the statistical properties of the proposed replicability\nassessment approaches and illustrate their usages by simulations and examples\nof real data analysis, including the data from the Reproducibility Project:\nPsychology and a systematic review of impacts of pre-existing cardiovascular\ndisease on COVID-19 outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:56:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhao", "Yi", ""], ["Wen", "Xiaoquan", ""]]}, {"id": "2105.03996", "submitter": "Leo Zabrocki", "authors": "L\\'eo Zabrocki and Marion Leroutier and Marie-Ab\\`ele Bind", "title": "Estimating the Causal Effects of Cruise Traffic on Air Pollution using\n  Randomization-Based Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Local environmental organizations and media have recently expressed concerns\nover air pollution induced by maritime traffic and its potential adverse health\neffects on the population of Mediterranean port cities. We explore this issue\nwith unique high-frequency data from Marseille, France's largest port for\ncruise ships, over the 2008-2018 period. Using a new pair-matching algorithm\ndesigned for time series data, we create hypothetical randomized experiments\nand estimate the variation in air pollutant concentrations caused by a\nshort-term increase in cruise vessel traffic. We carry out a\nrandomization-based approach to compute 95% Fisherian intervals (FI) for\nconstant treatment effects consistent with the matched data and the\nhypothetical intervention. At the hourly level, cruise vessels' arrivals\nincrease concentrations of nitrogen dioxide (NO$_{2}$) by 4.7 $\\mu g/m^3$ (95%\nFI: [1.4, 8.0]), of sulfur dioxide (SO$_{2}$) by 1.2 $\\mu g/m^3$ (95% FI:\n[-0.1, 2.5]), and of particulate matter (PM$_{10}$) by 4.6 $\\mu g/m^3$ (95% FI:\n[0.9, 8.3]). At the daily level, cruise traffic increases concentrations of\nNO$_{2}$ by 1.2 $\\mu g/m^3$ (95% FI: [-0.5, 3.0]) and of PM$_{10}$ by 1.3 $\\mu\ng/m^3$ (95% FI: [-0.3, 3.0]). Our results suggest that well-designed\nhypothetical randomized experiments provide a principled approach to better\nunderstand the negative externalities of maritime traffic.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:59:36 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zabrocki", "L\u00e9o", ""], ["Leroutier", "Marion", ""], ["Bind", "Marie-Ab\u00e8le", ""]]}, {"id": "2105.04072", "submitter": "Tiago Silva", "authors": "Tiago Tiburcio da Silva and Rodrigo Francisquini and Mari\\'a C. V.\n  Nascimento", "title": "Meteorological and human mobility data on predicting COVID-19 cases by a\n  novel hybrid decomposition method with anomaly detection analysis: a case\n  study in the capitals of Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In 2020, Brazil was the leading country in COVID-19 cases in Latin America,\nand capital cities were the most severely affected by the outbreak. Climates\nvary in Brazil due to the territorial extension of the country, its relief,\ngeography, and other factors. Since the most common COVID-19 symptoms are\nrelated to the respiratory system, many researchers have studied the\ncorrelation between the number of COVID-19 cases with meteorological variables\nlike temperature, humidity, rainfall, etc. Also, due to its high transmission\nrate, some researchers have analyzed the impact of human mobility on the\ndynamics of COVID-19 transmission. There is a dearth of literature that\nconsiders these two variables when predicting the spread of COVID-19 cases. In\nthis paper, we analyzed the correlation between the number of COVID-19 cases\nand human mobility, and meteorological data in Brazilian capitals. We found\nthat the correlation between such variables depends on the regions where the\ncities are located. We employed the variables with a significant correlation\nwith COVID-19 cases to predict the number of COVID-19 infections in all\nBrazilian capitals and proposed a prediction method combining the Ensemble\nEmpirical Mode Decomposition (EEMD) method with the Autoregressive Integrated\nMoving Average Exogenous inputs (ARIMAX) method, which we called EEMD-ARIMAX.\nAfter analyzing the results poor predictions were further investigated using a\nsignal processing-based anomaly detection method. Computational tests showed\nthat EEMD-ARIMAX achieved a forecast 26.73% better than ARIMAX. Moreover, an\nimprovement of 30.69% in the average root mean squared error (RMSE) was noticed\nwhen applying the EEMD-ARIMAX method to the data normalized after the anomaly\ndetection.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 02:06:51 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["da Silva", "Tiago Tiburcio", ""], ["Francisquini", "Rodrigo", ""], ["Nascimento", "Mari\u00e1 C. V.", ""]]}, {"id": "2105.04134", "submitter": "Daniel Barreiro Ures", "authors": "D. Barreiro-Ures, R. Cao and M. Francisco-Fern\\'andez", "title": "Bagging cross-validated bandwidth selection in nonparametric regression\n  estimation with applications to large-sized samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a well-known and widely used bandwidth selection method\nin nonparametric regression estimation. However, this technique has two\nremarkable drawbacks: (i) the large variability of the selected bandwidths, and\n(ii) the inability to provide results in a reasonable time for very large\nsample sizes. To overcome these problems, bagging cross-validation bandwidths\nare analyzed in this paper. This approach consists in computing the\ncross-validation bandwidths for a finite number of subsamples and then\nrescaling the averaged smoothing parameters to the original sample size. Under\na random-design regression model, asymptotic expressions up to a second-order\nfor the bias and variance of the leave-one-out cross-validation bandwidth for\nthe Nadaraya--Watson estimator are obtained. Subsequently, the asymptotic bias\nand variance and the limit distribution are derived for the bagged\ncross-validation selector. Suitable choices of the number of subsamples and the\nsubsample size lead to an $n^{-1/2}$ rate for the convergence in distribution\nof the bagging cross-validation selector, outperforming the rate $n^{-3/10}$ of\nleave-one-out cross-validation. Several simulations and an illustration on a\nreal dataset related to the COVID-19 pandemic show the behavior of our proposal\nand its better performance, in terms of statistical efficiency and computing\ntime, when compared to leave-one-out cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Barreiro-Ures", "D.", ""], ["Cao", "R.", ""], ["Francisco-Fern\u00e1ndez", "M.", ""]]}, {"id": "2105.04278", "submitter": "Wenyi Zhang", "authors": "Jiakun Liu, Wenyi Zhang, H. Vincent Poor", "title": "A Rate-Distortion Framework for Characterizing Semantic Information", "comments": "To appear at ISIT 2021, with an appendix added to include general\n  solution for jointly Gaussian models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rate-distortion problem motivated by the consideration of semantic\ninformation is formulated and solved. The starting point is to model an\ninformation source as a pair consisting of an intrinsic state which is not\nobservable, corresponding to the semantic aspect of the source, and an\nextrinsic observation which is subject to lossy source coding. The proposed\nrate-distortion problem seeks a description of the information source, via\nencoding the extrinsic observation, under two distortion constraints, one for\nthe intrinsic state and the other for the extrinsic observation. The\ncorresponding state-observation rate-distortion function is obtained, and a few\ncase studies of Gaussian intrinsic state estimation and binary intrinsic state\nclassification are studied.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 11:36:16 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Jiakun", ""], ["Zhang", "Wenyi", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2105.04330", "submitter": "Guido Kuersteiner", "authors": "Guido M. Kuersteiner and Ingmar R. Prucha and Ying Zeng", "title": "Efficient Peer Effects Estimators with Random Group Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study linear peer effects models where peers interact in groups,\nindividual's outcomes are linear in the group mean outcome and characteristics,\nand group effects are random. Our specification is motivated by the moment\nconditions imposed in Graham 2008. We show that these moment conditions can be\ncast in terms of a linear random group effects model and lead to a class of GMM\nestimators that are generally identified as long as there is sufficient\nvariation in group size. We also show that our class of GMM estimators contains\na Quasi Maximum Likelihood estimator (QMLE) for the random group effects model,\nas well as the Wald estimator of Graham 2008 and the within estimator of Lee\n2007 as special cases. Our identification results extend insights in Graham\n2008 that show how assumptions about random group effects as well as variation\nin group size can be used to overcome the reflection problem in identifying\npeer effects. Our QMLE and GMM estimators can easily be augmented with\nadditional covariates and are valid in situations with a large but finite\nnumber of different group sizes. Because our estimators are general moment\nbased procedures, using instruments other than binary group indicators in\nestimation is straight forward. Monte-Carlo simulations show that the bias of\nthe QMLE estimator decreases with the number of groups and the variation in\ngroup size, and increases with group size. We also prove the consistency and\nasymptotic normality of the estimator under reasonable assumptions.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:05:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kuersteiner", "Guido M.", ""], ["Prucha", "Ingmar R.", ""], ["Zeng", "Ying", ""]]}, {"id": "2105.04599", "submitter": "Yiming Xu", "authors": "Yiming Xu, Akil Narayan", "title": "Budget-limited distribution learning in multifidelity problems", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifidelity methods are widely used for statistical estimation of\nquantities of interest (QoIs) in uncertainty quantification using simulation\ncodes of differing costs and accuracies. Many methods approximate\nnumerical-valued statistics that represent only limited information of the\nQoIs. In this paper, we introduce a semi-parametric approach that aims to\neffectively describe the distribution of a scalar-valued QoI in the\nmultifidelity setup. Under a linear model hypothesis, we propose an\nexploration-exploitation strategy to reconstruct the full distribution of a\nscalar-valued QoI using samples from a subset of low-fidelity regressors. We\nderive an informative asymptotic bound for the mean 1-Wasserstein distance\nbetween the estimator and the true distribution, and use it to adaptively\nallocate computational budget for parametric estimation and non-parametric\nreconstruction. Assuming the linear model is correct, we prove that such a\nprocedure is consistent, and converges to the optimal policy (and hence optimal\ncomputational budget allocation) under an upper bound criterion as the budget\ngoes to infinity. A major advantage of our approach compared to several other\nmultifidelity methods is that it is automatic, and its implementation does not\nrequire a hierarchical model setup, cross-model information, or \\textit{a\npriori} known model statistics. Numerical experiments are provided in the end\nto support our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 18:29:43 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Xu", "Yiming", ""], ["Narayan", "Akil", ""]]}, {"id": "2105.04648", "submitter": "Hyungrok Do", "authors": "Hyungrok Do, Shinjini Nandi, Preston Putzel, Padhraic Smyth, Judy\n  Zhong", "title": "Joint Fairness Model with Applications to Risk Predictions for\n  Under-represented Populations", "comments": "62 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under-representation of certain populations, based on gender, race/ethnicity,\nand age, in data collection for predictive modeling may yield less-accurate\npredictions for the under-represented groups. Recently, this issue of fairness\nin predictions has attracted significant attention, as data-driven models are\nincreasingly utilized to perform crucial decision-making tasks. Methods to\nachieve fairness in the machine learning literature typically build a single\nprediction model subject to some fairness criteria in a manner that encourages\nfair prediction performances for all groups. These approaches have two major\nlimitations: i) fairness is often achieved by compromising accuracy for some\ngroups; ii) the underlying relationship between dependent and independent\nvariables may not be the same across groups. We propose a Joint Fairness Model\n(JFM) approach for binary outcomes that estimates group-specific classifiers\nusing a joint modeling objective function that incorporates fairness criteria\nfor prediction. We introduce an Accelerated Smoothing Proximal Gradient\nAlgorithm to solve the convex objective function, and demonstrate the\nproperties of the proposed JFM estimates. Next, we presented the key asymptotic\nproperties for the JFM parameter estimates. We examined the efficacy of the JFM\napproach in achieving prediction performances and parities, in comparison with\nthe Single Fairness Model, group-separate model, and group-ignorant model\nthrough extensive simulations. Finally, we demonstrated the utility of the JFM\nmethod in the motivating example to obtain fair risk predictions for\nunder-represented older patients diagnosed with coronavirus disease 2019\n(COVID-19).\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:05:39 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 14:47:52 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 15:16:51 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Do", "Hyungrok", ""], ["Nandi", "Shinjini", ""], ["Putzel", "Preston", ""], ["Smyth", "Padhraic", ""], ["Zhong", "Judy", ""]]}, {"id": "2105.04789", "submitter": "Mohammad Javad Davoudabadi Mr", "authors": "Mohammad Javad Davoudabadi, Daniel Pagendam, Christopher Drovandi,\n  Jeff Baldock, Gentry White", "title": "Modelling and predicting soil carbon sequestration: is current model\n  structure fit for purpose?", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Soil carbon accounting and prediction play a key role in building decision\nsupport systems for land managers selling carbon credits, in the spirit of the\nParis and Kyoto protocol agreements. Land managers typically rely on\ncomputationally complex models fit using sparse datasets to make these\naccountings and predictions. The model complexity and sparsity of the data can\nlead to over-fitting, leading to inaccurate results using new data or making\npredictions. Modellers address over-fitting by simplifying their models,\nneglecting some soil organic carbon (SOC) components. In this study, we\nintroduce two novel SOC models and a new RothC-like model and investigate how\nthe SOC components and complexity of the SOC models affect the SOC prediction\nin the presence of small and sparse time series data. We develop model\nselection methods that can identify the soil carbon model with the best\npredictive performance, in light of the available data. Through this analysis\nwe reveal that commonly used complex soil carbon models can over-fit in the\npresence of sparse time series data, and our simpler models can produce more\naccurate predictions.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 05:22:05 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Davoudabadi", "Mohammad Javad", ""], ["Pagendam", "Daniel", ""], ["Drovandi", "Christopher", ""], ["Baldock", "Jeff", ""], ["White", "Gentry", ""]]}, {"id": "2105.04813", "submitter": "Marvin Pizon", "authors": "Marvin G. Pizon and Emelyn F. Sagrado", "title": "Forecasting Disease Burden In Philippines: A Symbolic Regression\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Burden of disease measures the impact of living with illness and injury and\ndying prematurely and it is increasing worldwide leading cause of death both\nglobal and national. This paper aimed to propose an index of diseases and\nevaluate a mathematical model to describe the number of burden of disease by\ncause in the Philippines from 1990 - 2016. Through Principal Component Analysis\n(PCA) the diseases categorized as: passed on diseases, vector born diseases,\nnon-communicable diseases, accident, and intentional. Symbolic Regression\nAnalysis was carried out, the study revealed that the number of burden of\ndisease as categorized using CPA will continue decrease up to year 2020 except\non non-communicable disease.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:56:44 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Pizon", "Marvin G.", ""], ["Sagrado", "Emelyn F.", ""]]}, {"id": "2105.04828", "submitter": "Dominik Reinhard", "authors": "Dominik Reinhard and Michael Fau{\\ss} and Abdelhak M. Zoubir", "title": "Asymptotically Optimal Procedures for Sequential Joint Detection and\n  Estimation", "comments": "13 pages, 3 figures, 1 table. Under review in the IEEE Transactions\n  on Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of jointly testing multiple hypotheses and\nestimating a random parameter of the underlying distribution in a sequential\nsetup. The aim is to jointly infer the true hypothesis and the true parameter\nwhile using on average as few samples as possible and keeping the detection and\nestimation errors below predefined levels. Based on mild assumptions on the\nunderlying model, we propose an asymptotically optimal procedure, i.e., a\nprocedure that becomes optimal when the tolerated detection and estimation\nerror levels tend to zero. The implementation of the resulting asymptotically\noptimal stopping rule is computationally cheap and, hence, applicable for\nhigh-dimensional data. We further propose a projected quasi-Newton method to\noptimally chose the coefficients that parameterize the instantaneous cost\nfunction such that the constraints are fulfilled with equality. The proposed\ntheory is validated by numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 07:29:19 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Reinhard", "Dominik", ""], ["Fau\u00df", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "2105.04953", "submitter": "Riccardo Fogliato", "authors": "Riccardo Fogliato, Alice Xiang, Zachary Lipton, Daniel Nagin,\n  Alexandra Chouldechova", "title": "On the Validity of Arrest as a Proxy for Offense: Race and the\n  Likelihood of Arrest for Violent Crimes", "comments": "Accepted at AAAI/ACM Conference on AI, Ethics, and Society (AIES),\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of re-offense is considered in decision-making at many stages of the\ncriminal justice system, from pre-trial, to sentencing, to parole. To aid\ndecision makers in their assessments, institutions increasingly rely on\nalgorithmic risk assessment instruments (RAIs). These tools assess the\nlikelihood that an individual will be arrested for a new criminal offense\nwithin some time window following their release. However, since not all crimes\nresult in arrest, RAIs do not directly assess the risk of re-offense.\nFurthermore, disparities in the likelihood of arrest can potentially lead to\nbiases in the resulting risk scores. Several recent validations of RAIs have\ntherefore focused on arrests for violent offenses, which are viewed as being\nmore accurate reflections of offending behavior. In this paper, we investigate\nbiases in violent arrest data by analysing racial disparities in the likelihood\nof arrest for White and Black violent offenders. We focus our study on\n2007--2016 incident-level data of violent offenses from 16 US states as\nrecorded in the National Incident Based Reporting System (NIBRS). Our analysis\nshows that the magnitude and direction of the racial disparities depend on\nvarious characteristics of the crimes. In addition, our investigation reveals\nlarge variations in arrest rates across geographical locations and offense\ntypes. We discuss the implications of the observed disconnect between re-arrest\nand re-offense in the context of RAIs and the challenges around the use of data\nfrom NIBRS to correct for the sampling bias.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 11:45:52 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Fogliato", "Riccardo", ""], ["Xiang", "Alice", ""], ["Lipton", "Zachary", ""], ["Nagin", "Daniel", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "2105.04981", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi, Ray Bai, Jessica Liu, Silvia P. Canel\\'on, Edward I.\n  George, Yong Chen, Mary R. Boland", "title": "A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of\n  Adverse Pregnancy Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the determinants of adverse pregnancy outcomes like stillbirth and\npreterm birth is of considerable interest in epidemiology. Understanding the\nrole of both individual and community risk factors for these outcomes is\ncrucial for planning appropriate clinical and public health interventions. With\nthis goal, we develop geospatial mixed effects logistic regression models for\nadverse pregnancy outcomes. Our models account for both spatial autocorrelation\nand heterogeneity between neighborhoods. To mitigate the low incidence of\nstillbirth and preterm births in our data, we explore using class rebalancing\ntechniques to improve predictive power. To assess the informative value of the\ncovariates in our models, we use posterior distributions of their coefficients\nto gauge how well they can be distinguished from zero. As a case study, we\nmodel stillbirth and preterm birth in the city of Philadelphia, incorporating\nboth patient-level data from electronic health records (EHR) data and publicly\navailable neighborhood data at the census tract level. We find that\npatient-level features like self-identified race and ethnicity were highly\ninformative for both outcomes. Neighborhood-level factors were also\ninformative, with poverty important for stillbirth and crime important for\npreterm birth. Finally, we identify the neighborhoods in Philadelphia at\nhighest risk of stillbirth and preterm birth.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:35:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Bai", "Ray", ""], ["Liu", "Jessica", ""], ["Canel\u00f3n", "Silvia P.", ""], ["George", "Edward I.", ""], ["Chen", "Yong", ""], ["Boland", "Mary R.", ""]]}, {"id": "2105.05031", "submitter": "Kyriakos Flouris", "authors": "Kyriakos Flouris, Anna Volokitin, Gustav Bredell, Ender Konukoglu", "title": "Gradient flow encoding with distance optimization adaptive step size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The autoencoder model uses an encoder to map data samples to a lower\ndimensional latent space and then a decoder to map the latent space\nrepresentations back to the data space. Implicitly, it relies on the encoder to\napproximate the inverse of the decoder network, so that samples can be mapped\nto and back from the latent space faithfully. This approximation may lead to\nsub-optimal latent space representations. In this work, we investigate a\ndecoder-only method that uses gradient flow to encode data samples in the\nlatent space. The gradient flow is defined based on a given decoder and aims to\nfind the optimal latent space representation for any given sample through\noptimisation, eliminating the need of an approximate inversion through an\nencoder. Implementing gradient flow through ordinary differential equations\n(ODE), we leverage the adjoint method to train a given decoder. We further show\nempirically that the costly integrals in the adjoint method may not be entirely\nnecessary. Additionally, we propose a $2^{nd}$ order ODE variant to the method,\nwhich approximates Nesterov's accelerated gradient descent, with faster\nconvergence per iteration. Commonly used ODE solvers can be quite sensitive to\nthe integration step-size depending on the stiffness of the ODE. To overcome\nthe sensitivity for gradient flow encoding, we use an adaptive solver that\nprioritises minimising loss at each integration step. We assess the proposed\nmethod in comparison to the autoencoding model. In our experiments, GFE showed\na much higher data-efficiency than the autoencoding model, which can be crucial\nfor data scarce applications.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 13:38:23 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Flouris", "Kyriakos", ""], ["Volokitin", "Anna", ""], ["Bredell", "Gustav", ""], ["Konukoglu", "Ender", ""]]}, {"id": "2105.05290", "submitter": "Aditi Sen", "authors": "Aditi Sen (1) and Partha Lahiri (2) ((1) PhD student, Applied\n  Mathematics, Statistics and Scientific Computation, University of Maryland,\n  College Park, USA, (2) Director and Professor, The Joint Program in Survey\n  Methodology and Department of Mathematics, University of Maryland, College\n  Park, USA)", "title": "Estimation of mask effectiveness perception for small domains using\n  multiple data sources", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All pandemics are local; so learning about the impacts of pandemics on public\nhealth and related societal issues at granular levels is of great interest.\nCOVID-19 is affecting everyone in the globe and mask wearing is one of the few\nprecautions against it. To quantify people's perception of mask effectiveness\nand to prevent the spread of COVID-19 for small areas, we use Understanding\nAmerica Study's (UAS) survey data on COVID-19 as our primary data source. Our\ndata analysis shows that direct survey-weighted estimates for small areas could\nbe highly unreliable. In this paper we develop a synthetic estimation method to\nestimate proportions of mask effectiveness for small areas using a logistic\nmodel that combines information from multiple data sources. We select our\nworking model using an extensive data analysis facilitated by a new variable\nselection criterion for survey data and benchmarking ratios. We propose a\nJackknife method to estimate variance of our proposed estimator. From our data\nanalysis. it is evident that our proposed synthetic method outperforms direct\nsurvey-weighted estimator with respect to commonly used evaluation measures.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:41:04 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sen", "Aditi", ""], ["Lahiri", "Partha", ""]]}, {"id": "2105.05303", "submitter": "Thomas Sawczuk", "authors": "Thomas Sawczuk, Anna Palczewska and Ben Jones", "title": "Development of an expected possession value model to analyse team\n  attacking performances in rugby league", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aimed to provide a framework to evaluate team attacking\nperformances in rugby league using 59,233 plays from 180 Super League matches\nvia expected possession value (EPV) models. The EPV-308 split the pitch into\n308 5m x 5m zones, the EPV-77 split the pitch into 77 10m x 10m zones and the\nEPV-19 split the pitch in 19 zones of variable size dependent on the total zone\nvalue generated during a match. Attacking possessions were considered as Markov\nChains, allowing the value of each zone visited to be estimated based on the\noutcome of the possession. The Kullback-Leibler Divergence was used to evaluate\nthe reproducibility of the value generated from each zone (the reward\ndistribution) by teams between matches. The EPV-308 had the greatest\nvariability and lowest reproducibility, compared to EPV-77 and EPV-19. When six\nprevious matches were considered, the team's subsequent match attacking\nperformances had a similar reward distribution for EPV-19, EPV-77 and EPV-308\non 95 +/- 4%, 51 +/- 12% and 0 +/- 0% of occasions. This study supports the use\nof EPV-19 to evaluate team attacking performance in rugby league and provides a\nsimple framework through which attacking performances can be compared between\nteams.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:54:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sawczuk", "Thomas", ""], ["Palczewska", "Anna", ""], ["Jones", "Ben", ""]]}, {"id": "2105.05341", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang and Fushing Hsieh", "title": "An Encoding Approach for Stable Change Point Detection", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Without imposing prior distributional knowledge underlying multivariate time\nseries of interest, we propose a nonparametric change-point detection approach\nto estimate the number of change points and their locations along the temporal\naxis. We develop a structural subsampling procedure such that the observations\nare encoded into multiple sequences of Bernoulli variables. A maximum\nlikelihood approach in conjunction with a newly developed searching algorithm\nis implemented to detect change points on each Bernoulli process separately.\nThen, aggregation statistics are proposed to collectively synthesize\nchange-point results from all individual univariate time series into consistent\nand stable location estimations. We also study a weighting strategy to measure\nthe degree of relevance for different subsampled groups. Simulation studies are\nconducted and shown that the proposed change-point methodology for multivariate\ntime series has favorable performance comparing with currently popular\nnonparametric methods under various settings with different degrees of\ncomplexity. Real data analyses are finally performed on categorical, ordinal,\nand continuous time series taken from fields of genetics, climate, and finance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:00:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Xiaodong", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2105.05370", "submitter": "Xu Wu", "authors": "Ziyu Xie, Wen Jiang, Congjian Wang, Xu Wu", "title": "Bayesian Inverse Uncertainty Quantification of a MOOSE-based Melt Pool\n  Model for Additive Manufacturing Using Experimental Data", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive manufacturing (AM) technology is being increasingly adopted in a\nwide variety of application areas due to its ability to rapidly produce,\nprototype, and customize designs. AM techniques afford significant\nopportunities in regard to nuclear materials, including an accelerated\nfabrication process and reduced cost. High-fidelity modeling and simulation\n(M\\&S) of AM processes is being developed in Idaho National Laboratory (INL)'s\nMultiphysics Object-Oriented Simulation Environment (MOOSE) to support AM\nprocess optimization and provide a fundamental understanding of the various\nphysical interactions involved. In this paper, we employ Bayesian inverse\nuncertainty quantification (UQ) to quantify the input uncertainties in a\nMOOSE-based melt pool model for AM. Inverse UQ is the process of inversely\nquantifying the input uncertainties while keeping model predictions consistent\nwith the measurement data. The inverse UQ process takes into account\nuncertainties from the model, code, and data while simultaneously\ncharacterizing the uncertain distributions in the input parameters--rather than\nmerely providing best-fit point estimates. We employ measurement data on melt\npool geometry (lengths and depths) to quantify the uncertainties in several\nmelt pool model parameters. Simulation results using the posterior\nuncertainties have shown improved agreement with experimental data, as compared\nto those using the prior nominal values. The resulting parameter uncertainties\ncan be used to replace expert opinions in future uncertainty, sensitivity, and\nvalidation studies.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 23:53:13 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 22:21:20 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Xie", "Ziyu", ""], ["Jiang", "Wen", ""], ["Wang", "Congjian", ""], ["Wu", "Xu", ""]]}, {"id": "2105.05385", "submitter": "Tianxue Hu", "authors": "Tianxue Hu and Claire Arthur", "title": "A Statistical Model for Melody Reduction", "comments": "5 pages, 1 figure. Proceeding and presentation available at Future\n  Directions of Music Cognition but the conference has not yet officially\n  published until summer 2021. http://org.osu.edu/mascats/march-6-talks/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.IR eess.AS stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A commonly-cited reason for the poor performance of automatic chord\nestimation (ACE) systems within music information retrieval (MIR) is that\nnon-chord tones (i.e., notes outside the supporting harmony) contribute to\nerror during the labeling process. Despite the prevalence of machine learning\napproaches in MIR, there are cases where alternative approaches provide a\nsimpler alternative while allowing for insights into musicological practices.\nIn this project, we present a statistical model for predicting chord tones\nbased on music theory rules. Our model is currently focused on predicting chord\ntones in classical music, since composition in this style is highly\nconstrained, theoretically making the placement of chord tones highly\npredictable. Indeed, music theorists have labeling systems for every variety of\nnon-chord tone, primarily classified by the note's metric position and\nintervals of approach and departure. Using metric position, duration, and\nmelodic intervals as predictors, we build a statistical model for predicting\nchord tones using the TAVERN dataset. While our probabilistic approach is\nsimilar to other efforts in the domain of automatic harmonic analysis, our\nfocus is on melodic reduction rather than predicting harmony. However, we hope\nto pursue applications for ACE in the future. Finally, we implement our melody\nreduction model using an existing symbolic visualization tool, to assist with\nmelody reduction and non-chord tone identification for computational musicology\nresearchers and music theorists.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 01:10:35 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Hu", "Tianxue", ""], ["Arthur", "Claire", ""]]}, {"id": "2105.05451", "submitter": "Marvin Pizon", "authors": "Marvin G. Pizon, Ronald R. Baldo, Ruthlyn N. Villarante and Jessica D.\n  Balatero", "title": "Path Analysis Of Covid-19 with the Influence of Air Pressure, Air\n  Temperature, and Relative Humidity", "comments": null, "journal-ref": "International Journal of Advanced Research, 8(4), 224-232 (2020)", "doi": "10.21474/IJAR01/10771", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is one of the most infectious diseases\nand one of the greatest challenge due to global health crisis. The virus has\nbeen transmitted globally and spreading so fast with high incidence. While, the\nvirus still pandemic, the government scramble to seek antiviral treatment and\nvaccines to combat the diseases. This study was conducted to investigate the\ninfluence of air pressure, air temperature, and relative humidity on the number\nof confirmed cases in COVID-19. Based on the result, the calculation of\nreproduced correlation through path decompositions and subsequent comparison to\nthe empirical correlation indicated that the path model fits the empirical\ndata. The identified factor significantly influenced the number of confirmed\ncases of COVID-19. Therefore, the number of daily confirmed cases of COVID-19\nmay reduce as the amount of relative humidity increases; relative humidity will\nincrease as the amount of air temperature decreases; and the amount of air\ntemperature will decrease as the amount of air pressure decreases. Thus, it is\nrecommended that policy-making bodies consider the result of this study when\nimplementing programs for COVID-19 and increase public awareness on the effects\nof weather condition, as it is one of the factors to control the number of\nCOVID-19 cases.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:19:38 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Pizon", "Marvin G.", ""], ["Baldo", "Ronald R.", ""], ["Villarante", "Ruthlyn N.", ""], ["Balatero", "Jessica D.", ""]]}, {"id": "2105.05471", "submitter": "Joceline Lega", "authors": "Hannah R. Biegel, Joceline Lega", "title": "EpiCovDA: a mechanistic COVID-19 forecasting model with data\n  assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce a minimalist outbreak forecasting model that combines\ndata-driven parameter estimation with variational data assimilation. By\nfocusing on the fundamental components of nonlinear disease transmission and\nrepresenting data in a domain where model stochasticity simplifies into a\nprocess with independent increments, we design an approach that only requires\nfour parameters to be estimated. We illustrate this novel methodology on\nCOVID-19 forecasts. Results include case count and deaths predictions for the\nUS and all of its 50 states, the District of Columbia, and Puerto Rico. The\nmethod is computationally efficient and is not disease- or location-specific.\nIt may therefore be applied to other outbreaks or other countries, provided\ncase counts and/or deaths data are available.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:12:57 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Biegel", "Hannah R.", ""], ["Lega", "Joceline", ""]]}, {"id": "2105.05829", "submitter": "Soichiro Yamauchi", "authors": "Shiro Kuriwaki, Soichiro Yamauchi", "title": "Synthetic Area Weighting for Measuring Public Opinion in Small Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The comparison of subnational areas is ubiquitous but survey samples of these\nareas are often biased or prohibitively small. Researchers turn to methods such\nas multilevel regression and poststratification (MRP) to improve the efficiency\nof estimates by partially pooling data across areas via random effects.\nHowever, the random effect approach can pool observations only through\narea-level aggregates. We instead propose a weighting estimator, the synthetic\narea estimator, which weights on variables measured only in the survey to\npartially pool observations individually. The proposed method consists of\ntwo-step weighting: first to adjust differences across areas and then to adjust\nfor differences between the sample and population. Unlike MRP, our estimator\ncan directly use the national weights that are often estimated from pollsters\nusing proprietary information. Our approach also clarifies the assumptions\nneeded for valid partial pooling, without imposing an outcome model. We apply\nthe proposed method to estimate the support for immigration policies at the\ncongressional district level in Florida. Our empirical results show that small\narea estimation models with insufficient covariates can mask opinion\nheterogeneities across districts.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:41:25 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kuriwaki", "Shiro", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2105.05850", "submitter": "Marvin Pizon", "authors": "Marvin G. Pizon and Shiryl T. Ytoc", "title": "A Path Model to Infer Mathematics Performance: The Interrelated Impact\n  of Motivation, Attitude, Learning Style and Teaching Strategies Variables", "comments": "arXiv admin note: text overlap with arXiv:2105.05451", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The present study aims at exploring predictors influencing mathematics\nperformance. In particular, the research focuses on four subject components\nsuch as motivation, attitude towards mathematics, learning style, and teaching\nstrategies. The study respondents have involved a sample of 240 students from\nAgusan del Sur State College of Agriculture and Technology (ASSCAT). Path\nanalysis will be used to test the direct and indirect relations between the\npredictors and mathematics performance. Based on the result, the calculation of\nreproduced correlation through path decompositions and subsequent comparison to\nthe empirical correlation indicated that the path model fits the observed data.\nResults also revealed that a large proportion of mathematics performance could\nbe predicted from the attitude towards mathematics, learning style, and\nteaching strategies. Moreover, attitude towards mathematics, learning style,\nand teaching strategies influence mathematics performance directly and\nindirectly.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 06:26:14 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pizon", "Marvin G.", ""], ["Ytoc", "Shiryl T.", ""]]}, {"id": "2105.06142", "submitter": "Jessica Silva Lomba", "authors": "Jessica Silva Lomba and Maria Isabel Fraga Alves", "title": "Threshold selection for wave heights: asymptotic methods based on\n  L-moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two automatic threshold selection (TS) methods for Extreme Value analysis\nunder a peaks-over-threshold (POT) approach are presented and evaluated, both\nbuilt on: fitting the Generalized Pareto distribution (GPd) to excesses'\nsamples over candidate levels ; the GPd-specific relation between L-skewness\nand L-kurtosis; the asymptotic behaviour of the matching L-statistics.\nPerformance is illustrated on significant wave heights data sets and compared\nto the L-moment-based heuristic in [10], which is found to be favorable.\n  PUBLISHED VERSION AVAILABLE AT:\nhttps://www.spestatistica.pt/storage/app/uploads/public/609/28f/6d0/60928f6d08a0c016386627.pdf\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:42:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lomba", "Jessica Silva", ""], ["Alves", "Maria Isabel Fraga", ""]]}, {"id": "2105.06324", "submitter": "Roger Peng", "authors": "Roger D. Peng, Hilary S. Parker", "title": "Perspective on Data Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of data science currently enjoys a broad definition that includes a\nwide array of activities which borrow from many other established fields of\nstudy. Having such a vague characterization of a field in the early stages\nmight be natural, but over time maintaining such a broad definition becomes\nunwieldy and impedes progress. In particular, the teaching of data science is\nhampered by the seeming need to cover many different points of interest. Data\nscientists must ultimately identify the core of the field by determining what\nmakes the field unique and what it means to develop new knowledge in data\nscience. In this review we attempt to distill some core ideas from data science\nby focusing on the iterative process of data analysis and develop some\ngeneralizations from past experience. Generalizations of this nature could form\nthe basis of a theory of data science and would serve to unify and scale the\nteaching of data science to large audiences.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 14:24:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Peng", "Roger D.", ""], ["Parker", "Hilary S.", ""]]}, {"id": "2105.06534", "submitter": "Agatha Rodrigues Dr.", "authors": "Agatha Rodrigues and Lucas Lacerda and Rossana Pulcineli Vieira\n  Francisco", "title": "Brazilian Obstetric Observatory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covid-19 is responsible for high mortality in all countries, with the\nmaternal population it is no different. Countries with a high rate of maternal\nmortality have deficiencies in the health care of pregnant women and women who\nhave recently given birth, which will certainly be enhanced in a situation of\noverload in the health system, as occurred in this pandemic. Understanding the\nimpact of the pandemic on maternal health is essential to discuss public\npolicies and assist in solutions to future crises. With that in mind, we\npresent the Brazilian Obstetric Observatory COVID-19 (OOBr COVID-19). OOBr\nCOVID-19 is a dynamic panel with analyzes of the cases of pregnant and\npostpartum women with Severe Acute Respiratory Syndrome (SARI) during the\npandemic due to the new coronavirus. In this article, we present data loading,\ncase selections, and processing of the variables for the analyzes available in\nOOBr COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 20:04:09 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 15:07:18 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Rodrigues", "Agatha", ""], ["Lacerda", "Lucas", ""], ["Francisco", "Rossana Pulcineli Vieira", ""]]}, {"id": "2105.06559", "submitter": "Theodore Huang", "authors": "Theodore Huang, Gregory Idos, Christine Hong, Stephen Gruber, Giovanni\n  Parmigiani, Danielle Braun", "title": "Extending Models Via Gradient Boosting: An Application to Mendelian\n  Models", "comments": "46 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving existing widely-adopted prediction models is often a more efficient\nand robust way towards progress than training new models from scratch. Existing\nmodels may (a) incorporate complex mechanistic knowledge, (b) leverage\nproprietary information and, (c) have surmounted barriers to adoption. Compared\nto model training, model improvement and modification receive little attention.\nIn this paper we propose a general approach to model improvement: we combine\ngradient boosting with any previously developed model to improve model\nperformance while retaining important existing characteristics. To exemplify,\nwe consider the context of Mendelian models, which estimate the probability of\ncarrying genetic mutations that confer susceptibility to disease by using\nfamily pedigrees and health histories of family members. Via simulations we\nshow that integration of gradient boosting with an existing Mendelian model can\nproduce an improved model that outperforms both that model and the model built\nusing gradient boosting alone. We illustrate the approach on genetic testing\ndata from the USC-Stanford Cancer Genetics Hereditary Cancer Panel (HCP) study.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:21:05 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Huang", "Theodore", ""], ["Idos", "Gregory", ""], ["Hong", "Christine", ""], ["Gruber", "Stephen", ""], ["Parmigiani", "Giovanni", ""], ["Braun", "Danielle", ""]]}, {"id": "2105.06573", "submitter": "Apostolos Chalkis", "authors": "Ludovic Cal\\`es, Apostolos Chalkis, Ioannis Z. Emiris", "title": "The cross-sectional distribution of portfolio returns and applications", "comments": "49 pages, 12 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE econ.TH stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to develop new mathematical and computational tools for\nmodeling the distribution of portfolio returns across portfolios. We establish\nrelevant mathematical formulas and propose efficient algorithms, drawing upon\npowerful techniques in computational geometry and the literature on splines, to\ncompute the probability density function, the cumulative distribution function,\nand the k-th moment of the probability function. Our algorithmic tools and\nimplementations efficiently handle portfolios with 10000 assets, and compute\nmoments of order k up to 40 in a few seconds, thus handling real-life\nscenarios. We focus on the long-only strategy which is the most common type of\ninvestment, i.e. on portfolios whose weights are non-negative and sum up to 1;\nour approach is readily generalizable. Thus, we leverage a geometric\nrepresentation of the stock market, where the investment set defines a simplex\npolytope. The cumulative distribution function corresponds to a portfolio score\ncapturing the percentage of portfolios yielding a return not exceeding a given\nvalue. We introduce closed-form analytic formulas for the first 4 moments of\nthe cross-sectional returns distribution, as well as a novel algorithm to\ncompute all higher moments. We show that the first 4 moments are a direct\nmapping of the asset returns' moments. All of our algorithms and solutions are\nfully general and include the special case of equal asset returns, which was\nsometimes excluded in previous works. Finally, we apply our portfolio score in\nthe design of new performance measures and asset management. We found our\nscore-based optimal portfolios less concentrated than the mean-variance\nportfolio and much less risky in terms of ranking.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 22:29:12 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Cal\u00e8s", "Ludovic", ""], ["Chalkis", "Apostolos", ""], ["Emiris", "Ioannis Z.", ""]]}, {"id": "2105.06902", "submitter": "Ethan Lawler", "authors": "Ethan Lawler, Chris Field, Joanna Mills Flemming", "title": "A Play on Birds! The staRVe Package for Analyzing Spatio-Temporal\n  Point-Referenced Data in R", "comments": "27 pages, 7 figures, submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the \\proglang{R} package \\pkg{staRVe} for analyzing\nspatio-temporal point-referenced data in a hierarchical generalized linear\nmixed model framework. Our package is designed to be easy-to-use and\ncomputationally efficient for model fitting, covariate effect estimation,\nprediction, and model validation. Existing workflows can easily take advantage\nof our package, since data input and output uses the \\pkg{sf} and \\pkg{raster}\ndata formats. We develop an understanding of the model through simulation, and\nwork through a typical analysis using a case study of Carolina wren abundance.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:40:54 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lawler", "Ethan", ""], ["Field", "Chris", ""], ["Flemming", "Joanna Mills", ""]]}, {"id": "2105.06941", "submitter": "Konstantina Chalkou", "authors": "Konstantina Chalkou, Ewout Steyerberg, Patrick Bossuyt, Suvitha\n  Subramanian, Pascal Benkert, Jens Kuhle, Giulio Disanto, Ludwig Kappos,\n  Matthias Egger, Georgia Salanti", "title": "Development, validation and clinical usefulness of a prognostic model\n  for relapse in relapsing-remitting multiple sclerosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Prognosis on the occurrence of relapses in individuals with\nRelapsing-Remitting Multiple Sclerosis (RRMS), the most common subtype of\nMultiple Sclerosis (MS), could support individualized decisions and disease\nmanagement and could be helpful for efficiently selecting patients in future\nrandomized clinical trials. There are only three previously published\nprognostic models on this, all of them with important methodological\nshortcomings.\n  We aim to present the development, internal validation, and evaluation of the\npotential clinical benefit of a prognostic model for relapses for individuals\nwith RRMS using real world data. We followed seven steps to develop and\nvalidate the prognostic model. Finally, we evaluated the potential clinical\nbenefit of the developed prognostic model using decision curve analysis.\n  We selected eight baseline prognostic factors: age, sex, prior MS treatment,\nmonths since last relapse, disease duration, number of prior relapses, expanded\ndisability status scale (EDSS), and gadolinium enhanced lesions. We also\ndeveloped a web application where the personalized probabilities to relapse\nwithin two years are calculated automatically. The optimism-corrected\nc-statistic is 0.65 and the optimism-corrected calibration slope was 0.92. The\nmodel appears to be clinically useful between the range 15% and 30% of the\nthreshold probability to relapse.\n  The prognostic model we developed offers several advantages in comparison to\npreviously published prognostic models on RRMS. Importantly, we assessed the\npotential clinical benefit to better quantify the clinical impact of the model.\nOur web application, once externally validated in the future, could be used by\npatients and doctors to calculate the individualized probability to relapse\nwithin two years and to inform the management of their disease.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 16:34:22 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Chalkou", "Konstantina", ""], ["Steyerberg", "Ewout", ""], ["Bossuyt", "Patrick", ""], ["Subramanian", "Suvitha", ""], ["Benkert", "Pascal", ""], ["Kuhle", "Jens", ""], ["Disanto", "Giulio", ""], ["Kappos", "Ludwig", ""], ["Egger", "Matthias", ""], ["Salanti", "Georgia", ""]]}, {"id": "2105.06966", "submitter": "Mihaela Puica", "authors": "Mihaela Puica and Fred Espen Benth", "title": "A Spatio-Temporal Model for Predicting Wind Speeds in Southern\n  California", "comments": "Author Original Manuscript, under review in peer-reviwed journal\n  Communications in Statistics - Case Studies and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The share of wind power in fuel mixes worldwide has increased considerably.\nThe main ingredient when deriving wind power predictions are wind speed data;\nthe closer to the wind farms, the better they forecast the power supply. The\ncurrent paper proposes a hybrid model for predicting wind speeds at convenient\nlocations. It is then applied to Southern California power price area. We build\nrandom fields with time series of gridded historical forecasts and actual wind\nspeed observations. We estimate with ordinary kriging the spatial variability\nof the temporal parameters and derive predictions. The advantages of this work\nare twofold: (1) an accurate daily wind speed forecast at any location in the\narea and (2) a general method applicable to other markets.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:14:08 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Puica", "Mihaela", ""], ["Benth", "Fred Espen", ""]]}, {"id": "2105.06995", "submitter": "Nathan Hara", "authors": "Nathan C. Hara, Nicolas Unger, Jean-Baptiste Delisle, Rodrigo D\\'iaz,\n  Damien S\\'egransan", "title": "Improving exoplanet detection capabilities with the false inclusion\n  probability. Comparison with other detection criteria in the context of\n  radial velocities", "comments": "Accepted for publication in Astronomy & Astrophysics", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context. In exoplanet searches with radial velocity data, the most common\nstatistical significance metrics are the Bayes factor and the false alarm\nprobability (FAP). Both have proved useful, but do not directly address whether\nan exoplanet detection should be claimed. Furthermore, it is unclear which\ndetection threshold should be taken and how robust the detections are to model\nmisspecification. Aims. The present work aims at defining a detection criterion\nwhich conveys as precisely as possible the information needed to claim an\nexoplanet detection. We compare this new criterion to existing ones in terms of\nsensitivity and robustness. Methods. We define a significance metric called the\nfalse inclusion probability (FIP) based on the posterior probability of\npresence of a planet. Posterior distributions are computed with the nested\nsampling package Polychord. We show that for FIP and Bayes factor calculations,\ndefining priors on linear parameters as Gaussian mixture models allows to\nsignificantly speed up computations. The performances of the FAP, Bayes factor\nand FIP are studied with simulations as well as analytical arguments. We\ncompare the methods assuming the model is correct, then evaluate their\nsensitivity to the prior and likelihood choices. Results. Among other\nproperties, the FIP offers ways to test the reliability of the significance\nlevels, it is particularly efficient to account for aliasing and allows to\nexclude the presence of planets with a certain confidence. We find that, in our\nsimulations, the FIP outperforms existing detection metrics. We show that\nplanet detections are sensitive to priors on period and semi-amplitude and that\nletting free the noise parameters offers better performances than fixing a\nnoise model based on a fit to ancillary indicators.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:59:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hara", "Nathan C.", ""], ["Unger", "Nicolas", ""], ["Delisle", "Jean-Baptiste", ""], ["D\u00edaz", "Rodrigo", ""], ["S\u00e9gransan", "Damien", ""]]}, {"id": "2105.07060", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, Marco Longfils, Nicolas Remy", "title": "Trimmed Match Design for Randomized Paired Geo Experiments", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to measure the incremental Return On Ad Spend (iROAS) is a fundamental\nproblem for the online advertising industry. A standard modern tool is to run\nrandomized geo experiments, where experimental units are non-overlapping\nad-targetable geographical areas (Vaver & Koehler 2011). However, how to design\na reliable and cost-effective geo experiment can be complicated, for example:\n1) the number of geos is often small, 2) the response metric (e.g. revenue)\nacross geos can be very heavy-tailed due to geo heterogeneity, and furthermore\n3) the response metric can vary dramatically over time. To address these\nissues, we propose a robust nonparametric method for the design, called Trimmed\nMatch Design (TMD), which extends the idea of Trimmed Match (Chen & Au 2019)\nand furthermore integrates the techniques of optimal subset pairing and sample\nsplitting in a novel and systematic manner. Some simulation and real case\nstudies are presented. We also point out a few open problems for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:28:01 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 21:00:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chen", "Aiyou", ""], ["Longfils", "Marco", ""], ["Remy", "Nicolas", ""]]}, {"id": "2105.07811", "submitter": "Alexander Bauer", "authors": "Alexander Bauer, Andr\\'e Klima, Jana Gau{\\ss}, Hannah K\\\"umpel,\n  Andreas Bender, Helmut K\\\"uchenhoff", "title": "Mundus vult decipi, ergo decipiatur: Visual Communication of Uncertainty\n  in Election Polls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Election poll reporting often focuses on mean values and only subordinately\ndiscusses the underlying uncertainty. Subsequent interpretations are too often\nphrased as certain. Moreover, media coverage rarely adequately takes into\naccount the differences between now- and forecasts. These challenges were\nubiquitous in the context of the 2016 and 2020 U.S. presidential elections, but\nare also present in multi-party systems like Germany. We discuss potential\nsources of bias in nowcasting and forecasting and review the current standards\nin the visual presentation of survey-based nowcasts. Concepts are presented to\nattenuate the issue of falsely perceived accuracy. We discuss multiple visual\npresentation techniques for central aspects in poll reporting. One key idea is\nthe use of Probabilities of Events instead of party shares. The presented ideas\noffer modern and improved ways to communicate (changes in) the electoral mood\nfor the general media.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:02:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bauer", "Alexander", ""], ["Klima", "Andr\u00e9", ""], ["Gau\u00df", "Jana", ""], ["K\u00fcmpel", "Hannah", ""], ["Bender", "Andreas", ""], ["K\u00fcchenhoff", "Helmut", ""]]}, {"id": "2105.07834", "submitter": "Mariangela Guidolin", "authors": "Alessandro Bessi, Mariangela Guidolin, Piero Manfredi", "title": "Are renewable energies on a sustained path? Analysis of selected\n  case-studies from the pre-pandemic-era", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Provided widespread vaccination will bring the COVID-19 pandemic under full\ncontrol worldwide, the contrast to climate change and the energy transition as\none of its main actions will return at the top of national and international\npolicy agendas. This paper employs multivariate diffusion models to investigate\nand quantitatively assess the competitive power of renewable energy\ntechnologies and their perspectives along the invoked energy transition. The\nstudy was conducted for the period 1965-2019 on a number of selected case\nstudies, that were considered critically representative of the current\ntransition process in view of their energy and political context. The dynamic\nrelationship between renewable technologies and natural gas has been at the\ncore of the analysis, trying to establish whether gas could be considered as a\nbridging technology or a lock-in. The main findings show that in all the\nanalyzed countries RETs have exerted a strongly competitive effect towards gas.\nIn most cases, gas is found to have a bridging role, aiding the uptake of\nrenewables.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 13:48:57 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Bessi", "Alessandro", ""], ["Guidolin", "Mariangela", ""], ["Manfredi", "Piero", ""]]}, {"id": "2105.07863", "submitter": "Owen Madin", "authors": "Owen C. Madin (1), Simon Boothroyd (2), Richard A. Messerly (3), John\n  D. Chodera (4), Josh Fass (5), Michael R. Shirts (1) ((1) Department of\n  Chemical & Biological Engineering, University of Colorado Boulder, Boulder,\n  CO, (2) Boothroyd Scientific Consulting Ltd., London, United Kingdom, (3)\n  Theoretical Division, Los Alamos National Laboratory, Los Alamos, NM, (4)\n  Computational & Systems Biology Program, Sloan Kettering Institute, Memorial\n  Sloan Kettering Cancer Center, New York, NY, (5) Tri-Institutional PhD\n  Program in Computational Biology and Medicine, Weill Cornell Graduate School\n  of Medical Sciences, New York, NY)", "title": "Bayesian inference-driven model parameterization and model selection for\n  2CLJQ fluid models", "comments": "55 pages, 47 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A high level of physical detail in a molecular model improves its ability to\nperform high accuracy simulations, but can also significantly affect its\ncomplexity and computational cost. In some situations, it is worthwhile to add\nadditional complexity to a model to capture properties of interest; in others,\nadditional complexity is unnecessary and can make simulations computationally\ninfeasible. In this work we demonstrate the use of Bayes factors for molecular\nmodel selection, using Monte Carlo sampling techniques to evaluate the evidence\nfor different levels of complexity in the two-centered Lennard-Jones +\nquadrupole (2CLJQ) fluid model. Examining three levels of nested model\ncomplexity, we demonstrate that the use of variable quadrupole and bond length\nparameters in this model framework is justified only sometimes. We also explore\nthe effect of the Bayesian prior distribution on the Bayes factors, as well as\nways to propose meaningful prior distributions. This Bayesian Markov Chain\nMonte Carlo (MCMC) process is enabled by the use of analytical surrogate models\nthat accurately approximate the physical properties of interest. This work\npaves the way for further atomistic model selection work via Bayesian inference\nand surrogate modeling\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:15:24 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Madin", "Owen C.", ""], ["Boothroyd", "Simon", ""], ["Messerly", "Richard A.", ""], ["Chodera", "John D.", ""], ["Fass", "Josh", ""], ["Shirts", "Michael R.", ""]]}, {"id": "2105.07982", "submitter": "Moritz Herle", "authors": "Bianca De Stavola, Moritz Herle and Andrew Pickles", "title": "Framing causal questions in life course epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We describe the principles of counterfactual thinking in providing more\nprecise definitions of causal effects and some of the implications of this work\nfor the way in which causal questions in life course research are framed and\nevidence evaluated. Terminology is explained and examples of common life course\nanalyses are discussed that focus on the timing of exposures, the mediation of\ntheir effects, observed and unobserved confounders, and measurement error. The\nexamples are illustrated by analyses using singleton and twin cohort data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:08:15 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["De Stavola", "Bianca", ""], ["Herle", "Moritz", ""], ["Pickles", "Andrew", ""]]}, {"id": "2105.08004", "submitter": "Jonathan Koh", "authors": "Jonathan Koh, Fran\\c{c}ois Pimont, Jean-Luc Dupuy, Thomas Opitz", "title": "Spatiotemporal wildfire modeling through point processes with moderate\n  and extreme marks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate spatiotemporal modeling of conditions leading to moderate and large\nwildfires provides better understanding of mechanisms driving fire-prone\necosystems and improves risk management. We here develop a joint model for the\noccurrence intensity and the wildfire size distribution by combining\nextreme-value theory and point processes within a novel Bayesian hierarchical\nmodel, and use it to study daily summer wildfire data for the French\nMediterranean basin during 1995--2018. The occurrence component models wildfire\nignitions as a spatiotemporal log-Gaussian Cox process. Burnt areas are\nnumerical marks attached to points and are considered as extreme if they exceed\na high threshold. The size component is a two-component mixture varying in\nspace and time that jointly models moderate and extreme fires. We capture\nnon-linear influence of covariates (Fire Weather Index, forest cover) through\ncomponent-specific smooth functions, which may vary with season. We propose\nestimating shared random effects between model components to reveal and\ninterpret common drivers of different aspects of wildfire activity. This leads\nto increased parsimony and reduced estimation uncertainty with better\npredictions. Specific stratified subsampling of zero counts is implemented to\ncope with large observation vectors. We compare and validate models through\npredictive scores and visual diagnostics. Our methodology provides a holistic\napproach to explaining and predicting the drivers of wildfire activity and\nassociated uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:39:36 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:53:31 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Koh", "Jonathan", ""], ["Pimont", "Fran\u00e7ois", ""], ["Dupuy", "Jean-Luc", ""], ["Opitz", "Thomas", ""]]}, {"id": "2105.08013", "submitter": "Benjamin Seiler", "authors": "Benjamin B. Seiler, Masayoshi Mase, Art B. Owen", "title": "What makes you unique?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a uniqueness Shapley measure to compare the extent to\nwhich different variables are able to identify a subject. Revealing the value\nof a variable on subject $t$ shrinks the set of possible subjects that $t$\ncould be. The extent of the shrinkage depends on which other variables have\nalso been revealed. We use Shapley value to combine all of the reductions in\nlog cardinality due to revealing a variable after some subset of the other\nvariables has been revealed. This uniqueness Shapley measure can be aggregated\nover subjects where it becomes a weighted sum of conditional entropies.\nAggregation over subsets of subjects can address questions like how identifying\nis age for people of a given zip code. Such aggregates have a corresponding\nexpression in terms of cross entropies. We use uniqueness Shapley to\ninvestigate the differential effects of revealing variables from the North\nCarolina voter registration rolls and in identifying anomalous solar flares. An\nenormous speedup (approaching 2000 fold in one example) is obtained by using\nthe all dimension trees of Moore and Lee (1998) to store the cardinalities we\nneed.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:53:16 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Seiler", "Benjamin B.", ""], ["Mase", "Masayoshi", ""], ["Owen", "Art B.", ""]]}, {"id": "2105.08133", "submitter": "Tim Leung", "authors": "Tim Leung and Theodore Zhao", "title": "Adaptive Complementary Ensemble EMD and Energy-Frequency Spectra of\n  Cryptocurrency Prices", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the price dynamics of cryptocurrencies using adaptive complementary\nensemble empirical mode decomposition (ACE-EMD) and Hilbert spectral analysis.\nThis is a multiscale noise-assisted approach that decomposes any time series\ninto a number of intrinsic mode functions, along with the corresponding\ninstantaneous amplitudes and instantaneous frequencies. The decomposition is\nadaptive to the time-varying volatility of each cryptocurrency price evolution.\nDifferent combinations of modes allow us to reconstruct the time series using\ncomponents of different timescales. We then apply Hilbert spectral analysis to\ndefine and compute the instantaneous energy-frequency spectrum of each\ncryptocurrency to illustrate the properties of various timescales embedded in\nthe original time series.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:53:45 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Leung", "Tim", ""], ["Zhao", "Theodore", ""]]}, {"id": "2105.08180", "submitter": "Hao Yan", "authors": "Hao Yan, Nurretin Dorukhan Sergin, William A. Brenneman, Stephen\n  Joseph Lange, Shan Ba", "title": "Deep Multistage Multi-Task Learning for Quality Prediction of Multistage\n  Manufacturing Systems", "comments": "Accepted by Journal of Quality Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In multistage manufacturing systems, modeling multiple quality indices based\non the process sensing variables is important. However, the classic modeling\ntechnique predicts each quality variable one at a time, which fails to consider\nthe correlation within or between stages. We propose a deep multistage\nmulti-task learning framework to jointly predict all output sensing variables\nin a unified end-to-end learning framework according to the sequential system\narchitecture in the MMS. Our numerical studies and real case study have shown\nthat the new model has a superior performance compared to many benchmark\nmethods as well as great interpretability through developed variable selection\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:09:36 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Yan", "Hao", ""], ["Sergin", "Nurretin Dorukhan", ""], ["Brenneman", "William A.", ""], ["Lange", "Stephen Joseph", ""], ["Ba", "Shan", ""]]}, {"id": "2105.08188", "submitter": "Rachel Prudden", "authors": "Rachel Prudden, Niall Robinson, Peter Challenor, Richard Everson", "title": "Stochastic Downscaling to Chaotic Weather Regimes using Spatially\n  Conditioned Gaussian Random Fields with Adaptive Covariance", "comments": "32 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Downscaling aims to link the behaviour of the atmosphere at fine scales to\nproperties measurable at coarser scales, and has the potential to provide high\nresolution information at a lower computational and storage cost than numerical\nsimulation alone. This is especially appealing for targeting convective scales,\nwhich are at the edge of what is possible to simulate operationally. Since\nconvective scale weather has a high degree of independence from larger scales,\na generative approach is essential. We here propose a statistical method for\ndownscaling moist variables to convective scales using conditional Gaussian\nrandom fields, with an application to wet bulb potential temperature (WBPT)\ndata over the UK. Our model uses an adaptive covariance estimation to capture\nthe variable spatial properties at convective scales. We further propose a\nmethod for the validation, which has historically been a challenge for\ngenerative models.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:51:19 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Prudden", "Rachel", ""], ["Robinson", "Niall", ""], ["Challenor", "Peter", ""], ["Everson", "Richard", ""]]}, {"id": "2105.08377", "submitter": "Thierry Roncalli", "authors": "Thierry Roncalli, Amina Cherief, Fatma Karray-Meziou, Margaux Regnault", "title": "Liquidity Stress Testing in Asset Management -- Part 2. Modeling the\n  Asset Liquidity Risk", "comments": "86 pages, 36 figures, 44 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.PM q-fin.TR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article is part of a comprehensive research project on liquidity risk in\nasset management, which can be divided into three dimensions. The first\ndimension covers liability liquidity risk (or funding liquidity) modeling, the\nsecond dimension focuses on asset liquidity risk (or market liquidity)\nmodeling, and the third dimension considers the asset-liability management of\nthe liquidity gap risk (or asset-liability matching). The purpose of this\nresearch is to propose a methodological and practical framework in order to\nperform liquidity stress testing programs, which comply with regulatory\nguidelines (ESMA, 2019, 2020) and are useful for fund managers. The review of\nthe academic literature and professional research studies shows that there is a\nlack of standardized and analytical models. The aim of this research project is\nthen to fill the gap with the goal of developing mathematical and statistical\napproaches, and providing appropriate answers.\n  In this second article focused on asset liquidity risk modeling, we propose a\nmarket impact model to estimate transaction costs. After presenting a toy model\nthat helps to understand the main concepts of asset liquidity, we consider a\ntwo-regime model, which is based on the power-law property of price impact.\nThen, we define several asset liquidity measures such as liquidity cost,\nliquidation ratio and shortfall or time to liquidation in order to assess the\ndifferent dimensions of asset liquidity. Finally, we apply this asset liquidity\nframework to stocks and bonds and discuss the issues of calibrating the\ntransaction cost model.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:07:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Roncalli", "Thierry", ""], ["Cherief", "Amina", ""], ["Karray-Meziou", "Fatma", ""], ["Regnault", "Margaux", ""]]}, {"id": "2105.08484", "submitter": "Miguel Gonz\\'alez-Duque", "authors": "Miguel Gonz\\'alez-Duque, Rasmus Berg Palm and Sebastian Risi", "title": "Fast Game Content Adaptation Through Bayesian-based Player Modelling", "comments": "Accepted at CoG2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In games, as well as many user-facing systems, adapting content to users'\npreferences and experience is an important challenge. This paper explores a\nnovel method to realize this goal in the context of dynamic difficulty\nadjustment (DDA). Here the aim is to constantly adapt the content of a game to\nthe skill level of the player, keeping them engaged by avoiding states that are\neither too difficult or too easy. Current systems for DDA rely on expensive\ndata mining, or on hand-crafted rules designed for particular domains, and\nusually adapts to keep players in the flow, leaving no room for the designer to\npresent content that is purposefully easy or difficult. This paper presents\nFast Bayesian Content Adaption (FBCA), a system for DDA that is agnostic to the\ndomain and that can target particular difficulties. We deploy this framework in\ntwo different domains: the puzzle game Sudoku, and a simple Roguelike game. By\nmodifying the acquisition function's optimization, we are reliably able to\npresent a content with a bespoke difficulty for players with different skill\nlevels in less than five iterations for Sudoku and fifteen iterations for the\nsimple Roguelike. Our method significantly outperforms simpler DDA heuristics\nwith the added benefit of maintaining a model of the user. These results point\ntowards a promising alternative for content adaption in a variety of different\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 12:56:44 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 22:25:53 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Gonz\u00e1lez-Duque", "Miguel", ""], ["Palm", "Rasmus Berg", ""], ["Risi", "Sebastian", ""]]}, {"id": "2105.08493", "submitter": "Anna Zink", "authors": "Anna Zink and Sherri Rose", "title": "Identifying Undercompensated Groups Defined By Multiple Attributes in\n  Risk Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Risk adjustment in health care aims to redistribute payments to insurers\nbased on costs. However, risk adjustment formulas are known to underestimate\ncosts for some groups of patients. This undercompensation makes these groups\nunprofitable to insurers and creates incentives for insurers to discriminate.\nWe develop a machine learning method for \"group importance\" to identify\nunprofitable groups defined by multiple attributes, improving on the arbitrary\nnature of existing evaluations. This procedure was designed to evaluate the\nrisk adjustment formulas used in the U.S. health insurance Marketplaces as well\nas Medicare. We find that a number of previously unidentified groups with\nmultiple chronic conditions are undercompensated in the Marketplaces risk\nadjustment formula, while groups without chronic conditions tend to be\novercompensated in the Marketplaces. The magnitude of undercompensation when\ndefining groups with multiple attributes is larger than with single attributes.\nNo complex groups were found to be consistently under- or overcompensated in\nthe Medicare risk adjustment formula. Our work provides policy makers with new\ninformation on potential targets of discrimination in the health care system\nand a path towards more equitable health coverage.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:09:13 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 16:03:36 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Zink", "Anna", ""], ["Rose", "Sherri", ""]]}, {"id": "2105.08512", "submitter": "Laura Tupper", "authors": "Laura L. Tupper and Charles R. Keese and David S. Matteson", "title": "Classifying Contaminated Cell Cultures using Time Series Features", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We examine the use of time series data, derived from Electric Cell-substrate\nImpedance Sensing (ECIS), to differentiate between standard mammalian cell\ncultures and those infected with a mycoplasma organism. We perform\nfeature-based classification, extracting interpretable features from the ECIS\ntime courses. We can achieve high classification accuracy using only two\nfeatures, which depend on the cell line under examination. Initial results also\nshow the existence of experimental variation between plates and suggest types\nof features that may prove more robust to such variation. Our paper is the\nfirst to perform a broad examination of ECIS time course features in the\ncontext of detecting contamination, and to describe and suggest possibilities\nfor ameliorating plate-to-plate variation.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 01:51:29 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Tupper", "Laura L.", ""], ["Keese", "Charles R.", ""], ["Matteson", "David S.", ""]]}, {"id": "2105.08514", "submitter": "Matti Raitoharju", "authors": "Matti Raitoharju, Henri Nurminen, Demet Cilden-Guler, and Simo\n  S\\\"arkk\\\"a", "title": "Kalman filtering with empirical noise models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Kalman filter extensions assume Gaussian noise and when the noise is\nnon-Gaussian, usually other types of filters are used. These filters, such as\nparticle filter variants, are computationally more demanding than Kalman type\nfilters. In this paper, we present an algorithm for building models and using\nthem with a Kalman type filter when there is empirically measured data of the\nmeasurement errors. The paper evaluates the proposed algorithm in three\nexamples. The first example uses simulated Student-t distributed measurement\nerrors and the proposed algorithm is compared with algorithms designed\nspecifically for Student-t distribution. Last two examples use real measured\nerrors, one with real data from an Ultra Wideband (UWB) ranging system, and the\nother using low-Earth orbiting satellite magnetometer measurements. The results\nshow that the proposed algorithm is more accurate than algorithms that use\nGaussian assumptions and has similar accuracy to algorithms that are\nspecifically designed for a certain probability distribution.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 21:10:02 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Raitoharju", "Matti", ""], ["Nurminen", "Henri", ""], ["Cilden-Guler", "Demet", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2105.08606", "submitter": "Antoine D. Meyer", "authors": "Antoine D. Meyer (1), David A. van Dyk (1), Vinay L. Kashyap (2), Luis\n  F. Campos (3), David E. Jones (4), Aneta Siemiginowska (2), Andreas Zezas (2\n  and 5) ((1) Imperial College London, Statistics Section, Department of\n  Mathematics, (2) Center for Astrophysics, Harvard & Smithsonian, (3) Harvard\n  University, Department of Statistics, (4) Texas A&M University, Department of\n  Statistics, (5) Department of Physics, University of Crete)", "title": "eBASCS: Disentangling Overlapping Astronomical Sources II, using\n  Spatial, Spectral, and Temporal Information", "comments": null, "journal-ref": null, "doi": "10.1093/mnras/stab1456", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The analysis of individual X-ray sources that appear in a crowded field can\neasily be compromised by the misallocation of recorded events to their\noriginating sources. Even with a small number of sources, that nonetheless have\noverlapping point spread functions, the allocation of events to sources is a\ncomplex task that is subject to uncertainty. We develop a Bayesian method\ndesigned to sift high-energy photon events from multiple sources with\noverlapping point spread functions, leveraging the differences in their\nspatial, spectral, and temporal signatures. The method probabilistically\nassigns each event to a given source. Such a disentanglement allows more\ndetailed spectral or temporal analysis to focus on the individual component in\nisolation, free of contamination from other sources or the background. We are\nalso able to compute source parameters of interest like their locations,\nrelative brightness, and background contamination, while accounting for the\nuncertainty in event assignments. Simulation studies that include event arrival\ntime information demonstrate that the temporal component improves event\ndisambiguation beyond using only spatial and spectral information. The proposed\nmethods correctly allocate up to 65% more events than the corresponding\nalgorithms that ignore event arrival time information. We apply our methods to\ntwo stellar X-ray binaries, UV Cet and HBC515 A, observed with Chandra. We\ndemonstrate that our methods are capable of removing the contamination due to a\nstrong flare on UV Cet B in its companion approximately 40 times weaker during\nthat event, and that evidence for spectral variability at timescales of a few\nks can be determined in HBC515 Aa and HBC515 Ab.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:38:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Meyer", "Antoine D.", "", "2\n  and 5"], ["van Dyk", "David A.", "", "2\n  and 5"], ["Kashyap", "Vinay L.", "", "2\n  and 5"], ["Campos", "Luis F.", "", "2\n  and 5"], ["Jones", "David E.", "", "2\n  and 5"], ["Siemiginowska", "Aneta", "", "2\n  and 5"], ["Zezas", "Andreas", "", "2\n  and 5"]]}, {"id": "2105.08679", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Kiranmoy Chatterjee and Prajamitra Bhuyan", "title": "Estimation of Population Size with Heterogeneous Catchability and\n  Behavioural Dependence: Applications to Air and Water Borne Disease\n  Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation based on the capture-recapture experiment is an\ninteresting problem in various fields including epidemiology, criminology,\ndemography, etc. In many real-life scenarios, there exists inherent\nheterogeneity among the individuals and dependency between capture and\nrecapture attempts. A novel trivariate Bernoulli model is considered to\nincorporate these features, and the Bayesian estimation of the model parameters\nis suggested using data augmentation. Simulation results show robustness under\nmodel misspecification and the superiority of the performance of the proposed\nmethod over existing competitors. The method is applied to analyse real case\nstudies on epidemiological surveillance. The results provide interesting\ninsight on the heterogeneity and dependence involved in the capture-recapture\nmechanism. The methodology proposed can assist in effective decision-making and\npolicy formulation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:09:51 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:27:08 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 20:06:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "2105.08686", "submitter": "Mahsa Nadifar", "authors": "Mahsa Nadifar (1 and 2), Hossein Baghishani (1), Thomas Kneib (2) and\n  Afshin Fallah (3) ((1) Shahrood University of Technology, (2) Georg August\n  University, (3) Internatinal Imam Khomeini University)", "title": "Flexible Bayesian Modeling of Counts: Constructing Penalized Complexity\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the data, particularly in medicine and disease mapping are count.\nIndeed, the under or overdispersion problem in count data distrusts the\nperformance of the classical Poisson model. For taking into account this\nproblem, in this paper, we introduce a new Bayesian structured additive\nregression model, called gamma count, with enough flexibility in modeling\ndispersion. Setting convenient prior distributions on the model parameters is a\nmomentous issue in Bayesian statistics that characterize the nature of our\nuncertainty parameters. Relying on a recently proposed class of penalized\ncomplexity priors, motivated from a general set of construction principles, we\nderive the prior structure. The model can be formulated as a latent Gaussian\nmodel, and consequently, we can carry out the fast computation by using the\nintegrated nested Laplace approximation method. We investigate the proposed\nmethodology simulation study. Different expropriate prior distribution are\nexamined to provide reasonable sensitivity analysis. To explain the\napplicability of the proposed model, we analyzed two real-world data sets\nrelated to the larynx mortality cancer in Germany and the handball champions\nleague.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:16:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Nadifar", "Mahsa", "", "1 and 2"], ["Baghishani", "Hossein", ""], ["Kneib", "Thomas", ""], ["Fallah", "Afshin", ""]]}, {"id": "2105.08776", "submitter": "Kyu Ha Lee", "authors": "Sebastien Haneuse, Deborah Schrag, Francesca Dominici, Sharon-Lise\n  Normand, and Kyu Ha Lee", "title": "Measuring performance for end-of-life care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although not without controversy, readmission is entrenched as a hospital\nquality metric, with statistical analyses generally based on fitting a\nlogistic-Normal generalized linear mixed model. Such analyses, however, ignore\ndeath as a competing risk, although doing so for clinical conditions with high\nmortality can have profound effects; a hospitals seemingly good performance for\nreadmission may be an artifact of it having poor performance for mortality. In\nthis paper we propose novel multivariate hospital-level performance measures\nfor readmission and mortality, that derive from framing the analysis as one of\ncluster-correlated semi-competing risks data. We also consider a number of\nprofiling-related goals, including the identification of extreme performers and\na bivariate classification of whether the hospital has\nhigher-/lower-than-expected readmission and mortality rates, via a Bayesian\ndecision-theoretic approach that characterizes hospitals on the basis of\nminimizing the posterior expected loss for an appropriate loss function. In\nsome settings, particularly if the number of hospitals is large, the\ncomputational burden may be prohibitive. To resolve this, we propose a series\nof analysis strategies that will be useful in practice. Throughout the methods\nare illustrated with data from CMS on N=17,685 patients diagnosed with\npancreatic cancer between 2000-2012 at one of J=264 hospitals in California.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:49:06 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Haneuse", "Sebastien", ""], ["Schrag", "Deborah", ""], ["Dominici", "Francesca", ""], ["Normand", "Sharon-Lise", ""], ["Lee", "Kyu Ha", ""]]}, {"id": "2105.08835", "submitter": "Samuel W.K. Wong", "authors": "Samuel W.K. Wong and Zongjun Liu", "title": "Conformational variability of loops in the SARS-CoV-2 spike protein", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SARS-CoV-2 spike (S) protein facilitates viral infection, and has been\nthe focus of many structure determination efforts. This paper studies the\nconformations of loops in the S protein based on the available Protein Data\nBank (PDB) structures. Loops, as flexible regions of the protein, are known to\nbe involved in binding and can adopt multiple conformations. We identify the\nloop regions of the S protein, and examine their structural variability across\nthe PDB. While most loops had essentially one stable conformation, 17 of 44\nloop regions were observed to be structurally variable with multiple\nsubstantively distinct conformations. Loop modeling methods were then applied\nto the S protein loop targets, and loops with multiple conformations were found\nto be more challenging for the methods to predict accurately. Sequence variants\nand the up/down structural states of the receptor binding domain were also\nconsidered in the analysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:24:13 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Wong", "Samuel W. K.", ""], ["Liu", "Zongjun", ""]]}, {"id": "2105.08836", "submitter": "David Robertson", "authors": "David S. Robertson, Babak Choodari-Oskooei, Munya Dimairo, Laura\n  Flight, Philip Pallmann, Thomas Jaki", "title": "Point estimation for adaptive trial designs", "comments": "Fix references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent FDA guidance on adaptive clinical trial designs defines bias as \"a\nsystematic tendency for the estimate of treatment effect to deviate from its\ntrue value\", and states that it is desirable to obtain and report estimates of\ntreatment effects that reduce or remove this bias. In many adaptive designs,\nthe conventional end-of-trial point estimates of the treatment effects are\nprone to bias, because they do not take into account the potential and realised\ntrial adaptations. While much of the methodological developments on adaptive\ndesigns have tended to focus on control of type I error rates and power\nconsiderations, in contrast the question of biased estimation has received less\nattention. This article addresses this issue by providing a comprehensive\noverview of proposed approaches to remove or reduce the potential bias in point\nestimation of treatment effects in an adaptive design, as well as illustrating\nhow to implement them. We first discuss how bias can affect standard estimators\nand critically assess the negative impact this can have. We then describe and\ncompare proposed unbiased and bias-adjusted estimators of treatment effects for\ndifferent types of adaptive designs. Furthermore, we illustrate the computation\nof different estimators in practice using a real trial example. Finally, we\npropose a set of guidelines for researchers around the choice of estimators and\nthe reporting of estimates following an adaptive design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:26:11 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:58:54 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 12:32:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Robertson", "David S.", ""], ["Choodari-Oskooei", "Babak", ""], ["Dimairo", "Munya", ""], ["Flight", "Laura", ""], ["Pallmann", "Philip", ""], ["Jaki", "Thomas", ""]]}, {"id": "2105.08845", "submitter": "David Ellis", "authors": "Matthew Aldridge, David Ellis", "title": "Pooled testing and its applications in the COVID-19 pandemic", "comments": "Extended version of a book chapter to appear in \"Pandemics: Insurance\n  and Social Protection\", edited by M. C. Boado-Penas, J. Eisenberg and \\c{S}.\n  \\c{S}ahin and to be published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When testing for a disease such as COVID-19, the standard method is\nindividual testing: we take a sample from each individual and test these\nsamples separately. An alternative is pooled testing (or \"group testing\"),\nwhere samples are mixed together in different pools, and those pooled samples\nare tested. When the prevalence of the disease is low and the accuracy of the\ntest is fairly high, pooled testing strategies can be more efficient than\nindividual testing. In this chapter, we discuss the mathematics of pooled\ntesting and its uses during pandemics, in particular the COVID-19 pandemic. We\nanalyse some one- and two-stage pooling strategies under perfect and imperfect\ntests, and consider the practical issues in the application of such protocols.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 22:00:32 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 15:19:08 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Aldridge", "Matthew", ""], ["Ellis", "David", ""]]}, {"id": "2105.08859", "submitter": "Mikaela Meyer", "authors": "Mikaela Meyer, Ahmed Hassafy, Gina Lewis, Prasun Shrestha, Amelia M.\n  Haviland, and Daniel S. Nagin", "title": "Changes in Crime Rates During the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We estimate changes in the rates of five FBI Part 1 crime (homicide, auto\ntheft, burglary, robbery, and larceny) during the COVID-19 pandemic from March\nthrough December 2020. Using publicly available weekly crime count data from 29\nof the 70 largest cities in the U.S. from January 2018 through December 2020,\nthree different linear regression model specifications are used to detect\nchanges. One detects whether crime trends in four 2020 pre- and post-pandemic\nperiods differ from those in 2018 and 2019. A second looks in more detail at\nthe spring 2020 lockdowns to detect whether crime trends changed over\nsuccessive biweekly periods into the lockdown. The third uses a city-level\nopenness index that we created for the purpose of examining whether the degree\nof openness was associated with changing crime rates. For homicide and auto\ntheft, we find significant increases during all or most of the pandemic. By\ncontrast, we find significant declines in robbery and larceny during all or\npart of the pandemic and no significant changes in burglary over the course of\nthe pandemic. Only larceny rates fluctuated with the degree of each city's\nlockdown.\n  It is unusual for crime rates to move in different directions, and the\nreasons for the mixed findings for these five Part 1 Index crimes, one with no\nchange, two with sustained increases, and two with sustained decreases, are not\nyet known. We hypothesize that the reasons may be related to changes in\nopportunity, and the pandemic provides unique opportunities for future research\nto better understand the forces impacting crime rates. In the absence of a\nclear understanding of the mechanisms by which the pandemic affected crime, in\nthe spirit of evidence-based crime policy, we caution against advancing policy\nat this time based on lessons learned from the pandemic \"natural experiment.\"\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 00:05:31 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Meyer", "Mikaela", ""], ["Hassafy", "Ahmed", ""], ["Lewis", "Gina", ""], ["Shrestha", "Prasun", ""], ["Haviland", "Amelia M.", ""], ["Nagin", "Daniel S.", ""]]}, {"id": "2105.08864", "submitter": "Andrew Whetten", "authors": "Andrew B Whetten, Hannah Demler", "title": "Detection of Multidecadal Changes in Vegetation Dynamics and Association\n  with Intra-annual Climate Variability in the Columbia River Basin", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Leaf Area index is widely used metric for the assessment of vegetation\ndynamics and can be used to assess the impact of regional/local climate\nconditions. The underlying continuity of high resolution spatio-temporal\nphenological processes in the presence of extensive missing values poses a\nnumber of challenges in the detection of changes at a local and regional level.\nThe feasibility of functional data analysis methods were evaluated to improve\nthe exploration of such data. In this paper, an investigation of multidecadal\nvariation of leaf area index (LAI) is conducted in the Columbia Watershed, as\ndetected by NOAA AVHRR satellite imaging, and its inter- and intra-annual\ncorrelation with maximum temperature and precipitation using the ERA-Interim\nReanalysis from 1996 to 2017. A functional cluster analysis model was\nimplemented to identify regions in the Columbia Watershed that exhibit similar\nlong-term greening trends. Across these several regions, the primary source of\nannual LAI variation is a trend toward seasonally earlier and higher recordings\nof regional average maximum LAI. Further exploratory analysis reveals that\nalthough strongly correlated to LAI, maximum temperature and precipitation do\nnot exhibit clear longitudinal trends.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 00:32:58 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Whetten", "Andrew B", ""], ["Demler", "Hannah", ""]]}, {"id": "2105.09062", "submitter": "Silius M. Vandeskog", "authors": "Silius M. Vandeskog, Sara Martino, Daniela Castro-Camilo, H{\\aa}vard\n  Rue", "title": "Modelling short-term precipitation extremes with the blended generalised\n  extreme value distribution", "comments": "20 pages, 4 figures; reference added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The yearly maxima of short-term precipitation are modelled to produce\nimproved spatial maps of return levels over the south of Norway. The newly\nproposed blended generalised extreme value (bGEV) distribution is used as a\nsubstitute for the more standard generalised extreme value (GEV) distribution\nin order to simplify inference. Yearly precipitation maxima are modelled using\na Bayesian hierarchical model with a latent Gaussian field. Fast inference is\nperformed using the framework of integrated nested Laplace approximations\n(INLA). Inference is made less wasteful with a two-step procedure that performs\nseparate modelling of the scale parameter of the bGEV distribution using peaks\nover threshold data. Our model provides good estimates for large return levels\nof short-term precipitation, and it outperforms standard block maxima models.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:08:52 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 09:24:54 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Vandeskog", "Silius M.", ""], ["Martino", "Sara", ""], ["Castro-Camilo", "Daniela", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2105.09065", "submitter": "Richard Yim", "authors": "Richard Yim, Jamie Haddock, Deanna Needell", "title": "Statistical Learning for Best Practices in Tattoo Removal", "comments": "15 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The causes behind complications in laser-assisted tattoo removal are\ncurrently not well understood, and in the literature relating to tattoo removal\nthe emphasis on removal treatment is on removal technologies and tools, not\nbest parameters involved in the treatment process. Additionally, the very\nchallenge of determining best practices is difficult given the complexity of\ninteractions between factors that may correlate to these complications. In this\npaper we apply a battery of classical statistical methods and techniques to\nidentify features that may be closely correlated to causes of complication\nduring the tattoo removal process, and report quantitative evidence for\npotential best practices. We develop elementary statistical descriptions of\ntattoo data collected by the largest gang rehabilitation and reentry\norganization in the world, Homeboy Industries; perform parametric and\nnonparametric tests of significance; and finally, produce a statistical model\nexplaining treatment parameter interactions, as well as develop a ranking\nsystem for treatment parameters utilizing bootstrapping and gradient boosting.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 11:21:43 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Yim", "Richard", ""], ["Haddock", "Jamie", ""], ["Needell", "Deanna", ""]]}, {"id": "2105.09261", "submitter": "Rapha\\\"el d'Andrimont", "authors": "Rapha\\\"el d'Andrimont and Astrid Verhegghen and Guido Lemoine and\n  Pieter Kempeneers and Michele Meroni and Marijn van der Velde", "title": "From parcel to continental scale -- A first European crop type map based\n  on Sentinel-1 and LUCAS Copernicus in-situ observations", "comments": "19 pages, 11 Figures, 5 Tables (without appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detailed parcel-level crop type mapping for the whole European Union (EU) is\nnecessary for the evaluation of agricultural policies. The Copernicus program,\nand Sentinel-1 (S1) in particular, offers the opportunity to monitor\nagricultural land at a continental scale and in a timely manner. However, so\nfar the potential of S1 has not been explored at such a scale. Capitalizing on\nthe unique LUCAS 2018 Copernicus in-situ survey, we present the first\ncontinental crop type map at 10-m spatial resolution for the EU based on S1A\nand S1B Synthetic Aperture Radar observations for the year 2018. Random forest\nclassification algorithms are tuned to detect 19 different crop types. We\nassess the accuracy of this EU crop map with three approaches. First, the\naccuracy is assessed with independent LUCAS core in-situ observations over the\ncontinent. Second, an accuracy assessment is done specifically for main crop\ntypes from farmers declarations from 6 EU member countries or regions totaling\n>3M parcels and 8.21 Mha. Finally, the crop areas derived by classification are\ncompared to the subnational (NUTS 2) area statistics reported by Eurostat. The\noverall accuracy for the map is reported as 80.3% when grouping main crop\nclasses and 76% when considering all 19 crop type classes separately. Highest\naccuracies are obtained for rape and turnip rape with user and produced\naccuracies higher than 96%. The correlation between the remotely sensed\nestimated and Eurostat reported crop area ranges from 0.93 (potatoes) to 0.99\n(rape and turnip rape). Finally, we discuss how the framework presented here\ncan underpin the operational delivery of in-season high-resolution based crop\nmapping.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 17:17:45 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:43:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["d'Andrimont", "Rapha\u00ebl", ""], ["Verhegghen", "Astrid", ""], ["Lemoine", "Guido", ""], ["Kempeneers", "Pieter", ""], ["Meroni", "Michele", ""], ["van der Velde", "Marijn", ""]]}, {"id": "2105.09385", "submitter": "Guanhua Chen", "authors": "Jifan Gao, Philip L. Mar, Guanhua Chen", "title": "More Generalizable Models For Sepsis Detection Under Covariate Shift", "comments": "9 pages, 3 figures", "journal-ref": "American Medical Informatics Association (AMIA) 2021 Virtual\n  Informatics Summit", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sepsis is a major cause of mortality in the intensive care units (ICUs).\nEarly intervention of sepsis can improve clinical outcomes for sepsis patients.\nMachine learning models have been developed for clinical recognition of sepsis.\nA common assumption of supervised machine learning models is that the\ncovariates in the testing data follow the same distributions as those in the\ntraining data. When this assumption is violated (e.g., there is covariate\nshift), models that performed well for training data could perform badly for\ntesting data. Covariate shift happens when the relationships between covariates\nand the outcome stay the same, but the marginal distributions of the covariates\ndiffer among training and testing data. Covariate shift could make clinical\nrisk prediction model nongeneralizable. In this study, we applied covariate\nshift corrections onto common machine learning models and have observed that\nthese corrections can help the models be more generalizable under the\noccurrence of covariate shift when detecting the onset of sepsis.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 20:26:31 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Gao", "Jifan", ""], ["Mar", "Philip L.", ""], ["Chen", "Guanhua", ""]]}, {"id": "2105.09468", "submitter": "Hewei Tang Dr.", "authors": "Hewei Tang, Pengcheng Fu, Christopher S. Sherman, Jize Zhang, Xin Ju,\n  Fran\\c{c}ois Hamon, Nicholas A. Azzolina, Matthew Burton-Kelly, and Joseph P.\n  Morris", "title": "A Deep Learning-Accelerated Data Assimilation and Forecasting Workflow\n  for Commercial-Scale Geologic Carbon Storage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast assimilation of monitoring data to update forecasts of pressure buildup\nand carbon dioxide (CO2) plume migration under geologic uncertainties is a\nchallenging problem in geologic carbon storage. The high computational cost of\ndata assimilation with a high-dimensional parameter space impedes fast\ndecision-making for commercial-scale reservoir management. We propose to\nleverage physical understandings of porous medium flow behavior with deep\nlearning techniques to develop a fast history matching-reservoir response\nforecasting workflow. Applying an Ensemble Smoother Multiple Data Assimilation\nframework, the workflow updates geologic properties and predicts reservoir\nperformance with quantified uncertainty from pressure history and CO2 plumes\ninterpreted through seismic inversion. As the most computationally expensive\ncomponent in such a workflow is reservoir simulation, we developed surrogate\nmodels to predict dynamic pressure and CO2 plume extents under multi-well\ninjection. The surrogate models employ deep convolutional neural networks,\nspecifically, a wide residual network and a residual U-Net. The workflow is\nvalidated against a flat three-dimensional reservoir model representative of a\nclastic shelf depositional environment. Intelligent treatments are applied to\nbridge between quantities in a true-3D reservoir model and those in a\nsingle-layer reservoir model. The workflow can complete history matching and\nreservoir forecasting with uncertainty quantification in less than one hour on\na mainstream personal workstation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 16:38:29 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Tang", "Hewei", ""], ["Fu", "Pengcheng", ""], ["Sherman", "Christopher S.", ""], ["Zhang", "Jize", ""], ["Ju", "Xin", ""], ["Hamon", "Fran\u00e7ois", ""], ["Azzolina", "Nicholas A.", ""], ["Burton-Kelly", "Matthew", ""], ["Morris", "Joseph P.", ""]]}, {"id": "2105.09474", "submitter": "Stanley Lazic", "authors": "Stanley E. Lazic, Dominic P. Williams", "title": "Quantifying sources of uncertainty in drug discovery predictions with\n  probabilistic models", "comments": "34 pages, 9 figures", "journal-ref": "Artificial Intelligence in the Life Sciences (2021)", "doi": "10.1016/j.ailsci.2021.100004", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Knowing the uncertainty in a prediction is critical when making expensive\ninvestment decisions and when patient safety is paramount, but machine learning\n(ML) models in drug discovery typically provide only a single best estimate and\nignore all sources of uncertainty. Predictions from these models may therefore\nbe over-confident, which can put patients at risk and waste resources when\ncompounds that are destined to fail are further developed. Probabilistic\npredictive models (PPMs) can incorporate uncertainty in both the data and\nmodel, and return a distribution of predicted values that represents the\nuncertainty in the prediction. PPMs not only let users know when predictions\nare uncertain, but the intuitive output from these models makes communicating\nrisk easier and decision making better. Many popular machine learning methods\nhave a PPM or Bayesian analogue, making PPMs easy to fit into current\nworkflows. We use toxicity prediction as a running example, but the same\nprinciples apply for all prediction models used in drug discovery. The\nconsequences of ignoring uncertainty and how PPMs account for uncertainty are\nalso described. We aim to make the discussion accessible to a broad\nnon-mathematical audience. Equations are provided to make ideas concrete for\nmathematical readers (but can be skipped without loss of understanding) and\ncode is available for computational researchers\n(https://github.com/stanlazic/ML_uncertainty_quantification).\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:54:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lazic", "Stanley E.", ""], ["Williams", "Dominic P.", ""]]}, {"id": "2105.09512", "submitter": "Americo Cunha Jr", "authors": "A. Cunha Jr, R. Nasser, R. Sampaio, H. Lopes, and K. Breitman", "title": "Uncertainty quantification through Monte Carlo method in a cloud\n  computing setting", "comments": null, "journal-ref": "Computer Physics Communications, vol. 185, pp. 1355-1363, 2014", "doi": "10.1016/j.cpc.2014.01.006", "report-no": null, "categories": "stat.CO cs.MS math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Monte Carlo (MC) method is the most common technique used for uncertainty\nquantification, due to its simplicity and good statistical results. However,\nits computational cost is extremely high, and, in many cases, prohibitive.\nFortunately, the MC algorithm is easily parallelizable, which allows its use in\nsimulations where the computation of a single realization is very costly. This\nwork presents a methodology for the parallelization of the MC method, in the\ncontext of cloud computing. This strategy is based on the MapReduce paradigm,\nand allows an efficient distribution of tasks in the cloud. This methodology is\nillustrated on a problem of structural dynamics that is subject to\nuncertainties. The results show that the technique is capable of producing good\nresults concerning statistical moments of low order. It is shown that even a\nsimple problem may require many realizations for convergence of histograms,\nwhich makes the cloud computing strategy very attractive (due to its high\nscalability capacity and low-cost). Additionally, the results regarding the\ntime of processing and storage space usage allow one to qualify this new\nmethodology as a solution for simulations that require a number of MC\nrealizations beyond the standard.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 04:52:40 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Cunha", "A.", "Jr"], ["Nasser", "R.", ""], ["Sampaio", "R.", ""], ["Lopes", "H.", ""], ["Breitman", "K.", ""]]}, {"id": "2105.09618", "submitter": "Noa Malem-Shinitski", "authors": "Noa Malem-Shinitski, Cesar Ojeda and Manfred Opper", "title": "Nonlinear Hawkes Process with Gaussian Process Self Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditionally, Hawkes processes are used to model time--continuous point\nprocesses with history dependence. Here we propose an extended model where the\nself--effects are of both excitatory and inhibitory type and follow a Gaussian\nProcess. Whereas previous work either relies on a less flexible\nparameterization of the model, or requires a large amount of data, our\nformulation allows for both a flexible model and learning when data are scarce.\nWe continue the line of work of Bayesian inference for Hawkes processes, and\nour approach dispenses with the necessity of estimating a branching structure\nfor the posterior, as we perform inference on an aggregated sum of Gaussian\nProcesses. Efficient approximate Bayesian inference is achieved via data\naugmentation, and we describe a mean--field variational inference approach to\nlearn the model parameters. To demonstrate the flexibility of the model we\napply our methodology on data from three different domains and compare it to\npreviously reported results.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 09:20:35 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Malem-Shinitski", "Noa", ""], ["Ojeda", "Cesar", ""], ["Opper", "Manfred", ""]]}, {"id": "2105.09707", "submitter": "Beomjo Park", "authors": "Beomjo Park, Mikael Kuusela, Donata Giglio and Alison Gray", "title": "Spatio-temporal Local Interpolation of Global Ocean Heat Transport using\n  Argo Floats: A Debiased Latent Gaussian Process Approach", "comments": "27 pages, 10 figures with supplementary material 6 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world ocean plays a key role in redistributing heat in the climate system\nand hence in regulating Earth's climate. Yet statistical analysis of ocean heat\ntransport suffers from partially incomplete large-scale data intertwined with\ncomplex spatio-temporal dynamics, as well as from potential model\nmisspecification. We present a comprehensive spatio-temporal statistical\nframework tailored to interpolating the global ocean heat transport using\nin-situ Argo profiling float measurements. We formalize the statistical\nchallenges using latent local Gaussian process regression accompanied by a\ntwo-stage fitting procedure. We introduce an approximate\nExpectation-Maximization algorithm to jointly estimate both the mean field and\nthe covariance parameters, and refine the potentially under-specified mean\nfield model with a debiasing procedure. This approach provides data-driven\nglobal ocean heat transport fields that vary in both space and time and can\nprovide insights into crucial dynamical phenomena, such as El Ni{\\~n}o \\& La\nNi{\\~n}a, as well as the global climatological mean heat transport field, which\nby itself is of scientific interest. The proposed framework and the Argo-based\nestimates are thoroughly validated with state-of-the-art multimission satellite\nproducts and shown to yield realistic subsurface ocean heat transport\nestimates.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:57:12 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Park", "Beomjo", ""], ["Kuusela", "Mikael", ""], ["Giglio", "Donata", ""], ["Gray", "Alison", ""]]}, {"id": "2105.09776", "submitter": "Massimo Bonavita", "authors": "Massimo Bonavita", "title": "Exploring the structure of time-correlated model errors in the ECMWF\n  Data Assimilation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model errors are increasingly seen as a fundamental performance limiter in\nboth Numerical Weather Prediction and Climate Prediction simulations run with\nstate of the art Earth system digital twins.This has motivated recent efforts\naimed at estimating and correcting the systematic, predictable components of\nmodel error in a consistent data assimilation framework. While encouraging\nresults have been obtained with a careful examination of the spatial aspects of\nthe model error estimates, less attention has been devoted to the time\ncorrelation aspects of model errors and their impact on the assimilation cycle.\nIn this work we employ a Lagged Analysis Increment Covariance (LAIG) diagnostic\nto gain insight in the temporal evolution of systematic model errors in the\nECMWF operational data assimilation system, evaluate the effectiveness of the\ncurrent weak constraint 4DVar algorithm in reducing these types of errors and,\nbased on these findings,start exploring new ideas for the development of model\nerror estimation and correction strategies in data assimilation.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:24:53 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Bonavita", "Massimo", ""]]}, {"id": "2105.09812", "submitter": "Javad Kazemitabar", "authors": "Javad Kazemitabar", "title": "Double-Crossing Benford's Law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benford's law is widely used for fraud-detection nowadays. The underlying\nassumption for using the law is that a \"regular\" dataset follows the\nsignificant digit phenomenon. In this paper, we address the scenario where a\nshrewd fraudster manipulates a list of numbers in such a way that still\ncomplies with Benford's law. We develop a general family of distributions that\nprovides several degrees of freedom to such a fraudster such as minimum,\nmaximum, mean and size of the manipulated dataset. The conclusion further\ncorroborates the idea that Benford's law should be used with utmost discretion\nas a means for fraud detection.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 15:06:49 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Kazemitabar", "Javad", ""]]}, {"id": "2105.09881", "submitter": "Quang Nguyen", "authors": "Quang Nguyen", "title": "Poisson Modeling and Predicting English Premier League Goal Scoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The English Premier League is well-known for being not only one of the most\npopular professional sports leagues in the world, but also one of the toughest\ncompetitions to predict. The first purpose of this research was to verify the\nconsistency between goal scoring in the English Premier League and the Poisson\nprocess; specifically, the relationships between the number of goals scored in\na match and the Poisson distribution, the time between goals throughout the\ncourse of a season and the exponential distribution, and the time location of\ngoals during football games and the continuous uniform distribution. We found\nthat the Poisson process and the three probability distributions accurately\ndescribe Premier League goal scoring. In addition, Poisson regression was\nutilized to predict outcomes for a Premier League season, using different sets\nof season data and with a large number of simulations being involved. We\nexamined and compared various soccer metrics from our simulation results,\nincluding an English club's chances of being the champions, finishing in the\ntop four and bottom three, and relegation points.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:28:20 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nguyen", "Quang", ""]]}, {"id": "2105.09893", "submitter": "Mahsa Nadifar", "authors": "Mahsa Nadifar (1), Hossein Baghishani (1), Afshin Fallah (2) ((1)\n  Shahrood University of Technology, (2) IKIU)", "title": "A flexible Bayesian non-confounding spatial model for analysis of\n  dispersed count data in clinical studies", "comments": "arXiv admin note: text overlap with arXiv:1908.02344", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In employing spatial regression models for counts, we usually meet two\nissues. First, ignoring the inherent collinearity between covariates and the\nspatial effect would lead to causal inferences. Second, real count data usually\nreveal over or under-dispersion where the classical Poisson model is not\nappropriate to use. We propose a flexible Bayesian hierarchical modeling\napproach by joining non-confounding spatial methodology and a newly\nreconsidered dispersed count modeling from the renewal theory to control the\nissues. Specifically, we extend the methodology for analyzing spatial count\ndata based on the gamma distribution assumption for waiting times. The model\ncan be formulated as a latent Gaussian model, and consequently, we can carry\nout the fast computation using the integrated nested Laplace approximation\nmethod. We also examine different popular approaches for handling spatial\nconfounding and compare their performances in the presence of dispersion. We\nuse the proposed methodology to analyze a clinical dataset related to stomach\ncancer incidence in Slovenia and perform a simulation study to understand the\nproposed approach's merits better.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:55:28 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nadifar", "Mahsa", ""], ["Baghishani", "Hossein", ""], ["Fallah", "Afshin", ""]]}, {"id": "2105.10050", "submitter": "Feliu Serra-Burriel", "authors": "Feliu Serra-Burriel and Pedro Delicado and Fernando M. Cucchietti", "title": "Wildfires vegetation recovery through satellite remote sensing and\n  Functional Data Analysis", "comments": "35 Pages, 8 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years wildfires have caused havoc across the world, especially\naggravated in certain regions, due to climate change. Remote sensing has become\na powerful tool for monitoring fires, as well as for measuring their effects on\nvegetation over the following years. We aim to explain the dynamics of\nwildfires' effects on a vegetation index (previously estimated by causal\ninference through synthetic controls) from pre-wildfire available information\n(mainly proceeding from satellites). For this purpose, we use regression models\nfrom Functional Data Analysis, where wildfire effects are considered functional\nresponses, depending on elapsed time after each wildfire, while pre-wildfire\ninformation acts as scalar covariates. Our main findings show that vegetation\nrecovery after wildfires is a slow process, affected by many pre-wildfire\nconditions, among which the richness and diversity of vegetation is one of the\nbest predictors for the recovery.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 21:56:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Serra-Burriel", "Feliu", ""], ["Delicado", "Pedro", ""], ["Cucchietti", "Fernando M.", ""]]}, {"id": "2105.10084", "submitter": "Americo Cunha Jr", "authors": "Americo Cunha Jr, Rubens Sampaio", "title": "On the nonlinear stochastic dynamics of a continuous system with\n  discrete attached elements", "comments": null, "journal-ref": "Applied Mathematical Modelling, vol. 39, pp. 809-819, 2015", "doi": "10.1016/j.apm.2014.07.012", "report-no": null, "categories": "cond-mat.stat-mech cs.CE math.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical study on the influence of a discrete\nelement in the nonlinear dynamics of a continuous mechanical system subject to\nrandomness in the model parameters. This system is composed by an elastic bar,\nattached to springs and a lumped mass, with a random elastic modulus and\nsubjected to a Gaussian white-noise distributed external force. One can note\nthat the dynamic behavior of the bar is significantly altered when the lumped\nmass is varied, becoming, on the right extreme and for large values of the\nconcentrated mass, similar to a mass-spring system. It is also observed that\nthe system response is more influenced by the randomness for small values of\nthe lumped mass. The study conducted also show an irregular distribution of\nenergy through the spectrum of frequencies, asymmetries and multimodal behavior\nin the probability distributions of the lumped mass velocity.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 01:26:57 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cunha", "Americo", "Jr"], ["Sampaio", "Rubens", ""]]}, {"id": "2105.10168", "submitter": "Itziar Fern\\'andez", "authors": "I. Fern\\'andez, A. Rodr\\'iguez-Collado, Y. Larriba, A. Lamela, C.\n  Canedo, C. Rueda", "title": "FMM: An R Package for Modeling Rhythmic Patterns in Oscillatory Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper is dedicated to the R package FMM which implements a novel\napproach to describe rhythmic patterns in oscillatory signals. The frequency\nmodulated M\\\"obius (FMM) model is defined as a parametric signal plus a\ngaussian noise, where the signal can be described as a single or a sum of\nwaves. The FMM approach is flexible enough to describe a great variety of\nrhythmic patterns. The FMM package includes all required functions to fit and\nexplore single and multi-wave FMM models, as well as a restricted version that\nallows equality constraints between parameters representing a priori knowledge\nabout the shape to be included. Moreover, the FMM package can generate\nsynthetic data and visualize the results of the fitting process. The potential\nof this methodology is illustrated with examples of such biological\noscillations as the circadian rhythm in gene expression, the electrical\nactivity of the heartbeat and neuronal activity.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 07:28:11 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Fern\u00e1ndez", "I.", ""], ["Rodr\u00edguez-Collado", "A.", ""], ["Larriba", "Y.", ""], ["Lamela", "A.", ""], ["Canedo", "C.", ""], ["Rueda", "C.", ""]]}, {"id": "2105.10210", "submitter": "Anirban Mondal", "authors": "Kai Yin and Anirban Mondal", "title": "Bayesian Uncertainty Quantification of Local Volatility Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.NA math.NA stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local volatility is an important quantity in option pricing, portfolio\nhedging, and risk management. It is not directly observable from the market;\nhence calibrations of local volatility models are necessary using observable\nmarket data. Unlike most existing point-estimate methods, we cast the\nlarge-scale nonlinear inverse problem into the Bayesian framework, yielding a\nposterior distribution of the local volatility, which naturally quantifies its\nuncertainty. This extra uncertainty information enables traders and risk\nmanagers to make better decisions. To alleviate the computational cost, we\napply Karhunen--L\\`oeve expansion to reduce the dimensionality of the Gaussian\nProcess prior for local volatility. A modified two-stage adaptive Metropolis\nalgorithm is applied to sample the posterior probability distribution, which\nfurther reduces computational burdens caused by repetitive numerical forward\noption pricing model solver and time of heuristic tuning. We demonstrate our\nmethodology with both synthetic and market data.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 09:00:03 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Yin", "Kai", ""], ["Mondal", "Anirban", ""]]}, {"id": "2105.10265", "submitter": "Ahmed Awwad", "authors": "Ahmed Awwad", "title": "The impact of Over The Top service providers on the Global Mobile\n  Telecom Industry: A quantified analysis and recommendations for recovery", "comments": "28 Pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Telecom industry is significantly evolving all over the globe than ever.\nMobile users number is increasing remarkably. Telecom operators are investing\nto get more users connected and to improve user experience, however, they are\nfacing various challenges. Decrease of main revenue streams of voice calls, SMS\n(Short Message Service) and LDC (Long distance calls) with a significant\nincrease in data traffic. In contrary, with free cost, OTT (Over the top)\nproviders such as WhatsApp and Facebook communication services rendered over\nnetworks that built and owned by MNOs. Recently, OTT services gradually\nsubstituting the traditional MNOs` services and became ubiquitous with the help\nof the underlying data services provided by MNOs. The OTTs` services massive\npenetration into telecom industry is driving the MNOs to reconsider their\nstrategies and revenue sources.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 10:34:26 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Awwad", "Ahmed", ""]]}, {"id": "2105.10360", "submitter": "Doudou Zhou", "authors": "Doudou Zhou, and Tianxi Cai, and Junwei Lu", "title": "BELT: Block-wise Missing Embedding Learning Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has attracted attention in many fields, including\nstatistics, applied mathematics, and electrical engineering. Most of the works\nfocus on the independent sampling models under which the observed entries are\nsampled independently. Motivated by applications in the integration of multiple\nElectronic Health Record (EHR) datasets, we propose the method {\\bf B}lock-wise\nmissing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT) to treat\nrow-wise/column-wise missingness. Specifically, BELT can recover block-wise\nmissing matrices efficiently when every pair of matrices has an overlap. Our\nidea is to exploit the orthogonal Procrustes problem to align the eigenspace of\nthe two sub-matrices using their overlap, then complete the missing blocks by\nthe inner product of the two low-rank components. Besides, we prove the\nstatistical rate for the eigenspace of the underlying matrix, which is\ncomparable to the rate under the independently missing assumption. Simulation\nstudies show that the method performs well under a variety of configurations.\nIn the real data analysis, the method is applied to two tasks: (i) the\nintegrating of several point-wise mutual information matrices built by English\nEHR and Chinese medical text data, and (ii) the machine translation between\nEnglish and Chinese medical concepts. Our method shows an advantage over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:55:30 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:20:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Doudou", ""], ["Cai", "Tianxi", ""], ["Lu", "Junwei", ""]]}, {"id": "2105.10565", "submitter": "Adam Peterson", "authors": "Adam Peterson, Jana Hirsch, Brisa Sanchez", "title": "Spatial Temporal Aggregated Predictors to Examine Built Environment\n  Health Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose the spatial-temporal aggregated predictor (STAP) modeling\nframework to address measurement and estimation issues that arise when\nassessing the relationship between built environment features (BEF) and health\noutcomes. Many BEFs can be mapped as point locations and thus traditional\nexposure metrics are based on the number of features within a pre-specified\nspatial unit. The size of the spatial unit--or spatial scale--that is most\nappropriate for a particular health outcome is unknown and its choice\ninextricably impacts the estimated health effect. A related issue is the lack\nof knowledge of the temporal scale--or the length of exposure time that is\nnecessary for the BEF to render its full effect on the health outcome. The\nproposed STAP model enables investigators to estimate both the spatial and\ntemporal scales for a given BEF in a data-driven fashion, thereby providing a\nflexible solution for measuring the relationship between outcomes and spatial\nproximity to point-referenced exposures. Simulation studies verify the validity\nof our method for estimating the scales as well as the association between\navailability of BEFs' and health outcomes. We apply this method to estimate the\nspatial-temporal association between supermarkets and BMI using data from the\nMulti-Ethnic Atherosclerosis Study, demonstrating the method's applicability in\ncohort studies.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:45:31 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Peterson", "Adam", ""], ["Hirsch", "Jana", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2105.10624", "submitter": "Richard Berk", "authors": "Richard A. Berk", "title": "Post-Model-Selection Statistical Inference with Interrupted Time Series\n  Designs: An Evaluation of an Assault Weapons Ban in California", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been many claims in the media and a bit of respectable research\nabout the causes of variation in firearm sales. The challenges for causal\ninference can be quite daunting. This paper reports an analysis of daily\nhandgun sales in California from 1996 through 2018 using an interrupted time\nseries design and analysis. The design was introduced to social scientists in\n1963 by Campbell and Stanley, analysis methods were proposed by Box and Tiao in\n1975, and more recent treatments are easily found (Box et al., 2016). But this\napproach to causal inference can be badly overmatched by the data on handgun\nsales, especially when the causal effects are estimated. More important for\nthis paper are fundamental oversights in the standard statistical methods\nemployed. Test multiplicity problems are introduced by adaptive model selection\nbuilt into recommended practice. The challenges are computational and\nconceptual. Some progress is made on both problems that arguably improves on\npast research, but the take-home message may be to reduce aspirations about\nwhat can be learned.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:45:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Berk", "Richard A.", ""]]}, {"id": "2105.10890", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein and Jorge Mateu", "title": "Bayesian Effect Selection for Additive Quantile Regression with an\n  Analysis to Air Pollution Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques used in air pollution modelling usually lack the\npossibility to understand which predictors affect air pollution in which\nfunctional form; and are not able to regress on exceedances over certain\nthresholds imposed by authorities directly. The latter naturally induce\nconditional quantiles and reflect the seriousness of particular events. In the\npresent paper we focus on this important aspect by developing quantile\nregression models further. We propose a general Bayesian effect selection\napproach for additive quantile regression within a highly interpretable\nframework. We place separate normal beta prime spike and slab priors on the\nscalar importance parameters of effect parts and implement a fast Gibbs\nsampling scheme. Specifically, it enables to study quantile-specific covariate\neffects, allows these covariates to be of general functional form using\nadditive predictors, and facilitates the analysts' decision whether an effect\nshould be included linearly, non-linearly or not at all in the quantiles of\ninterest. In a detailed analysis on air pollution data in Madrid (Spain) we\nfind the added value of modelling extreme nitrogen dioxide (NO2) concentrations\nand how thresholds are driven differently by several climatological variables\nand traffic as a spatial proxy. Our results underpin the need of enhanced\nstatistical models to support short-term decisions and enable local authorities\nto mitigate or even prevent exceedances of NO2 concentration limits.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 09:02:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Klein", "Nadja", ""], ["Mateu", "Jorge", ""]]}, {"id": "2105.10965", "submitter": "Demian Pouzo", "authors": "Marina Dias and Demian Pouzo", "title": "Inference for multi-valued heterogeneous treatment effects when the\n  number of treated units is small", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN math.ST q-fin.EC stat.AP stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method for conducting asymptotically valid inference for\ntreatment effects in a multi-valued treatment framework where the number of\nunits in the treatment arms can be small and do not grow with the sample size.\nWe accomplish this by casting the model as a semi-/non-parametric conditional\nquantile model and using known finite sample results about the law of the\nindicator function that defines the conditional quantile. Our framework allows\nfor structural functions that are non-additively separable, with flexible\nfunctional forms and heteroskedasticy in the residuals, and it also encompasses\ncommonly used designs like difference in difference. We study the finite sample\nbehavior of our test in a Monte Carlo study and we also apply our results to\nassessing the effect of weather events on GDP growth.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:06:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dias", "Marina", ""], ["Pouzo", "Demian", ""]]}, {"id": "2105.11182", "submitter": "Hoang Nguyen", "authors": "Sune Karlsson, Stepan Mazur and Hoang Nguyen", "title": "Vector autoregression models with skewness and heavy tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With uncertain changes of the economic environment, macroeconomic downturns\nduring recessions and crises can hardly be explained by a Gaussian structural\nshock. There is evidence that the distribution of macroeconomic variables is\nskewed and heavy tailed. In this paper, we contribute to the literature by\nextending a vector autoregression (VAR) model to account for a more realistic\nassumption of the multivariate distribution of the macroeconomic variables. We\npropose a general class of generalized hyperbolic skew Student's t distribution\nwith stochastic volatility for the error term in the VAR model that allows us\nto take into account skewness and heavy tails. Tools for Bayesian inference and\nmodel selection using a Gibbs sampler are provided. In an empirical study, we\npresent evidence of skewness and heavy tails for monthly macroeconomic\nvariables. The analysis also gives a clear message that skewness should be\ntaken into account for better predictions during recessions and crises.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:15:04 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Karlsson", "Sune", ""], ["Mazur", "Stepan", ""], ["Nguyen", "Hoang", ""]]}, {"id": "2105.11353", "submitter": "Saumya Sakitha Ariyarathne", "authors": "Sakitha Ariyarathne, Harsha Gangammanavar, Raanju R. Sundararajan", "title": "Change Point Detection in Nonstationary Sub-Hourly Wind Time Series", "comments": "18 pages, 3 figures, 3 tables, and 5 sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a change point detection method for detecting\nchange points in multivariate nonstationary wind speed time series. The change\npoint method identifies changes in the covariance structure and decomposes the\nnonstationary multivariate time series into stationary segments. We also\npresent parametric and nonparametric simulation techniques to simulate new wind\ntime series within each stationary segment. The proposed simulation methods\nretain statistical properties of the original time series and therefore, can be\nemployed for simulation-based analysis of power systems planning and operations\nproblems. We demonstrate the capabilities of the change point detection method\nthrough computational experiments conducted on wind speed time series at\nfive-minute resolution. We also conduct experiments on the economic dispatch\nproblem to illustrate the impact of nonstationarity in wind generation on\nconventional generation and location marginal prices.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:37:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ariyarathne", "Sakitha", ""], ["Gangammanavar", "Harsha", ""], ["Sundararajan", "Raanju R.", ""]]}, {"id": "2105.11405", "submitter": "Ar\\'anzazu de Juan Fern\\'andez", "authors": "C. Seri and A. de Juan Fernandez", "title": "The relationship between economic growth and environment. Testing the\n  EKC hypothesis for Latin American countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We employ an ARDL bounds testing approach to cointegration and Unrestricted\nError Correction Models (UECMs) to estimate the relationship between income and\nCO2 emissions per capita in 21 Latin American Countries (LACs) over 1960-2017.\nUsing time series we estimate six different specifications of the model to take\ninto account the independent effect on CO2 emissions per capita of different\nfactors considered as drivers of different dynamics of CO2 emissions along the\ndevelopment path. This approach allows to address two concerns. First, the\nestimation of the model controlling for different variables serves to assess if\nthe EKC hypothesis is supported by evidence in any of the LACs considered and\nto evaluate if this evidence is robust to different model specifications.\nSecond, the inclusion of control variables accounting for the effect on CO2\nemissions is directed at increasing our understanding of CO2 emissions drivers\nin different countries. The EKC hypothesis effectively describes the long term\nincome-emissions relationship only in a minority of LACs and, in many cases,\nthe effect on CO2 emissions of different factors depends on the individual\ncountry experience and on the type and quantity of environmental policies\nadopted. Overall, these results call for increased environmental action in the\nregion.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:26:57 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Seri", "C.", ""], ["Fernandez", "A. de Juan", ""]]}, {"id": "2105.11409", "submitter": "Cristobal Gallego-Castillo", "authors": "Cristobal Gallego-Castillo, Alvaro Cuerva-Tejero, Mohanad Elagamy,\n  Oscar Lopez-Garcia, Sergio Avila-Sanchez", "title": "A tutorial on reproducing a predefined autocovariance function through\n  AR models: Application to stationary homogeneous isotropic turbulence", "comments": "28 pages, 19 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Sequential methods for synthetic realisation of random processes have a\nnumber of advantages compared with spectral methods. In this article, the\ndetermination of optimal autoregressive (AR) models for reproducing a\npredefined target autocovariance function of a random process is addressed. To\nthis end, a novel formulation of the problem is developed. This formulation is\nlinear and generalises the well-known Yule-Walker (YW) equations and a recent\napproach based on restricted AR models (Krenk approach). Two main features\ncharacterise the introduced formulation: (i) flexibility in the choice for the\nautocovariance equations employed in the model determination, and (ii)\nflexibility in the definition of the AR model scheme. Both features were\nexploited by a genetic algorithm to obtain optimal AR models for the particular\ncase of synthetic generation of homogeneous stationary isotropic turbulence\ntime series. The obtained models improved those obtained with the YW and Krenk\napproaches for the same model parsimony in terms of the global fitting of the\ntarget autocovariance function. Implications for the reproduced spectra are\nalso discussed. The formulation for the multivariate case is presented,\nhighlighting the causes behind some computational bottlenecks.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:59:13 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 10:36:14 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gallego-Castillo", "Cristobal", ""], ["Cuerva-Tejero", "Alvaro", ""], ["Elagamy", "Mohanad", ""], ["Lopez-Garcia", "Oscar", ""], ["Avila-Sanchez", "Sergio", ""]]}, {"id": "2105.11418", "submitter": "Ruijiang Gao", "authors": "Ruijiang Gao, Maytal Saar-tsechansky", "title": "Cost-Accuracy Aware Adaptive Labeling for Active Learning", "comments": "Accepted at AAAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional active learning algorithms assume a single labeler that produces\nnoiseless label at a given, fixed cost, and aim to achieve the best\ngeneralization performance for given classifier under a budget constraint.\nHowever, in many real settings, different labelers have different labeling\ncosts and can yield different labeling accuracies. Moreover, a given labeler\nmay exhibit different labeling accuracies for different instances. This setting\ncan be referred to as active learning with diverse labelers with varying costs\nand accuracies, and it arises in many important real settings. It is therefore\nbeneficial to understand how to effectively trade-off between labeling accuracy\nfor different instances, labeling costs, as well as the informativeness of\ntraining instances, so as to achieve the best generalization performance at the\nlowest labeling cost. In this paper, we propose a new algorithm for selecting\ninstances, labelers (and their corresponding costs and labeling accuracies),\nthat employs generalization bound of learning with label noise to select\ninformative instances and labelers so as to achieve higher generalization\naccuracy at a lower cost. Our proposed algorithm demonstrates state-of-the-art\nperformance on five UCI and a real crowdsourcing dataset.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 17:21:00 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gao", "Ruijiang", ""], ["Saar-tsechansky", "Maytal", ""]]}, {"id": "2105.11490", "submitter": "Sofia Ruiz Suarez", "authors": "Sofia Ruiz-Suarez, Vianey Leos-Barajas, Juan Manuel Morales", "title": "Hidden Markov and semi-Markov models: When and why are these models\n  useful to classify states in time series data?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) and their extensions have proven to be powerful\ntools for classification of observations that stem from systems with temporal\ndependence as they take into account that observations close in time to one\nanother are likely generated from the same state (i.e. class). In this paper,\nwe provide details for the implementation of four models for classification in\na supervised learning context: HMMs, hidden semi-Markov models (HSMMs),\nautoregressive-HMMs and autoregressive-HSMMs. Using simulations, we study the\nclassification performance under various degrees of model misspecification to\ncharacterize when it would be important to extend a basic HMM to an HSMM. As an\napplication of these techniques we use the models to classify accelerometer\ndata from Merino sheep to distinguish between four different behaviors of\ninterest. In particular in the field of movement ecology, collection of\nfine-scale animal movement data over time to identify behavioral states has\nbecome ubiquitous, necessitating models that can account for the dependence\nstructure in the data. We demonstrate that when the aim is to conduct\nclassification, various degrees of model misspecification of the proposed model\nmay not impede good classification performance unless there is high overlap\nbetween the state-dependent distributions.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 18:31:11 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Ruiz-Suarez", "Sofia", ""], ["Leos-Barajas", "Vianey", ""], ["Morales", "Juan Manuel", ""]]}, {"id": "2105.11512", "submitter": "David Barmherzig", "authors": "David A. Barmherzig and Ju Sun", "title": "Towards Low-Photon Nanoscale Imaging: Holographic Phase Retrieval via\n  Maximum Likelihood Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.IT math.IT physics.optics stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A new algorithmic framework is presented for holographic phase retrieval via\nmaximum likelihood optimization, which allows for practical and robust image\nreconstruction. This framework is especially well-suited for holographic\ncoherent diffraction imaging in the \\textit{low-photon regime}, where data is\nhighly corrupted by Poisson shot noise. Thus, this methodology provides a\nviable solution towards the advent of \\textit{low-photon nanoscale imaging},\nwhich is a fundamental challenge facing the current state of imaging\ntechnologies. Practical optimization algorithms are derived and implemented,\nand extensive numerical simulations demonstrate significantly improved image\nreconstruction versus the leading algorithms currently in use. Further\nexperiments compare the performance of popular holographic reference geometries\nto determine the optimal combined physical setup and algorithm pipeline for\npractical implementation. Additional features of these methods are also\ndemonstrated, which allow for fewer experimental constraints.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 19:44:43 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Barmherzig", "David A.", ""], ["Sun", "Ju", ""]]}, {"id": "2105.11547", "submitter": "Yuexuan Wu", "authors": "Yuexuan Wu, Suprateek Kundu, Jennifer S. Stevens, Negar Fani, Anuj\n  Srivastava", "title": "Elastic Shape Analysis of Brain Structures for Predictive Modeling of\n  PTSD", "comments": "33 pages; Supplementary Materials and interactive visualizations are\n  available in https://www.dropbox.com/home/Paper%20Interactive%20Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is increasing evidence on the importance of brain morphology in\npredicting and classifying mental disorders. However, the vast majority of\ncurrent shape approaches rely heavily on vertex-wise analysis that may not\nsuccessfully capture complexities of subcortical structures. Additionally, the\npast works do not include interactions between these structures and exposure\nfactors. Predictive modeling with such interactions is of paramount interest in\nheterogeneous mental disorders such as PTSD, where trauma exposure interacts\nwith brain shape changes to influence behavior. We propose a comprehensive\nframework that overcomes these limitations by representing brain substructures\nas continuous parameterized surfaces and quantifying their shape differences\nusing elastic shape metrics. Using the elastic shape metric, we compute shape\nsummaries of subcortical data and represent individual shapes by their\nprincipal scores. These representations allow visualization tools that help\nlocalize changes when these PCs are varied. Subsequently, these PCs, the\nauxiliary exposure variables, and their interactions are used for regression\nmodeling. We apply our method to data from the Grady Trauma Project, where the\ngoal is to predict clinical measures of PTSD using shapes of brain\nsubstructures. Our analysis revealed considerably greater predictive power\nunder the elastic shape analysis than widely used approaches such as\nvertex-wise shape analysis and even volumetric analysis. It helped identify\nlocal deformations in brain shapes related to change in PTSD severity. To our\nknowledge, this is one of the first brain shape analysis approaches that can\nseamlessly integrate the pre-processing steps under one umbrella for improved\naccuracy and are naturally able to account for interactions between brain shape\nand additional covariates to yield superior predictive performance when\nmodeling clinical outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 21:33:58 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Wu", "Yuexuan", ""], ["Kundu", "Suprateek", ""], ["Stevens", "Jennifer S.", ""], ["Fani", "Negar", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2105.11868", "submitter": "Ivan Iudice Ph.D.", "authors": "Donatella Darsena, Giacinto Gelli, Ivan Iudice, Francesco Verde", "title": "Detection and blind channel estimation for UAV-aided wireless sensor\n  networks in smart cities under mobile jamming attack", "comments": "16 pages, 7 figures, 4 tables, submitted to IEEE Internet of Things\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist several ways of integrating unmanned aerial vehicles (UAVs) into\nwireless sensor networks (WSNs) for smart city applications. Among the others,\na UAV can be employed as a relay in a \"store-carry and forward\" fashion by\nuploading data from ground sensors and meters and, then, downloading it to a\ncentral unit. However, both the uploading and downloading phases can be prone\nto potential threats and attacks. As a legacy from traditional wireless\nnetworks, the jamming attack is still one of the major and serious threats to\nUAV-aided communications, especially when the jammer is mobile, too, e.g., it\nis mounted on an UAV or inside a terrestrial vehicle. In this paper, we\ninvestigate anti-jamming communications in UAV-aided WSNs operating over\ndoubly-selective channels. In such a scenario, the signals transmitted by the\nlegitimate transmitters (sensors and meters in the uploading phase or the UAV\nin the downloading phase) and the malicious mobile jammer undergo both time\ndispersion due to multipath propagation effects and frequency dispersion caused\nby Doppler shifts. To suppress the jamming signal, we propose a blind\nphysical-layer technique that jointly exploits amplitudes, phases, time delays,\nand Doppler shifts differences between the two superimposed signals. Such\nparameters are estimated from data through the use of algorithms exploiting the\nalmost-cyclostationarity properties of the received signal. Simulation results\ncorroborate the antijamming capabilities of the proposed method, for different\nmobility scenario of the jammer.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:12:46 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Darsena", "Donatella", ""], ["Gelli", "Giacinto", ""], ["Iudice", "Ivan", ""], ["Verde", "Francesco", ""]]}, {"id": "2105.11886", "submitter": "Chen Xu", "authors": "Chen Xu, Yao Xie", "title": "Conformal Anomaly Detection on Spatio-Temporal Observations with Missing\n  Data", "comments": "Submitted to ICML 2021 Workshop--Distribution-free Uncertainty\n  Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a distribution-free, unsupervised anomaly detection method called\nECAD, which wraps around any regression algorithm and sequentially detects\nanomalies. Rooted in conformal prediction, ECAD does not require data\nexchangeability but approximately controls the Type-I error when data are\nnormal. Computationally, it involves no data-splitting and efficiently trains\nensemble predictors to increase statistical power. We demonstrate the superior\nperformance of ECAD on detecting anomalous spatio-temporal traffic flow.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:44:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 02:49:29 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Chen", ""], ["Xie", "Yao", ""]]}, {"id": "2105.11892", "submitter": "John R.J. Thompson", "authors": "John R.J. Thompson, Longlong Feng, R. Mark Reesor, Chuck Grace, Adam\n  Metzler", "title": "Measuring Financial Advice: aligning client elicited and revealed risk", "comments": "36 pages, 17 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Financial advisors use questionnaires and discussions with clients to\ndetermine a suitable portfolio of assets that will allow clients to reach their\ninvestment objectives. Financial institutions assign risk ratings to each\nsecurity they offer, and those ratings are used to guide clients and advisors\nto choose an investment portfolio risk that suits their stated risk tolerance.\nThis paper compares client Know Your Client (KYC) profile risk allocations to\ntheir investment portfolio risk selections using a value-at-risk discrepancy\nmethodology. Value-at-risk is used to measure elicited and revealed risk to\nshow whether clients are over-risked or under-risked, changes in KYC risk lead\nto changes in portfolio configuration, and cash flow affects a client's\nportfolio risk. We demonstrate the effectiveness of value-at-risk at measuring\nclients' elicited and revealed risk on a dataset provided by a private Canadian\nfinancial dealership of over $50,000$ accounts for over $27,000$ clients and\n$300$ advisors. By measuring both elicited and revealed risk using the same\nmeasure, we can determine how well a client's portfolio aligns with their\nstated goals. We believe that using value-at-risk to measure client risk\nprovides valuable insight to advisors to ensure that their practice is KYC\ncompliant, to better tailor their client portfolios to stated goals,\ncommunicate advice to clients to either align their portfolios to stated goals\nor refresh their goals, and to monitor changes to the clients' risk positions\nacross their practice.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:55:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Thompson", "John R. J.", ""], ["Feng", "Longlong", ""], ["Reesor", "R. Mark", ""], ["Grace", "Chuck", ""], ["Metzler", "Adam", ""]]}, {"id": "2105.11961", "submitter": "Lucas B\\\"ottcher", "authors": "Lucas B\\\"ottcher and Maria R. D'Orsogna and Tom Chou", "title": "A statistical model of COVID-19 testing in populations: effects of\n  sampling bias and testing errors", "comments": "13 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a statistical model for the testing of disease prevalence in a\npopulation. The model assumes a binary test result, positive or negative, but\nallows for biases in sample selection and both type I (false positive) and type\nII (false negative) testing errors. Our model also incorporates multiple test\ntypes and is able to distinguish between retesting and exclusion after testing.\nOur quantitative framework allows us to directly interpret testing results as a\nfunction of errors and biases. By applying our testing model to COVID-19\ntesting data and actual case data from specific jurisdictions, we are able to\nestimate and provide uncertainty quantification of indices that are crucial in\na pandemic, such as disease prevalence and fatality ratios.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 10:21:26 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["B\u00f6ttcher", "Lucas", ""], ["D'Orsogna", "Maria R.", ""], ["Chou", "Tom", ""]]}, {"id": "2105.11982", "submitter": "Xinyue Xiong", "authors": "Dongxia Wu, Liyao Gao, Xinyue Xiong, Matteo Chinazzi, Alessandro\n  Vespignani, Yi-An Ma, Rose Yu", "title": "Quantifying Uncertainty in Deep Spatiotemporal Forecasting", "comments": "arXiv admin note: text overlap with arXiv:2102.06684", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning is gaining increasing popularity for spatiotemporal\nforecasting. However, prior works have mostly focused on point estimates\nwithout quantifying the uncertainty of the predictions. In high stakes domains,\nbeing able to generate probabilistic forecasts with confidence intervals is\ncritical to risk assessment and decision making. Hence, a systematic study of\nuncertainty quantification (UQ) methods for spatiotemporal forecasting is\nmissing in the community. In this paper, we describe two types of\nspatiotemporal forecasting problems: regular grid-based and graph-based. Then\nwe analyze UQ methods from both the Bayesian and the frequentist point of view,\ncasting in a unified framework via statistical decision theory. Through\nextensive experiments on real-world road network traffic, epidemics, and air\nquality forecasting tasks, we reveal the statistical and computational\ntrade-offs for different UQ methods: Bayesian methods are typically more robust\nin mean prediction, while confidence levels obtained from frequentist methods\nprovide more extensive coverage over data variations. Computationally, quantile\nregression type methods are cheaper for a single confidence interval but\nrequire re-training for different intervals. Sampling based methods generate\nsamples that can form multiple confidence intervals, albeit at a higher\ncomputational cost.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 14:35:46 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 12:59:06 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wu", "Dongxia", ""], ["Gao", "Liyao", ""], ["Xiong", "Xinyue", ""], ["Chinazzi", "Matteo", ""], ["Vespignani", "Alessandro", ""], ["Ma", "Yi-An", ""], ["Yu", "Rose", ""]]}, {"id": "2105.11998", "submitter": "Randall Christensen", "authors": "Randall Christensen, Greg Droge, Robert Leishman", "title": "A Closed-Loop Linear Covariance Framework for Vehicle Path Planning in\n  an Uncertain Obstacle Field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning in uncertain environments is a key enabler of true vehicle\nautonomy. Over the past two decades, numerous approaches have been developed to\naccount for errors in the vehicle path while navigating complex and often\nuncertain environments. An important capability of such planning is the\nprediction of vehicle dispersion covariances about candidate paths. This work\ndevelops a new closed-loop linear covariance (CL-LinCov) framework applicable\nto wide range of autonomous system architectures. Extensions to current\nCL-LinCov frameworks are made to accommodate 1) the cascaded architecture\ntypical of autonomous vehicles and 2) the dual-use of continuous sensor\ninformation for both navigation and control. The closed-loop nature of the\nframework preserves the important coupling between the system dynamics,\nexogenous disturbances, and the guidance, navigation, and control algorithms.\nThe developed framework is applied to a simplified model of an unmanned aerial\nvehicle and validated by comparison via Monte Carlo analysis. The utility of\nthe CL-LinCov information is illustrated by its application to path planning in\nan uncertain obstacle field via a modified version of the rapidly exploring\nrandom tree algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 15:02:07 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Christensen", "Randall", ""], ["Droge", "Greg", ""], ["Leishman", "Robert", ""]]}, {"id": "2105.12065", "submitter": "Francesco Serafini", "authors": "Francesco Serafini (1), Mark Naylor (1), Finn Lindgren (1), Maximilian\n  Werner (2), Ian Main (1) ((1) University of Edinburgh, (2) University of\n  Bristol)", "title": "Ranking earthquake forecasts using proper scoring rules: Binary events\n  in a low probability environment", "comments": "33 pages, 18 figures. Work presented at vEGU21 as vPico presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Operational earthquake forecasting for risk management and communication\nduring seismic sequences depends on our ability to select an optimal\nforecasting model. To do this, we need to compare the performance of competing\nmodels with each other in prospective forecasting mode, and to rank their\nperformance using a fair, reproducible and reliable method. The Collaboratory\nfor the Study of Earthquake Predictability (CSEP) conducts such prospective\nearthquake forecasting experiments around the globe. One metric that has been\nproposed to rank competing models is the Parimutuel Gambling score, which has\nthe advantage of allowing alarm-based (categorical) forecasts to be compared\nwith probabilistic ones. Here we examine the suitability of this score for\nranking competing earthquake forecasts. First, we prove analytically that this\nscore is in general improper, meaning that, on average, it does not prefer the\nmodel that generated the data. Even in the special case where it is proper, we\nshow it can still be used in an improper way. Then, we compare its performance\nwith two commonly-used proper scores (the Brier and logarithmic scores), taking\ninto account the uncertainty around the observed average score. We estimate the\nconfidence intervals for the expected score difference which allows us to\ndefine if and when a model can be preferred. We extend the analysis to show how\nmuch data are required, in principle, for a test to express a preference\ntowards a particular forecast. Such thresholds could be used in experimental\ndesign to specify the duration, time windows, and spatial discretisation of\nearthquake models and forecasts. Our findings suggest the Parimutuel Gambling\nscore should not be used to distinguishing between multiple competing\nforecasts. They also enable a more rigorous approach to distinguish between the\npredictive skills of candidate forecasts in addition to their rankings.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:47:59 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Serafini", "Francesco", ""], ["Naylor", "Mark", ""], ["Lindgren", "Finn", ""], ["Werner", "Maximilian", ""], ["Main", "Ian", ""]]}, {"id": "2105.12271", "submitter": "Yu Wang", "authors": "Yu Wang and Alfred Hero", "title": "SG-PALM: a Fast Physically Interpretable Tensor Graphical Model", "comments": "Accepted in ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new graphical model inference procedure, called SG-PALM, for\nlearning conditional dependency structure of high-dimensional tensor-variate\ndata. Unlike most other tensor graphical models the proposed model is\ninterpretable and computationally scalable to high dimension. Physical\ninterpretability follows from the Sylvester generative (SG) model on which\nSG-PALM is based: the model is exact for any observation process that is a\nsolution of a partial differential equation of Poisson type. Scalability\nfollows from the fast proximal alternating linearized minimization (PALM)\nprocedure that SG-PALM uses during training. We establish that SG-PALM\nconverges linearly (i.e., geometric convergence rate) to a global optimum of\nits objective function. We demonstrate the scalability and accuracy of SG-PALM\nfor an important but challenging climate prediction problem: spatio-temporal\nforecasting of solar flares from multimodal imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 00:24:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wang", "Yu", ""], ["Hero", "Alfred", ""]]}, {"id": "2105.12286", "submitter": "Amadou Barry", "authors": "Amadou Barry, Nikhil Bhagwat, Bratislav Misic, Jean-Baptiste Poline\n  and Celia M. T. Greenwood", "title": "An algorithm-based multiple detection influence measure for high\n  dimensional regression using expectile", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of influential observations is an important part of data\nanalysis that can prevent erroneous conclusions drawn from biased estimators.\nHowever, in high dimensional data, this identification is challenging.\nClassical and recently-developed methods often perform poorly when there are\nmultiple influential observations in the same dataset. In particular, current\nmethods can fail when there is masking several influential observations with\nsimilar characteristics, or swamping when the influential observations are near\nthe boundary of the space spanned by well-behaved observations. Therefore, we\npropose an algorithm-based, multi-step, multiple detection procedure to\nidentify influential observations that addresses current limitations. Our\nthree-step algorithm to identify and capture undesirable variability in the\ndata, $\\asymMIP,$ is based on two complementary statistics, inspired by\nasymmetric correlations, and built on expectiles. Simulations demonstrate\nhigher detection power than competing methods. Use of the resulting asymptotic\ndistribution leads to detection of influential observations without the need\nfor computationally demanding procedures such as the bootstrap. The application\nof our method to the Autism Brain Imaging Data Exchange neuroimaging dataset\nresulted in a more balanced and accurate prediction of brain maturity based on\ncortical thickness. See our GitHub for a free R package that implements our\nalgorithm: \\texttt{asymMIP} (\\url{github.com/AmBarry/hidetify}).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:16:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Barry", "Amadou", ""], ["Bhagwat", "Nikhil", ""], ["Misic", "Bratislav", ""], ["Poline", "Jean-Baptiste", ""], ["Greenwood", "Celia M. T.", ""]]}, {"id": "2105.12730", "submitter": "Aaron King", "authors": "Aaron A. King, Qianying Lin, Edward L. Ionides", "title": "Markov Genealogy Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.PE q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a family of genealogy-valued Markov processes that are induced\nby a continuous-time Markov population process. We derive exact expressions for\nthe likelihood of a given genealogy conditional on the history of the\nunderlying population process. These lead to a version of the nonlinear\nfiltering equation, which can be used to design efficient Monte Carlo inference\nalgorithms. Existing full-information approaches for phylodynamic inference are\nspecial cases of the theory.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:36:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["King", "Aaron A.", ""], ["Lin", "Qianying", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2105.12785", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis and Kirk Goldsberry", "title": "The Anatomy of Corner 3s in the NBA: What makes them efficient, how are\n  they generated and how can defenses respond?", "comments": null, "journal-ref": "Workshop on AI for Sports Analytics at IJCA (2021)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern basketball is all about creating efficient shots, that is, shots with\nhigh payoff. This is not necessarily equivalent to creating looks with the\nhighest probability of success. In particular, the two most efficient shots in\nthe NBA - which are shots from the paint, i.e., extremely close to the basket,\nand three-point shots from the corner, i.e., at least 22 feet apart - have\ncompletely different spatial profiles when it comes to their distance from the\nbasket. The latter also means that they are pretty much at the opposing ends of\nthe spectrum when it comes to their probability of being made. Due to their\nefficiency, these are the most sought after shots from the offense, while the\ndefense is trying to contain them. However, in order to contain them one needs\nto first understand what makes them efficient in the first place and how they\nare generated. In this study we focus on the corner three point shots and using\nplayer tracking data we show that the main factor for their efficiency -\ncontrary to the belief from the sports mass media - is not the shorter distance\nto the basket compared to three-point shots above the break, but rather the\nfact that they are assisted at a very high rate (more than 90\\%). Furthermore,\nwe analyze the movement of the shooter and his defender and find that more than\nhalf of these shots involve a shooter anchored at the corner waiting for the\nkick out pass. We finally define a simplified game between the offense and\ndefense in these situation and we find that the Nash Equilibrium supports\neither committing to the corner shooter or to the drive to the basket, and not\nlingering between the two, which is what we observed from the defenses in our\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 18:37:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Pelechrinis", "Konstantinos", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "2105.12798", "submitter": "Giovanni Sansavini", "authors": "Steffen O.P. Blume, Francesco Corman, Giovanni Sansavini", "title": "Bayesian Origin-Destination Estimation in Networked Transit Systems\n  using Nodal In- and Outflow Counts", "comments": "36 pages (including appendix), 21 figures (including appendix),\n  submitted to Transportation Research Part B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose a Bayesian inference approach for static Origin-Destination\n(OD)-estimation in large-scale networked transit systems. The approach finds\nposterior distribution estimates of the OD-coefficients, which describe the\nrelative proportions of passengers travelling between origin and destination\nlocations, via a Hamiltonian Monte Carlo sampling procedure. We suggest two\ndifferent inference model formulations: the instantaneous-balance and\naverage-delay model. We discuss both models' sensitivity to various count\nobservation properties, and establish that the average-delay model is generally\nmore robust in determining the coefficient posteriors. The\ninstantaneous-balance model, however, requires lower resolution count\nobservations and produces comparably accurate estimates as the average-delay\nmodel, pending that count observations are only moderately interfered by trend\nfluctuations or the truncation of the observation window, and sufficient number\nof dispersed data records are available. We demonstrate that the Bayesian\nposterior distribution estimates provide quantifiable measures of the\nestimation uncertainty and prediction quality of the model, whereas the point\nestimates obtained from an alternative constrained quadratic programming\noptimisation approach only provide the residual errors between the predictions\nand observations. Moreover, the Bayesian approach proves more robust in scaling\nto high-dimensional underdetermined problems. The Bayesian\ninstantaneous-balance OD-coefficient posteriors are determined for the New York\nCity (NYC) subway network, based on several years of entry and exit count\nobservations recorded at station turnstiles across the network. The\naverage-delay model proves intractable on the real-world test scenario, given\nits computational time complexity and the incompleteness as well as coarseness\nof the turnstile records.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 19:13:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Blume", "Steffen O. P.", ""], ["Corman", "Francesco", ""], ["Sansavini", "Giovanni", ""]]}, {"id": "2105.12852", "submitter": "Marco Berrettini", "authors": "Marco Berrettini, Giuliano Galimberti, Saverio Ranciati and Thomas\n  Brendan Murphy", "title": "Flexible Bayesian modelling of concomitant covariate effects in mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixture models provide a useful tool to account for unobserved heterogeneity\nand are at the basis of many model-based clustering methods. In order to gain\nadditional flexibility, some model parameters can be expressed as functions of\nconcomitant covariates. In particular, component weights of the mixture can be\nlinked to the covariates through a multinomial logistic regression model, where\neach component weight is a function of the linear predictor involving one or\nmore covariates. The proposed contribution extends this approach by replacing\nthe linear predictor, used for the component weights, with an additive\nstructure, where each term is a smooth function of the covariates considered.\nAn estimation procedure within the Bayesian paradigm is suggested. In\nparticular, a data augmentation scheme based on differenced random utility\nmodels is exploited, and smoothness of the covariate effects is controlled by\nsuitable choices for the prior distributions of the spline coefficients. The\nperformance of the proposed methodology is investigated via simulation\nexperiments, and an application to an original dataset about UK parliamentary\nvotes on Brexit is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:22:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Berrettini", "Marco", ""], ["Galimberti", "Giuliano", ""], ["Ranciati", "Saverio", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2105.13002", "submitter": "Olivier Faugeras P.", "authors": "Olivier P. Faugeras and Gilles Pag\\`es", "title": "Risk Quantization by Magnitude and Propensity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel approach in the assessment of a random risk variable $X$\nby introducing magnitude-propensity risk measures $(m_X,p_X)$. This bivariate\nmeasure intends to account for the dual aspect of risk, where the magnitudes\n$x$ of $X$ tell how hign are the losses incurred, whereas the probabilities\n$P(X=x)$ reveal how often one has to expect to suffer such losses. The basic\nidea is to simultaneously quantify both the severity $m_X$ and the propensity\n$p_X$ of the real-valued risk $X$. This is to be contrasted with traditional\nunivariate risk measures, like VaR or Expected shortfall, which typically\nconflate both effects. In its simplest form, $(m_X,p_X)$ is obtained by mass\ntransportation in Wasserstein metric of the law $P^X$ of $X$ to a two-points\n$\\{0, m_X\\}$ discrete distribution with mass $p_X$ at $m_X$. The approach can\nalso be formulated as a constrained optimal quantization problem.\n  This allows for an informative comparison of risks on both the magnitude and\npropensity scales. Several examples illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:38:05 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Faugeras", "Olivier P.", ""], ["Pag\u00e8s", "Gilles", ""]]}, {"id": "2105.13081", "submitter": "Wagner Barreto-Souza", "authors": "Raanju R. Sundararajan and Wagner Barreto-Souza", "title": "Student-t Stochastic Volatility Model With Composite Likelihood\n  EM-Algorithm", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust stochastic volatility (SV) model having Student-t marginals is\nproposed. Our process is defined through a linear normal regression model\ndriven by a latent gamma process that controls temporal dependence. This gamma\nprocess is strategically chosen to enable us to find an explicit expression for\nthe pairwise joint density function of the Student-t response process. With\nthis at hand, we propose a composite likelihood (CL) based inference for our\nmodel, which can be straightforwardly implemented with a low computational\ncost. This is a remarkable feature of our Student-t SV process over existing SV\nmodels in the literature that involve computationally heavy algorithms for\nestimating parameters. Aiming at a precise estimation of the parameters related\nto the latent process, we propose a CL Expectation-Maximization algorithm and\ndiscuss a bootstrap approach to obtain standard errors. The finite-sample\nperformance of our composite likelihood methods is assessed through Monte Carlo\nsimulations. The methodology is motivated by an empirical application in the\nfinancial market. We analyze the relationship, across multiple time periods,\nbetween various US sector Exchange-Traded Funds returns and individual\ncompanies' stock price returns based on our novel Student-t model. This\nrelationship is further utilized in selecting optimal financial portfolios.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:09:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sundararajan", "Raanju R.", ""], ["Barreto-Souza", "Wagner", ""]]}, {"id": "2105.13270", "submitter": "Stephen Taylor", "authors": "Stephen R. Taylor", "title": "The Nanohertz Gravitational Wave Astronomer", "comments": "Draft of a short technical book to be published later this year by\n  Taylor & Francis. 156 pages. Comments and errata are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE gr-qc stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gravitational waves are a radically new way to peer into the darkest depths\nof the cosmos. Pulsars can be used to make direct detections of gravitational\nwaves through precision timing. When a gravitational wave passes between a\npulsar and the Earth, it stretches and squeezes the intermediate space-time,\nleading to deviations of the measured pulse arrival times away from model\nexpectations. Combining the data from many Galactic pulsars can corroborate\nsuch a signal, and enhance its detection significance. This technique is known\nas a Pulsar Timing Array (PTA). Here I provide an overview of PTAs as a\nprecision gravitational-wave detection instrument, then review the types of\nsignal and noise processes that we encounter in typical pulsar data analysis. I\ntake a pragmatic approach, illustrating how searches are performed in real\nlife, and where possible directing the reader to codes or techniques that they\ncan explore for themselves. The goal is to provide theoretical background and\npractical recipes for data exploration that allow the reader to join in the\nexciting hunt for very low frequency gravitational waves.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 16:16:04 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Taylor", "Stephen R.", ""]]}, {"id": "2105.13282", "submitter": "Weijian Liu", "authors": "Weijian Liu, Zhaojian Zhang, Jun Liu, Zheran Shang, Yong-Liang Wang", "title": "Detection of a rank-one signal with limited training data", "comments": "This manuscript is accepted by Signal Processing", "journal-ref": null, "doi": null, "report-no": "SIGPRO_108120", "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we reconsider the problem of detecting a matrix-valued\nrank-one signal in unknown Gaussian noise, which was previously addressed for\nthe case of sufficient training data. We relax the above assumption to the case\nof limited training data. We re-derive the corresponding generalized likelihood\nratio test (GLRT) and two-step GLRT (2S--GLRT) based on certain unitary\ntransformation on the test data. It is shown that the re-derived detectors can\nwork with low sample support. Moreover, in sample-abundant environments the\nre-derived GLRT is the same as the previously proposed GLRT and the re-derived\n2S--GLRT has better detection performance than the previously proposed\n2S--GLRT. Numerical examples are provided to demonstrate the effectiveness of\nthe re-derived detectors.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:07:13 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Liu", "Weijian", ""], ["Zhang", "Zhaojian", ""], ["Liu", "Jun", ""], ["Shang", "Zheran", ""], ["Wang", "Yong-Liang", ""]]}, {"id": "2105.13396", "submitter": "Zachary Neal", "authors": "Zachary P. Neal, Rachel Domagalski, and Bruce Sagan", "title": "Comparing Models for Extracting the Backbone of Bipartite Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Projections of bipartite or two-mode networks capture co-occurrences, and are\nused in diverse fields (e.g., ecology, economics, bibliometrics, politics) to\nrepresent unipartite networks that would otherwise be difficult or impossible\nto measure directly. A key challenge in analyzing such networks is determining\nwhether an observed number of co-occurrences is significant. Several models now\nexist for doing so and thus for extracting the backbone of bipartite\nprojections, but they have not been directly compared to each other. In this\npaper, we compare five such models -- fixed fill model (FFM) fixed row model\n(FRM), fixed column model (FCM), fixed degree sequence model (FDSM), and\nstochastic degree sequence model (SDSM) -- in terms of accuracy, speed,\nstatistical power, similarity, and community detection. We find that the\ncomputationally-fast SDSM offers a statistically conservative but close\napproximation of the computationally-impractical FDSM under a wide range of\nconditions, and that it correctly recovers a known community structure even\nwhen the signal is weak. Therefore, although each backbone model may have\nparticular applications, we recommend SDSM for extracting the backbone of most\nbipartite projections.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 18:56:04 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:24:53 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 15:02:14 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Neal", "Zachary P.", ""], ["Domagalski", "Rachel", ""], ["Sagan", "Bruce", ""]]}, {"id": "2105.13445", "submitter": "Christopher Tosh", "authors": "Christopher Tosh, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki\n  Vehtari, Daniel Hsu", "title": "The piranha problem: Large effects swimming in a small pond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some scientific fields, it is common to have certain variables of interest\nthat are of particular importance and for which there are many studies\nindicating a relationship with a different explanatory variable. In such cases,\nparticularly those where no relationships are known among explanatory\nvariables, it is worth asking under what conditions it is possible for all such\nclaimed effects to exist simultaneously. This paper addresses this question by\nreviewing some theorems from multivariate analysis that show, unless the\nexplanatory variables also have sizable effects on each other, it is impossible\nto have many such large effects. We also discuss implications for the\nreplication crisis in social science.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 20:56:35 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Tosh", "Christopher", ""], ["Greengard", "Philip", ""], ["Goodrich", "Ben", ""], ["Gelman", "Andrew", ""], ["Vehtari", "Aki", ""], ["Hsu", "Daniel", ""]]}, {"id": "2105.13454", "submitter": "Americo Cunha Jr", "authors": "Americo Cunha Jr, Christian Soize, Rubens Sampaio", "title": "Computational modeling of the nonlinear stochastic dynamics of\n  horizontal drillstrings", "comments": null, "journal-ref": "Computational Mechanics, vol. 55, pp. 849-878, 2015", "doi": "10.1007/s00466-015-1206-6", "report-no": null, "categories": "cs.CE math-ph math.MP physics.class-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work intends to analyze the nonlinear stochastic dynamics of\ndrillstrings in horizontal configuration. For this purpose, it considers a beam\ntheory, with effects of rotatory inertia and shear deformation, which is\ncapable of reproducing the large displacements that the beam undergoes. The\nfriction and shock effects, due to beam/borehole wall transversal impacts, as\nwell as the force and torque induced by bit-rock interaction, are also\nconsidered in the model. Uncertainties of bit-rock interaction model are taken\ninto account using a parametric probabilistic approach. Numerical simulations\nhave shown that the mechanical system of interest has a very rich nonlinear\nstochastic dynamics, which generate phenomena such as bit-bounce, stick-slip,\nand transverse impacts. A study aiming to maximize the drilling process\nefficiency, varying drillstring velocities of translation and rotation is\npresented. Also, the work presents the definition and solution of two\noptimizations problems, one deterministic and one robust, where the objective\nis to maximize drillstring rate of penetration into the soil respecting its\nstructural limits.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 21:22:56 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cunha", "Americo", "Jr"], ["Soize", "Christian", ""], ["Sampaio", "Rubens", ""]]}, {"id": "2105.13483", "submitter": "Matteo Guardiani", "authors": "Matteo Guardiani, Philipp Frank, Andrija Kosti\\'c, Gordian Edenhofer,\n  Jakob Roth, Berit Uhlmann, Torsten En{\\ss}lin", "title": "Non-parametric Bayesian Causal Modeling of the SARS-CoV-2 Viral Load\n  Distribution vs. Patient's Age", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The viral load of patients infected with SARS-CoV-2 varies on logarithmic\nscales and possibly with age. Controversial claims have been made in the\nliterature regarding whether the viral load distribution actually depends on\nthe age of the patients. Such a dependence would have implications for the\nCOVID-19 spreading mechanism, the age-dependent immune system reaction, and\nthus for policymaking. We hereby develop a method to analyze viral-load\ndistribution data as a function of the patients' age within a flexible,\nnon-parametric, hierarchical, Bayesian, and causal model. This method can be\napplied to other contexts as well, and for this purpose, it is made freely\navailable. The developed reconstruction method also allows testing for bias in\nthe data. This could be due to, e.g., bias in patient-testing and data\ncollection or systematic errors in the measurement of the viral load. We\nperform these tests by calculating the Bayesian evidence for each implied\npossible causal direction. When applying these tests to publicly available age\nand SARS-CoV-2 viral load data, we find a statistically significant increase in\nthe viral load with age, but only for one of the two analyzed datasets. If we\nconsider this dataset, and based on the current understanding of viral load's\nimpact on patients' infectivity, we expect a non-negligible difference in the\ninfectivity of different age groups. This difference is nonetheless too small\nto justify considering any age group as noninfectious.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:35:02 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guardiani", "Matteo", ""], ["Frank", "Philipp", ""], ["Kosti\u0107", "Andrija", ""], ["Edenhofer", "Gordian", ""], ["Roth", "Jakob", ""], ["Uhlmann", "Berit", ""], ["En\u00dflin", "Torsten", ""]]}, {"id": "2105.13600", "submitter": "Jiangbin Lyu Dr.", "authors": "Bifeng Ling, Jiangbin Lyu and Liqun Fu", "title": "Placement Optimization and Power Control in Intelligent Reflecting\n  Surface Aided Multiuser System", "comments": "2-col, 7 pages. This paper focuses on the multi-IRS placement\n  optimization and downlink AP power control for achieving max-min throughput\n  in a single-cell multi-user system. A ring-based IRS placement scheme is\n  proposed which utilizes the near-AP/near-user deployment modes. Closed-form\n  power control policy is devised to equalize the users' non-outage probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent reflecting surface (IRS) is a new and revolutionary technology\ncapable of reconfiguring the wireless propagation environment by controlling\nits massive low-cost passive reflecting elements. Different from prior works\nthat focus on optimizing IRS reflection coefficients or single-IRS placement,\nwe aim to maximize the minimum throughput of a single-cell multiuser system\naided by multiple IRSs, by joint multi-IRS placement and power control at the\naccess point (AP), which is a mixed-integer non-convex problem with drastically\nincreased complexity with the number of IRSs/users. To tackle this challenge, a\nring-based IRS placement scheme is proposed along with a power control policy\nthat equalizes the users' non-outage probability. An efficient searching\nalgorithm is further proposed to obtain a close-to-optimal solution for\narbitrary number of IRSs/rings. Numerical results validate our analysis and\nshow that our proposed scheme significantly outperforms the benchmark schemes\nwithout IRS and/or with other power control policies. Moreover, it is shown\nthat the IRSs are preferably deployed near AP for coverage range extension,\nwhile with more IRSs, they tend to spread out over the cell to cover more and\nget closer to target users.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 06:03:30 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ling", "Bifeng", ""], ["Lyu", "Jiangbin", ""], ["Fu", "Liqun", ""]]}, {"id": "2105.13762", "submitter": "Ioannis Kontoyiannis", "authors": "Ioannis Kontoyiannis and Lawrence Tray", "title": "Inferring community characteristics in labelled networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Labelled networks form a very common and important class of data, naturally\nappearing in numerous applications in science and engineering. A typical\ninference goal is to determine how the vertex labels(or {\\em features}) affect\nthe network's graph structure. A standard approach has been to partition the\nnetwork into blocks grouped by distinct values of the feature of interest. A\nblock-based random graph model -- typically a variant of the stochastic block\nmodel -- is then used to test for evidence of asymmetric behaviour within these\nfeature-based communities. Nevertheless, the resulting communities often do not\nproduce a natural partition of the graph. In this work, we introduce a new\ngenerative model, the feature-first block model (FFBM), which is more effective\nat describing vertex-labelled undirected graphs and also facilitates the use of\nricher queries on labelled networks. We develop a Bayesian framework for\ninference with this model, and we present a method to efficiently sample from\nthe posterior distribution of the FFBM parameters. The FFBM's structure is kept\ndeliberately simple to retain easy interpretability of the parameter values. We\napply the proposed methods to a variety of network data to extract the most\nimportant features along which the vertices are partitioned. The main\nadvantages of the proposed approach are that the whole feature-space is used\nautomatically, and features can be rank-ordered implicitly according to impact.\nAny features that do not significantly impact the high-level structure can be\ndiscarded to reduce the problem dimension. In cases where the vertex features\navailable do not readily explain the community structure in the resulting\nnetwork, the approach detects this and is protected against over-fitting.\nResults on several real-world datasets illustrate the performance of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:07:10 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Kontoyiannis", "Ioannis", ""], ["Tray", "Lawrence", ""]]}, {"id": "2105.13778", "submitter": "Jan Van Haaren", "authors": "Jan Van Haaren", "title": "\"Why Would I Trust Your Numbers?\" On the Explainability of Expected\n  Values in Soccer", "comments": "Paper accepted for presentation at the AI for Sports Analytics\n  workshop at IJCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many different approaches have been proposed to quantify the\nperformances of soccer players. Since player performances are challenging to\nquantify directly due to the low-scoring nature of soccer, most approaches\nestimate the expected impact of the players' on-the-ball actions on the\nscoreline. While effective, these approaches are yet to be widely embraced by\nsoccer practitioners. The soccer analytics community has primarily focused on\nimproving the accuracy of the models, while the explainability of the produced\nmetrics is often much more important to practitioners.\n  To help bridge the gap between scientists and practitioners, we introduce an\nexplainable Generalized Additive Model that estimates the expected value for\nshots. Unlike existing models, our model leverages features corresponding to\nwidespread soccer concepts. To this end, we represent the locations of shots by\nfuzzily assigning the shots to designated zones on the pitch that practitioners\nare familiar with. Our experimental evaluation shows that our model is as\naccurate as existing models, while being easier to explain to soccer\npractitioners.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 10:05:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Van Haaren", "Jan", ""]]}, {"id": "2105.13801", "submitter": "Jonathan Dumas", "authors": "Jonathan Dumas, Colin Cointe, Antoine Wehenkel, Antonio Sutera, Xavier\n  Fettweis, and Bertrand Corn\\'elusse", "title": "A Probabilistic Forecast-Driven Strategy for a Risk-Aware Participation\n  in the Capacity Firming Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The core contribution is to propose a probabilistic forecast-driven strategy,\nmodeled as a min-max-min robust optimization problem with recourse, and solved\nusing a Benders-dual cutting plane algorithm in a tractable manner. The\nconvergence is improved by building an initial set of cuts. In addition, a\ndynamic risk-averse parameters selection strategy based on the quantile\nforecasts distribution is proposed. A secondary contribution is to use a\nrecently developed deep learning model known as normalizing flows to generate\nquantile forecasts of renewable generation for the robust optimization problem.\nThis technique provides a general mechanism for defining expressive probability\ndistributions, only requiring the specification of a base distribution and a\nseries of bijective transformations. Overall, the robust approach improves the\nresults over a deterministic approach with nominal point forecasts by finding a\ntrade-off between conservative and risk-seeking policies. The case study uses\nthe photovoltaic generation monitored on-site at the University of Li\\`ege\n(ULi\\`ege), Belgium.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 13:13:07 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 12:24:20 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dumas", "Jonathan", ""], ["Cointe", "Colin", ""], ["Wehenkel", "Antoine", ""], ["Sutera", "Antonio", ""], ["Fettweis", "Xavier", ""], ["Corn\u00e9lusse", "Bertrand", ""]]}, {"id": "2105.13854", "submitter": "Andriy Temko Dr", "authors": "Alison O'Shea, Gordon Lightbody, Geraldine Boylan, Andriy Temko", "title": "Neonatal seizure detection from raw multi-channel EEG using a fully\n  convolutional architecture", "comments": null, "journal-ref": "Neural Networks (2020)", "doi": "10.1016/j.neunet.2019.11.023", "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A deep learning classifier for detecting seizures in neonates is proposed.\nThis architecture is designed to detect seizure events from raw\nelectroencephalogram (EEG) signals as opposed to the state-of-the-art hand\nengineered feature-based representation employed in traditional machine\nlearning based solutions. The seizure detection system utilises only\nconvolutional layers in order to process the multichannel time domain signal\nand is designed to exploit the large amount of weakly labelled data in the\ntraining stage. The system performance is assessed on a large database of\ncontinuous EEG recordings of 834h in duration; this is further validated on a\nheld-out publicly available dataset and compared with two baseline SVM based\nsystems.\n  The developed system achieves a 56% relative improvement with respect to a\nfeature-based state-of-the art baseline, reaching an AUC of 98.5%; this also\ncompares favourably both in terms of performance and run-time. The effect of\nvarying architectural parameters is thoroughly studied. The performance\nimprovement is achieved through novel architecture design which allows more\nefficient usage of available training data and end-to-end optimisation from the\nfront-end feature extraction to the back-end classification. The proposed\narchitecture opens new avenues for the application of deep learning to neonatal\nEEG, where the performance becomes a function of the amount of training data\nwith less dependency on the availability of precise clinical labels.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:08:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["O'Shea", "Alison", ""], ["Lightbody", "Gordon", ""], ["Boylan", "Geraldine", ""], ["Temko", "Andriy", ""]]}, {"id": "2105.14055", "submitter": "Francis Duval", "authors": "Francis Duval, Jean-Philippe Boucher, Mathieu Pigeon", "title": "How much telematics information do insurers need for claim\n  classification?", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has been shown several times in the literature that telematics data\ncollected in motor insurance help to better understand an insured's driving\nrisk. Insurers that use this data reap several benefits, such as a better\nestimate of the pure premium, more segmented pricing and less adverse\nselection. The flip side of the coin is that collected telematics information\nis often sensitive and can therefore compromise policyholders' privacy.\nMoreover, due to their large volume, this type of data is costly to store and\nhard to manipulate. These factors, combined with the fact that insurance\nregulators tend to issue more and more recommendations regarding the collection\nand use of telematics data, make it important for an insurer to determine the\nright amount of telematics information to collect. In addition to traditional\ncontract information such as the age and gender of the insured, we have access\nto a telematics dataset where information is summarized by trip. We first\nderive several features of interest from these trip summaries before building a\nclaim classification model using both traditional and telematics features. By\ncomparing a few classification algorithms, we find that logistic regression\nwith lasso penalty is the most suitable for our problem. Using this model, we\ndevelop a method to determine how much information about policyholders' driving\nshould be kept by an insurer. Using real data from a North American insurance\ncompany, we find that telematics data become redundant after about 3 months or\n4,000 kilometers of observation, at least from a claim classification\nperspective.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:48:48 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Duval", "Francis", ""], ["Boucher", "Jean-Philippe", ""], ["Pigeon", "Mathieu", ""]]}, {"id": "2105.14139", "submitter": "Sergey Ketkov", "authors": "Sergey S. Ketkov, Andrei S. Shilov, Oleg A. Prokopyev", "title": "On a class of data-driven combinatorial optimization problems under\n  uncertainty: a distributionally robust approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this study we analyze linear combinatorial optimization problems where the\ncost vector is not known a priori, but is only observable through a finite data\nset. In contrast to the related studies, we presume that the number of\nobservations with respect to particular components of the cost vector may vary.\nThe goal is to find a procedure that transforms the data set into an estimate\nof the expected value of the objective function (which is referred to as a\nprediction rule) and a procedure that retrieves a candidate decision (which is\nreferred to as a prescription rule). We aim at finding the least conservative\nprediction and prescription rules, which satisfy some specified asymptotic\nguarantees. We demonstrate that the resulting vector optimization problems\nadmit a weakly optimal solution, which can be obtained by solving a particular\ndistributionally robust optimization problem. Specifically, the decision-maker\nmay optimize the worst-case expected loss across all probability distributions\nwith given component-wise relative entropy distances from the empirical\nmarginal distributions. Finally, we perform numerical experiments to analyze\nthe out-of-sample performance of the proposed solution approach.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 23:17:35 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 20:32:53 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ketkov", "Sergey S.", ""], ["Shilov", "Andrei S.", ""], ["Prokopyev", "Oleg A.", ""]]}, {"id": "2105.14197", "submitter": "Cory McCartan", "authors": "Christopher T. Kenny (1), Shiro Kuriwaki (1), Cory McCartan (2), Evan\n  Rosenman (3), Tyler Simko (1), Kosuke Imai (1 and 2) ((1) Department of\n  Government, Harvard University, (2) Department of Statistics, Harvard\n  University, (3) Harvard Data Science Initiative)", "title": "The Impact of the U.S. Census Disclosure Avoidance System on\n  Redistricting and Voting Rights Analysis", "comments": "30 pages, 13 figures. General revisions, improved figures, and\n  updates reflecting new developments since initial publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The US Census Bureau plans to protect the privacy of 2020 Census respondents\nthrough its Disclosure Avoidance System (DAS), which attempts to achieve\ndifferential privacy guarantees by adding noise to the Census microdata. By\napplying redistricting simulation and analysis methods to DAS-protected 2010\nCensus data, we find that the protected data are not of sufficient quality for\nredistricting purposes. We demonstrate that the injected noise makes it\nimpossible for states to accurately comply with the One Person, One Vote\nprinciple. Our analysis finds that the DAS-protected data are biased against\ncertain areas, depending on voter turnout and partisan and racial composition,\nand that these biases lead to large and unpredictable errors in the analysis of\npartisan and racial gerrymanders. Finally, we show that the DAS algorithm does\nnot universally protect respondent privacy. Based on the names and addresses of\nregistered voters, we are able to predict their race as accurately using the\nDAS-protected data as when using the 2010 Census data. Despite this, the\nDAS-protected data can still inaccurately estimate the number of\nmajority-minority districts. We conclude with recommendations for how the\nCensus Bureau should proceed with privacy protection for the 2020 Census.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 03:32:36 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 17:54:52 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Kenny", "Christopher T.", "", "1 and 2"], ["Kuriwaki", "Shiro", "", "1 and 2"], ["McCartan", "Cory", "", "1 and 2"], ["Rosenman", "Evan", "", "1 and 2"], ["Simko", "Tyler", "", "1 and 2"], ["Imai", "Kosuke", "", "1 and 2"]]}, {"id": "2105.14222", "submitter": "Panos Toulis", "authors": "Panos Toulis, Jacob Bean", "title": "Randomization Inference of Periodicity in Unequally Spaced Time Series\n  with Application to Exoplanet Detection", "comments": "8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of periodicity is a fundamental task in many scientific areas\nof study. Existing methods rely on theoretical assumptions that the observation\ntimes have equal or i.i.d. spacings, and that common estimators, such as the\nperiodogram peak, are consistent and asymptotically normal. In practice,\nhowever, these assumptions are unrealistic as observation times usually exhibit\ndeterministic patterns -- e.g., the nightly observation cycle in astronomy --\nthat imprint nuisance periodicities in the data. These nuisance signals also\naffect the finite-sample distribution of estimators, which can substantially\ndeviate from normality. Here, we propose a set identification method, fusing\nideas from randomization inference and partial identification. In particular,\nwe develop a sharp test for any periodicity value, and then invert the test to\nbuild a confidence set. This approach is appropriate here because the\nconstruction of confidence sets does not rely on assumptions of regular or\nwell-behaved asymptotics. Notably, our inference is valid in finite samples\nwhen our method is fully implemented, while it can be asymptotically valid\nunder an approximate implementation designed to ease computation. Empirically,\nwe validate our method in exoplanet detection using radial velocity data. In\nthis context, our method correctly identifies the periodicity of the confirmed\nexoplanets in our sample. For some other, yet unconfirmed detections, we show\nthat the statistical evidence is weak, which illustrates the failure of\ntraditional statistical techniques. Last but not least, our method offers a\nconstructive way to resolve these identification issues via improved\nobservation designs. In exoplanet detection, these designs suggest meaningful\nimprovements in identifying periodicity even when a moderate amount of\nrandomization is introduced in scheduling radial velocity measurements.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:55:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Toulis", "Panos", ""], ["Bean", "Jacob", ""]]}, {"id": "2105.14397", "submitter": "Francois Meyer", "authors": "Daniel Ferguson, Francois G. Meyer", "title": "The Sample Fr\\'echet Mean (or Median) Graph of Sparse Graphs is Sparse", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.SI physics.data-an stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To characterize the \"average\" of a sample of graphs, one can compute the\nsample Frechet mean (or median) graph, which provides an interpretable summary\nof the graph sample. In this paper, we address the following foundational\nquestion: does the mean or median graph inherit the structural properties of\nthe graphs in the sample? An important graph property is the edge density.\nBecause sparse graphs provide prototypical models for real networks, one would\nlike to guarantee that the edge density be preserved when computing the sample\nmean (or median). In this paper, we prove that the edge density is an\nhereditary property, which can be transmitted from a graph sample to its sample\nFrechet mean (or median), irrespective of the method used to estimate the mean\nor the median. Specifically, we prove the following result: the number of edges\nof the Frechet mean (or median) graph of a set of graphs is bounded by the\nmaximal number of edges amongst all the graphs in the sample. We prove the\nresult for the graph Hamming distance, and the spectral adjacency pseudometric,\nusing very different arguments.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:40:43 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 02:52:21 GMT"}, {"version": "v3", "created": "Thu, 29 Jul 2021 00:39:09 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Ferguson", "Daniel", ""], ["Meyer", "Francois G.", ""]]}, {"id": "2105.14649", "submitter": "Bohan Wu", "authors": "Bohan Wu and Bradley Van Allen", "title": "A Comparison of Functional Principal Component Analysis Methods with\n  Accelerometry Applications", "comments": "undergraduate thesis paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The association between a person's physical activity and various health\noutcomes is an area of active research. The National Health and Nutrition\nExamination Survey (NHANES) data provide a valuable resource for studying these\nassociations. NHANES accelerometry data has been used by many to measure\nindividuals' activity levels. A common approach for analyzing accelerometry\ndata is functional principal component analysis (FPCA). The first part of the\npaper uses Poisson FPCA (PFPCA), Gaussian FPCA (GFPCA), and nonnegative and\nregularized function decomposition (NARFD) to extract features from the\ncount-valued NHANES accelerometry data. The second part of the paper compares\nlogistic regression, random forests, and AdaBoost models based on GFPCA, NARFD,\nor PFPCA scores in the context of mortality prediction. The results show that\nPoisson FPCA is the best FPCA model for the inference of accelerometry data,\nand the AdaBoost model based on Poisson FPCA scores gives the best mortality\nprediction results.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 23:16:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wu", "Bohan", ""], ["Van Allen", "Bradley", ""]]}]