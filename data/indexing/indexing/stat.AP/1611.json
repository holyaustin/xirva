[{"id": "1611.00083", "submitter": "Joseph Roy", "authors": "Amelia Kimball and Kailen Shantz and Christopher Eager and Joseph Roy", "title": "Confronting Quasi-Separation in Logistic Mixed Effects for Linguistic\n  Data: A Bayesian Approach", "comments": "Draft version of JQL accepted paper", "journal-ref": null, "doi": "10.1080/09296174.2018.1499457", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed effects regression models are widely used by language researchers.\nHowever, these regressions are implemented with an algorithm which may not\nconverge on a solution. While convergence issues in linear mixed effects models\ncan often be addressed with careful experiment design and model building,\nlogistic mixed effects models introduce the possibility of separation or\nquasi-separation, which can cause problems for model estimation that result in\nconvergence errors or in unreasonable model estimates. These problems cannot be\nsolved by experiment or model design. In this paper, we discuss\n(quasi-)separation with the language researcher in mind, explaining what it is,\nhow it causes problems for model estimation, and why it can be expected in\nlinguistic datasets. Using real linguistic datasets, we then show how Bayesian\nmodels can be used to overcome convergence issues introduced by\nquasi-separation, whereas frequentist approaches fail. On the basis of these\ndemonstrations, we advocate for the adoption of Bayesian models as a practical\nsolution to dealing with convergence issues when modeling binary linguistic\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 23:48:17 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 13:21:28 GMT"}, {"version": "v3", "created": "Fri, 7 Sep 2018 16:46:50 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Kimball", "Amelia", ""], ["Shantz", "Kailen", ""], ["Eager", "Christopher", ""], ["Roy", "Joseph", ""]]}, {"id": "1611.00458", "submitter": "Earl Lawrence", "authors": "Earl Lawrence, Scott Vander Wiel, Casey J. Law, Sarah Burke Spolaor,\n  and Geoffrey C. Bower", "title": "The Non-homogeneous Poisson Process for Fast Radio Burst Rates", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": "10.3847/1538-3881/aa844e", "report-no": null, "categories": "astro-ph.HE astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the non-homogeneous Poisson process (NHPP) for modeling\nthe rate of fast radio bursts (FRBs) and other infrequently observed\nastronomical events. The NHPP, well-known in statistics, can model changes in\nthe rate as a function of both astronomical features and the details of an\nobserving campaign. This is particularly helpful for rare events like FRBs\nbecause the NHPP can combine information across surveys, making the most of all\navailable information. The goal of the paper is two-fold. First, it is intended\nto be a tutorial on the use of the NHPP. Second, we build an NHPP model that\nincorporates beam patterns and a power law flux distribution for the rate of\nFRBs. Using information from 12 surveys including 15 detections, we find an\nall-sky FRB rate of 586.88 events per sky per day above a flux of 1 Jy (95\\%\nCI: 271.86, 923.72) and a flux power-law index of 0.91 (95\\% CI: 0.57, 1.25).\nOur rate is lower than other published rates, but consistent with the rate\ngiven in Champion et al. 2016.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 03:12:05 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Lawrence", "Earl", ""], ["Wiel", "Scott Vander", ""], ["Law", "Casey J.", ""], ["Spolaor", "Sarah Burke", ""], ["Bower", "Geoffrey C.", ""]]}, {"id": "1611.00460", "submitter": "Shahryar Minhas", "authors": "Shahryar Minhas and Peter D. Hoff and Michael D. Ward", "title": "Inferential Approaches for Network Analyses: AMEN for Latent Factor\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach to conduct inferential analyses on dyadic\ndata while accounting for interdependencies between observations through a set\nof additive and multiplicative effects (AME). The AME model is built on a\ngeneralized linear modeling framework and is thus flexible enough to be applied\nto a variety of contexts. We contrast the AME model to two prominent approaches\nin the literature: the latent space model (LSM) and the exponential random\ngraph model (ERGM). Relative to these approaches, we show that the AME approach\nis a) to be easy to implement; b) interpretable in a general linear model\nframework; c) computationally straightforward; d) not prone to degeneracy; e)\ncaptures 1st, 2nd, and 3rd order network dependencies; and f) notably\noutperforms ERGMs and LSMs on a variety of metrics and in an out-of-sample\ncontext. In summary, AME offers a straightforward way to undertake nuanced,\nprincipled inferential network analysis for a wide range of social science\nquestions.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 03:19:52 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2018 02:26:24 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Minhas", "Shahryar", ""], ["Hoff", "Peter D.", ""], ["Ward", "Michael D.", ""]]}, {"id": "1611.00538", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "S\\'andor Boz\\'oki, L\\'aszl\\'o Csat\\'o, J\\'ozsef Temesi", "title": "An application of incomplete pairwise comparison matrices for ranking\n  top tennis players", "comments": "14 pages, 2 figures", "journal-ref": "European Journal of Operational Research, 248(1): 211-218, 2016", "doi": "10.1016/j.ejor.2015.06.069", "report-no": null, "categories": "cs.AI cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison is an important tool in multi-attribute decision making.\nPairwise comparison matrices (PCM) have been applied for ranking criteria and\nfor scoring alternatives according to a given criterion. Our paper presents a\nspecial application of incomplete PCMs: ranking of professional tennis players\nbased on their results against each other. The selected 25 players have been on\nthe top of the ATP rankings for a shorter or longer period in the last 40\nyears. Some of them have never met on the court. One of the aims of the paper\nis to provide ranking of the selected players, however, the analysis of\nincomplete pairwise comparison matrices is also in the focus. The eigenvector\nmethod and the logarithmic least squares method were used to calculate weights\nfrom incomplete PCMs. In our results the top three players of four decades were\nNadal, Federer and Sampras. Some questions have been raised on the properties\nof incomplete PCMs and remains open for further investigation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:36:11 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Boz\u00f3ki", "S\u00e1ndor", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""], ["Temesi", "J\u00f3zsef", ""]]}, {"id": "1611.00544", "submitter": "Lloyd Elliott", "authors": "Lloyd T. Elliott and Yee Whye Teh", "title": "A nonparametric HMM for genetic imputation and coalescent inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic sequence data are well described by hidden Markov models (HMMs) in\nwhich latent states correspond to clusters of similar mutation patterns. Theory\nfrom statistical genetics suggests that these HMMs are nonhomogeneous (their\ntransition probabilities vary along the chromosome) and have large support for\nself transitions. We develop a new nonparametric model of genetic sequence\ndata, based on the hierarchical Dirichlet process, which supports these self\ntransitions and nonhomogeneity. Our model provides a parameterization of the\ngenetic process that is more parsimonious than other more general nonparametric\nmodels which have previously been applied to population genetics. We provide\ntruncation-free MCMC inference for our model using a new auxiliary sampling\nscheme for Bayesian nonparametric HMMs. In a series of experiments on male X\nchromosome data from the Thousand Genomes Project and also on data simulated\nfrom a population bottleneck we show the benefits of our model over the popular\nfinite model fastPHASE, which can itself be seen as a parametric truncation of\nour model. We find that the number of HMM states found by our model is\ncorrelated with the time to the most recent common ancestor in population\nbottlenecks. This work demonstrates the flexibility of Bayesian nonparametrics\napplied to large and complex genetic data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:48:04 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Elliott", "Lloyd T.", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.00953", "submitter": "Frank Dondelinger", "authors": "Frank Dondelinger, Sach Mukherjee and The Alzheimer's Disease\n  Neuroimaging Initiative", "title": "High-dimensional regression over disease subgroups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional regression over subgroups of observations. Our\nwork is motivated by biomedical problems, where disease subtypes, for example,\nmay differ with respect to underlying regression models, but sample sizes at\nthe subgroup-level may be limited. We focus on the case in which\nsubgroup-specific models may be expected to be similar but not necessarily\nidentical. Our approach is to treat subgroups as related problem instances and\njointly estimate subgroup-specific regression coefficients. This is done in a\npenalized framework, combining an $\\ell_1$ term with an additional term that\npenalizes differences between subgroup-specific coefficients. This gives\nsolutions that are globally sparse but that allow information-sharing between\nthe subgroups. We present algorithms for estimation and empirical results on\nsimulated data and using Alzheimer's disease, amyotrophic lateral sclerosis and\ncancer datasets. These examples demonstrate the gains our approach can offer in\nterms of prediction and the ability to estimate subgroup-specific sparsity\npatterns.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 10:58:20 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 13:51:38 GMT"}], "update_date": "2016-12-12", "authors_parsed": [["Dondelinger", "Frank", ""], ["Mukherjee", "Sach", ""], ["Initiative", "The Alzheimer's Disease Neuroimaging", ""]]}, {"id": "1611.01006", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Bayesian Heuristics for Group Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI cs.SY math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model of inference and heuristic decision-making in groups that\nis rooted in the Bayes rule but avoids the complexities of rational inference\nin partially observed environments with incomplete information, which are\ncharacteristic of group interactions. Our model is also consistent with a\ndual-process psychological theory of thinking: the group members behave\nrationally at the initiation of their interactions with each other (the slow\nand deliberative mode); however, in the ensuing decision epochs, they rely on a\nheuristic that replicates their experiences from the first stage (the fast\nautomatic mode). We specialize this model to a group decision scenario where\nprivate observations are received at the beginning, and agents aim to take the\nbest action given the aggregate observations of all group members. We study the\nimplications of the information structure together with the properties of the\nprobability distributions which determine the structure of the so-called\n\"Bayesian heuristics\" that the agents follow in our model. We also analyze the\ngroup decision outcomes in two classes of linear action updates and log-linear\nbelief updates and show that many inefficiencies arise in group decisions as a\nresult of repeated interactions between individuals, leading to overconfident\nbeliefs as well as choice-shifts toward extremes. Nevertheless, balanced\nregular structures demonstrate a measure of efficiency in terms of aggregating\nthe initial information of individuals. These results not only verify some\nwell-known insights about group decision-making but also complement these\ninsights by revealing additional mechanistic interpretations for the group\ndeclension-process, as well as psychological and cognitive intuitions about the\ngroup interaction model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:57:42 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.01024", "submitter": "Adrien Hitz", "authors": "Adrien S. Hitz and Robin J. Evans", "title": "Modeling Website Visits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multivariate model for the number of hits on a set of popular\nwebsites, and show it to accurately reflect the behavior recorded in a data set\nof Internet users in the United States. We assume that the random vector of\nvisits is distributed according to a censored multivariate normal with\nmarginals transformed to be discrete Pareto IV and, following the ideas of\nGaussian graphical models, we enforce sparsity on the inverse covariance matrix\nto reduce dimensionality and to visualize the dependence structure as a graph.\nThe model allows for an easy inclusion of covariates and is useful for\ncomprehending the behavior of Internet users as a function of their age and\ngender.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 14:01:20 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Hitz", "Adrien S.", ""], ["Evans", "Robin J.", ""]]}, {"id": "1611.01213", "submitter": "Nilabja Guha", "authors": "Keren Yang, Nilabja Guha, Yalchin Efendiev, Bani K. Mallick", "title": "Bayesian and Variational Bayesian approaches for flows in heterogenous\n  random media", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.04.034", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study porous media flows in heterogeneous stochastic media.\nWe propose an efficient forward simulation technique that is tailored for\nvariational Bayesian inversion. As a starting point, the proposed forward\nsimulation technique decomposes the solution into the sum of separable\nfunctions (with respect to randomness and the space), where each term is\ncalculated based on a variational approach. This is similar to Proper\nGeneralized Decomposition (PGD). Next, we apply a multiscale technique to solve\nfor each term and, further, decompose the random function into 1D fields. As a\nresult, our proposed method provides an approximation hierarchy for the\nsolution as we increase the number of terms in the expansion and, also,\nincrease the spatial resolution of each term. We use the hierarchical solution\ndistributions in a variational Bayesian approximation to perform uncertainty\nquantification in the inverse problem. We conduct a detailed numerical study to\nexplore the performance of the proposed uncertainty quantification technique\nand show the theoretical posterior concentration.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 22:31:05 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 01:25:27 GMT"}, {"version": "v3", "created": "Fri, 9 Feb 2018 02:53:41 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Yang", "Keren", ""], ["Guha", "Nilabja", ""], ["Efendiev", "Yalchin", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1611.01310", "submitter": "Angela Bitto", "authors": "Angela Bitto, Sylvia Fr\\\"uhwirth-Schnatter", "title": "Achieving Shrinkage in a Time-Varying Parameter Model Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage for time-varying parameter (TVP) models is investigated within a\nBayesian framework, with the aim to automatically reduce time-varying\nparameters to static ones, if the model is overfitting. This is achieved\nthrough placing the double gamma shrinkage prior on the process variances. An\nefficient Markov chain Monte Carlo scheme is developed, exploiting boosting\nbased on the ancillarity-sufficiency interweaving strategy. The method is\napplicable both to TVP models for univariate as well as multivariate time\nseries. Applications include a TVP generalized Phillips curve for EU area\ninflation modelling and a multivariate TVP Cholesky stochastic volatility model\nfor joint modelling of the returns from the DAX-30 index.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 10:21:55 GMT"}, {"version": "v2", "created": "Sun, 3 Jun 2018 13:48:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bitto", "Angela", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1611.01362", "submitter": "Murwan Siddig", "authors": "Muhammad Fikri Budiana, Murwan H. M. A. Siddig", "title": "Flowgraph Models and Analysis for Markov Jump Processes", "comments": "12 pages, 22 Figures, 7th European Business Research Conference held\n  in Rome, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Flowgraph models provide an alternative approach in modeling a multi-state\nstochastic process. One of the most widely used stochastic processes that have\nmany real-world applications especially in actuarial models is the Markov jump\nprocess or continuous- time Markov chain. However, finding waiting time\ndistributions between any two states in a Markov jump process can be very\ndifficult. Flowgraph analysis for Markov jump process comprises of modeling the\npossible states of the process, the interstates waiting time distribution, and\nworking on the moment generating function domain to obtain the total waiting\ntime distribution in form of density or survival function. This paper gives the\ntheory and computational method of flowgraph analysis, uses it in Markov\nprocess problems, and compares the traditional Markov process construction\nmethod with the flowgraph method to demonstrate the convenience and\npracticality of flowgraph analysis.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 19:23:53 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Budiana", "Muhammad Fikri", ""], ["Siddig", "Murwan H. M. A.", ""]]}, {"id": "1611.01387", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Necessary and Sufficient Condition for Nonsingular Fisher Information\n  Matrix in ARMA Models", "comments": "5 pages", "journal-ref": "The American Statistician 53 (1) June 1999", "doi": "10.1080/00031305.1999.10474433", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is demonstrated that a necessary and sufficient condition that the Fisher\ninformation matrix of an ARMA model be nonsingular is that the model not be\nredundant, that is, the autoregressive and moving-average polynomials do not\nshare common roots.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 14:14:54 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01439", "submitter": "Joram Soch", "authors": "Joram Soch, Carsten Allefeld", "title": "Exceedance Probabilities for the Dirichlet Distribution", "comments": "10 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an efficient method to calculate exceedance probabilities (EP) for\nthe Dirichlet distribution when the number of event types is larger than two.\nAlso, we present an intuitive application of Dirichlet EPs and compare our\nmethod to a sampling approach which is the current practice in neuroimaging\nmodel selection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 16:20:10 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Soch", "Joram", ""], ["Allefeld", "Carsten", ""]]}, {"id": "1611.01480", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "Why comparing survival curves between two subgroups may be misleading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse an issue when comparing survival curves between two subgroups. We\nshow that there is a direct relationship between estimates of subgroups'\nsurvival at a time point and positive and negative predictive values in the\nbinary classification settings. Our findings present a case where current\nmethods of comparing survival curves between subgroups may be misleading. We\nthink that this ought to be taken into account during the validation of\nprognostic diagnostic tests that predict two prognostic subgroups for a given\ndisease or treatment, when the validation data set consists of censored data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:26:00 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 14:56:08 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "1611.01485", "submitter": "Meike K\\\"ohler", "authors": "Meike K\\\"ohler, Nikolaus Umlauf, Andreas Beyerlein, Christiane\n  Winkler, Anette-Gabriele Ziegler and Sonja Greven", "title": "Flexible Bayesian additive joint models with an application to type 1\n  diabetes research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint modeling of longitudinal and time-to-event data is an important\ntool of growing popularity to gain insights into the association between a\nbiomarker and an event process. We develop a general framework of flexible\nadditive joint models that allows the specification of a variety of effects,\nsuch as smooth nonlinear, time-varying and random effects, in the longitudinal\nand survival parts of the models. Our extensions are motivated by the\ninvestigation of the relationship between fluctuating disease-specific markers,\nin this case autoantibodies, and the progression to the autoimmune disease type\n1 diabetes. By making use of Bayesian P-splines we are in particular able to\ncapture highly nonlinear subject-specific marker trajectories as well as a\ntime-varying association between the marker and the event process allowing new\ninsights into disease progression. The model is estimated within a Bayesian\nframework and implemented in the R-package bamlss.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:40:01 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 11:11:53 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["K\u00f6hler", "Meike", ""], ["Umlauf", "Nikolaus", ""], ["Beyerlein", "Andreas", ""], ["Winkler", "Christiane", ""], ["Ziegler", "Anette-Gabriele", ""], ["Greven", "Sonja", ""]]}, {"id": "1611.01535", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Parsimony, model adequacy and periodic correlation in forecasting time\n  series", "comments": "19 pages, 2 tables", "journal-ref": "International Statistical Review 61/3, 387-393 (1993)", "doi": "10.2307/1403750", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The merits of the modelling philosophy of Box \\& Jenkins (1970) are\nillustrated with a summary of our recent work on seasonal river flow\nforecasting. Specifically, this work demonstrates that the principle of\nparsimony, which has been questioned by several authors recently, is helpful in\nselecting the best model for forecasting seasonal river flow. Our work also\ndemonstrates the importance of model adequacy. An adequate model for seasonal\nriver flow must incorporate seasonal periodic correlation. The usual\nautoregressive-moving average (ARMA) and seasonal ARMA models are not adequate\nin this respect for seasonal river flow time series. A new diagnostic check,\nfor detecting periodic correlation in fitted ARMA models is developed in this\npaper. This diagnostic check is recommended for routine use when fitting\nseasonal ARMA models. It is shown that this diagnostic check indicates that\nmany seasonal economic time series also exhibit periodic correlation. Since the\nstandard forecasting methods are inadequate on this account, it can be\nconcluded that in many cases, the forecasts produced are sub-optimal. Finally,\na limitation of the arbitrary combination of forecasts is also illustrated.\nCombining forecasts from an adequate parsimonious model with an inadequate\nmodel did not improve the forecasts whereas combining the two forecasts of two\ninadequate models did yield an improvement in forecasting performance. These\nfindings also support the model building philosophy of Box \\& Jenkins. The\nnon-intuitive findings of Newbold \\& Granger (1974) and Winkler \\& Makridakis\n(1983) that the apparent arbitrary combination of forecasts from similar models\nwill lead to forecasting performance is not supported by our case study with\nriver flow forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:00:48 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01831", "submitter": "Md Nazmul Islam", "authors": "Md Nazmul Islam, Ana-Maria Staicu, and Eric van Heugten", "title": "Longitudinal Dynamic Functional Regression", "comments": "27 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This article develops flexible methodology to study the association between\nscalar outcomes and functional predictors observed over time, at many\ninstances, in longitudinal studies. We propose a parsimonious modeling\nframework to study time-varying regression that leads to superior prediction\nproperties and allows to reconstruct full trajectories of the response. The\nidea is to model the time-varying functional predictors using orthogonal basis\nfunctions and expand the time-varying regression coefficient using the same\nbasis. Numerical investigation through simulation studies and data analysis\nshow excellent performance in terms of accurate prediction and efficient\ncomputations, when compared with existing alternatives. The methods are\ninspired and applied to an animal science application, where of interest is to\nstudy the association between the feed intake of lactating sows and the\nminute-by-minute {temperature} throughout the 21st days of their lactation\nperiod. R code and an R illustration are provided at\nhttp://www4.stat.ncsu.edu/~staicu/software\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 19:41:32 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 05:31:53 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Islam", "Md Nazmul", ""], ["Staicu", "Ana-Maria", ""], ["van Heugten", "Eric", ""]]}, {"id": "1611.02157", "submitter": "Jan Korbel", "authors": "Mehmet Niyazi \\c{C}ankaya and Jan Korbel", "title": "On Statistical Properties of Jizba-Arimitsu Hybrid Entropy", "comments": null, "journal-ref": "Physica A 475, 2017, 1-10", "doi": "10.1016/j.physa.2017.02.009", "report-no": null, "categories": "cond-mat.stat-mech math-ph math.MP stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Jizba-Arimitsu entropy (also called hybrid entropy) combines axiomatics of\nR\\'enyi and Tsallis entropy. It has many common properties with them, on the\nother hand, some aspects as e.g., MaxEnt distributions, are completely\ndifferent from the former two entropies. In this paper, we demonstrate the\nstatistical properties of hybrid entropy, including the definition of hybrid\nentropy for continuous distributions, its relation to discrete entropy and\ncalculation of hybrid entropy for some well-known distributions. Additionally,\ndefinition of hybrid divergence and its connection to Fisher metric is also\ndiscussed. Interestingly, the main properties of continuous hybrid entropy and\nhybrid divergence are completely different from measures based on R\\'enyi and\nTsallis entropy. This motivates us to introduce average hybrid entropy, which\ncan be understood as an average between Tsallis and R\\'enyi entropy.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 06:43:16 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["\u00c7ankaya", "Mehmet Niyazi", ""], ["Korbel", "Jan", ""]]}, {"id": "1611.02403", "submitter": "Arjun Gopalaswamy", "authors": "Arjun M. Gopalaswamy and Mohan Delampady", "title": "Examining posterior propriety in the Bayesian analysis of\n  capture-recapture models", "comments": "16 pages, including Appendices, Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There lies a latent danger in utilizing some known mathematical results in\necology. Some results do not apply to the problem at hand. We identify one such\ntrend. Based on a couple of theorems in mathematical statistics, Link (2013)\ncautions ecologists about the inappropriateness of using the discrete uniform\nprior in their analysis under certain conditions and instead recommends the\nroutine use of the scale prior during analysis. This recommendation is been\nabsorbed immediately and widely among ecologists. In this study, we consider\nthe two fundamental capture-recapture models used widely in ecology, $M_0$ and\n$M_h$, and derive conditions for posterior propriety by examining the behavior\nof the right tail of the posterior distributions of animal population size $N$\nin a Bayesian analysis. We demonstrate that both these likelihoods are far more\nefficient than the ones considered in Link (2013). We argue that no\nparticularly prescriptive approach should be adopted by ecologists in regard to\nchoosing priors of the fear of posterior impropriety. Instead, we recommend the\nefficient construction of likelihoods for the problem and data on hand,\nchoosing priors based existing knowledge of a parameter of interest and\nencourage examining posterior propriety by asymptotic arguments as demonstrated\nin this study.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 06:24:28 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Gopalaswamy", "Arjun M.", ""], ["Delampady", "Mohan", ""]]}, {"id": "1611.02530", "submitter": "Daniel Rockmore", "authors": "Daryl R. DeFord, Daniel N. Rockmore", "title": "A Random Dot Product Model for Weighted Networks", "comments": "35 pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a generalization of the random dot product model for\nnetworks whose edge weights are drawn from a parametrized probability\ndistribution. We focus on the case of integer weight edges and show that many\npreviously studied models can be recovered as special cases of this\ngeneralization. Our model also determines a dimension--reducing embedding\nprocess that gives geometric interpretations of community structure and\ncentrality. The dimension of the embedding has consequences for the derived\ncommunity structure and we exhibit a stress function for determining\nappropriate dimensions. We use this approach to analyze a coauthorship network\nand voting data from the U.S. Senate.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 14:40:15 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["DeFord", "Daryl R.", ""], ["Rockmore", "Daniel N.", ""]]}, {"id": "1611.02818", "submitter": "Stephen Wu", "authors": "Stephen Wu, Panagiotis Angelikopoulos, James L. Beck, Petros\n  Koumoutsakos", "title": "Hierarchical Stochastic Model in Bayesian Inference: Theoretical\n  Implications and Efficient Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify two types of Hierarchical Bayesian Model found in the literature\nas Hierarchical Prior Model (HPM) and Hierarchical Stochastic Model (HSM).\nThen, we focus on studying the theoretical implications of the HSM. Using\nexamples of polynomial functions, we show that the HSM is capable of separating\ndifferent types of uncertainties in a system and quantifying uncertainty of\nreduced order models under the Bayesian model class selection framework. To\ntackle the huge computational cost for analyzing HSM, we propose an efficient\napproximation scheme based on Importance Sampling and Empirical Interpolation\nMethod. We illustrate our method using two examples - a Molecular Dynamics\nsimulation for Krypton and a pharmacokinetic/pharmacodynamic model for cancer\ndrug.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 05:09:29 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Wu", "Stephen", ""], ["Angelikopoulos", "Panagiotis", ""], ["Beck", "James L.", ""], ["Koumoutsakos", "Petros", ""]]}, {"id": "1611.02861", "submitter": "Moritz Kohls", "authors": "Moritz Kohls and Tanja Hernandez", "title": "Expected Coverage of Random Walk Mobility Algorithm", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unmanned aerial vehicles (UAVs) have been increasingly used for exploring\nareas. Many mobility algorithms were designed to achieve a fast coverage of a\ngiven area. We focus on analysing the expected coverage of the symmetric random\nwalk mobility algorithm with independent mobility. Therefore we proof the\ndependence of certain events and develop Markov models, in order to provide an\nanalytical solution for the expected coverage. The analytic solution is\nafterwards compared to those of another work and to simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:10:20 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 11:49:39 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Kohls", "Moritz", ""], ["Hernandez", "Tanja", ""]]}, {"id": "1611.02869", "submitter": "Jens Sj\\\"olund", "authors": "Jens Sj\\\"olund, Anders Eklund, Evren \\\"Ozarslan and Hans Knutsson", "title": "Gaussian process regression can turn non-uniform and undersampled\n  diffusion MRI data into diffusion spectrum imaging", "comments": "5 pages", "journal-ref": "2017 IEEE 14th International Symposium on Biomedical Imaging (ISBI\n  2017)", "doi": "10.1109/ISBI.2017.7950634", "report-no": null, "categories": "stat.AP cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use Gaussian process regression to accurately estimate the\ndiffusion MRI signal at arbitrary locations in q-space. By estimating the\nsignal on a grid, we can do synthetic diffusion spectrum imaging:\nreconstructing the ensemble averaged propagator (EAP) by an inverse Fourier\ntransform. We also propose an alternative reconstruction method guaranteeing a\nnonnegative EAP that integrates to unity. The reconstruction is validated on\ndata simulated from two Gaussians at various crossing angles. Moreover, we\ndemonstrate on non-uniformly sampled in vivo data that the method is far\nsuperior to linear interpolation, and allows a drastic undersampling of the\ndata with only a minor loss of accuracy. We envision the method as a potential\nreplacement for standard diffusion spectrum imaging, in particular when\nacquistion time is limited.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 09:54:47 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Sj\u00f6lund", "Jens", ""], ["Eklund", "Anders", ""], ["\u00d6zarslan", "Evren", ""], ["Knutsson", "Hans", ""]]}, {"id": "1611.03015", "submitter": "Andrii Babii", "authors": "Andrii Babii", "title": "Honest Confidence Sets in Nonparametric IV Regression and Other\n  Ill-Posed Models", "comments": null, "journal-ref": "Econometric Theory , 36(4), 2020, pp. 658-706", "doi": "10.1017/S0266466619000380", "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops inferential methods for a very general class of ill-posed\nmodels in econometrics encompassing the nonparametric instrumental variable\nregression, various functional regressions, and the density deconvolution. We\nfocus on uniform confidence sets for the parameter of interest estimated with\nTikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).\nSince it is impossible to have inferential methods based on the central limit\ntheorem, we develop two alternative approaches relying on the concentration\ninequality and bootstrap approximations. We show that expected diameters and\ncoverage properties of resulting sets have uniform validity over a large class\nof models, i.e., constructed confidence sets are honest. Monte Carlo\nexperiments illustrate that introduced confidence sets have reasonable width\nand coverage properties. Using U.S. data, we provide uniform confidence sets\nfor Engel curves for various commodities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 17:10:31 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 22:50:56 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 03:28:41 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 19:59:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""]]}, {"id": "1611.03021", "submitter": "Yu-Xiang Wang", "authors": "Ziqi Liu, Alexander J. Smola, Kyle Soska, Yu-Xiang Wang, Qinghua\n  Zheng, Jun Zhou", "title": "Attributing Hacks", "comments": "Appeared at AISTATS'17. Full version under review at the Electronic\n  Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an algorithm for estimating the provenance of hacks\non websites. That is, given properties of sites and the temporal occurrence of\nattacks, we are able to attribute individual attacks to joint causes and\nvulnerabilities, as well as estimating the evolution of these vulnerabilities\nover time. Specifically, we use hazard regression with a time-varying additive\nhazard function parameterized in a generalized linear form. The activation\ncoefficients on each feature are continuous-time functions over time. We\nformulate the problem of learning these functions as a constrained variational\nmaximum likelihood estimation problem with total variation penalty and show\nthat the optimal solution is a 0th order spline (a piecewise constant function)\nwith a finite number of known knots. This allows the inference problem to be\nsolved efficiently and at scale by solving a finite dimensional optimization\nproblem. Extensive experiments on real data sets show that our method\nsignificantly outperforms Cox's proportional hazard model. We also conduct a\ncase study and verify that the fitted functions are indeed recovering\nvulnerable features and real-life events such as the release of code to exploit\nthese features in hacker blogs.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 15:26:58 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 18:25:34 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Liu", "Ziqi", ""], ["Smola", "Alexander J.", ""], ["Soska", "Kyle", ""], ["Wang", "Yu-Xiang", ""], ["Zheng", "Qinghua", ""], ["Zhou", "Jun", ""]]}, {"id": "1611.03193", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Zhizhen Zhao, Amit Singer", "title": "Mahalanobis Distance for Class Averaging of Cryo-EM Images", "comments": "Final version accepted to the 14th IEEE International Symposium on\n  Biomedical Imaging (ISBI 2017)", "journal-ref": null, "doi": "10.1109/ISBI.2017.7950605", "report-no": null, "categories": "stat.AP cs.CV q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single particle reconstruction (SPR) from cryo-electron microscopy (EM) is a\ntechnique in which the 3D structure of a molecule needs to be determined from\nits contrast transfer function (CTF) affected, noisy 2D projection images taken\nat unknown viewing directions. One of the main challenges in cryo-EM is the\ntypically low signal to noise ratio (SNR) of the acquired images. 2D\nclassification of images, followed by class averaging, improves the SNR of the\nresulting averages, and is used for selecting particles from micrographs and\nfor inspecting the particle images. We introduce a new affinity measure, akin\nto the Mahalanobis distance, to compare cryo-EM images belonging to different\ndefocus groups. The new similarity measure is employed to detect similar\nimages, thereby leading to an improved algorithm for class averaging. We\nevaluate the performance of the proposed class averaging procedure on synthetic\ndatasets, obtaining state of the art classification.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 05:55:27 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2016 15:11:24 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 15:56:31 GMT"}, {"version": "v4", "created": "Wed, 25 Jan 2017 03:40:18 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhao", "Zhizhen", ""], ["Singer", "Amit", ""]]}, {"id": "1611.03219", "submitter": "Peiman Asadi", "authors": "Peiman Asadi, Sebastian Engelke, Anthony C. Davison", "title": "Statistical regionalization for estimation of extreme river discharges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regionalization methods have long been used to estimate high return levels of\nriver discharges at ungauged locations on a river network. In these methods,\nthe recorded discharge measurements of a group of similar, gauged, stations is\nused to estimate high quantiles at the target catchment that has no\nobservations. This group is called the region of influence and its similarity\nto the ungauged location is measured in terms of physical and meteorological\ncatchment attributes. We develop a statistical method for estimation of high\nreturn levels based on regionalizing the parameters of a generalized extreme\nvalue distribution. The region of influence is chosen in an optimal way,\nensuring similarity and in-group homogeneity. Our method is applied to\ndischarge data from the Rhine basin in Switzerland, and its performance at\nungauged locations is compared to that of classical regionalization methods.\nFor gauged locations we show how our approach improves the estimation\nuncertainty for long return periods by combining local measurements with those\nfrom the region of influence.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 08:45:56 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Asadi", "Peiman", ""], ["Engelke", "Sebastian", ""], ["Davison", "Anthony C.", ""]]}, {"id": "1611.03320", "submitter": "Santosh Yadav", "authors": "Santosh Kumar Yadav, Rohit Sinha, Prabin Kumar Bora", "title": "Electrocardiogram signal denoising using non-local wavelet transform\n  domain filtering", "comments": null, "journal-ref": "IET Signal Processing, vol. 9, no. 1, pp. 88-96, 2 (2015)", "doi": "10.1049/iet-spr.2014.0005", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ECG signals are usually corrupted by baseline wander, power-line\ninterference, muscle noise, etc. and numerous methods have been proposed to\nremove these noises. However, in case of wireless recording of the ECG signal\nit gets corrupted by the additive white Gaussian noise (AWGN). For the correct\ndiagnosis, removal of AWGN from ECG signals becomes necessary as it affects the\nall the diagnostic features. The natural signals exhibit correlation among\ntheir samples and this property has been exploited in various signal\nrestoration tasks. Motivated by that, in this work we propose a nonlocal\nwavelet transform domain ECG signal denoising method which exploits the\ncorrelations among both local and nonlocal samples of the signal. In the\nproposed method, the similar blocks of the samples are grouped in a matrix and\nthen denoising is achieved by the shrinkage of its two-dimensional discrete\nwavelet transform coefficients. The experiments performed on a number of ECG\nsignals show significant quantitative and qualitative improvement in denoising\nperformance over the existing ECG signal denoising methods.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 10:09:23 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Yadav", "Santosh Kumar", ""], ["Sinha", "Rohit", ""], ["Bora", "Prabin Kumar", ""]]}, {"id": "1611.03328", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Distributed Estimation and Learning over Heterogeneous Networks", "comments": "In Proceedings of the 53rd Annual Allerton Conference on\n  Communication, Control, and Computing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI cs.SY math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several estimation and learning problems that networked agents\nface when making decisions given their uncertainty about an unknown variable.\nOur methods are designed to efficiently deal with heterogeneity in both size\nand quality of the observed data, as well as heterogeneity over time\n(intermittence). The goal of the studied aggregation schemes is to efficiently\ncombine the observed data that is spread over time and across several network\nnodes, accounting for all the network heterogeneities. Moreover, we require no\nform of coordination beyond the local neighborhood of every network agent or\nsensor node. The three problems that we consider are (i) maximum likelihood\nestimation of the unknown given initial data sets, (ii) learning the true model\nparameter from streams of data that the agents receive intermittently over\ntime, and (iii) minimum variance estimation of a complete sufficient statistic\nfrom several data points that the networked agents collect over time. In each\ncase we rely on an aggregation scheme to combine the observations of all\nagents; moreover, when the agents receive streams of data over time, we modify\nthe update rules to accommodate the most recent observations. In every case, we\ndemonstrate the efficiency of our algorithms by proving convergence to the\nglobally efficient estimators given the observations of all agents. We\nsupplement these results by investigating the rate of convergence and providing\nfinite-time performance guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:05:34 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.03404", "submitter": "Jeffrey Regier", "authors": "Jeffrey Regier, Kiran Pamnany, Ryan Giordano, Rollin Thomas, David\n  Schlegel, Jon McAuliffe and Prabhat", "title": "Learning an Astronomical Catalog of the Visible Universe through\n  Scalable Bayesian Inference", "comments": "submitting to IPDPS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC astro-ph.IM cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Celeste is a procedure for inferring astronomical catalogs that attains\nstate-of-the-art scientific results. To date, Celeste has been scaled to at\nmost hundreds of megabytes of astronomical images: Bayesian posterior inference\nis notoriously demanding computationally. In this paper, we report on a\nscalable, parallel version of Celeste, suitable for learning catalogs from\nmodern large-scale astronomical datasets. Our algorithmic innovations include a\nfast numerical optimization routine for Bayesian posterior inference and a\nstatistically efficient scheme for decomposing astronomical optimization\nproblems into subproblems.\n  Our scalable implementation is written entirely in Julia, a new high-level\ndynamic programming language designed for scientific and numerical computing.\nWe use Julia's high-level constructs for shared and distributed memory\nparallelism, and demonstrate effective load balancing and efficient scaling on\nup to 8192 Xeon cores on the NERSC Cori supercomputer.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:16:04 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Regier", "Jeffrey", ""], ["Pamnany", "Kiran", ""], ["Giordano", "Ryan", ""], ["Thomas", "Rollin", ""], ["Schlegel", "David", ""], ["McAuliffe", "Jon", ""], ["Prabhat", "", ""]]}, {"id": "1611.03439", "submitter": "Wenge Guo", "authors": "Zhiying Qiu, Wenge Guo, Sanat Sarkar", "title": "Bonferroni-based gatekeeping procedure with retesting option", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex clinical trials, multiple research objectives are often grouped\ninto sets of objectives based on their inherent hierarchical relationships.\nConsequently, the hypotheses formulated to address these objectives are grouped\ninto ordered families of hypotheses and thus to be tested in a pre-defined\nsequence. In this paper, we introduce a novel Bonferroni based multiple testing\nprocedure for testing hierarchically ordered families of hypotheses. The\nproposed procedure allows the families to be sequentially tested more than once\nwith updated local critical values. It is proved to control the global\nfamilywise error rate strongly under arbitrary dependence. Implementation of\nthe procedure is illustrated using two examples. Finally, the procedure is\nextended to testing multiple families of hypotheses with a complex two-layer\nhierarchical structure.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 18:31:03 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Qiu", "Zhiying", ""], ["Guo", "Wenge", ""], ["Sarkar", "Sanat", ""]]}, {"id": "1611.03646", "submitter": "Ladislav Kristoufek", "authors": "Ladislav Kristoufek", "title": "Has global warming modified the relationship between sunspot numbers and\n  global temperatures?", "comments": "16 pages, 7 figures", "journal-ref": "Physica A, Volume 468, 15 February 2017, Pages 351-358", "doi": "10.1016/j.physa.2016.10.089", "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study time evolution of the relationship between sunspot numbers and\nglobal temperatures between 1880 and 2016 using wavelet coherence framework.\nThe results suggest that the relationship is stable in time. Changes in the\nsunspot numbers precede changes in the temperatures by more than two years as\nsuggested by the wavelet phase differences. This leading position of the sun\nactivity is stable in time as well. However, the relationship has been\ndisturbed by increasing $CO_2$ emissions since 1960s. Without controlling for\nthe effect of possible global warming, or more precisely the positive\nconnection between increasing $CO_2$ emissions and the global temperatures, the\nfindings would have been quite different. Combination of the cointegration\nanalysis and wavelet coherence framework has enabled uncovering a hidden\nrelationship between the solar activity and global temperatures, and possibly\nexplaining equivocal results in the topical literature.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 10:17:33 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kristoufek", "Ladislav", ""]]}, {"id": "1611.03787", "submitter": "Danica J. Sutherland", "authors": "Seth Flaxman and Danica J. Sutherland and Yu-Xiang Wang and Yee Whye\n  Teh", "title": "Understanding the 2016 US Presidential Election using ecological\n  inference and distribution regression with census microdata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine fine-grained spatially referenced census data with the vote\noutcomes from the 2016 US presidential election. Using this dataset, we perform\necological inference using distribution regression (Flaxman et al, KDD 2015)\nwith a multinomial-logit regression so as to model the vote outcome Trump,\nClinton, Other / Didn't vote as a function of demographic and socioeconomic\nfeatures. Ecological inference allows us to estimate \"exit poll\" style results\nlike what was Trump's support among white women, but for entirely novel\ncategories. We also perform exploratory data analysis to understand which\ncensus variables are predictive of voting for Trump, voting for Clinton, or not\nvoting for either. All of our methods are implemented in Python and R, and are\navailable online for replication.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 17:17:07 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 06:58:49 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Flaxman", "Seth", ""], ["Sutherland", "Danica J.", ""], ["Wang", "Yu-Xiang", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.03899", "submitter": "Richard Mann", "authors": "Richard P. Mann and Dirk Helbing", "title": "Optimal incentives for collective intelligence", "comments": null, "journal-ref": "PNAS 2017 114 (20) 5077-5082", "doi": "10.1073/pnas.1618722114", "report-no": null, "categories": "cs.GT math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective intelligence is the ability of a group to perform more effectively\nthan any individual alone. Diversity among group members is a key condition for\nthe emergence of collective intelligence, but maintaining diversity is\nchallenging in the face of social pressure to imitate one's peers. We\ninvestigate the role incentives play in maintaining useful diversity through an\nevolutionary game-theoretic model of collective prediction. We show that\nmarket-based incentive systems produce herding effects, reduce information\navailable to the group and suppress collective intelligence. In response, we\npropose a new incentive scheme that rewards accurate minority predictions, and\nshow that this produces optimal diversity and collective predictive accuracy.\nWe conclude that real-world systems should reward those who have demonstrated\naccuracy when majority opinion has been in error.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 22:21:24 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 12:18:41 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Mann", "Richard P.", ""], ["Helbing", "Dirk", ""]]}, {"id": "1611.04026", "submitter": "Laura Tupper", "authors": "Laura L. Tupper, David S. Matteson, John C. Handley", "title": "Mixed Data and Classification of Transit Stops", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analysis of the characteristics and behavior of individual bus stops can\nreveal clusters of similar stops, which can be of use in making routing and\nscheduling decisions, as well as determining what facilities to provide at each\nstop. This paper provides an exploratory analysis, including several possible\nclustering results, of a dataset provided by the Regional Transit Service of\nRochester, NY. The dataset describes ridership on public buses, recording the\ntime, location, and number of entering and exiting passengers each time a bus\nstops. A description of the overall behavior of bus ridership is followed by a\nstop-level analysis. We compare multiple measures of stop similarity, based on\nlocation, route information, and ridership volume over time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:53:38 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tupper", "Laura L.", ""], ["Matteson", "David S.", ""], ["Handley", "John C.", ""]]}, {"id": "1611.04134", "submitter": "Joris Heyman", "authors": "J. Heyman and F. Mettra and H.B. Ma and C. Ancey", "title": "Statistics of bedload transport over steep slopes: Separation of time\n  scales and collective motion", "comments": null, "journal-ref": "Geophysical Research Letters 40(1), 128-133, 2013", "doi": "10.1029/2012GL054280", "report-no": null, "categories": "physics.geo-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Steep slope streams show large fluctuations of sediment discharge across\nseveral time scales. These fluctuations may be inherent to the internal\ndynamics of the sediment transport process. A probabilistic framework thus\nseems appropriate to analyze such a process. In this letter, we present an\nexperimental study of bedload transport over a steep slope flume for small to\nmoderate Shields numbers. The sampling technique allows the acquisition of\nhigh-resolution time series of the solid discharge. The resolved time scales\nrange from $10^{-2}$s up to $10^{5}$s. We show that two distinct time scales\ncan be observed in the probability density function for the waiting time\nbetween moving particles. We make the point that the separation of time scales\nis related to collective dynamics. Proper statistics of a Markov process\nincluding collective entrainment are derived. The separation of time scales is\nrecovered theoretically for low entrainment rates.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 13:27:36 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Heyman", "J.", ""], ["Mettra", "F.", ""], ["Ma", "H. B.", ""], ["Ancey", "C.", ""]]}, {"id": "1611.04248", "submitter": "Jianfei Shen", "authors": "Jianfei Shen and Tianxiao Pang", "title": "Asymptotic Inference for AR(1) Penal Data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general asymptotic theory is given for the panel data AR(1) model with time\nseries independent in different cross sections. The theory covers the cases of\nstationary process, nearly non-stationary process, unit root process, mildly\nintegrated, mildly explosive and explosive processes. It is assumed that the\ncross-sectional dimension and time-series dimension are respectively $N$ and\n$T$. The results in this paper illustrate that whichever the process is, with\nan appropriate regularization, the least squares estimator of the\nautoregressive coefficient converges to a normal distribution with rate at\nleast $O(N^{-1/3})$. Since the variance is the key to characterize the normal\ndistribution, it is important to discuss the variance of the least squares\nestimator. We will show that when the autoregressive coefficient $\\rho$\nsatisfies $|\\rho|<1$, the variance declines at the rate $O((NT)^{-1/2})$, while\nthe rate changes to $O(N^{-1/2}T^{-1})$ when $\\rho=1$ and\n$O(N^{-1/2}\\rho^{-T+2})$ when $|\\rho|>1$. $\\rho=1$ is the critical point where\nthe convergence rate changes radically. The transition process is studied by\nassuming $\\rho$ depending on $T$ and going to $1$. An interesting phenomenon\ndiscovered in this paper is that, in the explosive case, the least squares\nestimator of the autoregressive coefficient has a standard normal limiting\ndistribution in panel data case while it may not has a limiting distribution in\nunivariate time series case.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 04:26:56 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Shen", "Jianfei", ""], ["Pang", "Tianxiao", ""]]}, {"id": "1611.04330", "submitter": "Eva Riccomagno", "authors": "Eleonora Saggini, Eva Riccomagno, Massimo Caccia, Henry P.Wynn", "title": "Adaptive Experimental Design for Path-following Performance Assessment\n  of Unmanned Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The definition of Good Experimental Methodologies (GEMs) in robotics is a\ntopic of widespread interest due also to the increasing employment of robots in\neveryday civilian life. The present work contributes to the ongoing discussion\non GEMs for Unmanned Surface Vehicles (USVs). It focuses on the definition of\nGEMs and provides specific guidelines for path-following experiments.\nStatistically designed experiments (DoE) offer a valid basis for developing an\nempirical model of the system being investigated. A two-step adaptive\nexperimental procedure for evaluating path-following performance and based on\nDoE, is tested on the simulator of the Charlie USV. The paper argues the\nnecessity of performing extensive simulations prior to the execution of field\ntrials.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 10:45:09 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Saggini", "Eleonora", ""], ["Riccomagno", "Eva", ""], ["Caccia", "Massimo", ""], ["Wynn", "Henry P.", ""]]}, {"id": "1611.04393", "submitter": "Amanmeet Garg", "authors": "Amanmeet Garg, Donghuan Lu, Karteek Popuri and Mirza Faisal Beg", "title": "Cortical Geometry Network and Topology Markers for Parkinson's Disease", "comments": "Presented at The MICCAI-BACON 16 Workshop (arXiv:1611.03363) Report\n  number: BACON/2016/02", "journal-ref": null, "doi": null, "report-no": "BACON/2016/02", "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neurodegeneration affects cortical gray matter leading to loss of cortical\nmantle volume. As a result of such volume loss, the geometrical arrangement of\nthe regions on the cortical surface is expected to be altered in comparison to\nhealthy brains. Here we present a novel method to study the alterations in\nbrain cortical surface geometry in Parkinson's disease (PD) subjects with a\n\\emph{Geometry Networks (GN)} framework. The local geometrical arrangement of\nthe cortical surface is captured as the 3D coordinates of the centroids of\nanatomically defined parcels on the surface. The inter-regional distance\nbetween cortical patches is the signal of interest and is captured as a\ngeometry network. We study its topology by computing the dimensionality of\nsimplicial complexes induced on a filtration of binary undirected networks for\neach geometry network. In a permutation statistics test, a statistically\nsignificant ($p<0.05$) difference was observed in the homology features between\nPD and healthy control groups highlighting its potential to differentiate\nbetween the groups and their potential utility in disease diagnosis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 14:33:47 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Garg", "Amanmeet", ""], ["Lu", "Donghuan", ""], ["Popuri", "Karteek", ""], ["Beg", "Mirza Faisal", ""]]}, {"id": "1611.04411", "submitter": "Renaud Tissier", "authors": "Renaud Tissier, Roula Tsonaka, Simon P. Mooijaart, Eline Slagboom,\n  Jeanine J. Houwing-Duistermaat", "title": "Secondary Phenotype Analysis in Ascertained Family Designs: Application\n  to the Leiden Longevity Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The case-control design is often used to test associations between the\ncase-control status and genetic variants. In addition to this primary phenotype\na number of additional traits, known as secondary phenotypes, are routinely\nrecorded and typically associations between genetic factors and these secondary\ntraits are studied too. Analysing secondary phenotypes in case-control studies\nmay lead to biased genetic effect estimates, especially when the marker tested\nis associated with the primary phenotype and when the primary and secondary\nphenotypes tested are correlated. Several methods have been proposed in the\nliterature to overcome the problem but they are limited to case-control studies\nand not directly applicable to more complex designs, such as the multiple-cases\nfamily studies. A proper secondary phenotype analysis, in this case, is\ncomplicated by the within families correlations on top of the biased sampling\ndesign. We propose a novel approach to accommodate the ascertainment process\nwhile explicitly modelling the familial relationships. Our approach pairs\nexisting methods for mixed-effects models with the retrospective likelihood\nframework and uses a multivariate probit model to capture the association\nbetween the mixed type primary and secondary phenotypes. To examine the\nefficiency and bias of the estimates we performed simulations under several\nscenarios for the association between the primary phenotype, secondary\nphenotype, and genetic markers. We will illustrate the method by analysing the\nassociation between triglyceride levels and glucose (secondary phenotypes) and\ngenetic markers from the Leiden Longevity study, a multiple-cases family study\nthat investigates longevity.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 15:14:45 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Tissier", "Renaud", ""], ["Tsonaka", "Roula", ""], ["Mooijaart", "Simon P.", ""], ["Slagboom", "Eline", ""], ["Houwing-Duistermaat", "Jeanine J.", ""]]}, {"id": "1611.04537", "submitter": "Frank Werner", "authors": "Katharina Proksch, Frank Werner, Axel Munk", "title": "Multiscale scanning in inverse problems", "comments": "55 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multiscale scanning method to determine active\ncomponents of a quantity $f$ w.r.t. a dictionary $\\mathcal{U}$ from\nobservations $Y$ in an inverse regression model $Y=Tf+\\xi$ with linear operator\n$T$ and general random error $\\xi$. To this end, we provide uniform confidence\nstatements for the coefficients $\\langle \\varphi, f\\rangle$, $\\varphi \\in\n\\mathcal U$, under the assumption that $(T^*)^{-1} \\left(\\mathcal U\\right)$ is\nof wavelet-type. Based on this we obtain a multiple test that allows to\nidentify the active components of $\\mathcal{U}$, i.e. $\\left\\langle f,\n\\varphi\\right\\rangle \\neq 0$, $\\varphi \\in \\mathcal U$, at controlled,\nfamily-wise error rate. Our results rely on a Gaussian approximation of the\nunderlying multiscale statistic with a novel scale penalty adapted to the\nill-posedness of the problem. The scale penalty furthermore ensures weak\nconvergence of the statistic's distribution towards a Gumbel limit under\nreasonable assumptions. The important special cases of tomography and\ndeconvolution are discussed in detail. Further, the regression case, when $T =\n\\text{id}$ and the dictionary consists of moving windows of various sizes\n(scales), is included, generalizing previous results for this setting. We show\nthat our method obeys an oracle optimality, i.e. it attains the same asymptotic\npower as a single-scale testing procedure at the correct scale. Simulations\nsupport our theory and we illustrate the potential of the method as an\ninferential tool for imaging. As a particular application we discuss\nsuper-resolution microscopy and analyze experimental STED data to locate single\nDNA origami.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:26:22 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:03:32 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Proksch", "Katharina", ""], ["Werner", "Frank", ""], ["Munk", "Axel", ""]]}, {"id": "1611.04660", "submitter": "Pranjul Yadav", "authors": "Pranjul Yadav, Lisiane Prunelli, Alexander Hoff, Michael Steinbach,\n  Bonnie Westra, Vipin Kumar, Gyorgy Simon", "title": "Causal Inference in Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aging population increasingly suffers from multiple chronic diseases\nsimultaneously, necessitating the comprehensive treatment of these conditions.\nFinding the optimal set of drugs for a combinatorial set of diseases is a\ncombinatorial pattern exploration problem. Association rule mining is a popular\ntool for such problems, but the requirement of health care for finding causal,\nrather than associative, patterns renders association rule mining unsuitable.\nTo address this issue, we propose a novel framework based on the Rubin-Neyman\ncausal model for extracting causal rules from observational data, correcting\nfor a number of common biases. Specifically, given a set of interventions and a\nset of items that define subpopulations (e.g., diseases), we wish to find all\nsubpopulations in which effective intervention combinations exist and in each\nsuch subpopulation, we wish to find all intervention combinations such that\ndropping any intervention from this combination will reduce the efficacy of the\ntreatment. A key aspect of our framework is the concept of closed intervention\nsets which extend the concept of quantifying the effect of a single\nintervention to a set of concurrent interventions. We also evaluated our causal\nrule mining framework on the Electronic Health Records (EHR) data of a large\ncohort of patients from Mayo Clinic and showed that the patterns we extracted\nare sufficiently rich to explain the controversial findings in the medical\nliterature regarding the effect of a class of cholesterol drugs on Type-II\nDiabetes Mellitus (T2DM).\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 00:59:21 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Yadav", "Pranjul", ""], ["Prunelli", "Lisiane", ""], ["Hoff", "Alexander", ""], ["Steinbach", "Michael", ""], ["Westra", "Bonnie", ""], ["Kumar", "Vipin", ""], ["Simon", "Gyorgy", ""]]}, {"id": "1611.04696", "submitter": "John Nagorski", "authors": "John Nagorski and Genevera I. Allen", "title": "Genomic Region Detection via Spatial Convex Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several modern genomic technologies, such as DNA-Methylation arrays, measure\nspatially registered probes that number in the hundreds of thousands across\nmultiplechromosomes. The measured probes are by themselves less interesting\nscientifically; instead scientists seek to discover biologically interpretable\ngenomic regions comprised of contiguous groups of probes which may act as\nbiomarkers of disease or serve as a dimension-reducing pre-processing step for\ndownstream analyses. In this paper, we introduce an unsupervised feature\nlearning technique which maps technological units (probes) to biological units\n(genomic regions) that are common across all subjects. We use ideas from fusion\npenalties and convex clustering to introduce a method for Spatial Convex\nClustering, or SpaCC. Our method is specifically tailored to detecting\nmulti-subject regions of methylation, but we also test our approach on the\nwell-studied problem of detecting segments of copy number variation. We\nformulate our method as a convex optimization problem, develop a massively\nparallelizable algorithm to find its solution, and introduce automated\napproaches for handling missing values and determining tuning parameters.\nThrough simulation studies based on real methylation and copy number variation\ndata, we show that SpaCC exhibits significant performance gains relative to\nexisting methods. Finally, we illustrate SpaCC's advantages as a pre-processing\ntechnique that reduces large-scale genomics data into a smaller number of\ngenomic regions through several cancer epigenetics case studies on subtype\ndiscovery, network estimation, and epigenetic-wide association.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 03:51:30 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Nagorski", "John", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1611.04704", "submitter": "RadhaKrishna Ganti", "authors": "Anjin Guo, Martin Haenggi, Radha Krishna Ganti", "title": "SIR Asymptotics in General Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the performance analyses of wireless networks, asymptotic quantities and\nproperties often pro- vide useful results and insights. The asymptotic analyses\nbecome especially important when complete analytical expressions of the\nperformance metrics of interest are not available, which is often the case if\none departs from very specific modeling assumptions. In this paper, we consider\nthe asymptotics of the SIR distribution in general wireless network models,\nincluding ad hoc and cellular networks, simple and non-simple point processes,\nand singular and bounded path loss models, for which, in most cases, finding\nanalytical expressions of the complete SIR distribution seems hopeless. We show\nthat the lower tails of the SIR distributions decay polynomially with the order\nsolely determined by the path loss exponent or the fading parameter, while the\nupper tails decay exponentially, with the exception of cellular networks with\nsingular path loss. In addition, we analyze the impact of the nearest\ninterferer on the asymptotic properties of the SIR distributions, and we\nformulate three crisp conjectures that -if true- determine the asymptotic\nbehavior in many cases based on the large-scale path loss properties of the\ndesired signal and/or nearest interferer only.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 04:45:12 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Guo", "Anjin", ""], ["Haenggi", "Martin", ""], ["Ganti", "Radha Krishna", ""]]}, {"id": "1611.05012", "submitter": "Yuting Ji", "authors": "Yuting Ji and Lang Tong", "title": "Multi-Area Interchange Scheduling under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of multi-area interchange scheduling under system uncertainty is\nconsidered. A new scheduling technique is proposed for a multi-proxy bus system\nbased on stochastic optimization that captures uncertainty in renewable\ngeneration and stochastic load. In particular, the proposed algorithm\niteratively optimizes the interface flows using a multidimensional demand and\nsupply functions. Optimality and convergence are guaranteed for both\nsynchronous and asynchronous scheduling under nominal assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 20:16:17 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Ji", "Yuting", ""], ["Tong", "Lang", ""]]}, {"id": "1611.05080", "submitter": "Hugo Gabriel Eyherabide Dr", "authors": "Hugo Gabriel Eyherabide", "title": "Neural stochastic codes, encoding and decoding", "comments": "The additional material and some of the theorems have been integrated\n  within the main results of the manuscript, and few typos have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.IT math.IT q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding brain function, constructing computational models and\nengineering neural prosthetics require assessing two problems, namely encoding\nand decoding, but their relation remains controversial. For decades, the\nencoding problem has been shown to provide insight into the decoding problem,\nfor example, by upper bounding the decoded information. However, here we show\nthat this need not be the case when studying response aspects beyond noise\ncorrelations, and trace back the actual causes of this major departure from\ntraditional views. To that end, we reformulate the encoding and decoding\nproblems from the observer or organism perspective. In addition, we study the\nrole of spike-time precision and response discrimination, among other response\naspects, using stochastic transformations of the neural responses, here called\nstochastic codes. Our results show that stochastic codes may cause different\ninformation losses when used to describe neural responses and when employed to\ntrain optimal decoders. Therefore, we conclude that response aspects beyond\nnoise correlations may play different roles in encoding and decoding. In\npractice, our results show for the first time that decoders constructed\nlow-quality descriptions of response aspects may operate optimally on\nhigh-quality descriptions and vice versa, thereby potentially yielding\nexperimental and computational savings, as well as new opportunities for\nsimplifying the design of computational brain models and neural prosthetics.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 22:26:50 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 11:19:12 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Eyherabide", "Hugo Gabriel", ""]]}, {"id": "1611.05117", "submitter": "Karthik Nagarajan", "authors": "Anantha K. Karthik and Rick S. Blum", "title": "Estimation Theory Based Robust Phase Offset Estimation in the Presence\n  of Delay Attacks", "comments": "30 pages, 4 figures, Journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of robust clock phase offset estimation for\nthe IEEE 1588 precision time protocol (PTP) in the presence of delay attacks.\nDelay attacks are one of the most effective cyber attacks in PTP, as they\ncannot be mitigated using typical security measures. In this paper, we consider\nthe case where the slave node can exchange synchronization messages with\nmultiple master nodes synchronized to the same clock. We first provide lower\nbounds on the best achievable performance for any phase offset estimation\nscheme in the presence of delay attacks. We then present a novel phase offset\nestimation scheme that employs the Expectation-Maximization algorithm for\ndetecting which of the master-slave communication links have been subject to\ndelay attacks. After discarding information from the links identified as\nattacked, which we show to be optimal, the optimal vector location parameter\nestimator is employed to estimate the phase offset of the slave node.\nSimulation results are presented to show that the proposed phase offset\nestimation scheme exhibits performance close to the lower bounds in a wide\nvariety of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 02:13:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Karthik", "Anantha K.", ""], ["Blum", "Rick S.", ""]]}, {"id": "1611.05288", "submitter": "Kathia Pinz\\'on Kath", "authors": "Kathia Pinz\\'on", "title": "Analysis of Price and Income Elasticities of Energy Demand in Ecuador: A\n  Dynamic OLS Approach", "comments": "33 pages, 4 figures, 8 tables", "journal-ref": "Pinz\\'on, K. (2017)", "doi": "10.1016/j.eap.2017.09.004", "report-no": "QUI-049942", "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Energy consumption in Ecuador has increased significantly during the last\ndecades, affecting negatively the financial position of the country since large\nenergy consumption subsidies are provided in its internal market and Ecuador is\nmostly a crude oil exporter and oil derivatives importer country. This research\nseeks to state the long run price and income elasticities of energy demand in\nEcuador, by analyzing information spanning the period from 1970 to 2015. A\ncointegration analysis and an estimation by using a Dynamic Ordinary Least\nSquares approach considering structural breaks is carried out. Results obtained\nare robust and suggest that in the long run energy demand in Ecuador is highly\nincome elastic, has no relationship with its price and has an almost unitary\nbut inverse relationship with the industrial production level. Conclusions and\neconomic policy suggestions are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 14:29:52 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Pinz\u00f3n", "Kathia", ""]]}, {"id": "1611.05289", "submitter": "Felipe Osorio", "authors": "Felipe Osorio, Ronny Vallejos, Francisco Cuevas", "title": "SpatialPack: Computing the Association Between Two Spatial Processes", "comments": "17 pages, 8 figures, R package SpatialPack\n  (https://cran.r-project.org/package=SpatialPack)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An R package SpatialPack that implements routines to compute point estimators\nand perform hypothesis testing of the spatial association between two\nstochastic sequences is introduced. These methods address the spatial\nassociation between two processes that have been observed over the same spatial\nlocations. We briefly review the methodologies for which the routines are\ndeveloped. The core routines have been implemented in C and linked to R to\nensure a reasonable computational speed. Three examples are presented to\nillustrate the use of the package with both simulated and real data. The\nparticular case of computing the association between two time series is also\nconsidered. Besides elementary plots and outputs we also provide a plot to\nvisualize the spatial correlation in all directions using a new graphical tool\ncalled codispersion map. The potential extensions of SpatialPack are also\ndiscussed.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 14:31:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Osorio", "Felipe", ""], ["Vallejos", "Ronny", ""], ["Cuevas", "Francisco", ""]]}, {"id": "1611.05302", "submitter": "Zeynep Baskurt", "authors": "Zeynep Baskurt and Lisa Strug", "title": "A composite likelihood ratio approach to the analysis of correlated\n  binary data in genetic association studies", "comments": "49 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function represents statistical evidence in the context of\ndata and a probability model. Considerable theory has demonstrated that\nevidence strength for different parameter values can be interpreted from the\nratio of likelihoods at different points on the likelihood curve. The\nlikelihood function can, however, be unknown or difficult to compute; e.g. for\ngenetic association studies with a binary outcome in large multi-generational\nfamilies. Composite likelihood is a convenient alternative to using the real\nlikelihood and here we show composite likelihoods have valid evidential\ninterpretation. We show that composite likelihoods, with a robust adjustment,\nhave two large sample performance properties that enable reliable evaluation of\nrelative evidence for different values on the likelihood curve: (1) The\ncomposite likelihood function will support the true value over the false value\nby an arbitrarily large factor; and (2) the probability of favouring a false\nvalue over a true value with high probability is small and bounded. Using an\nextensive simulation study, and in a genetic association analysis of reading\ndisability in large complex pedigrees, we show that the composite approach\nyields valid statistical inference. Results are compared to analyses using\ngeneralized estimating equations and show similar inference is obtained,\nalthough the composite approach results in a full likelihood solution that\nprovides additional complementary information.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:00:16 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Baskurt", "Zeynep", ""], ["Strug", "Lisa", ""]]}, {"id": "1611.05344", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "On composite likelihood in bivariate meta-analysis of diagnostic test\n  accuracy studies", "comments": "arXiv admin note: text overlap with arXiv:1502.07505", "journal-ref": "AStA Advances in Statistical Analysis, 2018, 102 (2), 211--227", "doi": "10.1007/s10182-017-0299-y", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The composite likelihood (CL) is amongst the computational methods used for\nestimation of the generalized linear mixed model (GLMM) in the context of\nbivariate meta-analysis of diagnostic test accuracy studies. Its advantage is\nthat the likelihood can be derived conveniently under the assumption of\nindependence between the random effects, but there has not been a clear\nanalysis of the merit or necessity of this method. For synthesis of diagnostic\ntest accuracy studies, a copula mixed model has been proposed in the\nbiostatistics literature. This general model includes the GLMM as a special\ncase and can also allow for flexible dependence modelling, different from\nassuming simple linear correlation structures, normality and tail independence\nin the joint tails. A maximum likelihood (ML) method, which is based on\nevaluating the bi-dimensional integrals of the likelihood with quadrature\nmethods has been proposed, and in fact it eases any computational difficulty\nthat might be caused by the double integral in the likelihood function. Both\nmethods are thoroughly examined with extensive simulations and illustrated with\ndata of a published meta-analysis. It is shown that the ML method has no\nnon-convergence issues or computational difficulties and at the same time\nallows estimation of the dependence between study-specific sensitivity and\nspecificity and thus prediction via summary receiver operating curves.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 16:22:48 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 09:54:12 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1611.05368", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Neural Style Representations and the Large-Scale Classification of\n  Artistic Style", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Proceedings of the Future Technologies Conference, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1611.05605", "submitter": "David Bartley", "authors": "David Bartley, James Slaven, and Martin Harper", "title": "Approximate Negative-Binomial Confidence Intervals: Asbestos Fiber\n  Counts", "comments": "Accepted for publication by Annals of Occupational Hygiene, Figure 4\n  corrected", "journal-ref": "Annals of Work Exposures and Health; 61: 237-247, 2017", "doi": "10.1093/annweh/wxw020", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The negative-binomial distribution is adopted for analyzing asbestos-fiber\ncounts so as to account for both the sampling errors in capturing only a finite\nnumber of fibers as well as the inevitable human variation in identifying and\ncounting sampled fibers. A simple approximation to this distribution is\ndeveloped for the derivation of quantiles and approximate confidence limits.\nThe success of the approximation depends critically on the use of the Stirling\nexpansion to sufficient order, on exact normalization of the approximating\ndistribution, on reasonable perturbation of quantities from the normal\ndistribution, and on accurately approximating sums by inverse-trapezoidal\nintegration. Accuracy of the approximation developed is checked through\nsimulation and also by comparison to traditional approximate confidence\nintervals in the specific case that the negative-binomial distribution\napproaches the Poisson distribution.\n  The resulting statistics are shown to relate directly to early research into\nthe accuracy of asbestos sampling and analysis. Uncertainty in estimating mean\nasbestos-fiber concentrations given only a single count is derived. Decision\nlimits (limits of detection (LOD)) and detection limits are considered for\ncontrolling false positive and negative detection assertions and are compared\nto traditional limits computed assuming normal distributions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 08:24:21 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 19:01:28 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Bartley", "David", ""], ["Slaven", "James", ""], ["Harper", "Martin", ""]]}, {"id": "1611.05778", "submitter": "Alessandro Bessi", "authors": "Alessandro Bessi", "title": "Towards the Modeling of Behavioral Trajectories of Users in Online\n  Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a methodology that allows to model behavioral\ntrajectories of users in online social media. First, we illustrate how to\nleverage the probabilistic framework provided by Hidden Markov Models (HMMs) to\nrepresent users by embedding the temporal sequences of actions they performed\nonline. We then derive a model-based distance between trained HMMs, and we use\nspectral clustering to find homogeneous clusters of users showing similar\nbehavioral trajectories. To provide platform-agnostic results, we apply the\nproposed approach to two different online social media --- i.e. Facebook and\nYouTube. We conclude discussing merits and limitations of our approach as well\nas future and promising research directions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:53:50 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 21:34:40 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Bessi", "Alessandro", ""]]}, {"id": "1611.05788", "submitter": "Jacob Abernethy", "authors": "Jacob Abernethy (University of Michigan), Cyrus Anderson (University\n  of Michigan), Alex Chojnacki (University of Michigan), Chengyu Dai\n  (University of Michigan), John Dryden (University of Michigan), Eric Schwartz\n  (University of Michigan), Wenbo Shen (University of Michigan), Jonathan\n  Stroud (University of Michigan), Laura Wendlandt (University of Michigan),\n  Sheng Yang (University of Michigan), Daniel Zhang (University of Michigan)", "title": "Data Science in Service of Performing Arts: Applying Machine Learning to\n  Predicting Audience Preferences", "comments": "Presented at the Data For Good Exchange 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing arts organizations aim to enrich their communities through the\narts. To do this, they strive to match their performance offerings to the taste\nof those communities. Success relies on understanding audience preference and\npredicting their behavior. Similar to most e-commerce or digital entertainment\nfirms, arts presenters need to recommend the right performance to the right\ncustomer at the right time. As part of the Michigan Data Science Team (MDST),\nwe partnered with the University Musical Society (UMS), a non-profit performing\narts presenter housed in the University of Michigan, Ann Arbor. We are\nproviding UMS with analysis and business intelligence, utilizing historical\nindividual-level sales data. We built a recommendation system based on\ncollaborative filtering, gaining insights into the artistic preferences of\ncustomers, along with the similarities between performances. To better\nunderstand audience behavior, we used statistical methods from customer-base\nanalysis. We characterized customer heterogeneity via segmentation, and we\nmodeled customer cohorts to understand and predict ticket purchasing patterns.\nFinally, we combined statistical modeling with natural language processing\n(NLP) to explore the impact of wording in program descriptions. These ongoing\nefforts provide a platform to launch targeted marketing campaigns, helping UMS\ncarry out its mission by allocating its resources more efficiently. Celebrating\nits 138th season, UMS is a 2014 recipient of the National Medal of Arts, and it\ncontinues to enrich communities by connecting world-renowned artists with\ndiverse audiences, especially students in their formative years. We aim to\ncontribute to that mission through data science and customer analytics.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 03:49:16 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Abernethy", "Jacob", "", "University of Michigan"], ["Anderson", "Cyrus", "", "University\n  of Michigan"], ["Chojnacki", "Alex", "", "University of Michigan"], ["Dai", "Chengyu", "", "University of Michigan"], ["Dryden", "John", "", "University of Michigan"], ["Schwartz", "Eric", "", "University of Michigan"], ["Shen", "Wenbo", "", "University of Michigan"], ["Stroud", "Jonathan", "", "University of Michigan"], ["Wendlandt", "Laura", "", "University of Michigan"], ["Yang", "Sheng", "", "University of Michigan"], ["Zhang", "Daniel", "", "University of Michigan"]]}, {"id": "1611.05977", "submitter": "Mostafa Rahmani", "authors": "Mostafa Rahmani, George Atia", "title": "Robust and Scalable Column/Row Sampling from Corrupted Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional sampling techniques fall short of drawing descriptive sketches\nof the data when the data is grossly corrupted as such corruptions break the\nlow rank structure required for them to perform satisfactorily. In this paper,\nwe present new sampling algorithms which can locate the informative columns in\npresence of severe data corruptions. In addition, we develop new scalable\nrandomized designs of the proposed algorithms. The proposed approach is\nsimultaneously robust to sparse corruption and outliers and substantially\noutperforms the state-of-the-art robust sampling algorithms as demonstrated by\nexperiments conducted using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 05:07:21 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Rahmani", "Mostafa", ""], ["Atia", "George", ""]]}, {"id": "1611.06010", "submitter": "Leopoldo Catania", "authors": "David Ardia, Kris Boudt and Leopoldo Catania", "title": "Value-at-Risk Prediction in R with the GAS Package", "comments": "10 pages 1 fig 2 tab", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GAS models have been recently proposed in time-series econometrics as\nvaluable tools for signal extraction and prediction. This paper details how\nfinancial risk managers can use GAS models for Value-at-Risk (VaR) prediction\nusing the novel GAS package for R. Details and code snippets for prediction,\ncomparison and backtesting with GAS models are presented. An empirical\napplication considering Dow Jones Index constituents investigates the VaR\nforecasting performance of GAS models.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 08:55:03 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Ardia", "David", ""], ["Boudt", "Kris", ""], ["Catania", "Leopoldo", ""]]}, {"id": "1611.06350", "submitter": "Roberta De Vito", "authors": "Roberta De Vito, Ruggero Bellio, Lorenzo Trippa, and Giovanni\n  Parmigiani", "title": "Multi-study Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce a novel class of factor analysis methodologies for the joint\nanalysis of multiple studies. The goal is to separately identify and estimate\n1) common factors shared across multiple studies, and 2) study-specific\nfactors. We develop a fast Expectation Conditional-Maximization algorithm for\nparameter estimates and we provide a procedure for choosing the common and\nspecific factor. We present simulations evaluating the performance of the\nmethod and we illustrate it by applying it to gene expression data in ovarian\ncancer. In both cases, we clarify the benefits of a joint analysis compared to\nthe standard factor analysis. We hope to have provided a valuable tool to\naccelerate the pace at which we can combine unsupervised analysis across\nmultiple studies, and understand the cross-study reproducibility of signal in\nmultivariate data.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 12:05:55 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2017 09:20:42 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 10:45:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["De Vito", "Roberta", ""], ["Bellio", "Ruggero", ""], ["Trippa", "Lorenzo", ""], ["Parmigiani", "Giovanni", ""]]}, {"id": "1611.06408", "submitter": "Johann Gagnon-Bartsch", "authors": "Johann Gagnon-Bartsch and Yotam Shem-Tov", "title": "The Classification Permutation Test: A Nonparametric Test for Equality\n  of Multivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gold standard for identifying causal relationships is a randomized\ncontrolled experiment. In many applications in the social sciences and\nmedicine, the researcher does not control the assignment mechanism and instead\nmay rely upon natural experiments, regression discontinuity designs, RCTs with\nattrition, or matching methods as a substitute to experimental randomization.\nThe standard testable implication of random assignment is covariate balance\nbetween the treated and control units. Covariate balance is therefore commonly\nused to validate the claim of \"as-if\" random assignment. We develop a new\nnonparametric test of covariate balance. Our Classification Permutation Test\n(CPT) is based on a combination of classification methods (e.g. logistic\nregression or random forests) with Fisherian permutation inference. The CPT is\nguaranteed to have correct coverage and is consistent under weak assumptions on\nthe chosen classifier. To illustrate the gains of using the CPT, we revisit\nfour real data examples: Lyall (2009); Green and Winik (2010); Eggers and\nHainmueller (2009); and Rouse (1995). Monte Carlo power simulations are used to\ncompare the CPT to two existing nonparametric tests of equality of multivariate\ndistributions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 18:24:01 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Gagnon-Bartsch", "Johann", ""], ["Shem-Tov", "Yotam", ""]]}, {"id": "1611.06713", "submitter": "Seth Flaxman", "authors": "Charles Loeffler and Seth Flaxman", "title": "Is Gun Violence Contagious?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing theories of gun violence predict stable spatial concentrations and\ncontagious diffusion of gun violence into surrounding areas. Recent empirical\nstudies have reported confirmatory evidence of such spatiotemporal diffusion of\ngun violence. However, existing tests cannot readily distinguish spatiotemporal\nclustering from spatiotemporal diffusion. This leaves as an open question\nwhether gun violence actually is contagious or merely clusters in space and\ntime. Compounding this problem, gun violence is subject to considerable\nmeasurement error with many nonfatal shootings going unreported to police.\nUsing point process data from an acoustical gunshot locator system and a\ncombination of Bayesian spatiotemporal point process modeling and space/time\ninteraction tests, this paper demonstrates that contemporary urban gun violence\ndoes diffuse, but only slightly, suggesting that a disease model for infectious\nspread of gun violence is a poor fit for the geographically stable and\ntemporally stochastic process observed.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 10:36:03 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Loeffler", "Charles", ""], ["Flaxman", "Seth", ""]]}, {"id": "1611.06715", "submitter": "Marcel Ausloos", "authors": "C. Herteliu, B.V. Ileanu, M. Ausloos, G. Rotundo", "title": "Pitfalls in testing with linear regression model by OLS", "comments": "6 pages, 16 references, prepared for Journal of Applied Quantitative\n  Methods; revised version, correcting misprints in title and abstract,\n  different order in author list", "journal-ref": "Journal of Applied Quantitative Methods 10(4), 65-67 (2015)", "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on Economic Letters DOI\nhttp://dx.doi.org/10.1016/j.econlet.2015.10.015. We show that due to some\nmethodological aspects the main conclusions of the above mentioned paper should\nbe a little bit altered.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 10:37:55 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 09:53:56 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Herteliu", "C.", ""], ["Ileanu", "B. V.", ""], ["Ausloos", "M.", ""], ["Rotundo", "G.", ""]]}, {"id": "1611.06753", "submitter": "Ningning Xia", "authors": "Cheng Liu, Ningning Xia and Jun Yu", "title": "Shrinkage estimation of covariance matrix for portfolio choice with high\n  frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the usefulness of high frequency data in estimating the\ncovariance matrix for portfolio choice when the portfolio size is large. A\ncomputationally convenient nonlinear shrinkage estimator for the integrated\ncovariance (ICV) matrix of financial assets is developed in two steps. The\neigenvectors of the ICV are first constructed from a designed time variation\nadjusted realized covariance matrix of noise-free log-returns of relatively low\nfrequency data. Then the regularized eigenvalues of the ICV are estimated by\nquasi-maximum likelihood based on high frequency data. The estimator is always\npositive definite and its inverse is the estimator of the inverse of ICV. It\nminimizes the limit of the out-of-sample variance of portfolio returns within\nthe class of rotation-equivalent estimators. It works when the number of\nunderlying assets is larger than the number of time series observations in each\nasset and when the asset price follows a general stochastic process. Our\ntheoretical results are derived under the assumption that the number of assets\n(p) and the sample size (n) satisfy p/n \\to y >0 as n goes to infty . The\nadvantages of our proposed estimator are demonstrated using real data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:21:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Liu", "Cheng", ""], ["Xia", "Ningning", ""], ["Yu", "Jun", ""]]}, {"id": "1611.06818", "submitter": "Anthea Monod", "authors": "Lorin Crawford, Anthea Monod, Andrew X. Chen, Sayan Mukherjee and\n  Ra\\'ul Rabad\\'an", "title": "Predicting Clinical Outcomes in Glioblastoma: An Application of\n  Topological and Functional Data Analysis", "comments": "30 pages, 9 figures, 1 table", "journal-ref": "Journal of the American Statistical Association (2019)", "doi": "10.1080/01621459.2019.1671198", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Glioblastoma multiforme (GBM) is an aggressive form of human brain cancer\nthat is under active study in the field of cancer biology. Its rapid\nprogression and the relative time cost of obtaining molecular data make other\nreadily-available forms of data, such as images, an important resource for\nactionable measures in patients. Our goal is to utilize information given by\nmedical images taken from GBM patients in statistical settings. To do this, we\ndesign a novel statistic---the smooth Euler characteristic transform\n(SECT)---that quantifies magnetic resonance images (MRIs) of tumors. Due to its\nwell-defined inner product structure, the SECT can be used in a wider range of\nfunctional and nonparametric modeling approaches than other previously proposed\ntopological summary statistics. When applied to a cohort of GBM patients, we\nfind that the SECT is a better predictor of clinical outcomes than both\nexisting tumor shape quantifications and common molecular assays. Specifically,\nwe demonstrate that SECT features alone explain more of the variance in GBM\npatient survival than gene expression, volumetric features, and morphometric\nfeatures. The main takeaways from our findings are thus twofold. First, they\nsuggest that images contain valuable information that can play an important\nrole in clinical prognosis and other medical decisions. Second, they show that\nthe SECT is a viable tool for the broader study of medical imaging informatics.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:56:44 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 17:23:16 GMT"}, {"version": "v3", "created": "Wed, 10 Oct 2018 11:44:00 GMT"}, {"version": "v4", "created": "Mon, 18 Feb 2019 00:10:16 GMT"}, {"version": "v5", "created": "Thu, 4 Jul 2019 08:17:32 GMT"}, {"version": "v6", "created": "Thu, 12 Sep 2019 21:24:25 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Crawford", "Lorin", ""], ["Monod", "Anthea", ""], ["Chen", "Andrew X.", ""], ["Mukherjee", "Sayan", ""], ["Rabad\u00e1n", "Ra\u00fal", ""]]}, {"id": "1611.06843", "submitter": "Yu-Xiang Wang", "authors": "Ziqi Liu and Alexander J. Smola and Kyle Soska and Yu-Xiang Wang and\n  Qinghua Zheng", "title": "Joint Hacking and Latent Hazard Rate Estimation", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe an algorithm for predicting the websites at risk in\na long range hacking activity, while jointly inferring the provenance and\nevolution of vulnerabilities on websites over continuous time. Specifically, we\nuse hazard regression with a time-varying additive hazard function\nparameterized in a generalized linear form. The activation coefficients on each\nfeature are continuous-time functions constrained with total variation penalty\ninspired by hacking campaigns. We show that the optimal solution is a 0th order\nspline with a finite number of adaptively chosen knots, and can be solved\nefficiently. Experiments on real data show that our method significantly\noutperforms classic methods while providing meaningful interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 15:42:00 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Liu", "Ziqi", ""], ["Smola", "Alexander J.", ""], ["Soska", "Kyle", ""], ["Wang", "Yu-Xiang", ""], ["Zheng", "Qinghua", ""]]}, {"id": "1611.06851", "submitter": "Antoine Barbieri", "authors": "Antoine Barbieri (UM), Jean Peyhardi (UM, Virtual Plants), Thierry\n  Conroy, Sophie Gourgou, Christian Lavergne, Caroline Mollevi", "title": "Item response models for the longitudinal analysis of health-related\n  quality of life in cancer clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical research regarding health-related quality of life (HRQoL) is a\nmajor challenge to better evaluate the impact of the treatments on their\neveryday life and to improve patients' care. Among the models that are used for\nthe longitudinal analysis of HRQoL, we focused on the mixed models from the\nitem response theory to analyze directly the raw data from questionnaires.\nUsing a recent classification of generalized linear models for categorical\ndata, we discussed about a conceptual selection of these models for the\nlongitudinal analysis of HRQoL in cancer clinical trials. Through\nmethodological and practical arguments, the adjacent and cumulative models seem\nparticularly suitable for this {context}. Specially in cancer clinical trials\nand for the comparison between two groups, the cumulative models has the\nadvantage of providing intuitive illustrations of results. To complete the\ncomparison studies already performed in literature, a simulation study based on\nrandom part of the mixed models is then carried out to compare the linear mixed\nmodel classically used to the discussed item response models. As expected, the\nsensitivity of item response models to detect random effect with lower variance\nis better than the linear mixed model sensitivity. Finally, a longitudinal\nanalysis of HRQoL data from cancer clinical trial is carried out using an item\nresponse cumulative model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 15:53:02 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Barbieri", "Antoine", "", "UM"], ["Peyhardi", "Jean", "", "UM, Virtual Plants"], ["Conroy", "Thierry", ""], ["Gourgou", "Sophie", ""], ["Lavergne", "Christian", ""], ["Mollevi", "Caroline", ""]]}, {"id": "1611.07230", "submitter": "Viet Chi Tran", "authors": "Viet Chi Tran (LAMA), Gwena\\\"elle Castellan (LPP), Anthony Cousien\n  (IAME (UMR\\_S\\_1137 / U1137)), Chi Tran", "title": "Nonparametric adaptive estimation of order 1 Sobol indices in stochastic\n  models, with an application to Epidemiology", "comments": null, "journal-ref": "Electronic journal of statistics , Shaker Heights, OH : Institute\n  of Mathematical Statistics, 2020, 14 (1), pp.50-81.\n  \\&\\#x27E8;10.1214/19-EJS1627\\&\\#x27E9", "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis is a set of methods aiming at quantifying the\ncontribution of an uncertain input parameter of the model (or combination of\nparameters) on the variability of the response. We consider here the estimation\nof the Sobol indices of order 1 which are commonly-used indicators based on a\ndecomposition of the output's variance. In a deterministic framework, when the\nsame inputs always give the same outputs, these indices are usually estimated\nby replicated simulations of the model. In a stochastic framework, when the\nresponse given a set of input parameters is not unique due to randomness in the\nmodel, metamodels are often used to approximate the mean and dispersion of the\nresponse by deterministic functions. We propose a new non-parametric estimator\nwithout the need of defining a metamodel to estimate the Sobol indices of order\n1. The estimator is based on warped wavelets and is adaptive in the regularity\nof the model. The convergence of the mean square error to zero, when the number\nof simulations of the model tend to infinity, is computed and an elbow effect\nis shown, depending on the regularity of the model. Applications in\nEpidemiology are carried to illustrate the use of non-parametric estimators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:08:15 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 07:35:39 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 10:01:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "Viet Chi", "", "LAMA"], ["Castellan", "Gwena\u00eblle", "", "LPP"], ["Cousien", "Anthony", "", "IAME"], ["Tran", "Chi", "", "LAMA"]]}, {"id": "1611.07237", "submitter": "Nathalie Akakpo", "authors": "Nathalie Akakpo (LPMA)", "title": "Multivariate Intensity Estimation via Hyperbolic Wavelet Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical procedure able in some way to overcome the curse\nof dimensionality without structural assumptions on the function to estimate.\nIt relies on a least-squares type penalized criterion and a new collection of\nmodels built from hyperbolic biorthogonal wavelet bases. We study its\nproperties in a unifying intensity estimation framework, where an oracle-type\ninequality and adaptation to mixed smoothness are shown to hold. Besides, we\ndescribe an algorithm for implementing the estimator with a quite reasonable\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:33:05 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 13:43:34 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Akakpo", "Nathalie", "", "LPMA"]]}, {"id": "1611.07412", "submitter": "Yili Hong", "authors": "Yimeng Xie, Zhongnan Jin, Yili Hong, and Jennifer H. Van Mullekom", "title": "Statistical Methods for Thermal Index Estimation Based on Accelerated\n  Destructive Degradation Test Data", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated destructive degradation test (ADDT) is a technique that is\ncommonly used by industries to access material's long-term properties. In many\napplications, the accelerating variable is usually the temperature. In such\ncases, a thermal index (TI) is used to indicate the strength of the material.\nFor example, a TI of 200C may be interpreted as the material can be expected to\nmaintain a specific property at a temperature of 200C for 100,000 hours. A\nmaterial with a higher TI possesses a stronger resistance to thermal damage. In\nliterature, there are three methods available to estimate the TI based on ADDT\ndata, which are the traditional method based on the least-squares approach, the\nparametric method, and the semiparametric method. In this chapter, we provide a\ncomprehensive review of the three methods and illustrate how the TI can be\nestimated based on different models. We also conduct comprehensive simulation\nstudies to show the properties of different methods. We provide thorough\ndiscussions on the pros and cons of each method. The comparisons and discussion\nin this chapter can be useful for practitioners and future industrial\nstandards.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:52:53 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Xie", "Yimeng", ""], ["Jin", "Zhongnan", ""], ["Hong", "Yili", ""], ["Van Mullekom", "Jennifer H.", ""]]}, {"id": "1611.07801", "submitter": "Daniel Jacob", "authors": "Daniel Jacob, Catherine Deborde, Marie Lefebvre, Mickael Maucourt,\n  Anick Moing", "title": "NMRProcFlow: A graphical and interactive tool dedicated to 1D spectra\n  processing for NMR-based metabolomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concerning NMR-based metabolomics, 1D spectra processing often requires an\nexpert eye for disentangling the intertwined peaks, and so far the best way is\nto proceed interactively with a spectra viewer. NMRProcFlow is a graphical and\ninteractive 1D NMR (1H \\& 13C) spectra processing tool dedicated to metabolic\nfingerprinting and targeted metabolomic, covering all spectra processing steps\nincluding baseline correction, chemical shift calibration, alignment. It does\nnot require programming skills. Biologists and NMR spectroscopists can easily\ninteract and develop synergies by visualizing the NMR spectra along with their\ncorresponding experimental-factor levels, thus setting a bridge between\nexperimental design and subsequent statistical analyses.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 13:59:57 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Jacob", "Daniel", ""], ["Deborde", "Catherine", ""], ["Lefebvre", "Marie", ""], ["Maucourt", "Mickael", ""], ["Moing", "Anick", ""]]}, {"id": "1611.07850", "submitter": "Randall Balestriero", "authors": "Randall Balestriero, Behnaam Aazhang", "title": "Robust Unsupervised Transient Detection With Invariant Representation\n  based on the Scattering Network", "comments": "10 pages + 1 reference page", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a sparse and invariant representation with low asymptotic\ncomplexity for robust unsupervised transient and onset zone detection in noisy\nenvironments. This unsupervised approach is based on wavelet transforms and\nleverages the scattering network from Mallat et al. by deriving frequency\ninvariance. This frequency invariance is a key concept to enforce robust\nrepresentations of transients in presence of possible frequency shifts and\nperturbations occurring in the original signal. Implementation details as well\nas complexity analysis are provided in addition of the theoretical framework\nand the invariance properties. In this work, our primary application consists\nof predicting the onset of seizure in epileptic patients from subdural\nrecordings as well as detecting inter-ictal spikes.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 15:54:00 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Balestriero", "Randall", ""], ["Aazhang", "Behnaam", ""]]}, {"id": "1611.07911", "submitter": "Simon Mak", "authors": "Simon Mak, Chih-Li Sung, Xingjian Wang, Shiang-Ting Yeh, Yu-Hung\n  Chang, V. Roshan Joseph, Vigor Yang, C. F. Jeff Wu", "title": "An efficient surrogate model for emulation and physics extraction of\n  large eddy simulations", "comments": "Submitted to JASA A&CS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quest for advanced propulsion and power-generation systems,\nhigh-fidelity simulations are too computationally expensive to survey the\ndesired design space, and a new design methodology is needed that combines\nengineering physics, computer simulations and statistical modeling. In this\npaper, we propose a new surrogate model that provides efficient prediction and\nuncertainty quantification of turbulent flows in swirl injectors with varying\ngeometries, devices commonly used in many engineering applications. The novelty\nof the proposed method lies in the incorporation of known physical properties\nof the fluid flow as {simplifying assumptions} for the statistical model. In\nview of the massive simulation data at hand, which is on the order of hundreds\nof gigabytes, these assumptions allow for accurate flow predictions in around\nan hour of computation time. To contrast, existing flow emulators which forgo\nsuch simplications may require more computation time for training and\nprediction than is needed for conducting the simulation itself. Moreover, by\naccounting for coupling mechanisms between flow variables, the proposed model\ncan jointly reduce prediction uncertainty and extract useful flow physics,\nwhich can then be used to guide further investigations.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 18:18:29 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 22:24:46 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Mak", "Simon", ""], ["Sung", "Chih-Li", ""], ["Wang", "Xingjian", ""], ["Yeh", "Shiang-Ting", ""], ["Chang", "Yu-Hung", ""], ["Joseph", "V. Roshan", ""], ["Yang", "Vigor", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1611.08261", "submitter": "Brian Bader", "authors": "Brian Bader", "title": "Automated, Efficient, and Practical Extreme Value Analysis with\n  Environmental Applications", "comments": "author's dissertation, 6 total chapters, 3 major chapters, Doctoral\n  Dissertations. Paper 1261. http://digitalcommons.uconn.edu/dissertations/1261", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the fundamental probabilistic theory of extremes has been well\ndeveloped, there are many practical considerations that must be addressed in\napplication. The contribution of this thesis is four-fold. The first concerns\nthe choice of r in the r largest order statistics modeling of extremes. The\nsecond contribution pertains to threshold selection in the peaks-over-threshold\napproach. The third combines a theoretical and methodological approach to\nimprove estimation within non-stationary regional frequency models of extremal\ndata\n  The methodology developed is demonstrated with climate based applications.\nLast, an overview of computational issues for extremes is provided, along with\na brief tutorial of the R package eva, which improves the functionality of\nexisting extreme value software, as well as providing new implementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:15:08 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bader", "Brian", ""]]}, {"id": "1611.08280", "submitter": "Chiwoo Park", "authors": "Xin Li, Alex Belianinov, Ondrej Dyck, Stephen Jesse, and Chiwoo Park", "title": "Two-Level Structural Sparsity Regularization for Identifying Lattices\n  and Defects in Noisy Images", "comments": "Accepted to Annals of Applied Statistics", "journal-ref": "Annals of Applied Statistics 2018, Vol. 12, No. 1, 348-377", "doi": "10.1214/17-AOAS1096", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a regularized regression model with a two-level\nstructural sparsity penalty applied to locate individual atoms in a noisy\nscanning transmission electron microscopy image (STEM). In crystals, the\nlocations of atoms is symmetric, condensed into a few lattice groups.\nTherefore, by identifying the underlying lattice in a given image, individual\natoms can be accurately located. We propose to formulate the identification of\nthe lattice groups as a sparse group selection problem. Furthermore, real\natomic scale images contain defects and vacancies, so atomic identification\nbased solely on a lattice group may result in false positives and false\nnegatives. To minimize error, model includes an individual sparsity\nregularization in addition to the group sparsity for a within-group selection,\nwhich results in a regression model with a two-level sparsity regularization.\nWe propose a modification of the group orthogonal matching pursuit (gOMP)\nalgorithm with a thresholding step to solve the atom finding problem. The\nconvergence and statistical analyses of the proposed algorithm are presented.\nThe proposed algorithm is also evaluated through numerical experiments with\nsimulated images. The applicability of the algorithm on determination of atom\nstructures and identification of imaging distortions and atomic defects was\ndemonstrated using three real STEM images. We believe this is an important step\ntoward automatic phase identification and assignment with the advent of genomic\ndatabases for materials.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 18:24:58 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 02:22:11 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 18:41:02 GMT"}, {"version": "v4", "created": "Fri, 1 Sep 2017 19:22:15 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Li", "Xin", ""], ["Belianinov", "Alex", ""], ["Dyck", "Ondrej", ""], ["Jesse", "Stephen", ""], ["Park", "Chiwoo", ""]]}, {"id": "1611.08719", "submitter": "Shinichiro Shirota Mr", "authors": "Shinichiro Shirota and Alan E. Gelfand", "title": "Space and circular time log Gaussian Cox processes with application to\n  crime event data", "comments": "accepted \"Annals of Applied Statistics\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view the locations and times of a collection of crime events as a\nspace-time point pattern. So, with either a nonhomogeneous Poisson process or\nwith a more general Cox process, we need to specify a space-time intensity. For\nthe latter, we need a \\emph{random} intensity which we model as a realization\nof a spatio-temporal log Gaussian process. Importantly, we view time as\ncircular not linear, necessitating valid separable and nonseparable covariance\nfunctions over a bounded spatial region crossed with circular time. In\naddition, crimes are classified by crime type. Furthermore, each crime event is\nrecorded by day of the year which we convert to day of the week marks.\n  The contribution here is to develop models to accommodate such data. Our\nspecifications take the form of hierarchical models which we fit within a\nBayesian framework. In this regard, we consider model comparison between the\nnonhomogeneous Poisson process and the log Gaussian Cox process. We also\ncompare separable vs. nonseparable covariance specifications.\n  Our motivating dataset is a collection of crime events for the city of San\nFrancisco during the year 2012. We have location, hour, day of the year, and\ncrime type for each event. We investigate models to enhance our understanding\nof the set of incidences.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 16:27:44 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Shirota", "Shinichiro", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1611.08791", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Learning without recall in directed circles and rooted trees", "comments": "American Control Conference (ACC), 2015", "journal-ref": null, "doi": "10.1109/ACC.2015.7171992", "report-no": null, "categories": "stat.AP cs.IT cs.SI math.IT math.PR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work investigates the case of a network of agents that attempt to learn\nsome unknown state of the world amongst the finitely many possibilities. At\neach time step, agents all receive random, independently distributed private\nsignals whose distributions are dependent on the unknown state of the world.\nHowever, it may be the case that some or any of the agents cannot distinguish\nbetween two or more of the possible states based only on their private\nobservations, as when several states result in the same distribution of the\nprivate signals. In our model, the agents form some initial belief (probability\ndistribution) about the unknown state and then refine their beliefs in\naccordance with their private observations, as well as the beliefs of their\nneighbors. An agent learns the unknown state when her belief converges to a\npoint mass that is concentrated at the true state. A rational agent would use\nthe Bayes' rule to incorporate her neighbors' beliefs and own private signals\nover time. While such repeated applications of the Bayes' rule in networks can\nbecome computationally intractable, in this paper, we show that in the\ncanonical cases of directed star, circle or path networks and their\ncombinations, one can derive a class of memoryless update rules that replicate\nthat of a single Bayesian agent but replace the self beliefs with the beliefs\nof the neighbors. This way, one can realize an exponentially fast rate of\nlearning similar to the case of Bayesian (fully rational) agents. The proposed\nrules are a special case of the Learning without Recall.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:22:30 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.08848", "submitter": "Elad Yom-Tov", "authors": "Elad Yom-Tov", "title": "Predicting drug recalls from Internet search engine queries", "comments": null, "journal-ref": null, "doi": "10.1109/JTEHM.2017.2732945", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batches of pharmaceutical are sometimes recalled from the market when a\nsafety issue or a defect is detected in specific production runs of a drug.\nSuch problems are usually detected when patients or healthcare providers report\nabnormalities to medical authorities. Here we test the hypothesis that\ndefective production lots can be detected earlier by monitoring queries to\nInternet search engines.\n  We extracted queries from the USA to the Bing search engine which mentioned\none of 5,195 pharmaceutical drugs during 2015 and all recall notifications\nissued by the Food and Drug Administration (FDA) during that year. By using\nattributes that quantify the change in query volume at the state level, we\nattempted to predict if a recall of a specific drug will be ordered by FDA in a\ntime horizon ranging from one to 40 days in future.\n  Our results show that future drug recalls can indeed be identified with an\nAUC of 0.791 and a lift at 5% of approximately 6 when predicting a recall will\noccur one day ahead. This performance degrades as prediction is made for longer\nperiods ahead. The most indicative attributes for prediction are sudden spikes\nin query volume about a specific medicine in each state. Recalls of\nprescription drugs and those estimated to be of medium-risk are more likely to\nbe identified using search query data.\n  These findings suggest that aggregated Internet search engine data can be\nused to facilitate in early warning of faulty batches of medicines.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 14:12:38 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Yom-Tov", "Elad", ""]]}, {"id": "1611.09122", "submitter": "Leonid Borisov", "authors": "Andronik Arutyunov, Leonid Borisov, Sergey Fedorov, Anastasiya\n  Ivchenko, Elizabeth Kirina-Lilinskaya, Yurii Orlov, Konstantin Osminin,\n  Sergey Shilin, Dmitriy Zeniuk", "title": "Statistical Properties of European Languages and Voynich Manuscript\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical properties of letters frequencies in European literature\ntexts are investigated. The determination of logarithmic dependence of letters\nsequence for one-language and two-language texts are examined. The pare of\nlanguages is suggested for Voynich Manuscript. The internal structure of\nManuscript is considered. The spectral portraits of two-letters distribution\nare constructed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 03:48:04 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Arutyunov", "Andronik", ""], ["Borisov", "Leonid", ""], ["Fedorov", "Sergey", ""], ["Ivchenko", "Anastasiya", ""], ["Kirina-Lilinskaya", "Elizabeth", ""], ["Orlov", "Yurii", ""], ["Osminin", "Konstantin", ""], ["Shilin", "Sergey", ""], ["Zeniuk", "Dmitriy", ""]]}, {"id": "1611.09158", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini", "title": "Spatio-Temporal Movements in Team Sports: A Visualization approach using\n  Motion Charts", "comments": "25 pages, 6 figures. arXiv admin note: text overlap with\n  arXiv:1602.06994 by other authors without attribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze the movements and to study the trajectories of players is a\ncrucial need for a team when it looks to improve its chances of winning a match\nor to understand its performances. State of the art tracking systems now\nproduce spatio-temporal traces of player trajectories with high definition and\nfrequency that has facilitated a variety of research efforts to extract insight\nfrom the trajectories. Despite many methods borrowed from different disciplines\n(machine learning, network and complex systems, GIS, computer vision,\nstatistics) has been proposed to answer to the needs of teams, a friendly and\neasy-to-use approach to visualize spatio-temporal movements is still missing.\nThis paper suggests the use of gvisMotionChart function in GoogleVis R package.\nI present and discuss results of a basketball case study. Data refers to a\nmatch played by an italian team militant in \"C-gold\" league on March 22nd,\n2016. With this case study I show that such a visualization approach could be\nuseful in supporting researcher on preliminar stages of their analysis on\nsports' movements, and to facilitate the interpretation of their results.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 16:23:55 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 18:03:30 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Metulini", "Rodolfo", ""]]}, {"id": "1611.09167", "submitter": "Marcel Ausloos", "authors": "Marcel Ausloos, Roy Cerqueti, and Claudio Lupi", "title": "Long-range properties and data validity for hydrogeological time series:\n  the case of the Paglia river", "comments": "prepared for Physica A; 23 pages; 42 references; 3 tables; 11 figures", "journal-ref": "Physica A 470 (2017) 39-50", "doi": "10.1016/j.physa.2016.11.137", "report-no": null, "categories": "physics.geo-ph cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores a large collection of about 377,000 observations,\nspanning more than 20 years with a frequency of 30 minutes, of the streamflow\nof the Paglia river, in central Italy. We analyze the long-term persistence\nproperties of the series by computing the Hurst exponent, not only in its\noriginal form but also under an evolutionary point of view by analyzing the\nHurst exponents over a rolling windows basis. The methodological tool adopted\nfor the persistence is the detrended fluctuation analysis (DFA), which is\nclassically known as suitable for our purpose. As an ancillary exploration, we\nimplement a control on the data validity by assessing if the data exhibit the\nregularity stated by Benford's law. Results are interesting under different\nviewpoints. First, we show that the Paglia river streamflow exhibits\nperiodicities which broadly suggest the existence of some common behaviour with\nEl Nino and the North Atlantic Oscillations: this specifically points to a (not\nnecessarily direct) effect of these oceanic phenomena on the hydrogeological\nequilibria of very far geographical zones: however, such an hypothesis needs\nfurther analyses to be validated. Second, the series of streamflows shows an\nantipersistent behaviour. Third, data are not consistent with Benford's law:\nthis suggests that the measurement criteria should be opportunely revised.\nFourth, the streamflow distribution is well approximated by a discrete\ngeneralized Beta distribution: this is well in accordance with the measured\nstreamflows being the outcome of a complex system.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 11:07:09 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Ausloos", "Marcel", ""], ["Cerqueti", "Roy", ""], ["Lupi", "Claudio", ""]]}, {"id": "1611.09341", "submitter": "Christopher Harms", "authors": "Christopher Harms", "title": "A Bayes Factor for Replications of ANOVA Results", "comments": "25 pages, 4 figures", "journal-ref": null, "doi": "10.1080/00031305.2018.1518787", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With an increasing number of replication studies performed in psychological\nscience, the question of how to evaluate the outcome of a replication attempt\ndeserves careful consideration. Bayesian approaches allow to incorporate\nuncertainty and prior information into the analysis of the replication attempt\nby their design. The Replication Bayes Factor, introduced by Verhagen &\nWagenmakers (2014), provides quantitative, relative evidence in favor or\nagainst a successful replication. In previous work by Verhagen & Wagenmakers\n(2014) it was limited to the case of $t$-tests. In this paper, the Replication\nBayes Factor is extended to $F$-tests in multi-group, fixed-effect ANOVA\ndesigns. Simulations and examples are presented to facilitate the understanding\nand to demonstrate the usefulness of this approach. Finally, the Replication\nBayes Factor is compared to other Bayesian and frequentist approaches and\ndiscussed in the context of replication attempts. R code to calculate\nReplication Bayes factors and to reproduce the examples in the paper is\navailable at https://osf.io/jv39h/.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:53:29 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 09:00:34 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 13:16:57 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Harms", "Christopher", ""]]}, {"id": "1611.09414", "submitter": "Amit Sharma", "authors": "Amit Sharma, Jake M. Hofman, Duncan J. Watts", "title": "Split-door criterion: Identification of causal effects through auxiliary\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating causal effects in time series data when\nfine-grained information about the outcome of interest is available.\nSpecifically, we examine what we call the split-door setting, where the outcome\nvariable can be split into two parts: one that is potentially affected by the\ncause being studied and another that is independent of it, with both parts\nsharing the same (unobserved) confounders. We show that under these conditions,\nthe problem of identification reduces to that of testing for independence among\nobserved variables, and present a method that uses this approach to\nautomatically find subsets of the data that are causally identified. We\ndemonstrate the method by estimating the causal impact of Amazon's recommender\nsystem on traffic to product pages, finding thousands of examples within the\ndataset that satisfy the split-door criterion. Unlike past studies based on\nnatural experiments that were limited to a single product category, our method\napplies to a large and representative sample of products viewed on the site. In\nline with previous work, we find that the widely-used click-through rate (CTR)\nmetric overestimates the causal impact of recommender systems; depending on the\nproduct category, we estimate that 50-80\\% of the traffic attributed to\nrecommender systems would have happened even without any recommendations. We\nconclude with guidelines for using the split-door criterion as well as a\ndiscussion of other contexts where the method can be applied.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 22:32:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 13:09:13 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sharma", "Amit", ""], ["Hofman", "Jake M.", ""], ["Watts", "Duncan J.", ""]]}, {"id": "1611.09435", "submitter": "Matthew Pietrosanu", "authors": "Matthew Pietrosanu", "title": "The analysis of topological structure in data using persistent homology:\n  applications to lexical word association networks", "comments": "Final report for undergraduate research project", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology is a technique recently developed in algebraic and\ncomputational topology well-suited to analysing structure in complex,\nhigh-dimensional data. In this paper, we exposit the theory of persistent\nhomology from first principles and detail a novel application of this method to\nthe field of computational linguistics. Using this method, we search for\nclusters and other topological features among closely-associated words of the\nEnglish language. Furthermore, we compare the clustering abilities of\npersistent homology and the commonly-used Markov clustering algorithm and\ndiscuss improvements to basic persistent homology techniques to increase its\nclustering efficacy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:50:02 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Pietrosanu", "Matthew", ""]]}, {"id": "1611.09477", "submitter": "John Mount", "authors": "Nina Zumel, John Mount", "title": "vtreat: a data.frame Processor for Predictive Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look at common problems found in data that is used for predictive modeling\ntasks, and describe how to address them with the vtreat R package. vtreat\nprepares real-world data for predictive modeling in a reproducible and\nstatistically sound manner. We describe the theory of preparing variables so\nthat data has fewer exceptional cases, making it easier to safely use models in\nproduction. Common problems dealt with include: infinite values, invalid\nvalues, NA, too many categorical levels, rare categorical levels, and new\ncategorical levels (levels seen during application, but not during training).\nOf special interest are techniques needed to avoid needlessly introducing\nundesirable nested modeling bias (which is a risk when using a\ndata-preprocessor).\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 03:53:16 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 18:10:46 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 13:20:36 GMT"}, {"version": "v4", "created": "Sat, 4 May 2019 15:43:58 GMT"}, {"version": "v5", "created": "Mon, 1 Jul 2019 17:40:05 GMT"}, {"version": "v6", "created": "Wed, 24 Jul 2019 14:34:29 GMT"}, {"version": "v7", "created": "Sat, 27 Jul 2019 22:56:55 GMT"}, {"version": "v8", "created": "Wed, 11 Sep 2019 15:37:34 GMT"}, {"version": "v9", "created": "Sun, 22 Sep 2019 22:04:13 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Zumel", "Nina", ""], ["Mount", "John", ""]]}, {"id": "1611.09766", "submitter": "Saptarshi Das", "authors": "Saptarshi Das, Barry Juans Ajiwibawa, Shre Kumar Chatterjee, Sanmitra\n  Ghosh, Koushik Maharatna, Srinandan Dasmahapatra, Andrea Vitaletti, Elisa\n  Masi, and Stefano Mancuso", "title": "Drift Removal in Plant Electrical Signals via IIR Filtering Using\n  Wavelet Energy", "comments": "12 pages, 9 figures, 1 table", "journal-ref": "Computers and Electronics in Agriculture, Volume 118, October\n  2015, Pages 15-23", "doi": "10.1016/j.compag.2015.08.013", "report-no": null, "categories": "stat.AP cs.SY physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plant electrical signals often contains low frequency drifts with or without\nthe application of external stimuli. Quantification of the randomness in plant\nsignals in a stimulus-specific way is hindered because the knowledge of vital\nfrequency information in the actual biological response is not known yet. Here\nwe design an optimum Infinite Impulse Response (IIR) filter which removes the\nlow frequency drifts and preserves the frequency spectrum corresponding to the\nrandom component of the unstimulated plant signals by bringing the bias due to\nunknown artifacts and drifts to a minimum. We use energy criteria of wavelet\npacket transform (WPT) for optimization based tuning of the IIR filter\nparameters. Such an optimum filter enforces that the energy distribution of the\npre-stimulus parts in different experiments are almost overlapped but under\ndifferent stimuli the distributions of the energy get changed. The reported\nresearch may popularize plant signal processing, as a separate field, besides\nother conventional bioelectrical signal processing paradigms.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 18:21:36 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Das", "Saptarshi", ""], ["Ajiwibawa", "Barry Juans", ""], ["Chatterjee", "Shre Kumar", ""], ["Ghosh", "Sanmitra", ""], ["Maharatna", "Koushik", ""], ["Dasmahapatra", "Srinandan", ""], ["Vitaletti", "Andrea", ""], ["Masi", "Elisa", ""], ["Mancuso", "Stefano", ""]]}, {"id": "1611.09791", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Koushik Maharatna, Fabio Apicella,\n  Georgia Chronaki, Federico Sicca, David Cohen, Filippo Muratori", "title": "On the Existence of Synchrostates in Multichannel EEG Signals during\n  Face-perception Tasks", "comments": "30 pages, 22 figures, 2 tables", "journal-ref": "Biomedical Physics & Engineering Express, vol. 1, no. 1, pp.\n  015002, 2015", "doi": "10.1088/2057-1976/1/1/015002", "report-no": null, "categories": "physics.med-ph cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase synchronisation in multichannel EEG is known as the manifestation of\nfunctional brain connectivity. Traditional phase synchronisation studies are\nmostly based on time average synchrony measures hence do not preserve the\ntemporal evolution of the phase difference. Here we propose a new method to\nshow the existence of a small set of unique phase synchronised patterns or\n\"states\" in multi-channel EEG recordings, each \"state\" being stable of the\norder of ms, from typical and pathological subjects during face perception\ntasks. The proposed methodology bridges the concepts of EEG microstates and\nphase synchronisation in time and frequency domain respectively. The analysis\nis reported for four groups of children including typical, Autism Spectrum\nDisorder (ASD), low and high anxiety subjects - a total of 44 subjects. In all\ncases, we observe consistent existence of these states - termed as\nsynchrostates - within specific cognition related frequency bands (beta and\ngamma bands), though the topographies of these synchrostates differ for\ndifferent subject groups with different pathological conditions. The\ninter-synchrostate switching follows a well-defined sequence capturing the\nunderlying inter-electrode phase relation dynamics in stimulus- and\nperson-centric manner. Our study is motivated from the well-known EEG\nmicrostate exhibiting stable potential maps over the scalp. However, here we\nreport a similar observation of quasi-stable phase synchronised states in\nmultichannel EEG. The existence of the synchrostates coupled with their unique\nswitching sequence characteristics could be considered as a potentially new\nfield over contemporary EEG phase synchronisation studies.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:03:29 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Apicella", "Fabio", ""], ["Chronaki", "Georgia", ""], ["Sicca", "Federico", ""], ["Cohen", "David", ""], ["Muratori", "Filippo", ""]]}, {"id": "1611.09820", "submitter": "Saptarshi Das", "authors": "Shre Kumar Chatterjee, Saptarshi Das, Koushik Maharatna, Elisa Masi,\n  Luisa Santopolo, Stefano Mancuso and Andrea Vitaletti", "title": "Exploring Strategies for Classification of External Stimuli Using\n  Statistical Features of the Plant Electrical Response", "comments": "22 pages, 7 figures, 9 tables", "journal-ref": "Journal of the Royal Society Interface, vol. 12, no. 104, pp.\n  20141225, March 2015", "doi": "10.1098/rsif.2014.1225", "report-no": null, "categories": "physics.bio-ph q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants sense their environment by producing electrical signals which in\nessence represent changes in underlying physiological processes. These\nelectrical signals, when monitored, show both stochastic and deterministic\ndynamics. In this paper, we compute 11 statistical features from the raw\nnon-stationary plant electrical signal time series to classify the stimulus\napplied (causing the electrical signal). By using different discriminant\nanalysis based classification techniques, we successfully establish that there\nis enough information in the raw electrical signal to classify the stimuli. In\nthe process, we also propose two standard features which consistently give good\nclassification results for three types of stimuli - Sodium Chloride (NaCl),\nSulphuric Acid (H2SO4) and Ozone (O3). This may facilitate reduction in the\ncomplexity involved in computing all the features for online classification of\nsimilar external stimuli in future.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:16:11 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Chatterjee", "Shre Kumar", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Masi", "Elisa", ""], ["Santopolo", "Luisa", ""], ["Mancuso", "Stefano", ""], ["Vitaletti", "Andrea", ""]]}, {"id": "1611.09829", "submitter": "Saptarshi Das", "authors": "Grazia Cappiello, Saptarshi Das, Evangelos B. Mazomenos, Koushik\n  Maharatna, George Koulaouzidis, John Morgan, and Paolo Emilio Puddu", "title": "A Statistical Index for Early Diagnosis of Ventricular Arrhythmia from\n  the Trend Analysis of ECG Phase-portraits", "comments": "25 pages, 16 figures, 2 tables", "journal-ref": "Physiological Measurement, vol. 36, no. 1, pp. 107-131, January\n  2015", "doi": "10.1088/0967-3334/36/1/107", "report-no": null, "categories": "physics.med-ph nlin.CD stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel statistical index for the early diagnosis\nof ventricular arrhythmia (VA) using the time delay phase-space reconstruction\n(PSR) technique, from the electrocardiogram (ECG) signal. Patients with two\nclasses of fatal VA - with preceding ventricular premature beats (VPBs) and\nwith no VPBs have been analysed using extensive simulations. Three subclasses\nof VA with VPBs viz. ventricular tachycardia (VT), ventricular fibrillation\n(VF) and VT followed by VF are analyzed using the proposed technique. Measures\nof descriptive statistics like mean ({\\mu}), standard deviation ({\\sigma}),\ncoefficient of variation (CV = {\\sigma}/{\\mu}), skewness ({\\gamma}) and\nkurtosis (\\{beta}) in phase-space diagrams are studied for a sliding window of\n10 beats of ECG signal using the box-counting technique. Subsequently, a hybrid\nprediction index which is composed of a weighted sum of CV and kurtosis has\nbeen proposed for predicting the impending arrhythmia before its actual\noccurrence. The early diagnosis involves crossing the upper bound of a hybrid\nindex which is capable of predicting an impending arrhythmia 356 ECG beats, on\naverage (with 192 beats standard deviation) before its onset when tested with\n32 VA patients (both with and without VPBs). The early diagnosis result is also\nverified using a leave out cross-validation (LOOCV) scheme with 96.88%\nsensitivity, 100% specificity and 98.44% accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:36:15 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Cappiello", "Grazia", ""], ["Das", "Saptarshi", ""], ["Mazomenos", "Evangelos B.", ""], ["Maharatna", "Koushik", ""], ["Koulaouzidis", "George", ""], ["Morgan", "John", ""], ["Puddu", "Paolo Emilio", ""]]}, {"id": "1611.09888", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, and Koushik Maharatna", "title": "Existence of Millisecond-order Stable States in Time-Varying Phase\n  Synchronization Measure in EEG Signals", "comments": "4 pages, 8 figures, 1 table", "journal-ref": "Engineering in Medicine and Biology Society (EMBC), 2013 35th\n  Annual International Conference of the IEEE, pp. 2539-2542, July 2013, Osaka,\n  Japan", "doi": "10.1109/EMBC.2013.6610057", "report-no": null, "categories": "physics.med-ph q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have developed a new measure of understanding the temporal\nevolution of phase synchronization for EEG signals using cross-electrode\ninformation. From this measure it is found that there exists a small number of\nwell-defined phase-synchronized states, each of which is stable for few\nmilliseconds during the execution of a face perception task. We termed these\nquasi-stable states as synchrostates. We used k-means clustering algorithms to\nestimate the optimal number of synchrostates from 100 trials of EEG signals\nover 128 channels. Our results show that these synchrostates exist consistently\nin all the different trials. It is also found that from the onset of the\nstimulus, switching between these synchrostates results in well-behaved\ntemporal sequence with repeatability which may be indicative of the dynamics of\nthe cognitive process underlying that task. Therefore these synchrostates and\ntheir temporal switching sequences may be used as a new measure of the\nstability of phase synchrony and information exchange between different regions\nof a human brain.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:22:04 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1611.09891", "submitter": "Saptarshi Das", "authors": "Wasifa Jamal, Saptarshi Das, Koushik Maharatna, Doga Kuyucu, Federico\n  Sicca, Lucia Billeci, Fabio Apicella, and Filippo Muratori", "title": "Using Brain Connectivity Measure of EEG Synchrostates for Discriminating\n  Typical and Autism Spectrum Disorder", "comments": "4 pages, 11 figures, 1 table", "journal-ref": "Neural Engineering (NER), 2013 6th International IEEE/EMBS\n  Conference on, pp. 1402-1405, Nov. 2013, San Diego, CA", "doi": "10.1109/NER.2013.6696205", "report-no": null, "categories": "physics.med-ph q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we utilized the concept of stable phase synchronization\ntopography - synchrostates - over the scalp derived from EEG recording for\nformulating brain connectivity network in Autism Spectrum Disorder (ASD) and\ntypically-growing children. A synchronization index is adapted for forming the\nedges of the connectivity graph capturing the stability of each of the\nsynchrostates. Such network is formed for 11 ASD and 12 control group children.\nComparative analyses of these networks using graph theoretic measures show that\nchildren with autism have a different modularity of such networks from typical\nchildren. This result could pave the way to a new modality for possible\nidentification of ASD from non-invasively recorded EEG data.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:26:30 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Kuyucu", "Doga", ""], ["Sicca", "Federico", ""], ["Billeci", "Lucia", ""], ["Apicella", "Fabio", ""], ["Muratori", "Filippo", ""]]}, {"id": "1611.10331", "submitter": "John Mount", "authors": "John Mount and Nina Zumel", "title": "Comparing Apples and Oranges: Two Examples of the Limits of Statistical\n  Inference, With an Application to Google Advertising Markets", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the classic Cramer-Rao bound limits how accurately one can\nsimultaneously estimate values of a large number of Google Ad campaigns (or\nsimilarly limit the measurement rate of many confounding A/B tests).\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:49:55 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Mount", "John", ""], ["Zumel", "Nina", ""]]}]