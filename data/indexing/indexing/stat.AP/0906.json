[{"id": "0906.0198", "submitter": "Vartan Choulakian", "authors": "Vartan Choulakian", "title": "Some Numerical Results on the Rank of Generic Three-Way Arrays over R", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is the introduction of a new method for the numerical\ncomputation of the rank of a three-way array. We show that the rank of a\nthree-way array over R is intimately related to the real solution set of a\nsystem of polynomial equations. Using this, we present some numerical results\nbased on the computation of Grobner bases.\n  Key words: Tensors; three-way arrays; Candecomp/Parafac; Indscal; generic\nrank; typical rank; Veronese variety; Segre variety; Grobner bases.\n  AMS classification: Primary 15A69; Secondary 15A72, 15A18.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2009 15:11:28 GMT"}], "update_date": "2009-06-02", "authors_parsed": [["Choulakian", "Vartan", ""]]}, {"id": "0906.1094", "submitter": "Ruiyan Luo", "authors": "Ruiyan Luo, Bret Larget", "title": "Modeling substitution and indel processes for AFLP marker evolution and\n  phylogenetic inference", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS212 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 222-248", "doi": "10.1214/08-AOAS212", "report-no": "IMS-AOAS-AOAS212", "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amplified fragment length polymorphism (AFLP) method produces anonymous\ngenetic markers from throughout a genome. We extend the nucleotide substitution\nmodel of AFLP evolution to additionally include insertion and deletion\nprocesses. The new Sub-ID model relaxes the common assumption that markers are\nindependent and homologous. We build a Markov chain Monte Carlo methodology\ntailored for the Sub-ID model to implement a Bayesian approach to infer AFLP\nmarker evolution. The method allows us to infer both the phylogenies and the\nsubset of markers that are possibly homologous. In addition, we can infer the\ngenome-wide relative rate of indels versus substitutions. In a case study with\nAFLP markers from sedges, a grass-like plant common in North America, we find\nthat accounting for insertion and deletion makes a difference in phylogenetic\ninference. The inference of topologies is not sensitive to the prior settings\nand the Jukes--Cantor assumption for nucleotide substitution. The model for\ninsertion and deletion we introduce has potential value in other phylogenetic\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2009 11:29:56 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Luo", "Ruiyan", ""], ["Larget", "Bret", ""]]}, {"id": "0906.1107", "submitter": "Maria-Pia Victoria-Feser", "authors": "Philippe Huber, Olivier Scaillet, Maria-Pia Victoria-Feser", "title": "Assessing multivariate predictors of financial market movements: A\n  latent factor framework for ordinal data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS213 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 249-271", "doi": "10.1214/08-AOAS213", "report-no": "IMS-AOAS-AOAS213", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the trading activity in Equity markets is directed to brokerage\nhouses. In exchange they provide so-called \"soft dollars,\" which basically are\namounts spent in \"research\" for identifying profitable trading opportunities.\nSoft dollars represent about USD 1 out of every USD 10 paid in commissions.\nObviously they are costly, and it is interesting for an institutional investor\nto determine whether soft dollar inputs are worth being used (and indirectly\npaid for) or not, from a statistical point of view. To address this question,\nwe develop association measures between what broker--dealers predict and what\nmarkets realize. Our data are ordinal predictions by two broker--dealers and\nrealized values on several markets, on the same ordinal scale. We develop a\nstructural equation model with latent variables in an ordinal setting which\nallows us to test broker--dealer predictive ability of financial market\nmovements. We use a multivariate logit model in a latent factor framework,\ndevelop a tractable estimator based on a Laplace approximation, and show its\nconsistency and asymptotic normality. Monte Carlo experiments reveal that both\nthe estimation method and the testing procedure perform well in small samples.\nThe method is then used to analyze our dataset.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2009 12:26:05 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Huber", "Philippe", ""], ["Scaillet", "Olivier", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "0906.1115", "submitter": "Michael L. Stein", "authors": "Michael L. Stein", "title": "Spatial interpolation of high-frequency monitoring data", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS208 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 272-291", "doi": "10.1214/08-AOAS208", "report-no": "IMS-AOAS-AOAS208", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate modelers generally require meteorological information on regular\ngrids, but monitoring stations are, in practice, sited irregularly. Thus, there\nis a need to produce public data records that interpolate available data to a\nhigh density grid, which can then be used to generate meteorological maps at a\nbroad range of spatial and temporal scales. In addition to point predictions,\nquantifications of uncertainty are also needed. One way to accomplish this is\nto provide multiple simulations of the relevant meteorological quantities\nconditional on the observed data taking into account the various uncertainties\nin predicting a space-time process at locations with no monitoring data. Using\na high-quality dataset of minute-by-minute measurements of atmospheric pressure\nin north-central Oklahoma, this work describes a statistical approach to\ncarrying out these conditional simulations. Based on observations at 11\nstations, conditional simulations were produced at two other sites with\nmonitoring stations. The resulting point predictions are very accurate and the\nmultiple simulations produce well-calibrated prediction uncertainties for\ntemporal changes in atmospheric pressure but are substantially overconservative\nfor the uncertainties in the predictions of (undifferenced) pressure.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2009 12:51:00 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Stein", "Michael L.", ""]]}, {"id": "0906.1117", "submitter": "Mark Culp", "authors": "Mark Culp, George Michailidis, Kjell Johnson", "title": "On multi-view learning with additive models", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS202 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 292-318", "doi": "10.1214/08-AOAS202", "report-no": "IMS-AOAS-AOAS202", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific settings data can be naturally partitioned into variable\ngroupings called views. Common examples include environmental (1st view) and\ngenetic information (2nd view) in ecological applications, chemical (1st view)\nand biological (2nd view) data in drug discovery. Multi-view data also occur in\ntext analysis and proteomics applications where one view consists of a graph\nwith observations as the vertices and a weighted measure of pairwise similarity\nbetween observations as the edges. Further, in several of these applications\nthe observations can be partitioned into two sets, one where the response is\nobserved (labeled) and the other where the response is not (unlabeled). The\nproblem for simultaneously addressing viewed data and incorporating unlabeled\nobservations in training is referred to as multi-view transductive learning. In\nthis work we introduce and study a comprehensive generalized fixed point\nadditive modeling framework for multi-view transductive learning, where any\nview is represented by a linear smoother. The problem of view selection is\ndiscussed using a generalized Akaike Information Criterion, which provides an\napproach for testing the contribution of each view. An efficient implementation\nis provided for fitting these models with both backfitting and local-scoring\ntype algorithms adjusted to semi-supervised graph-based learning. The proposed\ntechnique is assessed on both synthetic and real data sets and is shown to be\ncompetitive to state-of-the-art co-training and graph-based techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2009 13:08:57 GMT"}], "update_date": "2009-06-08", "authors_parsed": [["Culp", "Mark", ""], ["Michailidis", "George", ""], ["Johnson", "Kjell", ""]]}, {"id": "0906.1421", "submitter": "Snigdhansu Chatterjee", "authors": "Snigdhansu Chatterjee, Peihua Qiu", "title": "Distribution-free cumulative sum control charts using bootstrap-based\n  control limits", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS197 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 349-369", "doi": "10.1214/08-AOAS197", "report-no": "IMS-AOAS-AOAS197", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with phase II, univariate, statistical process control when\na set of in-control data is available, and when both the in-control and\nout-of-control distributions of the process are unknown. Existing process\ncontrol techniques typically require substantial knowledge about the in-control\nand out-of-control distributions of the process, which is often difficult to\nobtain in practice. We propose (a) using a sequence of control limits for the\ncumulative sum (CUSUM) control charts, where the control limits are determined\nby the conditional distribution of the CUSUM statistic given the last time it\nwas zero, and (b) estimating the control limits by bootstrap. Traditionally,\nthe CUSUM control chart uses a single control limit, which is obtained under\nthe assumption that the in-control and out-of-control distributions of the\nprocess are Normal. When the normality assumption is not valid, which is often\ntrue in applications, the actual in-control average run length, defined to be\nthe expected time duration before the control chart signals a process change,\nis quite different from the nominal in-control average run length. This\nlimitation is mostly eliminated in the proposed procedure, which is\ndistribution-free and robust against different choices of the in-control and\nout-of-control distributions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 07:12:03 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Chatterjee", "Snigdhansu", ""], ["Qiu", "Peihua", ""]]}, {"id": "0906.1428", "submitter": "Christopher J. Paciorek", "authors": "Christopher J. Paciorek, Jeff D. Yanosky, Robin C. Puett, Francine\n  Laden, Helen H. Suh", "title": "Practical large-scale spatio-temporal modeling of particulate matter\n  concentrations", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS204 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 370-397", "doi": "10.1214/08-AOAS204", "report-no": "IMS-AOAS-AOAS204", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last two decades have seen intense scientific and regulatory interest in\nthe health effects of particulate matter (PM). Influential epidemiological\nstudies that characterize chronic exposure of individuals rely on monitoring\ndata that are sparse in space and time, so they often assign the same exposure\nto participants in large geographic areas and across time. We estimate monthly\nPM during 1988--2002 in a large spatial domain for use in studying health\neffects in the Nurses' Health Study. We develop a conceptually simple\nspatio-temporal model that uses a rich set of covariates. The model is used to\nestimate concentrations of $PM_{10}$ for the full time period and $PM_{2.5}$\nfor a subset of the period. For the earlier part of the period, 1988--1998, few\n$PM_{2.5}$ monitors were operating, so we develop a simple extension to the\nmodel that represents $PM_{2.5}$ conditionally on $PM_{10}$ model predictions.\nIn the epidemiological analysis, model predictions of $PM_{10}$ are more\nstrongly associated with health effects than when using simpler approaches to\nestimate exposure. Our modeling approach supports the application in estimating\nboth fine-scale and large-scale spatial heterogeneity and capturing space--time\ninteraction through the use of monthly-varying spatial surfaces. At the same\ntime, the model is computationally feasible, implementable with standard\nsoftware, and readily understandable to the scientific audience. Despite\nsimplifying assumptions, the model has good predictive performance and\nuncertainty characterization.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 08:01:11 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Paciorek", "Christopher J.", ""], ["Yanosky", "Jeff D.", ""], ["Puett", "Robin C.", ""], ["Laden", "Francine", ""], ["Suh", "Helen H.", ""]]}, {"id": "0906.1433", "submitter": "Abhyuday Mandal", "authors": "Abhyuday Mandal, Pritam Ranjan, C. F. Jeff Wu", "title": "$\\mathcal{G}$-SELC: Optimization by sequential elimination of level\n  combinations using genetic algorithms and Gaussian processes", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS199 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 398-421", "doi": "10.1214/08-AOAS199", "report-no": "IMS-AOAS-AOAS199", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying promising compounds from a vast collection of feasible compounds\nis an important and yet challenging problem in the pharmaceutical industry. An\nefficient solution to this problem will help reduce the expenditure at the\nearly stages of drug discovery. In an attempt to solve this problem, Mandal, Wu\nand Johnson [Technometrics 48 (2006) 273--283] proposed the SELC algorithm.\nAlthough powerful, it fails to extract substantial information from the data to\nguide the search efficiently, as this methodology is not based on any\nstatistical modeling. The proposed approach uses Gaussian Process (GP) modeling\nto improve upon SELC, and hence named $\\mathcal{G}$-SELC. The performance of\nthe proposed methodology is illustrated using four and five dimensional test\nfunctions. Finally, we implement the new algorithm on a real pharmaceutical\ndata set for finding a group of chemical compounds with optimal properties.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 08:27:30 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Mandal", "Abhyuday", ""], ["Ranjan", "Pritam", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "0906.1444", "submitter": "Yacine A\\\"{\\i}t-Sahalia", "authors": "Yacine A\\\"it-Sahalia, Jialin Yu", "title": "High frequency market microstructure noise estimates and liquidity\n  measures", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS200 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 422-457", "doi": "10.1214/08-AOAS200", "report-no": "IMS-AOAS-AOAS200", "categories": "stat.AP q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using recent advances in the econometrics literature, we disentangle from\nhigh frequency observations on the transaction prices of a large sample of NYSE\nstocks a fundamental component and a microstructure noise component. We then\nrelate these statistical measurements of market microstructure noise to\nobservable characteristics of the underlying stocks and, in particular, to\ndifferent financial measures of their liquidity. We find that more liquid\nstocks based on financial characteristics have lower noise and noise-to-signal\nratio measured from their high frequency returns. We then examine whether there\nexists a common, market-wide, factor in high frequency stock-level measurements\nof noise, and whether that factor is priced in asset returns.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 09:17:45 GMT"}], "update_date": "2009-06-11", "authors_parsed": [["A\u00eft-Sahalia", "Yacine", ""], ["Yu", "Jialin", ""]]}, {"id": "0906.1457", "submitter": "Chong-Zhi Di", "authors": "Chong-Zhi Di, Ciprian M. Crainiceanu, Brian S. Caffo, Naresh M.\n  Punjabi", "title": "Multilevel functional principal component analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS206 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2009, Vol. 3, No. 1, 458-488", "doi": "10.1214/08-AOAS206", "report-no": "IMS-AOAS-AOAS206", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sleep Heart Health Study (SHHS) is a comprehensive landmark study of\nsleep and its impacts on health outcomes. A primary metric of the SHHS is the\nin-home polysomnogram, which includes two electroencephalographic (EEG)\nchannels for each subject, at two visits. The volume and importance of this\ndata presents enormous challenges for analysis. To address these challenges, we\nintroduce multilevel functional principal component analysis (MFPCA), a novel\nstatistical methodology designed to extract core intra- and inter-subject\ngeometric components of multilevel functional data. Though motivated by the\nSHHS, the proposed methodology is generally applicable, with potential\nrelevance to many modern scientific studies of hierarchical or longitudinal\nfunctional outcomes. Notably, using MFPCA, we identify and quantify\nassociations between EEG activity during sleep and adverse cardiovascular\noutcomes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2009 10:12:05 GMT"}], "update_date": "2009-06-09", "authors_parsed": [["Di", "Chong-Zhi", ""], ["Crainiceanu", "Ciprian M.", ""], ["Caffo", "Brian S.", ""], ["Punjabi", "Naresh M.", ""]]}, {"id": "0906.1637", "submitter": "Andrea Censi", "authors": "Andrea Censi", "title": "Geometric remarks on Kalman filtering with intermittent observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sinopoli et al. (TAC, 2004) considered the problem of optimal estimation for\nlinear systems with Gaussian noise and intermittent observations, available\naccording to a Bernoulli arrival process. They showed that there is a\n\"critical\" arrival probability of the observations, such that under that\nthreshold the expected value of the covariance matrix (i.e., the quadratic\nerror) of the estimate is unbounded. Sinopoli et al., and successive authors,\ninterpreted this result implying that the behavior of the system is\nqualitatively different above and below the threshold. This paper shows that\nthis is not necessarily the only interpretation. In fact, the critical\nprobability is different if one considers the average error instead of the\naverage quadratic error. More generally, finding a meaningful \"average\"\ncovariance is not as simple as taking the algebraic expected value. A rigorous\nway to frame the problem is in a differential geometric framework, by\nrecognizing that the set of covariance matrices (or better, the manifold of\nGaussian distributions) is not a flat space, and then studying the intrinsic\nRiemannian mean. Several metrics on this manifold are considered that lead to\ndifferent critical probabilities, or no critical probability at all.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2009 05:32:59 GMT"}], "update_date": "2009-06-10", "authors_parsed": [["Censi", "Andrea", ""]]}, {"id": "0906.1905", "submitter": "Patrick Guio", "authors": "P. Guio and N. Achilleos", "title": "The VOISE Algorithm: a Versatile Tool for Automatic Segmentation of\n  Astronomical Images", "comments": "9 pages, 7 figures; accepted for publication in MNRAS", "journal-ref": "Mon. Not. R. Astron. Soc. 398 (2009) 1254-1262", "doi": "10.1111/j.1365-2966.2009.15218.x", "report-no": null, "categories": "astro-ph.IM astro-ph.EP cs.CV physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The auroras on Jupiter and Saturn can be studied with a high sensitivity and\nresolution by the Hubble Space Telescope (HST) ultraviolet (UV) and\nfar-ultraviolet (FUV) Space Telescope spectrograph (STIS) and Advanced Camera\nfor Surveys (ACS) instruments. We present results of automatic detection and\nsegmentation of Jupiter's auroral emissions as observed by HST ACS instrument\nwith VOronoi Image SEgmentation (VOISE). VOISE is a dynamic algorithm for\npartitioning the underlying pixel grid of an image into regions according to a\nprescribed homogeneity criterion. The algorithm consists of an iterative\nprocedure that dynamically constructs a tessellation of the image plane based\non a Voronoi Diagram, until the intensity of the underlying image within each\nregion is classified as homogeneous. The computed tessellations allow the\nextraction of quantitative information about the auroral features such as mean\nintensity, latitudinal and longitudinal extents and length scales. These\noutputs thus represent a more automated and objective method of characterising\nauroral emissions than manual inspection.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2009 10:48:33 GMT"}], "update_date": "2009-09-11", "authors_parsed": [["Guio", "P.", ""], ["Achilleos", "N.", ""]]}, {"id": "0906.2234", "submitter": "Zhongyang Zhang", "authors": "Zhongyang Zhang, Kenneth Lange, Roel Ophoff, Chiara Sabatti", "title": "Reconstructing DNA copy number by penalized estimation and imputation", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS357 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 4, 1749-1773", "doi": "10.1214/10-AOAS357", "report-no": "IMS-AOAS-AOAS357", "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in genomics have underscored the surprising ubiquity of DNA\ncopy number variation (CNV). Fortunately, modern genotyping platforms also\ndetect CNVs with fairly high reliability. Hidden Markov models and algorithms\nhave played a dominant role in the interpretation of CNV data. Here we explore\nCNV reconstruction via estimation with a fused-lasso penalty as suggested by\nTibshirani and Wang [Biostatistics 9 (2008) 18--29]. We mount a fresh attack on\nthis difficult optimization problem by the following: (a) changing the penalty\nterms slightly by substituting a smooth approximation to the absolute value\nfunction, (b) designing and implementing a new MM (majorization--minimization)\nalgorithm, and (c) applying a fast version of Newton's method to jointly update\nall model parameters. Together these changes enable us to minimize the\nfused-lasso criterion in a highly effective way. We also reframe the\nreconstruction problem in terms of imputation via discrete optimization. This\napproach is easier and more accurate than parameter estimation because it\nrelies on the fact that only a handful of possible copy number states exist at\neach SNP. The dynamic programming framework has the added bonus of exploiting\ninformation that the current fused-lasso approach ignores. The accuracy of our\nimputations is comparable to that of hidden Markov models at a substantially\nlower computational cost.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2009 00:49:35 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2009 18:36:12 GMT"}, {"version": "v3", "created": "Wed, 5 May 2010 22:57:42 GMT"}, {"version": "v4", "created": "Fri, 7 May 2010 00:31:35 GMT"}, {"version": "v5", "created": "Mon, 10 Jan 2011 07:41:29 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Zhang", "Zhongyang", ""], ["Lange", "Kenneth", ""], ["Ophoff", "Roel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "0906.2789", "submitter": "Boudewijn Roukema", "authors": "Boudewijn F. Roukema", "title": "A first-digit anomaly in the 2009 Iranian presidential election", "comments": "36 pages, 19 figures, 11 tables; v2: 3 of 6 biggest cities satisfy\n  K7; v3: simulations give p=0.0007, K7a distribution is odd; v4: more\n  conservative simulation method, pre-election polls analysis added; v5:\n  discussion of election validity claims; v6: minor corrections", "journal-ref": "Journal of Applied Statistics 41 (2014) 164", "doi": "10.1080/02664763.2013.838664", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A local bootstrap method is proposed for the analysis of electoral vote-count\nfirst-digit frequencies, complementing the Benford's Law limit. The method is\ncalibrated on five presidential-election first rounds (2002--2006) and applied\nto the 2009 Iranian presidential-election first round. Candidate K has a highly\nsignificant (p< 0.15%) excess of vote counts starting with the digit 7. This\nleads to other anomalies, two of which are individually significant at p\\sim\n0.1%, and one at p\\sim 1%. Independently, Iranian pre-election opinion polls\nsignificantly reject the official results unless the five polls favouring\ncandidate A are considered alone. If the latter represent normalised data and a\nlinear, least-squares, equal-weighted fit is used, then either candidates R and\nK suffered a sudden, dramatic (70%\\pm 15%) loss of electoral support just prior\nto the election, or the official results are rejected (p\\sim 0.01%).\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 19:52:01 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2009 04:49:09 GMT"}, {"version": "v3", "created": "Fri, 26 Jun 2009 13:47:13 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2012 13:55:58 GMT"}, {"version": "v5", "created": "Sat, 24 Nov 2012 01:55:16 GMT"}, {"version": "v6", "created": "Sun, 2 Jun 2013 22:17:21 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Roukema", "Boudewijn F.", ""]]}, {"id": "0906.2885", "submitter": "Umberto Amato", "authors": "Umberto Amato, Anestis Antoniadis, Alexander Samarov, Alexander\n  Tsybakov", "title": "Noisy Independent Factor Analysis Model for Density Estimation and\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multivariate density estimation when the unknown\ndensity is assumed to follow a particular form of dimensionality reduction, a\nnoisy independent factor analysis (IFA) model. In this model the data are\ngenerated by a number of latent independent components having unknown\ndistributions and are observed in Gaussian noise. We do not assume that either\nthe number of components or the matrix mixing the components are known. We show\nthat the densities of this form can be estimated with a fast rate. Using the\nmirror averaging aggregation algorithm, we construct a density estimator which\nachieves a nearly parametric rate log^(1/4)n/sqrt(n), independent of the\ndimensionality of the data, as the sample size $n$ tends to infinity. This\nestimator is adaptive to the number of components, their distributions and the\nmixing matrix. We then apply this density estimator to construct nonparametric\nplug-in classifiers and show that they achieve the best obtainable rate of the\nexcess Bayes risk, to within a logarithmic factor independent of the dimension\nof the data. Applications of this classifier to simulated data sets and to real\ndata from a remote sensing experiment show promising results.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2009 10:59:35 GMT"}], "update_date": "2009-06-17", "authors_parsed": [["Amato", "Umberto", ""], ["Antoniadis", "Anestis", ""], ["Samarov", "Alexander", ""], ["Tsybakov", "Alexander", ""]]}, {"id": "0906.3090", "submitter": "Patrick Perry", "authors": "Patrick O. Perry, Patrick J. Wolfe", "title": "Minimax rank estimation for subspace tracking", "comments": "10 pages, 4 figures; final version", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing, vol. 4, pp.\n  504-513, 2010", "doi": "10.1109/JSTSP.2010.2048070", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rank estimation is a classical model order selection problem that arises in a\nvariety of important statistical signal and array processing systems, yet is\naddressed relatively infrequently in the extant literature. Here we present\nsample covariance asymptotics stemming from random matrix theory, and bring\nthem to bear on the problem of optimal rank estimation in the context of the\nstandard array observation model with additive white Gaussian noise. The most\nsignificant of these results demonstrates the existence of a phase transition\nthreshold, below which eigenvalues and associated eigenvectors of the sample\ncovariance fail to provide any information on population eigenvalues. We then\ndevelop a decision-theoretic rank estimation framework that leads to a simple\nordered selection rule based on thresholding; in contrast to competing\napproaches, however, it admits asymptotic minimax optimality and is free of\ntuning parameters. We analyze the asymptotic performance of our rank selection\nprocedure and conclude with a brief simulation study demonstrating its\npractical efficacy in the context of subspace tracking.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 07:41:36 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2009 22:53:40 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2010 15:38:56 GMT"}], "update_date": "2011-08-25", "authors_parsed": [["Perry", "Patrick O.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.3108", "submitter": "Stefano M. Iacus", "authors": "Stefano M. Iacus, Nakahiro Yoshida", "title": "Estimation for the change point of the volatility in a stochastic\n  differential equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multidimensional It\\^o process $Y=(Y_t)_{t\\in[0,T]}$ with some\nunknown drift coefficient process $b_t$ and volatility coefficient\n$\\sigma(X_t,\\theta)$ with covariate process $X=(X_t)_{t\\in[0,T]}$, the function\n$\\sigma(x,\\theta)$ being known up to $\\theta\\in\\Theta$. For this model we\nconsider a change point problem for the parameter $\\theta$ in the volatility\ncomponent. The change is supposed to occur at some point $t^*\\in (0,T)$. Given\ndiscrete time observations from the process $(X,Y)$, we propose quasi-maximum\nlikelihood estimation of the change point. We present the rate of convergence\nof the change point estimator and the limit thereoms of aymptotically mixed\ntype.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 08:34:21 GMT"}], "update_date": "2009-06-18", "authors_parsed": [["Iacus", "Stefano M.", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "0906.3287", "submitter": "Aaron Clauset", "authors": "Aaron Clauset and Kristian Skrede Gleditsch", "title": "The developmental dynamics of terrorist organizations", "comments": "28 pages, 8 figures, 4 tables, supplementary material", "journal-ref": "PLOS ONE 7(11): e48633 (2012)", "doi": "10.1371/journal.pone.0048633", "report-no": null, "categories": "physics.soc-ph nlin.AO physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify robust statistical patterns in the frequency and severity of\nviolent attacks by terrorist organizations as they grow and age. Using\ngroup-level static and dynamic analyses of terrorist events worldwide from\n1968-2008 and a simulation model of organizational dynamics, we show that the\nproduction of violent events tends to accelerate with increasing size and\nexperience. This coupling of frequency, experience and size arises from a\nfundamental positive feedback loop in which attacks lead to growth which leads\nto increased production of new attacks. In contrast, event severity is\nindependent of both size and experience. Thus larger, more experienced\norganizations are more deadly because they attack more frequently, not because\ntheir attacks are more deadly, and large events are equally likely to come from\nlarge and small organizations. These results hold across political ideologies\nand time, suggesting that the frequency and severity of terrorism may be\nconstrained by fundamental processes.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2009 23:11:56 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2011 14:42:16 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2012 21:08:50 GMT"}], "update_date": "2012-11-26", "authors_parsed": [["Clauset", "Aaron", ""], ["Gleditsch", "Kristian Skrede", ""]]}, {"id": "0906.3465", "submitter": "Genevera I. Allen", "authors": "Genevera I. Allen, Robert Tibshirani", "title": "Transposable regularized covariance models with an application to\n  missing data imputation", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS314 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 764-790", "doi": "10.1214/09-AOAS314", "report-no": "IMS-AOAS-AOAS314", "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data estimation is an important challenge with high-dimensional data\narranged in the form of a matrix. Typically this data matrix is transposable,\nmeaning that either the rows, columns or both can be treated as features. To\nmodel transposable data, we present a modification of the matrix-variate\nnormal, the mean-restricted matrix-variate normal, in which the rows and\ncolumns each have a separate mean vector and covariance matrix. By placing\nadditive penalties on the inverse covariance matrices of the rows and columns,\nthese so-called transposable regularized covariance models allow for maximum\nlikelihood estimation of the mean and nonsingular covariance matrices. Using\nthese models, we formulate EM-type algorithms for missing data imputation in\nboth the multivariate and transposable frameworks. We present theoretical\nresults exploiting the structure of our transposable models that allow these\nmodels and imputation methods to be applied to high-dimensional data.\nSimulations and results on microarray data and the Netflix data show that these\nimputation techniques often outperform existing methods and offer a greater\ndegree of flexibility.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2009 17:42:31 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2009 20:21:11 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2009 18:09:36 GMT"}, {"version": "v4", "created": "Tue, 9 Nov 2010 07:31:12 GMT"}], "update_date": "2010-11-10", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "0906.3558", "submitter": "Mikhail Simkin", "authors": "M.V. Simkin and V.P. Roychowdhury", "title": "Estimating achievement from fame", "comments": null, "journal-ref": "Significance 8 (2011) 22-26", "doi": "10.1111/j.1740-9713.2011.00473.x", "report-no": null, "categories": "physics.soc-ph cs.CY physics.hist-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report a method for estimating people's achievement based on their fame.\nEarlier we discovered (cond-mat/0310049) that fame of fighter pilot aces\n(measured as number of Google hits) grows exponentially with their achievement\n(number of victories). We hypothesize that the same functional relation between\nachievement and fame holds for other professions. This allows us to estimate\nachievement for professions where an unquestionable and universally accepted\nmeasure of achievement does not exist. We apply the method to Nobel Prize\nwinners in Physics. For example, we obtain that Paul Dirac, who is hundred\ntimes less famous than Einstein contributed to physics only two times less. We\ncompare our results with Landau's ranking.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2009 00:58:22 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2011 06:21:13 GMT"}], "update_date": "2011-03-16", "authors_parsed": [["Simkin", "M. V.", ""], ["Roychowdhury", "V. P.", ""]]}, {"id": "0906.4980", "submitter": "Patrick J. Wolfe", "authors": "Benjamin P. Olding and Patrick J. Wolfe", "title": "Inference for graphs and networks: Extending classical tools to modern\n  data", "comments": "16 pages, 6 figures; submitted for publication", "journal-ref": null, "doi": "10.1142/9781783263752_0001", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphs and networks provide a canonical representation of relational data,\nwith massive network data sets becoming increasingly prevalent across a variety\nof scientific fields. Although tools from mathematics and computer science have\nbeen eagerly adopted by practitioners in the service of network inference, they\ndo not yet comprise a unified and coherent framework for the statistical\nanalysis of large-scale network data. This paper serves as both an introduction\nto the topic and a first step toward formal inference procedures. We develop\nand illustrate our arguments using the example of hypothesis testing for\nnetwork structure. We invoke a generalized likelihood ratio framework and use\nit to highlight the growing number of topics in this area that require strong\ncontributions from statistical science. We frame our discussion in the context\nof previous work from across a variety of disciplines, and conclude by\noutlining fundamental statistical challenges whose solutions will in turn serve\nto advance the science of network inference.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2009 17:20:17 GMT"}], "update_date": "2014-08-11", "authors_parsed": [["Olding", "Benjamin P.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "0906.5546", "submitter": "Palaniappan Vellaisamy", "authors": "P. Vellaisamy", "title": "A-Collapsibility of Distribution Dependence and Quantile Regression\n  Coefficients", "comments": "The paper has fifteen pages and has been accepted for publication in\n  Scandinavian Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Yule-Simpson paradox notes that an association between random variables\ncan be reversed when averaged over a background variable. Cox and Wermuth\n(2003) introduced the concept of distribution dependence between two random\nvariables X and Y, and developed two dependence conditions, each of which\nguarantees that reversal cannot occur. Ma, Xie and Geng (2006) studied the\ncollapsibility of distribution dependence over a background variable W, under a\nrather strong homogeneity condition. Collapsibility ensures the association\nremains the same for conditional and marginal models, so that Yule-Simpson\nreversal cannot occur. In this paper, we investigate a more general condition\nfor avoiding effect reversal: A-collapsibility. The conditions of Cox and\nWermuth imply A-collapsibility, without assuming homogeneity. In fact, we show\nthat, when W is a binary variable, collapsibility is equivalent to\nA-collapsibility plus homogeneity, and A-collapsibility is equivalent to the\nconditions of Cox and Wermuth. Recently, Cox (2007) extended Cochran's result\non regression coefficients of conditional and marginal models, to quantile\nregression coefficients. The conditions of Cox and Wermuth are sufficient for\nA-collapsibility of quantile regression coefficients. If the conditional\ndistribution of W, given Y = y and X = x, belong to one-dimensional natural\nexponential family, they are also necessary. Some applications of\nA-collapsibility include the analysis of a contingency table, linear regression\nmodels and quantile regression models.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2009 14:39:32 GMT"}, {"version": "v2", "created": "Fri, 20 Aug 2010 09:43:16 GMT"}, {"version": "v3", "created": "Tue, 8 Feb 2011 07:55:26 GMT"}, {"version": "v4", "created": "Sat, 8 Oct 2011 14:34:43 GMT"}], "update_date": "2011-10-11", "authors_parsed": [["Vellaisamy", "P.", ""]]}]