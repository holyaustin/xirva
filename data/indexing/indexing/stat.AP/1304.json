[{"id": "1304.0057", "submitter": "Georg Hofmann", "authors": "Georg Hofmann", "title": "Importance sampling for the simulation of reinsurance losses", "comments": null, "journal-ref": "Proceedings of the 2013 Winter Simulation Conference (2013)\n  1025-1034", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is a well developed method in statistics. Given a random\nvariable $X$, the problem of estimating its expected value $\\mu$ is addressed.\nThe standard approach is to use the sample mean as an estimator $\\bar x$. In\nimportance sampling, a suitable variable $L$ is introduced such that the random\nvariable $X/L$ has an estimator with a smaller variance than that of $\\bar x$.\nAs a result, a smaller sample size can lead to the same estimation accuracy.\n  In the simulation of reinsurance financial terms for catastrophe loss,\nchoosing a general variable $L$ is difficult: Even before the application of\nfinancial terms, the loss distribution is often not modelled by a closed-form\ndistribution. After that, a wide range of financial terms can be applied that\nmakes the final distribution unpredictable. However, it is evident that the\nheavy tail of the resulting net loss distribution makes the use of importance\nsampling desirable. We propose an importance sampling technique using a power\nfunction transformation on the cumulative distribution function. The benefit of\nthis technique is that no prior knowledge of the loss distribution is required.\nIt is a new technique that has not been documented in the literature. The\ntransformation depends on the choice of the exponent $k$. For a specific\nexample we investigate desirable values of $k$.\n", "versions": [{"version": "v1", "created": "Sat, 30 Mar 2013 01:10:29 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Hofmann", "Georg", ""]]}, {"id": "1304.0212", "submitter": "Michal Brzezinski", "authors": "Michal Brzezinski", "title": "Do wealth distributions follow power laws? Evidence from \"rich lists\"", "comments": "17 pages, 6 figures, 1 table, 1 appendix", "journal-ref": null, "doi": "10.1016/j.physa.2014.03.052", "report-no": null, "categories": "q-fin.ST physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use data on wealth of the richest persons taken from the \"rich lists\"\nprovided by business magazines like Forbes to verify if upper tails of wealth\ndistributions follow, as often claimed, a power-law behaviour. The data sets\nused cover the world's richest persons over 1996-2012, the richest Americans\nover 1988-2012, the richest Chinese over 2006-2012 and the richest Russians\nover 2004-2011. Using a recently introduced comprehensive empirical methodology\nfor detecting power laws, which allows for testing goodness of fit as well as\nfor comparing the power-law model with rival distributions, we find that a\npower-law model is consistent with data only in 35% of the analysed data sets.\nMoreover, even if wealth data are consistent with the power-law model, usually\nthey are also consistent with some rivals like the log-normal or stretched\nexponential distributions.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 14:45:56 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Brzezinski", "Michal", ""]]}, {"id": "1304.0441", "submitter": "Federico Polito", "authors": "F. Polito, A. Petri, G. Pontuale, F. Dalton", "title": "Analysis of Metal Cutting Acoustic Emissions by Time Series Models", "comments": null, "journal-ref": "The International Journal of Advanced Manufacturing Technology,\n  Vol. 48 (9-12), 897-903, 2010", "doi": "10.1007/s00170-009-2357-4", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse some acoustic emission time series obtained from a lathe machining\nprocess. Considering the dynamic evolution of the process we apply two classes\nof well known stationary stochastic time series models. We apply a preliminary\nroot mean square (RMS) transformation followed by an ARMA analysis; results\nthereof are mainly related to the description of the continuous part (plastic\ndeformation) of the signal. An analysis of acoustic emission, as some previous\nworks show, may also be performed with the scope of understanding the evolution\nof the ageing process that causes the degradation of the working tools. Once\nthe importance of the discrete part of the acoustic emission signals (i.e.\nisolated amplitude bursts) in the ageing process is understood, we apply a\nstochastic analysis based on point processes waiting times between bursts and\nto identify a parameter with which to characterise the wear level of the\nworking tool. A Weibull distribution seems to adequately describe the waiting\ntimes distribution.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 09:00:43 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Polito", "F.", ""], ["Petri", "A.", ""], ["Pontuale", "G.", ""], ["Dalton", "F.", ""]]}, {"id": "1304.0542", "submitter": "Joshua Vogelstein", "authors": "David E. Carlson, Joshua T. Vogelstein, Qisong Wu, Wenzhao Lian,\n  Mingyuan Zhou, Colin R. Stoetzner, Daryl Kipke, Douglas Weber, David B.\n  Dunson, Lawrence Carin", "title": "Multichannel Electrophysiological Spike Sorting via Joint Dictionary\n  Learning & Mixture Modeling", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a construction for joint feature learning and clustering of\nmultichannel extracellular electrophysiological data across multiple recording\nperiods for action potential detection and discrimination (\"spike sorting\").\nOur construction improves over the previous state-of-the art principally in\nfour ways. First, via sharing information across channels, we can better\ndistinguish between single-unit spikes and artifacts. Second, our proposed\n\"focused mixture model\" (FMM) elegantly deals with units appearing,\ndisappearing, or reappearing over multiple recording days, an important\nconsideration for any chronic experiment. Third, by jointly learning features\nand clusters, we improve performance over previous attempts that proceeded via\na two-stage (\"frequentist\") learning process. Fourth, by directly modeling\nspike rate, we improve detection of sparsely spiking neurons. Moreover, our\nBayesian construction seamlessly handles missing data. We present\nstate-of-the-art performance without requiring manually tuning of many\nhyper-parameters on both a public dataset with partial ground truth and a new\nexperimental dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 06:31:17 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 02:50:15 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Carlson", "David E.", ""], ["Vogelstein", "Joshua T.", ""], ["Wu", "Qisong", ""], ["Lian", "Wenzhao", ""], ["Zhou", "Mingyuan", ""], ["Stoetzner", "Colin R.", ""], ["Kipke", "Daryl", ""], ["Weber", "Douglas", ""], ["Dunson", "David B.", ""], ["Carin", "Lawrence", ""]]}, {"id": "1304.0596", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene", "title": "A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process\n  Mixture of Gamma Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model with a Dirichlet process mixture of gamma\ndensities in the bulk part below threshold and a generalized Pareto density in\nthe tail for extreme value estimation. The proposed model is simple and\nflexible allowing us posterior density estimation and posterior inference for\nhigh quantiles. The model works well even for small sample sizes and in the\nabsence of prior information. We evaluate the performance of the proposed model\nthrough a simulation study. Finally, the proposed model is applied to a real\nenvironmental data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 11:49:11 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Fuquene", "Jairo", ""]]}, {"id": "1304.0861", "submitter": "Fabrice Gamboa", "authors": "Ekaterina Sergienko (- M\\'ethodes d'Analyse Stochastique des Codes et\n  Traitements Num\\'eriques, IMT), Fabrice Gamboa (- M\\'ethodes d'Analyse\n  Stochastique des Codes et Traitements Num\\'eriques, IMT), Daniel Busby\n  (IFPEN)", "title": "Shape invariant model approach for functional data analysis in\n  uncertainty and sensitivity studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic simulators model systems evolving over time. Often, it operates\niteratively over fixed number of time-steps. The output of such simulator can\nbe considered as time series or discrete functional outputs. Metamodeling is an\ne ective method to approximate demanding computer codes. Numerous metamodeling\ntechniques are developed for simulators with a single output. Standard approach\nto model a dynamic simulator uses the same method also for multi-time series\noutputs: the metamodel is evaluated independently at every time step. This can\nbe computationally demanding in case of large number of time steps. In some\ncases, simulator outputs for di erent combinations of input parameters have\nquite similar behaviour. In this paper, we propose an application of shape\ninvariant model approach to model dynamic simulators. This model assumes a\ncommon pattern shape curve and curve-specific di erences in amplitude and\ntiming are modelled with linear transformations. We provide an e cient\nalgorithm of transformation parameters estimation and subsequent prediction\nalgorithm. The method was tested with a CO2 storage reservoir case using an\nindustrial commercial simulator and compared with a standard single step\napproach. The method provides satisfactory predictivity and it does not depend\non the number of involved time steps.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 07:54:46 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Sergienko", "Ekaterina", "", "- M\u00e9thodes d'Analyse Stochastique des Codes et\n  Traitements Num\u00e9riques, IMT"], ["Gamboa", "Fabrice", "", "- M\u00e9thodes d'Analyse\n  Stochastique des Codes et Traitements Num\u00e9riques, IMT"], ["Busby", "Daniel", "", "IFPEN"]]}, {"id": "1304.0869", "submitter": "Conrad Sanderson", "authors": "Yongkang Wong, Shaokang Chen, Sandra Mau, Conrad Sanderson, Brian C.\n  Lovell", "title": "Patch-based Probabilistic Image Quality Assessment for Face Selection\n  and Improved Video-based Face Recognition", "comments": null, "journal-ref": "IEEE Conference on Computer Vision and Pattern Recognition\n  Workshops (CVPRW), pp. 74-81, 2011", "doi": "10.1109/CVPRW.2011.5981881", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In video based face recognition, face images are typically captured over\nmultiple frames in uncontrolled conditions, where head pose, illumination,\nshadowing, motion blur and focus change over the sequence. Additionally,\ninaccuracies in face localisation can also introduce scale and alignment\nvariations. Using all face images, including images of poor quality, can\nactually degrade face recognition performance. While one solution it to use\nonly the \"best\" subset of images, current face selection techniques are\nincapable of simultaneously handling all of the abovementioned issues. We\npropose an efficient patch-based face image quality assessment algorithm which\nquantifies the similarity of a face image to a probabilistic face model,\nrepresenting an \"ideal\" face. Image characteristics that affect recognition are\ntaken into account, including variations in geometric alignment (shift,\nrotation and scale), sharpness, head pose and cast shadows. Experiments on\nFERET and PIE datasets show that the proposed algorithm is able to identify\nimages which are simultaneously the most frontal, aligned, sharp and well\nilluminated. Further experiments on a new video surveillance dataset (termed\nChokePoint) show that the proposed method provides better face subsets than\nexisting face selection techniques, leading to significant improvements in\nrecognition accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 08:41:23 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 15:53:31 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Wong", "Yongkang", ""], ["Chen", "Shaokang", ""], ["Mau", "Sandra", ""], ["Sanderson", "Conrad", ""], ["Lovell", "Brian C.", ""]]}, {"id": "1304.1039", "submitter": "Sears Merritt", "authors": "Sears Merritt and Aaron Clauset", "title": "Environmental structure and competitive scoring advantages in team\n  competitions", "comments": "Main Text: 8 pages, 4 figures, 2 tables; Supplementary Information:\n  12 pages, 13 figures, 9 tables", "journal-ref": "Scientific Reports 3, 3067 (2013)", "doi": "10.1038/srep03067", "report-no": null, "categories": "physics.soc-ph cs.SI physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most professional sports, the structure of the environment is kept neutral\nso that scoring imbalances may be attributed to differences in team skill. It\nthus remains unknown what impact structural heterogeneities can have on scoring\ndynamics and producing competitive advantages. Applying a generative model of\nscoring dynamics to roughly 10 million team competitions drawn from an online\ngame, we quantify the relationship between a competition's structure and its\nscoring dynamics. Despite wide structural variations, we find the same\nthree-phase pattern in the tempo of events observed in many sports. Tempo and\nbalance are highly predictable from a competition's structural features alone\nand teams exploit environmental heterogeneities for sustained competitive\nadvantage. The most balanced competitions are associated with specific\nenvironmental heterogeneities, not from equally skilled teams. These results\nshed new light on the principles of balanced competition, and illustrate the\npotential of online game data for investigating social dynamics and\ncompetition.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 18:13:56 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Merritt", "Sears", ""], ["Clauset", "Aaron", ""]]}, {"id": "1304.1199", "submitter": "David Van Leeuwen", "authors": "David A. van Leeuwen and Niko Br\\\"ummer", "title": "The distribution of calibrated likelihood-ratios in speaker recognition", "comments": "Accepted to Interspeech 2013, fixed legend of fig 2", "journal-ref": "PROC INTERSPEECH 2013, ISSN 2308-457X, pp 1619-1623", "doi": null, "report-no": null, "categories": "stat.AP cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies properties of the score distributions of calibrated\nlog-likelihood-ratios that are used in automatic speaker recognition. We derive\nthe essential condition for calibration that the log likelihood ratio of the\nlog-likelihood-ratio is the log-likelihood-ratio. We then investigate what the\nconsequence of this condition is to the probability density functions (PDFs) of\nthe log-likelihood-ratio score. We show that if the PDF of the non-target\ndistribution is Gaussian, then the PDF of the target distribution must be\nGaussian as well. The means and variances of these two PDFs are interrelated,\nand determined completely by the discrimination performance of the recognizer\ncharacterized by the equal error rate. These relations allow for a new way of\ncomputing the offset and scaling parameters for linear calibration, and we\nderive closed-form expressions for these and show that for modern i-vector\nsystems with PLDA scoring this leads to good calibration, comparable to\ntraditional logistic regression, over a wide range of system performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 22:00:17 GMT"}, {"version": "v2", "created": "Mon, 27 May 2013 22:03:59 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2013 14:51:07 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["van Leeuwen", "David A.", ""], ["Br\u00fcmmer", "Niko", ""]]}, {"id": "1304.1565", "submitter": "Muhammad Asim Mubeen", "authors": "Asim M. Mubeen, Kevin H. Knuth", "title": "Bayesian Odds-Ratio Filters: A Template-Based Method for Online\n  Detection of P300 Evoked Responses", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC physics.med-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Template-based signal detection most often relies on computing a correlation,\nor a dot product, between an incoming data stream and a signal template. While\nsuch a correlation results in an ongoing estimate of the magnitude of the\nsignal in the data stream, it does not directly indicate the presence or\nabsence of a signal. Instead, the problem of signal detection is one of\nmodel-selection. Here we explore the use of the Bayesian odds-ratio (OR), which\nis the ratio of posterior probabilities of a signal-plus-noise model over a\nnoise-only model. We demonstrate this method by applying it to simulated\nelectroencephalographic (EEG) signals based on the P300 response, which is\nwidely used in both Brain Computer Interface (BCI) and Brain Machine Interface\n(BMI) systems. The efficacy of this algorithm is demonstrated by comparing the\nreceiver operating characteristic (ROC) curves of the OR-based (logOR) filter\nto the usual correlation method where we find a significant improvement in P300\ndetection. The logOR filter promises to improve the accuracy and speed of the\ndetection of evoked brain responses in BCI/BMI applications as well the\ndetection of template signals in general.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 21:27:40 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Mubeen", "Asim M.", ""], ["Knuth", "Kevin H.", ""]]}, {"id": "1304.1756", "submitter": "A.C. Thomas", "authors": "Michael A. Pane, Samuel L. Ventura, Rebecca C. Steorts, A.C. Thomas", "title": "Trouble With The Curve: Improving MLB Pitch Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PITCHf/x database has allowed the statistical analysis of of Major League\nBaseball (MLB) to flourish since its introduction in late 2006. Using PITCHf/x,\npitches have been classified by hand, requiring considerable effort, or using\nneural network clustering and classification, which is often difficult to\ninterpret. To address these issues, we use model-based clustering with a\nmultivariate Gaussian mixture model and an appropriate adjustment factor as an\nalternative to current methods. Furthermore, we describe a new pitch\nclassification algorithm based on our clustering approach to address the\nproblems of pitch misclassification. We illustrate our methods for various\npitchers from the PITCHf/x database that covers a wide variety of pitch types.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 16:27:49 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Pane", "Michael A.", ""], ["Ventura", "Samuel L.", ""], ["Steorts", "Rebecca C.", ""], ["Thomas", "A. C.", ""]]}, {"id": "1304.1834", "submitter": "Yongtao  Guan", "authors": "Yongtao Guan", "title": "Detecting the structure of haplotypes, local ancestry and excessive\n  local European ancestry in Mexicans", "comments": "28 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a two-layer hidden Markov model to detect structure of haplotypes\nfor unrelated individuals. This allows modeling two scales of linkage\ndisequilibrium (one within a group of haplotypes and one between groups),\nthereby taking advantage of rich haplotype information to infer local ancestry\nfor admixed individuals. Our method outperforms competing state-of-art methods,\nparticularly for regions of small ancestral track lengths. Applying our method\nto Mexican samples in HapMap3, we found five coding regions, ranging from $0.3\n-1.3$ megabase (Mb) in lengths, that exhibit excessive European ancestry\n(average dosage > 1.6). A particular interesting region of 1.1Mb (with average\ndosage 1.95) locates on Chromosome 2p23 that harbors two genes, PXDN and MYT1L,\nboth of which are associated with autism and schizophrenia. In light of the low\nprevalence of autism in Hispanics, this region warrants special attention. We\nconfirmed our findings using Mexican samples from the 1000 genomes project. A\nsoftware package implementing methods described in the paper is freely\navailable at \\url{http://bcm.edu/cnrc/mcmcmc}\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 23:24:42 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Guan", "Yongtao", ""]]}, {"id": "1304.1839", "submitter": "Radu Balan", "authors": "Radu Balan", "title": "Reconstruction of Signals from Magnitudes of Redundant Representations:\n  The Complex Case", "comments": "updated 6 Apr. 2013 version arXiv:1304.1839: to appear in Foundations\n  of Computational Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the question of reconstructing a vector in a\nfinite-dimensional complex Hilbert space when only the magnitudes of the\ncoefficients of the vector under a redundant linear map are known. We present\nnew invertibility results as well an iterative algorithm that finds the\nleast-square solution and is robust in the presence of noise. We analyze its\nnumerical performance by comparing it to the Cramer-Rao lower bound.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 00:45:13 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 17:52:56 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Balan", "Radu", ""]]}, {"id": "1304.1875", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon and Jean-Yves Tourneret and C\\'edric Richard and\n  Jos\\'e C. M. Bermudez and Stephen McLaughlin and Alfred O. Hero", "title": "Nonlinear unmixing of hyperspectral images: models and algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2013.2279274", "report-no": null, "categories": "physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering the problem of unmixing hyperspectral images, most of the\nliterature in the geoscience and image processing areas relies on the widely\nused linear mixing model (LMM). However, the LMM may be not valid and other\nnonlinear models need to be considered, for instance, when there are\nmulti-scattering effects or intimate interactions. Consequently, over the last\nfew years, several significant contributions have been proposed to overcome the\nlimitations inherent in the LMM. In this paper, we present an overview of\nrecent advances in nonlinear unmixing modeling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 10:21:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 14:01:41 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["Richard", "C\u00e9dric", ""], ["Bermudez", "Jos\u00e9 C. M.", ""], ["McLaughlin", "Stephen", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1304.2129", "submitter": "Mikkel Meyer Andersen", "authors": "Mikkel Meyer Andersen, Poul Svante Eriksen, Niels Morling", "title": "A gentle introduction to the discrete Laplace method for estimating\n  Y-STR haplotype frequencies", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Y-STR data simulated under a Fisher-Wright model of evolution with a\nsingle-step mutation model turns out to be well predicted by a method using\ndiscrete Laplace distributions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 08:07:19 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2013 13:29:01 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2013 17:00:37 GMT"}, {"version": "v4", "created": "Wed, 16 Oct 2013 16:47:02 GMT"}], "update_date": "2013-10-17", "authors_parsed": [["Andersen", "Mikkel Meyer", ""], ["Eriksen", "Poul Svante", ""], ["Morling", "Niels", ""]]}, {"id": "1304.2293", "submitter": "Aurelien Latouche", "authors": "Arthur Allignol, Jan Beyersmann, Thomas Gerds, Aur\\'elien Latouche\n  (CEDRIC)", "title": "A competing risks approach for nonparametric estimation of transition\n  probabilities in a non-Markov illness-death model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks model time to first event and type of first event. An example\nfrom hospital epidemiology is the incidence of hospital-acquired infection,\nwhich has to account for hospital discharge of non-infected patients as a\ncompeting risk. An illness-death model would allow to further study hospital\noutcomes of infected patients. Such a model typically relies on a Markov\nassumption. However, it is conceivable that the future course of an infected\npatient does not only depend on the time since hospital admission and current\ninfection status but also on the time since infection. We demonstrate how a\nmodified competing risks model can be used for nonparametric estimation of\ntransition probabilities when the Markov assumption is violated.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:21:16 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Allignol", "Arthur", "", "CEDRIC"], ["Beyersmann", "Jan", "", "CEDRIC"], ["Gerds", "Thomas", "", "CEDRIC"], ["Latouche", "Aur\u00e9lien", "", "CEDRIC"]]}, {"id": "1304.2312", "submitter": "Chen Yue", "authors": "Chen Yue, Vadim Zipunnikov, Pierre-Louis Bazin, Dzung Pham, Daniel\n  Reich, Ciprian Crainiceanu, Brian Caffo", "title": "Parametrization of white matter manifold-like structures using principal\n  surfaces", "comments": "27 pages, 5 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we are concerned with data generated from a diffusion\ntensor imaging (DTI) experiment. The goal is to parameterize manifold-like\nwhite matter tracts, such as the corpus callosum, using principal surfaces. We\napproach the problem by finding a geometrically motivated surface-based\nrepresentation of the corpus callosum and visualize the fractional anisotropy\n(FA) values projected onto the surface; the method applies to any other\ndiffusion summary as well as to other white matter tracts. We provide an\nalgorithm that 1) constructs the principal surface of a corpus callosum; 2)\nflattens the surface into a parametric 2D map; 3) projects associated FA values\non the map. The algorithm was applied to a longitudinal study containing 466\ndiffusion tensor images of 176 multiple sclerosis (MS) patients observed at\nmultiple visits. For each subject and visit the study contains a registered DTI\nscan of the corpus callosum at roughly 20,000 voxels. Extensive simulation\nstudies demonstrate fast convergence and robust performance of the algorithm\nunder a variety of challenging scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:58:04 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Yue", "Chen", ""], ["Zipunnikov", "Vadim", ""], ["Bazin", "Pierre-Louis", ""], ["Pham", "Dzung", ""], ["Reich", "Daniel", ""], ["Crainiceanu", "Ciprian", ""], ["Caffo", "Brian", ""]]}, {"id": "1304.2331", "submitter": "Niko Br\\\"ummer", "authors": "Niko Brummer and Johan du Preez", "title": "The PAV algorithm optimizes binary proper scoring rules", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been much recent interest in application of the\npool-adjacent-violators (PAV) algorithm for the purpose of calibrating the\nprobabilistic outputs of automatic pattern recognition and machine learning\nalgorithms. Special cost functions, known as proper scoring rules form natural\nobjective functions to judge the goodness of such calibration. We show that for\nbinary pattern classifiers, the non-parametric optimization of calibration,\nsubject to a monotonicity constraint, can be solved by PAV and that this\nsolution is optimal for all regular binary proper scoring rules. This extends\nprevious results which were limited to convex binary proper scoring rules. We\nfurther show that this result holds not only for calibration of probabilities,\nbut also for calibration of log-likelihood-ratios, in which case optimality\nholds independently of the prior probabilities of the pattern classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 19:49:51 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Brummer", "Niko", ""], ["Preez", "Johan du", ""]]}, {"id": "1304.2499", "submitter": "Nicolas Dobigeon", "authors": "Yoann Altmann and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Unsupervised Post-Nonlinear Unmixing of Hyperspectral Images Using a\n  Hamiltonian Monte Carlo Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2314022", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a nonlinear mixing model for hyperspectral image\nunmixing. The proposed model assumes that the pixel reflectances are\npost-nonlinear functions of unknown pure spectral components contaminated by an\nadditive white Gaussian noise. These nonlinear functions are approximated using\npolynomials leading to a polynomial post-nonlinear mixing model. A Bayesian\nalgorithm is proposed to estimate the parameters involved in the model yielding\nan unsupervised nonlinear unmixing algorithm. Due to the large number of\nparameters to be estimated, an efficient Hamiltonian Monte Carlo algorithm is\ninvestigated. The classical leapfrog steps of this algorithm are modified to\nhandle the parameter constraints. The performance of the unmixing strategy,\nincluding convergence and parameter tuning, is first evaluated on synthetic\ndata. Simulations conducted with real data finally show the accuracy of the\nproposed unmixing strategy for the analysis of hyperspectral images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 09:23:20 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Altmann", "Yoann", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1304.2828", "submitter": "Eric Bair", "authors": "Emily Colby and Eric Bair", "title": "Cross-Validation for Nonlinear Mixed Effects Models", "comments": "38 pages, 15 figures To be published in the Journal of\n  Pharmacokinetics and Pharmacodynamics", "journal-ref": "Journal of Pharmacokinetics and Pharmacodynamics, April 2013,\n  40(2): 243-252", "doi": "10.1007/s10928-013-9313-5", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is frequently used for model selection in a variety of\napplications. However, it is difficult to apply cross-validation to mixed\neffects models (including nonlinear mixed effects models or NLME models) due to\nthe fact that cross-validation requires \"out-of-sample\" predictions of the\noutcome variable, which cannot be easily calculated when random effects are\npresent. We describe two novel variants of cross-validation that can be applied\nto nonlinear mixed effects models. One variant, where out-of-sample predictions\nare based on post hoc estimates of the random effects, can be used to select\nthe overall structural model. Another variant, where cross-validation seeks to\nminimize the estimated random effects rather than the estimated residuals, can\nbe used to select covariates to include in the model. We show that these\nmethods produce accurate results in a variety of simulated data sets and apply\nthem to two publicly available population pharmacokinetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 01:57:21 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Colby", "Emily", ""], ["Bair", "Eric", ""]]}, {"id": "1304.2863", "submitter": "Rene Andrae", "authors": "Rene Andrae, Dae-Won Kim, Coryn A.L. Bailer-Jones", "title": "Assessment of stochastic and deterministic models of 6304 quasar\n  lightcurves from SDSS Stripe 82", "comments": "accepted by AA, 12 pages, 11 figures, 4 tables", "journal-ref": null, "doi": "10.1051/0004-6361/201321335", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optical light curves of many quasars show variations of tenths of a\nmagnitude or more on time scales of months to years. This variation often\ncannot be described well by a simple deterministic model. We perform a Bayesian\ncomparison of over 20 deterministic and stochastic models on 6304 QSO light\ncurves in SDSS Stripe 82. We include the damped random walk (or\nOrnstein-Uhlenbeck [OU] process), a particular type of stochastic model which\nrecent studies have focused on. Further models we consider are single and\ndouble sinusoids, multiple OU processes, higher order continuous autoregressive\nprocesses, and composite models. We find that only 29 out of 6304 QSO\nlightcurves are described significantly better by a deterministic model than a\nstochastic one. The OU process is an adequate description of the vast majority\nof cases (6023). Indeed, the OU process is the best single model for 3462 light\ncurves, with the composite OU process/sinusoid model being the best in 1706\ncases. The latter model is the dominant one for brighter/bluer QSOs.\nFurthermore, a non-negligible fraction of QSO lightcurves show evidence that\nnot only the mean is stochastic but the variance is stochastic, too. Our\nresults confirm earlier work that QSO light curves can be described with a\nstochastic model, but place this on a firmer footing, and further show that the\nOU process is preferred over several other stochastic and deterministic models.\nOf course, there may well exist yet better (deterministic or stochastic) models\nwhich have not been considered here.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 07:30:46 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Andrae", "Rene", ""], ["Kim", "Dae-Won", ""], ["Bailer-Jones", "Coryn A. L.", ""]]}, {"id": "1304.2865", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer and Edward de Villiers", "title": "The BOSARIS Toolkit: Theory, Algorithms and Code for Surviving the New\n  DCF", "comments": "presented at: The NIST SRE'11 Analysis Workshop, Atlanta, December\n  2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The change of two orders of magnitude in the 'new DCF' of NIST's SRE'10,\nrelative to the 'old DCF' evaluation criterion, posed a difficult challenge for\nparticipants and evaluator alike. Initially, participants were at a loss as to\nhow to calibrate their systems, while the evaluator underestimated the required\nnumber of evaluation trials. After the fact, it is now obvious that both\ncalibration and evaluation require very large sets of trials. This poses the\nchallenges of (i) how to decide what number of trials is enough, and (ii) how\nto process such large data sets with reasonable memory and CPU requirements.\nAfter SRE'10, at the BOSARIS Workshop, we built solutions to these problems\ninto the freely available BOSARIS Toolkit. This paper explains the principles\nand algorithms behind this toolkit. The main contributions of the toolkit are:\n1. The Normalized Bayes Error-Rate Plot, which analyses likelihood- ratio\ncalibration over a wide range of DCF operating points. These plots also help in\njudging the adequacy of the sizes of calibration and evaluation databases. 2.\nEfficient algorithms to compute DCF and minDCF for large score files, over the\nrange of operating points required by these plots. 3. A new score file format,\nwhich facilitates working with very large trial lists. 4. A faster logistic\nregression optimizer for fusion and calibration. 5. A principled way to define\nEER (equal error rate), which is of practical interest when the absolute error\ncount is small.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 07:32:31 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["Br\u00fcmmer", "Niko", ""], ["de Villiers", "Edward", ""]]}, {"id": "1304.2955", "submitter": "Arindam RoyChoudhury", "authors": "Arindam RoyChoudhury", "title": "Change in Recessive Lethal Alleles Frequency in Inbred Populations", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a population practicing consanguineous marriage, rare recessive lethal\nalleles (RRLA) have higher chances of affecting phenotypes. As inbreeding\ncauses more homozygosity and subsequently more deaths, the loss of individuals\nwith RRLA decreases the frequency of these alleles. Although this phenomenon is\nwell studied in general, here some hitherto unstudied cases are presented. An\nanalytical formula for the RRLA frequency is presented for infinite monoecious\npopulation practicing several different types of inbreeding. In finite diecious\npopulations, it is found that more severe inbreeding leads to quicker RRLA\nlosses, making the upcoming generations healthier. A population of size 10,000\npracticing 30% half-sib marriages loses more than 95% of its RRLA in 100\ngenerations; a population practicing 30% cousin marriages loses about 75% of\nits RRLA. Our findings also suggest that given enough resources to grow, a\nsmall inbred population will be able to rebound while losing the RRLA.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 13:39:06 GMT"}], "update_date": "2013-04-11", "authors_parsed": [["RoyChoudhury", "Arindam", ""]]}, {"id": "1304.3160", "submitter": "Rori Rohlfs", "authors": "Rori V. Rohlfs, Erin Murphy, Yun S. Song, Montgomery Slatkin", "title": "The influence of relatives on the efficiency and error rate of familial\n  searching", "comments": "main text: 19 pages, 4 tables, 2 figures supplemental text: 2 pages,\n  5 tables all together as single file", "journal-ref": "PLoS ONE 8(8): e70495 (2013)", "doi": "10.1371/journal.pone.0070495", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the consequences of adopting the criteria used by the state of\nCalifornia, as described by Myers et al. (2011), for conducting familial\nsearches. We carried out a simulation study of randomly generated profiles of\nrelated and unrelated individuals with 13-locus CODIS genotypes and YFiler\nY-chromosome haplotypes, on which the Myers protocol for relative\nidentification was carried out. For Y-chromosome sharing first degree\nrelatives, the Myers protocol has a high probability (80 - 99%) of identifying\ntheir relationship. For unrelated individuals, there is a low probability that\nan unrelated person in the database will be identified as a first-degree\nrelative. For more distant Y-haplotype sharing relatives (half-siblings, first\ncousins, half-first cousins or second cousins) there is a substantial\nprobability that the more distant relative will be incorrectly identified as a\nfirst-degree relative. For example, there is a 3 - 18% probability that a first\ncousin will be identified as a full sibling, with the probability depending on\nthe population background. Although the California familial search policy is\nlikely to identify a first degree relative if his profile is in the database,\nand it poses little risk of falsely identifying an unrelated individual in a\ndatabase as a first-degree relative, there is a substantial risk of falsely\nidentifying a more distant Y-haplotype sharing relative in the database as a\nfirst-degree relative, with the consequence that their immediate family may\nbecome the target for further investigation. This risk falls disproportionately\non those ethnic groups that are currently overrepresented in state and federal\ndatabases.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 22:53:47 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2013 22:16:03 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Rohlfs", "Rori V.", ""], ["Murphy", "Erin", ""], ["Song", "Yun S.", ""], ["Slatkin", "Montgomery", ""]]}, {"id": "1304.3347", "submitter": "Thomas Opitz", "authors": "T. Opitz, P. Tramini, N. Molinari", "title": "Spline regression for zero-inflated models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a regression model for count data when the classical generalized\nlinear model approach is too rigid due to a high outcome of zero counts and a\nnonlinear influence of continuous covariates. Zero-Inflation is applied to take\ninto account the presence of excess zeros with separate link functions for the\nzero and the nonzero component. Nonlinearity in covariates is captured by\nspline functions based on B-splines. Our algorithm relies on maximum-likelihood\nestimation and allows for adaptive box-constrained knots, thus improving the\ngoodness of the spline fit and allowing for detection of sensitivity\nchangepoints. A simulation study substantiates the numerical stability of the\nalgorithm to infer such models. The AIC criterion is shown to serve well for\nmodel selection, in particular if nonlinearities are weak such that BIC tends\nto overly simplistic models. We fit the introduced models to real data of\nchildren's dental sanity, linking caries counts with the so-called\nBody-Mass-Index (BMI) and other socioeconomic factors. This reveals a puzzling\nnonmonotonic influence of BMI on caries counts which is yet to be explained by\nclinical experts.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 15:48:02 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Opitz", "T.", ""], ["Tramini", "P.", ""], ["Molinari", "N.", ""]]}, {"id": "1304.3411", "submitter": "Seyed Hamed Alemohammad", "authors": "Reza Ardakanian, Seyed Hamed Alemohammad", "title": "Global Warming and Caspian Sea Level Fluctuations", "comments": "10 pages, 3 Figures, Proceedings of The First International\n  Conference on Water Resources and Climate Change in the MENA Region, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coastal regions have a high social, economical and environmental importance.\nDue to this importance the sea level fluctuations can have many bad\nconsequences. In this research the correlation between the increasing trend of\ntemperature in coastal stations due to Global Warming and the Caspian Sea level\nhas been established. The Caspian Sea level data has been received from the\nJason-1 satellite. It was resulted that the monthly correlation between the\ntemperature and sea level is high and also positive and almost the same for all\nthe stations. But the yearly correlation was negative. It means that the sea\nlevel has decreased by the increase in temperature.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 19:45:07 GMT"}, {"version": "v2", "created": "Mon, 23 Jun 2014 15:20:21 GMT"}], "update_date": "2014-06-24", "authors_parsed": [["Ardakanian", "Reza", ""], ["Alemohammad", "Seyed Hamed", ""]]}, {"id": "1304.3480", "submitter": "Nathan Hodas", "authors": "Nathan O. Hodas, Farshad Kooti, Kristina Lerman", "title": "Friendship Paradox Redux: Your Friends Are More Interesting Than You", "comments": "Accepted to ICWSM 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY nlin.AO physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feld's friendship paradox states that \"your friends have more friends than\nyou, on average.\" This paradox arises because extremely popular people, despite\nbeing rare, are overrepresented when averaging over friends. Using a sample of\nthe Twitter firehose, we confirm that the friendship paradox holds for >98% of\nTwitter users. Because of the directed nature of the follower graph on Twitter,\nwe are further able to confirm more detailed forms of the friendship paradox:\neveryone you follow or who follows you has more friends and followers than you.\nThis is likely caused by a correlation we demonstrate between Twitter activity,\nnumber of friends, and number of followers. In addition, we discover two new\nparadoxes: the virality paradox that states \"your friends receive more viral\ncontent than you, on average,\" and the activity paradox, which states \"your\nfriends are more active than you, on average.\" The latter paradox is important\nin regulating online communication. It may result in users having difficulty\nmaintaining optimal incoming information rates, because following additional\nusers causes the volume of incoming tweets to increase super-linearly. While\nusers may compensate for increased information flow by increasing their own\nactivity, users become information overloaded when they receive more\ninformation than they are able or willing to process. We compare the average\nsize of cascades that are sent and received by overloaded and underloaded\nusers. And we show that overloaded users post and receive larger cascades and\nthey are poor detector of small cascades.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 20:30:24 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hodas", "Nathan O.", ""], ["Kooti", "Farshad", ""], ["Lerman", "Kristina", ""]]}, {"id": "1304.3568", "submitter": "Pierre Chainais", "authors": "Pierre Chainais and C\\'edric Richard", "title": "Distributed dictionary learning over a sensor network", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of distributed dictionary learning, where a set of\nnodes is required to collectively learn a common dictionary from noisy\nmeasurements. This approach may be useful in several contexts including sensor\nnetworks. Diffusion cooperation schemes have been proposed to solve the\ndistributed linear regression problem. In this work we focus on a\ndiffusion-based adaptive dictionary learning strategy: each node records\nobservations and cooperates with its neighbors by sharing its local dictionary.\nThe resulting algorithm corresponds to a distributed block coordinate descent\n(alternate optimization). Beyond dictionary learning, this strategy could be\nadapted to many matrix factorization problems and generalized to various\nsettings. This article presents our approach and illustrates its efficiency on\nsome numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 08:47:38 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Chainais", "Pierre", ""], ["Richard", "C\u00e9dric", ""]]}, {"id": "1304.3589", "submitter": "Niko Br\\\"ummer", "authors": "Niko Br\\\"ummer", "title": "Tutorial for Bayesian forensic likelihood ratio", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian paradigm for presenting forensic evidence to court, it is\nrecommended that the weight of the evidence be summarized as a likelihood ratio\n(LR) between two opposing hypotheses of how the evidence could have been\nproduced. Such LRs are necessarily based on probabilistic models, the\nparameters of which may be uncertain. It has been suggested by some authors\nthat the value of the LR, being a function of the model parameters should\ntherefore also be considered uncertain and that this uncertainty should be\ncommunicated to the court. In this tutorial, we consider a simple example of a\n'fully Bayesian' solution, where model uncertainty is integrated out to produce\na value for the LR which is not uncertain. We show that this solution agrees\nwith common sense. In particular, the LR magnitude is a function of the amount\nof data that is available to estimate the model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 10:09:49 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Br\u00fcmmer", "Niko", ""]]}, {"id": "1304.3637", "submitter": "Trevor Bedford", "authors": "Trevor Bedford, Marc A. Suchard, Philippe Lemey, Gytis Dudas, Victoria\n  Gregory, Alan J. Hay, John W. McCauley, Colin A. Russell, Derek J. Smith,\n  Andrew Rambaut", "title": "Integrating influenza antigenic dynamics with molecular evolution", "comments": "32 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza viruses undergo continual antigenic evolution allowing mutant\nviruses to evade host immunity acquired to previous virus strains. Antigenic\nphenotype is often assessed through pairwise measurement of cross-reactivity\nbetween influenza strains using the hemagglutination inhibition (HI) assay.\nHere, we extend previous approaches to antigenic cartography, and\nsimultaneously characterize antigenic and genetic evolution by modeling the\ndiffusion of antigenic phenotype over a shared virus phylogeny. Using HI data\nfrom influenza lineages A/H3N2, A/H1N1, B/Victoria and B/Yamagata, we determine\npatterns of antigenic drift across viral lineages, showing that A/H3N2 evolves\nfaster and in a more punctuated fashion than other influenza lineages. We also\nshow that year-to-year antigenic drift appears to drive incidence patterns\nwithin each influenza lineage. This work makes possible substantial future\nadvances in investigating the dynamics of influenza and other\nantigenically-variable pathogens by providing a model that intimately combines\nmolecular and antigenic evolution.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 14:00:27 GMT"}, {"version": "v2", "created": "Fri, 20 Dec 2013 01:05:38 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Bedford", "Trevor", ""], ["Suchard", "Marc A.", ""], ["Lemey", "Philippe", ""], ["Dudas", "Gytis", ""], ["Gregory", "Victoria", ""], ["Hay", "Alan J.", ""], ["McCauley", "John W.", ""], ["Russell", "Colin A.", ""], ["Smith", "Derek J.", ""], ["Rambaut", "Andrew", ""]]}, {"id": "1304.3760", "submitter": "Eric Bair", "authors": "Sheila Gaynor and Eric Bair", "title": "Identification of relevant subtypes via preweighted sparse clustering", "comments": "Version 4: 49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods are used to identify homogeneous subgroups in a data\nset. In biomedical applications, one frequently applies cluster analysis in\norder to identify biologically interesting subgroups. In particular, one may\nwish to identify subgroups that are associated with a particular outcome of\ninterest. Conventional clustering methods generally do not identify such\nsubgroups, particularly when there are a large number of high-variance features\nin the data set. Conventional methods may identify clusters associated with\nthese high-variance features when one wishes to obtain secondary clusters that\nare more interesting biologically or more strongly associated with a particular\noutcome of interest. A modification of sparse clustering can be used to\nidentify such secondary clusters or clusters associated with an outcome of\ninterest. This method correctly identifies such clusters of interest in several\nsimulation scenarios. The method is also applied to a large prospective cohort\nstudy of temporomandibular disorders and a leukemia microarray data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 02:15:20 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2013 14:14:13 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 23:59:53 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gaynor", "Sheila", ""], ["Bair", "Eric", ""]]}, {"id": "1304.3800", "submitter": "Luca Martino", "authors": "Luca Martino and David Luengo", "title": "Extremely efficient generation of Gamma random variables for \\alpha >= 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gamma distribution is well-known and widely used in many signal\nprocessing and communications applications. In this letter, a simple and\nextremely efficient accept/reject algorithm is introduced for the generation of\nindependent random variables from a Gamma distribution with any shape parameter\n\\alpha >= 1. The proposed method uses another Gamma distribution with integer\n\\alpha_p <= \\alpha, from which samples can be easily drawn, as proposal\nfunction. For this reason, the new technique attains a higher acceptance rate\n(AR) for \\alpha >= 3 than all the methods currently available in the\nliterature, with AR tends to 1 as \\alpha\\ diverges.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 11:31:37 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 15:01:39 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2013 23:14:41 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Martino", "Luca", ""], ["Luengo", "David", ""]]}, {"id": "1304.3838", "submitter": "Eric Bair", "authors": "Eric Bair", "title": "Identification of significant features in DNA microarray data", "comments": "35 pages, 6 figures. To be published in WIREs Computational\n  Statistics", "journal-ref": "WIREs Comp Stat, 2013, 5(4): 309-325", "doi": "10.1002/wics.1260", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA microarrays are a relatively new technology that can simultaneously\nmeasure the expression level of thousands of genes. They have become an\nimportant tool for a wide variety of biological experiments. One of the most\ncommon goals of DNA microarray experiments is to identify genes associated with\nbiological processes of interest. Conventional statistical tests often produce\npoor results when applied to microarray data due to small sample sizes, noisy\ndata, and correlation among the expression levels of the genes. Thus, novel\nstatistical methods are needed to identify significant genes in DNA microarray\nexperiments. This article discusses the challenges inherent in DNA microarray\nanalysis and describes a series of statistical techniques that can be used to\novercome these challenges. The problem of multiple hypothesis testing and its\nrelation to microarray studies is also considered, along with several possible\nsolutions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 19:56:50 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Bair", "Eric", ""]]}, {"id": "1304.3839", "submitter": "Eric Bair", "authors": "Naomi Brownstein, Jianwen Cai, Gary Slade, and Eric Bair", "title": "Parameter estimation in Cox models with missing failure indicators and\n  the OPPERA study", "comments": "Version 4: 23 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a prospective cohort study, examining all participants for incidence of\nthe condition of interest may be prohibitively expensive. For example, the\n\"gold standard\" for diagnosing temporomandibular disorder (TMD) is a physical\nexamination by a trained clinician. In large studies, examining all\nparticipants in this manner is infeasible. Instead, it is common to use\nquestionnaires to screen for incidence of TMD and perform the \"gold standard\"\nexamination only on participants who screen positively. Unfortunately, some\nparticipants may leave the study before receiving the \"gold standard\"\nexamination. Within the framework of survival analysis, this results in missing\nfailure indicators. Motivated by the Orofacial Pain: Prospective Evaluation and\nRisk Assessment (OPPERA) study, a large cohort study of TMD, we propose a\nmethod for parameter estimation in survival models with missing failure\nindicators. We estimate the probability of being an incident case for those\nlacking a \"gold standard\" examination using logistic regression. These\nestimated probabilities are used to generate multiple imputations of case\nstatus for each missing examination that are combined with observed data in\nappropriate regression models. The variance introduced by the procedure is\nestimated using multiple imputation. The method can be used to estimate both\nregression coefficients in Cox proportional hazard models as well as incidence\nrates using Poisson regression. We simulate data with missing failure\nindicators and show that our method performs as well as or better than\ncompeting methods. Finally, we apply the proposed method to data from the\nOPPERA study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 20:14:49 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 14:52:58 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 09:51:30 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 06:20:25 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Brownstein", "Naomi", ""], ["Cai", "Jianwen", ""], ["Slade", "Gary", ""], ["Bair", "Eric", ""]]}, {"id": "1304.4066", "submitter": "Jos\\'{e} R. Zubizarreta", "authors": "Jos\\'e R. Zubizarreta, Dylan S. Small, Neera K. Goyal, Scott Lorch,\n  Paul R. Rosenbaum", "title": "Stronger instruments via integer programming in an observational study\n  of late preterm birth outcomes", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS582 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 25-50", "doi": "10.1214/12-AOAS582", "report-no": "IMS-AOAS-AOAS582", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an optimal nonbipartite match, a single population is divided into matched\npairs to minimize a total distance within matched pairs. Nonbipartite matching\nhas been used to strengthen instrumental variables in observational studies of\ntreatment effects, essentially by forming pairs that are similar in terms of\ncovariates but very different in the strength of encouragement to accept the\ntreatment. Optimal nonbipartite matching is typically done using network\noptimization techniques that can be quick, running in polynomial time, but\nthese techniques limit the tools available for matching. Instead, we use\ninteger programming techniques, thereby obtaining a wealth of new tools not\npreviously available for nonbipartite matching, including fine and near-fine\nbalance for several nominal variables, forced near balance on means and optimal\nsubsetting. We illustrate the methods in our on-going study of outcomes of\nlate-preterm births in California, that is, births of 34 to 36 weeks of\ngestation. Would lengthening the time in the hospital for such births reduce\nthe frequency of rapid readmissions? A straightforward comparison of babies who\nstay for a shorter or longer time would be severely biased, because the\nprincipal reason for a long stay is some serious health problem. We need an\ninstrument, something inconsequential and haphazard that encourages a shorter\nor a longer stay in the hospital. It turns out that babies born at certain\ntimes of day tend to stay overnight once with a shorter length of stay, whereas\nbabies born at other times of day tend to stay overnight twice with a longer\nlength of stay, and there is nothing particularly special about a baby who is\nborn at 11:00 pm.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 12:14:04 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Zubizarreta", "Jos\u00e9 R.", ""], ["Small", "Dylan S.", ""], ["Goyal", "Neera K.", ""], ["Lorch", "Scott", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1304.4156", "submitter": "Vincenzo Nicosia", "authors": "Fabrizio De Vico Fallani, Vincenzo Nicosia, Vito Latora, Mario Chavez", "title": "Non-parametric resampling of random walks for spectral network\n  clustering", "comments": "5 pages, 2 figures", "journal-ref": "Phys. Rev. E 89, 012802 (2014)", "doi": "10.1103/PhysRevE.89.012802", "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric resampling schemes have been recently introduced in complex\nnetwork analysis with the aim of assessing the statistical significance of\ngraph clustering and the robustness of community partitions. We propose here a\nmethod to replicate structural features of complex networks based on the\nnon-parametric resampling of the transition matrix associated with an unbiased\nrandom walk on the graph. We test this bootstrapping technique on synthetic and\nreal-world modular networks and we show that the ensemble of replicates\nobtained through resampling can be used to improve the performance of standard\nspectral algorithms for community detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 16:27:26 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 11:54:37 GMT"}], "update_date": "2014-02-25", "authors_parsed": [["Fallani", "Fabrizio De Vico", ""], ["Nicosia", "Vincenzo", ""], ["Latora", "Vito", ""], ["Chavez", "Mario", ""]]}, {"id": "1304.4200", "submitter": "Matt Taddy", "authors": "Matt Taddy", "title": "Efficiency and Structure in Multinomial Inverse Regression", "comments": "The main article is here: http://arxiv.org/abs/1012.2098", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the rejoinder for discussion of \"Multinomial Inverse Regression for\nText Analysis\", Journal of the American Statistical Association 108, 2013.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 19:02:15 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2013 14:34:43 GMT"}], "update_date": "2013-08-09", "authors_parsed": [["Taddy", "Matt", ""]]}, {"id": "1304.4433", "submitter": "Micha Mandel", "authors": "Micha Mandel, Manor Askenazi, Yi Zhang, Jarrod A. Marto", "title": "Variance function estimation in quantitative mass spectrometry with\n  application to iTRAQ labeling", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS572 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 1-24", "doi": "10.1214/12-AOAS572", "report-no": "IMS-AOAS-AOAS572", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and compares two methods for estimating the variance\nfunction associated with iTRAQ (isobaric tag for relative and absolute\nquantitation) isotopic labeling in quantitative mass spectrometry based\nproteomics. Measurements generated by the mass spectrometer are proportional to\nthe concentration of peptides present in the biological sample. However, the\niTRAQ reporter signals are subject to errors that depend on the peptide\namounts. The variance function of the errors is therefore an essential\nparameter for evaluating the results, but estimating it is complicated, as the\nnumber of nuisance parameters increases with sample size while the number of\nreplicates for each peptide remains small. Two experiments that were conducted\nwith the sole goal of estimating the variance function and its stability over\ntime are analyzed, and the resulting estimated variance function is used to\nanalyze an experiment targeting aberrant signaling cascades in cells harboring\ndistinct oncogenic mutations. Methods for constructing conservative $p$-values\nand confidence intervals are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 13:26:18 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Mandel", "Micha", ""], ["Askenazi", "Manor", ""], ["Zhang", "Yi", ""], ["Marto", "Jarrod A.", ""]]}, {"id": "1304.4439", "submitter": "Ying Yuan", "authors": "Ying Yuan, Hongtu Zhu, Martin Styner, John H. Gilmore, J. S. Marron", "title": "Varying coefficient model for modeling diffusion tensors along white\n  matter tracts", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS574 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 102-125", "doi": "10.1214/12-AOAS574", "report-no": "IMS-AOAS-AOAS574", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging provides important information on tissue structure\nand orientation of fiber tracts in brain white matter in vivo. It results in\ndiffusion tensors, which are $3\\times3$ symmetric positive definite (SPD)\nmatrices, along fiber bundles. This paper develops a functional data analysis\nframework to model diffusion tensors along fiber tracts as functional data in a\nRiemannian manifold with a set of covariates of interest, such as age and\ngender. We propose a statistical model with varying coefficient functions to\ncharacterize the dynamic association between functional SPD matrix-valued\nresponses and covariates. We calculate weighted least squares estimators of the\nvarying coefficient functions for the log-Euclidean metric in the space of SPD\nmatrices. We also develop a global test statistic to test specific hypotheses\nabout these coefficient functions and construct their simultaneous confidence\nbands. Simulated data are further used to examine the finite sample performance\nof the estimated varying coefficient functions. We apply our model to study\npotential gender differences and find a statistically significant aspect of the\ndevelopment of diffusion tensors along the right internal capsule tract in a\nclinical study of neurodevelopment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 13:36:32 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Yuan", "Ying", ""], ["Zhu", "Hongtu", ""], ["Styner", "Martin", ""], ["Gilmore", "John H.", ""], ["Marron", "J. S.", ""]]}, {"id": "1304.4441", "submitter": "Xiaojing Wang", "authors": "Xiaojing Wang, James O. Berger, Donald S. Burdick", "title": "Bayesian analysis of dynamic item response models in educational testing", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS608 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 126-153", "doi": "10.1214/12-AOAS608", "report-no": "IMS-AOAS-AOAS608", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item response theory (IRT) models have been widely used in educational\nmeasurement testing. When there are repeated observations available for\nindividuals through time, a dynamic structure for the latent trait of ability\nneeds to be incorporated into the model, to accommodate changes in ability.\nOther complications that often arise in such settings include a violation of\nthe common assumption that test results are conditionally independent, given\nability and item difficulty, and that test item difficulties may be partially\nspecified, but subject to uncertainty. Focusing on time series dichotomous\nresponse data, a new class of state space models, called Dynamic Item Response\n(DIR) models, is proposed. The models can be applied either retrospectively to\nthe full data or on-line, in cases where real-time prediction is needed. The\nmodels are studied through simulated examples and applied to a large collection\nof reading test data obtained from MetaMetrics, Inc.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 13:40:19 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Wang", "Xiaojing", ""], ["Berger", "James O.", ""], ["Burdick", "Donald S.", ""]]}, {"id": "1304.4445", "submitter": "Harrison Quick", "authors": "Harrison Quick, Sudipto Banerjee, Bradley P. Carlin", "title": "Modeling temporal gradients in regionally aggregated California asthma\n  hospitalization data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS600 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 154-176", "doi": "10.1214/12-AOAS600", "report-no": "IMS-AOAS-AOAS600", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in Geographical Information Systems (GIS) have led to the enormous\nrecent burgeoning of spatial-temporal databases and associated statistical\nmodeling. Here we depart from the rather rich literature in space-time modeling\nby considering the setting where space is discrete (e.g., aggregated data over\nregions), but time is continuous. Our major objective in this application is to\ncarry out inference on gradients of a temporal process in our data set of\nmonthly county level asthma hospitalization rates in the state of California,\nwhile at the same time accounting for spatial similarities of the temporal\nprocess across neighboring counties. Use of continuous time models here allows\ninference at a finer resolution than at which the data are sampled. Rather than\nuse parametric forms to model time, we opt for a more flexible stochastic\nprocess embedded within a dynamic Markov random field framework. Through the\nmatrix-valued covariance function we can ensure that the temporal process\nrealizations are mean square differentiable, and may thus carry out inference\non temporal gradients in a posterior predictive fashion. We use this approach\nto evaluate temporal gradients where we are concerned with temporal changes in\nthe residual and fitted rate curves after accounting for seasonality,\nspatiotemporal ozone levels and several spatially-resolved important\nsociodemographic covariates.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 13:44:14 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Quick", "Harrison", ""], ["Banerjee", "Sudipto", ""], ["Carlin", "Bradley P.", ""]]}, {"id": "1304.4448", "submitter": "Arno\\v{s}t Kom\\'{a}rek", "authors": "Arno\\v{s}t Kom\\'arek, Lenka Kom\\'arkov\\'a", "title": "Clustering for multivariate continuous and discrete longitudinal data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS580 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 177-200", "doi": "10.1214/12-AOAS580", "report-no": "IMS-AOAS-AOAS580", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple outcomes, both continuous and discrete, are routinely gathered on\nsubjects in longitudinal studies and during routine clinical follow-up in\ngeneral. To motivate our work, we consider a longitudinal study on patients\nwith primary biliary cirrhosis (PBC) with a continuous bilirubin level, a\ndiscrete platelet count and a dichotomous indication of blood vessel\nmalformations as examples of such longitudinal outcomes. An apparent\nrequirement is to use all the outcome values to classify the subjects into\ngroups (e.g., groups of subjects with a similar prognosis in a clinical\nsetting). In recent years, numerous approaches have been suggested for\nclassification based on longitudinal (or otherwise correlated) outcomes,\ntargeting not only traditional areas like biostatistics, but also rapidly\nevolving bioinformatics and many others. However, most available approaches\nconsider only continuous outcomes as a basis for classification, or if\nnoncontinuous outcomes are considered, then not in combination with other\noutcomes of a different nature. Here, we propose a statistical method for\nclustering (classification) of subjects into a prespecified number of groups\nwith a priori unknown characteristics on the basis of repeated measurements of\nseveral longitudinal outcomes of a different nature. This method relies on a\nmultivariate extension of the classical generalized linear mixed model where a\nmixture distribution is additionally assumed for random effects. We base the\ninference on a Bayesian specification of the model and simulation-based Markov\nchain Monte Carlo methodology. To apply the method in practice, we have\nprepared ready-to-use software for use in R (http://www.R-project.org). We also\ndiscuss evaluation of uncertainty in the classification and also discuss usage\nof a recently proposed methodology for model comparison - the selection of a\nnumber of clusters in our case - based on the penalized posterior deviance\nproposed by Plummer [Biostatistics 9 (2008) 523-539].\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 13:49:23 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Kom\u00e1rek", "Arno\u0161t", ""], ["Kom\u00e1rkov\u00e1", "Lenka", ""]]}, {"id": "1304.4540", "submitter": "Marta P\\'erez-Casany", "authors": "Marta P\\'erez-Casany and Aina Casellas", "title": "Marshall-Olkin Extended Zipf Distribution", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zipf distribution also known as scale-free distribution or discrete\nPareto distribution, is the particular case of Power Law distribution with\nsupport the strictly positive integers. It is a one-parameter distribution with\na linear behaviour in the log-log scale. In this paper the Zipf distribution is\ngeneralized by means of the Marshall-Olkin transformation. The new model has\nmore flexibility to adjust the probabilities of the first positive integer\nnumbers while keeping the linearity of the tail probabilities. The main\nproperties of the new model are presented, and several data sets are analyzed\nin order to show the gain obtained by using the generalized model.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 18:10:44 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 09:18:29 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["P\u00e9rez-Casany", "Marta", ""], ["Casellas", "Aina", ""]]}, {"id": "1304.4634", "submitter": "Leonardo Torres", "authors": "Leonardo Torres and Sidnei J. S. Sant'Anna and Corina C. Freitas and\n  Alejandro C. Frery", "title": "Speckle Reduction in Polarimetric SAR Imagery with Stochastic Distances\n  and Nonlocal Means", "comments": "Accepted for publication in Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CV cs.GR math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for reducing speckle in Polarimetric\nSynthetic Aperture Radar (PolSAR) imagery using Nonlocal Means and a\nstatistical test based on stochastic divergences. The main objective is to\nselect homogeneous pixels in the filtering area through statistical tests\nbetween distributions. This proposal uses the complex Wishart model to describe\nPolSAR data, but the technique can be extended to other models. The weights of\nthe location-variant linear filter are function of the p-values of tests which\nverify the hypothesis that two samples come from the same distribution and,\ntherefore, can be used to compute a local mean. The test stems from the family\nof (h-phi) divergences which originated in Information Theory. This novel\ntechnique was compared with the Boxcar, Refined Lee and IDAN filters. Image\nquality assessment methods on simulated and real data are employed to validate\nthe performance of this approach. We show that the proposed filter also\nenhances the polarimetric entropy and preserves the scattering information of\nthe targets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 22:11:53 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Torres", "Leonardo", ""], ["Sant'Anna", "Sidnei J. S.", ""], ["Freitas", "Corina C.", ""], ["Frery", "Alejandro C.", ""]]}, {"id": "1304.4748", "submitter": "Tao Yu", "authors": "Tao Yu, Chunming Zhang, Andrew L. Alexander, Richard J. Davidson", "title": "Local tests for identifying anisotropic diffusion areas in human brain\n  with DTI", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS573 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 201-225", "doi": "10.1214/12-AOAS573", "report-no": "IMS-AOAS-AOAS573", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging (DTI) plays a key role in analyzing the physical\nstructures of biological tissues, particularly in reconstructing fiber tracts\nof the human brain in vivo. On the one hand, eigenvalues of diffusion tensors\n(DTs) estimated from diffusion weighted imaging (DWI) data usually contain\nsystematic bias, which subsequently biases the diffusivity measurements\npopularly adopted in fiber tracking algorithms. On the other hand, correctly\naccounting for the spatial information is important in the construction of\nthese diffusivity measurements since the fiber tracts are typically spatially\nstructured. This paper aims to establish test-based approaches to identify\nanisotropic water diffusion areas in the human brain. These areas in turn\nindicate the areas passed by fiber tracts. Our proposed test statistic not only\ntakes into account the bias components in eigenvalue estimates, but also\nincorporates the spatial information of neighboring voxels. Under mild\nregularity conditions, we demonstrate that the proposed test statistic\nasymptotically follows a $\\chi^2$ distribution under the null hypothesis.\nSimulation and real DTI data examples are provided to illustrate the efficacy\nof our proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 09:26:21 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Yu", "Tao", ""], ["Zhang", "Chunming", ""], ["Alexander", "Andrew L.", ""], ["Davidson", "Richard J.", ""]]}, {"id": "1304.4773", "submitter": "Andreas Alfons", "authors": "Andreas Alfons, Christophe Croux, Sarah Gelper", "title": "Sparse least trimmed squares regression for analyzing high-dimensional\n  large data sets", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS575 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 226-248", "doi": "10.1214/12-AOAS575", "report-no": "IMS-AOAS-AOAS575", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse model estimation is a topic of high importance in modern data analysis\ndue to the increasing availability of data sets with a large number of\nvariables. Another common problem in applied statistics is the presence of\noutliers in the data. This paper combines robust regression and sparse model\nestimation. A robust and sparse estimator is introduced by adding an $L_1$\npenalty on the coefficient estimates to the well-known least trimmed squares\n(LTS) estimator. The breakdown point of this sparse LTS estimator is derived,\nand a fast algorithm for its computation is proposed. In addition, the sparse\nLTS is applied to protein and gene expression data of the NCI-60 cancer cell\npanel. Both a simulation study and the real data application show that the\nsparse LTS has better prediction performance than its competitors in the\npresence of leverage points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 11:37:37 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Alfons", "Andreas", ""], ["Croux", "Christophe", ""], ["Gelper", "Sarah", ""]]}, {"id": "1304.4799", "submitter": "Shili Lin", "authors": "Jingyuan Yang, Shili Lin", "title": "Robust partial likelihood approach for detecting imprinting and maternal\n  effects using case-control families", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS577 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 249-268", "doi": "10.1214/12-AOAS577", "report-no": "IMS-AOAS-AOAS577", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic imprinting and maternal effects are two epigenetic factors that have\nbeen increasingly explored for their roles in the etiology of complex diseases.\nThis is part of a concerted effort to find the \"missing heritability.\"\nAccordingly, statistical methods have been proposed to detect imprinting and\nmaternal effects simultaneously based on either a case-parent triads design or\na case-mother/control-mother pairs design. However, existing methods are\nfull-likelihood based and have to make strong assumptions concerning mating\ntype probabilities (nuisance parameters) to avoid overparametrization. In this\npaper we propose to augment the two popular study designs by combining them and\nincluding control-parent triads, so that our sample may contain a mixture of\ncase-parent/control-parent triads and case-mother/control-mother pairs. By\nmatching the case families with control families of the same structure and\nstratifying according to the familial genotypes, we are able to derive a\npartial likelihood that is free of the nuisance parameters. This renders\nunnecessary any unrealistic assumptions and leads to a robust procedure without\nsacrificing power. Our simulation study demonstrates that our partial\nlikelihood method has correct type I error rate, little bias and reasonable\npower under a variety of settings.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 12:50:48 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Yang", "Jingyuan", ""], ["Lin", "Shili", ""]]}, {"id": "1304.4876", "submitter": "Lucia Modugno mrs", "authors": "Lucia Modugno, Silvia Cagnone, Simone Giannerini", "title": "The Analysis of Tribal Art Prices: a Multilevel Model with\n  Autoregressive Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multilevel model specification with time series\ncomponents for the analysis of prices of artworks sold at auctions. Since\nauction data do not constitute a panel or a time series but are composed of\nrepeated cross-sections they require a specification with items at the first\nlevel nested in time points. An original feature of our approach is the\nderivation of full maximum likelihood estimators through the E-M algorithm. The\ndata analysed come from the first database of ethnic artworks sold in the most\nimportant auctions worldwide. The results show that the new specification\nimproves considerably over existing proposals both in terms of fit and\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 16:13:08 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Modugno", "Lucia", ""], ["Cagnone", "Silvia", ""], ["Giannerini", "Simone", ""]]}, {"id": "1304.4929", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "A new method to obtain risk neutral probability, without stochastic\n  calculus and price modeling, confirms the universal validity of\n  Black-Scholes-Merton formula and volatility's role", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to obtain the risk neutral probability of share\nprices without stochastic calculus and price modeling, via an embedding of the\nprice return modeling problem in Le Cam's statistical experiments framework.\nStrategies-probabilities $P_{t_0,n}$ and $P_{T,n}$ are thus determined and\nused, respectively,for the trader selling the share's European call option at\ntime $t_0$ and for the buyer who may exercise it in the future, at $T; \\ n$\nincreases with the number of share's transactions in $[t_0,T].$ When the\ntransaction times are dense in $[t_0,T]$ it is shown, with mild conditions,\nthat under each of these probabilities $\\log \\frac{S_T}{S_{t_0}}$ has\ninfinitely divisible distribution and in particular normal distribution for\n\"calm\" share; $S_t$ is the share's price at time $t.$ The price of the share's\ncall is the limit of the expected values of the call's payoff under the\ntranslated $P_{t_0,n}.$ It coincides for \"calm\" share prices with the\nBlack-Scholes-Merton formula with variance not necessarily proportional to\n$(T-t_0),$ thus confirming formula's universal validity without model\nassumptions. Additional results clarify volatility's role in the transaction\nand the behaviors of the trader and the buyer. Traders may use the pricing\nformulae after estimation of the unknown parameters.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 19:56:11 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2014 18:16:35 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 10:41:47 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "1304.5101", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonzalez and Maria Isabel Dorta-Gonzalez", "title": "Impact maturity times and citation time windows: The 2-year maximum\n  journal impact factor", "comments": "24 pages, 5 tables and 3 figures. arXiv admin note: text overlap with\n  arXiv:1007.4749, arXiv:1208.6122 by other authors", "journal-ref": null, "doi": "10.1016/j.joi.2013.03.005", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Journal metrics are employed for the assessment of scientific scholar\njournals from a general bibliometric perspective. In this context, the Thomson\nReuters journal impact factors (JIF) are the citation-based indicators most\nused. The 2-year journal impact factor (2-JIF) counts citations to one and two\nyear old articles, while the 5-year journal impact factor (5-JIF) counts\ncitations from one to five year old articles. Nevertheless, these indicators\nare not comparable among fields of science for two reasons: (i) each field has\na different impact maturity time, and (ii) because of systematic differences in\npublication and citation behaviour across disciplines. In fact, the 5-JIF\nfirstly appeared in the Journal Citation Reports (JCR) in 2007 with the purpose\nof making more comparable impacts in fields in which impact matures slowly.\nHowever, there is not an optimal fixed impact maturity time valid for all the\nfields. In some of them two years provides a good performance whereas in others\nthree or more years are necessary. Therefore, there is a problem when comparing\na journal from a field in which impact matures slowly with a journal from a\nfield in which impact matures rapidly. In this work, we propose the 2-year\nmaximum journal impact factor (2M-JIF), a new impact indicator that considers\nthe 2-year rolling citation time window of maximum impact instead of the\nprevious 2-year time window. Finally, an empirical application comparing 2-JIF,\n5-JIF, and 2M-JIF shows that the maximum rolling target window reduces the\nbetween-group variance with respect to the within-group variance in a random\nsample of about six hundred journals from eight different fields.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 12:21:08 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Dorta-Gonzalez", "Pablo", ""], ["Dorta-Gonzalez", "Maria Isabel", ""]]}, {"id": "1304.5107", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonzalez and Maria Isabel Dorta-Gonzalez", "title": "Comparing journals from different fields of Science and Social Science\n  through a JCR Subject Categories Normalized Impact Factor", "comments": "28 pages, 4 tables and 5 figures. arXiv admin note: text overlap with\n  arXiv:1007.4749 by other authors", "journal-ref": "Scientometrics 95(2), 645-672 (2013)", "doi": "10.1007/s11192-012-0929-9", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The journal Impact Factor (IF) is not comparable among fields of Science and\nSocial Science because of systematic differences in publication and citation\nbehaviour across disciplines. In this work, a decomposing of the field\naggregate impact factor into five normally distributed variables is presented.\nConsidering these factors, a Principal Component Analysis is employed to find\nthe sources of the variance in the JCR subject categories of Science and Social\nScience. Although publication and citation behaviour differs largely across\ndisciplines, principal components explain more than 78% of the total variance\nand the average number of references per paper is not the primary factor\nexplaining the variance in impact factors across categories. The Categories\nNormalized Impact Factor (CNIF) based on the JCR subject category list is\nproposed and compared with the IF. This normalization is achieved by\nconsidering all the indexing categories of each journal. An empirical\napplication, with one hundred journals in two or more subject categories of\neconomics and business, shows that the gap between rankings is reduced around\n32% in the journals analyzed. This gap is obtained as the maximum distance\namong the ranking percentiles from all categories where each journal is\nincluded.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 12:36:49 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Dorta-Gonzalez", "Pablo", ""], ["Dorta-Gonzalez", "Maria Isabel", ""]]}, {"id": "1304.5110", "submitter": "Pablo Dorta-Gonzalez", "authors": "Pablo Dorta-Gonzalez and Maria Isabel Dorta-Gonzalez", "title": "Central indexes to the citation distribution: A complement to the\n  h-index", "comments": "23 pages, 3 tables and 6 figures", "journal-ref": "Scientometrics 88(3), 729-745 (2011)", "doi": "10.1007/s11192-011-0453-3", "report-no": null, "categories": "cs.DL physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The citation distribution of a researcher shows the impact of their\nproduction and determines the success of their scientific career. However, its\napplication in scientific evaluation is difficult due to the bi-dimensional\ncharacter of the distribution. Some bibliometric indexes that try to synthesize\nin a numerical value the principal characteristics of this distribution have\nbeen proposed recently. In contrast with other bibliometric measures, the\nbiases that the distribution tails provoke, are reduced by the h-index.\nHowever, some limitations in the discrimination among researchers with\ndifferent publication habits are presented in this index. This index penalizes\nselective researchers, distinguished by the large number of citations received,\nas compared to large producers. In this work, two original sets of indexes, the\ncentral area indexes and the central interval indexes, that complement the\nh-index to include the central shape of the citation distribution, are proposed\nand compared.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 12:45:52 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Dorta-Gonzalez", "Pablo", ""], ["Dorta-Gonzalez", "Maria Isabel", ""]]}, {"id": "1304.5156", "submitter": "Yannis Yatracos", "authors": "Yannis G. Yatracos", "title": "Option pricing, Bayes risks and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical decision problem is hidden in the core of option pricing. A\nsimple form for the price C of a European call option is obtained via the\nminimum Bayes risk, R_B, of a 2-parameter estimation problem, thus justifying\ncalling C Bayes (B-)price. The result provides new insight in option pricing,\namong others obtaining C for some stock-price models using the underlying\nprobability instead of the risk neutral probability and giving R_B an economic\ninterpretation. When logarithmic stock prices follow Brownian motion, discrete\nnormal mixture and hyperbolic Levy motion the obtained B-prices are \"fair\"\nprices. A new expression for the price of American call option is also obtained\nand statistical modeling of R_B can be used when pricing European and American\ncall options.\n", "versions": [{"version": "v1", "created": "Thu, 18 Apr 2013 15:02:11 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Yatracos", "Yannis G.", ""]]}, {"id": "1304.5349", "submitter": "Debbie J. Dupuis", "authors": "Debbie J. Dupuis, Maria-Pia Victoria-Feser", "title": "Robust VIF regression with application to variable selection in large\n  data sets", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS584 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 319-341", "doi": "10.1214/12-AOAS584", "report-no": "IMS-AOAS-AOAS584", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sophisticated and automated means of data collection used by an\nincreasing number of institutions and companies leads to extremely large data\nsets. Subset selection in regression is essential when a huge number of\ncovariates can potentially explain a response variable of interest. The recent\nstatistical literature has seen an emergence of new selection methods that\nprovide some type of compromise between implementation (computational speed)\nand statistical optimality (e.g., prediction error minimization). Global\nmethods such as Mallows' $C_p$ have been supplanted by sequential methods such\nas stepwise regression. More recently, streamwise regression, faster than the\nformer, has emerged. A recently proposed streamwise regression approach based\non the variance inflation factor (VIF) is promising, but its least-squares\nbased implementation makes it susceptible to the outliers inevitable in such\nlarge data sets. This lack of robustness can lead to poor and suboptimal\nfeature selection. In our case, we seek to predict an individual's educational\nattainment using economic and demographic variables. We show how classical VIF\nperforms this task poorly and a robust procedure is necessary for policy\nmakers. This article proposes a robust VIF regression, based on fast robust\nestimators, that inherits all the good properties of classical VIF in the\nabsence of outliers, but also continues to perform well in their presence where\nthe classical approach fails.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 09:11:25 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Dupuis", "Debbie J.", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1304.5380", "submitter": "Juha Karvanen", "authors": "Juha Karvanen, Ari Rantanen, Lasse Luoma", "title": "Survey data and Bayesian analysis: a cost-efficient way to estimate\n  customer equity", "comments": null, "journal-ref": "Quantitative Marketing and Economics, Volume 12, Issue 3, Pages\n  305-329, 2014", "doi": "10.1007/s11129-014-9148-4", "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian framework for estimating the customer lifetime value\n(CLV) and the customer equity (CE) based on the purchasing behavior deducible\nfrom the market surveys on customer purchasing behavior. The proposed framework\nsystematically addresses the challenges faced when the future value of\ncustomers is estimated based on survey data. The scarcity of the survey data\nand the sampling variance are countered by utilizing the prior information and\nquantifying the uncertainty of the CE and CLV estimates by posterior\ndistributions. Furthermore, information on the purchase behavior of the\ncustomers of competitors available in the survey data is integrated to the\nframework. The introduced approach is directly applicable in the domains where\na customer relationship can be thought to be monogamous.\n  As an example on the use of the framework, we analyze a consumer survey on\nmobile phones carried out in Finland in February 2013. The survey data contains\nconsumer given information on the current and previous brand of the phone and\nthe times of the last two purchases.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 11:47:53 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2013 09:01:43 GMT"}, {"version": "v3", "created": "Fri, 30 May 2014 05:27:39 GMT"}], "update_date": "2014-08-12", "authors_parsed": [["Karvanen", "Juha", ""], ["Rantanen", "Ari", ""], ["Luoma", "Lasse", ""]]}, {"id": "1304.5401", "submitter": "Ronglai Shen", "authors": "Ronglai Shen, Sijian Wang, Qianxing Mo", "title": "Sparse integrative clustering of multiple omics data sets", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS578 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 269-294", "doi": "10.1214/12-AOAS578", "report-no": "IMS-AOAS-AOAS578", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High resolution microarrays and second-generation sequencing platforms are\npowerful tools to investigate genome-wide alterations in DNA copy number,\nmethylation and gene expression associated with a disease. An integrated\ngenomic profiling approach measures multiple omics data types simultaneously in\nthe same set of biological samples. Such approach renders an integrated data\nresolution that would not be available with any single data type. In this\nstudy, we use penalized latent variable regression methods for joint modeling\nof multiple omics data types to identify common latent variables that can be\nused to cluster patient samples into biologically and clinically relevant\ndisease subtypes. We consider lasso [J. Roy. Statist. Soc. Ser. B 58 (1996)\n267-288], elastic net [J. R. Stat. Soc. Ser. B Stat. Methodol. 67 (2005)\n301-320] and fused lasso [J. R. Stat. Soc. Ser. B Stat. Methodol. 67 (2005)\n91-108] methods to induce sparsity in the coefficient vectors, revealing\nimportant genomic features that have significant contributions to the latent\nvariables. An iterative ridge regression is used to compute the sparse\ncoefficient vectors. In model selection, a uniform design [Monographs on\nStatistics and Applied Probability (1994) Chapman & Hall] is used to seek\n\"experimental\" points that scattered uniformly across the search domain for\nefficient sampling of tuning parameter combinations. We compared our method to\nsparse singular value decomposition (SVD) and penalized Gaussian mixture model\n(GMM) using both real and simulated data sets. The proposed method is applied\nto integrate genomic, epigenomic and transcriptomic data for subtype analysis\nin breast and lung cancer data sets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 12:53:25 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Shen", "Ronglai", ""], ["Wang", "Sijian", ""], ["Mo", "Qianxing", ""]]}, {"id": "1304.5563", "submitter": "Qixin Wang", "authors": "Qixin Wang, Menghui Li, Hualong Zu, Mingyi Gao, Chenghua Cao, Li\n  Charlie Xia", "title": "A quantitative evaluation of health care system in US, China, and Sweden", "comments": "6 figures, 2 tables", "journal-ref": "HealthMED 4 (2013) 1064-1074", "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study is mainly aimed at evaluating the effectiveness of current health\ncare systems of several representative countries and improving that of the US.\nTo achieve these goals, a people-oriented non-linear evaluation model is\ndesigned. It comprises one major evaluation metric and four minor metrics. The\nmajor metric is constituted by combining possible factors that most\nsignificantly determine or affect the life expectancy of people in this\ncountry. The four minor metrics evaluate less important aspects of health care\nsystems and are subordinate to the major one. The authors rank some of the\nhealth care systems in the world according to the major metric and detect\nproblems in them with the help of minor ones. It is concluded that the health\ncare system of Sweden scores higher than the US and Chinese system scores lower\nthan that of the US. Especially, the health care system of US lags behind a\nlittle bit compared with its economic power. At last, it is reasonable for the\nAmerican government to optimize the arrangement of funding base on the result\nof goal programming model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 23:43:41 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Wang", "Qixin", ""], ["Li", "Menghui", ""], ["Zu", "Hualong", ""], ["Gao", "Mingyi", ""], ["Cao", "Chenghua", ""], ["Xia", "Li Charlie", ""]]}, {"id": "1304.5642", "submitter": "Emily Fox", "authors": "Sivan Aldor-Noiman, Lawrence D. Brown, Emily B. Fox, and Robert A.\n  Stine", "title": "Spatio-Temporal Low Count Processes with Application to Violent Crime\n  Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant interest in being able to predict where crimes will\nhappen, for example to aid in the efficient tasking of police and other\nprotective measures. We aim to model both the temporal and spatial dependencies\noften exhibited by violent crimes in order to make such predictions. The\ntemporal variation of crimes typically follows patterns familiar in time series\nanalysis, but the spatial patterns are irregular and do not vary smoothly\nacross the area. Instead we find that spatially disjoint regions exhibit\ncorrelated crime patterns. It is this indeterminate inter-region correlation\nstructure along with the low-count, discrete nature of counts of serious crimes\nthat motivates our proposed forecasting tool. In particular, we propose to\nmodel the crime counts in each region using an integer-valued first order\nautoregressive process. We take a Bayesian nonparametric approach to flexibly\ndiscover a clustering of these region-specific time series. We then describe\nhow to account for covariates within this framework. Both approaches adjust for\nseasonality. We demonstrate our approach through an analysis of weekly reported\nviolent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform\nstandard methods while additionally providing useful tools such as prediction\nintervals.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 15:46:32 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Aldor-Noiman", "Sivan", ""], ["Brown", "Lawrence D.", ""], ["Fox", "Emily B.", ""], ["Stine", "Robert A.", ""]]}, {"id": "1304.5759", "submitter": "Christopher Calderon", "authors": "Alexander Mont, Aubrey V. Wiegel, Diego Krapf, and Christopher P.\n  Calderon", "title": "Uncertainty Quantification of Discrete Association Problems in Image\n  Sequence-based Tracking", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cond-mat.mes-hall stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications, ranging from tracking molecular motion within cells to\nanalyzing complex animal foraging behavior, require algorithms for associating\na collection of spot-like particles in one image with particles contained in\nanother image. These associations are often made via network flow algorithms.\nHowever, it is often the case that many candidate association solutions (the\noutput of network flow algorithms) have nearly optimal scores; in this case,\nthe optimal assignment solution is of dubious quality. Algorithms for reliably\ncomputing the uncertainty of candidate association solutions are\nunder-developed in situations where many particles are tracked over multiple\nframes of data. This is due in part to the fact that exact uncertainty\nquantification (UQ) in large association problems is computationally\nintractable because the exact computation exhibits exponential dependence on\nthe number of particles tracked. We introduce a technique that can accurately\nand efficiently quantify association ambiguity (i.e., UQ for discrete\nassociation problems) without requiring the evaluation of the cost of each\nfeasible association solution. Our method can readily be wrapped around\nexisting tracking algorithms and can efficiently handle a variety of 2D\nassociation problems. The applications presented are focused on tracking\nmolecules in live cells. Our method is validated via both simulations and\nexperiments. The experimental applications aim to accurately form tracks and\nquantify diffusion of quantum dot labeled proteins from \\emph{in vivo}\nmeasurements; here association problems involving cost matrices possessing\nhundreds to thousands of rows/columns are encountered. For such large-scale\nproblems, we discuss how our approach can efficiently and accurately quantify\ninherent uncertainty in candidate data associations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 16:04:32 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Mont", "Alexander", ""], ["Wiegel", "Aubrey V.", ""], ["Krapf", "Diego", ""], ["Calderon", "Christopher P.", ""]]}, {"id": "1304.5816", "submitter": "Hema Swaminathan", "authors": "Ramya Vijaya and Rahul Lahoti and Hema Swaminathan", "title": "Moving from the Household to the Individual: Multidimensional Poverty\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current multidimensional measures of poverty continue to follow the\ntraditional income poverty approach of using household rather than the\nindividual as the unit of analysis. Household level measures are gender blind\nsince they ignore intra-household differences in resource allocation which have\nbeen shown to differ along gender lines. In this study we use new data from the\nKarnataka Household Asset Survey (KHAS) to construct an individual level\nmultidimensional poverty measure for Karnataka, India. Our results show that an\nindividual level measure can identify substantial gender differences in poverty\nthat are masked at the household level. We also find a large potential for\nmisclassification of poor individuals as non-poor when poverty is not assessed\nat the individual level.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 01:08:16 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Vijaya", "Ramya", ""], ["Lahoti", "Rahul", ""], ["Swaminathan", "Hema", ""]]}, {"id": "1304.5967", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty, Munmun Biswas, Sourabh Bhattacharya", "title": "Bayesian Nonparametric Estimation of Milky Way Model Parameters Using a\n  New Matrix-Variate Gaussian Process Based Method", "comments": "22 pages of main text; accepted for publication in the Electronic\n  Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop an inverse Bayesian approach to find the value of\nthe unknown model parameter vector that supports the real (or test) data, where\nthe data comprises measurements of a matrix-variate variable. The method is\nillustrated via the estimation of the unknown Milky Way feature parameter\nvector, using available test and simulated (training) stellar velocity data\nmatrices. The data is represented as an unknown function of the model\nparameters, where this high-dimensional function is modelled using a\nhigh-dimensional Gaussian Process (${\\cal GP}$). The model for this function is\ntrained using available training data and inverted by Bayesian means, to\nestimate the sought value of the model parameter vector at which the test data\nis realised. We achieve a closed-form expression for the posterior of the\nunknown parameter vector and the parameters of the invoked ${\\cal GP}$, given\ntest and training data. We perform model fitting by comparing the observed data\nwith predictions made at different summaries of the posterior probability of\nthe model parameter vector. As a supplement, we undertake a leave-one-out cross\nvalidation of our method.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 14:40:55 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2013 22:49:14 GMT"}, {"version": "v3", "created": "Tue, 2 Dec 2014 15:53:54 GMT"}, {"version": "v4", "created": "Tue, 26 May 2015 14:44:54 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Chakrabarty", "Dalia", ""], ["Biswas", "Munmun", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1304.5982", "submitter": "Dalia Chakrabarty Dr.", "authors": "Dalia Chakrabarty", "title": "A New Bayesian Test to test for the Intractability-Countering Hypothesis", "comments": "Accepted for publication in JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.GA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new test of hypothesis in which we seek the probability of the\nnull conditioned on the data, where the null is a simplification undertaken to\ncounter the intractability of the more complex model, that the simpler null\nmodel is nested within. With the more complex model rendered intractable, the\nnull model uses a simplifying assumption that capacitates the learning of an\nunknown parameter vector given the data. Bayes factors are shown to be known\nonly up to a ratio of unknown data-dependent constants--a problem that cannot\nbe cured using prescriptions similar to those suggested to solve the problem\ncaused to Bayes factor computation, by non-informative priors. Thus, a new test\nis needed in which we can circumvent Bayes factor computation. In this test, we\nundertake generation of data from the model in which the null hypothesis is\ntrue and can achieve support in the measured data for the null by comparing the\nmarginalised posterior of the model parameter given the measured data, to that\ngiven such generated data. However, such a ratio of marginalised posteriors can\nconfound interpretation of comparison of support in one measured data for a\nnull, with that in another data set for a different null. Given an application\nin which such comparison is undertaken, we alternatively define support in a\nmeasured data set for a null by identifying the model parameters that are less\nconsistent with the measured data than is minimally possible given the\ngenerated data, and realising that the higher the number of such parameter\nvalues, less is the support in the measured data for the null. Then, the\nprobability of the null conditional on the data is given within an MCMC-based\nscheme, by marginalising the posterior given the measured data, over parameter\nvalues that are as, or more consistent with the measured data, than with the\ngenerated data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 15:22:25 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 02:17:03 GMT"}, {"version": "v3", "created": "Sun, 18 Sep 2016 15:14:38 GMT"}], "update_date": "2016-09-20", "authors_parsed": [["Chakrabarty", "Dalia", ""]]}, {"id": "1304.6208", "submitter": "Minge Xie", "authors": "Minge Xie, Regina Y. Liu, C. V. Damaraju, William H. Olson", "title": "Incorporating external information in analyses of clinical trials with\n  binary outcomes", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS585 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 1, 342-368", "doi": "10.1214/12-AOAS585", "report-no": "IMS-AOAS-AOAS585", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  External information, such as prior information or expert opinions, can play\nan important role in the design, analysis and interpretation of clinical\ntrials. However, little attention has been devoted thus far to incorporating\nexternal information in clinical trials with binary outcomes, perhaps due to\nthe perception that binary outcomes can be treated as normally-distributed\noutcomes by using normal approximations. In this paper we show that these two\ntypes of clinical trials could behave differently, and that special care is\nneeded for the analysis of clinical trials with binary outcomes. In particular,\nwe first examine a simple but commonly used univariate Bayesian approach and\nobserve a technical flaw. We then study the full Bayesian approach using\ndifferent beta priors and a new frequentist approach based on the notion of\nconfidence distribution (CD). These approaches are illustrated and compared\nusing data from clinical studies and simulations. The full Bayesian approach is\ntheoretically sound, but surprisingly, under skewed prior distributions, the\nestimate derived from the marginal posterior distribution may not fall between\nthose from the marginal prior and the likelihood of clinical trial data. This\ncounterintuitive phenomenon, which we call the \"discrepant posterior\nphenomenon,\" does not occur in the CD approach. The CD approach is also\ncomputationally simpler and can be applied directly to any prior distribution,\nsymmetric or skewed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 08:47:44 GMT"}], "update_date": "2013-04-24", "authors_parsed": [["Xie", "Minge", ""], ["Liu", "Regina Y.", ""], ["Damaraju", "C. V.", ""], ["Olson", "William H.", ""]]}, {"id": "1304.6643", "submitter": "Maksims Fiosins", "authors": "Maxim Fioshin", "title": "Resampling Approach to the Estimation of the Aircraft Circulation Plan\n  Reliability", "comments": null, "journal-ref": "In Proc. of the 5-th International Conference \"Transport Systems\n  Telematics\", Katowice-Ustron, Poland, 2005, pp. 13-21", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper illustrates an application of the Resampling approach [2] for the\nestimation of the aircraft circulation plan reliability. Resampling is an\nintensive computer statistical method, which can be used effectively in the\ncase of small samples. Algorithm of the Resampling method for the given task is\nillustrated and variance of obtained estimators is calculated, which is the\nmeasure of the method effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 17:42:41 GMT"}, {"version": "v2", "created": "Sun, 12 May 2013 20:46:08 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Fioshin", "Maxim", ""]]}, {"id": "1304.6669", "submitter": "Maksims Fiosins", "authors": "Maxim Fioshin", "title": "Resampling Approach for the Calculation Processes and Information\n  Systems Models Estimation", "comments": "Riga, RTU, 2000, 34 pages. Promotion work summary", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work is devoted to the analysis of the Resampling method proposed by A.\nAndronov and to the analysis of the Resampling method application possibility\nto the estimation and simulation of the calculation and logical systems\nreliability. The work Simple and Hierarchical method properties are considered,\nalgorithms for variance are shown. The methods are applied for processes in the\nmultitask operation system and queries to database analysis, a comparison with\nthe classical method, that uses the empirical distribution functions, is made.\nNumerical examples illustrate the influence of different factors on the\nResampling method efficiency.\n  The task of the sample size optimization has been considered. The dynamic\nprogramming method is applied to minimize the variance of the Resampling\nestimator. Optimization is applied for the analysis of queries to database, the\nnumerical example illustrates the value of optimization.\n  The case of partially known distributions is considered. It is shown how to\nuse the Resampling approach in the case when the distributions of some input\nvariables are known. The method is applied to database query analysis and a\ncomparison with Hierarchical Resampling is made.\n  The construction of the Resampling confidence intervals is considered. The\nalgorithm for construction of the Resampling confidence intervals is shown and\nactual coverage probabilities are calculated. Examples for the multitask\noperation system analysis illustrate the calculation of the actual coverage\nprobability algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 16:08:53 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Fioshin", "Maxim", ""]]}, {"id": "1304.6670", "submitter": "Maksims Fiosins", "authors": "Maxim Fioshin, Helen Fioshina", "title": "Resampling Approach to the Estimation of Reliability Systems", "comments": null, "journal-ref": "Proc. of the International Conference \"Mathematical Methods in\n  Reliability\", Glasgow, UK, 2007, CD Proc", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is devoted to the resampling approach application to the\nreliability problems. This approach to reliability problems was first proposed\nby Ivnitsky (1967). Resampling is intensive statistical computer method, which\nis non-parametrical, that uses initial samples data in different combinations\nto simulate the process many times and get finally the estimator of the\ncharacteristics of interest. At the present paper simple resampling,\nhierarchical resampling, the case of one sample for several variables, the case\nof partially known distributions, analysis of degradation flow, analysis of\ndegradation-renewal process, construction of confidence intervals are\ndescribed. All those resampling application cases can be applied successfully\nto solve the reliability problems as an alternative to classical methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 17:37:15 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Fioshin", "Maxim", ""], ["Fioshina", "Helen", ""]]}, {"id": "1304.6777", "submitter": "Tauhid Zaman", "authors": "Tauhid Zaman, Emily B. Fox, Eric T. Bradlow", "title": "A Bayesian approach for predicting the popularity of tweets", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS741 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1583-1611", "doi": "10.1214/14-AOAS741", "report-no": "IMS-AOAS-AOAS741", "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We predict the popularity of short messages called tweets created in the\nmicro-blogging site known as Twitter. We measure the popularity of a tweet by\nthe time-series path of its retweets, which is when people forward the tweet to\nothers. We develop a probabilistic model for the evolution of the retweets\nusing a Bayesian approach, and form predictions using only observations on the\nretweet times and the local network or \"graph\" structure of the retweeters. We\nobtain good step ahead forecasts and predictions of the final total number of\nretweets even when only a small fraction (i.e., less than one tenth) of the\nretweet path is observed. This translates to good predictions within a few\nminutes of a tweet being posted, and has potential implications for\nunderstanding the spread of broader ideas, memes, or trends in social networks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 00:26:18 GMT"}, {"version": "v2", "created": "Mon, 3 Mar 2014 04:17:57 GMT"}, {"version": "v3", "created": "Mon, 24 Nov 2014 11:29:48 GMT"}], "update_date": "2014-11-25", "authors_parsed": [["Zaman", "Tauhid", ""], ["Fox", "Emily B.", ""], ["Bradlow", "Eric T.", ""]]}, {"id": "1304.6783", "submitter": "Haochang Shou", "authors": "Haochang Shou, Vadim Zipunnikov, Ciprian M. Crainiceanu and Sonja\n  Greven", "title": "Structured Functional Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by modern observational studies, we introduce a class of functional\nmodels that expands nested and crossed designs. These models account for the\nnatural inheritance of correlation structure from sampling design in studies\nwhere the fundamental sampling unit is a function or image. Inference is based\non functional quadratics and their relationship with the underlying covariance\nstructure of the latent processes. A computationally fast and scalable\nestimation procedure is developed for ultra-high dimensional data. Methods are\nillustrated in three examples: high-frequency accelerometer data for daily\nactivity, pitch linguistic data for phonetic analysis, and EEG data for\nstudying electrical brain activity during sleep.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 01:30:36 GMT"}], "update_date": "2013-04-26", "authors_parsed": [["Shou", "Haochang", ""], ["Zipunnikov", "Vadim", ""], ["Crainiceanu", "Ciprian M.", ""], ["Greven", "Sonja", ""]]}, {"id": "1304.6966", "submitter": "Zhandong  Liu", "authors": "Ying-Wooi Wan, Claire M. Mach, Genevera Allen, Matthew L. Anderson,\n  Zhandong Liu", "title": "On the Reproducibility of TCGA Ovarian Cancer MicroRNA Profiles", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0087782", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Dysregulated microRNA (miRNA) expression is a well-established feature of\nhuman cancer. However, the role of specific miRNAs in determining cancer\noutcomes remains unclear. Using Level 3 expression data from the Cancer Genome\nAtlas (TCGA), we identified 61 miRNAs that are associated with overall survival\nin 469 ovarian cancers profiled by microarray (p<0.01). We also identified 12\nmiRNAs that are associated with survival when miRNAs were profiled in the same\nspecimens using Next Generation Sequencing (miRNA-Seq) (p<0.01). Surprisingly,\nonly 1 miRNA transcript is associated with ovarian cancer survival in both\ndatasets. Our analyses indicate that this discrepancy is due to the fact that\nmiRNA levels reported by the two platforms correlate poorly, even after\ncorrecting for potential issues inherent to signal detection algorithms.\nFurther investigation is warranted.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 17:34:52 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 22:47:49 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Wan", "Ying-Wooi", ""], ["Mach", "Claire M.", ""], ["Allen", "Genevera", ""], ["Anderson", "Matthew L.", ""], ["Liu", "Zhandong", ""]]}, {"id": "1304.7399", "submitter": "Jared Glover", "authors": "Jared Glover and Sanja Popovic", "title": "Bingham Procrustean Alignment for Object Detection in Clutter", "comments": "Submitted to IROS 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new system for object detection in cluttered RGB-D images is presented. Our\nmain contribution is a new method called Bingham Procrustean Alignment (BPA) to\nalign models with the scene. BPA uses point correspondences between oriented\nfeatures to derive a probability distribution over possible model poses. The\norientation component of this distribution, conditioned on the position, is\nshown to be a Bingham distribution. This result also applies to the classic\nproblem of least-squares alignment of point sets, when point features are\norientation-less, and gives a principled, probabilistic way to measure pose\nuncertainty in the rigid alignment problem. Our detection system leverages BPA\nto achieve more reliable object detections in clutter.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 19:24:30 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Glover", "Jared", ""], ["Popovic", "Sanja", ""]]}, {"id": "1304.7406", "submitter": "Dean Eckles", "authors": "Eytan Bakshy, Dean Eckles", "title": "Uncertainty in Online Experiments with Dependent Data: An Evaluation of\n  Bootstrap Methods", "comments": "9 pages, 5 figures. This version corrects an error in one set of\n  simulations in Section 3.4 / Figure 4. All other results are unaffected", "journal-ref": "In Proceedings of the 19th ACM SIGKDD international conference on\n  Knowledge discovery and data mining (KDD '13). 2013. ACM, New York, NY, USA,\n  1303-1311", "doi": "10.1145/2487575.2488218", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online experiments exhibit dependence between users and items. For\nexample, in online advertising, observations that have a user or an ad in\ncommon are likely to be associated. Because of this, even in experiments\ninvolving millions of subjects, the difference in mean outcomes between control\nand treatment conditions can have substantial variance. Previous theoretical\nand simulation results demonstrate that not accounting for this kind of\ndependence structure can result in confidence intervals that are too narrow,\nleading to inaccurate hypothesis tests.\n  We develop a framework for understanding how dependence affects uncertainty\nin user-item experiments and evaluate how bootstrap methods that account for\ndiffering levels of dependence perform in practice. We use three real datasets\ndescribing user behaviors on Facebook - user responses to ads, search results,\nand News Feed stories - to generate data for synthetic experiments in which\nthere is no effect of the treatment on average by design. We then estimate\nempirical Type I error rates for each bootstrap method. Accounting for\ndependence within a single type of unit (i.e., within-user dependence) is often\nsufficient to get reasonable error rates. But when experiments have effects, as\none might expect in the field, accounting for multiple units with a multiway\nbootstrap can be necessary to get close to the advertised Type I error rates.\nThis work provides guidance to practitioners evaluating large-scale\nexperiments, and highlights the importance of analysis of inferential methods\nfor dependence structures common to online systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 20:54:41 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 21:47:35 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2013 20:58:51 GMT"}, {"version": "v4", "created": "Wed, 25 Oct 2017 15:31:03 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bakshy", "Eytan", ""], ["Eckles", "Dean", ""]]}, {"id": "1304.7417", "submitter": "Cong Li", "authors": "Cong Li, Can Yang, Joel Gelernter and Hongyu Zhao", "title": "Improving genetic risk prediction by leveraging pleiotropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task of human genetics studies is to accurately predict disease\nrisks in individuals based on genetic markers, which allows for identifying\nindividuals at high disease risks, and facilitating their disease treatment and\nprevention. Although hundreds of genome-wide association studies (GWAS) have\nbeen conducted on many complex human traits in recent years, there has been\nonly limited success in translating these GWAS data into clinically useful risk\nprediction models. The predictive capability of GWAS data is largely\nbottlenecked by the available training sample size due to the presence of\nnumerous variants carrying only small to modest effects. Recent studies have\nshown that different human traits may share common genetic bases. Therefore, an\nattractive strategy to increase the training sample size and hence improve the\nprediction accuracy is to integrate data of genetically correlated phenotypes.\nYet the utility of genetic correlation in risk prediction has not been explored\nin the literature. In this paper, we analyzed GWAS data for bipolar and related\ndisorders (BARD) and schizophrenia (SZ) with a bivariate ridge regression\nmethod, and found that jointly predicting the two phenotypes could\nsubstantially increase prediction accuracy as measured by the AUC (area under\nthe receiver operating characteristic curve). We also found similar prediction\naccuracy improvements when we jointly analyzed GWAS data for Crohn's disease\n(CD) and ulcerative colitis (UC). The empirical observations were substantiated\nthrough our comprehensive simulation studies, suggesting that a gain in\nprediction accuracy can be obtained by combining phenotypes with relatively\nhigh genetic correlations. Through both real data and simulation studies, we\ndemonstrated pleiotropy as a valuable asset that opens up a new opportunity to\nimprove genetic risk prediction in the future.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 02:12:14 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 01:04:31 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2013 16:09:25 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Li", "Cong", ""], ["Yang", "Can", ""], ["Gelernter", "Joel", ""], ["Zhao", "Hongyu", ""]]}, {"id": "1304.7435", "submitter": "Jose Paris", "authors": "J. F. Paris", "title": "Statistical characterization of kappa-mu shadowed fading", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a natural generalization of the kappa-mu fading\nchannel in which the line-of-sight (LOS) component is subject to shadowing.\nThis fading distribution has a clear physical interpretation, good analytical\nproperties and unifies the one-side Gaussian, Rayleigh, Nakagami-m, Ricean,\nkappa-mu and Ricean shadowed fading distributions. The three basic statistical\ncharacterizations, i.e. probability density function (PDF), cumulative\ndistribution function (CDF) and moment generating function (MGF), of the\nkappa-mu shadowed distribution are obtained in closed-form. Then, it is also\nshown that the sum and maximum distributions of independent but arbitrarily\ndistributed kappa-mu shadowed variates can be expressed in closed-form. This\nset of new statistical results is finally applied to the performance analysis\nof several wireless communication systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2013 07:08:55 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Paris", "J. F.", ""]]}, {"id": "1304.7713", "submitter": "Ana Georgina Flesia MS", "authors": "Ana Georgina Flesia, Javier Gimenez, Elena Rufeil Fiori", "title": "Markovian models for one dimensional structure estimation on heavily\n  noisy imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radar (SAR) images often exhibit profound appearance variations due to a\nvariety of factors including clutter noise produced by the coherent nature of\nthe illumination. Ultrasound images and infrared images have similar cluttered\nappearance, that make 1 dimensional structures, as edges and object boundaries\ndifficult to locate. Structure information is usually extracted in two steps:\nfirst, building and edge strength mask classifying pixels as edge points by\nhypothesis testing, and secondly estimating from that mask, pixel wide\nconnected edges. With constant false alarm rate (CFAR) edge strength detectors\nfor speckle clutter, the image needs to be scanned by a sliding window composed\nof several differently oriented splitting sub-windows. The accuracy of edge\nlocation for these ratio detectors depends strongly on the orientation of the\nsub-windows. In this work we propose to transform the edge strength detection\nproblem into a binary segmentation problem in the undecimated wavelet domain,\nsolvable using parallel 1d Hidden Markov Models. For general dependency models,\nexact estimation of the state map becomes computationally complex, but in our\nmodel, exact MAP is feasible. The effectiveness of our approach is demonstrated\non simulated noisy real-life natural images with available ground truth, while\nthe strength of our output edge map is measured with Pratt's, Baddeley an Kappa\nproficiency measures. Finally, analysis and experiments on three different\ntypes of SAR images, with different polarizations, resolutions and textures,\nillustrate that the proposed method can detect structure on SAR images\neffectively, providing a very good start point for active contour methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 16:57:47 GMT"}], "update_date": "2013-04-30", "authors_parsed": [["Flesia", "Ana Georgina", ""], ["Gimenez", "Javier", ""], ["Fiori", "Elena Rufeil", ""]]}, {"id": "1304.7829", "submitter": "Wei Lin", "authors": "Wei Lin, Rui Feng, Hongzhe Li", "title": "Regularization Methods for High-Dimensional Instrumental Variables\n  Regression With an Application to Genetical Genomics", "comments": "43 pages, 4 figures, to appear in Journal of the American Statistical\n  Association (http://www.tandfonline.com/r/JASA)", "journal-ref": null, "doi": "10.1080/01621459.2014.908125", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetical genomics studies, it is important to jointly analyze gene\nexpression data and genetic variants in exploring their associations with\ncomplex traits, where the dimensionality of gene expressions and genetic\nvariants can both be much larger than the sample size. Motivated by such modern\napplications, we consider the problem of variable selection and estimation in\nhigh-dimensional sparse instrumental variables models. To overcome the\ndifficulty of high dimensionality and unknown optimal instruments, we propose a\ntwo-stage regularization framework for identifying and estimating important\ncovariate effects while selecting and estimating optimal instruments. The\nmethodology extends the classical two-stage least squares estimator to high\ndimensions by exploiting sparsity using sparsity-inducing penalty functions in\nboth stages. The resulting procedure is efficiently implemented by coordinate\ndescent optimization. For the representative $L_1$ regularization and a class\nof concave regularization methods, we establish estimation, prediction, and\nmodel selection properties of the two-stage regularized estimators in the\nhigh-dimensional setting where the dimensionality of covariates and instruments\nare both allowed to grow exponentially with the sample size. The practical\nperformance of the proposed method is evaluated by simulation studies and its\nusefulness is illustrated by an analysis of mouse obesity data. Supplementary\nmaterials for this article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 01:21:18 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 07:33:37 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Lin", "Wei", ""], ["Feng", "Rui", ""], ["Li", "Hongzhe", ""]]}, {"id": "1304.8038", "submitter": "Ana Georgina Flesia MS", "authors": "Jackelyn M. Kembro and Ana Georgina Flesia and Raquel M. Gleiser and\n  Mar\\'ia A. Perillo and Ra\\'ul H. Mar\\'in", "title": "Assessment of long-range correlation in animal behaviour time series:\n  the temporal pattern of locomotor activity of Japanese quail (Coturnix\n  coturnix) and mosquito larva (Culex quinquefasciatus)", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2013.08.017", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study was to evaluate the performance of a classical method\nof fractal analysis, Detrended Fluctuation Analysis (DFA), in the analysis of\nthe dynamics of animal behavior time series. In order to correctly use DFA to\nassess the presence of long-range correlation, previous authors using\nstatistical model systems have stated that different aspects should be taken\ninto account such as: 1) the establishment by hypothesis testing of the absence\nof short term correlation, 2) an accurate estimation of a straight line in the\nlog-log plot of the fluctuation function, 3) the elimination of artificial\ncrossovers in the fluctuation function, and 4) the length of the time series.\nTaking into consideration these factors, herein we evaluated the presence of\nlong-range correlation in the temporal pattern of locomotor activity of\nJapanese quail ({\\sl Coturnix coturnix}) and mosquito larva ({\\sl Culex\nquinquefasciatus}). In our study, modeling the data with the general ARFIMA\nmodel, we rejected the hypothesis of short range correlations (d=0) in all\ncases. We also observed that DFA was able to distinguish between the artificial\ncrossover observed in the temporal pattern of locomotion of Japanese quail, and\nthe crossovers in the correlation behavior observed in mosquito larvae\nlocomotion. Although the test duration can slightly influence the parameter\nestimation, no qualitative differences were observed between different test\ndurations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 15:35:32 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kembro", "Jackelyn M.", ""], ["Flesia", "Ana Georgina", ""], ["Gleiser", "Raquel M.", ""], ["Perillo", "Mar\u00eda A.", ""], ["Mar\u00edn", "Ra\u00fal H.", ""]]}, {"id": "1304.8045", "submitter": "Buhm Han", "authors": "Buhm Han, Jae Hoon Sul, Eleazar Eskin, Paul I. W. de Bakker, Soumya\n  Raychaudhuri", "title": "A general framework for meta-analyzing dependent studies with\n  overlapping subjects in association mapping", "comments": "1/17/14: Minor text changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analysis of genome-wide association studies is increasingly popular and\nmany meta-analytic methods have been recently proposed. A majority of\nmeta-analytic methods combine information from multiple studies by assuming\nthat studies are independent since individuals collected in one study are\nunlikely to be collected again by another study. However, it has become\nincreasingly common to utilize the same control individuals among multiple\nstudies to reduce genotyping or sequencing cost. This causes those studies that\nshare the same individuals to be dependent, and spurious associations may arise\nif overlapping subjects are not taken into account in a meta-analysis. In this\npaper, we propose a general framework for meta-analyzing dependent studies with\noverlapping subjects. Given dependent studies, our approach \"decouples\" the\nstudies into independent studies such that meta-analysis methods assuming\nindependent studies can be applied. This enables many meta-analysis methods,\nsuch as the random effects model, to account for overlapping subjects. Another\nadvantage is that one can continue to use preferred software in the analysis\npipeline which may not support overlapping subjects. Using simulations and the\nWellcome Trust Case Control Consortium data, we show that our decoupling\napproach allows both the fixed and the random effects models to account for\noverlapping subjects while retaining desirable false positive rate and power.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 16:01:14 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2013 21:53:36 GMT"}, {"version": "v3", "created": "Fri, 17 Jan 2014 17:59:19 GMT"}], "update_date": "2014-01-20", "authors_parsed": [["Han", "Buhm", ""], ["Sul", "Jae Hoon", ""], ["Eskin", "Eleazar", ""], ["de Bakker", "Paul I. W.", ""], ["Raychaudhuri", "Soumya", ""]]}, {"id": "1304.8084", "submitter": "Jelena Fiosina", "authors": "Helen Afanasyeva", "title": "Statistical Analysis of Air Traffic in Latvian Region", "comments": null, "journal-ref": "In Proceedings of the 2nd Int. Conf. Simulation, Gaming, Training\n  and Business Process Reengineering in Operations. -Riga: RTU, 2000, pp.\n  125-129", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the research is statistical analyzes of air traffic in airport\n'Riga' zone. Special statistical methods oriented to the concrete object area -\nairspace of Latvia are developed. Some experiments are made to discover\nseason's and during twenty-four hours unstationarity of this process. Air\ntraffic intensity for some stationary period for some airways is estimated.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 17:34:52 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Afanasyeva", "Helen", ""]]}, {"id": "1304.8088", "submitter": "Jelena Fiosina", "authors": "Helen Afanasyeva", "title": "A Task of the Storage Control Theory in Transport Systems using\n  Resampling-method", "comments": null, "journal-ref": "In Proceedings of the 5-th Int. Conf. 'Transport Systems\n  Telematics', Katowice-Ustron, Poland, 2005, pp. 13-21", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The probability of shortage absence is estimated for the storage of some\ntransport system. The intensive computer methods of statistics are used in\ncorresponding processes simulation. The efficiency of suggested approach,\ntaking the mean square error of the estimator as the criterion, is illustrated\nwith corresponding numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 17:39:42 GMT"}, {"version": "v2", "created": "Sat, 18 May 2013 15:19:19 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Afanasyeva", "Helen", ""]]}]