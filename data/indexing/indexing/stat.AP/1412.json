[{"id": "1412.0248", "submitter": "Michael Lopez", "authors": "Michael J. Lopez, Gregory Matthews", "title": "Building an NCAA mens basketball predictive model and quantifying its\n  success", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The old adage says that it is better to be lucky than to be good, but when it\ncomes to winning NCAA tournament pools, do you need to be both? This paper\nattempts to answer this question using data from the 2014 men's basketball\ntournament and more than 400 predictions of game outcomes submitted to a\ncontest hosted by the website Kaggle. We begin by describing how we built a\nprediction model for men's basketball tournament outcomes under the binomial\nlog-likelihood loss function. Next, under different sets of true underlying\ngame probabilities, we simulate tournament outcomes and imputed pool standings,\nin an effort to determine how much of an entry's success can be attributed to\nluck. While one of our two submissions finished first in the Kaggle contest, we\nestimate that this winning entry had no more than about a 12% chance of doing\nso, even under the most optimistic of game probability scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 17:09:17 GMT"}], "update_date": "2014-12-02", "authors_parsed": [["Lopez", "Michael J.", ""], ["Matthews", "Gregory", ""]]}, {"id": "1412.0267", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong, Michal Koles\\'ar", "title": "A Simple Adjustment for Bandwidth Snooping", "comments": "54 pages and a 45 page supplement", "journal-ref": "The Review of Economic Studies, Volume 85, Issue 2, April 2018,\n  Pages 732-765,", "doi": "10.1093/restud/rdx051", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based estimators such as local polynomial estimators in regression\ndiscontinuity designs are often evaluated at multiple bandwidths as a form of\nsensitivity analysis. However, if in the reported results, a researcher selects\nthe bandwidth based on this analysis, the associated confidence intervals may\nnot have correct coverage, even if the estimator is unbiased. This paper\nproposes a simple adjustment that gives correct coverage in such situations:\nreplace the normal quantile with a critical value that depends only on the\nkernel and ratio of the maximum and minimum bandwidths the researcher has\nentertained. We tabulate these critical values and quantify the loss in\ncoverage for conventional confidence intervals. For a range of relevant cases,\na conventional 95% confidence interval has coverage between 70% and 90%, and\nour adjustment amounts to replacing the conventional critical value 1.96 with a\nnumber between 2.2 and 2.8. Our results also apply to other settings involving\ntrimmed data, such as trimming to ensure overlap in treatment effect\nestimation. We illustrate our approach with three empirical applications.\n", "versions": [{"version": "v1", "created": "Sun, 30 Nov 2014 19:47:01 GMT"}, {"version": "v2", "created": "Wed, 3 Dec 2014 20:25:09 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2015 21:38:34 GMT"}, {"version": "v4", "created": "Tue, 18 Oct 2016 17:08:58 GMT"}, {"version": "v5", "created": "Wed, 28 Jun 2017 13:59:57 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1412.0367", "submitter": "Valerie Poynor", "authors": "Valerie Poynor and Athanasios Kottas", "title": "Bayesian nonparametric modeling for mean residual life regression", "comments": "arXiv admin note: text overlap with arXiv:1411.7481", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean residual life function is a key functional for a survival\ndistribution. It has practically useful interpretation as the expected\nremaining lifetime given survival up to a particular time point, and it also\ncharacterizes the survival distribution. However, it has received limited\nattention in terms of inference methods under a probabilistic modeling\nframework. In this paper, we seek to provide general inference methodology for\nmean residual life regression. Survival data often include a set of predictor\nvariables for the survival response distribution, and in many cases it is\nnatural to include the covariates as random variables into the modeling. We\nthus propose a Dirichlet process mixture modeling approach for the joint\nstochastic mechanism of the covariates and survival responses. This approach\nimplies a flexible model structure for the mean residual life of the\nconditional response distribution, allowing general shapes for mean residual\nlife as a function of covariates given a specific time point, as well as a\nfunction of time given particular values of the covariate vector. To expand the\nscope of the modeling framework, we extend the mixture model to incorporate\ndependence across experimental groups, such as treatment and control groups.\nThis extension is built from a dependent Dirichlet process prior for the\ngroup-specific mixing distributions, with common locations and weights that\nvary across groups through latent bivariate beta distributed random variables.\nWe develop properties of the proposed regression models, and discuss methods\nfor prior specification and posterior inference. The different components of\nthe methodology are illustrated with simulated data sets. Moreover, the\nmodeling approach is applied to a data set comprising right censored survival\ntimes of patients with small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 07:43:37 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 22:44:36 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Poynor", "Valerie", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1412.0473", "submitter": "Isabell Franck", "authors": "Isabell M. Franck, P.S. Koutsourelakis", "title": "Sparse Variational Bayesian Approximations for Nonlinear Inverse\n  Problems: applications in nonlinear elastography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.NA physics.comp-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an efficient Bayesian framework for solving nonlinear,\nhigh-dimensional model calibration problems. It is based on a Variational\nBayesian formulation that aims at approximating the exact posterior by means of\nsolving an optimization problem over an appropriately selected family of\ndistributions. The goal is two-fold. Firstly, to find lower-dimensional\nrepresentations of the unknown parameter vector that capture as much as\npossible of the associated posterior density, and secondly to enable the\ncomputation of the approximate posterior density with as few forward calls as\npossible. We discuss how these objectives can be achieved by using a fully\nBayesian argumentation and employing the marginal likelihood or evidence as the\nultimate model validation metric for any proposed dimensionality reduction. We\ndemonstrate the performance of the proposed methodology for problems in\nnonlinear elastography where the identification of the mechanical properties of\nbiological materials can inform non-invasive, medical diagnosis. An Importance\nSampling scheme is finally employed in order to validate the results and assess\nthe efficacy of the approximations provided.\n", "versions": [{"version": "v1", "created": "Mon, 1 Dec 2014 13:40:02 GMT"}, {"version": "v2", "created": "Tue, 2 Dec 2014 10:13:17 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 10:07:27 GMT"}, {"version": "v4", "created": "Fri, 30 Oct 2015 18:09:54 GMT"}], "update_date": "2015-11-02", "authors_parsed": [["Franck", "Isabell M.", ""], ["Koutsourelakis", "P. S.", ""]]}, {"id": "1412.0793", "submitter": "Eiji Konaka", "authors": "Takeshi Izumi, Eiji Konaka", "title": "Monte-Carlo Simulation of J1 League Postseason System from the 2015\n  Season", "comments": "14 pages, 14 figures, and 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the new two stage and postseason system of J1 League, the top\ndivision of professional football in Japan, is simulated. Official regulation\ndefines that the new postseason from the 2015 season consists of five teams\nselected by different principles --- the top three teams by total season\npoints, and the winning teams of the 1st and 2nd half of the season.\n  This paper clarifies that there are overlaps within these five teams and the\naverage number of teams reaching the postseason is about 3.5. The probability\nthat the postseason is held with 3, 4, and 5 teams are estimated about 0.62,\n0.35, and 0.03, respectively.\n  From this result, this paper concludes that the new system of J1 League is\nbasically a one-stage system, whereas the new system is officially defined as a\ntwo-stage system, due to inappropriate design of the postseason.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 06:44:02 GMT"}, {"version": "v2", "created": "Fri, 12 Dec 2014 07:04:46 GMT"}, {"version": "v3", "created": "Tue, 23 Dec 2014 00:41:02 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Izumi", "Takeshi", ""], ["Konaka", "Eiji", ""]]}, {"id": "1412.0838", "submitter": "Anne Sabourin", "authors": "Anne Sabourin (LTCI)", "title": "Semi-parametric modeling of excesses above high multivariate thresholds\n  with censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to include censored data in a statistical analysis is a recur-rent issue\nin statistics. In multivariate extremes, the dependence structure of large\nobservations can be characterized in terms of a non parametric angular measure,\nwhile marginal excesses above asymptotically large thresholds have a parametric\ndistribution. In this work, a flexible semi-parametric Dirichlet mix-ture model\nfor angular measures is adapted to the context of censored data and missing\ncomponents. One major issue is to take into account censoring intervals\noverlapping the extremal threshold, without knowing whether the correspond-ing\nhidden data is actually extreme. Further, the censored likelihood needed for\nBayesian inference has no analytic expression. The first issue is tackled using\na Poisson process model for extremes, whereas a data augmentation scheme avoids\nmultivariate integration of the Poisson process intensity over both the\ncensored intervals and the failure region above threshold. The implemented MCMC\nalgorithm allows simultaneous estimation of marginal and dependence parameters,\nso that all sources of uncertainty other than model bias are cap-tured by\nposterior credible intervals. The method is illustrated on simulated and real\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 10:11:49 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Sabourin", "Anne", "", "LTCI"]]}, {"id": "1412.0968", "submitter": "Aaron King", "authors": "Aaron A. King, Matthieu Domenech de Cell\\`es, Felicia M. G. Magpantay,\n  and Pejman Rohani", "title": "Avoidable errors in the modeling of outbreaks of emerging pathogens,\n  with special reference to Ebola", "comments": "40 pages, 11 figures, 2 tables. Published by Proceedings of the Royal\n  Society of London, Series. Published online 1 April 2015", "journal-ref": "Proceedings of the Royal Society of London. Series B 282:\n  20150347, 2015", "doi": "10.1098/rspb.2015.0347", "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As an emergent infectious disease outbreak unfolds, public health response is\nreliant on information on key epidemiological quantities, such as transmission\npotential and serial interval. Increasingly, transmission models fit to\nincidence data are used to estimate these parameters and guide policy. Some\nwidely-used modeling practices lead to potentially large errors in parameter\nestimates and, consequently, errors in model-based forecasts. Even more\nworryingly, in such situations, confidence in parameter estimates and forecasts\ncan itself be far over-estimated, leading to the potential for large errors\nthat mask their own presence. Fortunately, straightforward and computationally\ninexpensive alternatives exist that avoid these problems. Here, we first use a\nsimulation study to demonstrate potential pitfalls of the standard practice of\nfitting deterministic models to cumulative incidence data. Next, we demonstrate\nan alternative based on stochastic models fit to raw data from an early phase\nof 2014 West Africa Ebola Virus Disease outbreak. We show not only that bias is\nthereby reduced, but that uncertainty in estimates and forecasts is better\nquantified and that, critically, lack of model fit is more readily diagnosed.\nWe conclude with a short list of principles to guide the modeling response to\nfuture infectious disease outbreaks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 16:39:42 GMT"}, {"version": "v2", "created": "Fri, 13 Feb 2015 17:57:15 GMT"}, {"version": "v3", "created": "Mon, 16 Feb 2015 02:12:31 GMT"}, {"version": "v4", "created": "Tue, 3 Mar 2015 21:19:02 GMT"}, {"version": "v5", "created": "Wed, 1 Apr 2015 12:07:35 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["King", "Aaron A.", ""], ["de Cell\u00e8s", "Matthieu Domenech", ""], ["Magpantay", "Felicia M. G.", ""], ["Rohani", "Pejman", ""]]}, {"id": "1412.1035", "submitter": "Michael Schuckers", "authors": "Michael Schuckers, Brian Macdonald", "title": "Accounting for Rink Effects in the National Hockey League's Real Time\n  Scoring System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recording of events in National Hockey League rinks is done through the Real\nTime Scoring System. This system records events such as hits, shots, faceoffs,\netc., as part of the play-by-play files that are made publicly available.\nSeveral previous studies have found that there are inconsistencies in the\nrecording of these events from rink to rink. In this paper, we propose a\nmethodology for estimation of the rink effects for each of the rinks in the\nNational Hockey League. Our aim is to build a model which accounts for the\nrelative differences between rinks. We use log-linear regression to model\ncounts of events per game with several predictors including team factors and\naverage score differential. The estimated rink effects can be used to reweight\nrecorded events so that can have comparable counts of events across rinks.\nApplying our methodology to data from six regular seasons, we find that there\nare some rinks with rink effects that are significant and consistent across\nthese seasons for multiple events.\n", "versions": [{"version": "v1", "created": "Tue, 2 Dec 2014 19:37:17 GMT"}], "update_date": "2014-12-03", "authors_parsed": [["Schuckers", "Michael", ""], ["Macdonald", "Brian", ""]]}, {"id": "1412.1243", "submitter": "Willy Rodr\\'iguez", "authors": "Olivier Mazet, Willy Rodr\\'iguez, Loun\\`es Chikhi", "title": "Demographic inference using genetic data from a single individual:\n  separating population size variation from population structure", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of sequencing technologies represents new opportunities\nfor population genetics research. It is expected that genomic data will\nincrease our ability to reconstruct the history of populations. While this\nincrease in genetic information will likely help biologists and anthropologists\nto reconstruct the demographic history of populations, it also represents new\nchallenges. Recent work has shown that structured populations generate signals\nof population size change. As a consequence it is often difficult to determine\nwhether demographic events such as expansions or contractions (bottlenecks)\ninferred from genetic data are real or due to the fact that populations are\nstructured in nature. Given that few inferential methods allow us to account\nfor that structure, and that genomic data will necessarily increase the\nprecision of parameter estimates, it is important to develop new approaches. In\nthe present study we analyse two demographic models. The first is a model of\ninstantaneous population size change whereas the second is the classical\nsymmetric island model. We (i) re-derive the distribution of coalescence times\nunder the two models for a sample of size two, (ii) use a maximum likelihood\napproach to estimate the parameters of these models (iii) validate this\nestimation procedure under a wide array of parameter combinations, (iv)\nimplement and validate a model choice procedure by using a Kolmogorov-Smirnov\ntest. Altogether we show that it is possible to estimate parameters under\nseveral models and perform efficient model choice using genetic data from a\nsingle diploid individual.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 09:28:29 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Mazet", "Olivier", ""], ["Rodr\u00edguez", "Willy", ""], ["Chikhi", "Loun\u00e8s", ""]]}, {"id": "1412.1303", "submitter": "James V. Zidek", "authors": "James V. Zidek, Gavin Shaddick, Carolyn G. Taylor", "title": "Reducing estimation bias in adaptively changing monitoring networks with\n  preferential site selection", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS745 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1640-1670", "doi": "10.1214/14-AOAS745", "report-no": "IMS-AOAS-AOAS745", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the topic of preferential sampling, specifically\nsituations where monitoring sites in environmental networks are preferentially\nlocated by the designers. This means the data arising from such networks may\nnot accurately characterize the spatio-temporal field they intend to monitor.\nApproaches that have been developed to mitigate the effects of preferential\nsampling in various contexts are reviewed and, building on these approaches, a\ngeneral framework for dealing with the effects of preferential sampling in\nenvironmental monitoring is proposed. Strategies for implementation are\nproposed, leading to a method for improving the accuracy of official statistics\nused to report trends and inform regulatory policy. An essential feature of the\nmethod is its capacity to learn the preferential selection process over time\nand hence to reduce bias in these statistics. Simulation studies suggest\ndramatic reductions in bias are possible. A case study demonstrates use of the\nmethod in assessing the levels of air pollution due to black smoke in the UK\nover an extended period (1970-1996). In particular, dramatic reductions in the\nestimates of the number of sites out of compliance are observed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 12:44:47 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Zidek", "James V.", ""], ["Shaddick", "Gavin", ""], ["Taylor", "Carolyn G.", ""]]}, {"id": "1412.1315", "submitter": "Nicoleta Serban", "authors": "Rensheng Zhou, Nicoleta Serban, Nagi Gebraeel", "title": "Degradation-based residual life prediction under different environments", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS749 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1671-1689", "doi": "10.1214/14-AOAS749", "report-no": "IMS-AOAS-AOAS749", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Degradation modeling has traditionally relied on historical signals to\nestimate the behavior of the underlying degradation process. Many models assume\nthat these historical signals are acquired under the same environmental\nconditions and can be observed along the entire lifespan of a component. In\nthis paper, we relax these assumptions and present a more general statistical\nframework for modeling degradation signals that may have been collected under\ndifferent types of environmental conditions. In addition, we consider\napplications where the historical signals are not necessarily observed\ncontinuously, that is, historical signals are sparse or fragmented. We consider\nthe case where historical degradation signals are collected under known\nenvironmental states and another case where the environmental conditions are\nunknown during the acquisition of these historical data. For the first case, we\nuse a classification algorithm to identify the environmental state of the units\noperating in the field. In the second case, a clustering step is required for\nclustering the historical degradation signals. The proposed model can provide\naccurate predictions of the lifetime or residual life distributions of\nengineering components that are still operated in the field. This is\ndemonstrated by using simulated degradation signals as well as vibration-based\ndegradation signals acquired from a rotating machinery setup.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 13:22:18 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Zhou", "Rensheng", ""], ["Serban", "Nicoleta", ""], ["Gebraeel", "Nagi", ""]]}, {"id": "1412.1331", "submitter": "Zhisheng Ye", "authors": "Zhisheng Ye, Hon Keung Tony Ng", "title": "On analysis of incomplete field failure data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS752 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1713-1727", "doi": "10.1214/14-AOAS752", "report-no": "IMS-AOAS-AOAS752", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many commercial products are sold with warranties and indirectly through\ndealers. The manufacturer-retailer distribution mechanism results in serious\nmissing data problems in field return data, as the sales date for an unreturned\nunit is generally unknown to the manufacturer. This study considers a general\nsetting for field failure data with unknown sales dates and a warranty limit. A\nstochastic expectation-maximization (SEM) algorithm is developed to estimate\nthe distributions of the sales lag (time between shipment to a retailer and\nsale to a customer) and the lifetime of the product under study. Extensive\nsimulations are used to evaluate the performance of the SEM algorithm and to\ncompare with the imputation method proposed by Ghosh [Ann. Appl. Stat. 4 (2010)\n1976-1999]. Three real examples illustrate the methodology proposed in this\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 14:05:22 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Ye", "Zhisheng", ""], ["Ng", "Hon Keung Tony", ""]]}, {"id": "1412.1344", "submitter": "Fouedjio Francky", "authors": "Francky Fouedjio, Nicolas Desassis, Thomas Romary", "title": "Estimation of Space Deformation Model for Non-stationary Random\n  Functions", "comments": "17 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stationary Random Functions have been successfully applied in geostatistical\napplications for decades. In some instances, the assumption of a homogeneous\nspatial dependence structure across the entire domain of interest is\nunrealistic. A practical approach for modelling and estimating non-stationary\nspatial dependence structure is considered. This consists in transforming a\nnon-stationary Random Function into a stationary and isotropic one via a\nbijective continuous deformation of the index space. So far, this approach has\nbeen successfully applied in the context of data from several independent\nrealizations of a Random Function. In this work, we propose an approach for\nnon-stationary geostatistical modelling using space deformation in the context\nof a single realization with possibly irregularly spaced data. The estimation\nmethod is based on a non-stationary variogram kernel estimator which serves as\na dissimilarity measure between two locations in the geographical space. The\nproposed procedure combines aspects of kernel smoothing, weighted non-metric\nmulti-dimensional scaling and thin-plate spline radial basis functions. On a\nsimulated data, the method is able to retrieve the true deformation.\nPerformances are assessed on both synthetic and real datasets. It is shown in\nparticular that our approach outperforms the stationary approach. Beyond the\nprediction, the proposed method can also serve as a tool for exploratory\nanalysis of the non-stationarity.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 14:42:13 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fouedjio", "Francky", ""], ["Desassis", "Nicolas", ""], ["Romary", "Thomas", ""]]}, {"id": "1412.1373", "submitter": "Fouedjio Francky", "authors": "Francky Fouedjio, Nicolas Desassis, Jacques Rivoirard", "title": "A Generalized Convolution Model and Estimation for Non-stationary Random\n  Functions", "comments": "24 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard geostatistical models assume second order stationarity of the\nunderlying Random Function. In some instances, there is little reason to expect\nthe spatial dependence structure to be stationary over the whole region of\ninterest. In this paper, we introduce a new model for second order\nnon-stationary Random Functions as a convolution of an orthogonal random\nmeasure with a spatially varying random weighting function. This new model is a\ngeneralization of the common convolution model where a non-random weighting\nfunction is used. The resulting class of non-stationary covariance functions is\nvery general, flexible and allows to retrieve classes of closed-form\nnon-stationary covariance functions known from the literature, for a suitable\nchoices of the random weighting functions family. Under the framework of a\nsingle realization and local stationarity, we develop parameter inference\nprocedure of these explicit classes of non-stationary covariance functions.\nFrom a local variogram non-parametric kernel estimator, a weighted local\nleast-squares approach in combination with kernel smoothing method is developed\nto estimate the parameters. Performances are assessed on two real datasets:\nsoil and rainfall data. It is shown in particular that the proposed approach\noutperforms the stationary one, according to several criteria. Beyond the\nspatial predictions, we also show how conditional simulations can be carried\nout in this non-stationary framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 15:47:34 GMT"}], "update_date": "2014-12-04", "authors_parsed": [["Fouedjio", "Francky", ""], ["Desassis", "Nicolas", ""], ["Rivoirard", "Jacques", ""]]}, {"id": "1412.1501", "submitter": "Rafael Stern", "authors": "Rafael B. Stern, Joseph B. Kadane", "title": "Compensating for the loss of a chance", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Civil liability for a lost chance applies to legal cases in which a tortious\naction changes the probabilities of the outcomes that can be obtained by the\nvictim. A central point in the application of this type of liability is the\nvaluation of damages. Despite the practical importance of the valuation of lost\nchances, the legal restrictions that guide it have rarely been discussed\nexplicitly. In order to discuss these restrictions, we propose an abstract\ndescription of a lost chance case in which there are multiple possible outcomes\nand the victim can make a choice that affects these outcomes. Given this\ndescription, we propose six conceptual questions to guide the valuation of lost\nchances. We discuss alternative answers to these questions and present the\nformulas that derive from them. More specifically, we show that the main\nformulas that have been proposed for medical misdiagnosis cases are particular\ninstances of the alternatives we discuss.\n", "versions": [{"version": "v1", "created": "Wed, 3 Dec 2014 21:42:33 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Stern", "Rafael B.", ""], ["Kadane", "Joseph B.", ""]]}, {"id": "1412.1567", "submitter": "Oliver Lang", "authors": "Mario Huemer, Oliver Lang", "title": "CWCU LMMSE Estimation: Prerequisites and Properties", "comments": "4 pages of content, 1 page with references, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical unbiasedness condition utilized e.g. by the best linear\nunbiased estimator (BLUE) is very stringent. By softening the \"global\"\nunbiasedness condition and introducing component-wise conditional unbiasedness\nconditions instead, the number of constraints limiting the estimator's\nperformance can in many cases significantly be reduced. In this work we\ninvestigate the component-wise conditionally unbiased linear minimum mean\nsquare error (CWCU LMMSE) estimator for different model assumptions. The\nprerequisites in general differ from the ones of the LMMSE estimator. We first\nderive the CWCU LMMSE estimator under the jointly Gaussian assumption of the\nmeasurements and the parameters. Then we focus on the linear model and discuss\nthe CWCU LMMSE estimator for jointly Gaussian parameters, and for mutually\nindependent (and otherwise arbitrarily distributed) parameters, respectively.\nIn all these cases the CWCU LMMSE estimator incorporates the prior mean and the\nprior covariance matrix of the parameter vector. For the remaining cases\noptimum linear CWCU estimators exist, but they may correspond to globally\nunbiased estimators that do not make use of prior statistical knowledge about\nthe parameters. Finally, the beneficial properties of the CWCU LMMSE estimator\nare demonstrated with the help of a well-known channel estimation application.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 07:16:01 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Huemer", "Mario", ""], ["Lang", "Oliver", ""]]}, {"id": "1412.1642", "submitter": "Ander Wilson", "authors": "Ander Wilson, Ana G. Rappold, Lucas M. Neas, Brian J. Reich", "title": "Modeling the effect of temperature on ozone-related mortality", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS754 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1728-1749", "doi": "10.1214/14-AOAS754", "report-no": "IMS-AOAS-AOAS754", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change is expected to alter the distribution of ambient ozone levels\nand temperatures which, in turn, may impact public health. Much research has\nfocused on the effect of short-term ozone exposures on mortality and morbidity\nwhile controlling for temperature as a confounder, but less is known about the\njoint effects of ozone and temperature. The extent of the health effects of\nchanging ozone levels and temperatures will depend on whether these effects are\nadditive or synergistic. In this paper we propose a spatial, semi-parametric\nmodel to estimate the joint ozone-temperature risk surfaces in 95 US urban\nareas. Our methodology restricts the ozone-temperature risk surfaces to be\nmonotone in ozone and allows for both nonadditive and nonlinear effects of\nozone and temperature. We use data from the National Mortality and Morbidity\nAir Pollution Study (NMMAPS) and show that the proposed model fits the data\nbetter than additive linear and nonlinear models. We then examine the\nsynergistic effect of ozone and temperature both nationally and locally and\nfind evidence of a nonlinear ozone effect and an ozone-temperature interaction\nat higher temperatures and ozone concentrations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 12:29:29 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Wilson", "Ander", ""], ["Rappold", "Ana G.", ""], ["Neas", "Lucas M.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1412.1670", "submitter": "Jian Kang", "authors": "Jian Kang, Thomas E. Nichols, Tor D. Wager, Timothy D. Johnson", "title": "A Bayesian hierarchical spatial point process model for multi-type\n  neuroimaging meta-analysis", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS757 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1800-1824", "doi": "10.1214/14-AOAS757", "report-no": "IMS-AOAS-AOAS757", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroimaging meta-analysis is an important tool for finding consistent\neffects over studies that each usually have 20 or fewer subjects. Interest in\nmeta-analysis in brain mapping is also driven by a recent focus on so-called\n\"reverse inference\": where as traditional \"forward inference\" identifies the\nregions of the brain involved in a task, a reverse inference identifies the\ncognitive processes that a task engages. Such reverse inferences, however,\nrequire a set of meta-analysis, one for each possible cognitive domain.\nHowever, existing methods for neuroimaging meta-analysis have significant\nlimitations. Commonly used methods for neuroimaging meta-analysis are not model\nbased, do not provide interpretable parameter estimates, and only produce null\nhypothesis inferences; further, they are generally designed for a single group\nof studies and cannot produce reverse inferences. In this work we address these\nlimitations by adopting a nonparametric Bayesian approach for meta-analysis\ndata from multiple classes or types of studies. In particular, foci from each\ntype of study are modeled as a cluster process driven by a random intensity\nfunction that is modeled as a kernel convolution of a gamma random field. The\ntype-specific gamma random fields are linked and modeled as a realization of a\ncommon gamma random field, shared by all types, that induces correlation\nbetween study types and mimics the behavior of a univariate mixed effects\nmodel. We illustrate our model on simulation studies and a meta-analysis of\nfive emotions from 219 studies and check model fit by a posterior predictive\nassessment. In addition, we implement reverse inference by using the model to\npredict study type from a newly presented study. We evaluate this predictive\nperformance via leave-one-out cross-validation that is efficiently implemented\nusing importance sampling techniques.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 14:06:04 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Kang", "Jian", ""], ["Nichols", "Thomas E.", ""], ["Wager", "Tor D.", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "1412.1684", "submitter": "Yang Feng", "authors": "Diego Franco Saldana, Yi Yu, and Yang Feng", "title": "How Many Communities Are There?", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic blockmodels and variants thereof are among the most widely used\napproaches to community detection for social networks and relational data. A\nstochastic blockmodel partitions the nodes of a network into disjoint sets,\ncalled communities. The approach is inherently related to clustering with\nmixture models; and raises a similar model selection problem for the number of\ncommunities. The Bayesian information criterion (BIC) is a popular solution,\nhowever, for stochastic blockmodels, the conditional independence assumption\ngiven the communities of the endpoints among different edges is usually\nviolated in practice. In this regard, we propose composite likelihood BIC\n(CL-BIC) to select the number of communities, and we show it is robust against\npossible misspecifications in the underlying stochastic blockmodel assumptions.\nWe derive the requisite methodology and illustrate the approach using both\nsimulated and real data. Supplementary materials containing the relevant\ncomputer code are available online.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 14:47:47 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2015 18:56:30 GMT"}], "update_date": "2015-09-16", "authors_parsed": [["Saldana", "Diego Franco", ""], ["Yu", "Yi", ""], ["Feng", "Yang", ""]]}, {"id": "1412.1786", "submitter": "Stan Zachary", "authors": "Stan Zachary and Chris Dent", "title": "Estimation of Joint Distribution of Demand and Available Renewables for\n  Generation Adequacy Assessment", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been a resurgence of interest in generation\nadequacy risk assessment, due to the need to include variable generation\nrenewables within such calculations. This paper will describe new statistical\napproaches to estimating the joint distribution of demand and available VG\ncapacity; this is required for the LOLE calculations used in many statutory\nadequacy studies, for example those of GB and PJM. The most popular estimation\ntechnique in the VG-integration literature is `hindcast', in which the historic\njoint distribution of demand and available VG is used as a predictive\ndistribution. Through the use of bootstrap statistical analysis, this paper\nwill show that due to extreme sparsity of data on times of high demand and low\nVG, hindcast results can suffer from sampling uncertainty to the extent that\nthey have little practical meaning. An alternative estimation approach, in\nwhich a marginal distribution of available VG is rescaled according to demand\nlevel, is thus proposed. This reduces sampling uncertainty at the expense of\nthe additional model structure assumption, and further provides a means of\nassessing the sensitivity of model outputs to the VG-demand relationship by\nvarying the function of demand by which the marginal VG distribution is\nrescaled.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 18:08:13 GMT"}], "update_date": "2014-12-05", "authors_parsed": [["Zachary", "Stan", ""], ["Dent", "Chris", ""]]}, {"id": "1412.1843", "submitter": "Heidi Fischer", "authors": "Heidi J. Fischer, Qunfang Zhang, Yifang Zhu, and Robert E. Weiss", "title": "Functional Time Series Models for Ultrafine Particle Distributions", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Bayesian random effect functional time series models to model the\nimpact of engine idling on ultrafine particle (UFP) counts inside school buses.\nUFPs are toxic to humans with health effects strongly linked to particle size.\nSchool engines emit particles primarily in the UFP size range and as school\nbuses idle at bus stops, UFPs penetrate into cabins through cracks, doors, and\nwindows. How UFP counts inside buses vary by particle size over time and under\ndifferent idling conditions is not yet well understood. We model UFP counts at\na given time with a cubic B-spline basis as a function of size and allow counts\nto increase over time at a size dependent rate once the engine turns on. We\nexplore alternate parametric models for the engine-on increase which also vary\nsmoothly over size. The log residual variance over size is modeled using a\nquadratic B-spline basis to account for heterogeneity and an autoregressive\nmodel is used for the residual. Model predictions are communicated graphically.\nThese methods provide information needed for regulating vehicle emissions to\nminimize UFP exposure in the future.\n", "versions": [{"version": "v1", "created": "Thu, 4 Dec 2014 21:17:18 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Fischer", "Heidi J.", ""], ["Zhang", "Qunfang", ""], ["Zhu", "Yifang", ""], ["Weiss", "Robert E.", ""]]}, {"id": "1412.1915", "submitter": "Xinxin Zhu", "authors": "Xinxin Zhu, Kenneth P. Bowman, Marc G. Genton", "title": "Incorporating geostrophic wind information for improved space-time\n  short-term wind speed forecasting", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS756 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1782-1799", "doi": "10.1214/14-AOAS756", "report-no": "IMS-AOAS-AOAS756", "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate short-term wind speed forecasting is needed for the rapid\ndevelopment and efficient operation of wind energy resources. This is, however,\na very challenging problem. Although on the large scale, the wind speed is\nrelated to atmospheric pressure, temperature, and other meteorological\nvariables, no improvement in forecasting accuracy was found by incorporating\nair pressure and temperature directly into an advanced space-time statistical\nforecasting model, the trigonometric direction diurnal (TDD) model. This paper\nproposes to incorporate the geostrophic wind as a new predictor in the TDD\nmodel. The geostrophic wind captures the physical relationship between wind and\npressure through the observed approximate balance between the pressure gradient\nforce and the Coriolis acceleration due to the Earth's rotation. Based on our\nnumerical experiments with data from West Texas, our new method produces more\naccurate forecasts than does the TDD model using air pressure and temperature\nfor 1- to 6-hour-ahead forecasts based on three different evaluation criteria.\nFurthermore, forecasting errors can be further reduced by using moving average\nhourly wind speeds to fit the diurnal pattern. For example, our new method\nobtains between 13.9% and 22.4% overall mean absolute error reduction relative\nto persistence in 2-hour-ahead forecasts, and between 5.3% and 8.2% reduction\nrelative to the best previous space-time methods in this setting.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 08:04:10 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Zhu", "Xinxin", ""], ["Bowman", "Kenneth P.", ""], ["Genton", "Marc G.", ""]]}, {"id": "1412.1922", "submitter": "Takao Kumazawa", "authors": "Takao Kumazawa, Yosihiko Ogata", "title": "Nonstationary ETAS models for nonstandard earthquakes", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS759 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1825-1852", "doi": "10.1214/14-AOAS759", "report-no": "IMS-AOAS-AOAS759", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditional intensity function of a point process is a useful tool for\ngenerating probability forecasts of earthquakes. The epidemic-type aftershock\nsequence (ETAS) model is defined by a conditional intensity function, and the\ncorresponding point process is equivalent to a branching process, assuming that\nan earthquake generates a cluster of offspring earthquakes (triggered\nearthquakes or so-called aftershocks). Further, the size of the\nfirst-generation cluster depends on the magnitude of the triggering (parent)\nearthquake. The ETAS model provides a good fit to standard earthquake\noccurrences. However, there are nonstandard earthquake series that appear under\ntransient stress changes caused by aseismic forces such as volcanic magma or\nfluid intrusions. These events trigger transient nonstandard earthquake swarms,\nand they are poorly fitted by the stationary ETAS model. In this study, we\nexamine nonstationary extensions of the ETAS model that cover nonstandard\ncases. These models allow the parameters to be time-dependent and can be\nestimated by the empirical Bayes method. The best model is selected among the\ncompeting models to provide the inversion solutions of nonstationary changes.\nTo address issues of the uniqueness and robustness of the inversion procedure,\nthis method is demonstrated on an inland swarm activity induced by the 2011\nTohoku-Oki, Japan earthquake of magnitude 9.0.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 08:50:20 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Kumazawa", "Takao", ""], ["Ogata", "Yosihiko", ""]]}, {"id": "1412.1977", "submitter": "Ugo Marzolino", "authors": "Ugo Marzolino and Toma\\v{z} Prosen", "title": "Quantum metrology with non-equilibrium steady states of quantum spin\n  chains", "comments": "12 pages, 10 figures", "journal-ref": "Phys. Rev. A 90, 062130 (2014)", "doi": "10.1103/PhysRevA.90.062130", "report-no": null, "categories": "quant-ph cond-mat.str-el nlin.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parameter estimations with probes being the boundary\ndriven/dissipated non- equilibrium steady states of XXZ spin 1/2 chains. The\nparameters to be estimated are the dissipation coupling and the anisotropy of\nthe spin-spin interaction. In the weak coupling regime we compute the scaling\nof the Fisher information, i.e. the inverse best sensitivity among all\nestimators, with the number of spins. We find superlinear scalings and\ntransitions between the distinct, isotropic and anisotropic, phases. We also\nlook at the best relative error which decreases with the number of particles\nfaster than the shot-noise only for the estimation of anisotropy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 12:18:47 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Marzolino", "Ugo", ""], ["Prosen", "Toma\u017e", ""]]}, {"id": "1412.2183", "submitter": "Pengfei Zang", "authors": "Richard A. Davis, Pengfei Zang, Tian Zheng", "title": "Reduced-Rank Covariance Estimation in Vector Autoregressive Modeling", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider reduced-rank modeling of the white noise covariance matrix in a\nlarge dimensional vector autoregressive (VAR) model. We first propose the\nreduced-rank covariance estimator under the setting where independent\nobservations are available. We derive the reduced-rank estimator based on a\nlatent variable model for the vector observation and give the analytical form\nof its maximum likelihood estimate. Simulation results show that the\nreduced-rank covariance estimator outperforms two competing covariance\nestimators for estimating large dimensional covariance matrices from\nindependent observations. Then we describe how to integrate the proposed\nreduced-rank estimator into the fitting of large dimensional VAR models, where\nwe consider two scenarios that require different model fitting procedures. In\nthe VAR modeling context, our reduced-rank covariance estimator not only\nprovides interpretable descriptions of the dependence structure of VAR\nprocesses but also leads to improvement in model-fitting and forecasting over\nunrestricted covariance estimators. Two real data examples are presented to\nillustrate these fitting procedures.\n", "versions": [{"version": "v1", "created": "Fri, 5 Dec 2014 23:53:40 GMT"}], "update_date": "2014-12-09", "authors_parsed": [["Davis", "Richard A.", ""], ["Zang", "Pengfei", ""], ["Zheng", "Tian", ""]]}, {"id": "1412.2345", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik, Dmitry Trushin and Ami Wiesel", "title": "Group Symmetric Robust Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider Tyler's robust covariance M-estimator under group\nsymmetry constraints. We assume that the covariance matrix is invariant to the\nconjugation action of a unitary matrix group, referred to as group symmetry.\nExamples of group symmetric structures include circulant, perHermitian and\nproper quaternion matrices. We introduce a group symmetric version of Tyler's\nestimator (STyler) and provide an iterative fixed point algorithm to compute\nit. The classical results claim that at least n=p+1 sample points in general\nposition are necessary to ensure the existence and uniqueness of Tyler's\nestimator, where p is the ambient dimension. We show that the STyler requires\nsignificantly less samples. In some groups even two samples are enough to\nguarantee its existence and uniqueness. In addition, in the case of elliptical\npopulations, we provide high probability bounds on the error of the STyler.\nThese too, quantify the advantage of exploiting the symmetry structure.\nFinally, these theoretical results are supported by numerical simulations.ted\nby numerical simulations.\n", "versions": [{"version": "v1", "created": "Sun, 7 Dec 2014 11:51:59 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 08:37:32 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2015 04:08:20 GMT"}], "update_date": "2015-09-30", "authors_parsed": [["Soloveychik", "Ilya", ""], ["Trushin", "Dmitry", ""], ["Wiesel", "Ami", ""]]}, {"id": "1412.2798", "submitter": "Rikke Ingebrigtsen", "authors": "Rikke Ingebrigtsen, Finn Lindgren, Ingelin Steinsland, Sara Martino", "title": "Estimation of a non-stationary model for annual precipitation in\n  southern Norway using replicates of the spatial field", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of stationary dependence structure parameters using only a single\nrealisation of the spatial process, typically leads to inaccurate estimates and\npoorly identified parameters. A common way to handle this is to fix some of the\nparameters, or within the Bayesian framework, impose prior knowledge. In many\napplied settings, stationary models are not flexible enough to model the\nprocess of interest, thus non-stationary spatial models are used. However, more\nflexible models usually means more parameters, and the identifiability problem\nbecomes even more challenging. We investigate aspects of estimation of a\nBayesian non-stationary spatial model for annual precipitation using\nobservations from multiple years. The model contains replicates of the spatial\nfield, which increases precision of the estimates and makes them less prior\nsensitive. Using R-INLA, we analyse precipitation data from southern Norway,\nand investigate statistical properties of the replicate model in a simulation\nstudy. The non-stationary spatial model we explore belongs to a recently\nintroduced class of stochastic partial differential equation (SPDE) based\nspatial models. This model class allows for non-stationary models with\nexplanatory variables in the dependence structure. We derive conditions to\nfacilitate prior specification for these types of non-stationary spatial\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 8 Dec 2014 22:18:46 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 08:21:22 GMT"}], "update_date": "2015-04-24", "authors_parsed": [["Ingebrigtsen", "Rikke", ""], ["Lindgren", "Finn", ""], ["Steinsland", "Ingelin", ""], ["Martino", "Sara", ""]]}, {"id": "1412.2984", "submitter": "Alexandre Janon", "authors": "Alexandre Janon (LM-Orsay,(M\\'ethodes d'Analyse Stochastique des Codes\n  et Traitements Num\\'eriques)), Ma\\\"elle Nodet (INRIA Grenoble Rh\\^one-Alpes /\n  LJK Laboratoire Jean Kuntzmann), Christophe Prieur, Cl\\'ementine Prieur\n  ((M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques),\n  INRIA Grenoble Rh\\^one-Alpes / LJK Laboratoire Jean Kuntzmann)", "title": "Global sensitivity analysis for the boundary control of an open channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to solve the global sensitivity analysis for a\nparticular control problem. More precisely, the boundary control problem of an\nopen-water channel is considered, where the boundary conditions are defined by\nthe position of a down stream overflow gate and an upper stream underflow gate.\nThe dynamics of the water depth and of the water velocity are described by the\nShallow Water equations, taking into account the bottom and friction slopes.\nSince some physical parameters are unknown, a stabilizing boundary control is\nfirst computed for their nominal values, and then a sensitivity anal-ysis is\nperformed to measure the impact of the uncertainty in the parameters on a given\nto-be-controlled output. The unknown physical parameters are de-scribed by some\nprobability distribution functions. Numerical simulations are performed to\nmeasure the first-order and total sensitivity indices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:57:04 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Janon", "Alexandre", "", "LM-Orsay,"], ["Nodet", "Ma\u00eblle", "", "INRIA Grenoble Rh\u00f4ne-Alpes /\n  LJK Laboratoire Jean Kuntzmann"], ["Prieur", "Christophe", ""], ["Prieur", "Cl\u00e9mentine", ""]]}, {"id": "1412.3158", "submitter": "Milos Stankovic", "authors": "Milo\\v{s} S. Stankovi\\'c, Nemanja Ili\\'c, Srdjan S. Stankovi\\'c", "title": "Distributed Stochastic Approximation: Weak Convergence and Network\n  Design", "comments": null, "journal-ref": null, "doi": "10.1109/TAC.2016.2545098", "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies distributed stochastic approximation algorithms based on\nbroadcast gossip on communication networks represented by digraphs. Weak\nconvergence of these algorithms is proved, and an associated ordinary\ndifferential equation (ODE) is formulated connecting convergence points with\nlocal objective functions and network properties. Using these results, a\nmethodology is proposed for network design, aimed at achieving the desired\nasymptotic behavior at consensus. Convergence rate of the algorithm is also\nanalyzed and further improved using an attached stochastic differential\nequation. Simulation results illustrate the theoretical concepts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 23:57:15 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 11:07:23 GMT"}], "update_date": "2016-09-05", "authors_parsed": [["Stankovi\u0107", "Milo\u0161 S.", ""], ["Ili\u0107", "Nemanja", ""], ["Stankovi\u0107", "Srdjan S.", ""]]}, {"id": "1412.3416", "submitter": "Angelique Cramer", "authors": "Angelique O.J. Cramer and Don van Ravenzwaaij and Dora Matzke and\n  Helen Steingroever and Ruud Wetzels and Raoul P.P.P. Grasman and Lourens J.\n  Waldorp and Eric-Jan Wagenmakers", "title": "Hidden Multiplicity in Multiway ANOVA: Prevalence and Remedies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychologists do not realize that exploratory use of the popular\nmultiway analysis of variance (ANOVA) harbors a multiple comparison problem. In\nthe case of two factors, three separate null hypotheses are subject to test\n(i.e., two main effects and one interaction). Consequently, the probability of\nat least one Type I error (if all null hypotheses are true) is 14% rather than\n5% if the three tests are independent. We explain the multiple comparison\nproblem and demonstrate that researchers almost never correct for it. To\nmitigate the problem, we describe four remedies: the omnibus F test, the\ncontrol of familywise error rate, the control of false discovery rate, and the\npreregistration of hypotheses.\n", "versions": [{"version": "v1", "created": "Wed, 10 Dec 2014 19:14:16 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2015 17:38:05 GMT"}], "update_date": "2015-07-13", "authors_parsed": [["Cramer", "Angelique O. J.", ""], ["van Ravenzwaaij", "Don", ""], ["Matzke", "Dora", ""], ["Steingroever", "Helen", ""], ["Wetzels", "Ruud", ""], ["Grasman", "Raoul P. P. P.", ""], ["Waldorp", "Lourens J.", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1412.3653", "submitter": "Keming Yu Dr", "authors": "Keming Yu, Rahim Alhamzawi, Frauke Becker and Joanne Lord", "title": "Statistical methods for body mass index: a selective review of the\n  literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obesity rates have been increasing over recent decades, causing significant\nconcern among policy makers. Excess body fat, commonly measured by body mass\nindex (BMI), is a major risk factor for several common disorders including\ndiabetes and cardiovascular disease, placing a substantial burden on health\ncare systems. % Body mass index (BMI) is one indicator for excess body fat. To\nguide effective public health action, we need to understand the complex system\nof intercorrelated influences on BMI. This paper will review both classical and\nmodern statistical methods for BMI analysis, highlighting that most of the\nclassical methods are simple and easy to implement but ignore the complexity of\ndata and structure, whereas modern methods do take complexity into\nconsideration but can be difficult to implement. A series of case studies are\npresented to illustrate these methods and some potentially useful new models\nare suggested.\n", "versions": [{"version": "v1", "created": "Thu, 11 Dec 2014 14:06:47 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Yu", "Keming", ""], ["Alhamzawi", "Rahim", ""], ["Becker", "Frauke", ""], ["Lord", "Joanne", ""]]}, {"id": "1412.4005", "submitter": "Jerome Bobin", "authors": "Jerome Bobin and Jeremy Rapin and Anthony Larue and Jean-Luc Starck", "title": "Sparsity and adaptivity for the blind separation of partially correlated\n  sources", "comments": "submitted to IEEE Transactions on signal processing", "journal-ref": null, "doi": "10.1109/TSP.2015.2391071", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind source separation (BSS) is a very popular technique to analyze\nmultichannel data. In this context, the data are modeled as the linear\ncombination of sources to be retrieved. For that purpose, standard BSS methods\nall rely on some discrimination principle, whether it is statistical\nindependence or morphological diversity, to distinguish between the sources.\nHowever, dealing with real-world data reveals that such assumptions are rarely\nvalid in practice: the signals of interest are more likely partially\ncorrelated, which generally hampers the performances of standard BSS methods.\nIn this article, we introduce a novel sparsity-enforcing BSS method coined\nAdaptive Morphological Component Analysis (AMCA), which is designed to retrieve\nsparse and partially correlated sources. More precisely, it makes profit of an\nadaptive re-weighting scheme to favor/penalize samples based on their level of\ncorrelation. Extensive numerical experiments have been carried out which show\nthat the proposed method is robust to the partial correlation of sources while\nstandard BSS techniques fail. The AMCA algorithm is evaluated in the field of\nastrophysics for the separation of physical components from microwave data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Dec 2014 14:41:14 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Bobin", "Jerome", ""], ["Rapin", "Jeremy", ""], ["Larue", "Anthony", ""], ["Starck", "Jean-Luc", ""]]}, {"id": "1412.4059", "submitter": "Daniel McCarthy", "authors": "Daniel M. McCarthy and Shane T. Jensen", "title": "Power Weighted Densities for Time Series Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While time series prediction is an important, actively studied problem, the\npredictive accuracy of time series models is complicated by non-stationarity.\nWe develop a fast and effective approach to allow for non-stationarity in the\nparameters of a chosen time series model. In our power-weighted density (PWD)\napproach, observations in the distant past are down-weighted in the likelihood\nfunction relative to more recent observations, while still giving the\npractitioner control over the choice of data model. One of the most popular\nnon-stationary techniques in the academic finance community, rolling window\nestimation, is a special case of our PWD approach. Our PWD framework is a\nsimpler alternative compared to popular state-space methods that explicitly\nmodel the evolution of an underlying state vector. We demonstrate the benefits\nof our PWD approach in terms of predictive performance compared to both\nstationary models and alternative non-stationary methods. In a financial\napplication to thirty industry portfolios, our PWD method has a significantly\nfavorable predictive performance and draws a number of substantive conclusions\nabout the evolution of the coefficients and the importance of market factors\nover time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 17:15:54 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 16:01:44 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2015 17:13:56 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["McCarthy", "Daniel M.", ""], ["Jensen", "Shane T.", ""]]}, {"id": "1412.4079", "submitter": "Ethan Anderes", "authors": "Ethan Anderes, Benjamin Wandelt, Guilhem Lavaux", "title": "Bayesian inference of CMB gravitational lensing", "comments": null, "journal-ref": null, "doi": "10.1088/0004-637X/808/2/152", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Planck satellite, along with several ground based telescopes, have mapped\nthe cosmic microwave background (CMB) at sufficient resolution and\nsignal-to-noise so as to allow a detection of the subtle distortions due to the\ngravitational influence of the intervening matter distribution. A natural\nmodeling approach is to write a Bayesian hierarchical model for the lensed CMB\nin terms of the unlensed CMB and the lensing potential. So far there has been\nno feasible algorithm for inferring the posterior distribution of the lensing\npotential from the lensed CMB map. We propose a solution that allows efficient\nMarkov Chain Monte Carlo sampling from the joint posterior of the lensing\npotential and the unlensed CMB map using the Hamiltonian Monte Carlo technique.\nThe main conceptual step in the solution is a re-parameterization of CMB\nlensing in terms of the lensed CMB and the \"inverse lensing\" potential. We\ndemonstrate a fast implementation on simulated data including noise and a sky\ncut, that uses a further acceleration based on a very mild approximation of the\ninverse lensing potential. We find that the resulting Markov Chain has short\ncorrelation lengths and excellent convergence properties, making it promising\nfor application to high resolution CMB data sets of the future.\n", "versions": [{"version": "v1", "created": "Fri, 12 Dec 2014 18:33:25 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 17:24:57 GMT"}], "update_date": "2015-08-06", "authors_parsed": [["Anderes", "Ethan", ""], ["Wandelt", "Benjamin", ""], ["Lavaux", "Guilhem", ""]]}, {"id": "1412.4383", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck, Stephen Wu, Hui Li", "title": "Robust Bayesian compressive sensing for signals in structural health\n  monitoring", "comments": "19 pages, 14 figures, 2 tables", "journal-ref": "Computer-Aided Civil and Infrastructure Engineering,29(3), pages\n  160-179, 2014", "doi": "10.1111/mice.12051", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In structural health monitoring (SHM) systems, massive amounts of data are\noften generated that need data compression techniques to reduce the cost of\nsignal transfer and storage. Compressive sensing (CS) is a novel data\nacquisition method whereby the compression is done in a sensor simultaneously\nwith the sampling. If the original sensed signal is sufficiently sparse in\nterms of some basis, the decompression can be done essentially perfectly up to\nsome critical compression ratio. In this article, a Bayesian compressive\nsensing (BCS) method is investigated that uses sparse Bayesian learning to\nreconstruct signals from a compressive sensor. By explicitly quantifying the\nuncertainty in the reconstructed signal, the BCS technique exhibits an obvious\nbenefit over existing regularized norm-minimization CS methods that provide a\nsingle signal estimate. However, current BCS algorithms suffer from a\nrobustness problem: sometimes the reconstruction errors are very large when the\nnumber of measurements is a lot less than the number of signal degrees of\nfreedom that are needed to capture the signal accurately in a directly sampled\nform. In this paper, we present improvements to the BCS reconstruction method\nto enhance its robustness so that even higher compression ratios can be used\nand we examine the tradeoff between efficiently compressing data and accurately\ndecompressing it. Synthetic data and actual acceleration data collected from a\nbridge SHM system are used as examples. Compared with the state-of-the-art BCS\nreconstruction algorithms, the improved BCS algorithm demonstrates superior\nperformance. With the same reconstruction error, the proposed BCS algorithm\nworks with relatively large compression ratios and it can achieve perfect\nlossless compression performance with quite high compression ratios.\nFurthermore, the error bars for the signal reconstruction are also quantified\neffectively.\n", "versions": [{"version": "v1", "created": "Sun, 14 Dec 2014 17:40:02 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Wu", "Stephen", ""], ["Li", "Hui", ""]]}, {"id": "1412.4479", "submitter": "Duncan Lee", "authors": "Duncan Lee and Christophe Sarran", "title": "Controlling for unmeasured confounding and spatial misalignment in\n  long-term air pollution and health studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health impact of long-term exposure to air pollution is now routinely\nestimated using spatial ecological studies, due to the recent widespread\navailability of spatial referenced pollution and disease data. However, this\nareal unit study design presents a number of statistical challenges, which if\nignored have the potential to bias the estimated pollution-health relationship.\nOne such challenge is how to control for the spatial autocorrelation present in\nthe data after accounting for the known covariates, which is caused by\nunmeasured confounding. A second challenge is how to adjust the functional form\nof the model to account for the spatial misalignment between the pollution and\ndisease data, which causes within-area variation in the pollution data. These\nchallenges have largely been ignored in existing long-term spatial air\npollution and health studies, so here we propose a novel Bayesian hierarchical\nmodel that addresses both challenges, and provide software to allow others to\napply our model to their own data. The effectiveness of the proposed model is\ncompared by simulation against a number of state of the art alternatives\nproposed in the literature, and is then used to estimate the impact of nitrogen\ndioxide and particulate matter concentrations on respiratory hospital\nadmissions in a new epidemiological study in England in 2010 at the Local\nAuthority level.\n", "versions": [{"version": "v1", "created": "Mon, 15 Dec 2014 07:21:34 GMT"}], "update_date": "2014-12-16", "authors_parsed": [["Lee", "Duncan", ""], ["Sarran", "Christophe", ""]]}, {"id": "1412.4912", "submitter": "Sylvain Le Corff", "authors": "Rainer Dahlhaus, Thierry Dumont (MODAL'X), Sylvain Le Corff\n  (LM-Orsay), Jan C. Neddermeyer", "title": "Statistical Inference for Oscillation Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model for time series with a specific oscillation pattern is proposed.\nThe model consists of a hidden phase process controlling the speed of polling\nand a nonparametric curve characterizing the pattern, leading together to a\ngeneralized state space model. Identifiability of the model is proved and a\nmethod for statistical inference based on a particle smoother and a\nnonparametric EM algorithm is developed. In particular, the oscillation pattern\nand the unobserved phase process are estimated. The proposed algorithms are\ncomputationally efficient and their performance is assessed through simulations\nand an application to human electrocardiogram recordings.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 08:19:01 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 09:07:48 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2016 08:37:56 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Dahlhaus", "Rainer", "", "MODAL'X"], ["Dumont", "Thierry", "", "MODAL'X"], ["Corff", "Sylvain Le", "", "LM-Orsay"], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1412.5000", "submitter": "Peng Ding", "authors": "Peng Ding, Avi Feller, and Luke Miratrix", "title": "Randomization Inference for Treatment Effect Variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applied researchers are increasingly interested in whether and how treatment\neffects vary in randomized evaluations, especially variation not explained by\nobserved covariates. We propose a model-free approach for testing for the\npresence of such unexplained variation. To use this randomization-based\napproach, we must address the fact that the average treatment effect, generally\nthe object of interest in randomized experiments, actually acts as a nuisance\nparameter in this setting. We explore potential solutions and advocate for a\nmethod that guarantees valid tests in finite samples despite this nuisance. We\nalso show how this method readily extends to testing for heterogeneity beyond a\ngiven model, which can be useful for assessing the sufficiency of a given\nscientific theory. We finally apply our method to the National Head Start\nImpact Study, a large-scale randomized evaluation of a Federal preschool\nprogram, finding that there is indeed significant unexplained treatment effect\nvariation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Dec 2014 13:57:41 GMT"}], "update_date": "2014-12-17", "authors_parsed": [["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Miratrix", "Luke", ""]]}, {"id": "1412.5247", "submitter": "Curtis Storlie", "authors": "Curtis Storlie, Joe Sexton, Scott Pakin, Michael Lang, Brian Reich,\n  William Rust", "title": "Modeling and Predicting Power Consumption of High Performance Computing\n  Jobs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Power is becoming an increasingly important concern for large supercomputing\ncenters. Due to cost concerns, data centers are becoming increasingly limited\nin their ability to enhance their power infrastructure to support increased\ncompute power on the machine-room floor. At Los Alamos National Laboratory it\nis projected that future-generation supercomputers will be power-limited rather\nthan budget-limited. That is, it will be less costly to acquire a large number\nof nodes than it will be to upgrade an existing data-center and machine-room\npower infrastructure to run that large number of nodes at full power. In the\npower-limited systems of the future, machines will in principle be capable of\ndrawing more power than they have available. Thus, power capping at the\nnode/job level must be used to ensure the total system power draw remains below\nthe available level. In this paper, we present a statistically grounded\nframework with which to predict (with uncertainty) how much power a given job\nwill need and use these predictions to provide an optimal node-level power\ncapping strategy. We model the power drawn by a given job (and subsequently by\nthe entire machine) using hierarchical Bayesian modeling with hidden Markov and\nDirichlet process models. We then demonstrate how this model can be used inside\nof a power-management scheme to minimize the affect of power capping on user\njobs.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 03:16:52 GMT"}, {"version": "v2", "created": "Mon, 11 May 2015 20:34:15 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Storlie", "Curtis", ""], ["Sexton", "Joe", ""], ["Pakin", "Scott", ""], ["Lang", "Michael", ""], ["Reich", "Brian", ""], ["Rust", "William", ""]]}, {"id": "1412.5351", "submitter": "Silvia Angela Osmetti", "authors": "Galina Andreeva, Raffaella Calabrese and Silvia Angela Osmetti", "title": "A comparative analysis of the UK and Italian small businesses using\n  Generalised Extreme Value models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a cross-country comparison of significant predictors of\nsmall business failure between Italy and the UK. Financial measures of\nprofitability, leverage, coverage, liquidity, scale and non-financial\ninformation are explored, some commonalities and differences are highlighted.\nSeveral models are considered, starting with the logis- tic regression which is\na standard approach in credit risk modelling. Some important improvements are\ninvestigated. Generalised Extreme Value (GEV) regression is applied to correct\nfor the symmetric link function of the logistic regression. The assumption of\nnon-linearity is relaxed through application of BGEVA, non-parametric additive\nmodel based on the GEV link function. Two methods of handling missing values\nare compared: multiple imputation and Weights of Evidence (WoE) transformation.\nThe results suggest that the best predictive performance is obtained by BGEVA,\nthus implying the necessity of taking into account the relative volume of\ndefaults and non-linear patterns when modelling SME performance. WoE for the\nmajority of models considered show better prediction as compared to multiple\nimputation, suggesting that missing values could be informative and should not\nbe assumed to be missing at random.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 11:50:09 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Andreeva", "Galina", ""], ["Calabrese", "Raffaella", ""], ["Osmetti", "Silvia Angela", ""]]}, {"id": "1412.5397", "submitter": "Juehui Shi", "authors": "Juehui Shi", "title": "Comprehensive Time-Series Regression Models Using GRETL -- U.S. GDP and\n  Government Consumption Expenditures & Gross Investment from 1980 to 2013", "comments": "82 Pages with Gretl codes included", "journal-ref": null, "doi": "10.2139/ssrn.2540535", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using Gretl, I apply ARMA, Vector ARMA, VAR, state-space model with a Kalman\nfilter, transfer-function and intervention models, unit root tests,\ncointegration test, volatility models (ARCH, GARCH, ARCH-M, GARCH-M,\nTaylor-Schwert GARCH, GJR, TARCH, NARCH, APARCH, EGARCH) to analyze quarterly\ntime series of GDP and Government Consumption Expenditures & Gross Investment\n(GCEGI) from 1980 to 2013. The article is organized as: (I) Definition; (II)\nRegression Models; (III) Discussion. Additionally, I discovered a unique\ninteraction between GDP and GCEGI in both the short-run and the long-run and\nprovided policy makers with some suggestions. For example in the short run, GDP\nresponded positively and very significantly (0.00248) to GCEGI, while GCEGI\nreacted positively but not too significantly (0.08051) to GDP. In the long run,\ncurrent GDP responded negatively and permanently (0.09229) to a shock in past\nGCEGI, while current GCEGI reacted negatively yet temporarily (0.29821) to a\nshock in past GDP. Therefore, policy makers should not adjust current GCEGI\nbased merely on the condition of current and past GDP. Although increasing\nGCEGI does help GDP in the short-term, significantly abrupt increase in GCEGI\nmight not be good to the long-term health of GDP. Instead, a balanced,\nsustainable, and economically viable solution is recommended, so that the\nshort-term benefits to the current economy from increasing GCEGI often largely\nsecured by the long-term loan outweigh or at least equal to the negative effect\nto the future economy from the long-term debt incurred by the loan. Finally, I\nfound that non-normally distributed volatility models generally perform better\nthan normally distributed ones. More specifically, TARCH-GED performs the best\nin the group of non-normally distributed, while GARCH-M does the best in the\ngroup of normally distributed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 14:03:38 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 02:11:35 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2019 05:58:19 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Shi", "Juehui", ""]]}, {"id": "1412.5565", "submitter": "Lawrence Bardwell", "authors": "Lawrence Bardwell and Paul Fearnhead", "title": "Bayesian detection of abnormal segments in multiple time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian approach to analysing multiple time-series with\nthe aim of detecting abnormal regions. These are regions where the properties\nof the data change from some normal or baseline behaviour. We allow for the\npossibility that such changes will only be present in a, potentially small,\nsubset of the time-series. We develop a general model for this problem, and\nshow how it is possible to accurately and efficiently perform Bayesian\ninference, based upon recursions that enable independent sampling from the\nposterior distribution. A motivating application for this problem comes from\ndetecting copy number variation (CNVs), using data from multiple individuals.\nPooling information across individuals can increase the power of detecting\nCNVs, but often a specific CNV will only be present in a small subset of the\nindividuals. We evaluate the Bayesian method on both simulated and real CNV\ndata, and give evidence that this approach is more accurate than a recently\nproposed method for analysing such data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 20:33:58 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2015 12:05:40 GMT"}], "update_date": "2015-08-17", "authors_parsed": [["Bardwell", "Lawrence", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1412.5656", "submitter": "Timothy Armstrong", "authors": "Timothy B. Armstrong", "title": "A Note on Minimax Testing and Confidence Intervals in Moment Inequality\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note uses a simple example to show how moment inequality models used in\nthe empirical economics literature lead to general minimax relative efficiency\ncomparisons. The main point is that such models involve inference on a low\ndimensional parameter, which leads naturally to a definition of \"distance\"\nthat, in full generality, would be arbitrary in minimax testing problems. This\ndefinition of distance is justified by the fact that it leads to a duality\nbetween minimaxity of confidence intervals and tests, which does not hold for\nother definitions of distance. Thus, the use of moment inequalities for\ninference in a low dimensional parametric model places additional structure on\nthe testing problem, which leads to stronger conclusions regarding minimax\nrelative efficiency than would otherwise be possible.\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 22:37:14 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Armstrong", "Timothy B.", ""]]}, {"id": "1412.5848", "submitter": "Francisco Louzada Professor", "authors": "Taciana K. O. Shimizu, Francisco Louzada, Adriano K. Suzuki", "title": "Analyzing Volleyball Data on a Compositional Regression Model Approach:\n  An Application to the Brazilian Men's Volleyball Super League 2011/2012 Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": "ArVix-FL-2014-03a", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volleyball has become a competitive sport with high physical and technical\nperformance. Matches results are based on the players and teams'skills as\ntechnical and tactical strategies to succeed in a championship. At this point,\nsome studies are carried out on the performance analysis of different match\nelements, contributing to the development of this sport. In this paper, we\nproposed a new approach to analyze volleyball data. The study is based on the\ncompositional data methodology modeling in regression model. The parameters are\nobtained through the maximum likelihood. We performed a simulation study to\nevaluate the estimation procedure in compositional regression model and we\nillustrated the proposed methodology considering real data set of volleyball.\n", "versions": [{"version": "v1", "created": "Thu, 18 Dec 2014 13:18:25 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Shimizu", "Taciana K. O.", ""], ["Louzada", "Francisco", ""], ["Suzuki", "Adriano K.", ""]]}, {"id": "1412.6129", "submitter": "Andriyan Suksmono Bayu", "authors": "Andriyan B. Suksmono", "title": "The Sample Allocation Problem and Non-Uniform Compressive Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses sample allocation problem (SAP) in frequency-domain\nCompressive Sampling (CS) of time-domain signals. An analysis that is relied on\ntwo fundamental CS principles; the Uniform Random Sampling (URS) and the\nUncertainty Principle (UP), is presented. We show that CS on a single- and\nmulti-band signals performs better if the URS is done only within the band and\nsuppress the out-band parts, compared to ordinary URS that ignore the band\nlimits. It means that sampling should only be done at the signal support, while\nthe non-support should be masked and suppressed in the reconstruction process.\nWe also show that for an N-length discrete time signal with K-number of\nfrequency components (Fourier coefficients), given the knowledge of the\nspectrum, URS leads to exact sampling on the location of the K-spectral peaks.\nThese results are used to formulate a sampling scheme when the boundaries of\nthe bands are not sharply distinguishable, such as in a triangular- or a\nstacked-band- spectral signals. When analyzing these cases, CS will face a\nparadox; in which narrowing the band leads to a more number of required\nsamples, whereas widening it leads to lessen the number. Accordingly; instead\nof signal analysis by dividing the signal's spectrum vertically into bands of\nfrequencies, slicing horizontally magnitude-wise yields less number of required\nsample and better reconstruction results. Moreover, it enables sample reuse\nthat reduces the sample number even further. The horizontal slicing and sample\nreuse methods imply non-uniform random sampling, where larger-magnitude part of\nthe spectrum should be allocated more sample than the lower ones.\n", "versions": [{"version": "v1", "created": "Mon, 24 Nov 2014 09:17:21 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Suksmono", "Andriyan B.", ""]]}, {"id": "1412.6137", "submitter": "Anum Ali", "authors": "Anum Ali, Mudassir Masood, Muhammad S. Sohail, Samir Al-Ghadhban and\n  Tareq Y. Al-Naffouri", "title": "Narrowband Interference Mitigation in SC-FDMA Using Bayesian Sparse\n  Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel narrowband interference (NBI) mitigation scheme\nfor SC-FDMA systems. The proposed NBI cancellation scheme exploits the\nfrequency domain sparsity of the unknown signal and adopts a low complexity\nBayesian sparse recovery procedure. At the transmitter, a few randomly chosen\nsub-carriers are kept data free to sense the NBI signal at the receiver.\nFurther, it is noted that in practice, the sparsity of the NBI signal is\ndestroyed by a grid mismatch between NBI sources and the system under\nconsideration. Towards this end, first an accurate grid mismatch model is\npresented that is capable of assuming independent offsets for multiple NBI\nsources. Secondly, prior to NBI reconstruction, the sparsity of the unknown\nsignal is restored by employing a sparsifying transform. To improve the\nspectral efficiency of the proposed scheme, a data-aided NBI recovery procedure\nis outlined that relies on adaptively selecting a subset of data carriers and\nuses them as additional measurements to enhance the NBI estimation. Finally,\nthe proposed scheme is extended to single-input multi-output systems by\nperforming a collaborative NBI support search over all antennas. Numerical\nresults are presented that depict the suitability of the proposed scheme for\nNBI mitigation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 13:17:10 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Ali", "Anum", ""], ["Masood", "Mudassir", ""], ["Sohail", "Muhammad S.", ""], ["Al-Ghadhban", "Samir", ""], ["Al-Naffouri", "Tareq Y.", ""]]}, {"id": "1412.6316", "submitter": "Lorenzo Hern\\'andez", "authors": "Lorenzo Hern\\'andez, Jorge Tejero, Jaime Vinuesa", "title": "Maximum Likelihood Estimation of the correlation parameters for\n  elliptical copulas", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an algorithm to obtain the maximum likelihood estimates of the\ncorrelation parameters of elliptical copulas. Previously existing methods for\nthis task were either fast but only approximate or exact but very\ntime-consuming, especially for high-dimensional problems. Our proposal combines\nthe advantages of both, since it obtains the exact estimates and its\nperformance makes it suitable for most practical applications. The algorithm is\ngiven with explicit expressions for the Gaussian and Student's t copulas.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 12:32:25 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Hern\u00e1ndez", "Lorenzo", ""], ["Tejero", "Jorge", ""], ["Vinuesa", "Jaime", ""]]}, {"id": "1412.6469", "submitter": "Marc Box", "authors": "Marc Box, Matt W. Jones, Nick Whiteley", "title": "A hidden Markov model for decoding and the analysis of replay in spike\n  trains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a hidden Markov model that describes variation in an animal's\nposition associated with varying levels of activity in action potential spike\ntrains of individual place cell neurons. The model incorporates a\ncoarse-graining of position, which we find to be a more parsimonious\ndescription of the system than other models. We use a sequential Monte Carlo\nalgorithm for Bayesian inference of model parameters, including the state space\ndimension, and we explain how to estimate position from spike train\nobservations (decoding). We obtain greater accuracy over other methods in the\nconditions of high temporal resolution and small neuronal sample size. We also\npresent a novel, model-based approach to the study of replay: the expression of\nspike train activity related to behaviour during times of motionlessness or\nsleep, thought to be integral to the consolidation of long-term memories. We\ndemonstrate how we can detect the time, information content and compression\nrate of replay events in simulated and real hippocampal data recorded from rats\nin two different environments, and verify the correlation between the times of\ndetected replay events and of sharp wave/ripples in the local field potential.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 18:08:02 GMT"}], "update_date": "2014-12-22", "authors_parsed": [["Box", "Marc", ""], ["Jones", "Matt W.", ""], ["Whiteley", "Nick", ""]]}, {"id": "1412.6520", "submitter": "James Long", "authors": "James P. Long, Eric C. Chi, Richard G. Baraniuk", "title": "Estimating a Common Period for a Set of Irregularly Sampled Functions\n  with Applications to Periodic Variable Star Data", "comments": "25 pages", "journal-ref": "Annals of Applied Statistics, 10(1):165-197, 2016", "doi": "10.1214/15-AOAS885", "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of a common period for a set of functions sampled\nat irregular intervals. The problem arises in astronomy, where the functions\nrepresent a star's brightness observed over time through different photometric\nfilters. While current methods can estimate periods accurately provided that\nthe brightness is well--sampled in at least one filter, there are no existing\nmethods that can provide accurate estimates when no brightness function is\nwell--sampled. In this paper we introduce two new methods for period estimation\nwhen brightnesses are poorly--sampled in all filters. The first, multiband\ngeneralized Lomb-Scargle (MGLS), extends the frequently used Lomb-Scargle\nmethod in a way that na\\\"{i}vely combines information across filters. The\nsecond, penalized generalized Lomb-Scargle (PGLS), builds on the first by more\nintelligently borrowing strength across filters. Specifically, we incorporate\nconstraints on the phases and amplitudes across the different functions using a\nnon--convex penalized likelihood function. We develop a fast algorithm to\noptimize the penalized likelihood by combining block coordinate descent with\nthe majorization-minimization (MM) principle. We illustrate our methods on\nsynthetic and real astronomy data. Both advance the state-of-the-art in period\nestimation; however, PGLS significantly outperforms MGLS when all functions are\nextremely poorly--sampled.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 20:50:01 GMT"}], "update_date": "2016-04-15", "authors_parsed": [["Long", "James P.", ""], ["Chi", "Eric C.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1412.6613", "submitter": "Quentin Berthet", "authors": "Quentin Berthet and Venkat Chandrasekaran", "title": "Resource Allocation for Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical estimation in many contemporary settings involves the\nacquisition, analysis, and aggregation of datasets from multiple sources, which\ncan have significant differences in character and in value. Due to these\nvariations, the effectiveness of employing a given resource (e.g., a sensing\ndevice or computing power) for gathering or processing data from a particular\nsource depends on the nature of that source. As a result, the appropriate\ndivision and assignment of a collection of resources to a set of data sources\ncan substantially impact the overall performance of an inferential strategy. In\nthis expository article, we adopt a general view of the notion of a resource\nand its effect on the quality of a data source, and we describe a framework for\nthe allocation of a given set of resources to a collection of sources in order\nto optimize a specified metric of statistical efficiency. We discuss several\nstylized examples involving inferential tasks such as parameter estimation and\nhypothesis testing based on heterogeneous data sources, in which optimal\nallocations can be computed either in closed form or via efficient numerical\nprocedures based on convex optimization.\n", "versions": [{"version": "v1", "created": "Sat, 20 Dec 2014 06:19:38 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Berthet", "Quentin", ""], ["Chandrasekaran", "Venkat", ""]]}, {"id": "1412.6779", "submitter": "Willem Kruijer", "authors": "Willem Kruijer, Martin Boer, Marcos Malosetti, Padraic J. Flood, Bas\n  Engel, Rik Kooke, Joost Keurentjes, Fred van Eeuwijk", "title": "Marker-based estimation of heritability in immortal populations", "comments": "6 figures. Contains all the supplementary information (p. 30-54)", "journal-ref": "Genetics (2015), Vol. 199(2), p. 379-398", "doi": "10.1534/genetics.114.167916", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heritability is a central parameter in quantitative genetics, both from an\nevolutionary and a breeding perspective. For plant traits heritability is\ntraditionally estimated by comparing within and between genotype variability.\nThis approach estimates broad-sense heritability, and does not account for\ndifferent genetic relatedness. With the availability of high-density markers\nthere is growing interest in marker based estimates of narrow-sense\nheritability, using mixed models in which genetic relatedness is estimated from\ngenetic markers. Such estimates have received much attention in human genetics\nbut are rarely reported for plant traits. A major obstacle is that current\nmethodology and software assume a single phenotypic value per genotype, hence\nrequiring genotypic means. An alternative that we propose here, is to use mixed\nmodels at individual plant or plot level. Using statistical arguments,\nsimulations and real data we investigate the feasibility of both approaches,\nand how these affect genomic prediction with G-BLUP and genome-wide association\nstudies. Heritability estimates obtained from genotypic means had very large\nstandard errors and were sometimes biologically unrealistic. Mixed models at\nindividual plant or plot level produced more realistic estimates, and for\nsimulated traits standard errors were up to 13 times smaller. Genomic\nprediction was also improved by using these mixed models, with up to a 49%\nincrease in accuracy. For GWAS on simulated traits, the use of individual plant\ndata gave almost no increase in power. The new methodology is applicable to any\ncomplex trait where multiple replicates of individual genotypes can be scored.\nThis includes important agronomic crops, as well as bacteria and fungi.\n", "versions": [{"version": "v1", "created": "Sun, 21 Dec 2014 13:15:23 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 11:39:29 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Kruijer", "Willem", ""], ["Boer", "Martin", ""], ["Malosetti", "Marcos", ""], ["Flood", "Padraic J.", ""], ["Engel", "Bas", ""], ["Kooke", "Rik", ""], ["Keurentjes", "Joost", ""], ["van Eeuwijk", "Fred", ""]]}, {"id": "1412.7334", "submitter": "Bj\\\"orn L\\\"ofdahl", "authors": "Boualem Djehiche and Bj\\\"orn L\\\"ofdahl", "title": "A hidden Markov approach to disability insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point and interval estimation of future disability inception and recovery\nrates are predominantly carried out by combining generalized linear models\n(GLM) with time series forecasting techniques into a two-step method involving\nparameter estimation from historical data and subsequent calibration of a time\nseries model. This approach may in fact lead to both conceptual and numerical\nproblems since any time trend components of the model are incoherently treated\nas both model parameters and realizations of a stochastic process. We suggest\nthat this general two-step approach can be improved in the following way:\nFirst, we assume a stochastic process form for the time trend component. The\ncorresponding transition densities are then incorporated into the likelihood,\nand the model parameters are estimated using the Expectation-Maximization\nalgorithm. We illustrate the modelling procedure by fitting the model to\nSwedish disability claims data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Dec 2014 12:12:35 GMT"}], "update_date": "2014-12-24", "authors_parsed": [["Djehiche", "Boualem", ""], ["L\u00f6fdahl", "Bj\u00f6rn", ""]]}, {"id": "1412.8563", "submitter": "Matt Taddy", "authors": "Matt Taddy, Matt Gardner, Liyun Chen, David Draper", "title": "A nonparametric Bayesian analysis of heterogeneous treatment effects in\n  digital experimentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials play an important role in how Internet companies\npredict the impact of policy decisions and product changes. In these `digital\nexperiments', different units (people, devices, products) respond differently\nto the treatment. This article presents a fast and scalable Bayesian\nnonparametric analysis of such heterogeneous treatment effects and their\nmeasurement in relation to observable covariates. New results and algorithms\nare provided for quantifying the uncertainty associated with treatment effect\nmeasurement via both linear projections and nonlinear regression trees (CART\nand Random Forests). For linear projections, our inference strategy leads to\nresults that are mostly in agreement with those from the frequentist\nliterature. We find that linear regression adjustment of treatment effect\naverages (i.e., post-stratification) can provide some variance reduction, but\nthat this reduction will be vanishingly small in the low-signal and\nlarge-sample setting of digital experiments. For regression trees, we provide\nuncertainty quantification for the machine learning algorithms that are\ncommonly applied in tree-fitting. We argue that practitioners should look to\nensembles of trees (forests) rather than individual trees in their analysis.\nThe ideas are applied on and illustrated through an example experiment\ninvolving 21 million unique users of EBay.com.\n", "versions": [{"version": "v1", "created": "Tue, 30 Dec 2014 04:38:20 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2015 20:13:41 GMT"}, {"version": "v3", "created": "Wed, 5 Aug 2015 22:23:12 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2015 05:01:50 GMT"}], "update_date": "2015-12-21", "authors_parsed": [["Taddy", "Matt", ""], ["Gardner", "Matt", ""], ["Chen", "Liyun", ""], ["Draper", "David", ""]]}]