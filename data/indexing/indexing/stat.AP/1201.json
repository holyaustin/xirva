[{"id": "1201.0022", "submitter": "Lotfi Chaari", "authors": "Lotfi Chaari, S\\'ebastien M\\'eriaux, Jean-Christophe Pesquet and\n  Philippe Ciuciu", "title": "Spatio-temporal wavelet regularization for parallel MRI reconstruction:\n  application to functional MRI", "comments": "arXiv admin note: substantial text overlap with arXiv:1103.3532", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel MRI is a fast imaging technique that enables the acquisition of\nhighly resolved images in space or/and in time. The performance of parallel\nimaging strongly depends on the reconstruction algorithm, which can proceed\neither in the original k-space (GRAPPA, SMASH) or in the image domain\n(SENSE-like methods). To improve the performance of the widely used SENSE\nalgorithm, 2D- or slice-specific regularization in the wavelet domain has been\ndeeply investigated. In this paper, we extend this approach using 3D-wavelet\nrepresentations in order to handle all slices together and address\nreconstruction artifacts which propagate across adjacent slices. The gain\ninduced by such extension (3D-Unconstrained Wavelet Regularized -SENSE:\n3D-UWR-SENSE) is validated on anatomical image reconstruction where no temporal\nacquisition is considered. Another important extension accounts for temporal\ncorrelations that exist between successive scans in functional MRI (fMRI). In\naddition to the case of 2D+t acquisition schemes addressed by some other\nmethods like kt-FOCUSS, our approach allows us to deal with 3D+t acquisition\nschemes which are widely used in neuroimaging. The resulting 3D-UWR-SENSE and\n4D-UWR-SENSE reconstruction schemes are fully unsupervised in the sense that\nall regularization parameters are estimated in the maximum likelihood sense on\na reference scan. The gain induced by such extensions is illustrated on both\nanatomical and functional image reconstruction, and also measured in terms of\nstatistical sensitivity for the 4D-UWR-SENSE approach during a fast\nevent-related fMRI protocol. Our 4D-UWR-SENSE algorithm outperforms the SENSE\nreconstruction at the subject and group levels (15 subjects) for different\ncontrasts of interest (eg, motor or computation tasks) and using different\nparallel acceleration factors (R=2 and R=4) on 2x2x3mm3 EPI images.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 18:26:14 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 11:11:30 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 21:42:10 GMT"}], "update_date": "2013-10-07", "authors_parsed": [["Chaari", "Lotfi", ""], ["M\u00e9riaux", "S\u00e9bastien", ""], ["Pesquet", "Jean-Christophe", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "1201.0040", "submitter": "Marian Grendar", "authors": "M. Grend\\'ar, J. \\v{S}kutov\\'a and V. \\v{S}pitalsk\\'y", "title": "Spam filtering by quantitative profiles", "comments": "supplementary material including the commented R source code can be\n  found at http://www.savbb.sk/~grendar/spam/Supplement.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instead of the 'bag-of-words' representation, in the quantitative profile\napproach to spam filtering and email categorization, an email is represented by\nan m-dimensional vector of numbers, with m fixed in advance. Inspired by Sroufe\net al. [Sroufe, P., Phithakkitnukoon, S., Dantu, R., and Cangussu, J. (2010).\nEmail shape analysis. In \\emph{LNCS}, 5935, pp. 18-29] two instances of\nquantitative profiles are considered: line profile and character profile.\nPerformance of these profiles is studied on the TREC 2007, CEAS 2008 and a\nprivate corpuses. At low computational costs, the two quantitative profiles\nachieve performance that is at least comparable to that of heuristic rules and\nnaive Bayes.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2011 14:18:35 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Grend\u00e1r", "M.", ""], ["\u0160kutov\u00e1", "J.", ""], ["\u0160pitalsk\u00fd", "V.", ""]]}, {"id": "1201.0153", "submitter": "David R. Bickel", "authors": "Zhenyu Yang, Zuojing Li, David R. Bickel", "title": "Empirical Bayes estimation of posterior probabilities of enrichment", "comments": "exhaustive revision of Zhenyu Yang and David R. Bickel, \"Minimum\n  Description Length Measures of Evidence for Enrichment\" (December 2010).\n  COBRA Preprint Series. Article 76. http://biostats.bepress.com/cobra/ps/art76", "journal-ref": "A comparative study of five estimators of the local false\n  discovery rate,\" BMC Bioinformatics 14, art. 87 (2013)", "doi": "10.1186/1471-2105-14-87", "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret differentially expressed genes or other discovered features,\nresearchers conduct hypothesis tests to determine which biological categories\nsuch as those of the Gene Ontology (GO) are enriched in the sense of having\ndifferential representation among the discovered features. We study application\nof better estimators of the local false discovery rate (LFDR), a probability\nthat the biological category has equivalent representation among the\npreselected features.\n  We identified three promising estimators of the LFDR for detecting\ndifferential representation: a semiparametric estimator (SPE), a normalized\nmaximum likelihood estimator (NMLE), and a maximum likelihood estimator (MLE).\nWe found that the MLE performs at least as well as the SPE for on the order of\n100 of GO categories even when the ideal number of components in its underlying\nmixture model is unknown. However, the MLE is unreliable when the number of GO\ncategories is small compared to the number of PMM components. Thus, if the\nnumber of categories is on the order of 10, the SPE is a more reliable LFDR\nestimator. The NMLE depends not only on the data but also on a specified value\nof the prior probability of differential representation. It is therefore an\nappropriate LFDR estimator only when the number of GO categories is too small\nfor application of the other methods.\n  For enrichment detection, we recommend estimating the LFDR by the MLE given\nat least a medium number (~100) of GO categories, by the SPE given a small\nnumber of GO categories (~10), and by the NMLE given a very small number (~1)\nof GO categories.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2011 16:59:25 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Yang", "Zhenyu", ""], ["Li", "Zuojing", ""], ["Bickel", "David R.", ""]]}, {"id": "1201.0167", "submitter": "Denis Chetverikov", "authors": "Denis Chetverikov", "title": "Adaptive Test of Conditional Moment Inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I construct a new test of conditional moment inequalities,\nwhich is based on studentized kernel estimates of moment functions with many\ndifferent values of the bandwidth parameter. The test automatically adapts to\nthe unknown smoothness of moment functions and has uniformly correct asymptotic\nsize. The test has high power in a large class of models with conditional\nmoment inequalities. Some existing tests have nontrivial power against\nn^{-1/2}-local alternatives in a certain class of these models whereas my\nmethod only allows for nontrivial testing against (n/\\log n)^{-1/2}-local\nalternatives in this class. There exist, however, other classes of models with\nconditional moment inequalities where the mentioned tests have much lower power\nin comparison with the test developed in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2011 19:03:12 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 19:18:08 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Chetverikov", "Denis", ""]]}, {"id": "1201.0220", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "Inference for High-Dimensional Sparse Econometric Models", "comments": null, "journal-ref": "Advances in Economics and Econometrics, 10th World Congress of\n  Econometric Society, 2011", "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about estimation and inference methods for high dimensional\nsparse (HDS) regression models in econometrics. High dimensional sparse models\narise in situations where many regressors (or series terms) are available and\nthe regression function is well-approximated by a parsimonious, yet unknown set\nof regressors. The latter condition makes it possible to estimate the entire\nregression function effectively by searching for approximately the right set of\nregressors. We discuss methods for identifying this set of regressors and\nestimating their coefficients based on $\\ell_1$-penalization and describe key\ntheoretical results. In order to capture realistic practical situations, we\nexpressly allow for imperfect selection of regressors and study the impact of\nthis imperfect selection on estimation and inference results. We focus the main\npart of the article on the use of HDS models and methods in the instrumental\nvariables model and the partially linear model. We present a set of novel\ninference results for these models and illustrate their use with applications\nto returns to schooling and growth regression.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 04:31:00 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1201.0224", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "Inference on Treatment Effects After Selection Amongst High-Dimensional\n  Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose robust methods for inference on the effect of a treatment variable\non a scalar outcome in the presence of very many controls. Our setting is a\npartially linear model with possibly non-Gaussian and heteroscedastic\ndisturbances. Our analysis allows the number of controls to be much larger than\nthe sample size. To make informative inference feasible, we require the model\nto be approximately sparse; that is, we require that the effect of confounding\nfactors can be controlled for up to a small approximation error by conditioning\non a relatively small number of controls whose identities are unknown. The\nlatter condition makes it possible to estimate the treatment effect by\nselecting approximately the right set of controls. We develop a novel\nestimation and uniformly valid inference method for the treatment effect in\nthis setting, called the \"post-double-selection\" method. Our results apply to\nLasso-type methods used for covariate selection as well as to any other model\nselection method that is able to find a sparse model with good approximation\nproperties.\n  The main attractive feature of our method is that it allows for imperfect\nselection of the controls and provides confidence intervals that are valid\nuniformly across a large class of models. In contrast, standard post-model\nselection estimators fail to provide uniform inference even in simple cases\nwith a small, fixed number of controls. Thus our method resolves the problem of\nuniform inference after model selection for a large, interesting class of\nmodels. We illustrate the use of the developed methods with numerical\nsimulations and an application to the effect of abortion on crime rates.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 04:37:19 GMT"}, {"version": "v2", "created": "Tue, 1 May 2012 01:30:10 GMT"}, {"version": "v3", "created": "Wed, 9 May 2012 15:28:20 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1201.0317", "submitter": "Brian Macdonald", "authors": "Brian Macdonald", "title": "Adjusted Plus-Minus for NHL Players using Ridge Regression with Goals,\n  Shots, Fenwick, and Corsi", "comments": "24 pages, 5 figures, 7 tables", "journal-ref": "Journal of Quantitative Analysis in Sports. Volume 8, Issue 3,\n  Pages -, ISSN (Online) 1559-0410, DOI: 10.1515/1559-0410.1447, October 2012", "doi": "10.1515/1559-0410.1447", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression-based adjusted plus-minus statistics were developed in basketball\nand have recently come to hockey. The purpose of these statistics is to provide\nan estimate of each player's contribution to his team, independent of the\nstrength of his teammates, the strength of his opponents, and other variables\nthat are out of his control. One of the main downsides of the ordinary least\nsquares regression models is that the estimates have large error bounds. Since\ncertain pairs of teammates play together frequently, collinearity is present in\nthe data and is one reason for the large errors. In hockey, the relative lack\nof scoring compared to basketball is another reason. To deal with these issues,\nwe use ridge regression, a method that is commonly used in lieu of ordinary\nleast squares regression when collinearity is present in the data. We also\ncreate models that use not only goals, but also shots, Fenwick rating (shots\nplus missed shots), and Corsi rating (shots, missed shots, and blocked shots).\nOne benefit of using these statistics is that there are roughly ten times as\nmany shots as goals, so there is much more data when using these statistics and\nthe resulting estimates have smaller error bounds. The results of our ridge\nregression models are estimates of the offensive and defensive contributions of\nforwards and defensemen during even strength, power play, and short handed\nsituations, in terms of goals per 60 minutes. The estimates are independent of\nstrength of teammates, strength of opponents, and the zone in which a player's\nshift begins.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 00:13:03 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2012 22:02:53 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Macdonald", "Brian", ""]]}, {"id": "1201.0650", "submitter": "Jerome Bobin", "authors": "Vincent Studer and Jerome Bobin and Makhlad Chahid and S. Hamed Shams\n  Mousavi and Emmanuel Candes and Maxime Dahan", "title": "Compressive Fluorescence Microscopy for Biological and Hyperspectral\n  Imaging", "comments": "Submitted to Proceedings of the National Academy of Sciences of the\n  United States of America", "journal-ref": null, "doi": "10.1073/pnas.1119511109", "report-no": null, "categories": "stat.AP physics.optics", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical theory of compressed sensing (CS) asserts that one can\nacquire signals from measurements whose rate is much lower than the total\nbandwidth. Whereas the CS theory is now well developed, challenges concerning\nhardware implementations of CS-based acquisition devices---especially in\noptics---have only started being addressed. This paper presents an\nimplementation of compressive sensing in fluorescence microscopy and its\napplications to biomedical imaging. Our CS microscope combines a dynamic\nstructured wide-field illumination and a fast and sensitive single-point\nfluorescence detection to enable reconstructions of images of fluorescent\nbeads, cells and tissues with undersampling ratios (between the number of\npixels and number of measurements) up to 32. We further demonstrate a\nhyperspectral mode and record images with 128 spectral channels and\nundersampling ratios up to 64, illustrating the potential benefits of CS\nacquisition for higher dimensional signals which typically exhibits extreme\nredundancy. Altogether, our results emphasize the interest of CS schemes for\nacquisition at a significantly reduced rate and point out to some remaining\nchallenges for CS fluorescence microscopy.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 14:55:33 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2012 05:08:39 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Studer", "Vincent", ""], ["Bobin", "Jerome", ""], ["Chahid", "Makhlad", ""], ["Mousavi", "S. Hamed Shams", ""], ["Candes", "Emmanuel", ""], ["Dahan", "Maxime", ""]]}, {"id": "1201.0651", "submitter": "Rainer Dahlhaus", "authors": "Rainer Dahlhaus, Istv\\'an Z. Kiss, and Jan C. Neddermeyer", "title": "On the relationship between the theory of cointegration and the theory\n  of phase synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of cointegration has been a leading theory in econometrics with\npowerful applications to macroeconomics during the last decades. On the other\nhand the theory of phase synchronization for weakly coupled complex oscillators\nhas been one of the leading theories in physics for many years with many\napplications to different areas of science. For example, in neuroscience phase\nsynchronization is regarded as essential for functional coupling of different\nbrain regions. In an abstract sense both theories describe the dynamic\nfluctuation around some equilibrium. In this paper, we point out that there\nexists a very close connection between both theories. Apart from phase jumps, a\nstochastic version of the Kuramoto equations can be approximated by a\ncointegrated system of difference equations. As one consequence, the rich\ntheory on statistical inference for cointegrated systems can immediately be\napplied for statistical inference on phase synchronization based on empirical\ndata. This includes tests for phase synchronization, tests for unidirectional\ncoupling and the identification of the equilibrium from data including phase\nshifts. We study two examples on a unidirectionally coupled R\\\"ossler-Lorenz\nsystem and on electrochemical oscillators. The methods from cointegration may\nalso be used to investigate phase synchronization in complex networks.\nConversely, there are many interesting results on phase synchronization which\nmay inspire new research on cointegration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 14:55:58 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2012 19:35:45 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 15:32:01 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Kiss", "Istv\u00e1n Z.", ""], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1201.0846", "submitter": "Camelia Goga", "authors": "Mohamed Chaouch and Camelia Goga", "title": "Using complex surveys to estimate the $L_1$-median of a functional\n  variable: application to electricity load curves", "comments": "to appear in International Statistical Review", "journal-ref": "International Statistical Review, 2012, Vol. 80, pages 40-59", "doi": "10.1111/j.1751-5823.2011.00172.x", "report-no": null, "categories": "stat.OT stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Mean profiles are widely used as indicators of the electricity consumption\nhabits of customers. Currently, in \\'Electricit\\'e De France (EDF), class load\nprofiles are estimated using point-wise mean function. Unfortunately, it is\nwell known that the mean is highly sensitive to the presence of outliers, such\nas one or more consumers with unusually high-levels of consumption. In this\npaper, we propose an alternative to the mean profile: the $L_1$-median profile\nwhich is more robust. When dealing with large datasets of functional data (load\ncurves for example), survey sampling approaches are useful for estimating the\nmedian profile avoiding storing the whole data. We propose here estimators of\nthe median trajectory using several sampling strategies and estimators. A\ncomparison between them is illustrated by means of a test population. We\ndevelop a stratification based on the linearized variable which substantially\nimproves the accuracy of the estimator compared to simple random sampling\nwithout replacement. We suggest also an improved estimator that takes into\naccount auxiliary information. Some potential areas for future research are\nalso highlighted.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 08:53:42 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Chaouch", "Mohamed", ""], ["Goga", "Camelia", ""]]}, {"id": "1201.0905", "submitter": "Alberto Hernando", "authors": "A. Hernando, R. Hernando, A. Plastino, A. R. Plastino", "title": "The workings of the Maximum Entropy Principle in collective human\n  behavior", "comments": "Additional material available at http://sthar.com/uploads/add.pdf", "journal-ref": "J. R. Soc. Interface 6 January 2013 vol. 10 no. 78 20120758", "doi": "10.1098/rsif.2012.0758", "report-no": null, "categories": "stat.AP physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit compelling evidence regarding how well does the MaxEnt principle\ndescribe the rank-distribution of city-populations via an exhaustive study of\nthe 50 Spanish provinces (more than 8000 cities) in a time-window of 15 years\n(1996-2010). We show that the dynamics that governs the population-growth is\nthe deciding factor that originates the observed distributions. The connection\nbetween dynamics and distributions is unravelled via MaxEnt.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 14:47:08 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 12:51:32 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Hernando", "A.", ""], ["Hernando", "R.", ""], ["Plastino", "A.", ""], ["Plastino", "A. R.", ""]]}, {"id": "1201.1000", "submitter": "Julien Carron", "authors": "Julien Carron", "title": "Information escaping the correlation hierarchy of the convergence field\n  in the study of cosmological parameters", "comments": "Matches version published in PRL. Only minor changes w.r.t. v1", "journal-ref": "Phys.Rev.Lett.108:071301,2012", "doi": "10.1103/PhysRevLett.108.071301", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using fits to numerical simulations, we show that the entire hierarchy of\nmoments quickly ceases to provide a complete description of the convergence\none-point probability density function leaving the linear regime. This suggests\nthat the full N-point correlation function hierarchy of the convergence field\nbecomes quickly generically incomplete and a very poor cosmological probe on\nnonlinear scales. At the scale of unit variance, only 5% of the Fisher\ninformation content of the one-point probability density function is still\ncontained in its hierarchy of moments, making clear that information escaping\nthe hierarchy is a far stronger effect than information propagating to higher\norder moments. It follows that the constraints on cosmological parameters\nachievable through extraction of the entire hierarchy become suboptimal by\nlarge amounts. A simple logarithmic mapping makes the moment hierarchy well\nsuited again for parameter extraction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 21:00:05 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2012 14:02:29 GMT"}], "update_date": "2012-02-27", "authors_parsed": [["Carron", "Julien", ""]]}, {"id": "1201.1085", "submitter": "Gergely Palla", "authors": "Gergely Tibely, Peter Pollner, Tamas Vicsek, Gergely Palla", "title": "Ontologies and tag-statistics", "comments": "Submitted to New Journal of Physics", "journal-ref": "New J. Phys. 14: 053009, (2012)", "doi": "10.1088/1367-2630/14/5/053009", "report-no": null, "categories": "physics.soc-ph cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the increasing popularity of collaborative tagging systems, the\nresearch on tagged networks, hypergraphs, ontologies, folksonomies and other\nrelated concepts is becoming an important interdisciplinary topic with great\nactuality and relevance for practical applications. In most collaborative\ntagging systems the tagging by the users is completely \"flat\", while in some\ncases they are allowed to define a shallow hierarchy for their own tags.\nHowever, usually no overall hierarchical organisation of the tags is given, and\none of the interesting challenges of this area is to provide an algorithm\ngenerating the ontology of the tags from the available data. In contrast, there\nare also other type of tagged networks available for research, where the tags\nare already organised into a directed acyclic graph (DAG), encapsulating the\n\"is a sub-category of\" type of hierarchy between each other. In this paper we\nstudy how this DAG affects the statistical distribution of tags on the nodes\nmarked by the tags in various real networks. We analyse the relation between\nthe tag-frequency and the position of the tag in the DAG in two large\nsub-networks of the English Wikipedia and a protein-protein interaction\nnetwork. We also study the tag co-occurrence statistics by introducing a 2d\ntag-distance distribution preserving both the difference in the levels and the\nabsolute distance in the DAG for the co-occurring pairs of tags. Our most\ninteresting finding is that the local relevance of tags in the DAG, (i.e.,\ntheir rank or significance as characterised by, e.g., the length of the\nbranches starting from them) is much more important than their global distance\nfrom the root. Furthermore, we also introduce a simple tagging model based on\nrandom walks on the DAG, capable of reproducing the main statistical features\nof tag co-occurrence.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 09:06:40 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Tibely", "Gergely", ""], ["Pollner", "Peter", ""], ["Vicsek", "Tamas", ""], ["Palla", "Gergely", ""]]}, {"id": "1201.1151", "submitter": "Claudia Kluppelberg", "authors": "Fred Espen Benth, Claudia Kl\\\"uppelberg, Gernot M\\\"uller, Linda Vos", "title": "Futures pricing in electricity markets based on stable CARMA spot models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model for the electricity spot price dynamics, which is able\nto capture seasonality, low-frequency dynamics and the extreme spikes in the\nmarket. Instead of the usual purely deterministic trend we introduce a\nnon-stationary independent increments process for the low-frequency dynamics,\nand model the large fluctuations by a non-Gaussian stable CARMA process. The\nmodel allows for analytic futures prices, and we apply these to model and\nestimate the whole market consistently. Besides standard parameter estimation,\nan estimation procedure is suggested, where we fit the non-stationary trend\nusing futures data with long time until delivery, and a robust $L^1$-filter to\nfind the states of the CARMA process. The procedure also involves the empirical\nand theoretical risk premiums which -- as a by-product -- are also estimated.\nWe apply this procedure to data from the German electricity exchange EEX, where\nwe split the empirical analysis into base load and peak load prices. We find an\noverall negative risk premium for the base load futures contracts, except for\ncontracts close to delivery, where a small positive risk premium is detected.\nThe peak load contracts, on the other hand, show a clear positive risk premium,\nwhen they are close to delivery, while the contracts in the longer end also\nhave a negative premium.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 12:34:05 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Benth", "Fred Espen", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["M\u00fcller", "Gernot", ""], ["Vos", "Linda", ""]]}, {"id": "1201.1375", "submitter": "Camelia Goga", "authors": "Camelia Goga and Anne Ruiz-Gazen", "title": "Efficient Estimation of Nonlinear Finite Population Parameters Using\n  Nonparametrics", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B, 2014, 76,\n  113-140", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Currently, the high-precision estimation of nonlinear parameters such as Gini\nindices, low-income proportions or other measures of inequality is particularly\ncrucial. In the present paper, we propose a general class of estimators for\nsuch parameters that take into account univariate auxiliary information assumed\nto be known for every unit in the population. Through a nonparametric\nmodel-assisted approach, we construct a unique system of survey weights that\ncan be used to estimate any nonlinear parameter associated with any study\nvariable of the survey, using a plug-in principle. Based on a rigorous\nfunctional approach and a linearization principle, the asymptotic variance of\nthe proposed estimators is derived, and variance estimators are shown to be\nconsistent under mild assumptions. The theory is fully detailed for penalized\nB-spline estimators together with suggestions for practical implementation and\nguidelines for choosing the smoothing parameters. The validity of the method is\ndemonstrated on data extracted from the French Labor Force Survey. Point and\nconfidence intervals estimation for the Gini index and the low-income\nproportion are derived. Theoretical and empirical results highlight our\ninterest in using a nonparametric approach versus a parametric one when\nestimating nonlinear parameters in the presence of auxiliary information.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 09:09:32 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 15:33:19 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Goga", "Camelia", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1201.1699", "submitter": "John Rodgers  Smith", "authors": "John R. Smith, Milan Nikolic, Stephen P. Smith", "title": "Automatic Methods for Handling Nearly Singular Covariance Structures\n  Using the Cholesky Decomposition of an Indefinite Matrix", "comments": "30 pages, 3 figures & 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-ex physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear models have found widespread use in statistical investigations. For\nevery linear model there exists a matrix representation for which the ReML\n(Restricted Maximum Likelihood) can be constructed from the elements of the\ncorresponding matrix. This method works in the standard manner when the\ncovariance structure is non-singular. It can also be used in the case where the\ncovariance structure is singular, because the method identifies particular\nnon-stochastic linear combinations of the observations which must be\nconstrained to zero. In order to use this method, the Cholesky decomposition\nhas to be generalized to symmetric and indefinite matrices using complex\narithmetic methods. This method is applied to the problem of determining the\nspatial size (vertex) for the Higgs Boson decay in the Higgs -> 4 lepton\nchannel. A comparison based on the Chi-Square variable from the vertex fit for\nHiggs signal and t-tbar background is presented and shows that the background\ncan be greatly suppressed using the Chi-Square variable. One of the major\nadvantages of this novel method over the currently adopted technique of\nb-tagging is that it is not affected by multiple interactions (pile up).\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2012 06:59:28 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2013 00:01:54 GMT"}], "update_date": "2013-07-31", "authors_parsed": [["Smith", "John R.", ""], ["Nikolic", "Milan", ""], ["Smith", "Stephen P.", ""]]}, {"id": "1201.2337", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Nial Friel", "title": "Bayesian model selection for exponential random graph models", "comments": "30 pages; Accepted to appear in Social Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential random graph models are a class of widely used exponential family\nmodels for social networks. The topological structure of an observed network is\nmodelled by the relative prevalence of a set of local sub-graph configurations\ntermed network statistics. One of the key tasks in the application of these\nmodels is which network statistics to include in the model. This can be thought\nof as statistical model selection problem. This is a very challenging\nproblem---the posterior distribution for each model is often termed \"doubly\nintractable\" since computation of the likelihood is rarely available, but also,\nthe evidence of the posterior is, as usual, intractable. The contribution of\nthis paper is the development of a fully Bayesian model selection method based\non a reversible jump Markov chain Monte Carlo algorithm extension of Caimo and\nFriel (2011) which estimates the posterior probability for each competing\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 15:56:03 GMT"}, {"version": "v2", "created": "Thu, 12 Jan 2012 11:02:26 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2013 09:39:06 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Caimo", "Alberto", ""], ["Friel", "Nial", ""]]}, {"id": "1201.2612", "submitter": "Thordis Thorarinsdottir", "authors": "Nina Schuhen, Thordis L. Thorarinsdottir and Tilmann Gneiting", "title": "Ensemble model output statistics for wind vectors", "comments": null, "journal-ref": null, "doi": "10.1175/MWR-D-12-00028.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate ensemble model output statistics (EMOS) technique for the\npostprocessing of ensemble forecasts of two-dimensional wind vectors is\nproposed, where the postprocessed probabilistic forecast takes the form of a\nbivariate normal probability density function. The postprocessed means and\nvariances of the wind vector components are linearly bias-corrected versions of\nthe ensemble means and ensemble variances, respectively, and the conditional\ncorrelation between the wind components is represented by a trigonometric\nfunction of the ensemble mean wind direction. In a case study on 48-hour\nforecasts of wind vectors over the North American Pacific Northwest with the\nUniversity of Washington Mesoscale Ensemble, the bivariate EMOS density\nforecasts were calibrated and sharp, and showed considerable improvement over\nthe raw ensemble and reference forecasts, including ensemble copula coupling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 16:40:47 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Schuhen", "Nina", ""], ["Thorarinsdottir", "Thordis L.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1201.2770", "submitter": "Alberto Caimo", "authors": "Alberto Caimo and Nial Friel", "title": "Bergm: Bayesian Exponential Random Graphs in R", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe the main featuress of the Bergm package for the\nopen-source R software which provides a comprehensive framework for Bayesian\nanalysis for exponential random graph models: tools for parameter estimation,\nmodel selection and goodness-of-fit diagnostics. We illustrate the capabilities\nof this package describing the algorithms through a tutorial analysis of two\nwell-known network datasets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 09:04:47 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2013 15:58:44 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2014 16:45:25 GMT"}], "update_date": "2014-01-29", "authors_parsed": [["Caimo", "Alberto", ""], ["Friel", "Nial", ""]]}, {"id": "1201.3087", "submitter": "Peter Klimek", "authors": "Peter Klimek, Yuri Yegorov, Rudolf Hanel, Stefan Thurner", "title": "Statistical detection of systematic election irregularities", "comments": "For data see also\n  http://www.complex-systems.meduniwien.ac.at/elections/election.html", "journal-ref": "Proceedings of the National Academy of Sciences USA 109,\n  16469-16473, 2012", "doi": "10.1073/pnas.1210722109", "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Democratic societies are built around the principle of free and fair\nelections, that each citizen's vote should count equal. National elections can\nbe regarded as large-scale social experiments, where people are grouped into\nusually large numbers of electoral districts and vote according to their\npreferences. The large number of samples implies certain statistical\nconsequences for the polling results which can be used to identify election\nirregularities. Using a suitable data collapse, we find that vote distributions\nof elections with alleged fraud show a kurtosis of hundred times more than\nnormal elections on certain levels of data aggregation. As an example we show\nthat reported irregularities in recent Russian elections are indeed well\nexplained by systematic ballot stuffing and develop a parametric model\nquantifying to which extent fraudulent mechanisms are present. We show that if\nspecific statistical properties are present in an election, the results do not\nrepresent the will of the people. We formulate a parametric test detecting\nthese statistical properties in election results. Remarkably, this technique\nproduces similar outcomes irrespective of the data resolution and thus allows\nfor cross-country comparisons.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2012 12:48:16 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2012 21:25:41 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2013 10:02:07 GMT"}, {"version": "v4", "created": "Thu, 27 Jun 2013 12:57:54 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Klimek", "Peter", ""], ["Yegorov", "Yuri", ""], ["Hanel", "Rudolf", ""], ["Thurner", "Stefan", ""]]}, {"id": "1201.3380", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Steven. M. Hill and Sach Mukherjee", "title": "On the relationship between ODEs and DBNs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Li et al. (Bioinformatics 27(19), 2686-91, 2011) proposed a method,\ncalled Differential Equation-based Local Dynamic Bayesian Network (DELDBN), for\nreverse engineering gene regulatory networks from time-course data. We commend\nthe authors for an interesting paper that draws attention to the close\nrelationship between dynamic Bayesian networks (DBNs) and differential\nequations (DEs). Their central claim is that modifying a DBN to model Euler\napproximations to the gradient rather than expression levels themselves is\nbeneficial for network inference. The empirical evidence provided is based on\ntime-course data with equally-spaced observations. However, as we discuss\nbelow, in the particular case of equally-spaced observations, Euler\napproximations and conventional DBNs lead to equivalent statistical models\nthat, absent artefacts due to the estimation procedure, yield networks with\nidentical inter-gene edge sets. Here, we discuss further the relationship\nbetween DEs and conventional DBNs and present new empirical results on\nunequally spaced data which demonstrate that modelling Euler approximations in\na DBN can lead to improved network reconstruction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 21:47:45 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2012 20:46:19 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Oates", "Chris. J.", ""], ["Hill", "Steven. M.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1201.3599", "submitter": "Ioannis Schizas", "authors": "Ioannis D. Schizas and Georgios B. Giannakis", "title": "Covariance Eigenvector Sparsity for Compression and Denoising", "comments": "IEEE Transcations on Signal Processing, 2012 (to appear)", "journal-ref": null, "doi": "10.1109/TSP.2012.2186130", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity in the eigenvectors of signal covariance matrices is exploited in\nthis paper for compression and denoising. Dimensionality reduction (DR) and\nquantization modules present in many practical compression schemes such as\ntransform codecs, are designed to capitalize on this form of sparsity and\nachieve improved reconstruction performance compared to existing\nsparsity-agnostic codecs. Using training data that may be noisy a novel\nsparsity-aware linear DR scheme is developed to fully exploit sparsity in the\ncovariance eigenvectors and form noise-resilient estimates of the principal\ncovariance eigenbasis. Sparsity is effected via norm-one regularization, and\nthe associated minimization problems are solved using computationally efficient\ncoordinate descent iterations. The resulting eigenspace estimator is shown\ncapable of identifying a subset of the unknown support of the eigenspace basis\nvectors even when the observation noise covariance matrix is unknown, as long\nas the noise power is sufficiently low. It is proved that the sparsity-aware\nestimator is asymptotically normal, and the probability to correctly identify\nthe signal subspace basis support approaches one, as the number of training\ndata grows large. Simulations using synthetic data and images, corroborate that\nthe proposed algorithms achieve improved reconstruction quality relative to\nalternatives.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 19:14:33 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Schizas", "Ioannis D.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1201.3935", "submitter": "Vincent Dubourg", "authors": "Vincent Dubourg and Jean-Marc Bourinet and Bruno Sudret", "title": "Reliability-based design optimization of imperfect shells using adaptive\n  kriging meta-models", "comments": "This paper has been withdrawn by the author in favor of arXiv:\n  1104.3479", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal and robust design of structures has gained much attention in the\npast ten years due to the ever increasing need for manufacturers to build\nrobust systems at the lowest cost. Reliability-based design optimization (RBDO)\nallows the analyst to minimize some cost function while ensuring some minimal\nperformances cast as admissible probabilities of failure for a set of\nperformance functions. In order to address real-world problems in which the\nperformance is assessed through computational models (e.g. large scale finite\nelement models) meta-modelling techniques have been developed in the past\ndecade. This paper introduces adaptive kriging surrogate models to solve the\nRBDO problem. The latter is cast in an augmented space that \"sums up\" the range\nof the design space and the aleatory uncertainty in the design parameters and\nthe environmental conditions. Thus the surrogate model is used (i) for\nevaluating robust estimates of the probabilities of failure (and for enhancing\nthe computational experimental design by adaptive sampling) in order to achieve\nthe requested accuracy and (ii) for applying the gradient-based optimization\nalgorithm. The approach is applied to the optimal design of imperfect stiffened\ncylinder shells used in submarine engineering. For this application the\nperformance of the structure is related to buckling which is addressed here by\nmeans of the asymptotic numerical method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 21:51:41 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 16:06:39 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Dubourg", "Vincent", ""], ["Bourinet", "Jean-Marc", ""], ["Sudret", "Bruno", ""]]}, {"id": "1201.4114", "submitter": "Thomas Loredo", "authors": "Thomas J. Loredo", "title": "Sines, steps and droplets: Semiparametric Bayesian modeling of arrival\n  time series", "comments": "4 pages, 1 figure; to appear in the proceedings of IAU Symposium 285,\n  \"New Horizons in Time Domain Astronomy\" (proceedings eds. Elizabeth Griffin,\n  Bob Hanisch, and Rob Seaman), Cambridge University Press; see\n  http://www.physics.ox.ac.uk/IAUS285/", "journal-ref": null, "doi": "10.1017/S1743921312000300", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I describe ongoing work developing Bayesian methods for flexible modeling of\narrival time series data without binning, aiming to improve detection and\nmeasurement of X-ray and gamma-ray pulsars, and of pulses in gamma-ray bursts.\nThe methods use parametric and semiparametric Poisson point process models for\nthe event rate, and by design have close connections to conventional\nfrequentist methods currently used in time-domain astronomy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 17:31:35 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Loredo", "Thomas J.", ""]]}, {"id": "1201.4118", "submitter": "Glen Coppersmith", "authors": "Glen A. Coppersmith and Carey E. Priebe", "title": "Vertex Nomination via Content and Context", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If I know of a few persons of interest, how can a combination of human\nlanguage technology and graph theory help me find other people similarly\ninteresting? If I know of a few people committing a crime, how can I determine\ntheir co-conspirators? Given a set of actors deemed interesting, we seek other\nactors who are similarly interesting. We use a collection of communications\nencoded as an attributed graph, where vertices represents actors and edges\nconnect pairs of actors that communicate. Attached to each edge is the set of\ndocuments wherein that pair of actors communicate, providing content in context\n- the communication topic in the context of who communicates with whom. In\nthese documents, our identified interesting actors communicate amongst each\nother and with other actors whose interestingness is unknown. Our objective is\nto nominate the most likely interesting vertex from all vertices with unknown\ninterestingness. As an illustrative example, the Enron email corpus consists of\ncommunications between actors, some of which are allegedly committing fraud.\nSome of their fraudulent activity is captured in emails, along with many\ninnocuous emails (both between the fraudsters and between the other employees\nof Enron); we are given the identities of a few fraudster vertices and asked to\nnominate other vertices in the graph as likely representing other actors\ncommitting fraud. Foundational theory and initial experimental results indicate\nthat approaching this task with a joint model of content and context improves\nthe performance (as measured by standard information retrieval measures) over\neither content or context alone.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 17:42:26 GMT"}], "update_date": "2012-01-20", "authors_parsed": [["Coppersmith", "Glen A.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1201.4129", "submitter": "Taiane Prass", "authors": "S\\'ilvia R. C. Lopes and Taiane S. Prass", "title": "Theoretical Results on FIEGARCH Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we present a theoretical study on the main properties of Fractionally\nIntegrated Exponential Generalized Autoregressive Conditional Heteroskedastic\n(FIEGARCH) processes. We analyze the conditions for the existence, the\ninvertibility, the stationarity and the ergodicity of these processes. We prove\nthat, if $\\{X_t\\}_{t \\in \\mathds{Z}}$ is a FIEGARCH$(p,d,q)$ process then,\nunder mild conditions, $\\{\\ln(X_t^2)\\}_{t\\in\\mathds{Z}}$ is an ARFIMA$(q,d,0)$,\nthat is, an autoregressive fractionally integrated moving average process. The\nconvergence order for the polynomial coefficients that describes the volatility\nis presented and results related to the spectral representation and to the\ncovariance structure of both processes $\\{\\ln(X_t^2)\\}_{t\\in\\mathds{Z}}$ and $\\\n{\\ln(\\sigma_t^2)\\}_{t\\in\\mathds{Z}}$ are also discussed. Expressions for the\nkurtosis and the asymmetry measures for any stationary FIEGARCH$(p,d,q)$\nprocess are also derived. The $h$-step ahead forecast for the processes\n$\\{X_t\\}_{t \\in \\mathds{Z}}$, $\\{\\ln(\\sigma_t^2)\\}_{t\\in\\mathds{Z}}$ and\n$\\{\\ln(X_t^2)\\}_{t\\in\\mathds{Z}}$ are given with their respective mean square\nerror forecast. The work also presents a Monte Carlo simulation study showing\nhow to generate, estimate and forecast based on six different FIEGARCH models.\nThe forecasting performance of six models belonging to the class of\nautoregressive conditional heteroskedastic models (namely, ARCH-type models)\nand radial basis models is compared through an empirical application to\nBrazilian stock market exchange index.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2012 18:17:05 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2013 17:58:00 GMT"}], "update_date": "2013-03-26", "authors_parsed": [["Lopes", "S\u00edlvia R. C.", ""], ["Prass", "Taiane S.", ""]]}, {"id": "1201.4239", "submitter": "Gabriele Martinelli", "authors": "Gabriele Martinelli, Jo Eidsvik and Ragnar Hauge", "title": "Dynamic Decision Making for Graphical Models Applied to Oil Exploration", "comments": "This paper has been withdrawn by the authors. 22 pages, 7 figures,\n  submitted", "journal-ref": null, "doi": "10.1016/j.ejor.2013.04.057", "report-no": "Technical Report in Statistics N. 12/2011, Dept. of Mathematical\n  Sciences, NTNU", "categories": "stat.AP cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the authors. We present a framework for\nsequential decision making in problems described by graphical models. The\nsetting is given by dependent discrete random variables with associated costs\nor revenues. In our examples, the dependent variables are the potential\noutcomes (oil, gas or dry) when drilling a petroleum well. The goal is to\ndevelop an optimal selection strategy that incorporates a chosen utility\nfunction within an approximated dynamic programming scheme. We propose and\ncompare different approximations, from simple heuristics to more complex\niterative schemes, and we discuss their computational properties. We apply our\nstrategies to oil exploration over multiple prospects modeled by a directed\nacyclic graph, and to a reservoir drilling decision problem modeled by a Markov\nrandom field. The results show that the suggested strategies clearly improve\nthe simpler intuitive constructions, and this is useful when selecting\nexploration policies.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2012 09:46:59 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2013 14:31:37 GMT"}], "update_date": "2013-07-01", "authors_parsed": [["Martinelli", "Gabriele", ""], ["Eidsvik", "Jo", ""], ["Hauge", "Ragnar", ""]]}, {"id": "1201.4261", "submitter": "Klaas Slooten", "authors": "Klaas Slooten and Ronald Meester", "title": "Forensic Identification: Database likelihood ratios and familial DNA\n  searching", "comments": "Changes w.r.t. v2: paper shortened for better readability; erroneous\n  result on searching for half-siblings with sibling index (due to simulation\n  error) replaced with correct one", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Familial Searching is the process of searching in a DNA database for\nrelatives of a certain individual. It is well known that in order to evaluate\nthe genetic evidence in favour of a certain given form of relatedness between\ntwo individuals, one needs to calculate the appropriate likelihood ratio, which\nis in this context called a Kinship Index. Suppose that the database contains,\nfor a given type of relative, at most one related individual. Given prior\nprobabilities for being the relative for all persons in the database, we derive\nthe likelihood ratio for each database member in favour of being that relative.\nThis likelihood ratio takes all the Kinship Indices between the target\nindividual and the members of the database into account. We also compute the\ncorresponding posterior probabilities. We then discuss two methods to select a\nsubset from the database that contains the relative with a known probability,\nor at least a useful lower bound thereof. One method needs prior probabilities\nand yields posterior probabilities, the other does not. We discuss the relation\nbetween the approaches, and illustrate the methods with familial searching\ncarried out in the Dutch National DNA Database.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2012 11:01:42 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2012 15:48:30 GMT"}, {"version": "v3", "created": "Sun, 21 Oct 2012 19:42:51 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Slooten", "Klaas", ""], ["Meester", "Ronald", ""]]}, {"id": "1201.4551", "submitter": "Hazuki Ishida", "authors": "Hazuki Ishida", "title": "Fossil fuel consumption and economic growth: causality relationship in\n  the world", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the author due to some inaccurate\ndescriptions in the section of INTRODUCTION and CONCLUSIONS.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2012 11:46:34 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2012 12:19:44 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2013 10:32:04 GMT"}], "update_date": "2013-06-19", "authors_parsed": [["Ishida", "Hazuki", ""]]}, {"id": "1201.4863", "submitter": "James Long", "authors": "James P. Long, Noureddine El Karoui, John A. Rice, Joseph W. Richards,\n  Joshua S. Bloom", "title": "Optimizing Automated Classification of Periodic Variable Stars in New\n  Synoptic Surveys", "comments": "30 pages, 25 figures", "journal-ref": null, "doi": "10.1086/664960", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient and automated classification of periodic variable stars is becoming\nincreasingly important as the scale of astronomical surveys grows. Several\nrecent papers have used methods from machine learning and statistics to\nconstruct classifiers on databases of labeled, multi--epoch sources with the\nintention of using these classifiers to automatically infer the classes of\nunlabeled sources from new surveys. However, the same source observed with two\ndifferent synoptic surveys will generally yield different derived metrics\n(features) from the light curve. Since such features are used in classifiers,\nthis survey-dependent mismatch in feature space will typically lead to degraded\nclassifier performance. In this paper we show how and why feature distributions\nchange using OGLE and \\textit{Hipparcos} light curves. To overcome survey\nsystematics, we apply a method, \\textit{noisification}, which attempts to\nempirically match distributions of features between the labeled sources used to\nconstruct the classifier and the unlabeled sources we wish to classify. Results\nfrom simulated and real--world light curves show that noisification can\nsignificantly improve classifier performance. In a three--class problem using\nlight curves from \\textit{Hipparcos} and OGLE, noisification reduces the\nclassifier error rate from 27.0% to 7.0%. We recommend that noisification be\nused for upcoming surveys such as Gaia and LSST and describe some of the\npromises and challenges of applying noisification to these surveys.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 21:01:42 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 22:03:11 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Long", "James P.", ""], ["Karoui", "Noureddine El", ""], ["Rice", "John A.", ""], ["Richards", "Joseph W.", ""], ["Bloom", "Joshua S.", ""]]}, {"id": "1201.5046", "submitter": "Gregory Nuel", "authors": "Vittorio Perduca and Christine Sinoquet and Raphael Mourad and Gregory\n  Nuel", "title": "Alternative Methods for H1 Simulations in Genome Wide Association\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the statistical power to detect susceptibility variants plays a\ncritical role in GWA studies both from the prospective and retrospective points\nof view. Power is empirically estimated by simulating phenotypes under a\ndisease model H1. For this purpose, the \"gold\" standard consists in simulating\ngenotypes given the phenotypes (e.g. Hapgen). We introduce here an alternative\napproach for simulating phenotypes under H1 that does not require generating\nnew genotypes for each simulation. In order to simulate phenotypes with a fixed\ntotal number of cases and under a given disease model, we suggest three\nalgorithms: i) a simple rejection algorithm; ii) a numerical Markov Chain\nMonte-Carlo (MCMC) approach; iii) and an exact and efficient backward sampling\nalgorithm. In our study, we validated the three algorithms both on a\ntoy-dataset and by comparing them with Hapgen on a more realistic dataset. As\nan application, we then conducted a simulation study on a 1000 Genomes Project\ndataset consisting of 629 individuals (314 cases) and 8,048 SNPs from\nChromosome X. We arbitrarily defined an additive disease model with two\nsusceptibility SNPs and an epistatic effect. The three algorithms are\nconsistent, but backward sampling is dramatically faster than the other two.\nOur approach also gives consistent results with Hapgen. Using our application\ndata, we showed that our limited design requires a biological a priori to limit\nthe investigated region. We also proved that epistatic effects can play a\nsignificant role even when simple marker statistics (e.g. trend) are used. We\nfinally showed that the overall performance of a GWA study strongly depends on\nthe prevalence of the disease: the larger the prevalence, the better the power.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 16:54:33 GMT"}], "update_date": "2012-01-25", "authors_parsed": [["Perduca", "Vittorio", ""], ["Sinoquet", "Christine", ""], ["Mourad", "Raphael", ""], ["Nuel", "Gregory", ""]]}, {"id": "1201.5076", "submitter": "Vural Aksakalli", "authors": "Vural Aksakalli, Elvan Ceyhan", "title": "Optimal obstacle placement with disambiguations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS556 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1730-1774", "doi": "10.1214/12-AOAS556", "report-no": "IMS-AOAS-AOAS556", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the optimal obstacle placement with disambiguations problem\nwherein the goal is to place true obstacles in an environment cluttered with\nfalse obstacles so as to maximize the total traversal length of a navigating\nagent (NAVA). Prior to the traversal, the NAVA is given location information\nand probabilistic estimates of each disk-shaped hindrance (hereinafter referred\nto as disk) being a true obstacle. The NAVA can disambiguate a disk's status\nonly when situated on its boundary. There exists an obstacle placing agent\n(OPA) that locates obstacles prior to the NAVA's traversal. The goal of the OPA\nis to place true obstacles in between the clutter in such a way that the NAVA's\ntraversal length is maximized in a game-theoretic sense. We assume the OPA\nknows the clutter spatial distribution type, but not the exact locations of\nclutter disks. We analyze the traversal length using repeated measures analysis\nof variance for various obstacle number, obstacle placing scheme and clutter\nspatial distribution type combinations in order to identify the optimal\ncombination. Our results indicate that as the clutter becomes more regular\n(clustered), the NAVA's traversal length gets longer (shorter). On the other\nhand, the traversal length tends to follow a concave-down trend as the number\nof obstacles increases. We also provide a case study on a real-world maritime\nminefield data set.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 18:20:47 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2012 00:21:30 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2013 08:04:44 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Aksakalli", "Vural", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1201.5169", "submitter": "Bin Zhu", "authors": "Bin Zhu, Jeremy M. G. Taylor and Peter X.-K. Song", "title": "Signal extraction and breakpoint identification for array CGH data using\n  robust state space model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Array comparative genomic hybridization(CGH) is a high resolution technique\nto assess DNA copy number variation. Identifying breakpoints where copy number\nchanges will enhance the understanding of the pathogenesis of human diseases,\nsuch as cancers. However, the biological variation and experimental errors\ncontained in array CGH data may lead to false positive identification of\nbreakpoints. We propose a robust state space model for array CGH data analysis.\nThe model consists of two equations: an observation equation and a state\nequation, in which both the measurement error and evolution error are specified\nto follow t-distributions with small degrees of freedom. The completely\nunspecified CGH profiles are estimated by a Markov Chain Monte Carlo(MCMC)\nalgorithm. Breakpoints and outliers are identified by a novel backward\nselection procedure based on posterior draws of the CGH profiles. Compared to\nthree other popular methods, our method demonstrates several desired features,\nincluding false positive rate control, robustness against outliers, and\nsuperior power of breakpoint detection. All these properties are illustrated\nusing simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2012 01:12:35 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["Zhu", "Bin", ""], ["Taylor", "Jeremy M. G.", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "1201.5893", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "A new stochastic differential equation modelling incidence and\n  prevalence with an application to systemic lupus erythematosus in England and\n  Wales, 1995", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reformulates a common illness-death model in terms of a new\nsystem of stochastical differential equations (SDEs). The SDEs are used to\nestimate epidemiological characteristics and burden of systemic lupus\nerythematosus in England and Wales in 1995.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 21:01:14 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1201.5968", "submitter": "Qiyu Jin", "authors": "Qiyu Jin, Ion Grama and Quansheng Liu", "title": "A New Poisson Noise Filter based on Weights Optimization", "comments": "26 pages, 7 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new image denoising algorithm when the data is contaminated by a\nPoisson noise. As in the Non-Local Means filter, the proposed algorithm is\nbased on a weighted linear combination of the bserved image. But in contract to\nthe latter where the weights are defined by a Gaussian kernel, we propose to\nchoose them in an optimal way. First some \"oracle\" weights are defined by\nminimizing a very tight upper bound of the Mean Square Error. For a practical\napplication the weights are estimated from the observed image. We prove that\nthe proposed filter converges at the usual optimal rate to the true image.\nSimulation results are presented to compare the performance of the presented\nfilter with conventional filtering methods.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2012 15:38:09 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Jin", "Qiyu", ""], ["Grama", "Ion", ""], ["Liu", "Quansheng", ""]]}, {"id": "1201.6385", "submitter": "Felix Thoemmes", "authors": "Felix Thoemmes", "title": "Propensity score matching in SPSS", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching is a tool for causal inference in non-randomized\nstudies that allows for conditioning on large sets of covariates. The use of\npropensity scores in the social sciences is currently experiencing a tremendous\nincrease; however it is far from a commonly used tool. One impediment towards a\nmore wide-spread use of propensity score methods is the reliance on specialized\nsoftware, because many social scientists still use SPSS as their main analysis\ntool. The current paper presents an implementation of various propensity score\nmatching methods in SPSS. Specifically the presented SPSS custom dialog allows\nresearchers to specify propensity score methods using the familiar\npoint-and-click interface. The software allows estimation of the propensity\nscore using logistic regression and specifying nearest-neighbor matching with\nmany options, e.g., calipers, region of common support, matching with and\nwithout replacement, and matching one to many units. Detailed balance\nstatistics and graphs are produced by the program.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2012 21:45:42 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Thoemmes", "Felix", ""]]}]