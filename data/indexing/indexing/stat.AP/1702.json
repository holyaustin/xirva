[{"id": "1702.00099", "submitter": "Ranjan Maitra", "authors": "Ye Tian, Ranjan Maitra, William Q. Meeker and Stephen D. Holland", "title": "A Statistical Framework for Improved Automatic Flaw Detection in\n  Nondestructive Evaluation Images", "comments": "To appear in Technometrics", "journal-ref": null, "doi": "10.1080/00401706.2016.1153000", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nondestructive evaluation (NDE) techniques are widely used to detect flaws in\ncritical components of systems like aircraft engines, nuclear power plants and\noil pipelines in order to prevent catastrophic events. Many modern NDE systems\ngenerate image data. In some applications an experienced inspector performs the\ntedious task of visually examining every image to provide accurate conclusions\nabout the existence of flaws. This approach is labor-intensive and can cause\nmisses due to operator ennui. Automated evaluation methods seek to eliminate\nhuman-factors variability and improve throughput. Simple methods based on peak\namplitude in an image are sometimes employed and a trained-operator-controlled\nrefinement that uses a dynamic threshold based on signal-to-noise ratio (SNR)\nhas also been implemented. We develop an automated and optimized detection\nprocedure that mimics these operations. The primary goal of our methodology is\nto reduce the number of images requiring expert visual evaluation by filtering\nout images that are overwhelmingly definitive on the existence or absence of a\nflaw. We use an appropriate model for the observed values of the SNR-detection\ncriterion to estimate the probability of detection. Our methodology outperforms\ncurrent methods in terms of its ability to detect flaws.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 01:00:14 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Tian", "Ye", ""], ["Maitra", "Ranjan", ""], ["Meeker", "William Q.", ""], ["Holland", "Stephen D.", ""]]}, {"id": "1702.00111", "submitter": "Ranjan Maitra", "authors": "Israel Almod\\'ovar-Rivera and Ranjan Maitra", "title": "FAST Adaptive Smoothing and Thresholding for Improved Activation\n  Detection in Low-Signal fMRI", "comments": "26 pages, 2 tables, 19 figures. Accepted for publication in IEEE\n  Transactions on Medical Imaging", "journal-ref": null, "doi": "10.1109/TMI.2019.2915052", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional Magnetic Resonance Imaging is a noninvasive tool for studying\ncerebral function. Many factors challenge activation detection, especially in\nlow-signal scenarios that arise in the performance of high-level cognitive\ntasks. We provide a fully automated fast adaptive smoothing and thresholding\n(FAST) algorithm that uses smoothing and extreme value theory on correlated\nstatistical parametric maps for thresholding. Performance on experiments\nspanning a range of low-signal settings is very encouraging. The methodology\nalso performs well in a study to identify the cerebral regions that perceive\nonly-auditory-reliable or only-visual-reliable speech stimuli.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 03:17:50 GMT"}, {"version": "v2", "created": "Thu, 2 Feb 2017 03:12:48 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 18:08:37 GMT"}, {"version": "v4", "created": "Sun, 5 May 2019 04:55:49 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Almod\u00f3var-Rivera", "Israel", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1702.00261", "submitter": "Leah R. Johnson", "authors": "Leah R. Johnson, Robert B. Gramacy, Jeremy Cohen, Erin Mordecai,\n  Courtney Murdock, Jason Rohr, Sadie J. Ryan, Anna M. Stewart-Ibarra, Daniel\n  Weikel", "title": "Phenomenological forecasting of disease incidence using heteroskedastic\n  Gaussian processes: a dengue case study", "comments": "39 pages, 13 figures, 4 tables, including appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2015 the US federal government sponsored a dengue forecasting competition\nusing historical case data from Iquitos, Peru and San Juan, Puerto Rico.\nCompetitors were evaluated on several aspects of out-of-sample forecasts\nincluding the targets of peak week, peak incidence during that week and total\nseason incidence across each of several seasons. Our team was one of the top\nperformers of that competition, outperforming all other teams in multiple\ntargets/locals. In this paper we report on our methodology, a large component\nof which, surprisingly, ignores the known biology of epidemics at large---in\nparticular relationships between dengue transmission and environmental\nfactors---and instead relies on flexible nonparametric nonlinear Gaussian\nprocess (GP) regression fits that \"memorize\" the trajectories of past seasons,\nand then \"match\" the dynamics of the unfolding season to past ones in\nreal-time. Our phenomenological approach has advantages in situations where\ndisease dynamics are less well understood, e.g., at sites with shorter\nhistories of disease (such as Iquitos), or where measurements and forecasts of\nancillary covariates like precipitation are unavailable and/or where the\nstrength of association with cases are as yet unknown. In particular, we show\nthat the GP approach generally outperforms a more classical generalized linear\n(autoregressive) model (GLM) that we developed to utilize abundant covariate\ninformation. We illustrate variations of our method(s) on the two benchmark\nlocales alongside a full summary of results submitted by other contest\ncompetitors.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 14:01:57 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 14:16:18 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 14:06:06 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Johnson", "Leah R.", ""], ["Gramacy", "Robert B.", ""], ["Cohen", "Jeremy", ""], ["Mordecai", "Erin", ""], ["Murdock", "Courtney", ""], ["Rohr", "Jason", ""], ["Ryan", "Sadie J.", ""], ["Stewart-Ibarra", "Anna M.", ""], ["Weikel", "Daniel", ""]]}, {"id": "1702.00298", "submitter": "Richard La", "authors": "Richard J. La", "title": "Cascading Failures in Interdependent Systems: Impact of Degree\n  Variability and Dependence", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study cascading failures in a system comprising interdependent\nnetworks/systems, in which nodes rely on other nodes both in the same system\nand in other systems to perform their function. The (inter-)dependence among\nnodes is modeled using a dependence graph, where the degree vector of a node\ndetermines the number of other nodes it can potentially cause to fail in each\nsystem through aforementioned dependency. In particular, we examine the impact\nof the variability and dependence properties of node degrees on the probability\nof cascading failures. We show that larger variability in node degrees hampers\nwidespread failures in the system, starting with random failures. Similarly,\npositive correlations in node degrees make it harder to set off an epidemic of\nfailures, thereby rendering the system more robust against random failures.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:01:06 GMT"}, {"version": "v2", "created": "Sat, 10 Feb 2018 18:35:07 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["La", "Richard J.", ""]]}, {"id": "1702.00434", "submitter": "Andrew Finley Dr.", "authors": "Andrew O. Finley, Abhirup Datta, Bruce C. Cook, Douglas C. Morton,\n  Hans E. Andersen, Sudipto Banerjee", "title": "Efficient algorithms for Bayesian Nearest Neighbor Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider alternate formulations of recently proposed hierarchical Nearest\nNeighbor Gaussian Process (NNGP) models (Datta et al., 2016a) for improved\nconvergence, faster computing time, and more robust and reproducible Bayesian\ninference. Algorithms are defined that improve CPU memory management and\nexploit existing high-performance numerical linear algebra libraries.\nComputational and inferential benefits are assessed for alternate NNGP\nspecifications using simulated datasets and remotely sensed light detection and\nranging (LiDAR) data collected over the US Forest Service Tanana Inventory Unit\n(TIU) in a remote portion of Interior Alaska. The resulting data product is the\nfirst statistically robust map of forest canopy for the TIU.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 19:44:14 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 19:25:08 GMT"}, {"version": "v3", "created": "Sat, 3 Mar 2018 02:24:33 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Finley", "Andrew O.", ""], ["Datta", "Abhirup", ""], ["Cook", "Bruce C.", ""], ["Morton", "Douglas C.", ""], ["Andersen", "Hans E.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1702.00501", "submitter": "Julia Fukuyama", "authors": "Julia Fukuyama", "title": "Adaptive gPCA: A method for structured dimensionality reduction", "comments": "26 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When working with large biological data sets, exploratory analysis is an\nimportant first step for understanding the latent structure and for generating\nhypotheses to be tested in subsequent analyses. However, when the number of\nvariables is large compared to the number of samples, standard methods such as\nprincipal components analysis give results which are unstable and difficult to\ninterpret.\n  To mitigate these problems, we have developed a method which allows the\nanalyst to incorporate side information about the relationships between the\nvariables in a way that encourages similar variables to have similar loadings\non the principal axes. This leads to a low-dimensional representation of the\nsamples which both describes the latent structure and which has axes which are\ninterpretable in terms of groups of closely related variables.\n  The method is derived by putting a prior encoding the relationships between\nthe variables on the data and following through the analysis on the posterior\ndistributions of the samples. We show that our method does well at\nreconstructing true latent structure in simulated data and we also demonstrate\nthe method on a dataset investigating the effects of antibiotics on the\ncomposition of bacteria in the human gut.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 23:38:57 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Fukuyama", "Julia", ""]]}, {"id": "1702.00556", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth and Andrew Gelman", "title": "The statistical significance filter leads to overconfident expectations\n  of replicability", "comments": "6 pages, 3 figures. Submitted to the conference Cognitive Science\n  2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that publishing results using the statistical significance\nfilter---publishing only when the p-value is less than 0.05---leads to a\nvicious cycle of overoptimistic expectation of the replicability of results.\nFirst, we show analytically that when true statistical power is relatively low,\ncomputing power based on statistically significant results will lead to\noverestimates of power. Then, we present a case study using 10 experimental\ncomparisons drawn from a recently published meta-analysis in psycholinguistics\n(J\\\"ager et al., 2017). We show that the statistically significant results\nyield an illusion of replicability. This illusion holds even if the researcher\ndoesn't conduct any formal power analysis but just uses statistical\nsignificance to informally assess robustness (i.e., replicability) of results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:14:21 GMT"}, {"version": "v2", "created": "Sun, 14 May 2017 06:24:02 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Vasishth", "Shravan", ""], ["Gelman", "Andrew", ""]]}, {"id": "1702.00564", "submitter": "Shravan Vasishth", "authors": "Shravan Vasishth, Nicolas Chopin, Robin Ryder, Bruno Nicenboim", "title": "Modelling dependency completion in sentence comprehension as a Bayesian\n  hierarchical mixture process: A case study involving Chinese relative clauses", "comments": "6 pages, 2 figures. To appear in the Proceedings of the Cognitive\n  Science Conference 2017, London, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a case-study demonstrating the usefulness of Bayesian hierarchical\nmixture modelling for investigating cognitive processes. In sentence\ncomprehension, it is widely assumed that the distance between linguistic\nco-dependents affects the latency of dependency resolution: the longer the\ndistance, the longer the retrieval time (the distance-based account). An\nalternative theory, direct-access, assumes that retrieval times are a mixture\nof two distributions: one distribution represents successful retrievals (these\nare independent of dependency distance) and the other represents an initial\nfailure to retrieve the correct dependent, followed by a reanalysis that leads\nto successful retrieval. We implement both models as Bayesian hierarchical\nmodels and show that the direct-access model explains Chinese relative clause\nreading time data better than the distance account.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 07:48:58 GMT"}, {"version": "v2", "created": "Fri, 5 May 2017 05:44:34 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Vasishth", "Shravan", ""], ["Chopin", "Nicolas", ""], ["Ryder", "Robin", ""], ["Nicenboim", "Bruno", ""]]}, {"id": "1702.00584", "submitter": "Hirley Alves", "authors": "Onel L. Alcaraz L\\'opez, Hirley Alves, Richard Demo Souza and Evelio\n  Mart\\'in Garc\\'ia Fern\\'andez", "title": "Ultra Reliable Short Message Relaying with Wireless Power Transfer", "comments": "Accepted in ICC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dual-hop wireless network where an energy constrained relay\nnode first harvests energy through the received radio-frequency signal from the\nsource, and then uses the harvested energy to forward the source's information\nto the destination node. The throughput and delay metrics are investigated for\na decode-and-forward relaying mechanism at finite blocklength regime and\ndelay-limited transmission mode. We consider ultra-reliable communication\nscenarios under discussion for the next fifth-generation of wireless systems,\nwith error and latency constraints. The impact on these metrics of the\nblocklength, information bits, and relay position is investigated.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 08:53:59 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["L\u00f3pez", "Onel L. Alcaraz", ""], ["Alves", "Hirley", ""], ["Souza", "Richard Demo", ""], ["Fern\u00e1ndez", "Evelio Mart\u00edn Garc\u00eda", ""]]}, {"id": "1702.00728", "submitter": "Tobias Michael Erhardt", "authors": "T. M. Erhardt, C. Czado and T. L. Thorarinsdottir", "title": "Evaluation of time series models under non-stationarity with application\n  to the comparison of regional climate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different disciplines pursue the aim to develop models which characterize\ncertain phenomena as accurately as possible. Climatology is a prime example,\nwhere the temporal evolution of the climate is modeled. In order to compare and\nimprove different models, methodology for a fair model evaluation is\nindispensable. As models and forecasts of a phenomenon are usually associated\nwith uncertainty, proper scoring rules, which are tools that account for this\nkind of uncertainty, are an adequate choice for model evaluation. However,\nunder the presence of non-stationarity, such a model evaluation becomes\nchallenging, as the characteristics of the phenomenon of interest change. We\nprovide methodology for model evaluation in the context of non-stationary time\nseries. Our methodology assumes stationarity of the time series in shorter\nmoving time windows. These moving windows, which are selected based on a\nchangepoint analysis, are used to characterize the uncertainty of the\nphenomenon/model for the corresponding time instances. This leads to the\nconcept of moving scores allowing for a temporal assessment of the model\nperformance. The merits of the proposed methodology are illustrated in a\nsimulation and a case study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 16:05:01 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Erhardt", "T. M.", ""], ["Czado", "C.", ""], ["Thorarinsdottir", "T. L.", ""]]}, {"id": "1702.00817", "submitter": "Renato J Cintra", "authors": "F. M. Bayer, R. J. Cintra", "title": "DCT-like Transform for Image Compression Requires 14 Additions Only", "comments": "6 pages, 3 figures, 1 table", "journal-ref": "Electronics Letters, Volume 48, Issue 15, pp. 919-921 (2012)", "doi": "10.1049/el.2012.1148", "report-no": null, "categories": "cs.MM cs.DS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low-complexity 8-point orthogonal approximate DCT is introduced. The\nproposed transform requires no multiplications or bit-shift operations. The\nderived fast algorithm requires only 14 additions, less than any existing DCT\napproximation. Moreover, in several image compression scenarios, the proposed\ntransform could outperform the well-known signed DCT, as well as\nstate-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 20:08:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Bayer", "F. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1702.01119", "submitter": "Justin Cheng", "authors": "Justin Cheng, Michael Bernstein, Cristian Danescu-Niculescu-Mizil,\n  Jure Leskovec", "title": "Anyone Can Become a Troll: Causes of Trolling Behavior in Online\n  Discussions", "comments": "Best Paper Award at CSCW 2017", "journal-ref": null, "doi": "10.1145/2998181.2998213", "report-no": null, "categories": "cs.SI cs.CY cs.HC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online communities, antisocial behavior such as trolling disrupts\nconstructive discussion. While prior work suggests that trolling behavior is\nconfined to a vocal and antisocial minority, we demonstrate that ordinary\npeople can engage in such behavior as well. We propose two primary trigger\nmechanisms: the individual's mood, and the surrounding context of a discussion\n(e.g., exposure to prior trolling behavior). Through an experiment simulating\nan online discussion, we find that both negative mood and seeing troll posts by\nothers significantly increases the probability of a user trolling, and together\ndouble this probability. To support and extend these results, we study how\nthese same mechanisms play out in the wild via a data-driven, longitudinal\nanalysis of a large online news discussion community. This analysis reveals\ntemporal mood effects, and explores long range patterns of repeated exposure to\ntrolling. A predictive model of trolling behavior shows that mood and\ndiscussion context together can explain trolling behavior better than an\nindividual's history of trolling. These results combine to suggest that\nordinary people can, under the right circumstances, behave like trolls.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 19:00:03 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Cheng", "Justin", ""], ["Bernstein", "Michael", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""], ["Leskovec", "Jure", ""]]}, {"id": "1702.01183", "submitter": "Sebastian Kurtek", "authors": "Weiyi Xie, Sebastian Kurtek, Karthik Bharath, Ying Sun", "title": "A Geometric Approach to Visualization of Variability in Functional Data", "comments": "Journal of the American Statistical Association, 2016", "journal-ref": null, "doi": "10.1080/01621459.2016.1256813", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for the construction and visualization of\nboxplot-type displays for functional data. We use a recent functional data\nanalysis framework, based on a representation of functions called square-root\nslope functions, to decompose observed variation in functional data into three\nmain components: amplitude, phase, and vertical translation. We then construct\nseparate displays for each component, using the geometry and metric of each\nrepresentation space, based on a novel definition of the median, the two\nquartiles, and extreme observations. The outlyingness of functional data is a\nvery complex concept. Thus, we propose to identify outliers based on any of the\nthree main components after decomposition. We provide a variety of\nvisualization tools for the proposed boxplot-type displays including surface\nplots. We evaluate the proposed method using extensive simulations and then\nfocus our attention on three real data applications including exploratory data\nanalysis of sea surface temperature functions, electrocardiogram functions and\ngrowth curves.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 22:00:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Xie", "Weiyi", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""], ["Sun", "Ying", ""]]}, {"id": "1702.01191", "submitter": "Sebastian Kurtek", "authors": "Karthik Bharath, Sebastian Kurtek, Arvind Rao, Veerabhadran\n  Baladandayuthapani", "title": "Radiologic Image-based Statistical Shape Analysis of Brain Tumors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a curve-based Riemannian-geometric approach for general\nshape-based statistical analyses of tumors obtained from radiologic images. A\nkey component of the framework is a suitable metric that (1) enables\ncomparisons of tumor shapes, (2) provides tools for computing descriptive\nstatistics and implementing principal component analysis on the space of tumor\nshapes, and (3) allows for a rich class of continuous deformations of a tumor\nshape. The utility of the framework is illustrated through specific statistical\ntasks on a dataset of radiologic images of patients diagnosed with glioblastoma\nmultiforme, a malignant brain tumor with poor prognosis. In particular, our\nanalysis discovers two patient clusters with very different survival, subtype\nand genomic characteristics. Furthermore, it is demonstrated that adding tumor\nshape information into survival models containing clinical and genomic\nvariables results in a significant increase in predictive power.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 22:18:08 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "1702.01201", "submitter": "Jacob Westfall", "authors": "Jacob Westfall", "title": "Statistical details of the default priors in the Bambi library", "comments": "Bambi on Github: https://github.com/bambinos/bambi", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a companion paper to Yarkoni and Westfall (2017), which describes the\nPython package Bambi for estimating Bayesian generalized linear mixed models\nusing a simple interface. Here I give the statistical details underlying the\ndefault, weakly informative priors used in all models when the user does not\nspecify the priors. Our approach is to first deduce what the variances of the\nslopes would be if we were instead to have defined the priors on the partial\ncorrelation scale, and then to set independent Normal priors on the slopes with\nvariances equal to these implied variances. Our approach is similar in spirit\nto that of Zellner's g-prior (Zellner 1986), in that it involves a multivariate\nnormal prior on the regression slopes, with a tuning parameter to control the\nwidth or informativeness of the priors irrespective of the scales of the data\nand predictors. The primary differences are that here the tuning parameter is\ndirectly interpretable as the standard deviation of the distribution of\nplausible partial correlations, and that this tuning parameter can have\ndifferent values for different coefficients. The default priors for the\nintercepts and random effects are ultimately based on the prior slope\nvariances.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 23:24:33 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 21:47:39 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Westfall", "Jacob", ""]]}, {"id": "1702.01206", "submitter": "Majnu John", "authors": "Rahul John and Majnu John", "title": "Adaptation of the visibility graph algorithm to find the time lag\n  between hydrogeological time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the time lag between two hydrogeologic time series (e.g.\nprecipitation and water levels in an aquifer) is of significance for a\nhydrogeologist-modeler. In this paper, we present a method to quantify such\nlags by adapting the visibility graph algorithm, which converts time series\ninto a mathematical graph. We present simulation results to assess the\nperformance of the method. We also illustrate the utility of our approach using\na real world hydrogeologic dataset.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 23:55:52 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["John", "Rahul", ""], ["John", "Majnu", ""]]}, {"id": "1702.01576", "submitter": "Ali Tajer", "authors": "Javad Heydari and Ali Tajer", "title": "Quickest Localization of Anomalies in Power Grids: A Stochastic\n  Graphical Framework", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agile localization of anomalous events plays a pivotal role in enhancing the\noverall reliability of the grid and avoiding cascading failures. This is\nespecially of paramount significance in the large-scale grids due to their\ngeographical expansions and the large volume of data generated. This paper\nproposes a stochastic graphical framework, by leveraging which it aims to\nlocalize the anomalies with the minimum amount of data. This framework\ncapitalizes on the strong correlation structures observed among the\nmeasurements collected from different buses. The proposed approach, at its\ncore, collects the measurements sequentially and progressively updates its\ndecision about the location of the anomaly. The process resumes until the\nlocation of the anomaly can be identified with desired reliability. We provide\na general theory for the quickest anomaly localization and also investigate its\napplication for quickest line outage localization. Simulations in the IEEE\n118-bus model are provided to establish the gains of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 11:45:39 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Heydari", "Javad", ""], ["Tajer", "Ali", ""]]}, {"id": "1702.01793", "submitter": "Renato J Cintra", "authors": "R. M. Campello de Souza, H. M. de Oliveira, R. J. Cintra", "title": "Multiuser Communication Based on the DFT Eigenstructure", "comments": "5 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The eigenstructure of the discrete Fourier transform (DFT) is examined and\nnew systematic procedures to generate eigenvectors of the unitary DFT are\nproposed. DFT eigenvectors are suggested as user signatures for data\ncommunication over the real adder channel (RAC). The proposed multiuser\ncommunication system over the 2-user RAC is detailed.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 21:10:43 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["de Souza", "R. M. Campello", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1702.01830", "submitter": "Hatef Monajemi Mr.", "authors": "Hatef Monajemi, David L. Donoho, Jeffrey C. Hoch and Adam D. Schuyler", "title": "Incoherence of Partial-Component Sampling in multidimensional NMR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In NMR spectroscopy, undersampling in the indirect dimensions causes\nreconstruction artifacts whose size can be bounded using the so-called {\\it\ncoherence}. In experiments with multiple indirect dimensions, new undersampling\napproaches were recently proposed: random phase detection (RPD)\n\\cite{Maciejewski11} and its generalization, partial component sampling (PCS)\n\\cite{Schuyler13}. The new approaches are fully aware of the fact that\nhigh-dimensional experiments generate hypercomplex-valued free induction\ndecays; they randomly acquire only certain low-dimensional components of each\nhigh-dimensional hypercomplex entry. We provide a classification of various\nhypercomplex-aware undersampling schemes, and define a hypercomplex-aware\ncoherence appropriate for such undersampling schemes; we then use it to\nquantify undersampling artifacts of RPD and various PCS schemes.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 00:32:25 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Monajemi", "Hatef", ""], ["Donoho", "David L.", ""], ["Hoch", "Jeffrey C.", ""], ["Schuyler", "Adam D.", ""]]}, {"id": "1702.01838", "submitter": "Madhuchhanda Bhattacharjee Dr.", "authors": "Madhuchhanda Bhattacharjee", "title": "Meta Analytic Data Integration for Phenotype Prediction: Application to\n  Chronic Fatigue Syndrome", "comments": "2 Figures, 6 Tables, 1 Supplementary info", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling plays key role in providing accurate prognosis and\nenables us to take a step closer to personalized treatment. We identified two\npotential sources of human induced biases that can lead to disparate\nconclusions. We illustrate through a complex phenotype that robust results can\nstill be drawn after accounting for such biases.\n  Often predictive models build based in high dimensional data suffers from the\ndrawback of lack of interpretability. To achieve interpretability in the form\nof description of the organism level phenomena in term of molecular or cellular\nlevel activities, functional and pathway information is often augmented.\nFunctional information can greatly facilitate the interpretation of the results\nof the predictive model.\n  However an important aspect of (vertical) data augmentation is routinely\nignored, that is there could be several stages of analysis where such\ninformation could be meaningfully integrated. There is no know criteria to\nenable us to assess the effect of such augmentation. A novel aspect of the\nproposed work is in exploring possibilities of stages of analysis where\nfunctional information may be incorporated and in assessing the extent to which\nthe ultimate conclusions would differ depending on level of amalgamation.\n  To boost our confidence on the key findings a first level of meta-analysis is\ndone by exploring different levels of data augmentation. This is followed by\ncomparison of predictive models across different definitions of the same\nphenotype developed by different groups, which is also an extended form of\nmeta-analysis.\n  We have used real life data on a complex phenotype to illustrate the above.\nThe data pertains to Chronic Fatigue Syndrome (CFS) and another novel aspect of\nthe current work is in modeling the underlying continuous symptom measurements\nfor CFS, which is the first for this disease to our knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 01:16:03 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bhattacharjee", "Madhuchhanda", ""]]}, {"id": "1702.01858", "submitter": "Ameer Pasha Hosseinbor", "authors": "A. Pasha Hosseinbor and Renat Zhdanov", "title": "2D Sinusoidal Parameter Estimation with Offset Term", "comments": "Sinusoid, Cramer-Rao Lower Bound, Maximum Likelihood Estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the parameter estimation of a 2D sinusoid. Although sinusoidal\nparameter estimation has been extensively studied, our model differs from those\nexamined in the available literature by the inclusion of an offset term. We\nderive both the maximum likelihood estimation (MLE) solution and the Cramer-Rao\nlower bound (CRLB) on the variance of the model's estimators.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 02:52:52 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Hosseinbor", "A. Pasha", ""], ["Zhdanov", "Renat", ""]]}, {"id": "1702.01890", "submitter": "Michael Chertkov", "authors": "Michael Chertkov, Sidhant Misra, Marc Vuffray, Dvijotham Krishnamurty,\n  and Pascal Van Hentenryck", "title": "Graphical Models and Belief Propagation-hierarchy for Optimal\n  Physics-Constrained Network Flows", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript we review new ideas and first results on application of\nthe Graphical Models approach, originated from Statistical Physics, Information\nTheory, Computer Science and Machine Learning, to optimization problems of\nnetwork flow type with additional constraints related to the physics of the\nflow. We illustrate the general concepts on a number of enabling examples from\npower system and natural gas transmission (continental scale) and distribution\n(district scale) systems.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 06:14:55 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Chertkov", "Michael", ""], ["Misra", "Sidhant", ""], ["Vuffray", "Marc", ""], ["Krishnamurty", "Dvijotham", ""], ["Van Hentenryck", "Pascal", ""]]}, {"id": "1702.01987", "submitter": "Manuel V\\'azquez", "authors": "Manuel A. V\\'azquez, Joaqu\\'in M\\'iguez", "title": "Importance sampling with transformed weights", "comments": "2 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance sampling (IS) method lies at the core of many Monte\nCarlo-based techniques. IS allows the approximation of a target probability\ndistribution by drawing samples from a proposal (or importance) distribution,\ndifferent from the target, and computing importance weights (IWs) that account\nfor the discrepancy between these two distributions. The main drawback of IS\nschemes is the degeneracy of the IWs, which significantly reduces the\nefficiency of the method. It has been recently proposed to use transformed IWs\n(TIWs) to alleviate the degeneracy problem in the context of Population Monte\nCarlo, which is an iterative version of IS. However, the effectiveness of this\ntechnique for standard IS is yet to be investigated. In this letter we\nnumerically assess the performance of IS when using TIWs, and show that the\nmethod can attain robustness to weight degeneracy thanks to a bias/variance\ntrade-off.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:45:25 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 10:38:25 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["V\u00e1zquez", "Manuel A.", ""], ["M\u00edguez", "Joaqu\u00edn", ""]]}, {"id": "1702.01995", "submitter": "Jaehong Jeong", "authors": "Jaehong Jeong, Stefano Castruccio, Paola Crippa, and Marc G. Genton", "title": "Reducing Storage of Global Wind Ensembles with Stochastic Generators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind has the potential to make a significant contribution to future energy\nresources. Locating the sources of this renewable energy on a global scale is\nhowever extremely challenging, given the difficulty to store very large data\nsets generated by modern computer models. We propose a statistical model that\naims at reproducing the data-generating mechanism of an ensemble of runs via a\nStochastic Generator (SG) of global annual wind data. We introduce an\nevolutionary spectrum approach with spatially varying parameters based on\nlarge-scale geographical descriptors such as altitude to better account for\ndifferent regimes across the Earth's orography. We consider a multi-step\nconditional likelihood approach to estimate the parameters that explicitly\naccounts for nonstationary features while also balancing memory storage and\ndistributed computation. We apply the proposed model to more than 18 million\npoints of yearly global wind speed. The proposed SG requires orders of\nmagnitude less storage for generating surrogate ensemble members from wind than\ndoes creating additional wind fields from the climate model, even if an\neffective lossy data compression algorithm is applied to the simulation output.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:12:51 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 10:55:44 GMT"}, {"version": "v3", "created": "Sun, 1 Oct 2017 10:28:18 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Jeong", "Jaehong", ""], ["Castruccio", "Stefano", ""], ["Crippa", "Paola", ""], ["Genton", "Marc G.", ""]]}, {"id": "1702.02025", "submitter": "Hau-tieng Wu", "authors": "Ruilin Li and Martin G. Frasch and Hau-tieng Wu", "title": "Efficient fetal-maternal ECG signal separation from two channel maternal\n  abdominal ECG via diffusion-based channel selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.med-ph physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a need for affordable, widely deployable maternal-fetal ECG monitors\nto improve maternal and fetal health during pregnancy and delivery. Based on\nthe diffusion-based channel selection, here we present the mathematical\nformalism and clinical validation of an algorithm capable of accurate\nseparation of maternal and fetal ECG from a two channel signal acquired over\nmaternal abdomen.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 14:06:58 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Li", "Ruilin", ""], ["Frasch", "Martin G.", ""], ["Wu", "Hau-tieng", ""]]}, {"id": "1702.02089", "submitter": "Saiful Islam Md", "authors": "Sadia Tasnim Swarna, Shamim Ehsan and Md. Saiful Islam", "title": "A Statistical Model for Ideal Team Selection for A National Cricket\n  Squad", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cricket is a game played between two teams which consists of eleven players\neach. Nowadays cricket game is becoming more and more popular in Bangladesh and\nother South Asian Countries. Before a match people are very enthusiastic about\nteam squads and \"Which players are playing today?\", \"How well will MR. X\nperform today?\" are the million dollar questions before a big match. This\narticle will propose a method using statistical data analysis for recommending\na national team squad. Recent match scorecards for domestic and international\nmatches played by a specific team in recent years are used to recommend the\nideal squad. Impact point or rating points of all players in different\nconditions are calculated and the best ones from different categories are\nchosen to form optimal line-ups. To evaluate the efficiency of impact point\nsystem, it will be tested with real time match data to see how much accuracy it\ngives.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 12:47:39 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Swarna", "Sadia Tasnim", ""], ["Ehsan", "Shamim", ""], ["Islam", "Md. Saiful", ""]]}, {"id": "1702.02149", "submitter": "Bulent Kiziltan", "authors": "B\\\"ulent K{\\i}z{\\i}ltan, Holger Baumgardt, and Abraham Loeb", "title": "An intermediate-mass black hole in the centre of the globular cluster 47\n  Tucanae", "comments": "Published in Nature, Corrigendum added", "journal-ref": null, "doi": "10.1038/nature21361", "report-no": null, "categories": "astro-ph.GA astro-ph.IM astro-ph.SR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intermediate mass black holes play a critical role in understanding the\nevolutionary connection between stellar mass and super-massive black holes.\nHowever, to date the existence of these species of black holes remains\nambiguous and their formation process is therefore unknown. It has been long\nsuspected that black holes with masses $10^{2}-10^{4}M_{\\odot}$ should form and\nreside in dense stellar systems. Therefore, dedicated observational campaigns\nhave targeted globular cluster for many decades searching for signatures of\nthese elusive objects. All candidates found in these targeted searches appear\nradio dim and do not have the X-ray to radio flux ratio predicted by the\nfundamental plane for accreting black holes. Based on the lack of an\nelectromagnetic counterpart upper limits of $2060 M_{\\odot}$ and $470\nM_{\\odot}$ have been placed on the mass of a putative black hole in 47 Tucanae\n(NGC 104) from radio and X-ray observations respectively. Here we show there is\nevidence for a central black hole in 47 Tuc with a mass of M$_{\\bullet}\\sim2300\nM_{\\odot}$$_{-850}^{+1500}$ when the dynamical state of the globular cluster is\nprobed with pulsars. The existence of an intermediate mass black hole in the\ncentre of one of the densest clusters with no detectable electromagnetic\ncounterpart suggests that the black hole is not accreting at a sufficient rate\nand therefore contrary to expectations is gas starved. This intermediate mass\nblack hole might be a member of electromagnetically invisible population of\nblack holes that are the elusive seeds leading to the formation of supermassive\nblack holes in galaxies.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 19:00:00 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 00:46:57 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["K\u0131z\u0131ltan", "B\u00fclent", ""], ["Baumgardt", "Holger", ""], ["Loeb", "Abraham", ""]]}, {"id": "1702.02264", "submitter": "Hongmei Liu", "authors": "Hongmei Liu and J. Sunil Rao", "title": "Precision Therapeutic Biomarker Identification with Application to the\n  Cancer Genome Project", "comments": "This manuscript contains 30 pages and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cancer cell lines have frequently been used to link drug sensitivity and\nresistance with genomic profiles. To capture genomic complexity in cancer, the\nCancer Genome Project (CGP) (Garnett et al., 2012) screened 639 human tumor\ncell lines with 130 drugs ranging from known chemotherapeutic agents to\nexperimental compounds. Questions of interest include: i) can cancer-specific\ntherapeutic biomarkers be detected, ii) can drug resistance patterns be\nidentified along with predictive strategies to circumvent resistance using\nalternate drugs, iii) can biomarkers of drug synergies be predicted ? To tackle\nthese questions, following statistical challenges still exist: i)biomarkers\ncluster among the cell lines; ii) clusters can overlap (e.g. a cell line may\nbelong to multiple clusters); iii) drugs should be modeled jointly. We\nintroduce a multivariate regression model with a latent overlapping cluster\nindicator variable to address above issues. A generalized finite mixture of\nmultivariate regression (FMMR) model in connection with the new model and a new\nEM algorithm for fitting are proposed. Re-analysis of the dataset sheds new\nlight on the therapeutic inter-relationships between cancers as well existing\nand novel drug behaviors for the treatment and management of cancer.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:32:19 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Liu", "Hongmei", ""], ["Rao", "J. Sunil", ""]]}, {"id": "1702.02268", "submitter": "Kok Haur Ng", "authors": "Kok-Haur Ng, Shelton Peiris, Jennifer So-kuen-Chan, David Allen,\n  Kooi-Huat Ng", "title": "Efficient Modelling & Forecasting with range based volatility models and\n  application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an alternative method for fitting CARR models using\ncombined estimating functions (CEF) by showing its usefulness in applications\nin economics and quantitative finance. The associated information matrix for\ncorresponding new estimates is derived to calculate the standard errors. A\nsimulation study is carried out to demonstrate its superiority relative to\nother two competitors: linear estimating functions (LEF) and the maximum\nlikelihood (ML). Results show that CEF estimates are more efficient than LEF\nand ML estimates when the error distribution is mis-specified. Taking a real\ndata set from financial economics, we illustrate the usefulness and\napplicability of the CEF method in practice and report reliable forecast values\nto minimize the risk in the decision making process.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 03:56:00 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Ng", "Kok-Haur", ""], ["Peiris", "Shelton", ""], ["So-kuen-Chan", "Jennifer", ""], ["Allen", "David", ""], ["Ng", "Kooi-Huat", ""]]}, {"id": "1702.02966", "submitter": "Anh Bui", "authors": "Anh Tuan Bui and Daniel W. Apley", "title": "A monitoring and diagnostic approach for stochastic textured surfaces", "comments": null, "journal-ref": null, "doi": "10.1080/00401706.2017.1302362", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a supervised-learning-based approach for monitoring and diagnosing\ntexture-related defects in manufactured products characterized by stochastic\ntextured surfaces that satisfy the locality and stationarity properties of\nMarkov random fields. Examples of stochastic textured surface data include\nimages of woven textiles; image or surface metrology data for machined, cast,\nor formed metal parts; microscopy images of material microstructure samples;\netc. To characterize the complex spatial statistical dependencies of in-control\nsamples of the stochastic textured surface, we use rather generic supervised\nlearning methods, which provide an implicit characterization of the joint\ndistribution of the surface texture. We propose two spatial moving statistics,\nwhich are computed from residual errors of the fitted supervised learning\nmodel, for monitoring and diagnosing local aberrations in the general spatial\nstatistical behavior of newly manufactured stochastic textured surface samples\nin a statistical process control context. We illustrate the approach using\nimages of textile fabric samples and simulated 2-D stochastic processes, for\nwhich the algorithm successfully detects local defects of various natures.\nSupplemental discussions, results, data and computer codes are available\nonline.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 20:10:32 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 17:31:36 GMT"}, {"version": "v3", "created": "Fri, 21 Jul 2017 16:12:43 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Bui", "Anh Tuan", ""], ["Apley", "Daniel W.", ""]]}, {"id": "1702.03252", "submitter": "Antoine Filipovic-Pierucci", "authors": "Antoine Filipovi\\'c-Pierucci, Kevin Zarca, Isabelle Durand-Zaleski", "title": "Markov Models for Health Economic Evaluations: The R Package heemod", "comments": "30 pages, 9 figures. To be published in: Journal of Statistical\n  Software, website at https://www.jstatsoft.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Health economic evaluation studies are widely used in public health to assess\nhealth strategies in terms of their cost-effectiveness and inform public\npolicies. We developed an R package for Markov models implementing most of the\nmodelling and reporting features described in reference textbooks and\nguidelines: deterministic and probabilistic sensitivity analysis, heterogeneity\nanalysis, time dependency on state-time and model-time (semi-Markov and\nnon-homogeneous Markov models), etc. In this paper we illustrate the features\nof heemod by building and analysing an example Markov model. We then explain\nthe design and the underlying implementation of the package.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 17:08:43 GMT"}, {"version": "v2", "created": "Tue, 25 Apr 2017 11:01:50 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Filipovi\u0107-Pierucci", "Antoine", ""], ["Zarca", "Kevin", ""], ["Durand-Zaleski", "Isabelle", ""]]}, {"id": "1702.03409", "submitter": "Kate Inasaridze", "authors": "Vera Bzhalava, Ketevan Inasaridze", "title": "Disruptive Behavior Disorder (DBD) Rating Scale for Georgian Population", "comments": "9 pages, 2 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presented study Parent/Teacher Disruptive Behavior Disorder (DBD)\nrating scale based on the Diagnostic and Statistical Manual of Mental Disorders\n(DSM-IV-TR [APA, 2000]) which was developed by Pelham and his colleagues\n(Pelham et al., 1992) was translated and adopted for assessment of childhood\nbehavioral abnormalities, especially ADHD, ODD and CD in Georgian children and\nadolescents. The DBD rating scale was translated into Georgian language using\nback translation technique by English language philologists and checked and\ncorrected by qualified psychologists and psychiatrist of Georgia. Children and\nadolescents in the age range of 6 to 16 years (N 290; Mean Age 10.50, SD=2.88)\nincluding 153 males (Mean Age 10.42, SD= 2.62) and 141 females (Mean Age 10.60,\nSD=3.14) were recruited from different public schools of Tbilisi and the\nNeurology Department of the Pediatric Clinic of the Tbilisi State Medical\nUniversity. Participants objectively were assessed via interviewing\nparents/teachers and qualified psychologists in three different settings\nincluding school, home and clinic. In terms of DBD total scores revealed\nstatistically significant differences between healthy controls (M=27.71,\nSD=17.26) and children and adolescents with ADHD (M=61.51, SD= 22.79).\nStatistically significant differences were found for inattentive subtype\nbetween control (M=8.68, SD=5.68) and ADHD (M=18.15, SD=6.57) groups. In\ngeneral it was shown that children and adolescents with ADHD had high score on\nDBD in comparison to typically developed persons. In the study also was\ndetermined gender wise prevalence in children and adolescents with ADHD, ODD\nand CD. The research revealed prevalence of males in comparison with females in\nall investigated categories.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 10:52:36 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Bzhalava", "Vera", ""], ["Inasaridze", "Ketevan", ""]]}, {"id": "1702.03613", "submitter": "Ming Yang", "authors": "You Lin, Ming Yang, Can Wan, Jianhui Wang, Yonghua Song", "title": "A Multi-model Combination Approach for Probabilistic Wind Power\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term probabilistic wind power forecasting can provide critical\nquantified uncertainty information of wind generation for power system\noperation and control. As the complicated characteristics of wind power\nprediction error, it would be difficult to develop a universal forecasting\nmodel dominating over other alternative models. Therefore, a novel multi-model\ncombination (MMC) approach for short-term probabilistic wind generation\nforecasting is proposed in this paper to exploit the advantages of different\nforecasting models. The proposed approach can combine different forecasting\nmodels those provide different kinds of probability density functions to\nimprove the probabilistic forecast accuracy. Three probabilistic forecasting\nmodels based on the sparse Bayesian learning, kernel density estimation and\nbeta distribution fitting are used to form the combined model. The parameters\nof the MMC model are solved based on Bayesian framework. Numerical tests\nillustrate the effectiveness of the proposed MMC approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 02:48:16 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Lin", "You", ""], ["Yang", "Ming", ""], ["Wan", "Can", ""], ["Wang", "Jianhui", ""], ["Song", "Yonghua", ""]]}, {"id": "1702.03762", "submitter": "Etienne Tanr\\'e", "authors": "Alexandre Richard, Patricio Orio and Etienne Tanr\\'e", "title": "An integrate-and-fire model to generate spike trains with long-range\n  dependence", "comments": null, "journal-ref": null, "doi": "10.1007/s10827-018-0680-1", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-range dependence (LRD) has been observed in a variety of phenomena in\nnature, and for several years also in the spiking activity of neurons. Often,\nthis is interpreted as originating from a non-Markovian system. Here we show\nthat a purely Markovian integrate-and-fire (IF) model, with a noisy slow\nadaptation term, can generate interspike intervals (ISIs) that appear as having\nLRD. However a proper analysis shows that this is not the case asymptotically.\nFor comparison, we also consider a new model of individual IF neuron with\nfractional (non-Markovian) noise. The correlations of its spike trains are\nstudied and proven to have LRD, unlike classical IF models. On the other hand,\nto correctly measure long-range dependence, it is usually necessary to know if\nthe data are stationary. Thus, a methodology to evaluate stationarity of the\nISIs is presented and applied to the various IF models. We explain that\nMarkovian IF models may seem to have LRD because of non-stationarities.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 13:19:07 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 16:30:12 GMT"}, {"version": "v3", "created": "Wed, 28 Mar 2018 04:50:58 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Richard", "Alexandre", ""], ["Orio", "Patricio", ""], ["Tanr\u00e9", "Etienne", ""]]}, {"id": "1702.03862", "submitter": "Marco Scutari", "authors": "Marco Scutari, Pietro Auconi, Guido Caldarelli, Lorenzo Franchi", "title": "Bayesian Networks Analysis of Malocclusion Data", "comments": "15 pages, 10 figures", "journal-ref": "Scientific Reports 2017, 7(15326)", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use Bayesian networks to determine and visualise the\ninteractions among various Class III malocclusion maxillofacial features during\ngrowth and treatment. We start from a sample of 143 patients characterised\nthrough a series of a maximum of 21 different craniofacial features. We\nestimate a network model from these data and we test its consistency by\nverifying some commonly accepted hypotheses on the evolution of these\ndisharmonies by means of Bayesian statistics. We show that untreated subjects\ndevelop different Class III craniofacial growth patterns as compared to\npatients submitted to orthodontic treatment with rapid maxillary expantion and\nfacemask therapy. Among treated patients the CoA segment (the maxillary length)\nand the ANB angle (the antero-posterior relation of the maxilla to the\nmandible) seem to be the skeletal subspaces that receive the main effect of the\ntreatment.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 17:57:14 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 18:47:21 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 09:11:02 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Scutari", "Marco", ""], ["Auconi", "Pietro", ""], ["Caldarelli", "Guido", ""], ["Franchi", "Lorenzo", ""]]}, {"id": "1702.04044", "submitter": "Stephen Lane", "authors": "Stephen E Lane, Richard Gao, Matthew Chisholm, Andrew P Robinson", "title": "Statistical profiling to predict the biosecurity risk presented by\n  non-compliant international passengers", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biosecurity risk material (BRM) presents a clear and significant threat to\nnational and international environmental and economic assets. Intercepting BRM\ncarried by non-compliant international passengers is a key priority of border\nbiosecurity services. Global travel rates are constantly increasing, which\ncomplicates this important responsibility, and necessitates judicious\nintervention. Selection of passengers for intervention is generally performed\nmanually, and the quality of the selection depends on the experience and\njudgement of the officer making the selection. In this article we report on a\ncase study to assess the predictive ability of statistical profiling methods\nthat predict non-compliance with biosecurity regulations using data obtained\nfrom regulatory documents as inputs. We then evaluate the performance arising\nfrom using risk predictions to select higher risk passengers for screening. We\nfind that both prediction performance and screening higher risk passengers from\nregulatory documents are superior to manual and random screening, and recommend\nthat authorities further investigate statistical profiling for efficient\nintervention of biosecurity risk material on incoming passengers.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 01:41:21 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Lane", "Stephen E", ""], ["Gao", "Richard", ""], ["Chisholm", "Matthew", ""], ["Robinson", "Andrew P", ""]]}, {"id": "1702.04052", "submitter": "Stephen Lane", "authors": "Stephen E Lane, Tony Arthur, Christina Aston, Sam Zhao, Andrew P\n  Robinson", "title": "When does poor governance presage biosecurity risk?", "comments": "15 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Border inspection, and the challenge of deciding which of the tens of\nmillions of consignments that arrive should be inspected, is a perennial\nproblem for regulatory authorities. The objective of these inspections is to\nminimise the risk of contraband entering the country. As an example, for\nregulatory authorities in charge of biosecurity material, consignments of goods\nare classified before arrival according to their economic tariff number\n(Department of Immigration and Border Protection, 2016). This classification,\nperhaps along with other information, is used as a screening step to determine\nwhether further biosecurity intervention, such as inspection, is necessary.\nOther information associated with consignments includes details such as the\ncountry of origin, supplier, and importer, for example.\n  The choice of which consignments to inspect has typically been informed by\nhistorical records of intercepted material. Fortunately for regulators,\ninterception is a rare event, however this sparsity undermines the utility of\nhistorical records for deciding which containers to inspect.\n  In this paper we report on an analysis that uses more detailed information to\ninform inspection. Using quarantine biosecurity as a case study, we create\nstatistical profiles using generalised linear mixed models and compare\ndifferent model specifications with historical information alone, demonstrating\nthe utility of a statistical modelling approach. We also demonstrate some\ngraphical model summaries that provide managers with insight into pathway\ngovernance.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 02:56:58 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Lane", "Stephen E", ""], ["Arthur", "Tony", ""], ["Aston", "Christina", ""], ["Zhao", "Sam", ""], ["Robinson", "Andrew P", ""]]}, {"id": "1702.04108", "submitter": "Qadri Mayyala", "authors": "Qadri Mayyala, Karim Abed-Meraim and Azzedine Zerguine", "title": "Structure-Based Subspace Method for Multi-Channel Blind System\n  Identification", "comments": "5 pages, Submitted to IEEE Signal Processing Letters, January 2017", "journal-ref": null, "doi": "10.1109/LSP.2017.2715418", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a novel subspace-based method for blind identification of\nmultichannel finite impulse response (FIR) systems is presented. Here, we\nexploit directly the impeded Toeplitz channel structure in the signal linear\nmodel to build a quadratic form whose minimization leads to the desired channel\nestimation up to a scalar factor. This method can be extended to estimate any\npredefined linear structure, e.g. Hankel, that is usually encountered in linear\nsystems. Simulation findings are provided to highlight the appealing advantages\nof the new structure-based subspace (SSS) method over the standard subspace\n(SS) method in certain adverse identification scenarii.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 08:26:33 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Mayyala", "Qadri", ""], ["Abed-Meraim", "Karim", ""], ["Zerguine", "Azzedine", ""]]}, {"id": "1702.04197", "submitter": "Ana Helena Tavares", "authors": "Ana Helena Tavares, Jakob Raymaekers, Peter J. Rousseeuw, Raquel M.\n  Silva, Carlos A. C. Bastos, Armando Pinho, Paula Brito, Vera Afreixo", "title": "Dissimilar Symmetric Word Pairs in the Human Genome", "comments": "Submitted 13-Feb-2017; accepted, after a minor revision, 17-Mar-2017;\n  11th International Conference on Practical Applications of Computational\n  Biology & Bioinformatics, PACBB 2017, Porto, Portugal, 21-23 June, 2017", "journal-ref": "Advances in Intelligent Systems and Computing, Vol 616, 248-256.\n  Springer, 2017", "doi": "10.1007/978-3-319-60816-7_30", "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we explore the dissimilarity between symmetric word pairs, by\ncomparing the inter-word distance distribution of a word to that of its\nreversed complement. We propose a new measure of dissimilarity between such\ndistributions. Since symmetric pairs with different patterns could point to\nevolutionary features, we search for the pairs with the most dissimilar\nbehaviour. We focus our study on the complete human genome and its\nrepeat-masked version.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 13:27:12 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 09:31:27 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Tavares", "Ana Helena", ""], ["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""], ["Silva", "Raquel M.", ""], ["Bastos", "Carlos A. C.", ""], ["Pinho", "Armando", ""], ["Brito", "Paula", ""], ["Afreixo", "Vera", ""]]}, {"id": "1702.04281", "submitter": "Sophie Hautphenne", "authors": "Sophie Hautphenne, Melanie Massaro, Katharine Turner", "title": "Fitting Markovian binary trees using global and individual demographic\n  data", "comments": null, "journal-ref": null, "doi": "10.1016/j.tpb.2019.04.007", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of branching processes called Markovian binary trees, in\nwhich the individuals lifetime and reproduction epochs are modeled using a\ntransient Markovian arrival process (TMAP). We estimate the parameters of the\nTMAP based on population data containing information on age-specific fertility\nand mortality rates. Depending on the degree of detail of the available data, a\nweighted non-linear regression method or a maximum likelihood method is\napplied. We discuss the optimal choice of the number of phases in the TMAP, and\nwe provide confidence intervals for the model outputs. The results are\nillustrated using real data on human and bird populations.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:35:31 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Hautphenne", "Sophie", ""], ["Massaro", "Melanie", ""], ["Turner", "Katharine", ""]]}, {"id": "1702.04450", "submitter": "J\\'ulio Caineta", "authors": "J\\'ulio Caineta", "title": "Applying Spatial Bootstrap and Bayesian Update in uncertainty assessment\n  at oil reservoir appraisal stages", "comments": "10 pages, 2 figures, Extended abstract of MS thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostatistical modeling of the reservoir intrinsic properties starts only\nwith sparse data available. These estimates will depend largely on the number\nof wells and their location. The drilling costs are so high that they do not\nallow new wells to be placed for uncertainty assessment. Besides that\ndifficulty, usual geostatistical models do not account for the uncertainty of\nconceptual models, which should be considered.\n  Spatial bootstrap is applied to assess the estimate reliability when\nresampling from original field is not an option. Considering different\nrealities (conceptual models) and different scenarios (estimates), spatial\nbootstrapping applied with Bayesian update allows uncertainty assessment of the\ninitial estimate and of the conceptual model.\n  In this work an approach is suggested to integrate both these techniques,\nresulting in a method to assess which models are more appropriate for a given\nscenario.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 03:02:20 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Caineta", "J\u00falio", ""]]}, {"id": "1702.04552", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Nirian Martin, Ayanendranath Basu and Leandro Pardo", "title": "A new class of robust two-sample Wald-type tests", "comments": "32 pages, Submitted to journal", "journal-ref": "The International Journal of Biostatistics (2018)", "doi": "10.1515/ijb-2017-0023", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric hypothesis testing associated with two independent samples arises\nfrequently in several applications in biology, medical sciences, epidemiology,\nreliability and many more. In this paper, we propose robust Wald-type tests for\ntesting such two sample problems using the minimum density power divergence\nestimators of the underlying parameters. In particular, we consider the simple\ntwo-sample hypothesis concerning the full parametric homogeneity of the samples\nas well as the general two-sample (composite) hypotheses involving nuisance\nparameters also. The asymptotic and theoretical robustness properties of the\nproposed Wald-type tests have been developed for both the simple and general\ncomposite hypotheses. Some particular cases of testing against one-sided\nalternatives are discussed with specific attention to testing the effectiveness\nof a treatment in clinical trials. Performances of the proposed tests have also\nbeen illustrated numerically through appropriate real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:26:02 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Basu", "Ayanendranath", ""], ["Pardo", "Leandro", ""]]}, {"id": "1702.04690", "submitter": "Jongbin Jung", "authors": "Jongbin Jung, Connor Concannon, Ravi Shroff, Sharad Goel, Daniel G.\n  Goldstein", "title": "Simple rules for complex decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From doctors diagnosing patients to judges setting bail, experts often base\ntheir decisions on experience and intuition rather than on statistical models.\nWhile understandable, relying on intuition over models has often been found to\nresult in inferior outcomes. Here we present a new method,\nselect-regress-and-round, for constructing simple rules that perform well for\ncomplex decisions. These rules take the form of a weighted checklist, can be\napplied mentally, and nonetheless rival the performance of modern machine\nlearning algorithms. Our method for creating these rules is itself simple, and\ncan be carried out by practitioners with basic statistics knowledge. We\ndemonstrate this technique with a detailed case study of judicial decisions to\nrelease or detain defendants while they await trial. In this application, as in\nmany policy settings, the effects of proposed decision rules cannot be directly\nobserved from historical data: if a rule recommends releasing a defendant that\nthe judge in reality detained, we do not observe what would have happened under\nthe proposed action. We address this key counterfactual estimation problem by\ndrawing on tools from causal inference. We find that simple rules significantly\noutperform judges and are on par with decisions derived from random forests\ntrained on all available features. Generalizing to 22 varied decision-making\ndomains, we find this basic result replicates. We conclude with an analytical\nframework that helps explain why these simple decision rules perform as well as\nthey do.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 17:33:37 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 21:06:31 GMT"}, {"version": "v3", "created": "Sun, 2 Apr 2017 22:37:29 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Jung", "Jongbin", ""], ["Concannon", "Connor", ""], ["Shroff", "Ravi", ""], ["Goel", "Sharad", ""], ["Goldstein", "Daniel G.", ""]]}, {"id": "1702.04808", "submitter": "Pixu Shi", "authors": "Pixu Shi and Hongzhe Li", "title": "A Model for Paired-Multinomial Data and Its Application to Analysis of\n  Data on a Taxonomic Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human microbiome studies, sequencing reads data are often summarized as\ncounts of bacterial taxa at various taxonomic levels specified by a taxonomic\ntree. This paper considers the problem of analyzing two repeated measurements\nof microbiome data from the same subjects. Such data are often collected to\nassess the change of microbial composition after certain treatment, or the\ndifference in microbial compositions across body sites. Existing models for\nsuch count data are limited in modeling the covariance structure of the counts\nand in handling paired multinomial count data. A new probability distribution\nis proposed for paired-multinomial count data, which allows flexible covariance\nstructure and can be used to model repeatedly measured multivariate count data.\nBased on this distribution, a test statistic is developed for testing the\ndifference in compositions based on paired multinomial count data. The proposed\ntest can be applied to the count data observed on a taxonomic tree in order to\ntest difference in microbiome compositions and to identify the subtrees with\ndifferent subcompositions. Simulation results indicate that proposed test has\ncorrect type 1 errors and increased power compared to some commonly used\nmethods. An analysis of an upper respiratory tract microbiome data set is used\nto illustrate the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 22:50:27 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Shi", "Pixu", ""], ["Li", "Hongzhe", ""]]}, {"id": "1702.04845", "submitter": "Paul Taylor", "authors": "Robert W. Cox, Gang Chen, Daniel R. Glen, Richard C. Reynolds, Paul A.\n  Taylor", "title": "FMRI Clustering in AFNI: False Positive Rates Redux", "comments": "7 figures in main text and 17 figures in Appendices; 50 pages.\n  Accepted in Brain Connectivity", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recent reports of inflated false positive rates (FPRs) in FMRI group analysis\ntools by Eklund et al. (2016) have become a large topic within (and outside)\nneuroimaging. They concluded that: existing parametric methods for determining\nstatistically significant clusters had greatly inflated FPRs (\"up to 70%,\"\nmainly due to the faulty assumption that the noise spatial autocorrelation\nfunction is Gaussian- shaped and stationary), calling into question potentially\n\"countless\" previous results; in contrast, nonparametric methods, such as their\napproach, accurately reflected nominal 5% FPRs. They also stated that AFNI\nshowed \"particularly high\" FPRs compared to other software, largely due to a\nbug in 3dClustSim. We comment on these points using their own results and\nfigures and by repeating some of their simulations. Briefly, while parametric\nmethods show some FPR inflation in those tests (and assumptions of\nGaussian-shaped spatial smoothness also appear to be generally incorrect),\ntheir emphasis on reporting the single worst result from thousands of\nsimulation cases greatly exaggerated the scale of the problem. Importantly, FPR\nstatistics depend on \"task\" paradigm and voxelwise p-value threshold; as such,\nwe show how results of their study provide useful suggestions for FMRI study\ndesign and analysis, rather than simply a catastrophic downgrading of the\nfield's earlier results. Regarding AFNI (which we maintain), 3dClustSim's\nbug-effect was greatly overstated - their own results show that AFNI results\nwere not \"particularly\" worse than others. We describe further updates in AFNI\nfor characterizing spatial smoothness more appropriately (greatly reducing\nFPRs, though some remain >5%); additionally, we outline two newly implemented\npermutation/randomization-based approaches producing FPRs clustered much more\ntightly about 5% for voxelwise p<=0.01.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 03:12:50 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Cox", "Robert W.", ""], ["Chen", "Gang", ""], ["Glen", "Daniel R.", ""], ["Reynolds", "Richard C.", ""], ["Taylor", "Paul A.", ""]]}, {"id": "1702.04846", "submitter": "Paul Taylor", "authors": "Robert W. Cox, Gang Chen, Daniel R. Glen, Richard C. Reynolds, Paul A.\n  Taylor", "title": "FMRI Clustering and False Positive Rates", "comments": "3 pages, 1 figure. A Letter accepted in PNAS", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently, Eklund et al. (2016) analyzed clustering methods in standard FMRI\npackages: AFNI (which we maintain), FSL, and SPM [1]. They claimed: 1) false\npositive rates (FPRs) in traditional approaches are greatly inflated,\nquestioning the validity of \"countless published fMRI studies\"; 2)\nnonparametric methods produce valid, but slightly conservative, FPRs; 3) a\ncommon flawed assumption is that the spatial autocorrelation function (ACF) of\nFMRI noise is Gaussian-shaped; and 4) a 15-year-old bug in AFNI's 3dClustSim\nsignificantly contributed to producing \"particularly high\" FPRs compared to\nother software. We repeated simulations from [1] (Beijing-Zang data [2], see\n[3]), and comment on each point briefly.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 03:16:14 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Cox", "Robert W.", ""], ["Chen", "Gang", ""], ["Glen", "Daniel R.", ""], ["Reynolds", "Richard C.", ""], ["Taylor", "Paul A.", ""]]}, {"id": "1702.04851", "submitter": "Kellie Ottoboni", "authors": "Kellie Ottoboni, Fraser Lewis, and Luigi Salmaso", "title": "An Empirical Comparison of Parametric and Permutation Tests for\n  Regression Analysis of Randomized Experiments", "comments": "28 pages, 2 figures, 3 tables", "journal-ref": null, "doi": "10.1080/19466315.2018.1458648", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hypothesis tests based on linear models are widely accepted by organizations\nthat regulate clinical trials. These tests are derived using strong assumptions\nabout the data-generating process so that the resulting inference can be based\non parametric distributions. Because these methods are well understood and\nrobust, they are sometimes applied to data that depart from assumptions, such\nas ordinal integer scores. Permutation tests are a nonparametric alternative\nthat require minimal assumptions which are often guaranteed by the\nrandomization that was conducted. We compare analysis of covariance (ANCOVA), a\nspecial case of linear regression that incorporates stratification, to several\npermutation tests based on linear models that control for pretreatment\ncovariates. In simulations of randomized experiments using models which violate\nsome of the parametric regression assumptions, the permutation tests maintain\npower comparable to ANCOVA. We illustrate the use of these permutation tests\nalongside ANCOVA using data from a clinical trial comparing the effectiveness\nof two treatments for gastroesophageal reflux disease. Given the considerable\ncosts and scientific importance of clinical trials, an additional nonparametric\nmethod, such as a linear model permutation test, may serve as a robustness\ncheck on the statistical inference for the main study endpoints.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 03:46:19 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 00:19:47 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Lewis", "Fraser", ""], ["Salmaso", "Luigi", ""]]}, {"id": "1702.04854", "submitter": "Wenzhun Huang", "authors": "Shanwen Zhang, Harry Wang, Wenzhun Huang", "title": "Two-stage Plant Species Recognition by Combining Local K-NN and Weighted\n  Sparse Representation", "comments": "16 pages, 3 figures, 2 tables, 26 references", "journal-ref": "2016,employed", "doi": null, "report-no": "6089 words", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In classical sparse representation based classification and weighted SRC\nalgorithms, the test samples are sparely represented by all training samples.\nThey emphasize the sparsity of the coding coefficients but without considering\nthe local structure of the input data. To overcome the shortcoming, aiming at\nthe difficult problem of plant leaf recognition on the large-scale database, a\ntwo-stage local similarity based classification learning method is proposed by\ncombining local mean-based classification method and local WSRC. In the first\nstage, LMC is applied to coarsely classifying the test sample. nearest\nneighbors of the test sample, as a neighbor subset, is selected from each\ntraining class, then the local geometric center of each class is calculated. S\ncandidate neighbor subsets of the test sample are determined with the first\nsmallest distances between the test sample and each local geometric center. In\nthe second stage, LWSRC is proposed to approximately represent the test sample\nthrough a linear weighted sum of all samples of the candidate neighbor subsets.\nThe rationale of the proposed method is as follows: the first stage aims to\neliminate the training samples that are far from the test sample and assume\nthat these samples have no effects on the ultimate classification decision,\nthen select the candidate neighbor subsets of the test sample. Thus the\nclassification problem becomes simple with fewer subsets; the second stage pays\nmore attention to those training samples of the candidate neighbor subsets in\nweighted representing the test sample. This is helpful to accurately represent\nthe test sample. Experimental results on the leaf image database demonstrate\nthat the proposed method not only has a high accuracy and low time cost, but\nalso can be clearly interpreted.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 04:08:56 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Zhang", "Shanwen", ""], ["Wang", "Harry", ""], ["Huang", "Wenzhun", ""]]}, {"id": "1702.05047", "submitter": "Stella Kapodistria", "authors": "Thomas Kenbeek and Stella Kapodistria and Alessandro Di Bucchianico", "title": "Data-driven online monitoring of wind turbines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condition based maintenance is a modern approach to maintenance which has\nbeen successfully used in several industrial sectors. In this paper we present\na concrete statistical approach to condition based maintenance for wind turbine\nby applying ideas from statistical process control. A specific problem in wind\nturbine maintenance is that failures of a certain part may have causes that\noriginate in other parts a long time ago. This calls for methods that can\nproduce timely warnings by combining sensor data from different sources. Our\nmethod improves on existing methods used in wind turbine maintenance by using\nadaptive alarm thresholds for the monitored parameters that correct for values\nof other relevant parameters. We illustrate our method with a case study that\nshows that our method is able to predict upcoming failures much earlier than\ncurrently used methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:23:06 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Kenbeek", "Thomas", ""], ["Kapodistria", "Stella", ""], ["Di Bucchianico", "Alessandro", ""]]}, {"id": "1702.05254", "submitter": "Christopher Kovach", "authors": "Christopher K. Kovach", "title": "A Biased Look at Phase Locking: Brief Critical Review and Proposed\n  Remedy", "comments": null, "journal-ref": "IEEE Tran. Sig. Proc. 65(17), 2017, 4468-4480", "doi": "10.1109/TSP.2017.2711517", "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of popular measures of dependence between pairs of band-limited\nsignals rely on analytic phase. A common misconception is that the dependence\nrevealed by these measures must be specific to the spectral range of the\nfiltered input signals. Implicitly or explicitly, obtaining analytic phase\ninvolves normalizing the signal by its own envelope, which is a nonlinear\noperation that introduces broad spectral leakage. We review how this generates\nbias and complicates the interpretation of commonly used measures of phase\nlocking. A specific example of this effect may create spurious phase locking as\na consequence of nonzero circular mean in the phase of input signals, which can\nbe viewed as spectral leakage to 0 Hz. Corrections for this problem which\nrecenter or uniformize the distribution of phase may fail when the amplitudes\nof the compared signals are correlated. To address the more general problem of\nspectral bias, a novel measure of phase locking is proposed, the\namplitude-weighted phase locking value (awPLV). This measure is closely related\nto coherence, but it removes ambiguities of interpretation that detract from\nthe latter.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 08:22:52 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Kovach", "Christopher K.", ""]]}, {"id": "1702.05462", "submitter": "Fabrizio Leisen", "authors": "Laurentiu Hinoveanu, Fabrizio Leisen and Cristiano Villa", "title": "Objective Bayesian Analysis for Change Point Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a loss-based approach to change point analysis. In\nparticular, we look at the problem from two perspectives. The first focuses on\nthe definition of a prior when the number of change points is known a priori.\nThe second contribution aims to estimate the number of change points by using a\nloss-based approach recently introduced in the literature. The latter considers\nchange point estimation as a model selection exercise. We show the performance\nof the proposed approach on simulated data and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:06:27 GMT"}, {"version": "v2", "created": "Sun, 7 Jan 2018 13:58:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hinoveanu", "Laurentiu", ""], ["Leisen", "Fabrizio", ""], ["Villa", "Cristiano", ""]]}, {"id": "1702.05662", "submitter": "Debangan Dey", "authors": "Soudeep Deb, Debangan Dey", "title": "Spatial modeling of shot conversion in soccer to single out goalscoring\n  ability", "comments": "Revised version: More detailed data analysis and comparison study\n  included", "journal-ref": null, "doi": "10.3233/JSA-190281", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goals are results of pin-point shots and it is a pivotal decision in soccer\nwhen, how and where to shoot. The main contribution of this study is two-fold.\nAt first, after showing that there exists high spatial correlation in the data\nof shots across games, we introduce a spatial process in the error structure to\nmodel the probability of conversion from a shot depending on positional and\nsituational covariates. The model is developed using a full Bayesian framework.\nSecondly, based on the proposed model, we define two new measures that can\nappropriately quantify the impact of an individual in soccer, by evaluating the\npositioning senses and shooting abilities of the players. As a practical\napplication, the method is implemented on Major League Soccer data from 2016/17\nseason.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 21:57:40 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 21:16:31 GMT"}, {"version": "v3", "created": "Mon, 6 Nov 2017 16:02:07 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 17:31:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Deb", "Soudeep", ""], ["Dey", "Debangan", ""]]}, {"id": "1702.05698", "submitter": "Wei Xiao", "authors": "Wei Xiao, Xiaolin Huang, Jorge Silva, Saba Emrani and Arin Chaudhuri", "title": "Online Robust Principal Component Analysis with Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust PCA methods are typically batch algorithms which requires loading all\nobservations into memory before processing. This makes them inefficient to\nprocess big data. In this paper, we develop an efficient online robust\nprincipal component methods, namely online moving window robust principal\ncomponent analysis (OMWRPCA). Unlike existing algorithms, OMWRPCA can\nsuccessfully track not only slowly changing subspace but also abruptly changed\nsubspace. By embedding hypothesis testing into the algorithm, OMWRPCA can\ndetect change points of the underlying subspaces. Extensive simulation studies\ndemonstrate the superior performance of OMWRPCA compared with other\nstate-of-art approaches. We also apply the algorithm for real-time background\nsubtraction of surveillance video.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 04:08:18 GMT"}, {"version": "v2", "created": "Mon, 20 Mar 2017 19:49:02 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["Xiao", "Wei", ""], ["Huang", "Xiaolin", ""], ["Silva", "Jorge", ""], ["Emrani", "Saba", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1702.05732", "submitter": "Philipp Pelz", "authors": "Philipp Michael Pelz, Wen Xuan Qiu, Robert B\\\"ucker, G\\\"unther\n  Kassier, R.J. Dwayne Miller", "title": "Low-dose cryo electron ptychography via non-convex Bayesian optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph math.OC physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electron ptychography has seen a recent surge of interest for phase sensitive\nimaging at atomic or near-atomic resolution. However, applications are so far\nmainly limited to radiation-hard samples because the required doses are too\nhigh for imaging biological samples at high resolution. We propose the use of\nnon-convex, Bayesian optimization to overcome this problem and reduce the dose\nrequired for successful reconstruction by two orders of magnitude compared to\nprevious experiments. We suggest to use this method for imaging single\nbiological macromolecules at cryogenic temperatures and demonstrate 2D\nsingle-particle reconstructions from simulated data with a resolution of 7.9\n\\AA$\\,$ at a dose of 20 $e^- / \\AA^2$. When averaging over only 15 low-dose\ndatasets, a resolution of 4 \\AA$\\,$ is possible for large macromolecular\ncomplexes. With its independence from microscope transfer function, direct\nrecovery of phase contrast and better scaling of signal-to-noise ratio,\ncryo-electron ptychography may become a promising alternative to Zernike\nphase-contrast microscopy.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 10:08:16 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Pelz", "Philipp Michael", ""], ["Qiu", "Wen Xuan", ""], ["B\u00fccker", "Robert", ""], ["Kassier", "G\u00fcnther", ""], ["Miller", "R. J. Dwayne", ""]]}, {"id": "1702.05832", "submitter": "Adrijo Chakraborty", "authors": "Adrijo Chakraborty, Gauri Sankar Datta, Abhyuday Mandal", "title": "Robust Hierarchical Bayes Small Area Estimation for Nested Error\n  Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  National statistical institutes in many countries are now mandated to produce\nreliable statistics for important variables such as population, income,\nunemployment, health outcomes, etc. for small areas, defined by geography\nand/or demography. Due to small samples from these areas, direct sample-based\nestimates are often unreliable. Model-based small area estimation is now\nextensively used to generate reliable statistics by \"borrowing strength\" from\nother areas and related variables through suitable models. Outliers adversely\ninfluence standard model-based small area estimates. To deal with outliers,\nSinha and Rao (2009) proposed a robust frequentist approach. In this article,\nwe present a robust Bayesian alternative to the nested error regression model\nfor unit-level data to mitigate outliers. We consider a two-component scale\nmixture of normal distributions for the unit-level error to model outliers and\npresent a computational approach to produce Bayesian predictors of small area\nmeans under a noninformative prior for model parameters. A real example and\nextensive simulations convincingly show robustness of our Bayesian predictors\nto outliers. Simulations comparison of these two procedures with Bayesian\npredictors by Datta and Ghosh (1991) and M-quantile estimators by Chambers et\nal. (2014) shows that our proposed procedure is better than the others in terms\nof bias, variability, and coverage probability of prediction intervals, when\nthere are outliers. The superior frequentist performance of our procedure shows\nits dual (Bayes and frequentist) dominance, and makes it attractive to all\npractitioners, both Bayesian and frequentist, of small area estimation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 01:50:34 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 02:19:52 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Chakraborty", "Adrijo", ""], ["Datta", "Gauri Sankar", ""], ["Mandal", "Abhyuday", ""]]}, {"id": "1702.05982", "submitter": "Albrecht Zimmermann", "authors": "Albrecht Zimmermann", "title": "Wages of wins: could an amateur make money from match outcome\n  predictions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the accuracies of models for match outcome predictions is nice and\nwell but in the end the real proof is in the money to be made by betting. To\nevaluate the question whether the models developed by us could be used easily\nto make money via sports betting, we evaluate three cases: NCAAB post-season,\nNBA season, and NFL season, and find that it is possible yet not without its\npitfalls. In particular, we illustrate that high accuracy does not\nautomatically equal high pay-out, by looking at the type of match-ups that are\npredicted correctly by different models.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 12:12:57 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Zimmermann", "Albrecht", ""]]}, {"id": "1702.06512", "submitter": "Andrew Crane-Droesch", "authors": "Andrew Crane-Droesch", "title": "Semiparametric panel data models using neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an estimator for semiparametric models that uses a\nfeed-forward neural network to fit the nonparametric component. Unlike many\nmethodologies from the machine learning literature, this approach is suitable\nfor longitudinal/panel data. It provides unbiased estimation of the parametric\ncomponent of the model, with associated confidence intervals that have\nnear-nominal coverage rates. Simulations demonstrate (1) efficiency, (2) that\nparametric estimates are unbiased, and (3) coverage properties of estimated\nintervals. An application section demonstrates the method by predicting\ncounty-level corn yield using daily weather data from the period 1981-2015,\nalong with parametric time trends representing technological change. The method\nis shown to out-perform linear methods such as OLS and ridge/lasso, as well as\nrandom forest. The procedures described in this paper are implemented in the R\npackage panelNNET.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 18:32:22 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 15:58:39 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Crane-Droesch", "Andrew", ""]]}, {"id": "1702.06650", "submitter": "Caixia Liu", "authors": "Caixia Liu, Xiaolu Zhou, Xiangdong Lei, Huabing Huang, Changhui Peng,\n  Xiaoyi Wang, Jianfeng Sun, Carl Zhou", "title": "Reducing the uncertainty in the forest volume-to-biomass relationship\n  built from limited field plots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of biomass estimation based on a volume-to-biomass relationship\nhas been applied in estimating forest biomass conventionally through the mean\nvolume (m3 ha-1). However, few studies have been reported concerning the\nverification of the volume-biomass equations regressed using field data. The\npossible bias may result from the volume measurements and extrapolations from\nsample plots to stands or a unit area. This paper addresses (i) how to verify\nthe volume-biomass equations, and (ii) how to reduce the bias while building\nthese equations. This paper presents an applicable method for verifying the\nfield data using reasonable wood densities, restricting the error in field data\nprocessing based on limited field plots, and achieving a better understanding\nof the uncertainty in building those equations. The verified and improved\nvolume-biomass equations are more reliable and will help to estimate forest\ncarbon sequestration and carbon balance at any large scale.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 02:21:02 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Liu", "Caixia", ""], ["Zhou", "Xiaolu", ""], ["Lei", "Xiangdong", ""], ["Huang", "Huabing", ""], ["Peng", "Changhui", ""], ["Wang", "Xiaoyi", ""], ["Sun", "Jianfeng", ""], ["Zhou", "Carl", ""]]}, {"id": "1702.06661", "submitter": "Meisam Hejazinia", "authors": "Meisam Hejazi Nia, Brian T. Ratchford, Norris Bruce", "title": "Social Learning and Diffusion of Pervasive Goods: An Empirical Study of\n  an African App Store", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, the authors develop a structural model that combines a macro\ndiffusion model with a micro choice model to control for the effect of social\ninfluence on the mobile app choices of customers over app stores. Social\ninfluence refers to the density of adopters within the proximity of other\ncustomers. Using a large data set from an African app store and Bayesian\nestimation methods, the authors quantify the effect of social influence and\ninvestigate the impact of ignoring this process in estimating customer choices.\nThe findings show that customer choices in the app store are explained better\nby offline than online density of adopters and that ignoring social influence\nin estimations results in biased estimates. Furthermore, the findings show that\nthe mobile app adoption process is similar to adoption of music CDs, among all\nother classic economy goods. A counterfactual analysis shows that the app store\ncan increase its revenue by 13.6% through a viral marketing policy (e.g., a\nsharing with friends and family button).\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 03:10:41 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Nia", "Meisam Hejazi", ""], ["Ratchford", "Brian T.", ""], ["Bruce", "Norris", ""]]}, {"id": "1702.06913", "submitter": "Christian Kleiber", "authors": "Christian Kleiber", "title": "Structural Change in (Economic) Time Series", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": "10.1007/978-3-319-64334-2_21", "report-no": null, "categories": "q-fin.ST physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for detecting structural changes, or change points, in time series\ndata are widely used in many fields of science and engineering. This chapter\nsketches some basic methods for the analysis of structural changes in time\nseries data. The exposition is confined to retrospective methods for univariate\ntime series. Several recent methods for dating structural changes are compared\nusing a time series of oil prices spanning more than 60 years. The methods\nbroadly agree for the first part of the series up to the mid-1980s, for which\nchanges are associated with major historical events, but provide somewhat\ndifferent solutions thereafter, reflecting a gradual increase in oil prices\nthat is not well described by a step function. As a further illustration, 1990s\ndata on the volatility of the Hang Seng stock market index are reanalyzed.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 12:13:24 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Kleiber", "Christian", ""]]}, {"id": "1702.06993", "submitter": "Sascha Desmettre", "authors": "Sascha Desmettre, Johan de Kock, Peter Ruckdeschel, Frank Thomas\n  Seifried", "title": "Generalized Pareto Processes and Liquidity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the modeling of liquidity risk in fund management in a dynamic\nsetting, we propose and investigate a class of time series models with\ngeneralized Pareto marginals: the autoregressive generalized Pareto process\n(ARGP), a modified ARGP (MARGP) and a thresholded ARGP (TARGP). These models\nare able to capture key data features apparent in fund liquidity data and\nreflect the underlying phenomena via easily interpreted, low-dimensional model\nparameters. We establish stationarity and ergodicity, provide a link to the\nclass of shot-noise processes, and determine the associated interarrival\ndistributions for exceedances. Moreover, we provide estimators for all relevant\nmodel parameters and establish consistency and asymptotic normality for all\nestimators (except the threshold parameter, which as usual must be dealt with\nseparately). Finally, we illustrate our approach using real-world fund\nredemption data, and we discuss the goodness-of-fit of the estimated models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 20:18:36 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Desmettre", "Sascha", ""], ["de Kock", "Johan", ""], ["Ruckdeschel", "Peter", ""], ["Seifried", "Frank Thomas", ""]]}, {"id": "1702.07007", "submitter": "Jakob Runge", "authors": "Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino\n  Sejdinovic", "title": "Detecting causal associations in large nonlinear time series datasets", "comments": "46 pages, 19 figures", "journal-ref": "Science Advances Vol. 5, no. 11, eaau4996 (2019)", "doi": "10.1126/sciadv.aau4996", "report-no": null, "categories": "stat.ME physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal relationships from observational time series data is a key\nproblem in disciplines such as climate science or neuroscience, where\nexperiments are often not possible. Data-driven causal inference is challenging\nsince datasets are often high-dimensional and nonlinear with limited sample\nsizes. Here we introduce a novel method that flexibly combines linear or\nnonlinear conditional independence tests with a causal discovery algorithm that\nallows to reconstruct causal networks from large-scale time series datasets. We\nvalidate the method on a well-established climatic teleconnection connecting\nthe tropical Pacific with extra-tropical temperatures and using large-scale\nsynthetic datasets mimicking the typical properties of real data. The\nexperiments demonstrate that our method outperforms alternative techniques in\ndetection power from small to large-scale datasets and opens up entirely new\npossibilities to discover causal networks from time series across a range of\nresearch fields.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:09:19 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 12:27:48 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Runge", "Jakob", ""], ["Nowack", "Peer", ""], ["Kretschmer", "Marlene", ""], ["Flaxman", "Seth", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "1702.07009", "submitter": "Jue Hou", "authors": "Ronghui Xu, Jue Hou and Christina D. Chambers", "title": "The Impact of Confounder Selection in Propensity Scores for Rare Events\n  Data - with Applications to Birth Defects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work was motivated by a recent study on birth defects of infants born to\npregnant women exposed to a certain medication for treating chronic diseases.\nOutcomes such as birth defects are rare events in the general population, which\noften translate to very small numbers of events in the unexposed group. As drug\nsafety studies in pregnancy are typically observational in nature, we control\nfor confounding in this rare events setting using propensity scores (PS). Using\nour empirical data, we noticed that the estimated odds ratio for birth defects\ndue to exposure varied drastically depending on the specific approach used. The\ncommonly used approaches with PS are matching, stratification, inverse\nprobability weighting (IPW) and regression adjustment. The extremely rare\nevents setting renders the matching or stratification infeasible. In addition,\nthe PS itself may be formed via different approaches to select confounders from\na relatively long list of potential confounders. We carried out simulation\nexperiments to compare different combinations of approaches: IPW or regression\nadjustment, with 1) including all potential confounders without selection, 2)\nselection based on univariate association between the candidate variable and\nthe outcome, 3) selection based on change in effects (CIE). The simulation\nshowed that IPW without selection leads to extremely large variances in the\nestimated odds ratio, which help to explain the empirical data analysis results\nthat we had observed. The simulation also showed that IPW with selection based\non univariate association with the outcome is preferred over IPW with CIE.\nRegression adjustment has small variances of the estimated odds ratio\nregardless of the selection methods used.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 21:21:54 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Xu", "Ronghui", ""], ["Hou", "Jue", ""], ["Chambers", "Christina D.", ""]]}, {"id": "1702.07326", "submitter": "Christina Lioma Assoc. Prof", "authors": "Niels Dalum Hansen and K{\\aa}re M{\\o}lbak and Ingemar J. Cox and\n  Christina Lioma", "title": "Time-Series Adaptive Estimation of Vaccination Uptake Using Web Search\n  Queries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating vaccination uptake is an integral part of ensuring public health.\nIt was recently shown that vaccination uptake can be estimated automatically\nfrom web data, instead of slowly collected clinical records or population\nsurveys. All prior work in this area assumes that features of vaccination\nuptake collected from the web are temporally regular. We present the first ever\nmethod to remove this assumption from vaccination uptake estimation: our method\ndynamically adapts to temporal fluctuations in time series web data used to\nestimate vaccination uptake. We show our method to outperform the state of the\nart compared to competitive baselines that use not only web data but also\ncurated clinical data. This performance improvement is more pronounced for\nvaccines whose uptake has been irregular due to negative media attention (HPV-1\nand HPV-2), problems in vaccine supply (DiTeKiPol), and targeted at children of\n12 years old (whose vaccination is more irregular compared to younger\nchildren).\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 18:24:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Hansen", "Niels Dalum", ""], ["M\u00f8lbak", "K\u00e5re", ""], ["Cox", "Ingemar J.", ""], ["Lioma", "Christina", ""]]}, {"id": "1702.07422", "submitter": "Chris Jewell PhD", "authors": "Poppy Miller, Jonathan Marshall, Nigel French, Chris Jewell", "title": "sourceR: Classification and Source Attribution of Infectious Agents\n  among Heterogeneous Populations", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": "10.1371/journal.pcbi.1005564", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zoonotic diseases are a major cause of morbidity, and productivity losses in\nboth humans and animal populations. Identifying the source of food-borne\nzoonoses (e.g. an animal reservoir or food product) is crucial for the\nidentification and prioritisation of food safety interventions. For many\nzoonotic diseases it is difficult to attribute human cases to sources of\ninfection because there is little epidemiological information on the cases.\nHowever, microbial strain typing allows zoonotic pathogens to be categorised,\nand the relative frequencies of the strain types among the sources and in human\ncases allows inference on the likely source of each infection. We introduce\nsourceR, an R package for quantitative source attribution, aimed at food-borne\ndiseases. It implements a fully joint Bayesian model using strain-typed\nsurveillance data from both human cases and source samples, capable of\nidentifying important sources of infection. The model measures the force of\ninfection from each source, allowing for varying survivability, pathogenicity\nand virulence of pathogen strains, and varying abilities of the sources to act\nas vehicles of infection. A Bayesian non-parametric (Dirichlet process)\napproach is used to cluster pathogen strain types by epidemiological behaviour,\navoiding model overfitting and allowing detection of strain types associated\nwith potentially high 'virulence'.\n  sourceR is demonstrated using Campylobacter jejuni isolate data collected in\nNew Zealand between 2005 and 2008. It enables straightforward attribution of\ncases of zoonotic infection to putative sources of infection by epidemiologists\nand public health decision makers. As sourceR develops, we intend it to become\nan important and flexible resource for food-borne disease attribution studies.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 23:25:32 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Miller", "Poppy", ""], ["Marshall", "Jonathan", ""], ["French", "Nigel", ""], ["Jewell", "Chris", ""]]}, {"id": "1702.07465", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Peter Mueller, Subhajit Sengupta and Yuan Ji", "title": "PairClone: A Bayesian Subclone Caller Based on Mutation Pairs", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2019", "doi": "10.1111/rssc.12328", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor cell populations can be thought of as being composed of homogeneous\ncell subpopulations, with each subpopulation being characterized by overlapping\nsets of single nucleotide variants (SNVs). Such subpopulations are known as\nsubclones and are an important target for precision medicine. Reconstructing\nsuch subclones from next-generation sequencing (NGS) data is one of the major\nchallenges in precision medicine. We present PairClone as a new tool to\nimplement this reconstruction. The main idea of PairClone is to model short\nreads mapped to pairs of proximal SNVs. In contrast, most existing methods use\nonly marginal reads for unpaired SNVs. Using Bayesian nonparametric models, we\nestimate posterior probabilities of the number, genotypes and population\nfrequencies of subclones in one or more tumor sample. We use the categorical\nIndian buffet process (cIBP) as a prior probability model for subclones that\nare represented as vectors of categorical matrices that record the\ncorresponding sets of mutation pairs. Performance of PairClone is assessed\nusing simulated and real datasets. An open source software package can be\nobtained at http://www.compgenome.org/pairclone.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 05:16:23 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Zhou", "Tianjian", ""], ["Mueller", "Peter", ""], ["Sengupta", "Subhajit", ""], ["Ji", "Yuan", ""]]}, {"id": "1702.07869", "submitter": "Jishnu Sadasivan", "authors": "Jishnu Sadasivan, Subhadip Mukherjee, and Chandra Sekhar Seelamantula", "title": "Signal Denoising Using the Minimum-Probability-of-Error Criterion", "comments": null, "journal-ref": "APSIPA Transactions on Signal and Information Processing 9 (2020)\n  e3", "doi": "10.1017/ATSIP.2019.27", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of signal denoising via transform-domain shrinkage\nbased on a novel $\\textit{risk}$ criterion called the minimum probability of\nerror (MPE), which measures the probability that the estimated parameter lies\noutside an $\\epsilon$-neighborhood of the actual value. However, the MPE,\nsimilar to the mean-squared error (MSE), depends on the ground-truth parameter,\nand has to be estimated from the noisy observations. We consider linear\nshrinkage-based denoising functions, wherein the optimum shrinkage parameter is\nobtained by minimizing an estimate of the MPE. When the probability of error is\nintegrated over $\\epsilon$, it leads to the expected $\\ell_1$ distortion. The\nproposed MPE and $\\ell_1$ distortion formulations are applicable to various\nnoise distributions by invoking a Gaussian mixture model approximation. Within\nthe realm of MPE, we also develop an extension of the transform-domain\nshrinkage by grouping transform coefficients, resulting in $\\textit{subband\nshrinkage}$. The denoising performance obtained within the proposed framework\nis shown to be better than that obtained using the minimum MSE-based approaches\nformulated within $\\textbf{$\\textit {Stein's unbiased risk estimation}$}$\n(SURE) framework, especially in the low measurement signal-to-noise ratio (SNR)\nregime. Performance comparison with three state-of-the-art denoising\nalgorithms, carried out on electrocardiogram signals and two test signals taken\nfrom the $\\textit{Wavelab}$ toolbox, exhibits that the MPE framework results in\nconsistent SNR gains for input SNRs below $5$ dB.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 10:26:59 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Sadasivan", "Jishnu", ""], ["Mukherjee", "Subhadip", ""], ["Seelamantula", "Chandra Sekhar", ""]]}, {"id": "1702.07909", "submitter": "Shane Jensen", "authors": "Colman Humphrey and Shane T. Jensen and Dylan Small and Rachel\n  Thurston", "title": "Urban Vibrancy and Safety in Philadelphia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analyses of urban environments have been recently improved\nthrough publicly available high resolution data and mapping technologies that\nhave been adopted across industries. These technologies allow us to create\nmetrics to empirically investigate urban design principles of the past\nhalf-century. Philadelphia is an interesting case study for this work, with its\nrapid urban development and population increase in the last decade. We outline\na data analysis pipeline for exploring the association between safety and local\nneighborhood features such as population, economic health and the built\nenvironment. As a particular example of our analysis pipeline, we focus on\nquantitative measures of the built environment that serve as proxies for\nvibrancy: the amount of human activity in a local area. Historically, vibrancy\nhas been very challenging to measure empirically. Measures based on land use\nzoning are not an adequate description of local vibrancy and so we construct a\ndatabase and set of measures of business activity in each neighborhood. We\nemploy several matching analyses to explore the relationship between\nneighborhood vibrancy and safety, such as comparing high crime versus low crime\nlocations within the same neighborhood. As additional sources of urban data\nbecome available, our analysis pipeline can serve as the template for further\ninvestigations into the relationships between safety, economic factors and the\nbuilt environment at the local neighborhood level.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 15:56:33 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 00:20:43 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 14:41:18 GMT"}, {"version": "v4", "created": "Wed, 16 Jan 2019 22:25:33 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Humphrey", "Colman", ""], ["Jensen", "Shane T.", ""], ["Small", "Dylan", ""], ["Thurston", "Rachel", ""]]}, {"id": "1702.07981", "submitter": "Yanxun Xu", "authors": "Fangzheng Xie and Mingyuan Zhou and Yanxun Xu", "title": "BayCount: A Bayesian Decomposition Method for Inferring Tumor\n  Heterogeneity using RNA-Seq Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor is heterogeneous - a tumor sample usually consists of a set of\nsubclones with distinct transcriptional profiles and potentially different\ndegrees of aggressiveness and responses to drugs. Understanding tumor\nheterogeneity is therefore critical to precise cancer prognosis and treatment.\nIn this paper, we introduce BayCount, a Bayesian decomposition method to infer\ntumor heterogeneity with highly over-dispersed RNA sequencing count data. Using\nnegative binomial factor analysis, BayCount takes into account both the\nbetween-sample and gene-specific random effects on raw counts of sequencing\nreads mapped to each gene. For posterior inference, we develop an efficient\ncompound Poisson based blocked Gibbs sampler. Through extensive simulation\nstudies and analysis of The Cancer Genome Atlas lung cancer and kidney cancer\nRNA sequencing count data, we show that BayCount is able to accurately estimate\nthe number of subclones, the proportions of these subclones in each tumor\nsample, and the gene expression profiles in each subclone. Our method\nrepresents the first effort in characterizing tumor heterogeneity using RNA\nsequencing count data that simultaneously removes the need of normalizing the\ncounts, achieves statistical robustness, and obtains biologically and\nclinically meaningful insights.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 03:10:41 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xie", "Fangzheng", ""], ["Zhou", "Mingyuan", ""], ["Xu", "Yanxun", ""]]}, {"id": "1702.08088", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir", "title": "Selection of training populations (and other subset selection problems)\n  with an accelerated genetic algorithm (STPGA: An R-package for selection of\n  training populations with a genetic algorithm)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal subset selection is an important task that has numerous algorithms\ndesigned for it and has many application areas. STPGA contains a special\ngenetic algorithm supplemented with a tabu memory property (that keeps track of\npreviously tried solutions and their fitness for a number of iterations), and\nwith a regression of the fitness of the solutions on their coding that is used\nto form the ideal estimated solution (look ahead property) to search for\nsolutions of generic optimal subset selection problems. I have initially\ndeveloped the programs for the specific problem of selecting training\npopulations for genomic prediction or association problems, therefore I give\ndiscussion of the theory behind optimal design of experiments to explain the\ndefault optimization criteria in STPGA, and illustrate the use of the programs\nin this endeavor. Nevertheless, I have picked a few other areas of application:\nsupervised and unsupervised variable selection based on kernel alignment,\nsupervised variable selection with design criteria, influential observation\nidentification for regression, solving mixed integer quadratic optimization\nproblems, balancing gains and inbreeding in a breeding population. Some of\nthese illustrations pertain new statistical approaches.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 21:23:33 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Akdemir", "Deniz", ""]]}, {"id": "1702.08140", "submitter": "Adrien Ickowicz", "authors": "Adrien Ickowicz and Jessica H. Ford and Keith R. Hayes", "title": "A mixture model approach to infer land-use influence on point referenced\n  water quality", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of water quality across space and time is of considerable\ninterest for both agricultural and public health reasons. The standard method\nto assess the water quality of a catchment, or a group of catchments, usually\ninvolves collecting point measurements of water quality and other additional\ninformation such as the date and time of measurements, rainfall amounts, the\nland-use and soil-type of the catchment and the elevation. Some of this\nauxiliary information will be point data, measured at the exact location,\nwhereas other such as land-use will be areal data often in a compositional\nformat. Two problems arise if analysts try to incorporate this information into\na statistical model in order to predict (for example) the influence of land-use\non water quality. First is the spatial change of support problem that arises\nwhen using areal data to predict outcomes at point locations. Secondly, the\nphysical process driving water quality is not compositional, rather it is the\nobservation process that provides compositional data. In this paper we present\nan approach that accounts for these two issues by using a latent variable to\nidentify the land-use that most likely influences water quality. This latent\nvariable is used in a spatial mixture model to help estimate the influence of\nland-use on water quality. We demonstrate the potential of this approach with\ndata from a water quality research study in the Mount Lofty range, in South\nAustralia.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 04:17:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Ickowicz", "Adrien", ""], ["Ford", "Jessica H.", ""], ["Hayes", "Keith R.", ""]]}, {"id": "1702.08185", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Benjamin Hofner, Elisabeth Waldmann, Tobias Hepp, Olaf\n  Gefeller, Matthias Schmid", "title": "An update on statistical boosting in biomedicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical boosting algorithms have triggered a lot of research during the\nlast decade. They combine a powerful machine-learning approach with classical\nstatistical modelling, offering various practical advantages like automated\nvariable selection and implicit regularization of effect estimates. They are\nextremely flexible, as the underlying base-learners (regression functions\ndefining the type of effect for the explanatory variables) can be combined with\nany kind of loss function (target function to be optimized, defining the type\nof regression setting). In this review article, we highlight the most recent\nmethodological developments on statistical boosting regarding variable\nselection, functional regression and advanced time-to-event modelling.\nAdditionally, we provide a short overview on relevant applications of\nstatistical boosting in biomedicine.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:33:26 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Mayr", "Andreas", ""], ["Hofner", "Benjamin", ""], ["Waldmann", "Elisabeth", ""], ["Hepp", "Tobias", ""], ["Gefeller", "Olaf", ""], ["Schmid", "Matthias", ""]]}, {"id": "1702.08262", "submitter": "Andreas Martin Kettner", "authors": "Andreas Martin Kettner, Mario Paolone", "title": "Sequential Discrete Kalman Filter for Real-Time State Estimation in\n  Power Distribution Systems: Theory and Implementation", "comments": "Index Terms: Active Distribution Network (ADN), Real-Time State\n  Estimator (RTSE), Phasor Measurement Unit (PMU), Sequential Discrete Kalman\n  Filter (SDKF), Field-Programmable Gate Array (FPGA)", "journal-ref": "IEEE Transactions on Instrumentation and Measurement, Volume 66,\n  Issue 9, September 2017", "doi": "10.1109/TIM.2017.2708278", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the feasibility of implementing Real-Time State\nEstimators (RTSEs) for Active Distribution Networks (ADNs) in\nField-Programmable Gate Arrays (FPGAs) by presenting an operational prototype.\nThe prototype is based on a Linear State Estimator (LSE) that uses\nsynchrophasor measurements from Phasor Measurement Units (PMUs). The underlying\nalgorithm is the Sequential Discrete Kalman Filter (SDKF), an equivalent\nformulation of the Discrete Kalman Filter (DKF) for the case of uncorrelated\nmeasurement noise. In this regard, this work formally proves the equivalence\nthe SDKF and the DKF, and highlights the suitability of the SDKF for an FPGA\nimplementation by means of a computational complexity analysis. The developed\nprototype is validated using a case study adapted from the IEEE 34-node\ndistribution test feeder.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 13:03:33 GMT"}, {"version": "v2", "created": "Mon, 3 Apr 2017 16:49:28 GMT"}, {"version": "v3", "created": "Tue, 4 Apr 2017 09:41:23 GMT"}, {"version": "v4", "created": "Wed, 5 Apr 2017 09:17:33 GMT"}, {"version": "v5", "created": "Tue, 27 Jun 2017 06:33:00 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Kettner", "Andreas Martin", ""], ["Paolone", "Mario", ""]]}, {"id": "1702.08560", "submitter": "Deborah Shutt", "authors": "Deborah P. Shutt, Carrie A. Manore, Stephen Pankavich, Aaron T.\n  Porter, Sara Y. Del Valle", "title": "Estimating the reproductive number, total outbreak size, and reporting\n  rates for Zika epidemics in South and Central America", "comments": "35 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As South and Central American countries prepare for increased birth defects\nfrom Zika virus outbreaks and plan for mitigation strategies to minimize\nongoing and future outbreaks, understanding important characteristics of Zika\noutbreaks and how they vary across regions is a challenging and important\nproblem. We developed a mathematical model for the 2015 Zika virus outbreak\ndynamics in Colombia, El Salvador, and Suriname. We fit the model to publicly\navailable data provided by the Pan American Health Organization, using\nApproximate Bayesian Computation to estimate parameter distributions and\nprovide uncertainty quantification. An important model input is the at-risk\nsusceptible population, which can vary with a number of factors including\nclimate, elevation, population density, and socio-economic status. We informed\nthis initial condition using the highest historically reported dengue incidence\nmodified by the probable dengue reporting rates in the chosen countries. The\nmodel indicated that a country-level analysis was not appropriate for Colombia.\nWe then estimated the basic reproduction number, or the expected number of new\nhuman infections arising from a single infected human, to range between 4 and 6\nfor El Salvador and Suriname with a median of 4.3 and 5.3, respectively. We\nestimated the reporting rate to be around 16% in El Salvador and 18% in\nSuriname with estimated total outbreak sizes of 73,395 and 21,647 people,\nrespectively. The uncertainty in parameter estimates highlights a need for\nresearch and data collection that will better constrain parameter ranges.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:22:03 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Shutt", "Deborah P.", ""], ["Manore", "Carrie A.", ""], ["Pankavich", "Stephen", ""], ["Porter", "Aaron T.", ""], ["Del Valle", "Sara Y.", ""]]}, {"id": "1702.08638", "submitter": "John Malik", "authors": "John Malik, Neil Reed, Chun-Li Wang, and Hautieng Wu", "title": "Single-lead f-wave extraction using diffusion geometry", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": "10.1088/1361-6579/aa707c", "report-no": null, "categories": "physics.data-an q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel single-lead f-wave extraction algorithm based on the modern diffusion\ngeometry data analysis framework is proposed. The algorithm is essentially an\naveraged beat subtraction algorithm, where the ventricular activity template is\nestimated by combining a newly designed metric, the \"diffusion distance,\" and\nthe non-local Euclidean median based on the non-linear manifold setup. We\ncoined the algorithm DD-NLEM. Two simulation schemes are considered, and the\nnew algorithm DD-NLEM outperforms traditional algorithms, including the average\nbeat subtraction, principal component analysis, and adaptive singular value\ncancellation, in different evaluation metrics with statistical significance.\nThe clinical potential is shown in the real Holter signal, and we introduce a\nnew score to evaluate the performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 04:25:44 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 16:31:26 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Malik", "John", ""], ["Reed", "Neil", ""], ["Wang", "Chun-Li", ""], ["Wu", "Hautieng", ""]]}]