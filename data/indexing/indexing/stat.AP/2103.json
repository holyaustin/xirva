[{"id": "2103.00054", "submitter": "Prathamesh Muzumdar", "authors": "Prathamesh Muzumdar, Ganga Prasad Basyal, Piyush Vyas", "title": "Moderating effects of retail operations and hard-sell sales techniques\n  on salesperson's interpersonal skills and customer repurchase intention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Salesperson's interpersonal skills have always played an important role in\ninfluencing various stages of customer's purchase decision. With the increase\nin retail outlets and merchandisers, retail operations have taken a pivotal\nrole in influencing the salesperson's sales practices and customer's purchase\ndecisions.This study tries to examine the influence of retail operations and\nhard-selling startegies on the relationship between salesperson's interpersonal\nskills and customer repurchase intention. Salesperson's interpersonal skills\nare the trained and tacit competencies that a salesperson employs to improve\ncustomer relationship and sales performance. Many organizations prefer skill\ntraining fails to attract repetitive purchases due to unavoidable extraneous\nfactors. It has become a necessity to understand the role of extraneous factors\nlike retail operations on the relationship between salesperson's interpersonal\nskills and customer repurchase intention. The findings suggest that retail\noperations significantly moderate the relationship between salesperson's\ninterpersonal skills and customer repurchase intention. We also find that\nhard-sell sales techniques play a significant moderating role in negatively\ninfleuncing customer repurchase intention . This study has important\nimplications for retailers and sales managers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 21:21:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Muzumdar", "Prathamesh", ""], ["Basyal", "Ganga Prasad", ""], ["Vyas", "Piyush", ""]]}, {"id": "2103.00264", "submitter": "Parley Ruogu Yang", "authors": "Parley Ruogu Yang", "title": "Forecasting high-frequency financial time series: an adaptive learning\n  approach with the order book data", "comments": "Key words: forecasting methods, statistical learning, high-frequency\n  order book", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM q-fin.TR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a forecast-centric adaptive learning model that engages\nwith the past studies on the order book and high-frequency data, with\napplications to hypothesis testing. In line with the past literature, we\nproduce brackets of summaries of statistics from the high-frequency bid and ask\ndata in the CSI 300 Index Futures market and aim to forecast the one-step-ahead\nprices. Traditional time series issues, e.g. ARIMA order selection,\nstationarity, together with potential financial applications are covered in the\nexploratory data analysis, which pave paths to the adaptive learning model. By\ndesigning and running the learning model, we found it to perform well compared\nto the top fixed models, and some could improve the forecasting accuracy by\nbeing more stable and resilient to non-stationarity. Applications to hypothesis\ntesting are shown with a rolling window, and further potential applications to\nfinance and statistics are outlined.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:42:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yang", "Parley Ruogu", ""]]}, {"id": "2103.00486", "submitter": "Mark He", "authors": "Mark He, Dylan Lu, Jason Xu, Rose Mary Xavier", "title": "Community Detection in Weighted Multilayer Networks with Ambient Noise", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of stochastic blockmodel for multilayer weighted\nnetworks that accounts for the presence of a global ambient noise governing\nbetween-block interactions. We induce a hierarchy of classifications in\nweighted multilayer networks by assuming that all but one cluster (block) are\ngoverned by unique local signals, while a single block behaves identically as\ninteractions across differing blocks (ambient noise). Hierarchical variational\ninference is employed to jointly detect and typologize blocks as signal or\nnoise. We call this model for multilayer weighted networks the Stochastic Block\n(with) Ambient Noise Model(SBANM) and develop an associated community detection\nalgorithm. Then we apply this method to subjects in the Philadelphia\nNeurodevelopmental Cohort to discover communities of subjects with similar\npsychopathological symptoms in relation to psychosis.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 07:47:28 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 05:05:35 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 15:08:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["He", "Mark", ""], ["Lu", "Dylan", ""], ["Xu", "Jason", ""], ["Xavier", "Rose Mary", ""]]}, {"id": "2103.00594", "submitter": "Camila Lorenz", "authors": "Camila Lorenz, Patricia Marques Moralejo Bermudi, Marcelo Antunes\n  Failla, Breno Souza de Aguiar, Tatiana Natasha Toporcov, Francisco\n  Chiaravalloti Neto and Ligia Vizeu Barrozo", "title": "Examining socioeconomic factors to understand the hospital case-fatality\n  rates of COVID-19 in the city of Sao Paulo, Brazil", "comments": "10 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding differences in hospital case-fatality rates (HCFRs) of\ncoronavirus disease (COVID-19) may help evaluate its severity and the capacity\nof the healthcare system to reduce mortality. We examined the variability in\nHCFRs of COVID-19 in relation to spatial inequalities in socioeconomic factors\nacross the city of Sao Paulo, Brazil. We found that HCFRs were higher for men\nand for individuals aged 60 years and older. Our models identified per capita\nincome as a significant factor that is negatively associated with the HCFRs of\nCOVID-19, even after adjusting by age, sex and presence of risk factors.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 19:19:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lorenz", "Camila", ""], ["Bermudi", "Patricia Marques Moralejo", ""], ["Failla", "Marcelo Antunes", ""], ["de Aguiar", "Breno Souza", ""], ["Toporcov", "Tatiana Natasha", ""], ["Neto", "Francisco Chiaravalloti", ""], ["Barrozo", "Ligia Vizeu", ""]]}, {"id": "2103.00629", "submitter": "Sarah Samorodnitsky", "authors": "Sarah Samorodnitsky, Katherine A. Hoadley, Eric F. Lock", "title": "A Hierarchical Spike-and-Slab Model for Pan-Cancer Survival Using\n  Pan-Omic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pan-omics, pan-cancer analysis has advanced our understanding of the\nmolecular heterogeneity of cancer, expanding what was known from single-cancer\nor single-omics studies. However, pan-cancer, pan-omics analyses have been\nlimited in their ability to use information from multiple sources of data\n(e.g., omics platforms) and multiple sample sets (e.g., cancer types) to\npredict important clinical outcomes, like overall survival. We address the\nissue of prediction across multiple high-dimensional sources of data and\nmultiple sample sets by using exploratory results from BIDIFAC+, a method for\nintegrative dimension reduction of bidimensionally-linked matrices, in a\npredictive model. We apply a Bayesian hierarchical model that performs variable\nselection using spike-and-slab priors which are modified to allow for the\nborrowing of information across clustered data. This method is used to predict\noverall patient survival from the Cancer Genome Atlas (TCGA) using data from 29\ncancer types and 4 omics sources. Our model selected patterns of variation\nidentified by BIDIFAC+ that differentiate clinical tumor subtypes with markedly\ndifferent survival outcomes. We also use simulations to evaluate the\nperformance of the modified spike-and-slab prior in terms of its variable\nselection accuracy and prediction accuracy under different underlying\ndata-generating frameworks. Software and code used for our analysis can be\nfound at https://github.com/sarahsamorodnitsky/HierarchicalSS_PanCanPanOmics/ .\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 21:35:29 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Samorodnitsky", "Sarah", ""], ["Hoadley", "Katherine A.", ""], ["Lock", "Eric F.", ""]]}, {"id": "2103.00674", "submitter": "Kai Zhang", "authors": "Kai Zhang, Zhigen Zhao, Wen Zhou", "title": "BEAUTY Powered BEAST", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.AP stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study inference about the uniform distribution with the proposed binary\nexpansion approximation of uniformity (BEAUTY) approach. Through an extension\nof the celebrated Euler's formula, we approximate the characteristic function\nof any copula distribution with a linear combination of means of binary\ninteractions from marginal binary expansions. This novel characterization\nenables a unification of many important existing tests through an approximation\nfrom some quadratic form of symmetry statistics, where the deterministic weight\nmatrix characterizes the power properties of each test. To achieve a uniformly\nhigh power, we study test statistics with data-adaptive weights through an\noracle approach, referred to as the binary expansion adaptive symmetry test\n(BEAST). By utilizing the properties of the binary expansion filtration, we\nshow that the Neyman-Pearson test of uniformity can be approximated by an\noracle weighted sum of symmetry statistics. The BEAST with this oracle leads\nall existing tests we considered in empirical power against all complex forms\nof alternatives. This oracle therefore sheds light on the potential of\nsubstantial improvements in power and on the form of optimal weights under each\nalternative. By approximating this oracle with data-adaptive weights, we\ndevelop the BEAST that improves the empirical power of many existing tests\nagainst a wide spectrum of common alternatives while providing clear\ninterpretation of the form of non-uniformity upon rejection. We illustrate the\nBEAST with a study of the relationship between the location and brightness of\nstars.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:36:15 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:08:13 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Kai", ""], ["Zhao", "Zhigen", ""], ["Zhou", "Wen", ""]]}, {"id": "2103.00702", "submitter": "Santiago Olivella", "authors": "Santiago Olivella, Tyler Pratt and Kosuke Imai", "title": "Dynamic Stochastic Blockmodel Regression for Network Data: Application\n  to International Militarized Conflicts", "comments": "21 pages (main text), 18 pages (appendix), 19 figures, submitted to\n  JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A primary goal of social science research is to understand how latent group\nmemberships predict the dynamic process of network evolution. In the modeling\nof international conflicts, for example, scholars hypothesize that membership\nin geopolitical coalitions shapes the decision to engage in militarized\nconflict. Such theories explain the ways in which nodal and dyadic\ncharacteristics affect the evolution of relational ties over time via their\neffects on group memberships. To aid the empirical testing of these arguments,\nwe develop a dynamic model of network data by combining a hidden Markov model\nwith a mixed-membership stochastic blockmodel that identifies latent groups\nunderlying the network structure. Unlike existing models, we incorporate\ncovariates that predict node membership in latent groups as well as the direct\nformation of edges between dyads. While prior substantive research often\nassumes the decision to engage in militarized conflict is independent across\nstates and static over time, we demonstrate that conflict patterns are driven\nby states' evolving membership in geopolitical blocs. Changes in monadic\ncovariates like democracy shift states between coalitions, generating\nheterogeneous effects on conflict over time and across states. The proposed\nmethodology, which relies on a variational approximation to a collapsed\nposterior distribution as well as stochastic optimization for scalability, is\nimplemented through an open-source software package.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:28:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Olivella", "Santiago", ""], ["Pratt", "Tyler", ""], ["Imai", "Kosuke", ""]]}, {"id": "2103.00772", "submitter": "Michael Evans", "authors": "Luai Al Labadi, Michael Evans and Qiaoyu Liang", "title": "ROC Analyses Based on Measuring Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ROC analyses are considered under a variety of assumptions concerning the\ndistributions of a measurement $X$ in two populations. These include the\nbinormal model as well as nonparametric models where little is assumed about\nthe form of distributions. The methodology is based on a characterization of\nstatistical evidence which is dependent on the specification of prior\ndistributions for the unknown population distributions as well as for the\nrelevant prevalence $w$ of the disease in a given population. In all cases,\nelicitation algorithms are provided to guide the selection of the priors.\nInferences are derived for the AUC as well as the cutoff $c$ used for\nclassification and the associated error characteristics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 05:57:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Labadi", "Luai Al", ""], ["Evans", "Michael", ""], ["Liang", "Qiaoyu", ""]]}, {"id": "2103.00844", "submitter": "Alessandra Micheletti", "authors": "Rongjiao Ji, Alessandra Micheletti, Natasa Krklec Jerinkic, Zoranka\n  Desnica", "title": "Emotion pattern detection on facial videos using functional statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is an increasing scientific interest in automatically analysing and\nunderstanding human behavior, with particular reference to the evolution of\nfacial expressions and the recognition of the corresponding emotions. In this\npaper we propose a technique based on Functional ANOVA to extract significant\npatterns of face muscles movements, in order to identify the emotions expressed\nby actors in recorded videos. We determine if there are time-related\ndifferences on expressions among emotional groups by using a functional F-test.\nSuch results are the first step towards the construction of a reliable\nautomatic emotion recognition system\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:31:08 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ji", "Rongjiao", ""], ["Micheletti", "Alessandra", ""], ["Jerinkic", "Natasa Krklec", ""], ["Desnica", "Zoranka", ""]]}, {"id": "2103.00874", "submitter": "Wei Li", "authors": "Xiuqing Li, Wei Li, Xinlin Yi, Qihang Huang, Yuhang Wang, Chenzhe Ye", "title": "Path-specific Underwater Acoustic Channel Tracking and its Application\n  in Passive Time Reversal Mirror", "comments": "Submitted to IEEE Journal of Oceanic Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the underwater acoustic channel which is time-variant and\ndoubly-spread in this work. Since conventional channel estimation and decision\nfeedback equalizer (DFE) can not work well for this type of channel, a\npath-specific underwater acoustic channel tracking is proposed. It is based on\nthe framework of Kalman filter. We provide a simplified sound propagation model\nas the state transition model. A multipath tracker is proposed which is\ntolerant of the model-mismatch. Then we can obtain the time-variant path number\nand path-specific parameters such as delay and Doppler scaling factor. We also\nconsider the application of the proposed path-specific underwater acoustic\nchannel tracking. We propose two types of passive time reversal mirror (PTRM)\nwith our path-specific parameters for time-variant and doubly-spread underwater\nacoustic channel. With the path-specific parameters obtained by the proposed\nchannel tracking, the proposed PTRM can not only match the time dispersion as\nconventional PTRM, but also the doubly-spread channel, since the path-specific\ndelay and Doppler scaler factor can help to match the channel in both time and\nfrequency domain. For extensive doubly-spread channel, we can further apply the\npath-specific compensation to the PTRM. Both simulations and experimental\nresults by data from 2016 Qiandao Lake experiment show the efficiency of\nproposed path-specific channel tracking and proposed PTRMs with path-specific\nparameters.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:50:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Xiuqing", ""], ["Li", "Wei", ""], ["Yi", "Xinlin", ""], ["Huang", "Qihang", ""], ["Wang", "Yuhang", ""], ["Ye", "Chenzhe", ""]]}, {"id": "2103.01097", "submitter": "Karthik Bharath", "authors": "Min Ho Cho, Sebastian Kurtek, Karthik Bharath", "title": "Tangent functional canonical correlation analysis for densities and\n  shapes, with applications to multimodal imaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is quite common for functional data arising from imaging data to assume\nvalues in infinite-dimensional manifolds. Uncovering associations between two\nor more such nonlinear functional data extracted from the same object across\nmedical imaging modalities can assist development of personalized treatment\nstrategies. We propose a method for canonical correlation analysis between\npaired probability densities or shapes of closed planar curves, routinely used\nin biomedical studies, which combines a convenient linearization and dimension\nreduction of the data using tangent space coordinates. Leveraging the fact that\nthe corresponding manifolds are submanifolds of unit Hilbert spheres, we\ndescribe how finite-dimensional representations of the functional data objects\ncan be easily computed, which then facilitates use of standard multivariate\ncanonical correlation analysis methods. We further construct and visualize\ncanonical variate directions directly on the space of densities or shapes.\nUtility of the method is demonstrated through numerical simulations and\nperformance on a magnetic resonance imaging dataset of Glioblastoma Multiforme\nbrain tumors.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:05:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Cho", "Min Ho", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""]]}, {"id": "2103.01201", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe, Massimiliano Marcellino, Dalibor Stevanovic", "title": "Can Machine Learning Catch the COVID-19 Recession?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on evidence gathered from a newly built large macroeconomic data set\nfor the UK, labeled UK-MD and comparable to similar datasets for the US and\nCanada, it seems the most promising avenue for forecasting during the pandemic\nis to allow for general forms of nonlinearity by using machine learning (ML)\nmethods. But not all nonlinear ML methods are alike. For instance, some do not\nallow to extrapolate (like regular trees and forests) and some do (when\ncomplemented with linear dynamic components). This and other crucial aspects of\nML-based forecasting in unprecedented times are studied in an extensive\npseudo-out-of-sample exercise.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:47:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["Marcellino", "Massimiliano", ""], ["Stevanovic", "Dalibor", ""]]}, {"id": "2103.01220", "submitter": "Prathamesh Muzumdar", "authors": "Prathamesh Muzumdar, George Kurian", "title": "Empirical study to explore the influence of salesperson's customer\n  orientation on customer loyalty", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.00054", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study tries to examine the influence of salesperson's customer\norientation on customer loyalty. Customer orientation is the approach taken by\na salesperson to improve customer relationship and increase sales. Many\norganizations prefer sales orientation as a strategic approach towards\nincreasing sales. Though successful in its objective, sales orientation fails\nto attract repetitive purchase. It has become a necessity to train frontline\nemployees to better understand the customer needs, keeping in mind the firm's\nultimate objective. This study examines the improvements customer orientation\ncan bring to increase repurchases thus leading to customer loyalty. The\nfindings suggest that product assortment, long lines of customers, customers'\nannual income, and the listening skills of salesperson were the significant\nantecedents of customer loyalty.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 05:15:59 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Muzumdar", "Prathamesh", ""], ["Kurian", "George", ""]]}, {"id": "2103.01254", "submitter": "Vincenzo Nardelli", "authors": "Giorgio Alleva, Giuseppe Arbia, Piero Demetrio Falorsi, Vincenzo\n  Nardelli, Alberto Zuliani", "title": "Spatial sampling design to improve the efficiency of the estimation of\n  the critical parameters of the SARS-CoV-2 epidemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pandemic linked to COVID-19 infection represents an unprecedented\nclinical and healthcare challenge for many medical researchers attempting to\nprevent its worldwide spread. This pandemic also represents a major challenge\nfor statisticians involved in quantifying the phenomenon and in offering timely\ntools for the monitoring and surveillance of critical pandemic parameters. In a\nrecent paper, Alleva et al. (2020) proposed a two-stage sample design to build\na continuous-time surveillance system designed to correctly quantify the number\nof infected people through an indirect sampling mechanism that could be\nrepeated in several waves over time to capture different target variables in\nthe different stages of epidemic development. The proposed method exploits the\nindirect sampling (Lavalle, 2007; Kiesl, 2016) method employed in the\nestimation of rare and elusive populations (Borchers, 2009; Lavall\\'ee and\nRivest, 2012) and a capture/recapture mechanism (Sudman, 1988; Thompson and\nSeber, 1996). In this paper, we extend the proposal of Alleva et al. (2020) to\ninclude a spatial sampling mechanism (M\\\"uller, 1998; Grafstr\\\"om et al., 2012,\nJauslin and Till\\`e, 2020) in the process of data collection to achieve the\nsame level of precision with fewer sample units, thereby facilitating the\nprocess of data collection in a situation where timeliness and costs are\ncrucial elements. We present the basic idea of the new sample design,\nanalytically prove the theoretical properties of the associated estimators and\nshow the relative advantages through a systematic simulation study where all\nthe typical elements of an epidemic are accounted for.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:08:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Alleva", "Giorgio", ""], ["Arbia", "Giuseppe", ""], ["Falorsi", "Piero Demetrio", ""], ["Nardelli", "Vincenzo", ""], ["Zuliani", "Alberto", ""]]}, {"id": "2103.01313", "submitter": "Donghui Yan", "authors": "Donghui Yan, Aiyou Chen, Buqing Yang", "title": "Towards Understanding the COVID-19 Case Fatality Rate", "comments": "13 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An important parameter for COVID-19 is the case fatality rate (CFR). It has\nbeen applied to wide applications, including the measure of the severity of the\ninfection, the estimation of the number of infected cases, risk assessment etc.\nHowever, there remains a lack of understanding on several aspects of CFR,\nincluding population factors that are important to CFR, the apparent\ndiscrepancy of CFRs in different countries, and how the age effect comes into\nplay. We analyze the CFRs at two different time snapshots, July 6 and Dec 28,\n2020, with one during the first wave and the other a second wave of the\nCOVID-19 pandemic. We consider two important population covariates, age and GDP\nas a proxy for the quality and abundance of public health. Extensive\nexploratory data analysis leads to some interesting findings. First, there is a\nclear exponential age effect among different age groups, and, more importantly,\nthe exponential index is almost invariant across countries and time in the\npandemic. Second, the roles played by the age and GDP are a little surprising:\nduring the first wave, age is a more significant factor than GDP, while their\nroles have switched during the second wave of the pandemic, which may be\npartially explained by the delay in time for the quality and abundance of\npublic health and medical research to factor in.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 21:09:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Yan", "Donghui", ""], ["Chen", "Aiyou", ""], ["Yang", "Buqing", ""]]}, {"id": "2103.01355", "submitter": "Weichi Yao", "authors": "Hoora Moradian and Weichi Yao and Denis Larocque and Jeffrey S.\n  Simonoff and Halina Frydman", "title": "Dynamic estimation with random forests for discrete-time survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying covariates are often available in survival studies and\nestimation of the hazard function needs to be updated as new information\nbecomes available. In this paper, we investigate several different\neasy-to-implement ways that random forests can be used for dynamic estimation\nof the survival or hazard function from discrete-time survival data. The\nresults from a simulation study indicate that all methods can perform well, and\nthat none dominates the others. In general, situations that are more difficult\nfrom an estimation point of view (such as weaker signals and less data) favour\na global fit, pooling over all time points, while situations that are easier\nfrom an estimation point of view (such as stronger signals and more data) favor\nlocal fits.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 23:46:46 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 17:40:10 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Moradian", "Hoora", ""], ["Yao", "Weichi", ""], ["Larocque", "Denis", ""], ["Simonoff", "Jeffrey S.", ""], ["Frydman", "Halina", ""]]}, {"id": "2103.01532", "submitter": "Ranjan Maitra", "authors": "Subrata Pal and Somak Dutta and Ranjan Maitra", "title": "Model-based Personalized Synthetic MR Imaging", "comments": "13 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic Magnetic Resonance (MR) imaging predicts images at new design\nparameter settings from a few observed MR scans. Model-based methods, that use\nboth the physical and statistical properties underlying the MR signal and its\nacquisition, can predict images at any setting from as few as three scans,\nallowing it to be used in individualized patient- and anatomy-specific\ncontexts. However, the estimation problem in model-based synthetic MR imaging\nis ill-posed and so regularization, in the form of correlated Gaussian Markov\nRandom Fields, is imposed on the voxel-wise spin-lattice relaxation time,\nspin-spin relaxation time and the proton density underlying the MR image. We\ndevelop theoretically sound but computationally practical matrix-free\nestimation methods for synthetic MR imaging. Our evaluations demonstrate\nexcellent ability of our methods to synthetize MR images in a clinical\nframework and also estimation and prediction accuracy and consistency. An added\nstrength of our model-based approach, also developed and illustrated here, is\nthe accurate estimation of standard errors of regional means in the synthesized\nimages.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:24:35 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 17:16:43 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pal", "Subrata", ""], ["Dutta", "Somak", ""], ["Maitra", "Ranjan", ""]]}, {"id": "2103.01650", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Conditional Precedence Orders for Stochastic Comparison of Random\n  Variables", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the stochastic orders for comparing random variables, considered in\nthe literature, are afflicted with two main drawbacks: (i) lack of connex\nproperty and (ii) lack of consideration of any dependence structure between the\nrandom variables. Both these drawbacks can be overcome at the cost of\ntransitivity with the stochastic precedence order, which may seem to be a good\nchoice in particular when only two random variables are under consideration, a\nsituation where the question of transitivity does not arise. In this paper, we\nshow that even under such favorable conditions, stochastic precedence order may\ndirect to misleading conclusion in certain situations and develop variations of\nthe order to address the phenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:23:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01727", "submitter": "Sugata Ghosh", "authors": "Sugata Ghosh and Asok K. Nanda", "title": "Departure-based Asymptotic Stochastic Order for Random Processes", "comments": "33 pages, reference hyperlink added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a specific asymptotic stochastic order for random\nprocesses based on the measure of departure discussed in the literature. As\napplications, we stochastically compare mixtures of order statistics and record\nvalues coming from two different homogeneous samples, as the sample size\nbecomes large.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:59:30 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 18:26:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ghosh", "Sugata", ""], ["Nanda", "Asok K.", ""]]}, {"id": "2103.01742", "submitter": "David Woods", "authors": "T. Maishman, S. Schaap, D.S. Silk, S.J. Nevitt, D.C. Woods, V.E.\n  Bowman", "title": "Statistical methods used to combine the effective reproduction number,\n  R(t), and other related measures of COVID-19 in the UK", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the COVID-19 pandemic, a range of epidemiological models have been used to\npredict the number of new daily infections, $I$, daily rate of exponential\ngrowth, $r$, and effective reproduction number, $R(t)$. These models differ in\ntheir approaches (e.g. mechanistic or empirical) and/or assumptions about\nspatial or age mixing, and some capture uncertainty in scientific understanding\nof disease dynamics, and/or have different simplifying assumptions. Combining\nestimates from multiple models to better understand the variation of these\noutcome measures is important to help inform decision making. We incorporate\nestimates of these outcome measures from a number of candidate models for\nspecific UK nations/regions using meta analysis. Random effects models have\nbeen implemented to accommodate differing modelling approaches and assumptions\nbetween candidate models. Restricted maximum likelihood (REML) is used to\nestimate the heterogeneity variance parameter, with two approaches to calculate\nthe confidence interval for the combined effect: standard Wald-type intervals\nand the Knapp and Hartung (KNHA) method. Approaches using REML alone and\nREML+KNHA provided similar ranges of variation for $R(t)$ and $r$. However,\ndifferences were observed when combining estimates on $I$, with the REML+KNHA\napproach providing more conservative confidence intervals. This is likely due\nto the limited number of candidate models contributing estimates for this\noutcome measure, coupled with the large variability observed between model\nestimates. Utilising these meta-analysis techniques has allowed for\nstatistically robust combined estimates to be calculated for key COVID-19\noutcome measures, allowing an overall assessment of the current response\nmeasures with associated uncertainty. This in turn allows timely and informed\ndecision making based on all available information.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:23:18 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Maishman", "T.", ""], ["Schaap", "S.", ""], ["Silk", "D. S.", ""], ["Nevitt", "S. J.", ""], ["Woods", "D. C.", ""], ["Bowman", "V. E.", ""]]}, {"id": "2103.01926", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe", "title": "Slow-Growing Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest's performance can be matched by a single slow-growing tree\n(SGT), which uses a learning rate to tame CART's greedy algorithm. SGT exploits\nthe view that CART is an extreme case of an iterative weighted least square\nprocedure. Moreover, a unifying view of Boosted Trees (BT) and Random Forests\n(RF) is presented. Greedy ML algorithms' outcomes can be improved using either\n\"slow learning\" or diversification. SGT applies the former to estimate a single\ndeep tree, and Booging (bagging stochastic BT with a high learning rate) uses\nthe latter with additive shallow trees. The performance of this tree ensemble\nquaternity (Booging, BT, SGT, RF) is assessed on simulated and real regression\ntasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:37:13 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 16:37:56 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Coulombe", "Philippe Goulet", ""]]}, {"id": "2103.01992", "submitter": "Mohammadhossein Toutiaee", "authors": "Indrajeet Y. Javeri, Mohammadhossein Toutiaee, Ismailcem B. Arpinar,\n  Tom W. Miller, John A. Miller", "title": "Improving Neural Networks for Time Series Forecasting using Data\n  Augmentation and AutoML", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods such as the Box-Jenkins method for time-series\nforecasting have been prominent since their development in 1970. Many\nresearchers rely on such models as they can be efficiently estimated and also\nprovide interpretability. However, advances in machine learning research\nindicate that neural networks can be powerful data modeling techniques, as they\ncan give higher accuracy for a plethora of learning problems and datasets. In\nthe past, they have been tried on time-series forecasting as well, but their\noverall results have not been significantly better than the statistical models\nespecially for intermediate length times series data. Their modeling capacities\nare limited in cases where enough data may not be available to estimate the\nlarge number of parameters that these non-linear models require. This paper\npresents an easy to implement data augmentation method to significantly improve\nthe performance of such networks. Our method, Augmented-Neural-Network, which\ninvolves using forecasts from statistical models, can help unlock the power of\nneural networks on intermediate length time-series and produces competitive\nresults. It shows that data augmentation, when paired with Automated Machine\nLearning techniques such as Neural Architecture Search, can help to find the\nbest neural architecture for a given time-series. Using the combination of\nthese, demonstrates significant enhancement in the forecasting accuracy of\nthree neural network-based models for a COVID-19 dataset, with a maximum\nimprovement in forecasting accuracy by 21.41%, 24.29%, and 16.42%,\nrespectively, over the neural networks that do not use augmented data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:20:49 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 18:18:04 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 04:31:10 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Javeri", "Indrajeet Y.", ""], ["Toutiaee", "Mohammadhossein", ""], ["Arpinar", "Ismailcem B.", ""], ["Miller", "Tom W.", ""], ["Miller", "John A.", ""]]}, {"id": "2103.02035", "submitter": "Kevin Kunzmann", "authors": "Kevin Kunzmann and Camilla Lingjaerde and Sheila Bird and Sylvia\n  Richardson", "title": "The `how' matters: A simulation-based assessment of the potential\n  contributions of LFD tests for school reopening in England", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During Covid-19 outbreaks, school closures are employed as part of\ngovernments' non-pharmaceutical interventions around the world to reduce the\nnumber of contacts and keep the reproduction number below 1. Yet, prolonged\nschool closures have profound negative impact on the future opportunities of\npupils, particularly from disadvantaged backgrounds, as well as additional\neconomic and social impacts by preventing their parents from returning to work.\nData on Covid-19 in children are sparse and policy frameworks are evolving\nquickly. We compare a set of potential policies to accompany the reopening of\nschools by means of an agent-based simulation tool. The policies and scenarios\nwe model reflect the public discussion and government guidelines in early March\n2021 in England before the planned nationwide reopening of schools on the 8th\nof March. A point of particular interest is the potential contribution of a\nmore wide-spread use of screening tests based on lateral flow devices. We\ncompare policies both with respect to their potential to contain new outbreaks\nof Covid-19 in schools and the proportion of schooldays lost due to isolation\nof pupils. We find that regular asymptomatic screening of the whole school as\nan addition to a policy built around isolation of symptomatic pupils and their\nclosest contacts is beneficial across a wide range of scenarios, including when\nscreening tests with relatively low test sensitivity are used. Multiple\nscreening tests per week bring only small additional benefits in some\nscenarios. These findings remain valid when test compliance is not enforced\nalthough the effectiveness of outbreak control is reduced.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:29:45 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 18:18:50 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 17:15:35 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Kunzmann", "Kevin", ""], ["Lingjaerde", "Camilla", ""], ["Bird", "Sheila", ""], ["Richardson", "Sylvia", ""]]}, {"id": "2103.02144", "submitter": "Qingyang Xu", "authors": "Qingyang Xu, Qingsong Wen, Liang Sun", "title": "Two-Stage Framework for Seasonal Time Series Forecasting", "comments": "5 pages, 2 figures, 3 tables, ICASSP 2021", "journal-ref": "IEEE ICASSP 2021", "doi": "10.1109/ICASSP39728.2021.9414118.", "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seasonal time series Forecasting remains a challenging problem due to the\nlong-term dependency from seasonality. In this paper, we propose a two-stage\nframework to forecast univariate seasonal time series. The first stage\nexplicitly learns the long-range time series structure in a time window beyond\nthe forecast horizon. By incorporating the learned long-range structure, the\nsecond stage can enhance the prediction accuracy in the forecast horizon. In\nboth stages, we integrate the auto-regressive model with neural networks to\ncapture both linear and non-linear characteristics in time series. Our\nframework achieves state-of-the-art performance on M4 Competition Hourly\ndatasets. In particular, we show that incorporating the intermediate results\ngenerated in the first stage to existing forecast models can effectively\nenhance their prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:53:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xu", "Qingyang", ""], ["Wen", "Qingsong", ""], ["Sun", "Liang", ""]]}, {"id": "2103.02163", "submitter": "Zhaoxia Yu", "authors": "Tong Shen, Gyorgy Lur, Xiangmin Xu, Zhaoxia Yu", "title": "To Deconvolve, or Not to Deconvolve: Inferences of Neuronal Activities\n  using Calcium Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the increasing popularity of calcium imaging data in neuroscience\nresearch, methods for analyzing calcium trace data are critical to address\nvarious questions. The observed calcium traces are either analyzed directly or\ndeconvolved to spike trains to infer neuronal activities. When both approaches\nare applicable, it is unclear whether deconvolving calcium traces is a\nnecessary step. In this article, we compare the performance of using calcium\ntraces or their deconvolved spike trains for three common analyses: clustering,\nprincipal component analysis (PCA), and population decoding. Our simulations\nand applications to real data suggest that the estimated spike data outperform\ncalcium trace data for both clustering and PCA. Although calcium trace data\nshow higher predictability than spike data at each time point, spike history or\ncumulative spike counts is comparable to or better than calcium traces in\npopulation decoding.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:57:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Shen", "Tong", ""], ["Lur", "Gyorgy", ""], ["Xu", "Xiangmin", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.02336", "submitter": "Claus Weihs", "authors": "Claus Weihs and Sarah Buschfeld", "title": "Combining Prediction and Interpretation in Decision Trees (PrInDT) -- a\n  Linguistic Example", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that conditional inference trees and ensembles are\nsuitable methods for modeling linguistic variation. As against earlier\nlinguistic applications, however, we claim that their suitability is strongly\nincreased if we combine prediction and interpretation. To that end, we have\ndeveloped a statistical method, PrInDT (Prediction and Interpretation with\nDecision Trees), which we introduce and discuss in the present paper.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:32:20 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 17:37:51 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Weihs", "Claus", ""], ["Buschfeld", "Sarah", ""]]}, {"id": "2103.02701", "submitter": "Mauricio Herrera", "authors": "Mauricio Herrera", "title": "Exploring the roles of local mobility patterns, socioeconomic\n  conditions, and lockdown policies in shaping the patterns of COVID-19 spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 crisis has shown that we can only prevent the risk of mass\ncontagion through timely, large-scale, coordinated, and decisive actions.\nHowever, frequently the models used by experts [from whom decision-makers get\ntheir main advice] focus on a single perspective [for example, the\nepidemiological one] and do not consider many of the multiple forces that\naffect the COVID-19 outbreak patterns. The epidemiological, socioeconomic, and\nhuman mobility context of COVID-19 can be considered as a complex adaptive\nsystem. So, these interventions (for example, lock-downs) could have many\nand/or unexpected ramifications. This situation makes it difficult to\nunderstand the overall effect produced by any public policy measure and,\ntherefore, to assess its real effectiveness and convenience. By using mobile\nphone data, socioeconomic data, and COVID-19 cases data recorded throughout the\npandemic development, we aim to understand and explain [make sense of] the\nobserved heterogeneous regional patterns of contagion across time and space. We\nwill also consider the causal effects produced by confinement policies by\ndeveloping data-based models to explore, simulate, and estimate these policies'\neffectiveness. We intend to develop a methodology to assess and improve public\npolicies' effectiveness associated with the fight against the pandemic,\nemphasizing its convenience, the precise time of its application, and\nextension. The contributions of this work can be used regardless of the region.\nThe only likely impediment is the availability of the appropriate data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:44:05 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 13:04:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Herrera", "Mauricio", ""]]}, {"id": "2103.02783", "submitter": "Xuze Zhang", "authors": "Xuze Zhang and Benjamin Kedem", "title": "Financial Application of Extended Residual Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Residual coherence is a graphical tool for selecting potential second-order\ninteraction terms as functions of a single time series and its lags. This paper\nextends the notion of residual coherence to account for interaction terms of\nmultiple time series. Moreover, an alternative criterion, integrated spectrum,\nis proposed to facilitate this graphical selection.\n  A financial market application shows that new insights can be gained\nregarding implied market volatility.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 01:56:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Xuze", ""], ["Kedem", "Benjamin", ""]]}, {"id": "2103.02832", "submitter": "Michael Price", "authors": "Michael Price and Jun Yan", "title": "The Effects of the NBA COVID Bubble on the NBA Playoffs: A Case Study\n  for Home-Court Advantage", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2020 NBA playoffs were played inside of a bubble in Disney World because\nof the COVID-19 pandemic. This meant that there were no fans in attendance,\ngames played on neutral courts and no traveling for teams, which in theory\nremoves home-court advantage from the games. This setting has attracted much\ndiscussion as analysts and fans debated the possible effects it may have on the\noutcome of games. Home-court advantage has historically played an influential\nrole in NBA playoff series outcomes. The 2020 playoff provided a unique\nopportunity to study the effects of the bubble and home-court advantage by\ncomparing the 2020 season with the seasons in the past. While many factors\ncontribute to the outcome of games, points scored is the deciding factor of who\nwins games, so scoring is the primary focus of this study. The specific\nmeasures of interest are team scoring totals and team shooting percentage on\ntwo-pointers, three-pointers, and free throws. Comparing these measures for\nhome teams and away teams in 2020 vs. 2017-2019 shows that the 2020 playoffs\nfavored away teams more than usual, particularly with two point shooting and\ntotal scoring.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:47:29 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Price", "Michael", ""], ["Yan", "Jun", ""]]}, {"id": "2103.02941", "submitter": "Yanfei Kang", "authors": "Evangelos Theodorou, Shengjie Wang, Yanfei Kang, Evangelos Spiliotis,\n  Spyros Makridakis, Vassilios Assimakopoulos", "title": "Exploring the representativeness of the M5 competition data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of the M5 competition, which focused on forecasting the\nhierarchical unit sales of Walmart, was to evaluate the accuracy and\nuncertainty of forecasting methods in the field in order to identify best\npractices and highlight their practical implications. However, whether the\nfindings of the M5 competition can be generalized and exploited by retail firms\nto better support their decisions and operation depends on the extent to which\nthe M5 data is representative of the reality, i.e., sufficiently represent the\nunit sales data of retailers that operate in different regions, sell different\ntypes of products, and consider different marketing strategies. To answer this\nquestion, we analyze the characteristics of the M5 time series and compare them\nwith those of two grocery retailers, namely Corporaci\\'on Favorita and a major\nGreek supermarket chain, using feature spaces. Our results suggest that there\nare only small discrepancies between the examined data sets, supporting the\nrepresentativeness of the M5 data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:45:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Theodorou", "Evangelos", ""], ["Wang", "Shengjie", ""], ["Kang", "Yanfei", ""], ["Spiliotis", "Evangelos", ""], ["Makridakis", "Spyros", ""], ["Assimakopoulos", "Vassilios", ""]]}, {"id": "2103.03158", "submitter": "Jacob Reinhold", "authors": "Jacob C. Reinhold, Aaron Carass, Jerry L. Prince", "title": "A Structural Causal Model for MR Images of Multiple Sclerosis", "comments": "MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Precision medicine involves answering counterfactual questions such as \"Would\nthis patient respond better to treatment A or treatment B?\" These types of\nquestions are causal in nature and require the tools of causal inference to be\nanswered, e.g., with a structural causal model (SCM). In this work, we develop\nan SCM that models the interaction between demographic information, disease\ncovariates, and magnetic resonance (MR) images of the brain for people with\nmultiple sclerosis. Inference in the SCM generates counterfactual images that\nshow what an MR image of the brain would look like if demographic or disease\ncovariates are changed. These images can be used for modeling disease\nprogression or used for image processing tasks where controlling for\nconfounders is necessary.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 17:04:26 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 16:31:14 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 20:55:53 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Reinhold", "Jacob C.", ""], ["Carass", "Aaron", ""], ["Prince", "Jerry L.", ""]]}, {"id": "2103.03184", "submitter": "Morgane Pierre-Jean", "authors": "Morgane Pierre-Jean (CNRGH), Florence Mauger (CNRGH),\n  Jean-Fran\\c{c}ois Deleuze (CNRGH), Edith Le Floch", "title": "PIntMF: Penalized Integrative Matrix Factorization Method for\n  Multi-Omics Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is more and more common to explore the genome at diverse levels and not\nonly at a single omic level. Through integrative statistical methods, omics\ndata have the power to reveal new biological processes, potential biomarkers,\nand subgroups of a cohort. The matrix factorization (MF) is a unsupervised\nstatistical method that allows giving a clustering of individuals, but also\nrevealing relevant omic variables from the various blocks. Here, we present\nPIntMF (Penalized Integrative Matrix Factorization), a model of MF with\nsparsity, positivity and equality constraints.To induce sparsity in the model,\nwe use a classical Lasso penalization on variable and individual matrices. For\nthe matrix of samples, sparsity helps for the clustering, and normalization\n(matching an equality constraint) of inferred coefficients is added for a\nbetter interpretation. Besides, we add an automatic tuning of the sparsity\nparameters using the famous glmnet package. We also proposed three criteria to\nhelp the user to choose the number of latent variables. PIntMF was compared to\nother state-of-the-art integrative methods including feature selection\ntechniques in both synthetic and real data. PIntMF succeeds in finding relevant\nclusters as well as variables in two types of simulated data (correlated and\nuncorrelated). Then, PIntMF was applied to two real datasets (Diet and cancer),\nand it reveals interpretable clusters linked to available clinical data. Our\nmethod outperforms the existing ones on two criteria (clustering and variable\nselection). We show that PIntMF is an easy, fast, and powerful tool to extract\npatterns and cluster samples from multi-omics data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 09:35:35 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pierre-Jean", "Morgane", "", "CNRGH"], ["Mauger", "Florence", "", "CNRGH"], ["Deleuze", "Jean-Fran\u00e7ois", "", "CNRGH"], ["Floch", "Edith Le", ""]]}, {"id": "2103.03281", "submitter": "Romeo Kienzler", "authors": "Romeo Kienzler and Ivan Nesic", "title": "CLAIMED, a visual and scalable component library for Trusted AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep Learning models are getting more and more popular but constraints on\nexplainability, adversarial robustness and fairness are often major concerns\nfor production deployment. Although the open source ecosystem is abundant on\naddressing those concerns, fully integrated, end to end systems are lacking in\nopen source. Therefore we provide an entirely open source, reusable component\nframework, visual editor and execution engine for production grade machine\nlearning on top of Kubernetes, a joint effort between IBM and the University\nHospital Basel. It uses Kubeflow Pipelines, the AI Explainability360 toolkit,\nthe AI Fairness360 toolkit and the Adversarial Robustness Toolkit on top of\nElyraAI, Kubeflow, Kubernetes and JupyterLab. Using the Elyra pipeline editor,\nAI pipelines can be developed visually with a set of jupyter notebooks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:31:19 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kienzler", "Romeo", ""], ["Nesic", "Ivan", ""]]}, {"id": "2103.03286", "submitter": "Javier Carcamo", "authors": "Amparo Ba\\'illo, Javier C\\'arcamo and Carlos Mora-Corral", "title": "Extremal points of Lorenz curves and applications to inequality analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find the set of extremal points of Lorenz curves with fixed Gini index and\ncompute the maximal $L^1$-distance between Lorenz curves with given values of\ntheir Gini coefficients. As an application we introduce a bidimensional index\nthat simultaneously measures relative inequality and dissimilarity between two\npopulations. This proposal employs the Gini indices of the variables and an\n$L^1$-distance between their Lorenz curves. The index takes values in a\nright-angled triangle, two of whose sides characterize perfect relative\ninequality-expressed by the Lorenz ordering between the underlying\ndistributions. Further, the hypotenuse represents maximal distance between the\ntwo distributions. As a consequence, we construct a chart to, graphically,\neither see the evolution of (relative) inequality and distance between two\nincome distributions over time or to compare the distribution of income of a\nspecific population between a fixed time point and a range of years. We prove\nthe mathematical results behind the above claims and provide a full description\nof the asymptotic properties of the plug-in estimator of this index. Finally,\nwe apply the proposed bidimensional index to several real EU-SILC income\ndatasets to illustrate its performance in practice.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:44:08 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ba\u00edllo", "Amparo", ""], ["C\u00e1rcamo", "Javier", ""], ["Mora-Corral", "Carlos", ""]]}, {"id": "2103.03305", "submitter": "Kevin Xu", "authors": "Mohammadreza Nemati, Haonan Zhang, Michael Sloma, Dulat Bekbolsynov,\n  Hong Wang, Stanislaw Stepkowski, and Kevin S. Xu", "title": "Predicting Kidney Transplant Survival using Multiple Feature\n  Representations for HLAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney transplantation can significantly enhance living standards for people\nsuffering from end-stage renal disease. A significant factor that affects graft\nsurvival time (the time until the transplant fails and the patient requires\nanother transplant) for kidney transplantation is the compatibility of the\nHuman Leukocyte Antigens (HLAs) between the donor and recipient. In this paper,\nwe propose new biologically-relevant feature representations for incorporating\nHLA information into machine learning-based survival analysis algorithms. We\nevaluate our proposed HLA feature representations on a database of over 100,000\ntransplants and find that they improve prediction accuracy by about 1%, modest\nat the patient level but potentially significant at a societal level. Accurate\nprediction of survival times can improve transplant survival outcomes, enabling\nbetter allocation of donors to recipients and reducing the number of\nre-transplants due to graft failure with poorly matched donors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:22:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nemati", "Mohammadreza", ""], ["Zhang", "Haonan", ""], ["Sloma", "Michael", ""], ["Bekbolsynov", "Dulat", ""], ["Wang", "Hong", ""], ["Stepkowski", "Stanislaw", ""], ["Xu", "Kevin S.", ""]]}, {"id": "2103.03348", "submitter": "Andrew Holbrook", "authors": "Andrew J. Holbrook, Xiang Ji, Marc A. Suchard", "title": "From viral evolution to spatial contagion: a biologically modulated\n  Hawkes model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutations sometimes increase contagiousness for evolving pathogens. During an\nepidemic, epidemiologists use viral genetics to infer a shared evolutionary\nhistory and connect this history to geographic spread. We propose a model that\ndirectly relates a pathogen's evolution to its spatial contagion dynamics --\neffectively combining the two epidemiological paradigms of phylogenetic\ninference and self-exciting process modeling -- and apply this\n\\emph{phylogenetic Hawkes process} to a Bayesian analysis of 23,422 viral cases\nfrom the 2014-2016 Ebola outbreak in West Africa. With a mere 1,610 samples\nproviding RNA data, our model is able to detect subsets of the Ebola virus with\nsignificantly elevated rates of spatiotemporal propagation.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:36:55 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 18:54:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Holbrook", "Andrew J.", ""], ["Ji", "Xiang", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2103.03462", "submitter": "Nicholas Kissel", "authors": "Nicholas Kissel, Lucas Mentch", "title": "Forward Stability and Model Path Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most scientific publications follow the familiar recipe of (i) obtain data,\n(ii) fit a model, and (iii) comment on the scientific relevance of the effects\nof particular covariates in that model. This approach, however, ignores the\nfact that there may exist a multitude of similarly-accurate models in which the\nimplied effects of individual covariates may be vastly different. This problem\nof finding an entire collection of plausible models has also received\nrelatively little attention in the statistics community, with nearly all of the\nproposed methodologies being narrowly tailored to a particular model class\nand/or requiring an exhaustive search over all possible models, making them\nlargely infeasible in the current big data era. This work develops the idea of\nforward stability and proposes a novel, computationally-efficient approach to\nfinding collections of accurate models we refer to as model path selection\n(MPS). MPS builds up a plausible model collection via a forward selection\napproach and is entirely agnostic to the model class and loss function\nemployed. The resulting model collection can be displayed in a simple and\nintuitive graphical fashion, easily allowing practitioners to visualize whether\nsome covariates can be swapped for others with minimal loss.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 04:01:45 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kissel", "Nicholas", ""], ["Mentch", "Lucas", ""]]}, {"id": "2103.03478", "submitter": "Meng Li", "authors": "Rongjie Liu, Meng Li, David B. Dunson", "title": "PPA: Principal Parcellation Analysis for Brain Connectomes and Multiple\n  Traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Our understanding of the structure of the brain and its relationships with\nhuman traits is largely determined by how we represent the structural\nconnectome. Standard practice divides the brain into regions of interest (ROIs)\nand represents the connectome as an adjacency matrix having cells measuring\nconnectivity between pairs of ROIs. Statistical analyses are then heavily\ndriven by the (largely arbitrary) choice of ROIs. In this article, we propose a\nnovel tractography-based representation of brain connectomes, which clusters\nfiber endpoints to define a data adaptive parcellation targeted to explain\nvariation among individuals and predict human traits. This representation leads\nto Principal Parcellation Analysis (PPA), representing individual brain\nconnectomes by compositional vectors building on a basis system of fiber\nbundles that captures the connectivity at the population level. PPA reduces\nsubjectivity and facilitates statistical analyses. We illustrate the proposed\napproach through applications to data from the Human Connectome Project (HCP)\nand show that PPA connectomes improve power in predicting human traits over\nstate-of-the-art methods based on classical connectomes, while dramatically\nimproving parsimony and maintaining interpretability. Our PPA package is\npublicly available on GitHub, and can be implemented routinely for diffusion\ntensor image data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:26:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Liu", "Rongjie", ""], ["Li", "Meng", ""], ["Dunson", "David B.", ""]]}, {"id": "2103.03538", "submitter": "Edgar Santos-Fernandez", "authors": "Edgar Santos-Fernandez, Jay M. Ver Hoef, Erin E. Peterson, James\n  McGree, Daniel Isaak, Kerrie Mengersen", "title": "Bayesian spatio-temporal models for stream networks", "comments": "26 pages, 10 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal models are widely used in many research areas including\necology. The recent proliferation of the use of in-situ sensors in streams and\nrivers supports space-time water quality modelling and monitoring in near\nreal-time. In this paper, we introduce a new family of dynamic spatio-temporal\nmodels, in which spatial dependence is established based on stream distance and\ntemporal autocorrelation is incorporated using vector autoregression\napproaches. We propose several variations of these novel models using a\nBayesian framework. Our results show that our proposed models perform well\nusing spatio-temporal data collected from real stream networks, particularly in\nterms of out-of-sample RMSPE. This is illustrated considering a case study of\nwater temperature data in the northwestern United States.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:33:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Santos-Fernandez", "Edgar", ""], ["Hoef", "Jay M. Ver", ""], ["Peterson", "Erin E.", ""], ["McGree", "James", ""], ["Isaak", "Daniel", ""], ["Mengersen", "Kerrie", ""]]}, {"id": "2103.03632", "submitter": "Michael Pfarrhofer", "authors": "Michael Pfarrhofer", "title": "Tail forecasts of inflation using time-varying parameter quantile\n  regressions", "comments": "JEL: C11, C22, C53, E31; Keywords: state space models, stochastic\n  volatility, unobserved component model, density forecasts, tail risk", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes methods for Bayesian inference in time-varying parameter\n(TVP) quantile regression (QR) models. We use data augmentation schemes to\nfacilitate the conditional likelihood, and render the model conditionally\nGaussian to develop an efficient Gibbs sampling algorithm. Regularization of\nthe high-dimensional parameter space is achieved via flexible dynamic shrinkage\npriors. A simple version of the TVP-QR based on an unobserved component (UC)\nmodel is applied to dynamically trace the quantiles of the distribution of\ninflation in the United States (US), the United Kingdom (UK) and the euro area\n(EA). We conduct an out-of-sample inflation forecasting exercise to assess\npredictive accuracy of the proposed framework versus several benchmarks using\nmetrics to capture performance in different parts of the distribution. The\nproposed model is competitive and performs particularly well for higher-order\nand tail forecasts. We analyze the resulting predictive distributions and find\nthat they are often skewed and feature heavier than normal tails.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:29:34 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pfarrhofer", "Michael", ""]]}, {"id": "2103.03706", "submitter": "Yair Daon", "authors": "Yair Daon, Amit Huppert, Uri Obolski", "title": "DOPE: D-Optimal Pooling Experimental design with application for\n  SARS-CoV-2 screening", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Testing individuals for the presence of severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2), the pathogen causing the coronavirus disease 2019\n(COVID-19), is crucial for curtailing transmission chains. Moreover, rapidly\ntesting many potentially infected individuals is often a limiting factor in\ncontrolling COVID-19 outbreaks. Hence, pooling strategies, wherein individuals\nare grouped and tested simultaneously, are employed. We present a novel pooling\nstrategy that implements D-Optimal Pooling Experimental design (DOPE). DOPE\ndefines optimal pooled tests as those maximizing the mutual information between\ndata and infection states. We estimate said mutual information via Monte-Carlo\nsampling and employ a discrete optimization heuristic for maximizing it. DOPE\noutperforms common pooling strategies both in terms of lower error rates and\nfewer tests utilized. DOPE holds several additional advantages: it provides\nposterior distributions of the probability of infection, rather than only\nbinary classification outcomes; it naturally incorporates prior information of\ninfection probabilities and test error rates; and finally, it can be easily\nextended to include other, newly discovered information regarding COVID-19.\nHence, we believe that implementation of Bayesian D-optimal experimental design\nholds a great promise for the efforts of combating COVID-19 and other future\npandemics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:31:05 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Daon", "Yair", ""], ["Huppert", "Amit", ""], ["Obolski", "Uri", ""]]}, {"id": "2103.03818", "submitter": "Zhaoxia Yu", "authors": "Tong Shen, Kevin Johnston, Gyorgy Lur, Michele Guindani, Hernando\n  Ombao, Zhaoxia Yu", "title": "Time-varying $\\ell_0$ optimization for Spike Inference from Multi-Trial\n  Calcium Recordings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Optical imaging of genetically encoded calcium indicators is a powerful tool\nto record the activity of a large number of neurons simultaneously over a long\nperiod of time from freely behaving animals. However, determining the exact\ntime at which a neuron spikes and estimating the underlying firing rate from\ncalcium fluorescence data remains challenging, especially for calcium imaging\ndata obtained from a longitudinal study. We propose a multi-trial time-varying\n$\\ell_0$ penalized method to jointly detect spikes and estimate firing rates by\nrobustly integrating evolving neural dynamics across trials. Our simulation\nstudy shows that the proposed method performs well in both spike detection and\nfiring rate estimation. We demonstrate the usefulness of our method on calcium\nfluorescence trace data from two studies, with the first study showing\ndifferential firing rate functions between two behaviors and the second study\nshowing evolving firing rate function across trials due to learning.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:05:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Shen", "Tong", ""], ["Johnston", "Kevin", ""], ["Lur", "Gyorgy", ""], ["Guindani", "Michele", ""], ["Ombao", "Hernando", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.03833", "submitter": "Harrison Quick", "authors": "Harrison Quick", "title": "Improving the Utility of Poisson-Distributed, Differentially Private\n  Synthetic Data via Prior Predictive Truncation with an Application to CDC\n  WONDER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CDC WONDER is a web-based tool for the dissemination of epidemiologic data\ncollected by the National Vital Statistics System. While CDC WONDER has\nbuilt-in privacy protections, they do not satisfy formal privacy protections\nsuch as differential privacy and thus are susceptible to targeted attacks.\nGiven the importance of making high-quality public health data publicly\navailable while preserving the privacy of the underlying data subjects, we aim\nto improve the utility of a recently developed approach for generating\nPoisson-distributed, differentially private synthetic data by using publicly\navailable information to truncate the range of the synthetic data.\nSpecifically, we utilize county-level population information from the U.S.\nCensus Bureau and national death reports produced by the CDC to inform prior\ndistributions on county-level death rates and infer reasonable ranges for\nPoisson-distributed, county-level death counts. In doing so, the requirements\nfor satisfying differential privacy for a given privacy budget can be reduced\nby several orders of magnitude, thereby leading to substantial improvements in\nutility. To illustrate our proposed approach, we consider a dataset comprised\nof over 26,000 cancer-related deaths from the Commonwealth of Pennsylvania\nbelonging to over 47,000 combinations of cause-of-death and demographic\nvariables such as age, race, sex, and county-of-residence and demonstrate the\nproposed framework's ability to preserve features such as geographic,\nurban/rural, and racial disparities present in the true data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:09:42 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Quick", "Harrison", ""]]}, {"id": "2103.03834", "submitter": "Till Koebe", "authors": "Till Koebe, Alejandra Arias-Salazar, Natalia Rojas-Perilla, Timo\n  Schmid", "title": "Intercensal updating using structure-preserving methods and satellite\n  imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Censuses are fundamental building blocks of most modern-day societies, yet\ncollected every ten years at best. We propose an extension of the widely\npopular census updating technique Structure Preserving Estimation by\nincorporating auxiliary information in order to take ongoing subnational\npopulation shifts into account. We apply our method by incorporating satellite\nimagery as additional source to derive annual small-area updates of\nmultidimensional poverty indicators from 2013 to 2020 for a population at risk:\nfemale-headed households in Senegal. We evaluate the performance of our\nproposal using data from two different census periods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:45:31 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Koebe", "Till", ""], ["Arias-Salazar", "Alejandra", ""], ["Rojas-Perilla", "Natalia", ""], ["Schmid", "Timo", ""]]}, {"id": "2103.03984", "submitter": "Ginno Mill\\'an Naveas", "authors": "G. Mill\\'an", "title": "Traffic Flows Analysis in High-Speed Computer Networks Using Time Series", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.02091", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article explores the required amount of time series points from a\nhigh-speed traffic network to accurately estimate the Hurst exponent. The\nmethodology consists in designing an experiment using estimators that are\napplied to time series, followed by addressing the minimum amount of points\nrequired to obtain accurate estimates of the Hurst exponent in real-time. The\nmethodology addresses the exhaustive analysis of the Hurst exponent considering\nbias behavior, standard deviation, mean square error, and convergence using\nfractional gaussian noise signals with stationary increases. Our results show\nthat the Whittle estimator successfully estimates the Hurst exponent in series\nwith few points. Based on the results obtained, a minimum length for the time\nseries is empirically proposed. Finally, to validate the results, the\nmethodology is applied to real traffic captures in a high-speed network based\non the IEEE 802.3ab standard.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 23:51:36 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mill\u00e1n", "G.", ""]]}, {"id": "2103.04139", "submitter": "Ashwini Venkatasubramaniam", "authors": "Ashwini Venkatasubramaniam and Julian Wolfson", "title": "visTree: Visualization of Subgroups for a Decision Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Decision trees are flexible prediction models which are constructed to\nquantify outcome-covariate relationships and characterize relevant population\nsubgroups. However, the standard graphical representation of fitted decision\ntrees highlights individual split points, and hence is suboptimal for\nvisualizing defined subgroups. In this paper, we present a novel visual\nrepresentation of decision trees which shifts the primary focus to\ncharacterizing subgroups, both in terms of their defining covariates and their\noutcome distribution. We implement our method in the \\texttt{visTree} package,\nwhich builds on the toolkit and infrastructure provided by the\n\\texttt{partykit} package and enables the visualization to be applied to varied\ndecision trees. Individual functions are demonstrated using data from the Box\nLunch study [French et al., 2014], a randomized trial to evaluate the effect of\nexposure to different lunch sizes on energy intake and body weight among\nworking adults.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:23:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Venkatasubramaniam", "Ashwini", ""], ["Wolfson", "Julian", ""]]}, {"id": "2103.04158", "submitter": "Florent Leclercq", "authors": "Florent Leclercq, Alan Heavens", "title": "On the accuracy and precision of correlation functions and field-level\n  inference in cosmology", "comments": "6 pages, 4 figures. Our code and data are publicly available at\n  https://github.com/florent-leclercq/correlations_vs_field", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparative study of the accuracy and precision of correlation\nfunction methods and full-field inference in cosmological data analysis. To do\nso, we examine a Bayesian hierarchical model that predicts log-normal fields\nand their two-point correlation function. Although a simplified analytic model,\nthe log-normal model produces fields that share many of the essential\ncharacteristics of the present-day non-Gaussian cosmological density fields. We\nuse three different statistical techniques: (i) a standard likelihood-based\nanalysis of the two-point correlation function; (ii) a likelihood-free\n(simulation-based) analysis of the two-point correlation function; (iii) a\nfield-level analysis, made possible by the more sophisticated data assimilation\ntechnique. We find that (a) standard assumptions made to write down a\nlikelihood for correlation functions can cause significant biases, a problem\nthat is alleviated with simulation-based inference; and (b) analysing the\nentire field offers considerable advantages over correlation functions, through\nhigher accuracy, higher precision, or both. The gains depend on the degree of\nnon-Gaussianity, but in all cases, including for weak non-Gaussianity, the\nadvantage of analysing the full field is substantial.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 17:03:21 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Leclercq", "Florent", ""], ["Heavens", "Alan", ""]]}, {"id": "2103.04198", "submitter": "Pratheepa Jeganathan", "authors": "Pratheepa Jeganathan and Susan P. Holmes", "title": "A Statistical Perspective on the Challenges in Molecular Microbial\n  Biology", "comments": "To appear in the Journal of Agricultural, Biological and\n  Environmental Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput sequencing (HTS)-based technology enables identifying and\nquantifying non-culturable microbial organisms in all environments. Microbial\nsequences have enhanced our understanding of the human microbiome, the soil and\nplant environment, and the marine environment. All molecular microbial data\npose statistical challenges due to contamination sequences from reagents, batch\neffects, unequal sampling, and undetected taxa. Technical biases and\nheteroscedasticity have the strongest effects, but different strains across\nsubjects and environments also make direct differential abundance testing\nunwieldy. We provide an introduction to a few statistical tools that can\novercome some of these difficulties and demonstrate those tools on an example.\nWe show how standard statistical methods, such as simple hierarchical mixture\nand topic models, can facilitate inferences on latent microbial communities. We\nalso review some nonparametric Bayesian approaches that combine visualization\nand uncertainty quantification. The intersection of molecular microbial biology\nand statistics is an exciting new venue. Finally, we list some of the important\nopen problems that would benefit from more careful statistical method\ndevelopment.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 21:14:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Jeganathan", "Pratheepa", ""], ["Holmes", "Susan P.", ""]]}, {"id": "2103.04462", "submitter": "Mauro Gasparini", "authors": "Mauro Gasparini", "title": "Improving Bayesian estimation of Vaccine Efficacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A full Bayesian approach to the estimation of Vaccine Efficacy is presented,\nwhich is an improvement over the currently used exact method conditional on the\ntotal number of cases. As an example, we reconsider the statistical sections of\nthe BioNTech/Pfizer protocol, which in 2020 has led to the first approved\nanti-Covid-19 vaccine.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 21:35:16 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gasparini", "Mauro", ""]]}, {"id": "2103.04472", "submitter": "Larry Wasserman", "authors": "Matteo Bonvini and Edward Kennedy and Valerie Ventura and Larry\n  Wasserman", "title": "Causal Inference in the Time of Covid-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we develop statistical methods for causal inference in\nepidemics. Our focus is in estimating the effect of social mobility on deaths\nin the Covid-19 pandemic. We propose a marginal structural model motivated by a\nmodified version of a basic epidemic model. We estimate the counterfactual time\nseries of deaths under interventions on mobility. We conduct several types of\nsensitivity analyses. We find that the data support the idea that reduced\nmobility causes reduced deaths, but the conclusion comes with caveats. There is\nevidence of sensitivity to model misspecification and unmeasured confounding\nwhich implies that the size of the causal effect needs to be interpreted with\ncaution. While there is little doubt the the effect is real, our work\nhighlights the challenges in drawing causal inferences from pandemic data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 22:35:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bonvini", "Matteo", ""], ["Kennedy", "Edward", ""], ["Ventura", "Valerie", ""], ["Wasserman", "Larry", ""]]}, {"id": "2103.04647", "submitter": "Santhosh Narayanan Dr.", "authors": "Santhosh Narayanan, Ioannis Kosmidis, Petros Dellaportas", "title": "Flexible marked spatio-temporal point processes with applications to\n  event sequences from association football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new family of marked point processes by focusing the\ncharacteristic properties of marked Hawkes processes exclusively to the space\nof marks, providing the freedom to specify a different model for the occurrence\ntimes. This is possible through a decomposition of the joint distribution of\nmarks and times that allows to separately specify the conditional distribution\nof marks given the filtration of the process and the current time. We develop a\nBayesian framework for the inference and prediction from this family of marked\npoint processes that can naturally accommodate process and point-specific\ncovariate information to drive cross-excitations, offering wide flexibility and\napplicability in the modelling of real-world processes. The framework is used\nhere for the modelling of in-game event sequences from association football,\nresulting not only in inferences about previously unquantified characteristics\nof the game dynamics and extraction of event-specific team abilities, but also\nin predictions for the occurrence of events of interest, such as goals, corners\nor fouls, in a specified interval of time.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 10:15:29 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Narayanan", "Santhosh", ""], ["Kosmidis", "Ioannis", ""], ["Dellaportas", "Petros", ""]]}, {"id": "2103.04697", "submitter": "Amelia Sim\\'o", "authors": "Mar\\'ia Victoria Ib\\'a\\~nez, Marina Mart\\'inez-Garcia and Amelia\n  Sim\\'o", "title": "A Review of Spatiotemporal Models for Count Data in R Packages. A Case\n  Study of COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spatio-temporal models for count data are required in a wide range of\nscientific fields and they have become particularly crucial nowadays because of\ntheir ability to analyse COVID-19-related data. Models for count data are\nneeded when the variable of interest take only non-negative integer values and\nthese integers arise from counting occurrences. Several R-packages are\ncurrently available to deal with spatiotemporal areal count data. Each package\nfocuses on different models and/or statistical methodologies. Unfortunately,\nthe results generated by these models are rarely comparable due to differences\nin notation and methods. The main objective of this paper is to present a\nreview describing the most important approaches that can be used to model and\nanalyse count data when questions of scientific interest concern both their\nspatial and their temporal behaviour and we monitor their performance under the\nsame data set.\n  For this review, we focus on the three R-packages that can be used for this\npurpose and the different models assessed are representative of the two most\nwidespread methodologies used to analyse spatiotemporal count data: the\nclassical approach (based on Penalised Likelihood or Estimating Equations) and\nthe Bayesian point of view.\n  A case study is analysed as an illustration of these different methodologies.\nIn this case study, these packages are used to model and predict daily\nhospitalisations from COVID-19 in 24 health regions within the Valencian\nCommunity (Spain), with data corresponding to the period from 28 June to 13\nDecember 2020. Because of the current urgent need for monitoring and predicting\ndata in the COVID-19 pandemic, this case study is, in itself, of particular\nimportance and can be considered the secondary objective of this work.\nSatisfactory and promising results have been obtained in this second goal.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:16:57 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 07:59:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ib\u00e1\u00f1ez", "Mar\u00eda Victoria", ""], ["Mart\u00ednez-Garcia", "Marina", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "2103.04721", "submitter": "Matthias Troffaes", "authors": "Ullrika Sahlin and Matthias C. M. Troffaes and Lennart Edsman", "title": "Robust decision analysis under severe uncertainty and ambiguous\n  tradeoffs: an invasive species case study", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian decision analysis is a useful method for risk management decisions,\nbut is limited in its ability to consider severe uncertainty in knowledge, and\nvalue ambiguity in management objectives. We study the use of robust Bayesian\ndecision analysis to handle problems where one or both of these issues arise.\nThe robust Bayesian approach models severe uncertainty through bounds on\nprobability distributions, and value ambiguity through bounds on utility\nfunctions. To incorporate data, standard Bayesian updating is applied on the\nentire set of distributions. To elicit our expert's utility representing the\nvalue of different management objectives, we use a modified version of the\nswing weighting procedure that can cope with severe value ambiguity. We\ndemonstrate these methods on an environmental management problem to eradicate\nan alien invasive marmorkrebs recently discovered in Sweden, which needed a\nrapid response despite substantial knowledge gaps if the species was still\npresent (i.e. severe uncertainty) and the need for difficult tradeoffs and\ncompeting interests (i.e. value ambiguity). We identify that the decision\nalternatives to drain the system and remove individuals in combination with\ndredging and sieving with or without a degradable biocide, or increasing pH,\nare consistently bad under the entire range of probability and utility bounds.\nThis case study shows how robust Bayesian decision analysis provides a\ntransparent methodology for integrating information in risk management problems\nwhere little data are available and/or where the tradeoffs ambiguous.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:55:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Sahlin", "Ullrika", ""], ["Troffaes", "Matthias C. M.", ""], ["Edsman", "Lennart", ""]]}, {"id": "2103.04867", "submitter": "Peter Kirwan", "authors": "Peter D. Kirwan, Suzanne Elgohari, Christopher H. Jackson, Brian D. M.\n  Tom, Sema Mandal, Daniela De Angelis, Anne M. Presanis", "title": "Trends in risks of severe events and lengths of stay for COVID-19\n  hospitalisations in England over the pre-vaccination era: results from the\n  Public Health England SARI-Watch surveillance scheme", "comments": "45 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Trends in hospitalised case-fatality risk (HFR), risk of\nintensive care unit (ICU) admission and lengths of stay for patients\nhospitalised for COVID-19 in England over the pre-vaccination era are unknown.\n  Methods: Data on hospital and ICU admissions with COVID-19 at 31 NHS trusts\nin England were collected by Public Health England's Severe Acute Respiratory\nInfections surveillance system and linked to death information. We applied\nparametric multi-state mixture models, accounting for censored outcomes and\nregressing risks and times between events on month of admission, geography, and\nbaseline characteristics.\n  Findings: 20,785 adults were admitted with COVID-19 in 2020. Between March\nand June/July/August estimated HFR reduced from 31.9% (95% confidence interval\n30.3-33.5%) to 10.9% (9.4-12.7%), then rose steadily from 21.6% (18.4-25.5%) in\nSeptember to 25.7% (23.0-29.2%) in December, with steeper increases among older\npatients, those with multi-morbidity and outside London/South of England. ICU\nadmission risk reduced from 13.9% (12.8-15.2%) in March to 6.2% (5.3-7.1%) in\nMay, rising to a high of 14.2% (11.1-17.2%) in September. Median length of stay\nin non-critical care increased during 2020, from 6.6 to 12.3 days for those\ndying, and from 6.1 to 9.3 days for those discharged.\n  Interpretation: Initial improvements in patient outcomes, corresponding to\ndevelopments in clinical practice, were not sustained throughout 2020, with HFR\nin December approaching the levels seen at the start of the pandemic, whilst\nmedian hospital stays have lengthened. The role of increased transmission, new\nvariants, case-mix and hospital pressures in increasing COVID-19 severity\nrequires urgent further investigation.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:19:21 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 13:49:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kirwan", "Peter D.", ""], ["Elgohari", "Suzanne", ""], ["Jackson", "Christopher H.", ""], ["Tom", "Brian D. M.", ""], ["Mandal", "Sema", ""], ["De Angelis", "Daniela", ""], ["Presanis", "Anne M.", ""]]}, {"id": "2103.04907", "submitter": "Fan Li", "authors": "Fan Li, Ashley L. Buchanan, Stephen R. Cole", "title": "Generalizing trial evidence to target populations in non-nested designs:\n  Applications to AIDS clinical trials", "comments": "43 pages, 3 tables and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Comparative effectiveness evidence from randomized trials may not be directly\ngeneralizable to a target population of substantive interest when, as in most\ncases, trial participants are not randomly sampled from the target population.\nMotivated by the need to generalize evidence from two trials conducted in the\nAIDS Clinical Trials Group (ACTG), we consider weighting, regression and doubly\nrobust estimators to estimate the causal effects of HIV interventions in a\nspecified population of people living with HIV in the USA. We focus on a\nnon-nested trial design and discuss strategies for both point and variance\nestimation of the target population average treatment effect. Specifically in\nthe generalizability context, we demonstrate both analytically and empirically\nthat estimating the known propensity score in trials does not increase the\nvariance for each of the weighting, regression and doubly robust estimators. We\napply these methods to generalize the average treatment effects from two ACTG\ntrials to specified target populations and operationalize key practical\nconsiderations. Finally, we report on a simulation study that investigates the\nfinite-sample operating characteristics of the generalizability estimators and\ntheir sandwich variance estimators.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:10:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Fan", ""], ["Buchanan", "Ashley L.", ""], ["Cole", "Stephen R.", ""]]}, {"id": "2103.04944", "submitter": "Michael Pfarrhofer", "authors": "Martin Feldkircher, Florian Huber, Gary Koop, Michael Pfarrhofer", "title": "Approximate Bayesian inference and forecasting in huge-dimensional\n  multi-country VARs", "comments": "JEL: C11, C30, E3, D31; Keywords: Multi-country models, macroeconomic\n  forecasting, vector autoregression", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Panel Vector Autoregressive (PVAR) model is a popular tool for\nmacroeconomic forecasting and structural analysis in multi-country applications\nsince it allows for spillovers between countries in a very flexible fashion.\nHowever, this flexibility means that the number of parameters to be estimated\ncan be enormous leading to over-parameterization concerns. Bayesian\nglobal-local shrinkage priors, such as the Horseshoe prior used in this paper,\ncan overcome these concerns, but they require the use of Markov Chain Monte\nCarlo (MCMC) methods rendering them computationally infeasible in high\ndimensions. In this paper, we develop computationally efficient Bayesian\nmethods for estimating PVARs using an integrated rotated Gaussian approximation\n(IRGA). This exploits the fact that whereas own country information is often\nimportant in PVARs, information on other countries is often unimportant. Using\nan IRGA, we split the the posterior into two parts: one involving own country\ncoefficients, the other involving other country coefficients. Fast methods such\nas approximate message passing or variational Bayes can be used on the latter\nand, conditional on these, the former are estimated with precision using MCMC\nmethods. In a forecasting exercise involving PVARs with up to $18$ variables\nfor each of $38$ countries, we demonstrate that our methods produce good\nforecasts quickly.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:02:59 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Feldkircher", "Martin", ""], ["Huber", "Florian", ""], ["Koop", "Gary", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "2103.05075", "submitter": "Alex Williams", "authors": "Alex H. Williams and Scott W. Linderman", "title": "Statistical Neuroscience in the Single Trial Limit", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Individual neurons often produce highly variable responses over nominally\nidentical trials, reflecting a mixture of intrinsic \"noise\" and systematic\nchanges in the animal's cognitive and behavioral state. In addition to\ninvestigating how noise and state changes impact neural computation,\nstatistical models of trial-to-trial variability are becoming increasingly\nimportant as experimentalists aspire to study naturalistic animal behaviors,\nwhich never repeat themselves exactly and may rarely do so even approximately.\nEstimating the basic features of neural response distributions may seem\nimpossible in this trial-limited regime. Fortunately, by identifying and\nleveraging simplifying structure in neural data -- e.g. shared gain modulations\nacross neural subpopulations, temporal smoothness in neural firing rates, and\ncorrelations in responses across behavioral conditions -- statistical\nestimation often remains tractable in practice. We review recent advances in\nstatistical neuroscience that illustrate this trend and have enabled novel\ninsights into the trial-by-trial operation of neural circuits.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:08:14 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Williams", "Alex H.", ""], ["Linderman", "Scott W.", ""]]}, {"id": "2103.05122", "submitter": "Sanket Jantre", "authors": "Sanket R. Jantre and Zichao Wendy Di", "title": "Low-Rank Tensor Regression for X-Ray Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tomographic imaging is useful for revealing the internal structure of a 3D\nsample. Classical reconstruction methods treat the object of interest as a\nvector to estimate its value. Such an approach, however, can be inefficient in\nanalyzing high-dimensional data because of the underexploration of the\nunderlying structure. In this work, we propose to apply a tensor-based\nregression model to perform tomographic reconstruction. Furthermore, we explore\nthe low-rank structure embedded in the corresponding tensor form. As a result,\nour proposed method efficiently reduces the dimensionality of the unknown\nparameters, which is particularly beneficial for ill-posed inverse problem\nsuffering from insufficient data. We demonstrate the robustness of our proposed\napproach on synthetic noise-free data as well as on Gaussian noise-added data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:20:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jantre", "Sanket R.", ""], ["Di", "Zichao Wendy", ""]]}, {"id": "2103.05220", "submitter": "Mingyuan Meng", "authors": "Bingxin Gu, Mingyuan Meng, Lei Bi, Jinman Kim, David Dagan Feng, and\n  Shaoli Song", "title": "Prediction of 5-year Progression-Free Survival in Advanced\n  Nasopharyngeal Carcinoma with Pretreatment PET/CT using Multi-Modality Deep\n  Learning-based Radiomics", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.CV cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning-based Radiomics (DLR) has achieved great success on medical\nimage analysis. In this study, we aim to explore the capability of DLR for\nsurvival prediction in NPC. We developed an end-to-end multi-modality DLR model\nusing pretreatment PET/CT images to predict 5-year Progression-Free Survival\n(PFS) in advanced NPC. A total of 170 patients with pathological confirmed\nadvanced NPC (TNM stage III or IVa) were enrolled in this study. A 3D\nConvolutional Neural Network (CNN), with two branches to process PET and CT\nseparately, was optimized to extract deep features from pretreatment\nmulti-modality PET/CT images and use the derived features to predict the\nprobability of 5-year PFS. Optionally, TNM stage, as a high-level clinical\nfeature, can be integrated into our DLR model to further improve prognostic\nperformance. For a comparison between CR and DLR, 1456 handcrafted features\nwere extracted, and three top CR methods were selected as benchmarks from 54\ncombinations of 6 feature selection methods and 9 classification methods.\nCompared to the three CR methods, our multi-modality DLR models using both PET\nand CT, with or without TNM stage (named PCT or PC model), resulted in the\nhighest prognostic performance. Furthermore, the multi-modality PCT model\noutperformed single-modality DLR models using only PET and TNM stage (PT model)\nor only CT and TNM stage (CT model). Our study identified potential\nradiomics-based prognostic model for survival prediction in advanced NPC, and\nsuggests that DLR could serve as a tool for aiding in cancer management.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:43:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Gu", "Bingxin", ""], ["Meng", "Mingyuan", ""], ["Bi", "Lei", ""], ["Kim", "Jinman", ""], ["Feng", "David Dagan", ""], ["Song", "Shaoli", ""]]}, {"id": "2103.05325", "submitter": "Ron Schindler", "authors": "Ron Schindler, Michael J\\\"ansch, Heiko Johannsen, Andr\\'as B\\'alint", "title": "An analysis of European crash data and scenario specification for heavy\n  truck safety system development within the AEROFLEX project", "comments": "Proceedings of 8th Transport Research Arena TRA 2020, April 27-30,\n  2020, Helsinki, Finland", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Heavy goods vehicles (HGVs) are involved in 4.5% of police-reported road\ncrashes in Europe and 14.2% of fatal road crashes. Active and passive safety\nsystems can help to prevent crashes or mitigate the consequences but need\ndetailed scenarios to be designed effectively. The aim of this paper is to give\na comprehensive and up-to-date analysis of HGV crashes in Europe. The analysis\nis based on general statistics from CARE, results about trucks weighing 16 tons\nor more from national crash databases and a detailed study of in-depth crash\ndata from GIDAS. Three scenarios are identified that should be addressed by\nfuture safety systems: (1) rear-end crashes with other vehicles in which the\ntruck is the striking partner, (2) conflicts during right turn maneuvers of the\ntruck and a cyclist and (3) pedestrians crossing the road perpendicular to the\ndirection of travel of the truck.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:08:29 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Schindler", "Ron", ""], ["J\u00e4nsch", "Michael", ""], ["Johannsen", "Heiko", ""], ["B\u00e1lint", "Andr\u00e1s", ""]]}, {"id": "2103.05432", "submitter": "Vaishnavi Subramanian", "authors": "Vaishnavi Subramanian, Tanveer Syeda-Mahmood, Minh N. Do", "title": "Multimodal fusion using sparse CCA for breast cancer survival prediction", "comments": "Accepted for poster presentation at International Symposium on\n  Biomedical Imaging (ISBI) 2021. 4 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP q-bio.GN stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Effective understanding of a disease such as cancer requires fusing multiple\nsources of information captured across physical scales by multimodal data. In\nthis work, we propose a novel feature embedding module that derives from\ncanonical correlation analyses to account for intra-modality and inter-modality\ncorrelations. Experiments on simulated and real data demonstrate how our\nproposed module can learn well-correlated multi-dimensional embeddings. These\nembeddings perform competitively on one-year survival classification of\nTCGA-BRCA breast cancer patients, yielding average F1 scores up to 58.69% under\n5-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:23:50 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Subramanian", "Vaishnavi", ""], ["Syeda-Mahmood", "Tanveer", ""], ["Do", "Minh N.", ""]]}, {"id": "2103.05499", "submitter": "Yuan Shijie", "authors": "Yuan Ji, Shijie Yuan", "title": "Lessons Learned from the Bayesian Design and Analysis for the BNT162b2\n  COVID-19 Vaccine Phase 3 Trial", "comments": "COVID-19, Bayesian credible interval, Confidence interval", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phase III BNT162b2 mRNA COVID-19 vaccine trial is based on a Bayesian\ndesign and analysis, and the main evidence of vaccine efficacy is presented in\nBayesian statistics. Confusion and mistakes are produced in the presentation of\nthe Bayesian results. Some key statistics, such as Bayesian credible intervals,\nare mislabeled and stated as confidence intervals. Posterior probabilities of\nthe vaccine efficacy are not reported as the main results. We illustrate the\nmain differences in the reporting of Bayesian analysis results for a clinical\ntrial and provide four recommendations. We argue that statistical evidence from\na Bayesian trial, when presented properly, is easier to interpret and directly\naddresses the main clinical questions, thereby better supporting regulatory\ndecision making. We also recommend using abbreviation \"BI\" to represent\nBayesian credible intervals as a differentiation to \"CI\" which stands for\nconfidence interval.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jan 2021 10:33:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Ji", "Yuan", ""], ["Yuan", "Shijie", ""]]}, {"id": "2103.05689", "submitter": "Lucy D'Agostino McGowan", "authors": "Lucy D'Agostino McGowan, Roger D. Peng, Stephanie C. Hicks", "title": "Design Principles for Data Analysis", "comments": "arXiv admin note: text overlap with arXiv:1903.07639", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The data science revolution has led to an increased interest in the practice\nof data analysis. While much has been written about statistical thinking, a\ncomplementary form of thinking that appears in the practice of data analysis is\ndesign thinking -- the problem-solving process to understand the people for\nwhom a product is being designed. For a given problem, there can be significant\nor subtle differences in how a data analyst (or producer of a data analysis)\nconstructs, creates, or designs a data analysis, including differences in the\nchoice of methods, tooling, and workflow. These choices can affect the data\nanalysis products themselves and the experience of the consumer of the data\nanalysis. Therefore, the role of a producer can be thought of as designing the\ndata analysis with a set of design principles. Here, we introduce design\nprinciples for data analysis and describe how they can be mapped to data\nanalyses in a quantitative, objective and informative manner. We also provide\nempirical evidence of variation of principles within and between both producers\nand consumers of data analyses. Our work leads to two insights: it suggests a\nformal mechanism to describe data analyses based on the design principles for\ndata analysis, and it provides a framework to teach students how to build data\nanalyses using formal design principles.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:48:25 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["McGowan", "Lucy D'Agostino", ""], ["Peng", "Roger D.", ""], ["Hicks", "Stephanie C.", ""]]}, {"id": "2103.05791", "submitter": "Qibin Duan", "authors": "Qibin Duan, Clare A. McGrory, Glenn Brown, Kerrie Mengersen and\n  You-Gan Wang", "title": "Spatio-temporal quantile regression analysis revealing more nuanced\n  patterns of climate change: a study of long-term daily temperature in\n  Australia", "comments": "30 pages, 10 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate change is commonly associated with an overall increase in mean\ntemperature in a defined past time period. Many studies consider temperature\ntrends at the global scale, but the literature is lacking in in-depth analysis\nof the temperature trends across Australia in recent decades. In addition to\nheterogeneity in mean and median values, daily Australia temperature data\nsuffers from quasi-periodic heterogeneity in variance. However, this issue has\nbarely been overlooked in climate research. A contribution of this article is\nthat we propose a joint model of quantile regression and variability. By\naccounting appropriately for the heterogeneity in these types of data, our\nanalysis reveals that daily maximum temperature is warming by 0.21 Celsius per\ndecade and daily minimum temperature by 0.13 Celsius per decade. However, our\nmodeling also shows nuanced patterns of climate change depends on location,\nseason, and the percentiles of the temperature series over Australia.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 00:02:24 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Duan", "Qibin", ""], ["McGrory", "Clare A.", ""], ["Brown", "Glenn", ""], ["Mengersen", "Kerrie", ""], ["Wang", "You-Gan", ""]]}, {"id": "2103.05880", "submitter": "Sakae Oya", "authors": "Sakae Oya", "title": "A Bayesian Graphical Approach for Large-Scale Portfolio Management with\n  Fewer Historical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing a large-scale portfolio with many assets is one of the most\nchallenging tasks in the field of finance. It is partly because estimation of\neither covariance or precision matrix of asset returns tends to be unstable or\neven infeasible when the number of assets $p$ exceeds the number of\nobservations $n$. For this reason, most of the previous studies on portfolio\nmanagement have focused on the case of $p < n$. To deal with the case of $p >\nn$, we propose to use a new Bayesian framework based on adaptive graphical\nLASSO for estimating the precision matrix of asset returns in a large-scale\nportfolio. Unlike the previous studies on graphical LASSO in the literature,\nour approach utilizes a Bayesian estimation method for the precision matrix\nproposed by Oya and Nakatsuma (2020) so that the positive definiteness of the\nprecision matrix should be always guaranteed. As an empirical application, we\nconstruct the global minimum variance portfolio of $p=100$ for various values\nof $n$ with the proposed approach as well as the non-Bayesian graphical LASSO\napproach, and compare their out-of-sample performance with the equal weight\nportfolio as the benchmark. In this comparison, the proposed approach produces\nmore stable results than the non-Bayesian approach in terms of Sharpe ratio,\nportfolio composition and turnover. Furthermore, the proposed approach succeeds\nin estimating the precision matrix even if $n$ is much smaller than $p$ and the\nnon-Bayesian approach fails to do so.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 05:52:28 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Oya", "Sakae", ""]]}, {"id": "2103.05909", "submitter": "Luca Maestrini", "authors": "Luca Maestrini, Robert G. Aykroyd and Matt P. Wand", "title": "A Variational Inference Framework for Inverse Problems", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for fitting inverse problem models via variational\nBayes approximations. This methodology guarantees flexibility to statistical\nmodel specification for a broad range of applications, good accuracy\nperformances and reduced model fitting times, when compared with standard\nMarkov chain Monte Carlo methods. The message passing and factor graph fragment\napproach to variational Bayes we describe facilitates streamlined\nimplementation of approximate inference algorithms and forms the basis to\nsoftware development. Such approach allows for supple inclusion of numerous\nresponse distributions and penalizations into the inverse problem model. Albeit\nour analysis is circumscribed to one- and two-dimensional response variables,\nwe lay down an infrastructure where streamlining algorithmic steps based on\nnullifying weak interactions between variables are extendible to inverse\nproblems in higher dimensions. Image processing applications motivated by\nbiomedical and archaeological problems are included as illustrations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:37:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Maestrini", "Luca", ""], ["Aykroyd", "Robert G.", ""], ["Wand", "Matt P.", ""]]}, {"id": "2103.05921", "submitter": "Christian Bongiorno", "authors": "Damien Challet, Christian Bongiorno, Guillaume Pelletier", "title": "Financial factors selection with knockoffs: fund replication,\n  explanatory and prediction networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.126105", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the knockoff procedure to factor selection in finance. By building\nfake but realistic factors, this procedure makes it possible to control the\nfraction of false discovery in a given set of factors. To show its versatility,\nwe apply it to fund replication and to the inference of explanatory and\nprediction networks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:25:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Challet", "Damien", ""], ["Bongiorno", "Christian", ""], ["Pelletier", "Guillaume", ""]]}, {"id": "2103.06023", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "Bal\\'azs R. Sziklai and P\\'eter Bir\\'o and L\\'aszl\\'o Csat\\'o", "title": "The efficacy of tournament designs", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tournaments are a widely used mechanism to rank alternatives in a noisy\nenvironment. We investigate a fundamental issue of economics in tournament\ndesign: what is the best usage of limited resources, that is, how should the\nalternatives be compared pairwise to best approximate their true but latent\nranking. We consider various formats including knockout tournaments,\nmulti-stage championships consisting of round-robin groups followed by single\nelimination, and the Swiss-system. They are evaluated via Monte-Carlo\nsimulations under six different assumptions on winning probabilities. Comparing\nthe same pairs of alternatives multiple times turns out to be an inefficacious\npolicy. The Swiss-system is found to be the most accurate among all these\ndesigns, especially in its ability to rank all participants. A possible\nexplanation is that it does not eliminate an alternative after a single loss,\nwhile it takes the history of the comparisons into account. Hence, this\nparticular format may deserve more attention from the decision-makers such as\nthe governing bodies of major sports.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 12:46:27 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 11:02:22 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sziklai", "Bal\u00e1zs R.", ""], ["Bir\u00f3", "P\u00e9ter", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2103.06121", "submitter": "Roger Guimera", "authors": "Gael Poux-Medard, Sergio Cobo-Lopez, Jordi Duch, Roger Guimera, Marta\n  Sales-Pardo", "title": "Complex decision-making strategies in a stock market experiment\n  explained as the combination of few simple strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.SI physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many studies have shown that there are regularities in the way human beings\nmake decisions. However, our ability to obtain models that capture such\nregularities and can accurately predict unobserved decisions is still limited.\nWe tackle this problem in the context of individuals who are given information\nrelative to the evolution of market prices and asked to guess the direction of\nthe market. We use a networks inference approach with stochastic block models\n(SBM) to find the model and network representation that is most predictive of\nunobserved decisions. Our results suggest that users mostly use recent\ninformation (about the market and about their previous decisions) to guess.\nFurthermore, the analysis of SBM groups reveals a set of strategies used by\nplayers to process information and make decisions that is analogous to\nbehaviors observed in other contexts. Our study provides and example on how to\nquantitatively explore human behavior strategies by representing decisions as\nnetworks and using rigorous inference and model-selection approaches.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 13:02:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Poux-Medard", "Gael", ""], ["Cobo-Lopez", "Sergio", ""], ["Duch", "Jordi", ""], ["Guimera", "Roger", ""], ["Sales-Pardo", "Marta", ""]]}, {"id": "2103.06152", "submitter": "Maria L. Daza-Torres", "authors": "Maria L. Daza-Torres, Marcos A. Capistr\\'an, Antonio Capella, J.\n  Andr\\'es Christen", "title": "Bayesian sequential data assimilation for COVID-19 forecasting", "comments": "22 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian sequential data assimilation method for COVID-19\nforecasting. It is assumed that suitable transmission, epidemic and observation\nmodels are available and previously validated and the transmission and epidemic\nmodels are coded into a dynamical system. The observation model depends on the\ndynamical system state variables and parameters, and is cast as a likelihood\nfunction. We elicit prior distributions of the effective population size, the\ndynamical system initial conditions and infectious contact rate, and use Markov\nChain Monte Carlo sampling to make inference and prediction of quantities of\ninterest (QoI) at the onset of the epidemic outbreak. The forecast is\nsequentially updated over a sliding window of epidemic records as new data\nbecomes available. Prior distributions for the state variables at the new\nforecasting time are assembled using the dynamical system, calibrated for the\nprevious forecast. Moreover, changes in the contact rate and effective\npopulation size are naturally introduced through auto-regressive models on the\ncorresponding parameters. We show our forecasting method's performance using a\nSEIR type model and COVID-19 data from several Mexican localities.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:05:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Daza-Torres", "Maria L.", ""], ["Capistr\u00e1n", "Marcos A.", ""], ["Capella", "Antonio", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "2103.06347", "submitter": "Ivor Cribben", "authors": "Martin Ondrus, Emily Olds, Ivor Cribben", "title": "Factorized Binary Search: change point detection in the network\n  structure of multivariate high-dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) time series data presents a\nunique opportunity to understand temporal brain connectivity, and models that\nuncover the complex dynamic workings of this organ are of keen interest in\nneuroscience. Change point models can capture and reflect the dynamic nature of\nbrain connectivity, however methods that translate well into a high-dimensional\ncontext (where $p>>n$) are scarce. To this end, we introduce\n$\\textit{factorized binary search}$ (FaBiSearch), a novel change point\ndetection method in the network structure of multivariate high-dimensional time\nseries. FaBiSearch uses non-negative matrix factorization, an unsupervised\ndimension reduction technique, and a new binary search algorithm to identify\nmultiple change points. In addition, we propose a new method for network\nestimation for data between change points. We show that FaBiSearch outperforms\nanother state-of-the-art method on simulated data sets and we apply FaBiSearch\nto a resting-state and to a task-based fMRI data set. In particular, for the\ntask-based data set, we explore network dynamics during the reading of Chapter\n9 in $\\textit{Harry Potter and the Sorcerer's Stone}$ and find that change\npoints across subjects coincide with key plot twists. Further, we find that the\ndensity of networks was positively related to the frequency of speech between\ncharacters in the story. Finally, we make all the methods discussed available\nin the R package $\\textbf{fabisearch}$ on CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:25:20 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ondrus", "Martin", ""], ["Olds", "Emily", ""], ["Cribben", "Ivor", ""]]}, {"id": "2103.06421", "submitter": "Yuan Shijie", "authors": "Xiaolei Lin, Jiaying Lyu, Shijie Yuan, Sue-Jane Wang, Yuan Ji", "title": "BaySize: Bayesian Sample Size Planning for Phase I Dose-Finding Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose BaySize, a sample size calculator for phase I clinical trials\nusing Bayesian models. BaySize applies the concept of effect size in dose\nfinding, assuming the MTD is defined based on an equivalence interval.\nLeveraging a decision framework that involves composite hypotheses, BaySize\nutilizes two prior distributions, the fitting prior (for model fitting) and\nsampling prior (for data generation), to conduct sample size calculation under\ndesirable statistical power. Look-up tables are generated to facilitate\npractical applications. To our knowledge, BaySize is the first sample size tool\nthat can be applied to a broad range of phase I trial designs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:51:42 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lin", "Xiaolei", ""], ["Lyu", "Jiaying", ""], ["Yuan", "Shijie", ""], ["Wang", "Sue-Jane", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.06567", "submitter": "Savita Pareek", "authors": "Savita Pareek, Kalyan Das, and Siuli Mukhopadhyay", "title": "Likelihood-based missing data analysis in multivariate crossover trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For gene expression data measured in a crossover trial, a multivariate\nmixed-effects model seems to be most appropriate. Standard statistical\ninference fails to provide reliable results when some responses are missing.\nParticularly for crossover studies, missingness is a serious concern as the\ntrial requires a small number of participants. A Monte Carlo EM (MCEM) based\ntechnique has been adopted to deal with this situation. Along with estimation,\na MCEM likelihood ratio test (LRTs) is developed for testing the fixed effects\nin such a multivariate crossover model with missing data. Intensive simulation\nstudies have been carried out prior to the analysis of the gene expression\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:50:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Pareek", "Savita", ""], ["Das", "Kalyan", ""], ["Mukhopadhyay", "Siuli", ""]]}, {"id": "2103.06585", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Simultaneous comparisons of treatments versus control (Dunnett-type\n  tests) for location-scale alternatives", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Commonly, the comparisons of treatment groups versus a control is performed\nfor location effects only where possible scale effects are considered as\ndisturbing. Sometimes scale effects are also relevant, as a kind of early\nindicator for changes. Here several approaches for Dunnett-type tests for\nlocation or scale effects are proposed and compared by a simulation study. Two\nreal data examples are analysed accordingly and the related R-code is available\nin the Appendix.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:29:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2103.06813", "submitter": "I. Esra Buyuktahtakin", "authors": "Xuecheng Yin, I. Esra Buyuktahtakin, Bhumi P. Patel", "title": "COVID-19: Optimal Allocation of Ventilator Supply under Uncertainty and\n  Risk", "comments": "35 pages, 6 figures, 10 tables, Under Review for a Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CE q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents a new risk-averse multi-stage stochastic\nepidemics-ventilator-logistics compartmental model to address the resource\nallocation challenges of mitigating COVID-19. This epidemiological logistics\nmodel involves the uncertainty of untested asymptomatic infections and\nincorporates short-term human migration. Disease transmission is also\nforecasted through a new formulation of transmission rates that evolve over\nspace and time with respect to various non-pharmaceutical interventions, such\nas wearing masks, social distancing, and lockdown. The proposed multi-stage\nstochastic model overviews different scenarios on the number of asymptomatic\nindividuals while optimizing the distribution of resources, such as\nventilators, to minimize the total expected number of newly infected and\ndeceased people. The Conditional Value at Risk (CVaR) is also incorporated into\nthe multi-stage mean-risk model to allow for a trade-off between the weighted\nexpected loss due to the outbreak and the expected risks associated with\nexperiencing disastrous pandemic scenarios. We apply our multi-stage mean-risk\nepidemics-ventilator-logistics model to the case of controlling the COVID-19 in\nhighly-impacted counties of New York and New Jersey. We calibrate, validate,\nand test our model using actual infection, population, and migration data. The\nresults indicate that short-term migration influences the transmission of the\ndisease significantly. The optimal number of ventilators allocated to each\nregion depends on various factors, including the number of initial infections,\ndisease transmission rates, initial ICU capacity, the population of a\ngeographical location, and the availability of ventilator supply. Our\ndata-driven modeling framework can be adapted to study the disease transmission\ndynamics and logistics of other similar epidemics and pandemics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 01:48:35 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Yin", "Xuecheng", ""], ["Buyuktahtakin", "I. Esra", ""], ["Patel", "Bhumi P.", ""]]}, {"id": "2103.06858", "submitter": "Enzo Cerullo", "authors": "Enzo Cerullo, Hayley E. Jones, Terry J. Quinn, Nicola J. Cooper, Alex\n  J. Sutton", "title": "Meta-analysis of dichotomous and polytomous diagnostic tests without a\n  gold standard", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard methods for the meta-analysis of diagnostic tests without a gold\nstandard are limited to dichotomous data. Multivariate probit models are used\nto analyze correlated binary data, and can be extended to multivariate ordered\nprobit models to model polytomous (i.e. non-binary) data. Within the context of\nan imperfect gold standard, they have previously been used for the analysis of\ndichotomous and polytomous diagnostic tests in a single study, and for the\nmeta-analysis of dichotomous tests. In this paper, we developed a hierarchical,\nlatent class multivariate probit model for the meta-analysis of polytomous and\ndichotomous diagnostic tests without a gold standard. The model can accommodate\na hierarchical partial pooling model on the conditional within-study\ncorrelations, which allow us to obtain summary estimates of joint test\naccuracy. Dichotomous tests use probit regression likelihoods and polytomous\ntests use ordinal probit regression likelihoods. We fitted the models using\nStan, which uses a state-of-the-art Hamiltonian Monte Carlo algorithm. We\napplied the models to a dataset in which studies evaluated the accuracy of\ntests, and combinations of tests, for deep vein thrombosis. We first\ndemonstrate the issues with dichotomising test accuracy data a priori without a\ngold standard, and then we apply a model which does not dichotomise the data.\nWe fitted models assuming conditional independence and dependence between\ntests, as well as models assuming a perfect gold standard, and compared model\nfit and summary estimates.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:38:56 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cerullo", "Enzo", ""], ["Jones", "Hayley E.", ""], ["Quinn", "Terry J.", ""], ["Cooper", "Nicola J.", ""], ["Sutton", "Alex J.", ""]]}, {"id": "2103.06885", "submitter": "Philip Waggoner", "authors": "Philip D. Waggoner", "title": "Modern Dimension Reduction", "comments": "83 pages, 36 figures, to appear in the Cambridge University Press\n  Elements in Quantitative and Computational Methods for the Social Sciences\n  series", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data are not only ubiquitous in society, but are increasingly complex both in\nsize and dimensionality. Dimension reduction offers researchers and scholars\nthe ability to make such complex, high dimensional data spaces simpler and more\nmanageable. This Element offers readers a suite of modern unsupervised\ndimension reduction techniques along with hundreds of lines of R code, to\nefficiently represent the original high dimensional data space in a simplified,\nlower dimensional subspace. Launching from the earliest dimension reduction\ntechnique principal components analysis and using real social science data, I\nintroduce and walk readers through application of the following techniques:\nlocally linear embedding, t-distributed stochastic neighbor embedding (t-SNE),\nuniform manifold approximation and projection, self-organizing maps, and deep\nautoencoders. The result is a well-stocked toolbox of unsupervised algorithms\nfor tackling the complexities of high dimensional data so common in modern\nsociety. All code is publicly accessible on Github.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 14:54:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Waggoner", "Philip D.", ""]]}, {"id": "2103.06939", "submitter": "Preston Biro", "authors": "Preston Biro and Stephen G. Walker", "title": "A Reinforcement Learning Based Approach to Play Calling in Football", "comments": "62 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the vast amount of data collected on football and the growth of\ncomputing abilities, many games involving decision choices can be optimized.\nThe underlying rule is the maximization of an expected utility of outcomes and\nthe law of large numbers. The data available allows us to compute with high\naccuracy the probabilities of outcomes of decisions and the well defined points\nsystem in the game allows us to have the necessary terminal utilities. With\nsome well established theory we can then optimize choices at a single play\nlevel.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 20:23:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Biro", "Preston", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2103.06979", "submitter": "Dmitry Ivanenko Alexandrovich", "authors": "D. O. Ivanenko, R. V. Pogorielov", "title": "Parameter estimation in models generated by SDE's with symmetric alpha\n  stable noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article considers vector parameter estimators in statistical models\ngenerated by Levy processes. An improved one step estimator is presented that\ncan be used for improving any other estimator. Combined numerical methods for\noptimization problems are proposed. A software has been developed and a\ncorrespondent testing and comparison have been presented.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:35:33 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ivanenko", "D. O.", ""], ["Pogorielov", "R. V.", ""]]}, {"id": "2103.07104", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno, Yulun Zhou, Marta Kryven, David Theurel,\n  Alessandro Rizzo, Paolo Santi, Joshua Tenenbaum, Carlo Ratti", "title": "Vector-based Pedestrian Navigation in Cities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do pedestrians choose their paths within city street networks? Human path\nplanning has been extensively studied at the aggregate level of mobility flows,\nand at the individual level with strictly designed behavioural experiments.\nHowever, a comprehensive, individual-level model of how humans select\npedestrian paths in real urban environments is still lacking. Here, we analyze\nhuman path planning behaviour in a large dataset of individual pedestrians,\nwhose GPS traces were continuously recorded as they pursued their daily goals.\nThrough statistical analysis we reveal two robust empirical discoveries, namely\nthat (1) people increasingly deviate from the shortest path as the distance\nbetween origin and destination increases, and (2) individual choices exhibit\ndirection-dependent asymmetries when origin and destination are swapped. In\norder to address the above findings, which cannot be explained by existing\nmodels, we develop a vector-based navigation framework motivated by the neural\nevidence of direction-encoding cells in hippocampal brain networks, and by\nbehavioural evidence of vector navigation in animals. Modelling pedestrian path\npreferences by vector-based navigation increases the model's predictive power\nby 35%, compared to a model based on minimizing distance with stochastic\neffects. We show that these empirical findings and modelling results generalise\nacross two major US cities with drastically different street networks,\nsuggesting that vector-based navigation is a universal property of human path\nplanning, independent of specific city environments. Our results offer a\nsimple, unified explanation of numerous findings about human navigation, and\nposit a computational mechanism that may underlie the human capacity to\nefficiently navigate in environments at various scales.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 06:37:16 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Bongiorno", "Christian", ""], ["Zhou", "Yulun", ""], ["Kryven", "Marta", ""], ["Theurel", "David", ""], ["Rizzo", "Alessandro", ""], ["Santi", "Paolo", ""], ["Tenenbaum", "Joshua", ""], ["Ratti", "Carlo", ""]]}, {"id": "2103.07200", "submitter": "Tsz Chai Fung", "authors": "Tsz Chai Fung, George Tzougas, Mario Wuthrich", "title": "Mixture composite regression models with multi-type feature selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to present a mixture composite regression model for\nclaim severity modelling. Claim severity modelling poses several challenges\nsuch as multimodality, heavy-tailedness and systematic effects in data. We\ntackle this modelling problem by studying a mixture composite regression model\nfor simultaneous modeling of attritional and large claims, and for considering\nsystematic effects in both the mixture components as well as the mixing\nprobabilities. For model fitting, we present a group-fused regularization\napproach that allows us for selecting the explanatory variables which\nsignificantly impact the mixing probabilities and the different mixture\ncomponents, respectively. We develop an asymptotic theory for this regularized\nestimation approach, and fitting is performed using a novel Generalized\nExpectation-Maximization algorithm. We exemplify our approach on real motor\ninsurance data set.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:48:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Fung", "Tsz Chai", ""], ["Tzougas", "George", ""], ["Wuthrich", "Mario", ""]]}, {"id": "2103.07272", "submitter": "Jacopo Diquigiovanni", "authors": "Marco Petretta, Lorenzo Schiavon, Jacopo Diquigiovanni", "title": "Mar-Co: a new dependence structure to model match outcomes in football", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The approaches commonly used to model the number of goals in a football match\nare characterised by strong assumptions about the dependence between the number\nof goals scored by the two competing teams and about their marginal\ndistribution. In this work, we argue that the assumptions traditionally made\nare not always based on solid arguments and sometimes they can be hardly\njustified. In light of this, we propose a modification of the Dixon and Coles\n(1997) model by relaxing the assumption of Poisson-distributed marginal\nvariables and by introducing an innovative dependence structure. Specifically,\nwe define the joint distribution of the number of goals scored during a match\nby means of thoroughly chosen marginal (Mar-) and conditional distributions\n(-Co). The resulting Mar-Co model is able to balance flexibility and conceptual\nsimplicity. A real data application involving five European leagues suggests\nthat the introduction of the novel dependence structure allows to capture and\ninterpret fundamental league-specific dynamics. In terms of betting\nperformance, the newly introduced Mar-Co model does not perform worse than the\nDixon and Coles one in a traditional framework (i.e. 1-X-2 bet) and it\noutperforms the competing model when a more comprehensive dependence structure\nis needed (i.e. Under/Over 2.5 bet).\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:47:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Petretta", "Marco", ""], ["Schiavon", "Lorenzo", ""], ["Diquigiovanni", "Jacopo", ""]]}, {"id": "2103.07382", "submitter": "Antonios Kamariotis", "authors": "Antonios Kamariotis, Eleni Chatzi, Daniel Straub", "title": "Value of information from vibration-based structural health monitoring\n  extracted via Bayesian model updating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantifying the value of the information extracted from a structural health\nmonitoring (SHM) system is an important step towards convincing decision makers\nto implement these systems. We quantify this value by adaptation of the\nBayesian decision analysis framework. In contrast to previous works, we model\nin detail the entire process of data generation to processing, model updating\nand reliability calculation, and investigate it on a deteriorating bridge\nsystem. The framework assumes that dynamic response data are obtained in a\nsequential fashion from deployed accelerometers, subsequently processed by an\noutput-only operational modal analysis scheme for identifying the system's\nmodal characteristics. We employ a classical Bayesian model updating\nmethodology to sequentially learn the deterioration and estimate the structural\ndamage evolution over time. This leads to sequential updating of the structural\nreliability, which constitutes the basis for a preposterior Bayesian decision\nanalysis. Alternative actions are defined and a heuristic-based approach is\nemployed for the life-cycle optimization. By solving the preposterior Bayesian\ndecision analysis, one is able to quantify the benefit of the availability of\nlong-term SHM vibrational data. Numerical investigations show that this\nframework can provide quantitative measures on the optimality of an SHM system\nin a specific decision context.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:23:57 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Kamariotis", "Antonios", ""], ["Chatzi", "Eleni", ""], ["Straub", "Daniel", ""]]}, {"id": "2103.07410", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai, Saad Mouti, Marjorie Lima do Vale, Sumantra Ray, Jeffrey\n  Bohn and Lisa Goldberg", "title": "A Resampling Approach For causal Inference On Novel Two-Point\n  Time-Series With Application To Identify Risk Factors For Type-2 Diabetes And\n  Cardiovascular Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-point time-series data, characterized by baseline and follow-up\nobservations, are frequently encountered in health research. We study a novel\ntwo-point time series structure without a control group, which is driven by an\nobservational routine clinical dataset collected to monitor key risk markers of\ntype-$2$ diabetes (T2D) and cardiovascular disease (CVD). We propose a\nresampling approach called 'I-Rand' for independently sampling one of the two\ntime points for each individual and making inference on the estimated causal\neffects based on matching methods. The proposed method is illustrated with data\nfrom a service-based dietary intervention to promote a low-carbohydrate diet\n(LCD), designed to impact risk of T2D and CVD. Baseline data contain a\npre-intervention health record of study participants, and health data after LCD\nintervention are recorded at the follow-up visit, providing a two-point\ntime-series pattern without a parallel control group. Using this approach we\nfind that obesity is a significant risk factor of T2D and CVD, and an LCD\napproach can significantly mitigate the risks of T2D and CVD. We provide code\nthat implements our method.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:10:54 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dai", "Xiaowu", ""], ["Mouti", "Saad", ""], ["Vale", "Marjorie Lima do", ""], ["Ray", "Sumantra", ""], ["Bohn", "Jeffrey", ""], ["Goldberg", "Lisa", ""]]}, {"id": "2103.07431", "submitter": "Cord A. M\\\"uller", "authors": "Katy Klauenberg, Cord A. M\\\"uller, and Clemens Elster", "title": "Hypothesis-based acceptance sampling for modules F and F1 of the\n  European Measuring Instruments Directive", "comments": "accepted, Statistics and Public Policy", "journal-ref": "Statistics and Public Policy, 2021, Vol. 8, No. 1, 9-17", "doi": "10.1080/2330443X.2021.1900762", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millions of measuring instruments are verified each year before being placed\non the markets worldwide. In the EU, such initial conformity assessments are\nregulated by the Measuring Instruments Directive (MID). The MID modules F and\nF1 on product verification allow for statistical acceptance sampling, whereby\nonly random subsets of instruments need to be inspected. This paper\nre-interprets the acceptance sampling conditions formulated by the MID. The new\ninterpretation is contrasted with the one advanced in WELMEC guide 8.10, and\nthree advantages have become apparent. Firstly, an economic advantage of the\nnew interpretation is a producers' risk bounded from above, such that measuring\ninstruments with sufficient quality are accepted with a guaranteed probability\nof no less than 95 %. Secondly, a conceptual advantage is that the new MID\ninterpretation fits into the well-known, formal framework of statistical\nhypothesis testing. Thirdly, the new interpretation applies unambiguously to\nfinite-sized lots, even very small ones. We conclude that the new\ninterpretation is to be preferred and suggest re-formulating the statistical\nsampling conditions in the MID. Re-interpreting the MID conditions implies that\ncurrently available sampling plans are either not admissible or not optimal. We\nderive a new acceptance sampling scheme and recommend its application.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:49:14 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Klauenberg", "Katy", ""], ["M\u00fcller", "Cord A.", ""], ["Elster", "Clemens", ""]]}, {"id": "2103.07533", "submitter": "Jacques de Chalendar", "authors": "Jacques A. de Chalendar and Peter W. Glynn", "title": "On Incorporating Forecasts into Linear State Space Model Markov Decision\n  Processes", "comments": "Accepted manuscript in an upcoming special issue of the Philosophical\n  Transactions of the Royal Society A on the Mathematics of Energy Systems", "journal-ref": null, "doi": "10.1098/rsta.2019.0430", "report-no": null, "categories": "math.OC cs.LG cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weather forecast information will very likely find increasing application in\nthe control of future energy systems. In this paper, we introduce an augmented\nstate space model formulation with linear dynamics, within which one can\nincorporate forecast information that is dynamically revealed alongside the\nevolution of the underlying state variable. We use the martingale model for\nforecast evolution (MMFE) to enforce the necessary consistency properties that\nmust govern the joint evolution of forecasts with the underlying state. The\nformulation also generates jointly Markovian dynamics that give rise to Markov\ndecision processes (MDPs) that remain computationally tractable. This paper is\nthe first to enforce MMFE consistency requirements within an MDP formulation\nthat preserves tractability.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:21:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["de Chalendar", "Jacques A.", ""], ["Glynn", "Peter W.", ""]]}, {"id": "2103.07661", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn and Siegfried Kropf", "title": "Closed testing procedures for treatment-versus-control comparisons and\n  multiple correlated endpoint", "comments": "12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Preferably in two- or three-arm randomized clinical trials, a few (2,3)\ncorrelated multiple primary endpoints are considered. In addition to the closed\ntesting principle based on different global tests, two max(maxT) tests are\ncompared with respect to any-pairs, all-pairs and individual power in a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 09:15:11 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hothorn", "Ludwig A.", ""], ["Kropf", "Siegfried", ""]]}, {"id": "2103.07678", "submitter": "Ehsan Farahbakhsh", "authors": "Hojat Shirmard, Ehsan Farahbakhsh, Dietmar Muller, Rohitash Chandra", "title": "A review of machine learning in processing remote sensing data for\n  mineral exploration", "comments": "74 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a primary step in mineral exploration, a variety of features are mapped\nsuch as lithological units, alteration types, structures, and minerals. These\nfeatures are extracted to aid decision-making in targeting ore deposits.\nDifferent types of remote sensing data including satellite optical and radar,\nairborne, and drone-based data make it possible to overcome problems associated\nwith mapping these important parameters on the field. The rapid increase in the\nvolume of remote sensing data obtained from different platforms has allowed\nscientists to develop advanced, innovative, and powerful data processing\nmethodologies. Machine learning methods can help in processing a wide range of\nremote sensing data and in determining the relationship between the reflectance\ncontinuum and features of interest. Moreover, these methods are robust in\nprocessing spectral and ground truth measurements against noise and\nuncertainties. In recent years, many studies have been carried out by\nsupplementing geological surveys with remote sensing data, and this area is now\nconsidered a hotspot in geoscience research. This paper reviews the\nimplementation and adaptation of some popular and recently established machine\nlearning methods for remote sensing data processing and investigates their\napplications for exploring different ore deposits. Lastly, the challenges and\nfuture directions in this critical interdisciplinary field are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:36:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shirmard", "Hojat", ""], ["Farahbakhsh", "Ehsan", ""], ["Muller", "Dietmar", ""], ["Chandra", "Rohitash", ""]]}, {"id": "2103.07680", "submitter": "Martin Scharpenberg", "authors": "Werner Brannath, Martin Scharpenberg, Sylvia Schmidt", "title": "Single-stage, three-arm, adaptive test strategies for non-inferiority\n  trials with an unstable reference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For indications where only unstable reference treatments are available and\nuse of placebo is ethically justified, three-arm `gold standard' designs with\nan experimental, reference and placebo arm are recommended for non-inferiority\ntrials. In such designs, the demonstration of efficacy of the reference or\nexperimental treatment is a requirement. They have the disadvantage that only\nlittle can be concluded from the trial if the reference fails to be\nefficacious. To overcome this, we investigate novel single-stage, adaptive test\nstrategies where non-inferiority is tested only if the reference shows\nsufficient efficacy and otherwise $\\delta$-superiority of the experimental\ntreatment over placebo is tested. With a properly chosen superiority margin,\n$\\delta$-superiority indirectly shows non-inferiority. We optimize the sample\nsize for several decision rules and find that the natural, data driven test\nstrategy, which tests non-inferiority if the reference's efficacy test is\nsignificant, leads to the smallest overall and placebo sample sizes. We proof\nthat under specific constraints on the sample sizes, this procedure controls\nthe family-wise error rate. All optimal sample sizes are found to meet this\nconstraint. We finally show how to account for a relevant placebo drop-out rate\nin an efficient way and apply the new test strategy to a real life data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:41:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Brannath", "Werner", ""], ["Scharpenberg", "Martin", ""], ["Schmidt", "Sylvia", ""]]}, {"id": "2103.07717", "submitter": "Farzad Sabzikar", "authors": "Jinu Kabala, Farzad Sabzikar", "title": "Statistical inference for ARTFIMA time series with stable innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Autoregressive tempered fractionally integrated moving average with stable\ninnovations modifies the power-law kernel of the fractionally integrated time\nseries model by adding an exponential tempering factor. The tempered time\nseries is a stationary model that can exhibits semi-long-range dependence. This\npaper develops the basic theory of the tempered time series model, including\ndependence structure and parameter estimation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 13:26:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kabala", "Jinu", ""], ["Sabzikar", "Farzad", ""]]}, {"id": "2103.07746", "submitter": "Shu Wang", "authors": "Shu Wang, Ji-Hyun Lee", "title": "A Simulation Study Evaluating Phase I Clinical Trial Designs for\n  Combinational Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nowadays, more and more clinical trials choose combinational agents as the\nintervention to achieve better therapeutic responses. However, dose-finding for\ncombinational agents is much more complicated than single agent as the full\norder of combination dose toxicity is unknown. Therefore, regular phase I\ndesigns are not able to identify the maximum tolerated dose (MTD) of\ncombinational agents. Motivated by such needs, plenty of novel phase I clinical\ntrial designs for combinational agents were proposed. With so many available\ndesigns, research that compare their performances, explore parameters' impacts,\nand provide recommendations is very limited. Therefore, we conducted a\nsimulation study to evaluate multiple phase I designs that proposed to identify\nsingle MTD for combinational agents under various scenarios. We also explored\ninfluences of different design parameters. In the end, we summarized the pros\nand cons of each design, and provided a general guideline in design selection.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 16:37:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wang", "Shu", ""], ["Lee", "Ji-Hyun", ""]]}, {"id": "2103.07756", "submitter": "Pengxiang Wu", "authors": "Yikai Zhang, Songzhu Zheng, Pengxiang Wu, Mayank Goswami, Chao Chen", "title": "Learning with Feature-Dependent Label Noise: A Progressive Approach", "comments": "ICLR 2021 (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Label noise is frequently observed in real-world large-scale datasets. The\nnoise is introduced due to a variety of reasons; it is heterogeneous and\nfeature-dependent. Most existing approaches to handling noisy labels fall into\ntwo categories: they either assume an ideal feature-independent noise, or\nremain heuristic without theoretical guarantees. In this paper, we propose to\ntarget a new family of feature-dependent label noise, which is much more\ngeneral than commonly used i.i.d. label noise and encompasses a broad spectrum\nof noise patterns. Focusing on this general noise family, we propose a\nprogressive label correction algorithm that iteratively corrects labels and\nrefines the model. We provide theoretical guarantees showing that for a wide\nvariety of (unknown) noise patterns, a classifier trained with this strategy\nconverges to be consistent with the Bayes classifier. In experiments, our\nmethod outperforms SOTA baselines and is robust to various noise types and\nlevels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:34:22 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 07:28:12 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 05:34:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Yikai", ""], ["Zheng", "Songzhu", ""], ["Wu", "Pengxiang", ""], ["Goswami", "Mayank", ""], ["Chen", "Chao", ""]]}, {"id": "2103.07818", "submitter": "Yiqun Chen", "authors": "Yiqun T. Chen, Sean W. Jewell, Daniela M. Witten", "title": "Quantifying uncertainty in spikes estimated from calcium imaging data", "comments": "51 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, a number of methods have been proposed to estimate the times\nat which neurons spike on the basis of calcium imaging data. However,\nquantifying the uncertainty associated with these estimated spikes remains an\nopen problem. We consider a simple and well-studied model for calcium imaging\ndata, which states that calcium decays exponentially in the absence of a spike,\nand instantaneously increases when a spike occurs. We wish to test the null\nhypothesis that the neuron did not spike -- i.e., that there was no increase in\ncalcium -- at a particular timepoint at which a spike was estimated. In this\nsetting, classical hypothesis tests lead to inflated Type I error, because the\nspike was estimated on the same data. To address this problem, we propose a\nselective inference approach to test the null hypothesis. We describe an\nefficient algorithm to compute finite-sample p-values that control selective\nType I error, and confidence intervals with correct selective coverage, for\nspikes estimated using a recent proposal from the literature. We apply our\nproposal in simulation and on calcium imaging data from the spikefinder\nchallenge.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 00:03:56 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Chen", "Yiqun T.", ""], ["Jewell", "Sean W.", ""], ["Witten", "Daniela M.", ""]]}, {"id": "2103.07868", "submitter": "Zhuo Qu", "authors": "Zhuo Qu and Marc G. Genton", "title": "Sparse Functional Boxplots for Multivariate Curves", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces the sparse functional boxplot and the intensity sparse\nfunctional boxplot as practical exploratory tools that make visualization\npossible for both complete and sparse functional data. These visualization\ntools can be used either in the univariate or multivariate functional setting.\nThe sparse functional boxplot, which is based on the functional boxplot,\ndepicts sparseness characteristics in the envelope of the 50\\% central region,\nthe median curve, and the outliers. The proportion of missingness at each time\nindex within the central region is colored in gray. The intensity sparse\nfunctional boxplot displays the relative intensity of sparse points in the\ncentral region, revealing where data are more or less sparse. The two-stage\nfunctional boxplot, a derivation from the functional boxplot to better detect\noutliers, is also extended to its sparse form. Several depth proposals for\nsparse multivariate functional data are evaluated and outlier detection is\ntested in simulations under various data settings and sparseness scenarios. The\npractical applications of the sparse functional boxplot and intensity sparse\nfunctional boxplot are illustrated with two public health datasets.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 08:28:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Qu", "Zhuo", ""], ["Genton", "Marc G.", ""]]}, {"id": "2103.07937", "submitter": "Ursula Laa", "authors": "Ursula Laa, German Valencia", "title": "Pandemonium: a clustering tool to partition parameter space --\n  application to the B anomalies", "comments": "40 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an hep-ex hep-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the interactive tool pandemonium to cluster model predictions\nthat depend on a set of parameters. The model predictions are used to define\nthe coordinates in observable space which go into the clustering. The results\nof this partitioning are then visualized in both observable and parameter space\nto study correlations between them. The tool offers multiple choices for\ncoordinates, distance functions and linkage methods within hierarchical\nclustering. It provides a set of diagnostic statistics and visualization\nmethods to study the clustering results in order to interpret the outcome. The\nmethods are most useful in an interactive environment that enables exploration,\nand we have implemented them with a graphical user interface in R. We\ndemonstrate the concepts with an application to phenomenological studies in\nflavor physics in the context of the so-called B anomalies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 14:20:12 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Laa", "Ursula", ""], ["Valencia", "German", ""]]}, {"id": "2103.08035", "submitter": "Subhadeep Paul", "authors": "Kartik Lovekar, Srijan Sengupta, Subhadeep Paul", "title": "Testing for the Network Small-World Property", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have long observed that the \"small-world\" property, which\ncombines the concepts of high transitivity or clustering with a low average\npath length, is ubiquitous for networks obtained from a variety of disciplines\nincluding social sciences, biology, neuroscience, and ecology. However, we find\nthree shortcomings of the currently popular definition and detection methods\nrendering the concept less powerful. First, the classical definition combines\nhigh transitivity with a low average path length in a rather ad-hoc fashion\nwhich confounds the two separate aspects. We find that in several cases,\nnetworks get flagged as \"small world\" by the current methodology solely because\nof their high transitivity. Second, the detection methods lack a formal\nstatistical inference, and third, the comparison is typically performed against\nsimplistic random graph models as the baseline which ignores well-known network\ncharacteristics. We propose three innovations to address these issues. First,\nwe decouple the properties of high transitivity and low average path length as\nseparate events to test for. Second, we define the property as a statistical\ntest between a suitable null model and a superimposed alternative model. Third,\nthe test is performed using parametric bootstrap with several null models to\nallow a wide range of background structures in the network. In addition to the\nbootstrap tests, we also propose an asymptotic test under the\nErd\\\"{o}s-Ren\\'{y}i null model for which we provide theoretical guarantees on\nthe asymptotic level and power. Applying the proposed methods on a large number\nof network datasets, we uncover new insights about their small-world property.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 21:00:12 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lovekar", "Kartik", ""], ["Sengupta", "Srijan", ""], ["Paul", "Subhadeep", ""]]}, {"id": "2103.08055", "submitter": "Basil Maag", "authors": "Basil Maag, Stefan Feuerriegel, Mathias Kraus, Maytal Saar-Tsechansky,\n  Thomas Z\\\"uger", "title": "Modeling Longitudinal Dynamics of Comorbidities", "comments": null, "journal-ref": null, "doi": "10.1145/3450439.3451871", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medicine, comorbidities refer to the presence of multiple, co-occurring\ndiseases. Due to their co-occurring nature, the course of one comorbidity is\noften highly dependent on the course of the other disease and, hence,\ntreatments can have significant spill-over effects. Despite the prevalence of\ncomorbidities among patients, a comprehensive statistical framework for\nmodeling the longitudinal dynamics of comorbidities is missing. In this paper,\nwe propose a probabilistic model for analyzing comorbidity dynamics over time\nin patients. Specifically, we develop a coupled hidden Markov model with a\npersonalized, non-homogeneous transition mechanism, named Comorbidity-HMM. The\nspecification of our Comorbidity-HMM is informed by clinical research: (1) It\naccounts for different disease states (i. e., acute, stable) in the disease\nprogression by introducing latent states that are of clinical meaning. (2) It\nmodels a coupling among the trajectories from comorbidities to capture\nco-evolution dynamics. (3) It considers between-patient heterogeneity (e. g.,\nrisk factors, treatments) in the transition mechanism. Based on our model, we\ndefine a spill-over effect that measures the indirect effect of treatments on\npatient trajectories through coupling (i. e., through comorbidity\nco-evolution). We evaluated our proposed Comorbidity-HMM based on 675 health\ntrajectories where we investigate the joint progression of diabetes mellitus\nand chronic liver disease. Compared to alternative models without coupling, we\nfind that our Comorbidity-HMM achieves a superior fit. Further, we quantify the\nspill-over effect, that is, to what extent diabetes treatments are associated\nwith a change in the chronic liver disease from an acute to a stable disease\nstate. To this end, our model is of direct relevance for both treatment\nplanning and clinical research in the context of comorbidities.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:19:21 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Maag", "Basil", ""], ["Feuerriegel", "Stefan", ""], ["Kraus", "Mathias", ""], ["Saar-Tsechansky", "Maytal", ""], ["Z\u00fcger", "Thomas", ""]]}, {"id": "2103.08250", "submitter": "Feng Li", "authors": "Matthias Anderer and Feng Li", "title": "Forecasting reconciliation with a top-down alignment of independent\n  level forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical forecasting with intermittent time series is a challenge in both\nresearch and empirical studies. The overall forecasting performance is heavily\naffected by the forecasting accuracy of intermittent time series at bottom\nlevels. In this paper, we present a forecasting reconciliation approach that\ntreats the bottom level forecast as latent to ensure higher forecasting\naccuracy on the upper levels of the hierarchy. We employ a pure deep learning\nforecasting approach N-BEATS for continuous time series on top levels and a\nwidely used tree-based algorithm LightGBM for the bottom level intermittent\ntime series. The hierarchical forecasting with alignment approach is simple and\nstraightforward to implement in practice. It sheds light on an orthogonal\ndirection for forecasting reconciliation. When there is difficulty finding an\noptimal reconciliation, allowing suboptimal forecasts at a lower level could\nretain a high overall performance. The approach in this empirical study was\ndeveloped by the first author during the M5 Forecasting Accuracy competition\nranking second place. The approach is business orientated and could be\nbeneficial for business strategic planning.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:00:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Anderer", "Matthias", ""], ["Li", "Feng", ""]]}, {"id": "2103.08321", "submitter": "Stefano M. Iacus", "authors": "Fabrizio Natale, Stefano Maria Iacus, Alessandra Conte, Spyridon\n  Spyratos, Francesco Sermi", "title": "Territorial differences in the spread of COVID-19 in European regions\n  and US counties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.data-an q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article explores the territorial differences in the onset and spread of\nCOVID-19 and the excess mortality associated with the pandemic, across the\nEuropean NUTS3 regions and US counties. Both in Europe and in the US, the\npandemic arrived earlier and recorded higher Rt values in urban regions than in\nintermediate and rural ones. A similar gap is also found in the data on excess\nmortality. In the weeks during the first phase of the pandemic, urban regions\nin EU countries experienced excess mortality of up to 68pp more than rural\nones. We show that, during the initial days of the pandemic, territorial\ndifferences in Rt by the degree of urbanisation can be largely explained by the\nlevel of internal, inbound and outbound mobility. The differences in the spread\nof COVID-19 by rural-urban typology and the role of mobility are less clear\nduring the second wave. This could be linked to the fact that the infection is\nwidespread across territories, to changes in mobility patterns during the\nsummer period as well as to the different containment measures which reverse\nthe causality between mobility and Rt.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:15:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Natale", "Fabrizio", ""], ["Iacus", "Stefano Maria", ""], ["Conte", "Alessandra", ""], ["Spyratos", "Spyridon", ""], ["Sermi", "Francesco", ""]]}, {"id": "2103.08332", "submitter": "Fernando Palluzzi", "authors": "Fernando Palluzzi and Mario Grassi", "title": "SEMgraph: An R Package for Causal Network Analysis of High-Throughput\n  Data with Structural Equation Models", "comments": "28 pages; 4 figures; original article; R package; development version\n  available at https://github.com/fernandoPalluzzi/SEMgraph", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the advent of high-throughput sequencing (HTS) in molecular biology and\nmedicine, the need for scalable statistical solutions for modeling complex\nbiological systems has become of critical importance. The increasing number of\nplatforms and possible experimental scenarios raised the problem of integrating\nlarge amounts of new heterogeneous data and current knowledge, to test novel\nhypotheses and improve our comprehension of physiological processes and\ndiseases. Although network theory provided a framework to represent biological\nsystems and study their hidden properties, different algorithms still offer low\nreproducibility and robustness, dependence on user-defined setup, and poor\ninterpretability. Here we discuss the R package SEMgraph, combining network\nanalysis and causal inference within the framework of structural equation\nmodeling (SEM). It provides a fully automated toolkit, managing complex\nbiological systems as multivariate networks, ensuring robustness and\nreproducibility through data-driven evaluation of model architecture and\nperturbation, that is readily interpretable in terms of causal effects among\nsystem components. In addition, SEMgraph offers several functions for perturbed\npath finding, model reduction, and parallelization options for the analysis of\nlarge interaction networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:24:40 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Palluzzi", "Fernando", ""], ["Grassi", "Mario", ""]]}, {"id": "2103.08341", "submitter": "Timothy Wolock", "authors": "Timothy M Wolock, Seth R Flaxman, Kathryn A Risher, Tawanda Dadirai,\n  Simon Gregson, Jeffrey W Eaton", "title": "Evaluating distributional regression strategies for modelling\n  self-reported sexual age-mixing", "comments": "Main text: 25 pages, 7 figures, 5 tables; Appendix: 24 pages, 11\n  figures, 10 tables; Submitted to eLife", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The age dynamics of sexual partnership formation determine patterns of\nsexually transmitted disease transmission and have long been a focus of\nresearchers studying human immunodeficiency virus. Data on self-reported sexual\npartner age distributions are available from a variety of sources. We sought to\nexplore statistical models that accurately predict the distribution of sexual\npartner ages over age and sex. We identified which probability distributions\nand outcome specifications best captured variation in partner age and\nquantified the benefits of modelling these data using distributional\nregression. We found that distributional regression with a sinh-arcsinh\ndistribution replicated observed partner age distributions most accurately\nacross three geographically diverse data sets. This framework can be extended\nwith well-known hierarchical modelling tools and can help improve estimates of\nsexual age-mixing dynamics.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 12:31:52 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wolock", "Timothy M", ""], ["Flaxman", "Seth R", ""], ["Risher", "Kathryn A", ""], ["Dadirai", "Tawanda", ""], ["Gregson", "Simon", ""], ["Eaton", "Jeffrey W", ""]]}, {"id": "2103.08450", "submitter": "Maochao Xu", "authors": "Mingyue Zhang Wu, Jinzhu Luo, Xing Fang, Maochao Xu, Peng Zhao", "title": "Modeling Multivariate Cyber Risks: Deep Learning Dating Extreme Value\n  Theory", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling cyber risks has been an important but challenging task in the domain\nof cyber security. It is mainly because of the high dimensionality and heavy\ntails of risk patterns. Those obstacles have hindered the development of\nstatistical modeling of the multivariate cyber risks. In this work, we propose\na novel approach for modeling the multivariate cyber risks which relies on the\ndeep learning and extreme value theory. The proposed model not only enjoys the\nhigh accurate point predictions via deep learning but also can provide the\nsatisfactory high quantile prediction via extreme value theory. The simulation\nstudy shows that the proposed model can model the multivariate cyber risks very\nwell and provide satisfactory prediction performances. The empirical evidence\nbased on real honeypot attack data also shows that the proposed model has very\nsatisfactory prediction performances.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:18:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Wu", "Mingyue Zhang", ""], ["Luo", "Jinzhu", ""], ["Fang", "Xing", ""], ["Xu", "Maochao", ""], ["Zhao", "Peng", ""]]}, {"id": "2103.08669", "submitter": "Camila Lorenz", "authors": "Gleice Margarete de Souza Concei\\c{c}\\~ao, Gerson Laurindo Barbosa,\n  Camila Lorenz, Ana Carolina Dias Bocewicz, Lidia Maria Reis Santana,\n  Cristiano Corr\\^ea de Azevedo Marques, Francisco Chiaravalloti-Neto", "title": "Effect of social isolation in dengue cases in the state of Sao Paulo,\n  Brazil: an analysis during the COVID-19 pandemic", "comments": "15 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Background: Studies have shown that human mobility is an important factor in\ndengue epidemiology. Changes in mobility resulting from COVID-19 pandemic set\nup a real-life situation to test this hypothesis. Our objective was to evaluate\nthe effect of reduced mobility due to this pandemic in the occurrence of dengue\nin the state of S\\~ao Paulo, Brazil. Method: It is an ecological study of time\nseries, developed between January and August 2020. We use the number of\nconfirmed dengue cases and residential mobility, on a daily basis, from\nsecondary information sources. Mobility was represented by the daily percentage\nvariation of residential population isolation, obtained from the Google\ndatabase. We modeled the relationship between dengue occurrence and social\ndistancing by negative binomial regression, adjusted for seasonality. We\nrepresent the social distancing dichotomously (isolation versus no isolation)\nand consider lag for isolation from the dates of occurrence of dengue. Results:\nThe risk of dengue decreased around 9.1% (95% CI: 14.2 to 3.7) in the presence\nof isolation, considering a delay of 20 days between the degree of isolation\nand the dengue first symptoms. Conclusions: We have shown that mobility can\nplay an important role in the epidemiology of dengue and should be considered\nin surveillance and control activities\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 19:25:18 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Concei\u00e7\u00e3o", "Gleice Margarete de Souza", ""], ["Barbosa", "Gerson Laurindo", ""], ["Lorenz", "Camila", ""], ["Bocewicz", "Ana Carolina Dias", ""], ["Santana", "Lidia Maria Reis", ""], ["Marques", "Cristiano Corr\u00eaa de Azevedo", ""], ["Chiaravalloti-Neto", "Francisco", ""]]}, {"id": "2103.08732", "submitter": "Luis Alberto Barboza", "authors": "Luis A. Barboza, Paola V\\'asquez, Gustavo Mery, Fabio Sanchez, Yury E.\n  Garc\\'ia, Juan G. Calvo, Tania Rivas and Daniel Salas", "title": "The role of mobility and sanitary measures on Covid-19 in Costa Rica,\n  March through July 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to infer the effects that changes on human mobility\nhad on the transmission dynamics during the first four months of the SARS-CoV-2\noutbreak in Costa Rica, before community transmission was established in the\ncountry. By using parametric and non parametric detection change-point\ntechniques we were able to identify two different periods where at least the\ntrend and variability of new daily cases significantly changed. In order to\ncombine this information with population movement, we use data from Google\nMobility Trends that allow us to estimate the lag between the rate of new daily\ncases and each of the categories established by Google. The information is then\nused to establish an association between changes in population mobility and the\nsanitary measures taken during the study period.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:43:55 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Barboza", "Luis A.", ""], ["V\u00e1squez", "Paola", ""], ["Mery", "Gustavo", ""], ["Sanchez", "Fabio", ""], ["Garc\u00eda", "Yury E.", ""], ["Calvo", "Juan G.", ""], ["Rivas", "Tania", ""], ["Salas", "Daniel", ""]]}, {"id": "2103.08754", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou, Yuan Ji", "title": "Incorporating External Data into the Analysis of Clinical Trials via\n  Bayesian Additive Regression Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most clinical trials involve the comparison of a new treatment to a control\narm (e.g., the standard of care) and the estimation of a treatment effect.\nExternal data, including historical clinical trial data and real-world\nobservational data, are commonly available for the control arm. Borrowing\ninformation from external data holds the promise of improving the estimation of\nrelevant parameters and increasing the power of detecting a treatment effect if\nit exists. In this paper, we propose to use Bayesian additive regression trees\n(BART) for incorporating external data into the analysis of clinical trials,\nwith a specific goal of estimating the conditional or population average\ntreatment effect. BART naturally adjusts for patient-level covariates and\ncaptures potentially heterogeneous treatment effects across different data\nsources, achieving flexible borrowing. Simulation studies demonstrate that BART\ncompares favorably to a hierarchical linear model and a normal-normal\nhierarchical model. We illustrate the proposed method with an acupuncture\ntrial.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:54:17 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.08761", "submitter": "Asim Dey", "authors": "Asim K. Dey, Vyacheslav Lyubchich, and Yulia R. Gel", "title": "Modeling Weather-induced Home Insurance Risks with Support Vector\n  Machine Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Insurance industry is one of the most vulnerable sectors to climate change.\nAssessment of future number of claims and incurred losses is critical for\ndisaster preparedness and risk management. In this project, we study the effect\nof precipitation on a joint dynamics of weather-induced home insurance claims\nand losses. We discuss utility and limitations of such machine learning\nprocedures as Support Vector Machines and Artificial Neural Networks, in\nforecasting future claim dynamics and evaluating associated uncertainties. We\nillustrate our approach by application to attribution analysis and forecasting\nof weather-induced home insurance claims in a middle-sized city in the Canadian\nPrairies.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 23:13:32 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dey", "Asim K.", ""], ["Lyubchich", "Vyacheslav", ""], ["Gel", "Yulia R.", ""]]}, {"id": "2103.08794", "submitter": "Jorge Lucero", "authors": "Jorge C. Lucero", "title": "Identification of COVID-19 mortality patterns in Brazil by a functional\n  QR decomposition analysis", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a functional extension of the QR decomposition of\nlinear algebra and shows its application to identify independent patterns\n(curves) of COVID-19 mortality in Brazil's states. The problem is treated as a\nsubset selection one, and regions of influence of each pattern are then\ndetermined by fitting the mortality curves of the remaining states to the main\nindependent ones. Three main patterns are detected: (1) a two-peak curve in\ncentral and southern states, (2) a curve with an early single peak concentrated\nin the Amazonian state of Roraima, and (3) a curve with and early peak and a\nlarge recent increase in the Amazonian, northeastern and southeastern states.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 01:27:16 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lucero", "Jorge C.", ""]]}, {"id": "2103.08874", "submitter": "Antonio Elias Fernandez", "authors": "Yasser Aleman-Gomez (1), Ana Arribas-Gil (2), Manuel Desco (3 and 4),\n  Antonio Elias-Fernandez (5), Juan Romo (5) ((1) Medical Image Analysis\n  Laboratory, University of Lausanne, Lausanne, Switzerland, (2) Instituto\n  UC3M-Santander de Big Data, Universidad Carlos III de Madrid, Getafe, Spain,\n  (3) Biomedical Imaging and lnstrumentation Group, Hospital General\n  Universitario Gregorio Mara\\~non, Madrid, Spain, (4) Departamento de\n  Bioingenieria e Ingenieria Aeroespacial, Universidad Carlos III de Madrid,\n  Getafe, Spain (5) Departamento de Estadistica, Universidad Carlos III de\n  Madrid, Getafe, Spain)", "title": "Visualizing Outliers in High Dimensional Functional Data for Task fMRI\n  data exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Task-based functional magnetic resonance imaging (task fMRI) is a\nnon-invasive technique that allows identifying brain regions whose activity\nchanges when individuals are asked to perform a given task. This contributes to\nthe understanding of how the human brain is organized in functionally distinct\nsubdivisions. Task fMRI experiments from high-resolution scans provide hundred\nof thousands of longitudinal signals for each individual, corresponding to\nmeasurements of brain activity over each voxel of the brain along the duration\nof the experiment. In this context, we propose some visualization techniques\nfor high dimensional functional data relying on depth-based notions that allow\nfor computationally efficient 2-dim representations of tfMRI data and that shed\nlight on sample composition, outlier presence and individual variability. We\nbelieve that this step is crucial previously to any inferential approach\nwilling to identify neuroscientific patterns across individuals, tasks and\nbrain regions. We illustrate the proposed technique through a simulation study\nand demonstrate its application on a motor and language task fMRI experiment.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:49:45 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Aleman-Gomez", "Yasser", "", "3 and 4"], ["Arribas-Gil", "Ana", "", "3 and 4"], ["Desco", "Manuel", "", "3 and 4"], ["Elias-Fernandez", "Antonio", ""], ["Romo", "Juan", ""]]}, {"id": "2103.08951", "submitter": "Lisa Schut", "authors": "Lisa Schut, Oscar Key, Rory McGrath, Luca Costabello, Bogdan\n  Sacaleanu, Medb Corcoran and Yarin Gal", "title": "Generating Interpretable Counterfactual Explanations By Implicit\n  Minimisation of Epistemic and Aleatoric Uncertainties", "comments": "21 pages, 13 Figures", "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactual explanations (CEs) are a practical tool for demonstrating why\nmachine learning classifiers make particular decisions. For CEs to be useful,\nit is important that they are easy for users to interpret. Existing methods for\ngenerating interpretable CEs rely on auxiliary generative models, which may not\nbe suitable for complex datasets, and incur engineering overhead. We introduce\na simple and fast method for generating interpretable CEs in a white-box\nsetting without an auxiliary model, by using the predictive uncertainty of the\nclassifier. Our experiments show that our proposed algorithm generates more\ninterpretable CEs, according to IM1 scores, than existing methods.\nAdditionally, our approach allows us to estimate the uncertainty of a CE, which\nmay be important in safety-critical applications, such as those in the medical\ndomain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:20:24 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Schut", "Lisa", ""], ["Key", "Oscar", ""], ["McGrath", "Rory", ""], ["Costabello", "Luca", ""], ["Sacaleanu", "Bogdan", ""], ["Corcoran", "Medb", ""], ["Gal", "Yarin", ""]]}, {"id": "2103.08973", "submitter": "James Durant", "authors": "James H. Durant, Lucas Wilkins, Keith Butler, Joshaniel F. K. Cooper", "title": "Determining the maximum information gain and optimising experimental\n  design in neutron reflectometry using the Fisher information", "comments": "Revised submission to the Journal of Applied Crystallography", "journal-ref": null, "doi": "10.1107/S160057672100563X", "report-no": null, "categories": "physics.data-an cond-mat.soft physics.comp-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An approach based on the Fisher information (FI) is developed to quantify the\nmaximum information gain and optimal experimental design in neutron\nreflectometry experiments. In these experiments, the FI can be analytically\ncalculated and used to provide sub-second predictions of parameter\nuncertainties. This approach can be used to influence real-time decisions about\nmeasurement angle, measurement time, contrast choice and other experimental\nconditions based on parameters of interest. The FI provides a lower bound on\nparameter estimation uncertainties and these are shown to decrease with the\nsquare root of measurement time, providing useful information for the planning\nand scheduling of experimental work. As the FI is computationally inexpensive\nto calculate, it can be computed repeatedly during the course of an experiment,\nsaving costly beam time by signalling that sufficient data has been obtained;\nor saving experimental datasets by signalling that an experiment needs to\ncontinue. The approach's predictions are validated through the introduction of\nan experiment simulation framework that incorporates instrument-specific\nincident flux profiles, and through the investigation of measuring the\nstructural properties of a phospholipid bilayer.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:01:19 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 12:29:06 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 11:05:04 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Durant", "James H.", ""], ["Wilkins", "Lucas", ""], ["Butler", "Keith", ""], ["Cooper", "Joshaniel F. K.", ""]]}, {"id": "2103.09316", "submitter": "Olanrewaju Akande", "authors": "Zhenhua Wang, Olanrewaju Akande, Jason Poulos and Fan Li", "title": "Are deep learning models superior for missing data imputation in large\n  surveys? Evidence from an empirical comparison", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) is the state-of-the-art approach for dealing with\nmissing data arising from non-response in sample surveys. Multiple imputation\nby chained equations (MICE) is the most widely used MI method, but it lacks\ntheoretical foundation and is computationally intensive. Recently, MI methods\nbased on deep learning models have been developed with encouraging results in\nsmall studies. However, there has been limited research on systematically\nevaluating their performance in realistic settings comparing to MICE,\nparticularly in large-scale surveys. This paper provides a general framework\nfor using simulations based on real survey data and several performance metrics\nto compare MI methods. We conduct extensive simulation studies based on the\nAmerican Community Survey data to compare repeated sampling properties of four\nmachine learning based MI methods: MICE with classification trees, MICE with\nrandom forests, generative adversarial imputation network, and multiple\nimputation using denoising autoencoders. We find the deep learning based MI\nmethods dominate MICE in terms of computational time; however, MICE with\nclassification trees consistently outperforms the deep learning MI methods in\nterms of bias, mean squared error, and coverage under a range of realistic\nsettings.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:24:04 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Zhenhua", ""], ["Akande", "Olanrewaju", ""], ["Poulos", "Jason", ""], ["Li", "Fan", ""]]}, {"id": "2103.09547", "submitter": "Elias Laurin Meyer", "authors": "Elias Laurin Meyer, Peter Mesenbrink, Cornelia Dunger-Baldauf,\n  Ekkehard Glimm, Yuhan Li, Franz K\\\"onig", "title": "Decision rules for identifying combination therapies in open-entry,\n  randomized controlled platform trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and conduct of platform trials have become increasingly popular\nfor drug development programs, attracting interest from statisticians,\nclinicians and regulatory agencies. Many statistical questions related to\ndesigning platform trials - such as the impact of decision rules, sharing of\ninformation across cohorts, and allocation ratios on operating characteristics\nand error rates - remain unanswered. In many platform trials, the definition of\nerror rates is not straightforward as classical error rate concepts are not\napplicable. In particular, the strict control of the family-wise Type I error\nrate often seems unreasonably rigid. For an open-entry, exploratory platform\ntrial design comparing combination therapies to the respective monotherapies\nand standard-of-care, we define a set of error rates and operating\ncharacteristics and then use these to compare a set of design parameters under\na range of simulation assumptions. When setting up the simulations, we aimed\nfor realistic trial trajectories, e.g. in case one compound is found to be\nsuperior to standard-of-care, it could become the new standard-of-care in\nfuture cohorts. Our results indicate that the method of data sharing, exact\nspecification of decision rules and quality of the biomarker used to make\ninterim decisions all strongly contribute to the operating characteristics of\nthe platform trial. Together with the potential flexibility and complexity of a\nplatform trial, which also impact the achieved operating characteristics, this\nimplies that utmost care needs to be given to evaluation of different\nassumptions and design parameters at the design stage.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:22:15 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Meyer", "Elias Laurin", ""], ["Mesenbrink", "Peter", ""], ["Dunger-Baldauf", "Cornelia", ""], ["Glimm", "Ekkehard", ""], ["Li", "Yuhan", ""], ["K\u00f6nig", "Franz", ""]]}, {"id": "2103.09647", "submitter": "Suzanne Thornton", "authors": "Suzanne Thornton, Dooti Roy, Stephen Parry, Donna LaLonde, Wendy\n  Martinez, Renee Ellis, David Corliss", "title": "Best Practices for Collecting Gender and Sex Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The measurement and analysis of human sex and gender is a nuanced problem\nwith many overlapping considerations including statistical bias, data privacy,\nand the ethical treatment of study subjects. Traditionally, human gender and\nsex have been categorized and measured with respect to an artificial binary\nsystem. The continuation of this tradition persists mainly because it is easy\nto replication and not, as we argue, because it produces the most valuable\nscientific information. Sex and gender identity data is crucial for many\napplications of statistical analysis and many modern scientists acknowledge the\nlimitations of the current system. However, discrimination against sex and\ngender minorities poses very real privacy concerns when collecting and\ndistributing gender and sex data. As such, extra thoughtfulness and care is\nessential to design safe and informative scientific studies. In this paper, we\npresent statistically informed recommendations for the data collection and\nanalysis of human subjects that not only respect each individual's identity and\nprotect their privacy, but also establish standards for collecting higher\nquality data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:42:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Thornton", "Suzanne", ""], ["Roy", "Dooti", ""], ["Parry", "Stephen", ""], ["LaLonde", "Donna", ""], ["Martinez", "Wendy", ""], ["Ellis", "Renee", ""], ["Corliss", "David", ""]]}, {"id": "2103.09705", "submitter": "Jingchen Hu", "authors": "Jingchen Hu, Joerg Drechsler, Hang J. Kim", "title": "Accuracy Gains from Privacy Amplification Through Sampling for\n  Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in differential privacy demonstrated that (sub)sampling can\namplify the level of protection. For example, for $\\epsilon$-differential\nprivacy and simple random sampling with sampling rate $r$, the actual privacy\nguarantee is approximately $r\\epsilon$, if a value of $\\epsilon$ is used to\nprotect the output from the sample. In this paper, we study whether this\namplification effect can be exploited systematically to improve the accuracy of\nthe privatized estimate. Specifically, assuming the agency has information for\nthe full population, we ask under which circumstances accuracy gains could be\nexpected, if the privatized estimate would be computed on a random sample\ninstead of the full population. We find that accuracy gains can be achieved for\ncertain regimes. However, gains can typically only be expected, if the\nsensitivity of the output with respect to small changes in the database does\nnot depend too strongly on the size of the database. We only focus on\nalgorithms that achieve differential privacy by adding noise to the final\noutput and illustrate the accuracy implications for two commonly used\nstatistics: the mean and the median. We see our research as a first step\ntowards understanding the conditions required for accuracy gains in practice\nand we hope that these findings will stimulate further research broadening the\nscope of differential privacy algorithms and outputs considered.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:02:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hu", "Jingchen", ""], ["Drechsler", "Joerg", ""], ["Kim", "Hang J.", ""]]}, {"id": "2103.09718", "submitter": "Zhaoxia Yu", "authors": "Dustin Pluta, Xiangmin Xu, Daniel L. Gillen, Zhaoxia Yu", "title": "A Measurement of In-Betweenness and Inference Based on Shape Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a statistical framework to investigate whether a given\nsubpopulation lies between two other subpopulations in a multivariate feature\nspace. This methodology is motivated by a biological question from a\ncollaborator: Is a newly discovered cell type between two known types in\nseveral given features? We propose two in-betweenness indices (IBI) to quantify\nthe in-betweenness exhibited by a random triangle formed by the summary\nstatistics of the three subpopulations. Statistical inference methods are\nprovided for triangle shape and IBI metrics. The application of our methods is\ndemonstrated in three examples: the classic Iris data set, a study of risk of\nrelapse across three breast cancer subtypes, and the motivating neuronal cell\ndata with measured electrophysiological features.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:22:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pluta", "Dustin", ""], ["Xu", "Xiangmin", ""], ["Gillen", "Daniel L.", ""], ["Yu", "Zhaoxia", ""]]}, {"id": "2103.09872", "submitter": "Yves Rozenholc", "authors": "Dorota Desaulle, C\\'eline Hoffmann, Bernard Hainque and Yves Rozenholc", "title": "Differential analysis in Transcriptomic: The strength of randomly\n  picking 'reference' genes", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptomic analysis are characterized by being not directly quantitative\nand only providing relative measurements of expression levels up to an unknown\nindividual scaling factor. This difficulty is enhanced for differential\nexpression analysis. Several methods have been proposed to circumvent this lack\nof knowledge by estimating the unknown individual scaling factors however, even\nthe most used one, are suffering from being built on hardly justifiable\nbiological hypotheses or from having weak statistical background. Only two\nmethods withstand this analysis: one based on largest connected graph component\nhardly usable for large amount of expressions like in NGS, the second based on\n$\\log$-linear fits which unfortunately require a first step which uses one of\nthe methods described before.\n  We introduce a new procedure for differential analysis in the context of\ntranscriptomic data. It is the result of pooling together several differential\nanalyses each based on randomly picked genes used as reference genes. It\nprovides a differential analysis free from the estimation of the individual\nscaling factors or any other knowledge. Theoretical properties are investigated\nboth in term of FWER and power. Moreover in the context of Poisson or negative\nbinomial modelization of the transcriptomic expressions, we derived a test with\nnon asymptotic control of its bounds. We complete our study by some empirical\nsimulations and apply our procedure to a real data set of hepatic miRNA\nexpressions from a mouse model of non-alcoholic steatohepatitis (NASH), the\nCDAHFD model. This study on real data provides new hits with good biological\nexplanations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:18:44 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 14:14:19 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 01:57:15 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Desaulle", "Dorota", ""], ["Hoffmann", "C\u00e9line", ""], ["Hainque", "Bernard", ""], ["Rozenholc", "Yves", ""]]}, {"id": "2103.09974", "submitter": "Lingsong Meng", "authors": "Lingsong Meng, Dorina Avram, George Tseng, Zhiguang Huo", "title": "Outcome-guided Sparse K-means for Disease Subtype Discovery via\n  Integrating Phenotypic Data with High-dimensional Transcriptomic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discovery of disease subtypes is an essential step for developing\nprecision medicine, and disease subtyping via omics data has become a popular\napproach. While promising, subtypes obtained from current approaches are not\nnecessarily associated with clinical outcomes. With the rich clinical data\nalong with the omics data in modern epidemiology cohorts, it is urgent to\ndevelop an outcome-guided clustering algorithm to fully integrate the\nphenotypic data with the high-dimensional omics data. Hence, we extended a\nsparse K-means method to an outcome-guided sparse K-means (GuidedSparseKmeans)\nmethod, which incorporated a phenotypic variable from the clinical dataset to\nguide gene selections from the high-dimensional omics data. We demonstrated the\nsuperior performance of the GuidedSparseKmeans by comparing with existing\nclustering methods in simulations and applications of high-dimensional\ntranscriptomic data of breast cancer and Alzheimer's disease. Our algorithm has\nbeen implemented into an R package, which is publicly available on GitHub\n(https://github.com/LingsongMeng/GuidedSparseKmeans).\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 01:35:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Meng", "Lingsong", ""], ["Avram", "Dorina", ""], ["Tseng", "George", ""], ["Huo", "Zhiguang", ""]]}, {"id": "2103.10077", "submitter": "Tomas Masak", "authors": "Tomas Masak, Tomas Rubin, Victor Panaretos", "title": "Inference and Computation for Sparsely Sampled Random Surfaces", "comments": "34 pages, 9 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-parametric inference for functional data over two-dimensional domains\nentails additional computational and statistical challenges, compared to the\none-dimensional case. Separability of the covariance is commonly assumed to\naddress these issues in the densely observed regime. Instead, we consider the\nsparse regime, where the latent surfaces are observed only at few irregular\nlocations with additive measurement error, and propose an estimator of\ncovariance based on local linear smoothers. Consequently, the assumption of\nseparability reduces the intrinsically four-dimensional smoothing problem into\nseveral two-dimensional smoothers and allows the proposed estimator to retain\nthe classical minimax-optimal convergence rate for two-dimensional smoothers.\nEven when separability fails to hold, imposing it can be still advantageous as\na form of regularization. A simulation study reveals a favorable bias-variance\ntrade-off and massive speed-ups achieved by our approach. Finally, the proposed\nmethodology is used for qualitative analysis of implied volatility surfaces\ncorresponding to call options, and for prediction of the latent surfaces based\non information from the entire data set, allowing for uncertainty\nquantification. Our cross-validated out-of-sample quantitative results show\nthat the proposed methodology outperforms the common approach of pre-smoothing\nevery implied volatility surface separately.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:20:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Masak", "Tomas", ""], ["Rubin", "Tomas", ""], ["Panaretos", "Victor", ""]]}, {"id": "2103.10122", "submitter": "Abderrahim Halimi", "authors": "Abderrahim Halimi, Aurora Maccarone, Robert Lamb, Gerald S. Buller,\n  Stephen McLaughlin", "title": "Robust and Guided Bayesian Reconstruction of Single-Photon 3D Lidar\n  Data: Application to Multispectral and Underwater Imaging", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D Lidar imaging can be a challenging modality when using multiple\nwavelengths, or when imaging in high noise environments (e.g., imaging through\nobscurants). This paper presents a hierarchical Bayesian algorithm for the\nrobust reconstruction of multispectral single-photon Lidar data in such\nenvironments. The algorithm exploits multi-scale information to provide robust\ndepth and reflectivity estimates together with their uncertainties to help with\ndecision making. The proposed weight-based strategy allows the use of available\nguide information that can be obtained by using state-of-the-art learning based\nalgorithms. The proposed Bayesian model and its estimation algorithm are\nvalidated on both synthetic and real images showing competitive results\nregarding the quality of the inferences and the computational complexity when\ncompared to the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 09:50:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Halimi", "Abderrahim", ""], ["Maccarone", "Aurora", ""], ["Lamb", "Robert", ""], ["Buller", "Gerald S.", ""], ["McLaughlin", "Stephen", ""]]}, {"id": "2103.10251", "submitter": "Anthony Strittmatter", "authors": "Tobias Cagala, Ulrich Glogowsky, Johannes Rincke, Anthony Strittmatter", "title": "Optimal Targeting in Fundraising: A Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ineffective fundraising lowers the resources charities can use for goods\nprovision. We combine a field experiment and a causal machine-learning approach\nto increase a charity's fundraising effectiveness. The approach optimally\ntargets fundraising to individuals whose expected donations exceed solicitation\ncosts. Among past donors, optimal targeting substantially increases donations\n(net of fundraising costs) relative to benchmarks that target everybody or no\none. Instead, individuals who were previously asked but never donated should\nnot be targeted. Further, the charity requires only publicly available\ngeospatial information to realize the gains from targeting. We conclude that\ncharities not engaging in optimal targeting waste resources.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:06:35 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 14:54:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cagala", "Tobias", ""], ["Glogowsky", "Ulrich", ""], ["Rincke", "Johannes", ""], ["Strittmatter", "Anthony", ""]]}, {"id": "2103.10329", "submitter": "Heyang Thomas Li", "authors": "Glenn R. Myers, Andrew M. Kingston, Shane J. Latham, Benoit Recur,\n  Thomas Li, Michael L. Turner, Levi Beeching, and Adrian P. Sheppard", "title": "Rapidly-converging multigrid reconstruction of cone-beam tomographic\n  data", "comments": "7 pages, 4 figures", "journal-ref": "Developments in X-Ray tomography X. Vol. 9967. International\n  Society for Optics and Photonics, 2016", "doi": null, "report-no": null, "categories": "physics.med-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the context of large-angle cone-beam tomography (CBCT), we present a\npractical iterative reconstruction (IR) scheme designed for rapid convergence\nas required for large datasets. The robustness of the reconstruction is\nprovided by the \"space-filling\" source trajectory along which the experimental\ndata is collected. The speed of convergence is achieved by leveraging the\nhighly isotropic nature of this trajectory to design an approximate\ndeconvolution filter that serves as a pre-conditioner in a multi-grid scheme.\nWe demonstrate this IR scheme for CBCT and compare convergence to that of more\ntraditional techniques.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:20:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Myers", "Glenn R.", ""], ["Kingston", "Andrew M.", ""], ["Latham", "Shane J.", ""], ["Recur", "Benoit", ""], ["Li", "Thomas", ""], ["Turner", "Michael L.", ""], ["Beeching", "Levi", ""], ["Sheppard", "Adrian P.", ""]]}, {"id": "2103.10335", "submitter": "Jethro Browell", "authors": "Jethro Browell and Matteo Fasiolo", "title": "Probabilistic Forecasting of Regional Net-load with Conditional Extremes\n  and Gridded NWP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing penetration of embedded renewables makes forecasting net-load,\nconsumption less embedded generation, a significant and growing challenge. Here\na framework for producing probabilistic forecasts of net-load is proposed with\nparticular attention given to the tails of predictive distributions, which are\nrequired for managing risk associated with low-probability events. Only small\nvolumes of data are available in the tails, by definition, so estimation of\npredictive models and forecast evaluation requires special attention. We\npropose a solution based on a best-in-class load forecasting methodology\nadapted for net-load, and model the tails of predictive distributions with the\nGeneralised Pareto Distribution, allowing its parameters to vary smoothly as\nfunctions of covariates. The resulting forecasts are shown to be calibrated and\nsharper than those produced with unconditional tail distributions. In a\nuse-case inspired evaluation exercise based on reserve setting, the conditional\ntails are shown to reduce the overall volume of reserve required to manage a\ngiven risk. Furthermore, they identify periods of high risk not captured by\nother methods. The proposed method therefore enables user to both reduce costs\nand avoid excess risk.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:52:43 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 14:05:19 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Browell", "Jethro", ""], ["Fasiolo", "Matteo", ""]]}, {"id": "2103.10337", "submitter": "Anna Petrovskaia", "authors": "Anna Petrovskaia, Gleb Ryzhakov, Ivan Oseledets", "title": "Optimal soil sampling design based on the maxvol algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial soil sampling is an integral part of a soil survey aimed at creating\na soil map. We propose considering the soil sampling procedure as a task of\noptimal design. In practical terms, optimal experiments can reduce\nexperimentation costs, as they allow the researcher to obtain one optimal set\nof points. We present a sampling design, based on the fundamental idea of\nselecting sample locations by performing an optimal design method called the\nmaxvol algorithm. It is shown that the maxvol-base algorithm has a high\npotential for practical usage. Our method outperforms popular sampling methods\nin soil taxa prediction based on topographical features of the site and deals\nwith massive agricultural datasets in a reasonable time.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:54:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Petrovskaia", "Anna", ""], ["Ryzhakov", "Gleb", ""], ["Oseledets", "Ivan", ""]]}, {"id": "2103.10463", "submitter": "Andr\\'e Gillibert", "authors": "Andr\\'e Gillibert (1 and 2), Jacques B\\'enichou (2 and 3), Bruno\n  Falissard (1) ((1) INSERM UMR 1178, Universit\\'e Paris Sud, Maison de Solenn,\n  Paris, France (2) Department of Biostatistics and Clinical Research, CHU\n  Rouen, Rouen, France (3) Inserm U 1219, Normandie University, Rouen, France)", "title": "Two-sided confidence interval of a binomial proportion: how to choose?", "comments": "20 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduction: estimation of confidence intervals (CIs) of binomial\nproportions has been reviewed more than once but the directional\ninterpretation, distinguishing the overestimation from the underestimation, was\nneglected while the sample size and theoretical proportion variances from\nexperiment to experiment have not been formally taken in account. Herein, we\ndefine and apply new evaluation criteria, then give recommendations for the\npractical use of these CIs.\n  Materials & methods: Google Scholar was used for bibliographic research.\nEvaluation criteria were (i) one-sided conditional errors, (ii) one-sided local\naverage errors assuming a random theoretical proportion and (iii) expected\nhalf-widths of CIs.\n  Results: Wald's CI did not control any of the risks, even when the expected\nnumber of successes reached 32. The likelihood ratio CI had a better balance\nthan the logistic Wald CI. The Clopper-Pearson mid-P CI controlled well\none-sided local average errors whereas the simple Clopper-Pearson CI was\nstrictly conservative on both one-sided conditional errors. The percentile and\nbasic bootstrap CIs had the same bias order as Wald's CI whereas the\nstudentized CIs and BCa, modified for discrete bootstrap distributions, were\nless biased but not as efficient as the parametric methods. The half-widths of\nCIs mirrored local average errors.\n  Conclusion: we recommend using the Clopper-Pearson mid-P CI for the\nestimation of a proportion except for observed-theoretical proportion\ncomparison under controlled experimental conditions in which the\nClopper-Pearson CI may be better.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 14:53:13 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Gillibert", "Andr\u00e9", "", "1 and 2"], ["B\u00e9nichou", "Jacques", "", "2 and 3"], ["Falissard", "Bruno", ""]]}, {"id": "2103.10477", "submitter": "Nicholas Boyd", "authors": "Nicholas Boyd, Kalim Mir", "title": "Sequencing by Emergence: Modeling and Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequencing by Emergence (SEQE) is a new single-molecule nucleic acid\n(DNA/RNA) sequencing technology that estimates sequence as an emergent property\nof the binding and localization of a repertoire of short oligonucleotide\nprobes. SEQE promises to deliver accurate, ultra-long, haplotype-phased reads\nat the whole genome-scale for very low cost within 10 minutes. The data SEQE\ngenerates requires entirely new inference techniques. In this paper we\nintroduce a probabilistic model of the SEQE measurement process and an\nalgorithm that estimates sequence by solving a convex relaxation of the\ncorresponding maximum likelihood problem. We demonstrate the effectiveness of\nour algorithm on a variety of simulated datasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:55:13 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Boyd", "Nicholas", ""], ["Mir", "Kalim", ""]]}, {"id": "2103.10535", "submitter": "Mario Marino", "authors": "Mario Marino, Susanna Levantesi, Andrea Nigri", "title": "Deepening Lee-Carter for longevity projections with uncertainty\n  estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undoubtedly, several countries worldwide endure to experience a continuous\nincrease in life expectancy, extending the challenges of life actuaries and\ndemographers in forecasting mortality. Although several stochastic mortality\nmodels have been proposed in past literature, the mortality forecasting\nresearch remains a crucial task. Recently, various research works encourage the\nadequacy of deep learning models to extrapolate suitable pattern within\nmortality data. Such a learning models allow to achieve accurate point\npredictions, albeit also uncertainty measures are necessary to support both\nmodel estimates reliability and risk evaluations. To the best of our knowledge,\nmachine and deep learning literature in mortality forecasting lack for studies\nabout uncertainty estimation. As new advance in mortality forecasting, we\nformalizes the deep Neural Networks integration within the Lee-Carter\nframework, posing a first bridge between the deep learning and the mortality\ndensity forecasts. We test our model proposal in a numerical application\nconsidering three representative countries worldwide and both genders,\nscrutinizing two different fitting periods. Exploiting the meaning of both\nbiological reasonableness and plausibility of forecasts, as well as performance\nmetrics, our findings confirm the suitability of deep learning models to\nimprove the predictive capacity of the Lee-Carter model, providing more\nreliable mortality boundaries also on the long-run.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:35:25 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Marino", "Mario", ""], ["Levantesi", "Susanna", ""], ["Nigri", "Andrea", ""]]}, {"id": "2103.10605", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe, Maximilian G\\\"obel", "title": "On Spurious Causality, CO2, and Global Temperature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stips, Macias, Coughlan, Garcia-Gorriz, and Liang (2016, Nature Scientific\nReports) use information flows (Liang, 2008, 2014) to establish causality from\nvarious forcings to global temperature. We show that the formulas being used\nhinges on a simplifying assumption that is nearly always rejected by the data.\nWe propose an adequate measure of information flow based on Vector\nAutoregressions, and find that most results in Stips et al. (2016) cannot be\ncorroborated. Then, it is discussed which modeling choices (e.g., the choice of\nCO2 series and assumptions about simultaneous relationships) may help in\nextracting credible estimates of causal flows and the transient climate\nresponse simply by looking at the joint dynamics of two climatic time series.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:58:06 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["G\u00f6bel", "Maximilian", ""]]}, {"id": "2103.10712", "submitter": "Jos\\'e Ant\\'onio Ferreira", "authors": "Jos\\'e A. Ferreira", "title": "Comments on \"Factors associated with the spatial heterogeneity of the\n  first wave of COVID-19 in France: a nationwide geo-epidemiological study\" by\n  Gaudart et al", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a recent paper, Jean Gaudart and colleagues studied the factors associated\nwith the spatial heterogeneity of the first wave of COVID-19 in France. We make\nsome critical comments on their work which may be useful for future, similar\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:57:50 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Ferreira", "Jos\u00e9 A.", ""]]}, {"id": "2103.10885", "submitter": "Yangxinyu Xie", "authors": "Ngoc Mai Tran, Evdokia Nikolova, David Kulpanowski, Yangxinyu Xie,\n  Joshua Ong", "title": "Predicting Covid-19 EMS Incidents from Daily Hospitalization Trends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introduction: The aim of our retrospective study was to quantify the impact\nof Covid-19 on the temporal distribution of Emergency Medical Services (EMS)\ndemand in Travis County, Austin, Texas and propose a robust model to forecast\nCovid-19 EMS incidents. Methods: We analyzed the temporal distribution of EMS\ncalls in the Austin-Travis County area between January 1st, 2019 and December\n31st, 2020. Change point detection was performed to identify critical dates\nmarking changes in EMS call distributions and time series regression was\napplied for forecasting Covid-19 EMS incidents. Results: Two critical dates\nmarked the impact of Covid-19 on the distribution of EMS calls: March 17th,\nwhen the daily number of non-pandemic EMS incidents dropped significantly, and\nMay 13th, by which the daily number of EMS calls climbed back to 75% of the\nnumber in pre-Covid-19 time. New daily count of the hospitalization of Covid-19\npatients alone proves a powerful predictor of the number of pandemic EMS calls,\nwith an r2 value equal to 0.85. In particular, for every 2.5 cases where EMS\ntakes a Covid-19 patient to a hospital, 1 person is admitted. Conclusion: The\nmean daily number of non-pandemic EMS demand was significantly less than the\nperiod prior to Covid-19 pandemic. The number of EMS calls for Covid-19\nsymptoms can be predicted from the daily new hospitalization of Covid-19\npatients. These findings may be of interest to EMS departments as they plan for\nfuture pandemics, including the ability to predict pandemic-related calls in an\neffort to adjust a targeted response.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:09:38 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 16:34:39 GMT"}, {"version": "v3", "created": "Sun, 23 May 2021 23:42:41 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 18:15:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Tran", "Ngoc Mai", ""], ["Nikolova", "Evdokia", ""], ["Kulpanowski", "David", ""], ["Xie", "Yangxinyu", ""], ["Ong", "Joshua", ""]]}, {"id": "2103.10901", "submitter": "Hanjia Lyu", "authors": "Tanqiu Jiang, Sidhant K. Bendre, Hanjia Lyu, Jiebo Luo", "title": "From Static to Dynamic Prediction: Wildfire Risk Assessment Based on\n  Multiple Environmental Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wildfire is one of the biggest disasters that frequently occurs on the west\ncoast of the United States. Many efforts have been made to understand the\ncauses of the increases in wildfire intensity and frequency in recent years. In\nthis work, we propose static and dynamic prediction models to analyze and\nassess the areas with high wildfire risks in California by utilizing a\nmultitude of environmental data including population density, Normalized\nDifference Vegetation Index (NDVI), Palmer Drought Severity Index (PDSI), tree\nmortality area, tree mortality number, and altitude. Moreover, we focus on a\nbetter understanding of the impacts of different factors so as to inform\npreventive actions. To validate our models and findings, we divide the land of\nCalifornia into 4,242 grids of 0.1 degrees $\\times$ 0.1 degrees in latitude and\nlongitude, and compute the risk of each grid based on spatial and temporal\nconditions. By performing counterfactual analysis, we uncover the effects of\nseveral possible methods on reducing the number of high risk wildfires. Taken\ntogether, our study has the potential to estimate, monitor, and reduce the\nrisks of wildfires across diverse areas provided that such environment data is\navailable.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:56:17 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Jiang", "Tanqiu", ""], ["Bendre", "Sidhant K.", ""], ["Lyu", "Hanjia", ""], ["Luo", "Jiebo", ""]]}, {"id": "2103.10912", "submitter": "Sen Hu", "authors": "Sen Hu, Adrian O'Hagan", "title": "Copula Averaging for Tail Dependence in Insurance Claims Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysing dependent risks is an important task for insurance companies. A\ndependency is reflected in the fact that information about one random variable\nprovides information about the likely distribution of values of another random\nvariable. Insurance companies in particular must investigate such dependencies\nbetween different lines of business and the effects that an extreme loss event,\nsuch as an earthquake or hurricane, has across multiple lines of business\nsimultaneously. Copulas provide a popular model-based approach to analysing the\ndependency between risks, and the coefficient of tail dependence is a measure\nof dependence for extreme losses. Besides commonly used empirical estimators\nfor estimating the tail dependence coefficient, copula fitting can lead to\nestimation of such coefficients directly or can verify their existence.\nGenerally, a range of copula models is available to fit a data set well,\nleading to multiple different tail dependence results; a method based on\nBayesian model averaging is designed to obtain a unified estimate of tail\ndependence. In this article, this model-based coefficient estimation method is\nillustrated through a variety of copula fitting approaches and results are\npresented for several simulated data sets and also a real general insurance\nloss data set.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:16:22 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Hu", "Sen", ""], ["O'Hagan", "Adrian", ""]]}, {"id": "2103.10933", "submitter": "Joshua Carmichael", "authors": "Joshua D Carmichael", "title": "Hypothesis Tests on Rayleigh Wave Radiation Pattern Shapes: A\n  Theoretical Assessment of Idealized Source Screening", "comments": "This manuscript has been authored with number LA-UR-20-27299 by Triad\n  National Security under Contract with the U.S. Department of Energy, Office\n  of Defense Nuclear Nonproliferation Research and Development", "journal-ref": null, "doi": null, "report-no": "LA-UR-20-27299", "categories": "physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Shallow seismic sources excite Rayleigh wave ground motion with azimuthally\ndependent radiation patterns. We place binary hypothesis tests on theoretical\nmodels of such radiation patterns to screen cylindrically symmetric sources\n(like explosions) from non-symmetric sources (like non-vertical dip-slip, or\nnon-VDS faults). These models for data include sources with several unknown\nparameters, contaminated by Gaussian noise and embedded in a layered\nhalf-space. The generalized maximum likelihood ratio tests that we derive from\nthese data models produce screening statistics and decision rules that depend\non measured, noisy ground motion at discrete sensor locations. We explicitly\nquantify how the screening power of these statistics increase with the size of\nany dip-slip and strike-slip components of the source, relative to noise\n(faulting signal strength), and how they vary with network geometry. As\napplications of our theory, we apply these tests to (1) find optimal sensor\nlocations that maximize the probability of screening non-circular radiation\npatterns, and (2) invert for the largest non-VDS faulting signal that could be\nmistakenly attributed to an explosion with damage, at a particular attribution\nprobability. Lastly, we quantify how certain errors that are sourced by opening\ncracks increase screening rate errors. While such theoretical solutions are\nideal and require future validation, they remain important in underground\nexplosion monitoring scenarios because they provide fundamental physical limits\non the discrimination power of tests that screen explosive from non-VDS\nfaulting sources.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2021 16:49:21 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Carmichael", "Joshua D", ""]]}, {"id": "2103.10936", "submitter": "Emmanuel Agbo", "authors": "Emmanuel Agbo", "title": "Forecasting of Meteorological variables using statistical methods and\n  tools", "comments": "20 pages, 7 figures, 5 tables, 5910 words", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to understand the role of statistical methods for the forecasting of\nclimatological parameters cannot be trivialized. This study gives an in depth\nreview on the different variations of the Mann-Kendall (M-K) trend test and how\nthey can be applied, regression techniques (Simple and Multiple), the\nAngstrom-Prescott model for solar radiation, etc. The study then goes ahead to\napply some of them with data obtained from the Nigerian Meteorological Agency\n(NiMet), and applying tools like the python programming language and Wolfram\nMathematica. Results show that the maximum ambient temperature for Calabar is\nincreasing (Z=2.52) significantly after the calculated p-value < 0.05\n(significant level). The seasonal M-K test was also applied for the dry and wet\nseasons and both were found to be increasing (Z=3.23 and Z=4.04 respectively)\nafter their calculated p-values < 0.05. The relationship between refractivity\nand other meteorological parameters relating to it was discerned using partial\ndifferential equations giving the gradient of each with refractivity; this was\ncompared with results from the correlation matrix to show that the water vapour\ncontents of the atmosphere contributes significantly to the variation of\nrefractivity. Multiple linear regression has also been adopted to give an\naccurate model for the prediction of refractivity in the region after the\nresidual error between the calculated refractivity and predicted refractivity\nwas minimal.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 13:41:19 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Agbo", "Emmanuel", ""]]}, {"id": "2103.10944", "submitter": "Buddhika Nettasinghe", "authors": "Buddhika Nettasinghe, Nazanin Alipourfard, Vikram Krishnamurthy,\n  Kristina Lerman", "title": "Emergence of Structural Inequalities in Scientific Citation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY cs.DL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural inequalities persist in society, conferring systematic advantages\nto some people at the expense of others, for example, by giving them\nsubstantially more influence and opportunities. Using bibliometric data about\nauthors of scientific publications, we identify two types of structural\ninequalities in scientific citations. First, female authors, who represent a\nminority of researchers, receive less recognition for their work (through\ncitations) relative to male authors; second, authors affiliated with top-ranked\ninstitutions, who are also a minority, receive substantially more recognition\ncompared to other authors. We present a model for the growth of directed\ncitation networks and show that citations disparities arise from individual\npreferences to cite authors from the same group (homophily), highly cited or\nactive authors (preferential attachment), as well as the size of the group and\nhow frequently new authors join. We analyze the model and show that its\npredictions align well with real-world observations. Our theoretical and\nempirical analysis also suggests potential strategies to mitigate structural\ninequalities in science. In particular, we find that merely increasing the\nminority group size does little to narrow the disparities. Instead, reducing\nthe homophily of each group, frequently adding new authors to a research field\nwhile providing them an accessible platform among existing, established\nauthors, together with balanced group sizes can have the largest impact on\nreducing inequality. Our work highlights additional complexities of mitigating\nstructural disparities stemming from asymmetric relations (e.g., directed\ncitations) compared to symmetric relations (e.g., collaborations).\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:53:08 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 02:52:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nettasinghe", "Buddhika", ""], ["Alipourfard", "Nazanin", ""], ["Krishnamurthy", "Vikram", ""], ["Lerman", "Kristina", ""]]}, {"id": "2103.11125", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Zhi Li, Dandan Zeng, Yongliang Wang", "title": "Mining geometric constraints from crowd-sourced radio signals and its\n  application to indoor positioning", "comments": "20 pages, 11 figures, accepted to publish on IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-sourcing has become a promising way to build} a feature-based indoor\npositioning system that has lower labour and time costs. It can make full use\nof the widely deployed infrastructure as well as built-in sensors on mobile\ndevices. One of the key challenges is to generate the reference feature map\n(RFM), a database used for localization, by {aligning crowd-sourced\n{trajectories according to associations embodied in the data. In order to\nfacilitate the data fusion using crowd-sourced inertial sensors and radio\nsignals, this paper proposes an approach to adaptively mining geometric\ninformation. This is the essential for generating spatial associations between\ntrajectories when employing graph-based optimization methods. The core idea is\nto estimate the functional relationship to map the similarity/dissimilarity\nbetween radio signals to the physical space based on the relative positions\nobtained from inertial sensors and their associated radio signals. Namely, it\nis adaptable to different modalities of data and can be implemented in a\nself-supervised way. We verify the generality of the proposed approach through\ncomprehensive experimental analysis: i) qualitatively comparing the estimation\nof geometric mapping models and the alignment of crowd-sourced trajectories;\nii) quantitatively evaluating the positioning performance. The 68\\% of the\npositioning error is less than 4.7 $\\mathrm{m}$ using crowd-sourced RFM, which\nis on a par with manually collected RFM, in a multi-storey shopping mall, which\ncovers more than 10, 000 $ \\mathrm{m}^2 $.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:42:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Caifa", ""], ["Li", "Zhi", ""], ["Zeng", "Dandan", ""], ["Wang", "Yongliang", ""]]}, {"id": "2103.11170", "submitter": "Rebecca Anthopolos", "authors": "Rebecca Anthopolos and Ying Wei and Qixuan Chen", "title": "Modeling Heterogeneity and Missing Data of Multiple Longitudinal\n  Outcomes in Electronic Health Records", "comments": "Main text: 15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In electronic health records (EHRs), latent subgroups of patients may exhibit\ndistinctive patterning in their longitudinal health trajectories. For such\ndata, growth mixture models (GMMs) enable classifying patients into different\nlatent classes based on individual trajectories and hypothesized risk factors.\nHowever, the application of GMMs is hindered by the special missing data\nproblem in EHRs, which manifests two patient-led missing data processes: the\nvisit process and the response process for an EHR variable conditional on a\npatient visiting the clinic. If either process is associated with the process\ngenerating the longitudinal outcomes, then valid inferences require accounting\nfor a nonignorable missing data mechanism. We propose a Bayesian shared\nparameter model that links GMMs of multiple longitudinal health outcomes, the\nvisit process, and the response process of each outcome given a visit using a\ndiscrete latent class variable. Our focus is on multiple longitudinal health\noutcomes for which there can be a clinically prescribed visit schedule. We\ndemonstrate our model in EHR measurements on early childhood weight and height\nz-scores. Using data simulations, we illustrate the statistical properties of\nour method with respect to subgroup-specific or marginal inferences. We built\nthe R package EHRMiss for model fitting, selection, and checking.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:54:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Anthopolos", "Rebecca", ""], ["Wei", "Ying", ""], ["Chen", "Qixuan", ""]]}, {"id": "2103.11254", "submitter": "Shuyu Lu", "authors": "Shuyu Lu, Ruoyu Chen, Wei Wei, Xinghua Lu", "title": "Understanding Heart-Failure Patients EHR Clinical Features via SHAP\n  Interpretation of Tree-Based Machine Learning Model Predictions", "comments": "Submitted to AMIA 2021 Annual Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heart failure (HF) is a major cause of mortality. Accurately monitoring HF\nprogress and adjust therapies are critical for improving patient outcomes. An\nexperienced cardiologist can make accurate HF stage diagnoses based on\ncombination of symptoms, signs, and lab results from the electronic health\nrecords (EHR) of a patient, without directly measuring heart function. We\nexamined whether machine learning models, more specifically the XGBoost model,\ncan accurately predict patient stage based on EHR, and we further applied the\nSHapley Additive exPlanations (SHAP) framework to identify informative features\nand their interpretations. Our results indicate that based on structured data\nfrom EHR, our models could predict patients' ejection fraction (EF) scores with\nmoderate accuracy. SHAP analyses identified informative features and revealed\npotential clinical subtypes of HF. Our findings provide insights on how to\ndesign computing systems to accurately monitor disease progression of HF\npatients through continuously mining patients' EHR data.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 22:17:05 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Shuyu", ""], ["Chen", "Ruoyu", ""], ["Wei", "Wei", ""], ["Lu", "Xinghua", ""]]}, {"id": "2103.11353", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Analysing the restricted assignment problem of the group draw in sports\n  tournaments", "comments": "16 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "05A05, 68U20, 68W40, 91B14", "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many sports tournaments contain a group stage where the allocation of teams\nis subject to some constraints. The standard draw procedure extracts the teams\nfrom pots sequentially and places them in the first available group in\nalphabetical order such that at least one assignment of the teams still to be\ndrawn remains acceptable. We show how this mechanism is connected to generating\npermutations and provide a backtracking algorithm to find the solution for any\ngiven sequence. The consequences of draw restrictions are investigated through\nthe case study of the European Qualifiers for the 2022 FIFA World Cup. We\nquantify the departure of its draw procedure from even distribution and propose\ntwo alternative approaches to increase the excitement of the draw.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:06:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "2103.11539", "submitter": "ShengLi Tzeng", "authors": "Heng-Hui Lue, ShengLi Tzeng", "title": "Interpretable, predictive spatio-temporal models via enhanced Pairwise\n  Directions Estimation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This article concerns the predictive modeling for spatio-temporal data as\nwell as model interpretation using data information in space and time.\nIntrinsically, we develop a novel approach based on dimension reduction for\nsuch data in order to capture nonlinear mean structures without requiring a\nprespecified parametric model. In addition to prediction as a common interest,\nthis approach focuses more on the exploration of geometric information in the\ndata. The method of Pairwise Directions Estimation (PDE) is incorporated in our\napproach to implement the data-driven function searching of spatial structures\nand temporal patterns, useful in exploring data trends. The benefit of using\ngeometrical information from the method of PDE is highlighted. We further\nenhance PDE, referring to it as PDE+, by using resolution adaptive fixed rank\nkriging to estimate the random effects not explained in the mean structures.\nOur proposal can not only produce more accurate and explainable prediction, but\nalso increase the computation efficiency for model building. Several simulation\nexamples are conducted and comparisons are made with four existing methods. The\nresults demonstrate that the proposed PDE+ method is very useful for exploring\nand interpreting the patterns of trend for spatio-temporal data. Illustrative\napplications to two real datasets are also presented.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 02:00:10 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lue", "Heng-Hui", ""], ["Tzeng", "ShengLi", ""]]}, {"id": "2103.11598", "submitter": "Li Yang", "authors": "Li Yang", "title": "Adaptive Degradation Process with Deep Learning-Driven Trajectory", "comments": "This work will be submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remaining useful life (RUL) estimation is a crucial component in the\nimplementation of intelligent predictive maintenance and health management.\nDeep neural network (DNN) approaches have been proven effective in RUL\nestimation due to their capacity in handling high-dimensional non-linear\ndegradation features. However, the applications of DNN in practice face two\nchallenges: (a) online update of lifetime information is often unavailable, and\n(b) uncertainties in predicted values may not be analytically quantified. This\npaper addresses these issues by developing a hybrid DNN-based prognostic\napproach, where a Wiener-based-degradation model is enhanced with adaptive\ndrift to characterize the system degradation. An LSTM-CNN encoder-decoder is\ndeveloped to predict future degradation trajectories by jointly learning noise\ncoefficients as well as drift coefficients, and adaptive drift is updated via\nBayesian inference. A computationally efficient algorithm is proposed for the\ncalculation of RUL distributions. Numerical experiments are presented using\nturbofan engines degradation data to demonstrate the superior accuracy of RUL\nprediction of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 06:00:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yang", "Li", ""]]}, {"id": "2103.11706", "submitter": "Mario W\\\"uthrich V.", "authors": "M. Merz, R. Richman, T. Tsanakas, M.V. W\\\"uthrich", "title": "Interpreting Deep Learning Models with Marginal Attribution by\n  Conditioning on Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vastly growing literature on explaining deep learning models has emerged.\nThis paper contributes to that literature by introducing a global\ngradient-based model-agnostic method, which we call Marginal Attribution by\nConditioning on Quantiles (MACQ). Our approach is based on analyzing the\nmarginal attribution of predictions (outputs) to individual features (inputs).\nSpecificalllly, we consider variable importance by mixing (global) output\nlevels and, thus, explain how features marginally contribute across different\nregions of the prediction space. Hence, MACQ can be seen as a marginal\nattribution counterpart to approaches such as accumulated local effects (ALE),\nwhich study the sensitivities of outputs by perturbing inputs. Furthermore,\nMACQ allows us to separate marginal attribution of individual features from\ninteraction effect, and visually illustrate the 3-way relationship between\nmarginal attribution, output level, and feature value.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:20:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Merz", "M.", ""], ["Richman", "R.", ""], ["Tsanakas", "T.", ""], ["W\u00fcthrich", "M. V.", ""]]}, {"id": "2103.11773", "submitter": "Fan Cheng", "authors": "Fan Cheng, Rob J Hyndman, Anastasios Panagiotelis", "title": "Manifold learning with approximate nearest neighbors", "comments": "46 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Manifold learning algorithms are valuable tools for the analysis of\nhigh-dimensional data, many of which include a step where nearest neighbors of\nall observations are found. This can present a computational bottleneck when\nthe number of observations is large or when the observations lie in more\ngeneral metric spaces, such as statistical manifolds, which require all\npairwise distances between observations to be computed. We resolve this problem\nby using a broad range of approximate nearest neighbor algorithms within\nmanifold learning algorithms and evaluating their impact on embedding accuracy.\nWe use approximate nearest neighbors for statistical manifolds by exploiting\nthe connection between Hellinger/Total variation distance for discrete\ndistributions and the L2/L1 norm. Via a thorough empirical investigation based\non the benchmark MNIST dataset, it is shown that approximate nearest neighbors\nlead to substantial improvements in computational time with little to no loss\nin the accuracy of the embedding produced by a manifold learning algorithm.\nThis result is robust to the use of different manifold learning algorithms, to\nthe use of different approximate nearest neighbor algorithms, and to the use of\ndifferent measures of embedding accuracy. The proposed method is applied to\nlearning statistical manifolds data on distributions of electricity usage. This\napplication demonstrates how the proposed methods can be used to visualize and\nidentify anomalies and uncover underlying structure within high-dimensional\ndata in a way that is scalable to large datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 12:04:23 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Cheng", "Fan", ""], ["Hyndman", "Rob J", ""], ["Panagiotelis", "Anastasios", ""]]}, {"id": "2103.11952", "submitter": "Darren Grant", "authors": "Darren Grant", "title": "Uncovering Bias in Order Assignment", "comments": "29 pages of text, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Many real life situations require a set of items to be repeatedly placed in a\nrandom sequence. In such circumstances, it is often desirable to test whether\nsuch randomization indeed obtains, yet this problem has received very limited\nattention in the literature. This paper articulates the key features of this\nproblem and presents three \"untargeted\" tests that require no a priori\ninformation from the analyst. These methods are used to analyze the order in\nwhich lottery numbers are drawn in Powerball, the order in which contestants\nperform on American Idol, and the order of candidates on primary election\nballots in Texas and West Virginia. In this last application, multiple\ndeviations from full randomization are detected, with potentially serious\npolitical and legal consequences. The form these deviations take varies,\ndepending on institutional factors, which sometimes necessitates the use of\ntests that exchange power for increased robustness.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:47:10 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 20:13:27 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Grant", "Darren", ""]]}, {"id": "2103.12094", "submitter": "Harry Spearing", "authors": "Harry Spearing, Jonathan Tawn, David Irons, Tim Paulden", "title": "Modelling intransitivity in pairwise comparisons with application to\n  baseball data", "comments": "26 pages, 7 figures, 2 tables in the main text. 17 pages in the\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In most commonly used ranking systems, some level of underlying transitivity\nis assumed. If transitivity exists in a system then information about pairwise\ncomparisons can be translated to other linked pairs. For example, if typically\nA beats B and B beats C, this could inform us about the expected outcome\nbetween A and C. We show that in the seminal Bradley-Terry model knowing the\nprobabilities of A beating B and B beating C completely defines the probability\nof A beating C, with these probabilities determined by individual skill levels\nof A, B and C. Users of this model tend not to investigate the validity of this\ntransitive assumption, nor that some skill levels may not be statistically\nsignificantly different from each other; the latter leading to false\nconclusions about rankings. We provide a novel extension to the Bradley-Terry\nmodel, which accounts for both of these features: the intransitive\nrelationships between pairs of objects are dealt with through interaction terms\nthat are specific to each pair; and by partitioning the $n$ skills into\n$A+1\\leq n$ distinct clusters, any differences in the objects' skills become\nsignificant, given appropriate $A$. With $n$ competitors there are $n(n-1)/2$\ninteractions, so even in multiple round robin competitions this gives too many\nparameters to efficiently estimate. Therefore we separately cluster the\n$n(n-1)/2$ values of intransitivity into $K$ clusters, giving $(A,K)$\nestimatable values respectively, typically with $A+K<n$. Using a Bayesian\nhierarchical model, $(A,K)$ are treated as unknown, and inference is conducted\nvia a reversible jump Markov chain Monte Carlo (RJMCMC) algorithm. The model is\nshown to have an improved fit out of sample in both simulated data and when\napplied to American League baseball data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:00:19 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Spearing", "Harry", ""], ["Tawn", "Jonathan", ""], ["Irons", "David", ""], ["Paulden", "Tim", ""]]}, {"id": "2103.12149", "submitter": "Buddhika Nettasinghe", "authors": "Buddhika Nettasinghe, Nazanin Alipourfard, Vikram Krishnamurthy,\n  Kristina Lerman", "title": "A Directed, Bi-Populated Preferential Attachment Model with Applications\n  to Analyzing the Glass Ceiling Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.SY eess.SY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preferential attachment, homophily and, their consequences such as the glass\nceiling effect have been well-studied in the context of undirected networks.\nHowever, the lack of an intuitive, theoretically tractable model of a directed,\nbi-populated~(i.e.,~containing two groups) network with variable levels of\npreferential attachment, homophily and growth dynamics~(e.g.,~the rate at which\nnew nodes join, whether the new nodes mostly follow existing nodes or the\nexisting nodes follow them, etc.) has largely prevented such consequences from\nbeing explored in the context of directed networks, where they more naturally\noccur due to the asymmetry of links. To this end, we present a rigorous\ntheoretical analysis of the \\emph{Directed Mixed Preferential Attachment} model\nand, use it to analyze the glass ceiling effect in directed networks. More\nspecifically, we derive the closed-form expressions for the power-law exponents\nof the in- and out- degree distributions of each group~(minority and majority)\nand, compare them with each other to obtain insights. In particular, our\nresults yield answers to questions such as: \\emph{when does the minority group\nhave a heavier out-degree (or in-degree) distribution compared to the majority\ngroup? what effect does frequent addition of edges between existing nodes have\non the in- and out- degree distributions of the majority and minority groups?}.\nSuch insights shed light on the interplay between the structure~(i.e., the in-\nand out- degree distributions of the two groups) and dynamics~(characterized\ncollectively by the homophily, preferential attachment, group sizes and growth\ndynamics) of various real-world networks. Finally, we utilize the obtained\nanalytical results to characterize the conditions under which the glass ceiling\neffect emerge in a directed network. Our analytical results are supported by\ndetailed numerical results.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 19:48:26 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Nettasinghe", "Buddhika", ""], ["Alipourfard", "Nazanin", ""], ["Krishnamurthy", "Vikram", ""], ["Lerman", "Kristina", ""]]}, {"id": "2103.12198", "submitter": "Jacob Nogas", "authors": "Joseph Jay Williams, Jacob Nogas, Nina Deliu, Hammad Shaikh, Sofia S.\n  Villar, Audrey Durand, Anna Rafferty", "title": "Challenges in Statistical Analysis of Data Collected by a Bandit\n  Algorithm: An Empirical Exploration in Applications to Adaptively Randomized\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-armed bandit algorithms have been argued for decades as useful for\nadaptively randomized experiments. In such experiments, an algorithm varies\nwhich arms (e.g. alternative interventions to help students learn) are assigned\nto participants, with the goal of assigning higher-reward arms to as many\nparticipants as possible. We applied the bandit algorithm Thompson Sampling\n(TS) to run adaptive experiments in three university classes. Instructors saw\ngreat value in trying to rapidly use data to give their students in the\nexperiments better arms (e.g. better explanations of a concept). Our\ndeployment, however, illustrated a major barrier for scientists and\npractitioners to use such adaptive experiments: a lack of quantifiable insight\ninto how much statistical analysis of specific real-world experiments is\nimpacted (Pallmann et al, 2018; FDA, 2019), compared to traditional uniform\nrandom assignment. We therefore use our case study of the ubiquitous two-arm\nbinary reward setting to empirically investigate the impact of using Thompson\nSampling instead of uniform random assignment. In this setting, using common\nstatistical hypothesis tests, we show that collecting data with TS can as much\nas double the False Positive Rate (FPR; incorrectly reporting differences when\nnone exist) and the False Negative Rate (FNR; failing to report differences\nwhen they exist)...\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:05:18 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 14:44:02 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Williams", "Joseph Jay", ""], ["Nogas", "Jacob", ""], ["Deliu", "Nina", ""], ["Shaikh", "Hammad", ""], ["Villar", "Sofia S.", ""], ["Durand", "Audrey", ""], ["Rafferty", "Anna", ""]]}, {"id": "2103.12206", "submitter": "Andrew Ying", "authors": "Andrew Ying, Eric J. Tchetgen Tchetgen", "title": "A New Causal Approach to Account for Treatment Switching in Randomized\n  Experiments under a Structural Cumulative Survival Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treatment switching in a randomized controlled trial is said to occur when a\npatient randomized to one treatment arm switches to another treatment arm\nduring follow-up. This can occur at the point of disease progression, whereby\npatients in the control arm may be offered the experimental treatment. It is\nwidely known that failure to account for treatment switching can seriously\ndilute the estimated effect of treatment on overall survival. In this paper, we\naim to account for the potential impact of treatment switching in a re-analysis\nevaluating the treatment effect of NucleosideReverse Transcriptase Inhibitors\n(NRTIs) on a safety outcome (time to first severe or worse sign or symptom) in\nparticipants receiving a new antiretroviral regimen that either included or\nomitted NRTIs in the Optimized Treatment That Includes or OmitsNRTIs (OPTIONS)\ntrial. We propose an estimator of a treatment causal effect under a structural\ncumulative survival model (SCSM) that leverages randomization as an\ninstrumental variable to account for selective treatment switching. Unlike\nRobins' accelerated failure time model often used to address treatment\nswitching, the proposed approach avoids the need for artificial censoring for\nestimation. We establish that the proposed estimator is uniformly consistent\nand asymptotically Gaussian under standard regularity conditions. A consistent\nvariance estimator is also given and a simple resampling approach provides\nuniform confidence bands for the causal difference comparing treatment groups\novertime on the cumulative intensity scale. We develop an R package named\n\"ivsacim\" implementing all proposed methods, freely available to download from\nR CRAN. We examine the finite performance of the estimator via extensive\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:26:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ying", "Andrew", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "2103.12227", "submitter": "Tat-Thang Vo", "authors": "Tat-Thang Vo and Stijn Vansteelandt", "title": "Challenges in systematic reviews and meta-analyses of mediation analyses", "comments": "Currently under peer-review at the American Journal of Epidemiology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Systematic reviews and meta-analyses of mediation studies are increasingly\nbeing implemented in practice. Nonetheless, the methodology for conducting such\nreview and analysis is still in a development phase, with much room for\nimprovement. In this paper, we highlight and discuss challenges that\ninvestigators face in mediation systematic reviews and meta-analyses, then\npropose ways of accommodating these in practice.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 23:26:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Vo", "Tat-Thang", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2103.12515", "submitter": "Guangchun Ruan", "authors": "Guangchun Ruan, Jiahan Wu, Haiwang Zhong, Qing Xia, Le Xie", "title": "Quantitative Assessment of U.S. Bulk Power Systems and Market Operations\n  during COVID-19", "comments": "Journal paper, 19 pages, also available at EnerarXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting in early 2020, the novel coronavirus disease (COVID-19) severely\naffected the U.S., causing substantial changes in the operations of bulk power\nsystems and electricity markets. In this paper, we develop a data-driven\nanalysis to substantiate the pandemic's impacts from the perspectives of power\nsystem security, electric power generation, electric power demand and\nelectricity prices. Our results suggest that both electric power demand and\nelectricity prices have discernibly dropped during the COVID-19 pandemic.\nGeographical variances in the impact are observed and quantified, and the bulk\npower market and power system operations in the northeast region are most\nseverely affected. All the data sources, assessment criteria, and analysis\ncodes reported in this paper are available on a GitHub repository.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 03:40:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ruan", "Guangchun", ""], ["Wu", "Jiahan", ""], ["Zhong", "Haiwang", ""], ["Xia", "Qing", ""], ["Xie", "Le", ""]]}, {"id": "2103.12648", "submitter": "Fabian Stephany", "authors": "Otto K\\\"assi, Vili Lehdonvirta, Fabian Stephany", "title": "How Many Online Workers are there in the World? A Data-Driven Assessment", "comments": "16 pages, four figures, two tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  An unknown number of people around the world are earning income by working\nthrough online labour platforms such as Upwork and Amazon Mechanical Turk. We\ncombine data collected from various sources to build a data-driven assessment\nof the number of such online workers (also known as online freelancers)\nglobally. Our headline estimate is that there are 163 million freelancer\nprofiles registered on online labour platforms globally. Approximately 19\nmillion of them have obtained work through the platform at least once, and 5\nmillion have completed at least 10 projects or earned at least $1000. These\nnumbers suggest a substantial growth from 2015 in registered worker accounts,\nbut much less growth in amount of work completed by workers. Our results\nindicate that online freelancing represents a non-trivial segment of labour\ntoday, but one that is spread thinly across countries and sectors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:00:30 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 13:39:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["K\u00e4ssi", "Otto", ""], ["Lehdonvirta", "Vili", ""], ["Stephany", "Fabian", ""]]}, {"id": "2103.12661", "submitter": "Radka Jersakova", "authors": "Radka Jersakova, James Lomax, James Hetherington, Brieuc Lehmann,\n  George Nicholson, Mark Briers, Chris Holmes", "title": "Bayesian imputation of COVID-19 positive test counts for nowcasting\n  under reporting lag", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Obtaining up to date information on the number of UK COVID-19 regional\ninfections is hampered by the reporting lag in positive test results for people\nwith COVID-19 symptoms. In the UK, for \"Pillar 2\" swab tests for those showing\nsymptoms, it can take up to five days for results to be collated. We make use\nof the stability of the under reporting process over time to motivate a\nstatistical temporal model that infers the final total count given the partial\ncount information as it arrives. We adopt a Bayesian approach that provides for\nsubjective priors on parameters and a hierarchical structure for an underlying\nlatent intensity process for the infection counts. This results in a smoothed\ntime-series representation now-casting the expected number of daily counts of\npositive tests with uncertainty bands that can be used to aid decision making.\nInference is performed using sequential Monte Carlo.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:30:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Jersakova", "Radka", ""], ["Lomax", "James", ""], ["Hetherington", "James", ""], ["Lehmann", "Brieuc", ""], ["Nicholson", "George", ""], ["Briers", "Mark", ""], ["Holmes", "Chris", ""]]}, {"id": "2103.12802", "submitter": "Ankit Kumar", "authors": "Ankit Kumar, Sharmodeep Bhattacharyya, Kristofer Bouchard", "title": "Numerical Characterization of Support Recovery in Sparse Regression with\n  Correlated Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sparse regression is frequently employed in diverse scientific settings as a\nfeature selection method. A pervasive aspect of scientific data that hampers\nboth feature selection and estimation is the presence of strong correlations\nbetween predictive features. These fundamental issues are often not appreciated\nby practitioners, and jeapordize conclusions drawn from estimated models. On\nthe other hand, theoretical results on sparsity-inducing regularized regression\nsuch as the Lasso have largely addressed conditions for selection consistency\nvia asymptotics, and disregard the problem of model selection, whereby\nregularization parameters are chosen. In this numerical study, we address these\nissues through exhaustive characterization of the performance of several\nregression estimators, coupled with a range of model selection strategies.\nThese estimators and selection criteria were examined across correlated\nregression problems with varying degrees of signal to noise, distribution of\nthe non-zero model coefficients, and model sparsity. Our results reveal a\nfundamental tradeoff between false positive and false negative control in all\nregression estimators and model selection criteria examined. Additionally, we\nare able to numerically explore a transition point modulated by the\nsignal-to-noise ratio and spectral properties of the design covariance matrix\nat which the selection accuracy of all considered algorithms degrades. Overall,\nwe find that SCAD coupled with BIC or empirical Bayes model selection performs\nthe best feature selection across the regression problems considered.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 19:13:26 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Kumar", "Ankit", ""], ["Bhattacharyya", "Sharmodeep", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "2103.12853", "submitter": "Elizabeth Bismut", "authors": "Elizabeth Bismut and Daniel Straub", "title": "A unified model of inspection and monitoring quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-destructive evaluation (NDE) through inspection and monitoring is an\nintegral part of asset integrity management. The relationship between the\ncondition of interest and the quantity measured by NDE is described with\nprobabilistic models such as PoD or ROC curves. These models are used to assess\nthe quality of the information provided by NDE systems, which is affected by\nfactors such as the experience of the inspector, environmental conditions, ease\nof access, or imprecision in the measuring device. In this paper, we show how\nthe different probabilistic models of NDE are connected within a unifying\nframework. Using this framework, we derive insights into how these models\nshould be learned, calibrated, and applied. We investigate how the choice of\nthe model can affect the maintenance decisions taken on the basis of NDE\nresults. In addition, we analyze the impact of experimental design on the\nperformance of a given NDE system in a decision-making context.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:26:30 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bismut", "Elizabeth", ""], ["Straub", "Daniel", ""]]}, {"id": "2103.12984", "submitter": "Moeen Mostafavi", "authors": "Moeen Mostafavi, Maria Phillips, Yichen Jiang, Michael D. Porter, Paul\n  Freedman", "title": "A tale of two metrics: Polling and financial contributions as a measure\n  of performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Campaign analysis is an integral part of American democracy and has many\ncomplexities in its dynamics. Experts have long sought to understand these\ndynamics and evaluate campaign performance using a variety of techniques. We\nexplore campaign financing and standing in the polls as two components of\ncampaign performance in the context of the 2020 Democratic primaries. We show\nwhere these measures exhibit represent similar dynamics and where they differ.\nWe focus on identifying change points in the trend for all candidates using\njoinpoint regression models. We find how these change points identify major\nevents such as failure or success in a debate. Joinpoint regression reveals who\nthe voters support when they stop supporting a specific candidate. This study\ndemonstrates the value of joinpoint regression in political campaign analysis\nand it represents a crossover of this technique into the political domain\nbuilding a foundation for continued exploration and use of this method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:41:02 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Mostafavi", "Moeen", ""], ["Phillips", "Maria", ""], ["Jiang", "Yichen", ""], ["Porter", "Michael D.", ""], ["Freedman", "Paul", ""]]}, {"id": "2103.13131", "submitter": "Andrew Whiteman", "authors": "Andrew S. Whiteman, Andreas J. Bartsch, Jian Kang, and Timothy D.\n  Johnson", "title": "Bayesian Inference for Brain Activity from Functional Magnetic Resonance\n  Imaging Collected at Two Spatial Resolutions", "comments": "37 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroradiologists and neurosurgeons increasingly opt to use functional\nmagnetic resonance imaging (fMRI) to map functionally relevant brain regions\nfor noninvasive presurgical planning and intraoperative neuronavigation. This\napplication requires a high degree of spatial accuracy, but the fMRI\nsignal-to-noise ratio (SNR) decreases as spatial resolution increases. In\npractice, fMRI scans can be collected at multiple spatial resolutions, and it\nis of interest to make more accurate inference on brain activity by combining\ndata with different resolutions. To this end, we develop a new Bayesian model\nto leverage both better anatomical precision in high resolution fMRI and higher\nSNR in standard resolution fMRI. We assign a Gaussian process prior to the mean\nintensity function and develop an efficient, scalable posterior computation\nalgorithm to integrate both sources of data. We draw posterior samples using an\nalgorithm analogous to Riemann manifold Hamiltonian Monte Carlo in an expanded\nparameter space. We illustrate our method in analysis of presurgical fMRI data,\nand show in simulation that it infers the mean intensity more accurately than\nalternatives that use either the high or standard resolution fMRI data alone.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:13:46 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Whiteman", "Andrew S.", ""], ["Bartsch", "Andreas J.", ""], ["Kang", "Jian", ""], ["Johnson", "Timothy D.", ""]]}, {"id": "2103.13228", "submitter": "Oleksandr Didkovskyi", "authors": "Oleksandr Didkovskyi, Giovanni Azzone, Alessandra Menafoglio,\n  Piercesare Secchi", "title": "Social and material vulnerability in the face of seismic hazard: an\n  analysis of the Italian case", "comments": "23 page, 20 figures; abstract corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The assessment of the vulnerability of a community endangered by seismic\nhazard is of paramount importance for planning a precision policy aimed at the\nprevention and reduction of its seismic risk. We aim at measuring the\nvulnerability of the Italian municipalities exposed to seismic hazard, by\nanalyzing the open data offered by the Mappa dei Rischi dei Comuni Italiani\nprovided by ISTAT, the Italian National Institute of Statistics. Encompassing\nthe Index of Social and Material Vulnerability already computed by ISTAT, we\nalso consider as referents of the latent social and material vulnerability of a\ncommunity, its demographic dynamics and the age of the building stock where the\ncommunity resides. Fusing the analyses of different indicators, within the\ncontext of seismic risk we offer a tentative ranking of the Italian\nmunicipalities in terms of their social and material vulnerability, together\nwith differential profiles of their dominant fragilities which constitute the\nbasis for planning precision policies aimed at seismic risk prevention and\nreduction.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:39:20 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 08:41:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Didkovskyi", "Oleksandr", ""], ["Azzone", "Giovanni", ""], ["Menafoglio", "Alessandra", ""], ["Secchi", "Piercesare", ""]]}, {"id": "2103.13236", "submitter": "Stavros Nikolakopoulos", "authors": "Stavros Nikolakopoulos and Ioannis Ntzoufras", "title": "Meta Analysis of Bayes Factors", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bayes Factors, the Bayesian tool for hypothesis testing, are receiving\nincreasing attention in the literature. Compared to their frequentist rivals\n($p$-values or test statistics), Bayes Factors have the conceptual advantage of\nproviding evidence both for and against a null hypothesis and they can be\ncalibrated so that they do not depend so heavily on the sample size. However,\nresearch on the synthesis of Bayes Factors arising from individual studies has\nreceived very limited attention. In this work we review and propose methods for\ncombining Bayes Factors from multiple studies, depending on the level of\ninformation available. In the process, we provide insights with respect to the\ninterplay between frequentist and Bayesian evidence. We also clarify why some\nintuitive suggestions in the literature can be misleading. We assess the\nperformance of the methods discussed via a simulation study and apply the\nmethods in an example from the field of psychology.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:56:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Nikolakopoulos", "Stavros", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2103.13296", "submitter": "Carly Lupton-Smith", "authors": "Carly Lupton-Smith, Elena Badillo Goicoechea, Megan Collins, Justin\n  Lessler, M. Kate Grabowski, and Elizabeth A. Stuart", "title": "Consistency between household and county measures of K-12 onsite\n  schooling during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The academic, socioemotional, and health impacts of school policies\nthroughout the COVID-19 pandemic have been a source of many important questions\nthat require accurate information about the extent of onsite schooling that has\nbeen occurring throughout the pandemic. This paper investigates school\noperational status data sources during the COVID-19 pandemic, comparing\nself-report data collected nationally on the household level through a\nFacebook-based survey with data collected at district and county levels\nthroughout the country. The percentage of households reporting in-person\ninstruction within each county is compared to the district and county data at\nthe state and county levels. The results show high levels of consistency\nbetween the sources at the state level and for large counties. The consistency\nlevels across sources support the usage of the Facebook-based COVID-19 Symptom\nSurvey as a source to answer questions about the educational experiences,\nfactors, and impacts related to K-12 education across the nation during the\npandemic.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:15:56 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lupton-Smith", "Carly", ""], ["Goicoechea", "Elena Badillo", ""], ["Collins", "Megan", ""], ["Lessler", "Justin", ""], ["Grabowski", "M. Kate", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2103.13313", "submitter": "Brady Moon", "authors": "Thiago A. Rodrigues, Jay Patrikar, Arnav Choudhry, Jacob Feldgoise,\n  Vaibhav Arcot, Aradhana Gahlaut, Sophia Lau, Brady Moon, Bastian Wagner, H.\n  Scott Matthews, Sebastian Scherer, Constantine Samaras", "title": "In-flight positional and energy use data set of a DJI Matrice 100\n  quadcopter for small package delivery", "comments": "13 pages, 11 figures, submitted to Scientific Data", "journal-ref": null, "doi": "10.1038/s41597-021-00930-x", "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We autonomously direct a small quadcopter package delivery Uncrewed Aerial\nVehicle (UAV) or \"drone\" to take off, fly a specified route, and land for a\ntotal of 209 flights while varying a set of operational parameters. The vehicle\nwas equipped with onboard sensors, including GPS, IMU, voltage and current\nsensors, and an ultrasonic anemometer, to collect high-resolution data on the\ninertial states, wind speed, and power consumption. Operational parameters,\nsuch as commanded ground speed, payload, and cruise altitude, are varied for\neach flight. This large data set has a total flight time of 10 hours and 45\nminutes and was collected from April to October of 2019 covering a total\ndistance of approximately 65 kilometers. The data collected were validated by\ncomparing flights with similar operational parameters. We believe these data\nwill be of great interest to the research and industrial communities, who can\nuse the data to improve UAV designs, safety, and energy efficiency, as well as\nadvance the physical understanding of in-flight operations for package delivery\ndrones.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:23:37 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rodrigues", "Thiago A.", ""], ["Patrikar", "Jay", ""], ["Choudhry", "Arnav", ""], ["Feldgoise", "Jacob", ""], ["Arcot", "Vaibhav", ""], ["Gahlaut", "Aradhana", ""], ["Lau", "Sophia", ""], ["Moon", "Brady", ""], ["Wagner", "Bastian", ""], ["Matthews", "H. Scott", ""], ["Scherer", "Sebastian", ""], ["Samaras", "Constantine", ""]]}, {"id": "2103.13357", "submitter": "Zhiyuan Li", "authors": "Zhiyuan Li", "title": "A Two-Stage Variable Selection Approach for Correlated High Dimensional\n  Predictors", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When fitting statistical models, some predictors are often found to be\ncorrelated with each other, and functioning together. Many group variable\nselection methods are developed to select the groups of predictors that are\nclosely related to the continuous or categorical response. These existing\nmethods usually assume the group structures are well known. For example,\nvariables with similar practical meaning, or dummy variables created by\ncategorical data. However, in practice, it is impractical to know the exact\ngroup structure, especially when the variable dimensional is large. As a\nresult, the group variable selection results may be selected. To solve the\nchallenge, we propose a two-stage approach that combines a variable clustering\nstage and a group variable stage for the group variable selection problem. The\nvariable clustering stage uses information from the data to find a group\nstructure, which improves the performance of the existing group variable\nselection methods. For ultrahigh dimensional data, where the predictors are\nmuch larger than observations, we incorporated a variable screening method in\nthe first stage and shows the advantages of such an approach. In this article,\nwe compared and discussed the performance of four existing group variable\nselection methods under different simulation models, with and without the\nvariable clustering stage. The two-stage method shows a better performance, in\nterms of the prediction accuracy, as well as in the accuracy to select active\npredictors. An athlete's data is also used to show the advantages of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:28:34 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Zhiyuan", ""]]}, {"id": "2103.13455", "submitter": "Chandan Singh", "authors": "Chandan Singh, Guha Balakrishnan, Pietro Perona", "title": "Matched sample selection with GANs for mitigating attribute confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring biases of vision systems with respect to protected attributes like\ngender and age is critical as these systems gain widespread use in society.\nHowever, significant correlations between attributes in benchmark datasets make\nit difficult to separate algorithmic bias from dataset bias. To mitigate such\nattribute confounding during bias analysis, we propose a matching approach that\nselects a subset of images from the full dataset with balanced attribute\ndistributions across protected attributes. Our matching approach first projects\nreal images onto a generative adversarial network (GAN)'s latent space in a\nmanner that preserves semantic attributes. It then finds image matches in this\nlatent space across a chosen protected attribute, yielding a dataset where\nsemantic and perceptual attributes are balanced across the protected attribute.\nWe validate projection and matching strategies with qualitative, quantitative,\nand human annotation experiments. We demonstrate our work in the context of\ngender bias in multiple open-source facial-recognition classifiers and find\nthat bias persists after removing key confounders via matching. Code and\ndocumentation to reproduce the results here and apply the methods to new data\nis available at https://github.com/csinva/matching-with-gans .\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:18:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Singh", "Chandan", ""], ["Balakrishnan", "Guha", ""], ["Perona", "Pietro", ""]]}, {"id": "2103.13494", "submitter": "Bennet Sakelaris", "authors": "Laura Albrecht, Paulina Czarnecki, Bennet Sakelaris", "title": "Investigating the Relationship Between Air Quality and COVID-19\n  Transmission", "comments": "Supplementary materials available at:\n  https://jds-online.org/journal/JDS/article/549/info", "journal-ref": null, "doi": "10.6339/21-JDS1010", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is hypothesized that short-term exposure to air pollution may influence\nthe transmission of aerosolized pathogens such as COVID-19. We used data from\n23 provinces in Italy to build a generalized additive model to investigate the\nassociation between the effective reproductive number of the disease and air\nquality while controlling for ambient environmental variables and changes in\nhuman mobility. The model finds that there is a positive, nonlinear\nrelationship between the density of particulate matter in the air and COVID-19\ntransmission, which is in alignment with similar studies on other respiratory\nillnesses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:24:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Albrecht", "Laura", ""], ["Czarnecki", "Paulina", ""], ["Sakelaris", "Bennet", ""]]}, {"id": "2103.13693", "submitter": "Yuan Shijie", "authors": "Shijie Yuan, Tianjian Zhou, Yawen Lin, and Yuan Ji", "title": "The Ci3+3 Design for Dual-Agent Combination Dose-Finding Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a rule-based statistical design for combination dose-finding\ntrials with two agents. The Ci3+3 design is an extension of the i3+3 design\nwith simple decision rules comparing the observed toxicity rates and\nequivalence intervals that define the maximum tolerated dose combination. Ci3+3\nconsists of two stages to allow fast and efficient exploration of the\ndose-combination space. Statistical inference is restricted to a beta-binomial\nmodel for dose evaluation, and the entire design is built upon a set of fixed\nrules. We show via simulation studies that the Ci3+3 design exhibits similar\nand comparable operating characteristics to more complex designs utilizing\nmodel-based inferences. We believe that the Ci3+3 design may provide an\nalternative choice to help simplify the design and conduct of combination\ndose-finding trials in practice.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:17:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yuan", "Shijie", ""], ["Zhou", "Tianjian", ""], ["Lin", "Yawen", ""], ["Ji", "Yuan", ""]]}, {"id": "2103.13877", "submitter": "Igor Kavrakov", "authors": "Igor Kavrakov, Allan McRobie and Guido Morgenthal", "title": "Data-driven Aerodynamic Analysis of Structures using Gaussian Processes", "comments": "20 pages, 30 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.flu-dyn cs.LG cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An abundant amount of data gathered during wind tunnel testing and health\nmonitoring of structures inspires the use of machine learning methods to\nreplicate the wind forces. These forces are critical for both the design and\nlife-cycle assessment of lifeline structures such as bridges. This paper\npresents a data-driven Gaussian Process-Nonlinear Finite Impulse Response\n(GP-NFIR) model of the nonlinear self-excited forces acting on bridges.\nConstructed in a nondimensional form, the model takes the effective wind angle\nof attack as lagged exogenous input and outputs a probability distribution of\nthe aerodynamic forces. The nonlinear latent function, mapping the input to the\noutput, is modeled by a GP regression. Consequently, the model is\nnonparametric, and as such, it avoids setting up the latent function's\nstructure a priori. The training input is designed as band-limited random\nharmonic motion that consists of vertical and rotational displacements. Once\ntrained, the model can predict the aerodynamic forces for both prescribed input\nmotion and coupled aeroelastic analysis. The presented concept is first\nverified for a flat plate's analytical, linear solution by predicting the\nself-excited forces and flutter velocity. Finally, the framework is applied to\na streamlined and bluff bridge deck based on Computational Fluid Dynamics (CFD)\ndata. Here, the model's ability to predict nonlinear aerodynamic forces,\ncritical flutter limit, and post-flutter behavior are highlighted. Further\napplications of the presented framework are foreseen in the design and online\nreal-time monitoring of slender line-like structures.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 11:22:24 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Kavrakov", "Igor", ""], ["McRobie", "Allan", ""], ["Morgenthal", "Guido", ""]]}, {"id": "2103.13879", "submitter": "Qi Wang", "authors": "Hengfang Deng, Qi Wang", "title": "Examining mobility data justice during 2017 Hurricane Harvey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural disasters can significantly disrupt human mobility in urban areas.\nStudies have attempted to understand and quantify such disruptions using\ncrowdsourced mobility data sets. However, limited research has studied the\njustice issues of mobility data in the context of natural disasters. The lack\nof research leaves us without an empirical foundation to quantify and control\nthe possible biases in the data. This study, using 2017 Hurricane Harvey as a\ncase study, explores three aspects of mobility data that could potentially\ncause injustice: representativeness, quality, and precision. We find\nrepresentativeness being a major factor contributing to mobility data\ninjustice. There is a persistent disparity of representativeness across\nneighborhoods of different socioeconomic characteristics before, during, and\nafter the hurricane's landfall. Additionally, we observed significant drops of\ndata precision during the hurricane, adding uncertainty to locate people and\nunderstand their movements during extreme weather events. The findings\nhighlight the necessity in understanding and controlling the possible bias of\nmobility data as well as developing practical tools through data justice lenses\nin collecting and analyzing data during disasters.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:08:18 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Deng", "Hengfang", ""], ["Wang", "Qi", ""]]}, {"id": "2103.14035", "submitter": "Mayana Pereira", "authors": "Mayana Pereira, Allen Kim, Joshua Allen, Kevin White, Juan Lavista\n  Ferres and Rahul Dodhia", "title": "U.S. Broadband Coverage Data Set: A Differentially Private Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Broadband connectivity is a key metric in today's economy. In an era of rapid\nexpansion of the digital economy, it directly impacts GDP. Furthermore, with\nthe COVID-19 guidelines of social distancing, internet connectivity became\nnecessary to everyday activities such as work, learning, and staying in touch\nwith family and friends. This paper introduces a publicly available U.S.\nBroadband Coverage data set that reports broadband coverage percentages at a\nzip code-level. We also explain how we used differential privacy to guarantee\nthat the privacy of individual households is preserved. Our data set also\ncontains error ranges estimates, providing information on the expected error\nintroduced by differential privacy per zip code. We describe our error range\ncalculation method and show that this additional data metric does not induce\nany privacy losses.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:32:04 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:46:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Pereira", "Mayana", ""], ["Kim", "Allen", ""], ["Allen", "Joshua", ""], ["White", "Kevin", ""], ["Ferres", "Juan Lavista", ""], ["Dodhia", "Rahul", ""]]}, {"id": "2103.14138", "submitter": "Maria Suveges Dr", "authors": "Prince John and Alessandra R. Brazzale and Maria S\\\"uveges", "title": "Margin-free classification and new class detection using finite\n  Dirichlet mixtures", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP astro-ph.IM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a margin-free finite mixture model which allows us to\nsimultaneously classify objects into known classes and to identify possible new\nobject types using a set of continuous attributes. This application is\nmotivated by the needs of identifying and possibly detecting new types of a\nparticular kind of stars known as variable stars. We first suitably transform\nthe physical attributes of the stars onto the simplex to achieve scale\ninvariance while maintaining their dependence structure. This allows us to\ncompare data collected by different sky surveys which can have different\nscales. The model hence combines a mixture of Dirichlet mixtures to represent\nthe known classes with the semi-supervised classification strategy of Vatanen\net al. (2012) for outlier detection. In line with previous work on\nsemiparametric model-based clustering, the single Dirichlet distributions can\nbe seen as providing the baseline pattern of the data. These are then combined\nto effectively model the complex distributions of the attributes for the\ndifferent classes. The model is estimated using a hierarchical two-step\nprocedure which combines a suitably adapted version of the\nExpectation-Maximization (EM) algorithm with Bayes' rule. We validate our model\non a reliable sample of periodic variable stars available in the literature\n(Dubath et al., 2011) achieving an overall classification accuracy of 71.95%, a\nsensitivity of 86.11% and a specificity of 99.79% for new class detection.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 21:17:41 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["John", "Prince", ""], ["Brazzale", "Alessandra R.", ""], ["S\u00fcveges", "Maria", ""]]}, {"id": "2103.14220", "submitter": "Jason Poulos", "authors": "Jason Poulos", "title": "Amnesty Policy and Elite Persistence in the Postbellum South: Evidence\n  from a Regression Discontinuity Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the impact of Reconstruction-era amnesty policy on\nthe officeholding and wealth of elites in the postbellum South. Amnesty policy\nrestricted the political and economic rights of Southern elites for nearly\nthree years during Reconstruction. I estimate the effect of being excluded from\namnesty on elites' future wealth and political power using a regression\ndiscontinuity design that compares individuals just above and below a wealth\nthreshold that determined exclusion from amnesty. Results on a sample of\nReconstruction convention delegates show that exclusion from amnesty\nsignificantly decreased the likelihood of ex-post officeholding. I find no\nevidence that exclusion impacted later census wealth for Reconstruction\ndelegates or for a larger sample of known slaveholders who lived in the South\nin 1860. These findings are in line with previous studies evidencing both\nchanges to the identity of the political elite, and the continuity of economic\nmobility among the planter elite across the Civil War and Reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:19:55 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 19:26:27 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Poulos", "Jason", ""]]}, {"id": "2103.14444", "submitter": "Rodney Vasconcelos Fonseca", "authors": "Rodney Fonseca, Alu\\'isio Pinheiro and Abdourrahmane Atto", "title": "Wavelet Spatio-Temporal Change Detection on multi-temporal PolSAR images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce WECS (Wavelet Energies Correlation Sreening), an unsupervised\nsparse procedure to detect spatio-temporal change points on multi-temporal SAR\n(POLSAR) images or even on sequences of very high resolution images. The\nprocedure is based on wavelet approximation for the multi-temporal images,\nwavelet energy apportionment, and ultra-high dimensional correlation screening\nfor the wavelet coefficients. We present two complimentary wavelet measures in\norder to detect sudden and/or cumulative changes, as well as for the case of\nstationary or non-stationary multi-temporal images. We show WECS performance on\nsynthetic multi-temporal image data. We also apply the proposed method to a\ntime series of 85 satellite images in the border region of Brazil and the\nFrench Guiana. The images were captured from November 08, 2015 to December 09\n2017.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:55:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fonseca", "Rodney", ""], ["Pinheiro", "Alu\u00edsio", ""], ["Atto", "Abdourrahmane", ""]]}, {"id": "2103.14559", "submitter": "Julien Bect", "authors": "Julien Bect (L2S, GdR MASCOT-NUM), Souleymane Zio (L2S, GdR\n  MASCOT-NUM), Guillaume Perrin (LDG, DAM/DIF, GdR MASCOT-NUM), Claire\n  Cannamela (DAM/DIF, GdR MASCOT-NUM), Emmanuel Vazquez (L2S, GdR MASCOT-NUM)", "title": "On the quantification of discretization uncertainty: comparison of two\n  paradigms", "comments": null, "journal-ref": "14th World Congress in Computational Mechanics and ECCOMAS\n  Congress 2020 (WCCM-ECCOMAS), Jan 2021, Virtual conference, originally\n  scheduled in Paris, France", "doi": "10.23967/wccm-eccomas.2020.260", "report-no": null, "categories": "physics.comp-ph physics.med-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerical models based on partial differential equations (PDE), or\nintegro-differential equations, are ubiquitous in engineering and science,\nmaking it possible to understand or design systems for which physical\nexperiments would be expensive-sometimes impossible-to carry out. Such models\nusually construct an approximate solution of the underlying continuous\nequations, using discretization methods such as finite differences or the\nfinite elements method. The resulting discretization error introduces a form of\nuncertainty on the exact but unknown value of any quantity of interest (QoI),\nwhich affects the predictions of the numerical model alongside other sources of\nuncertainty such as parametric uncertainty or model inadequacy. The present\narticle deals with the quantification of this discretization uncertainty.A\nfirst approach to this problem, now standard in the V\\&V (Verification and\nValidation) literature, uses the grid convergence index (GCI) originally\nproposed by P. Roache in the field of computational fluid dynamics (CFD), which\nis based on the Richardson extrapolation technique. Another approach, based on\nBayesian inference with Gaussian process models, was more recently introduced\nin the statistical literature. In this work we present and compare these two\nparadigms for the quantification of discretization uncertainty, which have been\ndevelopped in different scientific communities, and assess the potential of\nthe-younger-Bayesian approach to provide a replacement for the well-established\nGCI-based approach, with better probabilistic foundations. The methods are\nillustrated and evaluated on two standard test cases from the literature\n(lid-driven cavity and Timoshenko beam).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 07:51:43 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Zio", "Souleymane", "", "L2S, GdR\n  MASCOT-NUM"], ["Perrin", "Guillaume", "", "LDG, DAM/DIF, GdR MASCOT-NUM"], ["Cannamela", "Claire", "", "DAM/DIF, GdR MASCOT-NUM"], ["Vazquez", "Emmanuel", "", "L2S, GdR MASCOT-NUM"]]}, {"id": "2103.14604", "submitter": "Sharan Srinivas", "authors": "Suchithra Rajendran, Sharan Srinivas, Trenton Grimshaw", "title": "Predicting Demand for Air Taxi Urban Aviation Services using Machine\n  Learning Algorithms", "comments": "24 pages, 3 figures, 4 tables", "journal-ref": "Journal of Air Transport Management 92 (2021): 102043", "doi": "10.1016/j.jairtraman.2021.102043", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This research focuses on predicting the demand for air taxi urban air\nmobility (UAM) services during different times of the day in various geographic\nregions of New York City using machine learning algorithms (MLAs). Several\nride-related factors (such as month of the year, day of the week and time of\nthe day) and weather-related variables (such as temperature, weather conditions\nand visibility) are used as predictors for four popular MLAs, namely, logistic\nregression, artificial neural networks, random forests, and gradient boosting.\nExperimental results suggest gradient boosting to consistently provide higher\nprediction performance. Specific locations, certain time periods and weekdays\nconsistently emerged as critical predictors.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:12:43 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Rajendran", "Suchithra", ""], ["Srinivas", "Sharan", ""], ["Grimshaw", "Trenton", ""]]}, {"id": "2103.14731", "submitter": "Runze Liu", "authors": "Runze Liu, Chau-Wai Wong, Huaiyu Dai", "title": "Modeling the Nonsmoothness of Modern Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.IV eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern neural networks have been successful in many regression-based tasks\nsuch as face recognition, facial landmark detection, and image generation. In\nthis work, we investigate an intuitive but understudied characteristic of\nmodern neural networks, namely, the nonsmoothness. The experiments using\nsynthetic data confirm that such operations as ReLU and max pooling in modern\nneural networks lead to nonsmoothness. We quantify the nonsmoothness using a\nfeature named the sum of the magnitude of peaks (SMP) and model the\ninput-output relationships for building blocks of modern neural networks.\nExperimental results confirm that our model can accurately predict the\nstatistical behaviors of the nonsmoothness as it propagates through such\nbuilding blocks as the convolutional layer, the ReLU activation, and the max\npooling layer. We envision that the nonsmoothness feature can potentially be\nused as a forensic tool for regression-based applications of neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:55:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Runze", ""], ["Wong", "Chau-Wai", ""], ["Dai", "Huaiyu", ""]]}, {"id": "2103.14871", "submitter": "Mithun Ghosh", "authors": "Prama Debnath, Mithun Ghosh", "title": "Multivariate Gaussian Process Incorporated Predictive Model for Stream\n  Turbine Power Plant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steam power turbine-based power plant approximately contributes 90% of the\ntotal electricity produced in the United States. Mainly steam turbine consists\nof multiple types of turbine, boiler, attemperator, reheater, etc. Power is\nproduced through the steam with high pressure and temperature that is conducted\nby the turbines. The total power generation of the power plant is highly\nnonlinear considering all these elements in the model. We perform a predictive\nmodeling approach to detect the power generation from these turbines by the\nGaussian process (GP) model. As there are multiple interconnected turbines, we\nconsider a multivariate Gaussian process (MGP) modeling to predict the power\ngeneration from these turbines which can capture the cross-correlations between\nthe turbines. Also, the sensitivity analysis of the input parameters is\nconstructed for each turbine to find out the most important parameters.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 10:06:52 GMT"}, {"version": "v2", "created": "Sun, 4 Apr 2021 06:43:26 GMT"}, {"version": "v3", "created": "Sat, 10 Apr 2021 10:34:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Debnath", "Prama", ""], ["Ghosh", "Mithun", ""]]}, {"id": "2103.14931", "submitter": "Claus Weihs", "authors": "Claus Weihs and Sarah Buschfeld", "title": "NesPrInDT: Nested undersampling in PrInDT", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend our PrInDT method (Weihs, Buschfeld 2021) towards\nadditional undersampling of one of the predictors. This helps us to handle\nmultiple unbalanced data sets, i.e. data sets that are not only unbalanced with\nrespect to the class variable but also in one of the predictor variables.\nBeyond the advantages of such an approach, our study reveals that the balanced\naccuracy in the full data set can be much lower than in the predictor\nundersamples. We discuss potential reasons for this problem and draw\nmethodological conclusions for linguistic studies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 15:34:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weihs", "Claus", ""], ["Buschfeld", "Sarah", ""]]}, {"id": "2103.15018", "submitter": "David Ritzwoller", "authors": "Thomas J. DiCiccio, David M. Ritzwoller, Joseph P. Romano, Azeem M.\n  Shaikh", "title": "Confidence Intervals for Seroprevalence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper concerns the construction of confidence intervals in standard\nseroprevalence surveys. In particular, we discuss methods for constructing\nconfidence intervals for the proportion of individuals in a population infected\nwith a disease using a sample of antibody test results and measurements of the\ntest's false positive and false negative rates. We begin by documenting erratic\nbehavior in the coverage probabilities of standard Wald and percentile\nbootstrap intervals when applied to this problem. We then consider two\nalternative sets of intervals constructed with test inversion. The first set of\nintervals are approximate, using either asymptotic or bootstrap approximation\nto the finite-sample distribution of a chosen test statistic. We consider\nseveral choices of test statistic, including maximum likelihood estimators and\ngeneralized likelihood ratio statistics. We show with simulation that, at\nempirically relevant parameter values and sample sizes, the coverage\nprobabilities for these intervals are close to their nominal level and are\napproximately equi-tailed. The second set of intervals are shown to contain the\ntrue parameter value with probability at least equal to the nominal level, but\ncan be conservative in finite samples. To conclude, we outline the application\nof the methods that we consider to several related problems, and we provide a\nset of practical recommendations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 23:58:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["DiCiccio", "Thomas J.", ""], ["Ritzwoller", "David M.", ""], ["Romano", "Joseph P.", ""], ["Shaikh", "Azeem M.", ""]]}, {"id": "2103.15034", "submitter": "Susu Zhang", "authors": "Susu Zhang, Zhi Wang, Jitong Qi, Jingchen Liu, Zhiliang Ying", "title": "Accurate Assessment via Process Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate assessment of students' ability is the key task of a test.\nAssessments based on final responses are the standard. As the infrastructure\nadvances, substantially more information is observed. One of such instances is\nthe process data that is collected by computer-based interactive items, which\ncontain a student's detailed interactive processes. In this paper, we show both\ntheoretically and empirically that appropriately including such information in\nthe assessment will substantially improve relevant assessment precision. The\nprecision is measured empirically by out-of-sample test reliability.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 03:16:57 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Susu", ""], ["Wang", "Zhi", ""], ["Qi", "Jitong", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "2103.15036", "submitter": "Susu Zhang", "authors": "Susu Zhang, Xueying Tang, Qiwei He, Jingchen Liu, Zhiliang Ying", "title": "External Correlates of Adult Digital Problem-Solving Behavior: Log Data\n  Analysis of a Large-Scale Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the action sequence data (i.e., log data) from the problem-solving in\ntechnology-rich environments assessment on the 2012 Programme for the\nInternational Assessment of Adult Competencies survey, the current study\nexamines the associations between adult digital problem-solving behavior and\nseveral demographic and cognitive variables. Action sequence features extracted\nusing multidimensional scaling (Tang, Wang, He, Liu, & Ying, 2019) and\nsequence-to-sequence autoencoders (Tang, Wang, Liu, & Ying, 2019) were used to\npredict test-taker external characteristics. Features extracted from action\nsequences were consistently found to contain more information on demographic\nand cognitive characteristics than final scores. Partial least squares analyses\nfurther revealed systematic associations between behavioral patterns and\ndemographic/cognitive characteristics.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 03:29:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Susu", ""], ["Tang", "Xueying", ""], ["He", "Qiwei", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "2103.15156", "submitter": "Zhijie Sasha Dong", "authors": "Lingyu Meng and Zhijie Sasha Dong", "title": "Ridesharing Evacuation Model of Disaster Response", "comments": "6 pages, 3 figures. Proceeding of the 2020 IISE Annual Conference,\n  https://www.proquest.com/openview/403992", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Timely evacuation is crucial to disaster response, as people can avoid\nsuffering and loss of lives when a major disaster happens. With the development\nof sharing economy, ridesharing has the advantage of reducing congestion,\nsaving travel time, and optimizing transportation mode to improve disaster\nevacuation efficiency. The paper proposes to integrate the concept of\nridesharing into evacuation and develops a mixed-integer programming model for\nthis problem. A real-world case study based on Houston is used to validate the\nproposed model. A series of instances are designed to compare the evacuation\nefficiency using two indicators, evacuation percentage and average travel\ndistance. Results reveal that increasing the number of vehicles to help carless\nindividuals might not be the most efficient method in this model. Moreover,\nthis model offers a specific response strategy based on different disaster\nscales, which not only develops a better evacuation plan for the people but\nalso provides relief agencies insights on resource utilization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 15:37:21 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 14:25:34 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 05:03:41 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 01:11:39 GMT"}, {"version": "v5", "created": "Thu, 22 Jul 2021 07:57:42 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 17:02:35 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Meng", "Lingyu", ""], ["Dong", "Zhijie Sasha", ""]]}, {"id": "2103.15342", "submitter": "Yiming Xu", "authors": "Yiming Xu, Vahid Keshavarzzadeh, Robert M. Kirby, Akil Narayan", "title": "A bandit-learning approach to multifidelity approximation", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multifidelity approximation is an important technique in scientific\ncomputation and simulation. In this paper, we introduce a bandit-learning\napproach for leveraging data of varying fidelities to achieve precise estimates\nof the parameters of interest. Under a linear model assumption, we formulate a\nmultifidelity approximation as a modified stochastic bandit, and analyze the\nloss for a class of policies that uniformly explore each model before\nexploiting. Utilizing the estimated conditional mean-squared error, we propose\na consistent algorithm, adaptive Explore-Then-Commit (AETC), and establish a\ncorresponding trajectory-wise optimality result. These results are then\nextended to the case of vector-valued responses, where we demonstrate that the\nalgorithm is efficient without the need to worry about estimating\nhigh-dimensional parameters. The main advantage of our approach is that we\nrequire neither hierarchical model structure nor \\textit{a priori} knowledge of\nstatistical information (e.g., correlations) about or between models. Instead,\nthe AETC algorithm requires only knowledge of which model is a trusted\nhigh-fidelity model, along with (relative) computational cost estimates of\nquerying each model. Numerical experiments are provided at the end to support\nour theoretical findings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:29:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xu", "Yiming", ""], ["Keshavarzzadeh", "Vahid", ""], ["Kirby", "Robert M.", ""], ["Narayan", "Akil", ""]]}, {"id": "2103.15434", "submitter": "Karim Barigou", "authors": "Karim Barigou (ISFA), Pierre-Olivier Goffard (ISFA), St\\'ephane Loisel\n  (ISFA), Yahia Salhi (ISFA)", "title": "Bayesian model averaging for mortality forecasting using\n  leave-future-out validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the evolution of mortality rates plays a central role for life\ninsurance and pension funds. Various stochastic frameworks have been developed\nto model mortality patterns taking into account the main stylized facts driving\nthese patterns. However, relying on the prediction of one specific model can be\ntoo restrictive and lead to some well documented drawbacks including model\nmisspecification, parameter uncertainty and overfitting. To address these\nissues we first consider mortality modelling in a Bayesian Negative-Binomial\nframework to account for overdispersion and the uncertainty about the parameter\nestimates in a natural and coherent way. Model averaging techniques, which\nconsists in combining the predictions of several models, are then considered as\na response to model misspecifications. In this paper, we propose two methods\nbased on leave-future-out validation which are compared to the standard\nBayesian model averaging (BMA) based on marginal likelihood. Using\nout-of-sample errors is a well-known workaround for overfitting issues. We show\nthat it also produces better forecasts. An intensive numerical study is carried\nout over a large range of simulation setups to compare the performances of the\nproposed methodologies. An illustration is then proposed on real-life mortality\ndatasets which includes a sensitivity analysis to a Covid-type scenario.\nOverall, we found that both methods based on out-of-sample criterion outperform\nthe standard BMA approach in terms of prediction performance and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:05:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Barigou", "Karim", "", "ISFA"], ["Goffard", "Pierre-Olivier", "", "ISFA"], ["Loisel", "St\u00e9phane", "", "ISFA"], ["Salhi", "Yahia", "", "ISFA"]]}, {"id": "2103.15478", "submitter": "Tim Davis", "authors": "T P Davis", "title": "The study of variability in engineering design, an appreciation and a\n  retrospective", "comments": "11 pages wiuth 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We explore the concept of parameter design applied to the production of glass\nbeads in the manufacture of metal-encapsulated transistors. The main motivation\nis to complete the analysis hinted at in the original publication by Jim\nMorrison in 1957, which was an early example of discussing the idea of\ntransmitted variation in engineering design, and an influential paper in the\ndevelopment of analytic parameter design as a data-centric engineering\nactivity. Parameter design is a secondary design activity focussed on selecting\nthe nominals of the design variables, to simultaneously achieve the required\nfunctional output, with minimum variance.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:30:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Davis", "T P", ""]]}, {"id": "2103.15611", "submitter": "Yili Hong", "authors": "Zhongnan Jin and Lu Lu and Khaled Bedair and Yili Hong", "title": "Modeling Bivariate Geyser Eruption System with Covariate-Adjusted\n  Recurrent Event Process", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geyser eruption is one of the most popular signature attractions at the\nYellowstone National Park. The interdependence of geyser eruptions and impacts\nof covariates are of interest to researchers in geyser studies. In this paper,\nwe propose a parametric covariate-adjusted recurrent event model for estimating\nthe eruption gap time. We describe a general bivariate recurrent event process,\nwhere a bivariate lognormal distribution and a Gumbel copula with different\nmarginal distributions are used to model an interdependent dual-type event\nsystem. The maximum likelihood approach is used to estimate model parameters.\nThe proposed method is applied to analyzing the Yellowstone geyser eruption\ndata for a bivariate geyser system and offers a deeper understanding of the\nevent occurrence mechanism of individual events as well as the system as a\nwhole. A comprehensive simulation study is conducted to evaluate the\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 13:44:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jin", "Zhongnan", ""], ["Lu", "Lu", ""], ["Bedair", "Khaled", ""], ["Hong", "Yili", ""]]}, {"id": "2103.15704", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Sherveen Riazati, Nick Caplan and Phil Hayes", "title": "Are Multilevel functional models the next step in sports biomechanics\n  and wearable technology? A case study of Knee Biomechanics patterns in\n  typical training sessions of recreational runners", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper illustrates how multilevel functional models can detect and\ncharacterize biomechanical changes along different sport training sessions. Our\nanalysis focuses on the relevant cases to identify differences in knee\nbiomechanics in recreational runners during low and high-intensity exercise\nsessions with the same energy expenditure by recording $20$ steps. To do so, we\nreview the existing literature of multilevel models, and then, we propose a new\nhypothesis test to look at the changes between different levels of the\nmultilevel model as low and high-intensity training sessions. We also evaluate\nthe reliability of measures recorded in three-dimension knee angles from the\nfunctional intra-class correlation coefficient (ICC) obtained from the\ndecomposition performed with the multilevel funcional model taking into account\n$20$ measures recorded in each test. The results show that there are no\nstatistically significant differences between the two modes of exercise.\nHowever, we have to be careful with the conclusions since, as we have shown,\nhuman gait-patterns are very individual and heterogeneous between groups of\nathletes, and other alternatives to the p-value may be more appropriate to\ndetect statistical differences in biomechanical changes in this context.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:43:09 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 22:19:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Matabuena", "Marcos", ""], ["Riazati", "Sherveen", ""], ["Caplan", "Nick", ""], ["Hayes", "Phil", ""]]}, {"id": "2103.15859", "submitter": "Zhanlin Liu", "authors": "Aman Ankit, Zhanlin Liu, Scott B. Miles, Youngjun Choe", "title": "U.S. Power Resilience for 2002--2019", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Prolonged power outages debilitate the economy and threaten public health.\nExisting research is generally limited in its scope to a single event, an\noutage cause, or a region. Here, we provide one of the most comprehensive\nanalyses of U.S. power outages for 2002--2019. We categorized all outage data\ncollected under U.S. federal mandates into four outage causes and computed\nindustry-standard reliability metrics. Our spatiotemporal analysis reveals six\nof the most resilient U.S. states since 2010, improvement of power resilience\nagainst natural hazards in the south and northeast regions, and a\ndisproportionately large number of human attacks for its population in the\nWestern Electricity Coordinating Council region. Our regression analysis\nidentifies several statistically significant predictors and hypotheses for\npower resilience. Furthermore, we propose a novel framework for analyzing\noutage data using differential weighting and influential points to better\nunderstand power resilience. We share curated data and code as Supplementary\nMaterials.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:10:43 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 06:46:02 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 08:20:32 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Ankit", "Aman", ""], ["Liu", "Zhanlin", ""], ["Miles", "Scott B.", ""], ["Choe", "Youngjun", ""]]}, {"id": "2103.15864", "submitter": "Agnimitra Dasgupta", "authors": "Agnimitra Dasgupta and Carlo Graziani and Zichao Wendy Di", "title": "Gaussian Process for Tomography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tomographic reconstruction, despite its revolutionary impact on a wide range\nof applications, suffers from its ill-posed nature in that there is no unique\nsolution because of limited and noisy measurements. Traditional\noptimization-based reconstruction relies on regularization to address this\nissue; however, it faces its own challenge because the type of regularizer and\nchoice of regularization parameter are a critical but difficult decision.\nMoreover, traditional reconstruction yields point estimates for the\nreconstruction with no further indication of the quality of the solution. In\nthis work we address these challenges by exploring Gaussian processes (GPs).\nOur proposed GP approach yields not only the reconstructed object through the\nposterior mean but also a quantification of the solution uncertainties through\nthe posterior covariance. Furthermore, we explore the flexibility of the GP\nframework to provide a robust model of the information across various length\nscales in the object, as well as the complex noise in the measurements. We\nillustrate the proposed approach on both synthetic and real tomography images\nand show its unique capability of uncertainty quantification in the presence of\nvarious types of noise, as well as reconstruction comparison with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:16:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Dasgupta", "Agnimitra", ""], ["Graziani", "Carlo", ""], ["Di", "Zichao Wendy", ""]]}, {"id": "2103.15919", "submitter": "Max Goplerud", "authors": "Max Goplerud", "title": "Modelling Heterogeneity Using Bayesian Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to estimate heterogeneity, e.g. the effect of some variable differing\nacross observations, is a key question in political science. Methods for doing\nso make simplifying assumptions about the underlying nature of the\nheterogeneity to draw reliable inferences. This paper allows a common way of\nsimplifying complex phenomenon (placing observations with similar effects into\ndiscrete groups) to be integrated into regression analysis. The framework\nallows researchers to (i) use their prior knowledge to guide which groups are\npermissible and (ii) appropriately quantify uncertainty. The paper does this by\nextending work on \"structured sparsity\" from a traditional penalized likelihood\napproach to a Bayesian one by deriving new theoretical results and inferential\ntechniques. It shows that this method outperforms state-of-the-art methods for\nestimating heterogeneous effects when the underlying heterogeneity is grouped\nand more effectively identifies groups of observations with different effects\nin observational data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:54:25 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Goplerud", "Max", ""]]}, {"id": "2103.15965", "submitter": "Sina Aghaei", "authors": "Sina Aghaei, Andr\\'es G\\'omez, Phebe Vayanos", "title": "Strong Optimal Classification Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees are among the most popular machine learning models and are\nused routinely in applications ranging from revenue management and medicine to\nbioinformatics. In this paper, we consider the problem of learning optimal\nbinary classification trees. Literature on the topic has burgeoned in recent\nyears, motivated both by the empirical suboptimality of heuristic approaches\nand the tremendous improvements in mixed-integer optimization (MIO) technology.\nYet, existing MIO-based approaches from the literature do not leverage the\npower of MIO to its full extent: they rely on weak formulations, resulting in\nslow convergence and large optimality gaps. To fill this gap in the literature,\nwe propose an intuitive flow-based MIO formulation for learning optimal binary\nclassification trees. Our formulation can accommodate side constraints to\nenable the design of interpretable and fair decision trees. Moreover, we show\nthat our formulation has a stronger linear optimization relaxation than\nexisting methods. We exploit the decomposable structure of our formulation and\nmax-flow/min-cut duality to derive a Benders' decomposition method to speed-up\ncomputation. We propose a tailored procedure for solving each decomposed\nsubproblem that provably generates facets of the feasible set of the MIO as\nconstraints to add to the main problem. We conduct extensive computational\nexperiments on standard benchmark datasets on which we show that our proposed\napproaches are 31 times faster than state-of-the art MIO-based techniques and\nimprove out of sample performance by up to 8%.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:40:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Aghaei", "Sina", ""], ["G\u00f3mez", "Andr\u00e9s", ""], ["Vayanos", "Phebe", ""]]}, {"id": "2103.16012", "submitter": "Rishabh Chauhan", "authors": "Rishabh Singh Chauhan, Matthew Wigginton Conway, Denise Capasso da\n  Silva, Deborah Salon, Ali Shamshiripour, Ehsan Rahimi, Sara Khoeini, Abolfazl\n  Mohammadian, Sybil Derrible, and Ram Pendyala", "title": "A database of travel-related behaviors and attitudes before, during, and\n  after COVID-19 in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic has impacted billions of people around the world. To\ncapture some of these impacts in the United States, we are conducting a\nnationwide longitudinal survey collecting information about travel-related\nbehaviors and attitudes before, during, and after the COVID-19 pandemic. The\nsurvey questions cover a wide range of topics including commuting, daily\ntravel, air travel, working from home, online learning, shopping, and risk\nperception, along with attitudinal, socioeconomic, and demographic information.\nVersion 1.0 of the survey contains 8,723 responses that are publicly available.\nThe survey is deployed over multiple waves to the same respondents to monitor\nhow behaviors and attitudes evolve over time. This article details the\nmethodology adopted for the collection, cleaning, and processing of the data.\nIn addition, the data are weighted to be representative of national and\nregional demographics. This survey dataset can aid researchers, policymakers,\nbusinesses, and government agencies in understanding both the extent of\nbehavioral shifts and the likelihood that these changes will persist after\nCOVID-19.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:57:59 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 20:36:43 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chauhan", "Rishabh Singh", ""], ["Conway", "Matthew Wigginton", ""], ["da Silva", "Denise Capasso", ""], ["Salon", "Deborah", ""], ["Shamshiripour", "Ali", ""], ["Rahimi", "Ehsan", ""], ["Khoeini", "Sara", ""], ["Mohammadian", "Abolfazl", ""], ["Derrible", "Sybil", ""], ["Pendyala", "Ram", ""]]}, {"id": "2103.16125", "submitter": "Giovanna Jona Lasinio Prof.", "authors": "Sara Martino, Daniela Silvia Pace, Stefano Moro, Edoardo Casoli,\n  Daniele Ventura, Alessandro Frachea, Margherita Silvestri, Antonella\n  Arcangeli, Giancarlo Giacomini, Giandomenico Ardizzone, Giovanna Jona Lasinio", "title": "Integration of presence-only data from several sources. A case study on\n  dolphins' spatial distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presence-only data are a typical occurrence in species distribution modeling.\nThey include the presence locations and no information on the absence. Their\nmodeling usually does not account for detection biases. In this work, we aim to\nmerge three different sources of information to model the presence of marine\nmammals. The approach is fully general and it is applied to two species of\ndolphins in the Central Tyrrhenian Sea (Italy) as a case study. Data come from\nthe Italian Environmental Protection Agency (ISPRA) and Sapienza University of\nRome research campaigns, and from a careful selection of social media (SM)\nimages and videos. We build a Log Gaussian Cox process where different\ndetection functions describe each data source. For the SM data, we analyze\nseveral choices that allow accounting for detection biases. Our findings allow\nfor a correct understanding of Stenella coeruleoalba and Tursiops truncatus\ndistribution in the study area. The results prove that the proposed approach is\nbroadly applicable, it can be widely used, and it is easily implemented in the\nR software using INLA and inlabru. We provide examples' code with simulated\ndata in the supplementary materials.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 07:22:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Martino", "Sara", ""], ["Pace", "Daniela Silvia", ""], ["Moro", "Stefano", ""], ["Casoli", "Edoardo", ""], ["Ventura", "Daniele", ""], ["Frachea", "Alessandro", ""], ["Silvestri", "Margherita", ""], ["Arcangeli", "Antonella", ""], ["Giacomini", "Giancarlo", ""], ["Ardizzone", "Giandomenico", ""], ["Lasinio", "Giovanna Jona", ""]]}, {"id": "2103.16157", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos, Evangelos Spiliotis and Anastasios Panagiotelis", "title": "Model combinations through revised base-rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard selection criteria for forecasting models focus on information that\nis calculated for each series independently, disregarding the general\ntendencies and performances of the candidate models. In this paper, we propose\na new way to statistical model selection and model combination that\nincorporates the base-rates of the candidate forecasting models, which are then\nrevised so that the per-series information is taken into account. We examine\ntwo schemes that are based on the precision and sensitivity information from\nthe contingency table of the base rates. We apply our approach on pools of\nexponential smoothing models and a large number of real time series and we show\nthat our schemes work better than standard statistical benchmarks. We discuss\nthe connection of our approach to other cross-learning approaches and offer\ninsights regarding implications for theory and practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:30:59 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 18:54:07 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Spiliotis", "Evangelos", ""], ["Panagiotelis", "Anastasios", ""]]}, {"id": "2103.16192", "submitter": "Prashant Mahajan Dr", "authors": "Prashant Mahajan (R. C. Patel Institute of Technology, Shirpur)", "title": "Selection of an engineering institution: Students perceptions of choice\n  characteristics and suitability under the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": "10.20944/preprints202103.0665.v1", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  COVID19 has impacted Indian engineering institutions (EIs) enormously. It has\ntightened its knot around EIs that forced their previous half shut shades\ncompletely down to prevent the risk of spreading COVID19. In such a situation,\nfetching new enrollments on EI campuses is a difficult and challenging task, as\nstudents behavior and family preferences have changed drastically due to mental\nstress and emotions attached to them. Consequently, it becomes a prerequisite\nto examine the choice characteristics influencing the selection of EI during\nthe COVID-19 pandemic to make it normal for new enrollments.\n  The purpose of this study is to critically examine choice characteristics\nthat affect students choice for EI and consequently to explore relationships\nbetween institutional characteristics and the suitability of EI during the\nCOVID19 pandemic across students characteristics. The findings of this study\nrevealed dissimilarities across students characteristics regarding the\nsuitability of EIs under pandemic conditions. Regression analysis revealed that\nEI characteristics such as proximity, image and reputation, quality education\nand curriculum delivery have significantly contributed to suitability under\nCOVID19. At the micro level, multiple relationships were noted between EI\ncharacteristics and the suitability of EI under the pandemic across students\ncharacteristics. The study has successfully demonstrated how choice\ncharacteristics can be executed to regulate the suitability of EI under the\nCOVID19 pandemic for the inclusion of diversity. It is useful for policy makers\nand academicians to reposition EIs that fetch diversity during the pandemic.\nThis study is the first to provide insights into the performance of choice\ncharacteristics and their relationship with the suitability of EIs under a\npandemic and can be a yardstick in administering new enrollments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:19:17 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Mahajan", "Prashant", "", "R. C. Patel Institute of Technology, Shirpur"]]}, {"id": "2103.16244", "submitter": "Sean Pinkney", "authors": "Sean Pinkney", "title": "An Improved and Extended Bayesian Synthetic Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An improved and extended Bayesian synthetic control model is presented,\nexpanding upon the latent factor model in Tuomaala 2019. The changes we make\ninclude 1) standardization of the data prior to model fit - which improves\nefficiency and generalization across different data sets; 2) adding time\nvarying covariates; 3) adding the ability to have multiple treated units; 4)\nfitting the latent factors within the Bayesian model; and, 5) a sparsity\ninducing prior to automatically tune the number of latent factors. We\ndemonstrate the similarity of estimates to two traditional synthetic control\nstudies in Abadie, Diamond, and Hainmueller 2010 and Abadie, Diamond, and\nHainmueller 2015 and extend to multiple target series with a new example of\nestimating digital website visitation from changes in data collection due to\ndigital privacy laws.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:48:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Pinkney", "Sean", ""]]}, {"id": "2103.16310", "submitter": "Bahman Rostami-Tabar", "authors": "Bahman Rostami-Tabar, Mohamed Zied Babai and Aris Syntetos", "title": "To aggregate or not to aggregate: Forecasting of finite autocorrelated\n  demand", "comments": "38 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal aggregation is an intuitively appealing approach to deal with demand\nuncertainty. There are two types of temporal aggregation: non-overlapping and\noverlapping. Most of the supply chain forecasting literature has focused so far\non the former and there is no research that analyses the latter for\nauto-correlated demands. In addition, most of the analytical research to-date\nassumes infinite demand series' lengths whereas, in practice, forecasting is\nbased on finite demand histories. The length of the demand history is an\nimportant determinant of the comparative performance of the two approaches but\nhas not been given sufficient attention in the literature. In this paper we\nexamine the effectiveness of temporal aggregation for forecasting finite\nauto-correlated demand. We do so by means of an analytical study of the\nforecast accuracy of aggregation and non-aggregation approaches based on mean\nsquared error. We complement this with a numerical analysis to explore the\nimpact of demand parameters and the length of the series on (comparative)\nperformance. We also conduct an empirical evaluation to validate the analytical\nresults using monthly time series of the M4-competition dataset. We find the\ndegree of auto-correlation, the forecast horizon and the length of the series\nto be important determinants of forecast accuracy. We discuss the merits of\neach approach and highlight their implications for real world practices.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:03:00 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rostami-Tabar", "Bahman", ""], ["Babai", "Mohamed Zied", ""], ["Syntetos", "Aris", ""]]}, {"id": "2103.16387", "submitter": "Carlo Romano Marcello Alessandro Santagiustina", "authors": "Carlo Santagiustina and Massimo Warglien", "title": "The Unfolding Structure of Arguments in Online Debates: The case of a\n  No-Deal Brexit", "comments": "Main article (18 pages, 7 figures) & Supplementary material (25\n  pages, 7 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CY stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the last decade, political debates have progressively shifted to social\nmedia. Rhetorical devices employed by online actors and factions that operate\nin these debating arenas can be captured and analysed to conduct a statistical\nreading of societal controversies and their argumentation dynamics. In this\npaper, we propose a five-step methodology, to extract, categorize and explore\nthe latent argumentation structures of online debates. Using Twitter data about\na \"no-deal\" Brexit, we focus on the expected effects in case of materialisation\nof this event. First, we extract cause-effect claims contained in tweets using\nRegEx that exploit verbs related to Creation, Destruction and Causation.\nSecond, we categorise extracted \"no-deal\" effects using a Structural Topic\nModel estimated on unigrams and bigrams. Third, we select controversial effect\ntopics and explore within-topic argumentation differences between self-declared\npartisan user factions. We hence type topics using estimated covariate effects\non topic propensities, then, using the topics correlation network, we study the\ntopological structure of the debate to identify coherent topical\nconstellations. Finally, we analyse the debate time dynamics and infer\nlead/follow relations among factions. Results show that the proposed\nmethodology can be employed to perform a statistical rhetorics analysis of\ndebates, and map the architecture of controversies across time. In particular,\nthe \"no-deal\" Brexit debate is shown to have an assortative argumentation\nstructure heavily characterized by factional constellations of arguments, as\nwell as by polarized narrative frames invoked through verbs related to Creation\nand Destruction. Our findings highlight the benefits of implementing a systemic\napproach to the analysis of debates, which allows the unveiling of topical and\nfactional dependencies between arguments employed in online debates.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:29:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Santagiustina", "Carlo", ""], ["Warglien", "Massimo", ""]]}, {"id": "2103.16685", "submitter": "Carmen Jim\\'enez-Mesa", "authors": "Carmen Jim\\'enez-Mesa, Javier Ram\\'irez, John Suckling, Jonathan\n  V\\\"oglein, Johannes Levin, Juan Manuel G\\'orriz, Alzheimer's Disease\n  Neuroimaging Initiative ADNI, Dominantly Inherited Alzheimer Network DIAN", "title": "Deep Learning in current Neuroimaging: a multivariate approach with\n  power and type I error control but arguable generalization ability", "comments": "26 pages, 10 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminative analysis in neuroimaging by means of deep/machine learning\ntechniques is usually tested with validation techniques, whereas the associated\nstatistical significance remains largely under-developed due to their\ncomputational complexity. In this work, a non-parametric framework is proposed\nthat estimates the statistical significance of classifications using deep\nlearning architectures. In particular, a combination of autoencoders (AE) and\nsupport vector machines (SVM) is applied to: (i) a one-condition, within-group\ndesigns often of normal controls (NC) and; (ii) a two-condition, between-group\ndesigns which contrast, for example, Alzheimer's disease (AD) patients with NC\n(the extension to multi-class analyses is also included). A random-effects\ninference based on a label permutation test is proposed in both studies using\ncross-validation (CV) and resubstitution with upper bound correction (RUB) as\nvalidation methods. This allows both false positives and classifier overfitting\nto be detected as well as estimating the statistical power of the test. Several\nexperiments were carried out using the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) dataset, the Dominantly Inherited Alzheimer Network (DIAN)\ndataset, and a MCI prediction dataset. We found in the permutation test that CV\nand RUB methods offer a false positive rate close to the significance level and\nan acceptable statistical power (although lower using cross-validation). A\nlarge separation between training and test accuracies using CV was observed,\nespecially in one-condition designs. This implies a low generalization ability\nas the model fitted in training is not informative with respect to the test\nset. We propose as solution by applying RUB, whereby similar results are\nobtained to those of the CV test set, but considering the whole set and with a\nlower computational cost per iteration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:15:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Jim\u00e9nez-Mesa", "Carmen", ""], ["Ram\u00edrez", "Javier", ""], ["Suckling", "John", ""], ["V\u00f6glein", "Jonathan", ""], ["Levin", "Johannes", ""], ["G\u00f3rriz", "Juan Manuel", ""], ["ADNI", "Alzheimer's Disease Neuroimaging Initiative", ""], ["DIAN", "Dominantly Inherited Alzheimer Network", ""]]}, {"id": "2103.16820", "submitter": "Eiji Konaka", "authors": "Miwa Shirai and Eiji Konaka", "title": "Can the center lane really have advantages for swimmers?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In swimming competitions, the center lane is considered to be the best lane.\nThe main objective of this study is to discuss the validity of the assertion,\n\"good swimmers should be placed in the center lanes because these lanes has\nadvantage.\" This study extracts the (positive) lane bias in the center lanes\nfrom many records utilizing statistical hypothesis tests. The data is collected\nfrom nationwide competitions of Japan over 10 years. The data set contains more\nthan 5900 swimmers and 56000 records. No significant evidence was obtained on\nthe lane advantage of the center lanes.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:38:04 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:39:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shirai", "Miwa", ""], ["Konaka", "Eiji", ""]]}, {"id": "2103.16894", "submitter": "Stefano M. Iacus", "authors": "Stefano Maria Iacus, Carlos Santamaria, Francesco Sermi, Spyridon\n  Spyratos, Dario Tarchi, Michele Vespe", "title": "Mobility Functional Areas and COVID-19 Spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a new concept of functional areas called Mobility\nFunctional Areas (MFAs), i.e., the geographic zones highly interconnected\naccording to the analysis of mobile positioning data. The MFAs do not coincide\nnecessarily with administrative borders as they are built observing natural\nhuman mobility and, therefore, they can be used to inform, in a bottom-up\napproach, local transportation, health and economic policies. After presenting\nthe methodology behind the MFAs, this study focuses on the link between the\nCOVID-19 pandemic and the MFAs in Austria. It emerges that the MFAs registered\nan average number of infections statistically larger than the areas in the rest\nof the country, suggesting the usefulness of the MFAs in the context of\ntargeted re-escalation policy responses to this health crisis.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:24:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Iacus", "Stefano Maria", ""], ["Santamaria", "Carlos", ""], ["Sermi", "Francesco", ""], ["Spyratos", "Spyridon", ""], ["Tarchi", "Dario", ""], ["Vespe", "Michele", ""]]}, {"id": "2103.17030", "submitter": "Karl Mosler", "authors": "Karl Mosler", "title": "Representative endowments and uniform Gini orderings of multi-attribute\n  inequality", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For the comparison of inequality in multiple attributes the use of\ngeneralized Gini indices is proposed. Spectral social evaluation functions are\nused in the multivariate setting, and Gini dominance orderings are introduced\nthat are uniform in attribute weights. Classes of spectral evaluators are\nconsidered that are parameterized by their aversion to inequality. Then a\nset-valued representative endowment is defined that characterizes\n$d$-dimensioned inequality. It consists of all points above the lower border of\na convex compact in $R^d$, while the pointwise ordering of such endowments\ncorresponds to uniform Gini dominance. Properties of uniform Gini dominance are\nderived, including relations to other orderings of $d$-variate distributions\nsuch as usual multivariate stochastic order and convex order. The\nmulti-dimensioned representative endowment can be efficiently calculated from\ndata; in a sampling context, it consistently estimates its population version.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:34:21 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Mosler", "Karl", ""]]}, {"id": "2103.17255", "submitter": "Sooie-Hoe Loke", "authors": "Jos\\'e Miguel Flores Contr\\'o, Kira Henshaw, Sooie-Hoe Loke,\n  S\\'everine Arnold and Corina Constantinescu", "title": "Subsidising Inclusive Insurance to Reduce Poverty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.PR q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a compound Poisson-type model for households'\ncapital. Using risk theory techniques, we determine the probability of a\nhousehold falling under the poverty line. Microinsurance is then introduced to\nanalyse its impact as an insurance solution for the lower income class. Our\nresults validate those previously obtained with this type of model, showing\nthat microinsurance alone is not sufficient to reduce the probability of\nfalling into the area of poverty for specific groups of people, since premium\npayments constrain households' capital growth. This indicates the need for\nadditional aid particularly from the government. As such, we propose several\npremium subsidy strategies and discuss the role of government in subsidising\nmicroinsurance to help reduce poverty.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:23:41 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Contr\u00f3", "Jos\u00e9 Miguel Flores", ""], ["Henshaw", "Kira", ""], ["Loke", "Sooie-Hoe", ""], ["Arnold", "S\u00e9verine", ""], ["Constantinescu", "Corina", ""]]}]