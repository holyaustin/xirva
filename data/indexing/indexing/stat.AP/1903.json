[{"id": "1903.00135", "submitter": "Richard La", "authors": "Michael Lin, Richard J. La and Nuno C. Martins", "title": "Stabilizing a Queue Subject to Action-Dependent Server Performance", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discrete-time system comprising a first-come-first-served\nqueue, a non-preemptive server, and a scheduler that governs the assignment of\ntasks in the queue to the server. The server has an availability state that\nindicates, at each instant, whether the server is busy working on a task or is\navailable. In the latter case, if the queue is nonempty, then a task-assignment\ncontrol policy implemented by the scheduler either assigns a new task to the\nserver or allows it to rest. The server also has an integer-valued activity\nstate that is non-increasing during rest periods, and is non-decreasing\notherwise. An instantaneous service rate function ascribes to each possible\nvalue of the activity state a probability that the server can complete a task\nin one time step. For a typical instantaneous service rate function, the\ncompletion probability decreases (server performance worsens) as the activity\nstate increases. The scheduler policy has access to the queue size and the\nentire state of the server.\n  In this article, we study the problem of designing scheduler policies that\nstabilize the queue. We show that stability, whenever viable, can be achieved\nby a simple policy that bases its decisions on the availability state, a\nthreshold applied to the activity state, and a flag that indicates when the\nqueue is empty. The supremum of the service rates achievable by stabilizing\npolicies can be determined by a finite search. Our results remain valid even\nwhen the instantaneous service rate function is not monotonic.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 03:05:00 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2020 13:15:09 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 16:50:48 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Lin", "Michael", ""], ["La", "Richard J.", ""], ["Martins", "Nuno C.", ""]]}, {"id": "1903.00196", "submitter": "Caroline Svahn", "authors": "Caroline Svahn, Oleg Sysoev, Mirsad \\v{C}irki\\'c, Fredrik Gunnarsson\n  and Joel Berglund", "title": "Inter-frequency radio signal quality prediction for handover, evaluated\n  in 3GPP LTE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radio resource management in cellular networks is typically based on device\nmeasurements reported to the serving base station. Frequent measuring of signal\nquality on available frequencies would allow for highly reliable networks and\noptimal connection at all times. However, these measurements are associated\nwith costs, such as dedicated device time for performing measurements when the\ndevice will be unavailable for communication. To reduce the costs, we consider\npredictions of inter-frequency radio quality measurements that are useful to\nassess potential inter-frequency handover decisions. In this contribution, we\nhave considered measurements from a live 3GPP LTE network. We demonstrate that\nstraightforward applications of the most commonly used machine learning models\nare unable to provide high accuracy predictions. Instead, we propose a novel\napproach with a duo-threshold for high accuracy decision recommendations. Our\napproach leads to class specific prediction accuracies as high as 92% and 95%,\nstill drastically reducing the need for inter-frequency measurements.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 08:14:38 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Svahn", "Caroline", ""], ["Sysoev", "Oleg", ""], ["\u010cirki\u0107", "Mirsad", ""], ["Gunnarsson", "Fredrik", ""], ["Berglund", "Joel", ""]]}, {"id": "1903.00251", "submitter": "Mikkel Slot Nielsen", "authors": "Mikkel Slot Nielsen and Victor Rohde", "title": "A surrogate model for estimating extreme tower loads on wind turbines\n  based on random forest proximities", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we present a surrogate model, which can be used to\nestimate extreme tower loads on a wind turbine from a number of signals and a\nsuitable simulation tool. Due to the requirements of the International\nElectrotechnical Commission (IEC) Standard 61400-1, assessing extreme tower\nloads on wind turbines constitutes a key component of the design phase. The\nproposed model imputes tower loads by matching observed signals with simulated\nquantities using proximities induced by random forests. In this way the\nalgorithm's adaptability to high-dimensional and sparse settings is exploited\nwithout using regression-based surrogate loads (which may display misleading\nprobabilistic characteristics). Finally, the model is applied to estimate tower\nloads on an operating wind turbine from data on its operational statistics.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 11:38:33 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 22:48:10 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Nielsen", "Mikkel Slot", ""], ["Rohde", "Victor", ""]]}, {"id": "1903.00288", "submitter": "Christina Stoehr", "authors": "Christina Stoehr, John A D Aston and Claudia Kirch", "title": "Detecting changes in the covariance structure of functional time series\n  with application to fMRI data", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional magnetic resonance imaging (fMRI) data provides information\nconcerning activity in the brain and in particular the interactions between\nbrain regions. Resting state fMRI data is widely used for inferring\nconnectivities in the brain which are not due to external factors. As such\nanalyzes strongly rely on stationarity, change point procedures can be applied\nin order to detect possible deviations from this crucial assumption. In this\npaper, we model fMRI data as functional time series and develop tools for the\ndetection of deviations from covariance stationarity via change point\nalternatives. We propose a nonparametric procedure which is based on dimension\nreduction techniques. However, as the projection of the functional time series\non a finite and rather low-dimensional subspace involves the risk of missing\nchanges which are orthogonal to the projection space, we also consider two test\nstatistics which take the full functional structure into account. The proposed\nmethods are compared in a simulation study and applied to more than 100 resting\nstate fMRI data sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 13:32:06 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Stoehr", "Christina", ""], ["Aston", "John A D", ""], ["Kirch", "Claudia", ""]]}, {"id": "1903.00423", "submitter": "Anastasia Chatzilena", "authors": "Anastasia Chatzilena, Edwin van Leeuwen, Oliver Ratmann, Marc\n  Baguelin, Nikolaos Demiris", "title": "Contemporary statistical inference for infectious disease models using\n  Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the application of recent statistical advances\nto inference of infectious disease dynamics. We describe the fitting of a class\nof epidemic models using Hamiltonian Monte Carlo and Variational Inference as\nimplemented in the freely available Stan software. We apply the two methods to\nreal data from outbreaks as well as routinely collected observations. Our\nresults suggest that both inference methods are computationally feasible in\nthis context, and show a trade-off between statistical efficiency versus\ncomputational speed. The latter appears particularly relevant for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 17:29:39 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 17:02:40 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 15:45:09 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Chatzilena", "Anastasia", ""], ["van Leeuwen", "Edwin", ""], ["Ratmann", "Oliver", ""], ["Baguelin", "Marc", ""], ["Demiris", "Nikolaos", ""]]}, {"id": "1903.00516", "submitter": "Stanislav Borysov S", "authors": "Stanislav S. Borysov, Jeppe Rich", "title": "Introducing Super Pseudo Panels: Application to Transport Preference\n  Dynamics", "comments": "22 pages, 10 figures, 5 tables", "journal-ref": null, "doi": "10.1007/s11116-020-10137-5", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for constructing synthetic pseudo-panel data from\ncross-sectional data. The pseudo panel and the preferences it intends to\ndescribe is constructed at the individual level and is not affected by\naggregation bias across cohorts. This is accomplished by creating a\nhigh-dimensional probabilistic model representation of the entire data set,\nwhich allows sampling from the probabilistic model in such a way that all of\nthe intrinsic correlation properties of the original data are preserved. The\nkey to this is the use of deep learning algorithms based on the Conditional\nVariational Autoencoder (CVAE) framework. From a modelling perspective, the\nconcept of a model-based resampling creates a number of opportunities in that\ndata can be organized and constructed to serve very specific needs of which the\nforming of heterogeneous pseudo panels represents one. The advantage, in that\nrespect, is the ability to trade a serious aggregation bias (when aggregating\ninto cohorts) for an unsystematic noise disturbance. Moreover, the approach\nmakes it possible to explore high-dimensional sparse preference distributions\nand their linkage to individual specific characteristics, which is not possible\nif applying traditional pseudo-panel methods. We use the presented approach to\nreveal the dynamics of transport preferences for a fixed pseudo panel of\nindividuals based on a large Danish cross-sectional data set covering the\nperiod from 2006 to 2016. The model is also utilized to classify individuals\ninto 'slow' and 'fast' movers with respect to the speed at which their\npreferences change over time. It is found that the prototypical fast mover is a\nyoung woman who lives as a single in a large city whereas the typical slow\nmover is a middle-aged man with high income from a nuclear family who lives in\na detached house outside a city.\n", "versions": [{"version": "v1", "created": "Fri, 1 Mar 2019 19:58:23 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Borysov", "Stanislav S.", ""], ["Rich", "Jeppe", ""]]}, {"id": "1903.00588", "submitter": "Gelu M. Nita", "authors": "Gelu M. Nita (1), Aard Keimpema (2), Zsolt Paragi (2) ((1) Center for\n  Solar-Terrestrial Research, New Jersey Institute of Technology, (2) Joint\n  institute for VLBI ERIC)", "title": "Statistical discrimination of RFI and astronomical transients in 2-bit\n  digitized time domain signals", "comments": "16 pages, 9 figures", "journal-ref": null, "doi": "10.1142/S2251171719400087", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the performance of the generalized Spectral Kurtosis (SK)\nestimator in detecting and discriminating natural and artificial very short\nduration transients in the 2-bit sampling time domain Very-Long-Baseline\nInterferometry (VLBI) data. We demonstrate that, while both types of transients\nmay be efficiently detected, their natural or artificial nature cannot be\ndistinguished if only a time domain SK analysis is performed. However, these\ntwo types of transients become distinguishable from each other in the spectral\ndomain, after a 32-bit FFT operation is performed on the 2-bit time domain\nvoltages. We discuss the implication of these findings on the ability of the\nSpectral Kurtosis estimator to automatically detect bright astronomical\ntransient signals of interests -- such as pulsar or fast radio bursts (FRB) --\nin VLBI data streams that have been severely contaminated by unwanted radio\nfrequency interference.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 00:46:02 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Nita", "Gelu M.", ""], ["Keimpema", "Aard", ""], ["Paragi", "Zsolt", ""]]}, {"id": "1903.00604", "submitter": "Richard Berk", "authors": "Richard A. Berk, Susan B. Sorenson", "title": "An Algorithmic Approach to Forecasting Rare Violent Events: An\n  Illustration Based in IPV Perpetration", "comments": "25 pages, 3 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass violence, almost no matter how defined, is (thankfully) rare. Rare\nevents are very difficult to study in a systematic manner. Standard statistical\nprocedures can fail badly and usefully accurate forecasts of rare events often\nare little more than an aspiration. We offer an unconventional approach for the\nstatistical analysis of rare events illustrated by an extensive case study. We\nreport research whose goal is to learn about the attributes of very high risk\nIPV perpetrators and the circumstances associated with their IPV incidents\nreported to the police. Very high risk is defined as having a high probability\nof committing a repeat IPV assault in which the victim is injured. Such\nindividuals represent a very small fraction of all IPV perpetrators; these acts\nof violence are relatively rare. To learn about them nevertheless, we apply in\na novel fashion three algorithms sequentially to data collected from a large\nmetropolitan police department: stochastic gradient boosting, a genetic\nalgorithm inspired by natural selection, and agglomerative clustering. We try\nto characterize not just perpetrators who on balance are predicted to\nre-offend, but who are very likely to re-offend in a manner that leads to\nvictim injuries. With this strategy, we learn a lot. We also provide a new way\nto estimate the importance of risk predictors. There are lessons for the study\nof other rare forms of violence especially when instructive forecasts are\nsought. In the absence of sufficiently accurate forecasts, scarce prevention\nresources cannot be allocated where they are most needed.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 02:07:57 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Berk", "Richard A.", ""], ["Sorenson", "Susan B.", ""]]}, {"id": "1903.00682", "submitter": "Leonardo Egidi PhD", "authors": "Teresa Fazia, Leonardo Egidi, Burcu Ayoglu, Ashley Beecham, Pier Paolo\n  Bitti, Anna Ticca, Jacob L. McCauley, Peter Nilsson, Carlo Berzuini, Luisa\n  Bernardinelli", "title": "Bayesian Mendelian Randomization identifies disease causing proteins via\n  pedigree data, partially observed exposures and correlated instruments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background\n  In a study performed on multiplex Multiple Sclerosis (MS) Sardinian families\nto identify disease causing plasma proteins, application of Mendelian\nRandomization (MR) methods encounters difficulties due to relatedness of\nindividuals, correlation between finely mapped genotype instrumental variables\n(IVs) and presence of missing exposures.\n  Method\n  We specialize the method of Berzuini et al (2018) to deal with these\ndifficulties. The proposed method allows pedigree structure to enter the\nspecification of the outcome distribution via kinship matrix, and treating\nmissing exposures as additional parameters to be estimated from the data. It\nalso acknowledges possible correlation between instruments by replacing the\noriginally proposed independence prior for IV-specific pleiotropic effect with\na g-prior. Based on correlated (r2< 0.2) IVs, we analysed the data of four\ncandidate MS-causing proteins by using both the independence and the g-prior.\n  Results\n  95% credible intervals for causal effect for proteins IL12A and STAT4 lay\nwithin the strictly negative real semiaxis, in both analyses, suggesting\npotential causality. Those instruments whose estimated pleiotropic effect\nexceeded 85% of total effect on outcome were found to act in trans. Analysis\nvia frequentist MR gave inconsistent results. Replacing the independence with a\ng-prior led to smaller credible intervals for causal effect.\n  Conclusions\n  Bayesian MR may be a good way to study disease causation at a protein level\nbased on family data and moderately correlated instruments.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 11:10:21 GMT"}, {"version": "v2", "created": "Mon, 23 Sep 2019 14:38:12 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Fazia", "Teresa", ""], ["Egidi", "Leonardo", ""], ["Ayoglu", "Burcu", ""], ["Beecham", "Ashley", ""], ["Bitti", "Pier Paolo", ""], ["Ticca", "Anna", ""], ["McCauley", "Jacob L.", ""], ["Nilsson", "Peter", ""], ["Berzuini", "Carlo", ""], ["Bernardinelli", "Luisa", ""]]}, {"id": "1903.00790", "submitter": "Taewoon Kong", "authors": "Taewoon Kong and Brani Vidakovic", "title": "Non-decimated Quaternion Wavelet Spectral Tools with Applications", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quaternion wavelets are redundant wavelet transforms generalizing\ncomplex-valued non-decimated wavelet transforms. In this paper we propose a\nmatrix-formulation for non-decimated quaternion wavelet transforms and define\nspectral tools for use in machine learning tasks. Since quaternionic algebra is\nan extension of complex algebra, quaternion wavelets bring redundancy in the\ncomponents that proves beneficial in wavelet based tasks. Specifically, the\nwavelet coefficients in the decomposition are quaternion-valued numbers that\ndefine the modulus and three phases.\n  The novelty of this paper is definition of non-decimated quaternion wavelet\nspectra based on the modulus and phase-dependent statistics as low-dimensional\nsummaries for 1-D signals or 2-D images. A structural redundancy in\nnon-decimated wavelets and a componential redundancy in quaternion wavelets are\nlinked to extract more informative features. In particular, we suggest an\nimproved way of classifying signals and images based on their scaling indices\nin terms of spectral slopes and information contained in the three quaternionic\nphases. We show that performance of the proposed method significantly improves\nwhen compared to the standard versions of wavelets including the complex-valued\nwavelets.\n  To illustrate performance of the proposed spectral tools we provide two\nexamples of application on real-data problems: classification of sounds using\nscaling in high-frequency recordings over time and monitoring of steel rolling\nprocess using the fractality of captured digitized images. The proposed tools\nare compared with the counterparts based on standard wavelet transforms.\n", "versions": [{"version": "v1", "created": "Sat, 2 Mar 2019 23:31:38 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Kong", "Taewoon", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1903.00817", "submitter": "Manh Toan Ho Mr.", "authors": "Quan-Hoang Vuong, Quang-Khiem Bui, Viet-Phuong La, Thu-Trang Vuong,\n  Manh-Toan Ho, Hong-Kong T. Nguyen, Hong-Ngoc Nguyen, Kien-Cuong P. Nghiem,\n  Manh-Tung Ho", "title": "Cultural evolution in Vietnam's early 20th century: a Bayesian networks\n  analysis of Franco-Chinese house designs", "comments": "35 pages, cultural evolution, Hanoi architecture, Old Quarter, house\n  fa\\c{c}ade, Buddhism, FrancoChinese style, French colonialism, Bayesian\n  network, Hamiltonian Markov chain Monte Carlo", "journal-ref": "Social Sciences & Humanities Open 1(1), 2019, 100001", "doi": "10.1016/j.ssaho.2019.100001", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The study of cultural evolution has taken on an increasingly\ninterdisciplinary and diverse approach in explicating phenomena of cultural\ntransmission and adoptions. Inspired by this computational movement, this study\nuses Bayesian networks analysis, combining both the frequentist and the\nHamiltonian Markov chain Monte Carlo (MCMC) approach, to investigate the highly\nrepresentative elements in the cultural evolution of a Vietnamese city's\narchitecture in the early 20th century. With a focus on the fa\\c{c}ade design\nof 68 old houses in Hanoi's Old Quarter (based on 78 data lines extracted from\n248 photos), the study argues that it is plausible to look at the aesthetics,\narchitecture and designs of the house fa\\c{c}ade to find traces of cultural\nevolution in Vietnam, which went through more than six decades of French\ncolonization and centuries of sociocultural influence from China. The in-depth\ntechnical analysis, though refuting the presumed model on the probabilistic\ndependency among the variables, yields several results, the most notable of\nwhich is the strong influence of Buddhism over the decorations of the house\nfa\\c{c}ade. Particularly, in the top 5 networks with the best Bayesian\nInformation criterion (BIC) scores and p<0.05, the variable for decorations\n(DC) always has a direct probabilistic dependency on the variable B for\nBuddhism. The paper then checks the robustness of these models using\nHamiltonian MCMC method and find the posterior distributions of the models'\ncoefficients all satisfy the technical requirement. Finally, this study\nsuggests integrating Bayesian statistics in social sciences in general and for\nstudy of cultural evolution and architectural transformation in particular.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 03:47:11 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Vuong", "Quan-Hoang", ""], ["Bui", "Quang-Khiem", ""], ["La", "Viet-Phuong", ""], ["Vuong", "Thu-Trang", ""], ["Ho", "Manh-Toan", ""], ["Nguyen", "Hong-Kong T.", ""], ["Nguyen", "Hong-Ngoc", ""], ["Nghiem", "Kien-Cuong P.", ""], ["Ho", "Manh-Tung", ""]]}, {"id": "1903.01029", "submitter": "Yingying Xu", "authors": "Yingying Xu, Joon Lee, Joel A. Dubin", "title": "Similarity-based Random Survival Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting time-to-event outcomes in large databases can be a challenging but\nimportant task. One example of this is in predicting the time to a clinical\noutcome for patients in intensive care units (ICUs), which helps to support\ncritical medical treatment decisions. In this context, the time to an event of\ninterest could be, for example, survival time or time to recovery from a\ndisease/ailment observed within the ICU. The massive health datasets generated\nfrom the uptake of Electronic Health Records (EHRs) are quite heterogeneous as\npatients can be quite dissimilar in their relationship between the feature\nvector and the outcome, adding more noise than information to prediction. In\nthis paper, we propose a modified random forest method for survival data that\nidentifies similar cases in an attempt to improve accuracy for predicting\ntime-to-event outcomes; this methodology can be applied in various settings,\nincluding with ICU databases. We also introduce an adaptation of our\nmethodology in the case of dependent censoring. Our proposed method is\ndemonstrated in the Medical Information Mart for Intensive Care (MIMIC-III)\ndatabase, and, in addition, we present properties of our methodology through a\ncomprehensive simulation study. Introducing similarity to the random survival\nforest method indeed provides improved predictive accuracy compared to random\nsurvival forest alone across the various analyses we undertook.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:09:37 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 14:10:34 GMT"}], "update_date": "2019-08-06", "authors_parsed": [["Xu", "Yingying", ""], ["Lee", "Joon", ""], ["Dubin", "Joel A.", ""]]}, {"id": "1903.01035", "submitter": "Thomas Metzger", "authors": "Thomas A. Metzger and Christopher T. Franck", "title": "Detection of latent heteroscedasticity and group-based regression\n  effects in linear models via Bayesian model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard linear modeling approaches make potentially simplistic assumptions\nregarding the structure of categorical effects that may obfuscate more complex\nrelationships governing data. For example, recent work focused on the two-way\nunreplicated layout has shown that hidden groupings among the levels of one\ncategorical predictor frequently interact with the ungrouped factor. We extend\nthe notion of a \"latent grouping factor\" to linear models in general. The\nproposed work allows researchers to determine whether an apparent grouping of\nthe levels of a categorical predictor reveals a plausible hidden structure\ngiven the observed data. Specifically, we offer Bayesian model selection-based\napproaches to reveal latent group-based heteroscedasticity, regression effects,\nand/or interactions. Failure to account for such structures can produce\nmisleading conclusions. Since the presence of latent group structures is\nfrequently unknown a priori to the researcher, we use fractional Bayes factor\nmethods and mixture $g$-priors to overcome lack of prior information.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 01:29:35 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Metzger", "Thomas A.", ""], ["Franck", "Christopher T.", ""]]}, {"id": "1903.01048", "submitter": "Kai Liu", "authors": "Kai Liu, Ravi Srinivasan, Lauren Ancel Meyers", "title": "Early Detection of Influenza outbreaks in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Public health surveillance systems often fail to detect emerging infectious\ndiseases, particularly in resource limited settings. By integrating relevant\nclinical and internet-source data, we can close critical gaps in coverage and\naccelerate outbreak detection. Here, we present a multivariate algorithm that\nuses freely available online data to provide early warning of emerging\ninfluenza epidemics in the US. We evaluated 240 candidate predictors and found\nthat the most predictive combination does \\textit{not} include surveillance or\nelectronic health records data, but instead consists of eight Google search and\nWikipedia pageview time series reflecting changing levels of interest in\ninfluenza-related topics. In cross validation on 2010-2016 data, this algorithm\nsounds alarms an average of 16.4 weeks prior to influenza activity reaching the\nCenter for Disease Control and Prevention (CDC) threshold for declaring the\nstart of the season. In an out-of-sample test on data from the rapidly-emerging\nfall wave of the 2009 H1N1 pandemic, it recognized the threat five weeks in\nadvance of this surveillance threshold. Simpler algorithms, including fixed\nweek-of-the-year triggers, lag the optimized alarms by only a few weeks when\ndetecting seasonal influenza, but fail to provide early warning in the 2009\npandemic scenario. This demonstrates a robust method for designing next\ngeneration outbreak detection algorithms. By combining scan statistics with\nmachine learning, it identifies tractable combinations of data sources (from\namong thousands of candidates) that can provide early warning of emerging\ninfectious disease threats worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 02:43:17 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Liu", "Kai", ""], ["Srinivasan", "Ravi", ""], ["Meyers", "Lauren Ancel", ""]]}, {"id": "1903.01186", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis Thorarinsdottir, Anders L{\\o}land and Alex Lenkoski", "title": "Probabilistic Forecasting of Temporal Trajectories of Regional Power\n  Production - Part 1: Wind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Renewable energy sources provide a constantly increasing contribution to the\ntotal energy production worldwide. However, the power generation from these\nsources is highly variable due to their dependence on meteorological\nconditions. Accurate forecasts for the production at various temporal and\nspatial scales are thus needed for an efficiently operating electricity market.\nIn this article - part 1 - we propose fully probabilistic prediction models for\nspatially aggregated wind power production at an hourly time scale with lead\ntimes up to several days using weather forecasts from numerical weather\nprediction systems as covariates. After an appropriate cubic transformation of\nthe power production, we build up a multivariate Gaussian prediction model\nunder a Bayesian inference framework which incorporates the temporal error\ncorrelation. In an application to predict wind production in Germany, the\nmethod provides calibrated and skillful forecasts. Comparison is made between\nseveral formulations of the correlation structure.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:43:50 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Thorarinsdottir", "Thordis", ""], ["L\u00f8land", "Anders", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1903.01188", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis Thorarinsdottir, Anders L{\\o}land and Alex Lenkoski", "title": "Probabilistic Forecasting of Temporal Trajectories of Regional Power\n  Production - Part 2: Photovoltaic Solar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully probabilistic prediction model for spatially aggregated\nsolar photovoltaic (PV) power production at an hourly time scale with lead\ntimes up to several days using weather forecasts from numerical weather\nprediction systems as covariates. After an appropriate logarithmic\ntransformation of the power production, we develop a multivariate Gaussian\nprediction model under a Bayesian inference framework. The model incorporates\nthe temporal error correlation yielding physically consistent forecast\ntrajectories. Several formulations of the correlation structure are proposed\nand investigated. Our method is one of a few approaches that issue full\npredictive distributions for PV power production. In a case study of PV power\nproduction in Germany, the method gives calibrated and skillful forecasts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Mar 2019 11:48:06 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Thorarinsdottir", "Thordis", ""], ["L\u00f8land", "Anders", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1903.01601", "submitter": "Behnam Malmir", "authors": "Behnam Malmir, Shing I Chang, Malgorzata Rys and Dylan Darter", "title": "Quantifying Gait Changes Using Microsoft Kinect and Sample Entropy", "comments": "This article is an updated version of a paper entitled 'Quantifying\n  Gait Changes Using Microsoft Kinect and Sample Entropy' presented at the 2018\n  Industrial and Systems Engineering Research Conference (ISERC) in Orlando,\n  Florida (May 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CG cs.HC stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This study describes a method to quantify potential gait changes in human\nsubjects. Microsoft Kinect devices were used to provide and track coordinates\nof fifteen different joints of a subject over time. Three male subjects walk a\n10-foot path multiple times with and without motion-restricting devices. Their\nwalking patterns were recorded via two Kinect devices through frontal and\nsagittal planes. A modified sample entropy (SE) value was computed to quantify\nthe variability of the time series for each joint. The SE values with and\nwithout motion-restricting devices were used to compare the changes in each\njoint. The preliminary results of the experiments show that the proposed\nquantification method can detect differences in walking patterns with and\nwithout motion-restricting devices. The proposed method has the potential to be\napplied to track personal progress in physical therapy sessions.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 00:16:23 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""], ["Rys", "Malgorzata", ""], ["Darter", "Dylan", ""]]}, {"id": "1903.01803", "submitter": "Arnaud Cadas", "authors": "Arnaud Cadas and Ana Busic", "title": "The power disaggregation algorithms and their applications to demand\n  dispatch", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We were interested in solving a power disaggregation problem which comes down\nto estimating the power consumption of each device given the total power\nconsumption of the whole house. We started by looking at the Factorial\nHierarchical Dirichlet Process - Hidden Semi-Markov Model. However, the\ninference method had a complexity which scales withthe number of observations.\nThus, we developed an online algorithm based on particle filters. We applied\nthe method to data from Pecan Street https://dataport.cloud/ using Python. We\napplied the disaggregation algorithm to the control techniques used in Demand\nDispatch.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 13:06:54 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Cadas", "Arnaud", ""], ["Busic", "Ana", ""]]}, {"id": "1903.01841", "submitter": "Taylor Brown", "authors": "Taylor R. Brown", "title": "A Factor Stochastic Volatility Model with Markov-Switching Panic Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of factor stochastic volatility models requires choosing the number\nof latent factors used to describe the dynamics of the financial returns\nprocess; however, empirical evidence suggests that the number and makeup of\npertinent factors is time-varying and economically situational. We present a\nnovel factor stochastic volatility model that allows for random subsets of\nassets to have their members experience non-market-wide panics. These\nparticipating assets will experience an increase in their variances and\nwithin-group covariances. We also give an estimation algorithm for this model\nthat takes advantage of recent results on Particle Markov chain Monte Carlo\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 5 Mar 2019 14:23:00 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Brown", "Taylor R.", ""]]}, {"id": "1903.02129", "submitter": "Yura Kim", "authors": "Yura Kim, Elizaveta Levina", "title": "Graph-aware Modeling of Brain Connectivity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connections in the brain are frequently represented by weighted\nnetworks, with nodes representing locations in the brain, and edges\nrepresenting the strength of connectivity between these locations. One\nchallenge in analyzing such data is that inference at the individual edge level\nis not particularly biologically meaningful; interpretation is more useful at\nthe level of so-called functional regions, or groups of nodes and connections\nbetween them; this is often called \"graph-aware\" inference in the neuroimaging\nliterature. However, pooling over functional regions leads to significant loss\nof information and lower accuracy. Another challenge is correlation among edge\nweights within a subject, which makes inference based on independence\nassumptions unreliable. We address both these challenges with a linear mixed\neffects model, which accounts for functional regions and for edge dependence,\nwhile still modeling individual edge weights to avoid loss of information. The\nmodel allows for comparing two populations, such as patients and healthy\ncontrols, both at the functional regions level and at individual edge level,\nleading to biologically meaningful interpretations. We fit this model to a\nresting state fMRI data on schizophrenics and healthy controls, obtaining\ninterpretable results consistent with the schizophrenia literature.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 00:59:28 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 19:02:22 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Kim", "Yura", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1903.02318", "submitter": "Urtats Etxegarai Susaeta", "authors": "U. Etxegarai, E. Portillo, J. Irazusta, L. A. Koefoed, N. Kasabov", "title": "A heuristic approach for lactate threshold estimation for training\n  decision-making: An accessible and easy to use solution for recreational\n  runners", "comments": "25 pages, 11 figures", "journal-ref": null, "doi": "10.1016/j.ejor.2019.08.023", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a heuristic as operational tool to estimate the lactate\nthreshold and to facilitate its integration into the training process of\nrecreational runners is proposed. To do so, we formalize the principles for the\nlactate threshold estimation from empirical data and an iterative methodology\nthat enables experience based learning. This strategy arises as a robust and\nadaptive approach to solve data analysis problems. We compare the results of\nthe heuristic with the most commonly used protocol by making a first\nquantitative error analysis to show its reliability. Additionally, we provide a\ncomputational algorithm so that this quantitative analysis can be easily\nperformed in other lactate threshold protocols. With this work, we have shown\nthat a heuristic %60 of 'endurance running speed reserve', serves for the same\npurpose of the most commonly used protocol in recreational runners, but\nimproving its operational limitations of accessibility and consistent use.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 11:09:58 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Etxegarai", "U.", ""], ["Portillo", "E.", ""], ["Irazusta", "J.", ""], ["Koefoed", "L. A.", ""], ["Kasabov", "N.", ""]]}, {"id": "1903.02345", "submitter": "Aldo Faisal", "authors": "Matthieu Komorowski, Leo A. Celi, Omar Badawi, Anthony C. Gordon and\n  A. Aldo Faisal", "title": "Understanding the Artificial Intelligence Clinician and optimal\n  treatment strategies for sepsis in intensive care", "comments": "13 pages and a number of figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this document, we explore in more detail our published work (Komorowski,\nCeli, Badawi, Gordon, & Faisal, 2018) for the benefit of the AI in Healthcare\nresearch community. In the above paper, we developed the AI Clinician system,\nwhich demonstrated how reinforcement learning could be used to make useful\nrecommendations towards optimal treatment decisions from intensive care data.\nSince publication a number of authors have reviewed our work (e.g. Abbasi,\n2018; Bos, Azoulay, & Martin-Loeches, 2019; Saria, 2018). Given the difference\nof our framework to previous work, the fact that we are bridging two very\ndifferent academic communities (intensive care and machine learning) and that\nour work has impact on a number of other areas with more traditional\ncomputer-based approaches (biosignal processing and control, biomedical\nengineering), we are providing here additional details on our recent\npublication.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 12:59:28 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Komorowski", "Matthieu", ""], ["Celi", "Leo A.", ""], ["Badawi", "Omar", ""], ["Gordon", "Anthony C.", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "1903.02538", "submitter": "Tobias M\\\"utze", "authors": "Tobias M\\\"utze, Susanna Salem, Norbert Benda, Heinz Schmidli, Tim\n  Friede", "title": "Blinded continuous information monitoring of recurrent event endpoints\n  with time trends in clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blinded sample size re-estimation and information monitoring based on blinded\ndata has been suggested to mitigate risks due to planning uncertainties\nregarding nuisance parameters. Motivated by a randomized controlled trial in\npediatric multiple sclerosis (MS), a continuous monitoring procedure for\noverdispersed count data was proposed recently. However, this procedure assumed\nconstant event rates, an assumption often not met in practice. Here we extend\nthe procedure to accommodate time trends in the event rates considering two\nblinded approaches: (a) the mixture approach modeling the number of events by a\nmixture of two negative binomial distributions, and (b) the lumping approach\napproximating the marginal distribution of the event counts by a negative\nbinomial distribution. Through simulations the operating characteristics of the\nproposed procedures are investigated under decreasing event rates. We find that\nthe type I error rate is not inflated relevantly by either of the monitoring\nprocedures, with the exception of strong time dependencies where the procedure\nassuming constant rates exhibits some inflation. Furthermore, the procedure\naccommodating time trends has generally favorable power properties compared to\nthe procedure based on constant rates which stops often too late. The proposed\nmethod is illustrated by the clinical trial in pediatric MS.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:24:02 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["M\u00fctze", "Tobias", ""], ["Salem", "Susanna", ""], ["Benda", "Norbert", ""], ["Schmidli", "Heinz", ""], ["Friede", "Tim", ""]]}, {"id": "1903.02543", "submitter": "Helena Mihaljevic", "authors": "Helena Mihaljevi\\'c and Marie-Fran\\c{c}oise Roy", "title": "A data analysis of women's trails among ICM speakers", "comments": "World Meeting of Women in Mathematics (WM^2), Satellite to the\n  International COngress of Mathematicians 2018, Rio de Janeiro, Brazil, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The International Congress of Mathematicians (ICM), inaugurated in 1897, is\nthe greatest effort of the mathematical community to strengthen international\ncommunication and connections across all mathematical fields. Meetings of the\nICM have historically hosted some of the most prominent mathematicians of their\ntime. Receiving an invitation to present a talk at an ICM signals the high\ninternational reputation of the recipient, and is akin to entering a `hall of\nfame for mathematics'. Women mathematicians attended the ICMs from the start.\nWith the invitation of Laura Pisati to present a lecture in 1908 in Rome and\nthe plenary talk of Emmy Noether in 1932 in Zurich, they entered the grand\ninternational stage of their field. At the congress in 2014 in Seoul, Maryam\nMirzakhani became the first woman to be awarded the Fields Medal, the most\nprestigious award in mathematics. In this article, we dive into assorted data\nsources to follow the footprints of women among the ICM invited speakers,\nanalyzing their demographics and topic distributions, and providing glimpses\ninto their diverse biographies.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:46:18 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 16:56:28 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Mihaljevi\u0107", "Helena", ""], ["Roy", "Marie-Fran\u00e7oise", ""]]}, {"id": "1903.02546", "submitter": "Shuang Li", "authors": "Shuang Li and James Lynch", "title": "A Sketch of Some Stochastic Models and Analysis Methods for Fiber Bundle\n  Failure under Increasing Tensile Load", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiber bundle models (FBM's) have been used to model the failure of fibrous\ncomposites as load-sharing systems since the 1960's when Rosen (1964 and 1965)\nconducted some remarkable experiments on unidirectional fibrous composites.\nThese experiments gave seminal insights into their failure under increasing\ntensile load. However, over the last thirty years FBM's have been used to model\ncatastrophic failure in other situations by the physical science community and\nothers. The purpose of this paper is to sketch some research on load-sharing\nmodels and statistical analysis methods that have been overlooked by this\ncommunity. These are illustrated by summarizing the findings regarding Rosen's\nSpecimen A experiments and presenting the necessary results needed for this.\nRelated research about the bundle breaking strength distribution and the joint\ndistribution (the Gibbs measure) regarding the state (failed or unfailed) of\nthe bundle components at a given load per component, $s$, is also given.\n", "versions": [{"version": "v1", "created": "Wed, 6 Mar 2019 18:52:33 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["Li", "Shuang", ""], ["Lynch", "James", ""]]}, {"id": "1903.02706", "submitter": "Amir Karami", "authors": "Amir Karami, Vishal Shah, Reza Vaezi, Amit Bansal", "title": "Twitter Speaks: A Case of National Disaster Situational Awareness", "comments": "17 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, we have been faced with a series of natural disasters\ncausing a tremendous amount of financial, environmental, and human losses. The\nunpredictable nature of natural disasters' behavior makes it hard to have a\ncomprehensive situational awareness (SA) to support disaster management. Using\nopinion surveys is a traditional approach to analyze public concerns during\nnatural disasters; however, this approach is limited, expensive, and\ntime-consuming. Luckily the advent of social media has provided scholars with\nan alternative means of analyzing public concerns. Social media enable users\n(people) to freely communicate their opinions and disperse information\nregarding current events including natural disasters. This research emphasizes\nthe value of social media analysis and proposes an analytical framework:\nTwitter Situational Awareness (TwiSA). This framework uses text mining methods\nincluding sentiment analysis and topic modeling to create a better SA for\ndisaster preparedness, response, and recovery. TwiSA has also effectively\ndeployed on a large number of tweets and tracks the negative concerns of people\nduring the 2015 South Carolina flood.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 03:02:00 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Karami", "Amir", ""], ["Shah", "Vishal", ""], ["Vaezi", "Reza", ""], ["Bansal", "Amit", ""]]}, {"id": "1903.02774", "submitter": "Katarzyna Reluga", "authors": "Katarzyna Reluga, Mar\\'ia Jos\\'e Lombard\\'ia, Stefan Andreas Sperlich", "title": "Simultaneous inference for mixed and small area parameters", "comments": "28 pages, 5 figures, a new version includes some changes regarding\n  the notation as well as methodological developments of the construction of\n  simultaneous prediction intervals and multiple tests", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address simultaneous inference for mixed parameters which are the key\ningredients in small area estimation. We assume linear mixed model framework.\nFirstly, we analyse statistical properties of a max-type statistic and use it\nto construct simultaneous prediction intervals as well as to implement multiple\ntesting procedure. Secondly, we derive bands based on the volume-of-tube\nformula. In addition, we adapt some of the simultaneous inference methods from\nregression and nonparametric curve estimation and compare them with our\napproaches. Simultaneous intervals are necessary to compare clusters since the\npresently available intervals are not statistically valid for such analysis.\nThe proposed testing procedures can be used to validate certain statements\nabout the set of mixed parameters or to test pairwise differences. Our proposal\nis accompanied by simulation experiments and a data example on small area\nhousehold incomes. Both of them demonstrate an excellent performance and\nutility of our techniques.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:53:13 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 08:18:16 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Reluga", "Katarzyna", ""], ["Lombard\u00eda", "Mar\u00eda Jos\u00e9", ""], ["Sperlich", "Stefan Andreas", ""]]}, {"id": "1903.02830", "submitter": "Marcos Capistran  Dr", "authors": "Marcos A Capistran, Juan Antonio Infante del Rio", "title": "Estimating a pressure dependent thermal conductivity coefficient with\n  applications in food technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a method to estimate a pressure dependent thermal\nconductivity coefficient arising in a heat diffusion model with applications in\nfood technology. To address the known smoothing effect of the direct problem,\nwe model the uncertainty of the conductivity coefficient as a hierarchical\nGaussian Markov random field (GMRF) restricted to uniqueness conditions for the\nsolution of the inverse problem established in Fraguela et al. Furthermore, we\npropose a Single Variable Exchange Metropolis-Hastings algorithm to sample the\ncorresponding conditional probability distribution of the conductivity\ncoefficient given observations of the temperature. Sensitivity analysis of the\ndirect problem suggests that large integration times are necessary to identify\nthe thermal conductivity coefficient. Numerical evidence indicates that a\nsignal to noise ratio of roughly 1000 suffices to reliably retrieve the thermal\nconductivity coefficient.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 11:01:44 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Capistran", "Marcos A", ""], ["del Rio", "Juan Antonio Infante", ""]]}, {"id": "1903.02909", "submitter": "Oliver Crook", "authors": "Oliver M. Crook, Kathryn S. Lilley, Laurent Gatto, Paul D. W. Kirk", "title": "Semi-Supervised Non-Parametric Bayesian Modelling of Spatial Proteomics", "comments": "44 pages, 10 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding sub-cellular protein localisation is an essential component to\nanalyse context specific protein function. Recent advances in quantitative\nmass-spectrometry (MS) have led to high resolution mapping of thousands of\nproteins to sub-cellular locations within the cell. Novel modelling\nconsiderations to capture the complex nature of these data are thus necessary.\nWe approach analysis of spatial proteomics data in a non-parametric Bayesian\nframework, using mixtures of Gaussian process regression models. The Gaussian\nprocess regression model accounts for correlation structure within a\nsub-cellular niche, with each mixture component capturing the distinct\ncorrelation structure observed within each niche. Proteins with a priori\nlabelled locations motivate using semi-supervised learning to inform the\nGaussian process hyperparameters. We moreover provide an efficient\nHamiltonian-within-Gibbs sampler for our model. As in other recent work, we\nreduce the computational burden associated with inversion of covariance\nmatrices by exploiting the structure in the covariance matrix. A tensor\ndecomposition of our covariance matrices allows extended Trench and Durbin\nalgorithms to be applied it order to reduce the computational complexity of\ninversion and hence accelerate computation. A stand-alone R-package\nimplementing these methods using high-performance C++ libraries is available\nat: https://github.com/ococrook/toeplitz\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 13:52:43 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 17:09:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Crook", "Oliver M.", ""], ["Lilley", "Kathryn S.", ""], ["Gatto", "Laurent", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "1903.02964", "submitter": "Kody Law", "authors": "Ryan Bennink, Ajay Jasra, Kody J. H. Law, Pavel Lougovski", "title": "Estimation and uncertainty quantification for the output from quantum\n  simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO quant-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating certain distributions over $\\{0,1\\}^d$ is\nconsidered here. The distribution represents a quantum system of $d$ qubits,\nwhere there are non-trivial dependencies between the qubits. A maximum entropy\napproach is adopted to reconstruct the distribution from exact moments or\nobserved empirical moments. The Robbins Monro algorithm is used to solve the\nintractable maximum entropy problem, by constructing an unbiased estimator of\nthe un-normalized target with a sequential Monte Carlo sampler at each\niteration. In the case of empirical moments, this coincides with a maximum\nlikelihood estimator. A Bayesian formulation is also considered in order to\nquantify posterior uncertainty. Several approaches are proposed in order to\ntackle this challenging problem, based on recently developed methodologies. In\nparticular, unbiased estimators of the gradient of the log posterior are\nconstructed and used within a provably convergent Langevin-based Markov chain\nMonte Carlo method. The methods are illustrated on classically simulated output\nfrom quantum simulators.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 14:55:30 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Bennink", "Ryan", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Lougovski", "Pavel", ""]]}, {"id": "1903.03104", "submitter": "James Bagrow", "authors": "Abigail Hotaling and James P. Bagrow", "title": "Accurate inference of crowdsourcing properties when using efficient\n  allocation strategies", "comments": "21 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.HC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Allocation strategies improve the efficiency of crowdsourcing by decreasing\nthe work needed to complete individual tasks accurately. However, these\nalgorithms introduce bias by preferentially allocating workers onto easy tasks,\nleading to sets of completed tasks that are no longer representative of all\ntasks. This bias challenges inference of problem-wide properties such as\ntypical task difficulty or crowd properties such as worker completion times,\nimportant information that goes beyond the crowd responses themselves. Here we\nstudy inference about problem properties when using an allocation algorithm to\nimprove crowd efficiency. We introduce Decision-Explicit Probability Sampling\n(DEPS), a method to perform inference of problem properties while accounting\nfor the potential bias introduced by an allocation strategy. Experiments on\nreal and synthetic crowdsourcing data show that DEPS outperforms baseline\ninference methods while still leveraging the efficiency gains of the allocation\nmethod. The ability to perform accurate inference of general properties when\nusing non-representative data allows crowdsourcers to extract more knowledge\nout of a given crowdsourced dataset.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 18:58:34 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Hotaling", "Abigail", ""], ["Bagrow", "James P.", ""]]}, {"id": "1903.03149", "submitter": "Azra Bihorac", "authors": "Tezcan Ozrazgat-Baslanti, Amir Motaei, Rubab Islam, Haleh\n  Hashemighouchani, Matthew Ruppert, R. W. M. A. Madushani, Mark S. Segal,\n  Gloria Lipori, Azra Bihorac, Charles Hobson", "title": "Development and validation of computable Phenotype to Identify and\n  Characterize Kidney Health in Adult Hospitalized Patients", "comments": "Changes in Funding, and authorship sequence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Acute kidney injury (AKI) is a common complication in\nhospitalized patients and a common cause for chronic kidney disease (CKD) and\nincreased hospital cost and mortality. By timely detection of AKI and AKI\nprogression, effective preventive or therapeutic measures could be offered.\nThis study aims to develop and validate an electronic phenotype to identify\npatients with CKD and AKI. Methods: A database with electronic health records\ndata from a retrospective study cohort of 84,352 hospitalized adults was\ncreated. This repository includes demographics, comorbidities, vital signs,\nlaboratory values, medications, diagnoses and procedure codes for all index\nadmission, 12 months prior and 12 months follow-up encounters. We developed\nalgorithms to identify CKD and AKI based on the Kidney Disease: Improving\nGlobal Outcomes (KDIGO) criteria. To measure diagnostic performance of the\nalgorithms, clinician experts performed clinical adjudication of AKI and CKD on\n300 selected cases. Results: Among 149,136 encounters, identified CKD by\nmedical history was 12% which increased to 16% using creatinine criteria. Among\n130,081 encounters with sufficient data for AKI phenotyping 21% had AKI. The\ncomparison of CKD phenotyping algorithm to manual chart review yielded PPV of\n0.87, NPV of 0.99, sensitivity of 0.99, and specificity of 0.89. The comparison\nof AKI phenotyping algorithm to manual chart review yielded PPV of 0.99, NPV of\n0.95 , sensitivity 0.98, and specificity 0.98. Conclusions: We developed\nphenotyping algorithms that yielded very good performance in identification of\npatients with CKD and AKI in validation cohort. This tool may be useful in\nidentifying patients with kidney disease in a large population, in assessing\nthe quality and value of care in such patients.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 19:51:23 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 18:21:35 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Ozrazgat-Baslanti", "Tezcan", ""], ["Motaei", "Amir", ""], ["Islam", "Rubab", ""], ["Hashemighouchani", "Haleh", ""], ["Ruppert", "Matthew", ""], ["Madushani", "R. W. M. A.", ""], ["Segal", "Mark S.", ""], ["Lipori", "Gloria", ""], ["Bihorac", "Azra", ""], ["Hobson", "Charles", ""]]}, {"id": "1903.03202", "submitter": "Xiao Qiao", "authors": "Alexander James, Yaser S. Abu-Mostafa, Xiao Qiao", "title": "Nowcasting Recessions using the SVM Machine Learning Algorithm", "comments": "My company policy about sharing research papers has been changed. As\n  a result, I would like to withdraw the paper, with the full understanding\n  that previous version will remain accessible. Thank you very much", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN cs.LG econ.GN q-fin.EC stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel application of Support Vector Machines (SVM), an\nimportant Machine Learning algorithm, to determine the beginning and end of\nrecessions in real time. Nowcasting, \"forecasting\" a condition about the\npresent time because the full information about it is not available until\nlater, is key for recessions, which are only determined months after the fact.\nWe show that SVM has excellent predictive performance for this task, and we\nprovide implementation details to facilitate its use in similar problems in\neconomics and finance.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 15:04:35 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 14:53:06 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["James", "Alexander", ""], ["Abu-Mostafa", "Yaser S.", ""], ["Qiao", "Xiao", ""]]}, {"id": "1903.03223", "submitter": "Owen Ward", "authors": "Jing Wu, Owen Ward, James Curley, Tian Zheng", "title": "Markov-Modulated Hawkes Processes for Sporadic and Bursty Event\n  Occurrences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling event dynamics is central to many disciplines. Patterns in observed\nevent arrival times are commonly modeled using point processes. Such event\narrival data often exhibits self-exciting, heterogeneous and sporadic trends,\nwhich is challenging for conventional models. It is reasonable to assume that\nthere exists a hidden state process that drives different event dynamics at\ndifferent states. In this paper, we propose a Markov Modulated Hawkes Process\n(MMHP) model for learning such a mixture of event dynamics and develop\ncorresponding inference algorithms. Numerical experiments using synthetic data\ndemonstrate that MMHP with the proposed estimation algorithms consistently\nrecover the true hidden state process in simulations, while email data from a\nlarge university and data from an animal behavior study show that the procedure\ncaptures distinct event dynamics that reveal interesting social structures in\nthe real data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 00:07:28 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 15:50:09 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:13:55 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Wu", "Jing", ""], ["Ward", "Owen", ""], ["Curley", "James", ""], ["Zheng", "Tian", ""]]}, {"id": "1903.03676", "submitter": "Lei Zhang", "authors": "Lei Zhang, Sean Howard, Tom Montpool, Jessica Moore, Krittika Mahajan,\n  Andriy Miranskyy", "title": "RESTORE: Automated Regression Testing for Datasets", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.SE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data mining, the data in various business cases (e.g., sales, marketing,\nand demography) gets refreshed periodically. During the refresh, the old\ndataset is replaced by a new one. Confirming the quality of the new dataset can\nbe challenging because changes are inevitable.\n  How do analysts distinguish reasonable real-world changes vs. errors related\nto data capture or data transformation? While some of the errors are easy to\nspot, the others may be more subtle. In order to detect such types of errors,\nan analyst will typically have to examine the data manually and assess if the\ndata produced are \"believable\". Due to the scale of data, such examination is\ntedious and laborious. Thus, to save the analyst's time, it is important to\ndetect these errors automatically. However, both the literature and the\nindustry are still lacking methods to assess the difference between old and new\nversions of a dataset during the refresh process.\n  In this paper, we present a comprehensive set of tests for the detection of\nabnormalities in a refreshed dataset, based on the information obtained from a\nprevious vintage of the dataset. We implement these tests in automated test\nharness made available as an open-source package, called RESTORE, for R\nlanguage. The harness accepts flat or hierarchical numeric datasets. We also\npresent a validation case study, where we apply our test harness to\nhierarchical demographic datasets. The results of the study and feedback from\ndata scientists using the package suggest that RESTORE enables fast and\nefficient detection of errors in the data as well as decreases the cost of\ntesting.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 21:47:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Zhang", "Lei", ""], ["Howard", "Sean", ""], ["Montpool", "Tom", ""], ["Moore", "Jessica", ""], ["Mahajan", "Krittika", ""], ["Miranskyy", "Andriy", ""]]}, {"id": "1903.03883", "submitter": "Peng Ding", "authors": "Peng Ding", "title": "Two seemingly paradoxical results in linear models: the variance\n  inflation factor and the analysis of covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A result from a standard linear model course is that the variance of the\nordinary least squares (OLS) coefficient of a variable will never decrease when\nincluding additional covariates. The variance inflation factor (VIF) measures\nthe increase of the variance. Another result from a standard linear model or\nexperimental design course is that including additional covariates in a linear\nmodel of the outcome on the treatment indicator will never increase the\nvariance of the OLS coefficient of the treatment at least asymptotically. This\ntechnique is called the analysis of covariance (ANCOVA), which is often used to\nimprove the efficiency of treatment effect estimation. So we have two\nparadoxical results: adding covariates never decreases the variance in the\nfirst result but never increases the variance in the second result. In fact,\nthese two results are derived under different assumptions. More precisely, the\nVIF result conditions on the treatment indicators but the ANCOVA result\naverages over them. Comparing the estimators with and without adjusting for\nadditional covariates in a completely randomized experiment, I show that the\nformer has smaller variance averaging over the treatment indicators, and the\nlatter has smaller variance at the cost of a larger bias conditioning on the\ntreatment indicators. Therefore, there is no real paradox.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 22:38:07 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 22:37:00 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 05:47:22 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Ding", "Peng", ""]]}, {"id": "1903.03933", "submitter": "Sergey Alyaev", "authors": "Sergey Alyaev, Erich Suter, Reidar Bratvold, Aojie Hong, and Xiaodong\n  Luo", "title": "A Decision Support System for Multi-target Geosteering", "comments": "Published in Open Access: Journal of Petroleum Science and\n  Engineering 2019 (106381)", "journal-ref": "Journal of Petroleum Science and Engineering Volume 183, December\n  2019, 106381", "doi": "10.1016/j.petrol.2019.106381", "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geosteering is a sequential decision process under uncertainty. The goal of\ngeosteering is to maximize the expected value of the well, which should be\ndefined by an objective value-function for each operation.\n  In this paper we present a real-time decision support system (DSS) for\ngeosteering that aims to approximate the uncertainty in the geological\ninterpretation with an ensemble of geomodel realizations. As the drilling\noperation progresses, the ensemble Kalman filter is used to sequentially update\nthe realizations using the measurements from real-time logging while drilling.\nAt every decision point a discrete dynamic programming algorithm computes all\npotential well trajectories for the entire drilling operation and the\ncorresponding value of the well for each realization. Then, the DSS considers\nall immediate alternatives (continue/steer/stop) and chooses the one that gives\nthe best predicted value across the realizations. This approach works for a\nvariety of objectives and constraints and suggests reproducible decisions under\nuncertainty. Moreover, it has real-time performance.\n  The system is tested on synthetic cases in a layer-cake geological\nenvironment where the target layer should be selected dynamically based on the\nprior (pre-drill) model and the electromagnetic observations received while\ndrilling. The numerical closed-loop simulation experiments demonstrate the\nability of the DSS to perform successful geosteering and landing of a well for\ndifferent geological configurations of drilling targets. Furthermore, the DSS\nallows to adjust and reweight the objectives, making the DSS useful before\nfully automated geosteering becomes reality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 06:07:31 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 09:22:52 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Alyaev", "Sergey", ""], ["Suter", "Erich", ""], ["Bratvold", "Reidar", ""], ["Hong", "Aojie", ""], ["Luo", "Xiaodong", ""]]}, {"id": "1903.04035", "submitter": "George Liberopoulos", "authors": "George Liberopoulos and Isidoros Tsikis", "title": "Retailer response to wholesale stockouts", "comments": null, "journal-ref": null, "doi": null, "report-no": "Working Paper, Department of Mechanical Engineering, University of\n  Thessaly, Volos, Greece, Nov 2012", "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to identify the immediate and future retailer\nresponse to wholesale stockouts. We perform a statistical analysis of\nhistorical customer order and delivery data of a local tool wholesaler and\ndistributor, whose customers are retailers, over a period of four years. We\ninvestigate the effect of customer service on the order fill rate and the rate\nof future demand, where the customer service is defined in terms of timely\ndelivery and the fill rate is defined as the fraction of the order that is\neventually materialized, i.e., is not cancelled following a stockout.We find\nthat for customers who order frequently, stockouts have an adverse effect on\nthe fill rate of their orders and on the frequency but not the value of their\nfuture demand; however, this latter effect seems to be more short-term than\nlong-term. Practically all studies on the effects of stockouts measure\nimmediate reported/intended consumer purchase incidence and choice decision\nbehavior in response to stockouts in retail environments, mostly based on\nsurveys. This study looks at how stockouts affect future demand in a wholesale\nenvironment, based on historical behavioral data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 10 Mar 2019 18:23:28 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Liberopoulos", "George", ""], ["Tsikis", "Isidoros", ""]]}, {"id": "1903.04421", "submitter": "Rob Brisk", "authors": "Rob Brisk, Raymond R Bond. Dewar D Finlay, James McLaughlin, Alicja\n  Piadlo, Stephen J Leslie, David E Gossman, Ian B A Menown and David J\n  McEneaney", "title": "Augmenting expert detection of early coronary artery occlusion from 12\n  lead electrocardiograms using deep learning", "comments": "Our attempts to produce what we considered to be an acceptable level\n  of explainability from our algorithm have not yielded a satisfactory account\n  of its internal logic and we do not feel this is acceptable from a clinical\n  application. We will publish a fuller account of our work on this issue and\n  its implications on the validation of clinical deep learning algorithms in\n  the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early diagnosis of acute coronary artery occlusion based on electrocardiogram\n(ECG) findings is essential for prompt delivery of primary percutaneous\ncoronary intervention. Current ST elevation (STE) criteria are specific but\ninsensitive. Consequently, it is likely that many patients are missing out on\npotentially life-saving treatment. Experts combining non-specific ECG changes\nwith STE detect ischaemia with higher sensitivity, but at the cost of\nspecificity. We show that a deep learning model can detect ischaemia caused by\nacute coronary artery occlusion with a better balance of sensitivity and\nspecificity than STE criteria, existing computerised analysers or expert\ncardiologists.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 16:33:10 GMT"}, {"version": "v2", "created": "Wed, 13 Mar 2019 09:16:26 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 13:19:45 GMT"}, {"version": "v4", "created": "Mon, 18 Nov 2019 10:15:13 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Brisk", "Rob", ""], ["Finlay", "Raymond R Bond. Dewar D", ""], ["McLaughlin", "James", ""], ["Piadlo", "Alicja", ""], ["Leslie", "Stephen J", ""], ["Gossman", "David E", ""], ["Menown", "Ian B A", ""], ["McEneaney", "David J", ""]]}, {"id": "1903.04701", "submitter": "Konrad Sakowski", "authors": "Monika J. Piotrowska and Konrad Sakowski", "title": "Analysis of the AOK Lower Saxony hospitalisation records data (years\n  2008 -- 2015)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidrug-resistant Enterobacteriaceae (MDR-E) have become a major public\nhealth threat in many European countries. While traditional infection control\nstrategies primarily target the containment of intra-hospital transmission,\nthere is growing evidence highlighting the importance of inter-hospital patient\ntraffic for the spread of MDR-E within healthcare systems.\n  Our aim is to propose a network model, which will reflect patient traffic in\nvarious European healthcare systems and will thus provide the framework to\nstudy systematically transmission dynamics of MDR-E and the effectiveness of\ninfection control strategies to contain their spread within and potentially\nacross healthcare systems. However, to do that first we need to analyse real\npatients data and base on that propose network model reflecting the complexity\nof the real hospital network connections and dynamics of patient transfers\nbetween healthcare facilities.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 02:05:14 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Piotrowska", "Monika J.", ""], ["Sakowski", "Konrad", ""]]}, {"id": "1903.04881", "submitter": "John Muschelli III", "authors": "John Muschelli", "title": "ROC and AUC with a Binary Predictor: a Potentially Misleading Metric", "comments": "16 pages, 3 figures, code. J Classif (2019)", "journal-ref": null, "doi": "10.1007/s00357-019-09345-1", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In analysis of binary outcomes, the receiver operator characteristic (ROC)\ncurve is heavily used to show the performance of a model or algorithm. The ROC\ncurve is informative about the performance over a series of thresholds and can\nbe summarized by the area under the curve (AUC), a single number. When a\npredictor is categorical, the ROC curve has one less than number of categories\nas potential thresholds; when the predictor is binary there is only one\nthreshold. As the AUC may be used in decision-making processes on determining\nthe best model, it important to discuss how it agrees with the intuition from\nthe ROC curve. We discuss how the interpolation of the curve between thresholds\nwith binary predictors can largely change the AUC. Overall, we show using a\nlinear interpolation from the ROC curve with binary predictors corresponds to\nthe estimated AUC, which is most commonly done in software, which we believe\ncan lead to misleading results. We compare R, Python, Stata, and SAS software\nimplementations. We recommend using reporting the interpolation used and\ndiscuss the merit of using the step function interpolator, also referred to as\nthe \"pessimistic\" approach by Fawcett (2006).\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 12:57:58 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 21:28:48 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Muschelli", "John", ""]]}, {"id": "1903.04891", "submitter": "Richard D. Gill", "authors": "Martin Neil, Norman Fenton, David Lagnado, Richard D. Gill", "title": "Modelling Competing Legal Arguments using Bayesian Model Comparison and\n  Averaging", "comments": null, "journal-ref": "Artif. Intell. Law 27, 403-430 (2019)", "doi": "10.1007/s10506-019-09250-3", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models of legal arguments generally aim to produce a single\nintegrated model, combining each of the legal arguments under consideration.\nThis combined approach implicitly assumes that variables and their\nrelationships can be represented without any contradiction or misalignment, and\nin a way that makes sense with respect to the competing argument narratives.\nThis paper describes a novel approach to compare and 'average' Bayesian models\nof legal arguments that have been built independently and with no attempt to\nmake them consistent in terms of variables, causal assumptions or\nparametrisation. The approach involves assessing whether competing models of\nlegal arguments are explained or predict facts uncovered before or during the\ntrial process. Those models that are more heavily disconfirmed by the facts are\ngiven lower weight, as model plausibility measures, in the Bayesian model\ncomparison and averaging framework adopted. In this way a plurality of\narguments is allowed yet a single judgement based on all arguments is possible\nand rational.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 10:00:52 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 16:35:02 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Neil", "Martin", ""], ["Fenton", "Norman", ""], ["Lagnado", "David", ""], ["Gill", "Richard D.", ""]]}, {"id": "1903.04955", "submitter": "Tuan-Binh Nguyen", "authors": "Tuan-Binh Nguyen, J\\'er\\^ome-Alexis Chevalier and Bertrand Thirion", "title": "ECKO: Ensemble of Clustered Knockoffs for multivariate inference on fMRI\n  data", "comments": "Accepted to 26th International Conference on Information Processing\n  in Medical Imaging (IPMI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous improvement in medical imaging techniques allows the acquisition\nof higher-resolution images. When these are used in a predictive setting, a\ngreater number of explanatory variables are potentially related to the\ndependent variable (the response). Meanwhile, the number of acquisitions per\nexperiment remains limited. In such high dimension/small sample size setting,\nit is desirable to find the explanatory variables that are truly related to the\nresponse while controlling the rate of false discoveries. To achieve this goal,\nnovel multivariate inference procedures, such as knockoff inference, have been\nproposed recently. However, they require the feature covariance to be\nwell-defined, which is impossible in high-dimensional settings. In this paper,\nwe propose a new algorithm, called Ensemble of Clustered Knockoffs, that allows\nto select explanatory variables while controlling the false discovery rate\n(FDR), up to a prescribed spatial tolerance. The core idea is that\nknockoff-based inference can be applied on groups (clusters) of voxels, which\ndrastically reduces the problem's dimension; an ensembling step then removes\nthe dependence on a fixed clustering and stabilizes the results. We benchmark\nthis algorithm and other FDR-controlling methods on brain imaging datasets and\nobserve empirical gains in sensitivity, while the false discovery rate is\ncontrolled at the nominal level.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 14:37:32 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Nguyen", "Tuan-Binh", ""], ["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Thirion", "Bertrand", ""]]}, {"id": "1903.04999", "submitter": "Ethan Lawler", "authors": "Ethan Lawler, Kim Whoriskey, William H. Aeberhard, Chris Field, Joanna\n  Mills Flemming", "title": "The conditionally autoregressive hidden Markov model (CarHMM): Inferring\n  behavioural states from animal tracking data exhibiting conditional\n  autocorrelation", "comments": null, "journal-ref": null, "doi": "10.1007/s13253-019-00366-2", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the central interests of animal movement ecology is relating movement\ncharacteristics to behavioural characteristics. The traditional discrete-time\nstatistical tool for inferring unobserved behaviours from movement data is the\nhidden Markov model (HMM). While the HMM is an important and powerful tool,\nsometimes it is not flexible enough to appropriately fit the data. Data for\nmarine animals often exhibit conditional autocorrelation, self-dependence of\nthe step length process which cannot be explained solely by the behavioural\nstate, which violates one of the main assumptions of the HMM. Using a grey seal\ntrack as an example, along with multiple simulation scenarios, we motivate and\ndevelop the conditionally autoregressive hidden Markov model (CarHMM), which is\na generalization of the HMM designed specifically to handle conditional\nautocorrelation.\n  In addition to introducing and examining the new CarHMM, we provide\nguidelines for all stages of an analysis using either an HMM or CarHMM. These\ninclude guidelines for pre-processing location data to obtain deflection angles\nand step lengths, model selection, and model checking. In addition to these\npractical guidelines, we link estimated model parameters to biologically\nmeaningful quantities such as activity budget and residency time. We also\nprovide interpretations of traditional \"foraging\" and \"transiting\" behaviours\nin the context of the new CarHMM parameters.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 15:30:15 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Lawler", "Ethan", ""], ["Whoriskey", "Kim", ""], ["Aeberhard", "William H.", ""], ["Field", "Chris", ""], ["Flemming", "Joanna Mills", ""]]}, {"id": "1903.05036", "submitter": "John Tipton", "authors": "John R. Tipton, Mevin B. Hooten, Connor Nolan, Robert K. Booth, and\n  Jason McLachlan", "title": "Predicting paleoclimate from compositional data using multivariate\n  Gaussian process inverse prediction", "comments": "20 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate compositional count data arise in many applications including\necology, microbiology, genetics, and paleoclimate. A frequent question in the\nanalysis of multivariate compositional count data is what values of a\ncovariate(s) give rise to the observed composition. Learning the relationship\nbetween covariates and the compositional count allows for inverse prediction of\nunobserved covariates given compositional count observations. Gaussian\nprocesses provide a flexible framework for modeling functional responses with\nrespect to a covariate without assuming a functional form. Many scientific\ndisciplines use Gaussian process approximations to improve prediction and make\ninference on latent processes and parameters. When prediction is desired on\nunobserved covariates given realizations of the response variable, this is\ncalled inverse prediction. Because inverse prediction is mathematically and\ncomputationally challenging, predicting unobserved covariates often requires\nfitting models that are different from the hypothesized generative model. We\npresent a novel computational framework that allows for efficient inverse\nprediction using a Gaussian process approximation to generative models. Our\nframework enables scientific learning about how the latent processes co-vary\nwith respect to covariates while simultaneously providing predictions of\nmissing covariates. The proposed framework is capable of efficiently exploring\nthe high dimensional, multi-modal latent spaces that arise in the inverse\nproblem. To demonstrate flexibility, we apply our method in a generalized\nlinear model framework to predict latent climate states given multivariate\ncount data. Based on cross-validation, our model has predictive skill\ncompetitive with current methods while simultaneously providing formal,\nstatistical inference on the underlying community dynamics of the biological\nsystem previously not available.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 16:37:36 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Tipton", "John R.", ""], ["Hooten", "Mevin B.", ""], ["Nolan", "Connor", ""], ["Booth", "Robert K.", ""], ["McLachlan", "Jason", ""]]}, {"id": "1903.05403", "submitter": "Marina Friedrich", "authors": "Marina Friedrich, Eric Beutner, Hanno Reuvers, Stephan Smeekes,\n  Jean-Pierre Urbain, Whitney Bader, Bruno Franco, Bernard Lejeune, Emmanuel\n  Mahieu", "title": "A statistical analysis of time trends in atmospheric ethane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ethane is the most abundant non-methane hydrocarbon in the Earth's atmosphere\nand an important precursor of tropospheric ozone through various chemical\npathways. Ethane is also an indirect greenhouse gas (global warming potential),\ninfluencing the atmospheric lifetime of methane through the consumption of the\nhydroxyl radical (OH). Understanding the development of trends and identifying\ntrend reversals in atmospheric ethane is therefore crucial. Our dataset\nconsists of four series of daily ethane columns obtained from ground-based FTIR\nmeasurements. As many other decadal time series, our data are characterized by\nautocorrelation, heteroskedasticity, and seasonal effects. Additionally,\nmissing observations due to instrument failure or unfavorable measurement\nconditions are common in such series. The goal of this paper is therefore to\nanalyze trends in atmospheric ethane with statistical tools that correctly\naddress these data features. We present selected methods designed for the\nanalysis of time trends and trend reversals. We consider bootstrap inference on\nbroken linear trends and smoothly varying nonlinear trends. In particular, for\nthe broken trend model, we propose a bootstrap method for inference on the\nbreak location and the corresponding changes in slope. For the smooth trend\nmodel we construct simultaneous confidence bands around the nonparametrically\nestimated trend. Our autoregressive wild bootstrap approach, combined with a\nseasonal filter, is able to handle all issues mentioned above.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 10:32:19 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 11:47:08 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Friedrich", "Marina", ""], ["Beutner", "Eric", ""], ["Reuvers", "Hanno", ""], ["Smeekes", "Stephan", ""], ["Urbain", "Jean-Pierre", ""], ["Bader", "Whitney", ""], ["Franco", "Bruno", ""], ["Lejeune", "Bernard", ""], ["Mahieu", "Emmanuel", ""]]}, {"id": "1903.05481", "submitter": "Nadhir Ben Rached", "authors": "Nadhir Ben Rached, Abla Kammoun, Mohamed-Slim Alouini, Raul Tempone", "title": "An Accurate Sample Rejection Estimator for the Estimation of Outage\n  Probability of EGC Receivers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we evaluate the outage probability (OP) for L-branch equal gain\ncombining (EGC) diversity receivers operating over fading channels, i.e.\nequivalently the cumulative distribution function (CDF) of the sum of the L\nchannel envelopes. In general, closed form expressions of OP values are\nunobtainable. The use of Monte Carlo (MC) simulations is not considered a good\nalternative as it requires a large number of samples for small values of OP,\nmaking MC simulations very expensive. In this paper, we use the concept of\nimportance sampling (IS), being known to yield accurate estimates using fewer\nsimulation runs. Our proposed IS scheme is essentially based on sample\nrejection where the IS probability density function (PDF) is the truncation of\nthe underlying PDF over the L dimensional sphere. It assumes the knowledge of\nthe CDF of the sum of the L channel gains in a closed-form expression. Such an\nassumption is not restrictive since it holds for various challenging fading\nmodels. We apply our approach to the case of independent Rayleigh, correlated\nRayleigh, and independent and identically distributed Rice fading models. Next,\nwe extend our approach to the interesting scenario of generalised selection\ncombining receivers combined with EGC under the independent Rayleigh fading\nenvironment. For each case, we prove the desired bounded relative error\nproperty. Finally, we validate these theoretical results through some selected\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 13:38:17 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Rached", "Nadhir Ben", ""], ["Kammoun", "Abla", ""], ["Alouini", "Mohamed-Slim", ""], ["Tempone", "Raul", ""]]}, {"id": "1903.05701", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Chiara Sabatti, Emmanuel J. Cand\\`es", "title": "Rejoinder: \"Gene Hunting with Hidden Markov Model Knockoffs\"", "comments": "12 pages, 4 figures", "journal-ref": "Biometrika, Volume 106, Issue 1, 1 March 2019, Pages 35-45", "doi": "10.1093/biomet/asy075", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deepen and enlarge the reflection on the possible advantages\nof a knockoff approach to genome wide association studies (Sesia et al., 2018),\nstarting from the discussions in Bottolo & Richardson (2019); Jewell & Witten\n(2019); Rosenblatt et al. (2019) and Marchini (2019). The discussants bring up\na number of important points, either related to the knockoffs methodology in\ngeneral, or to its specific application to genetic studies. In the following we\noffer some clarifications, mention relevant recent developments and highlight\nsome of the still open problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 19:58:31 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Sesia", "Matteo", ""], ["Sabatti", "Chiara", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1903.05715", "submitter": "Heather Battey Dr", "authors": "Henrique Helfer Hoeltgebaum and Heather Battey", "title": "HCmodelSets: An R package for specifying sets of well-fitting models in\n  regression with a large number of potential explanatory variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of regression with a large number of explanatory variables,\nCox and Battey (2017) emphasize that if there are alternative reasonable\nexplanations of the data that are statistically indistinguishable, one should\naim to specify as many of these explanations as is feasible. The standard\npractice, by contrast, is to report a single model effective for prediction.\nThe present paper illustrates the R implementation of the new ideas in the\npackage `HCmodelSets', using simple reproducible examples and real data.\nResults of some simulation experiments are also reported.\n", "versions": [{"version": "v1", "created": "Wed, 13 Mar 2019 21:15:42 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Hoeltgebaum", "Henrique Helfer", ""], ["Battey", "Heather", ""]]}, {"id": "1903.05851", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Peng Shi, Jae Youn Ahn", "title": "Implementation of Frequency-Severity Association in BMS Ratemaking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bonus-Malus System (BMS) in insurance is a premium adjustment mechanism\nwidely used in a posteriori ratemaking process to set the premium for the next\ncontract period based on a policyholder's claim history. The current practice\nin BMS implementation relies on the assumption of independence between claim\nfrequency and severity, despite the fact that a series of recent studies report\nevidence of a significant frequency-severity relationship, particularly in\nautomobile insurance. To address this discrepancy, we propose a copula-based\ncorrelated random effects model to accommodate the dependence between claim\nfrequency and severity, and further illustrate how to incorporate such\ndependence into the current BMS. We derive analytical solutions to the optimal\nrelativities under the proposed framework and provide numerical experiments\nbased on real data analysis to assess the effect of frequency-severity\ndependence in BMS ratemaking.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 08:18:30 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Oh", "Rosy", ""], ["Shi", "Peng", ""], ["Ahn", "Jae Youn", ""]]}, {"id": "1903.06165", "submitter": "Francisco J. Beron-Vera", "authors": "P. Miron, F.J. Beron-Vera, M.J. Olascoaga and P. Koltai", "title": "Markov-chain-inspired search for MH370", "comments": "Submitted to Chaos", "journal-ref": "Chaos 29, 041105 (2019)", "doi": "10.1063/1.5092132", "report-no": null, "categories": "stat.AP math.ST physics.ao-ph physics.pop-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov-chain models are constructed for the probabilistic description of the\ndrift of marine debris from Malaysian Airlines flight MH370. En route from\nKuala Lumpur to Beijing, the MH370 mysteriously disappeared in the southeastern\nIndian Ocean on 8 March 2014, somewhere along the arc of the 7th ping ring\naround the Inmarsat-3F1 satellite position when the airplane lost contact. The\nmodels are obtained by discretizing the motion of undrogued satellite-tracked\nsurface drifting buoys from the global historical data bank. A spectral\nanalysis, Bayesian estimation, and the computation of most probable paths\nbetween the Inmarsat arc and confirmed airplane debris beaching sites are shown\nto constrain the crash site, near 25$^{\\circ}$S on the Inmarsat arc.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 14:36:33 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Miron", "P.", ""], ["Beron-Vera", "F. J.", ""], ["Olascoaga", "M. J.", ""], ["Koltai", "P.", ""]]}, {"id": "1903.06286", "submitter": "Peng Ding", "authors": "Peng Ding, Fan Li", "title": "A bracketing relationship between difference-in-differences and\n  lagged-dependent-variable adjustment", "comments": "To appear in Political Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Difference-in-differences is a widely-used evaluation strategy that draws\ncausal inference from observational panel data. Its causal identification\nrelies on the assumption of parallel trends, which is scale dependent and may\nbe questionable in some applications. A common alternative is a regression\nmodel that adjusts for the lagged dependent variable, which rests on the\nassumption of ignorability conditional on past outcomes. In the context of\nlinear models, \\citet{APbook} show that the difference-in-differences and\nlagged-dependent-variable regression estimates have a bracketing relationship.\nNamely, for a true positive effect, if ignorability is correct, then mistakenly\nassuming parallel trends will overestimate the effect; in contrast, if the\nparallel trends assumption is correct, then mistakenly assuming ignorability\nwill underestimate the effect. We show that the same bracketing relationship\nholds in general nonparametric (model-free) settings. We also extend the result\nto semiparametric estimation based on inverse probability weighting. We provide\nthree examples to illustrate the theoretical results with replication files in\n\\citet{ding2019bracketingData}.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 22:35:36 GMT"}, {"version": "v2", "created": "Thu, 4 Apr 2019 04:18:30 GMT"}, {"version": "v3", "created": "Sat, 22 Jun 2019 00:11:00 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Ding", "Peng", ""], ["Li", "Fan", ""]]}, {"id": "1903.06296", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Igor Rychlik", "title": "Deformed SPDE models with an application to spatial modeling of\n  significant wave height", "comments": "22 pages, 12 figures", "journal-ref": "Spatial Statistics (2020)", "doi": "10.1016/j.spasta.2020.100449", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-stationary Gaussian random field model is developed based on a\ncombination of the stochastic partial differential equation (SPDE) approach and\nthe classical deformation method. With the deformation method, a stationary\nfield is defined on a domain which is deformed so that the field becomes\nnon-stationary. We show that if the stationary field is a Mat'ern field defined\nas a solution to a fractional SPDE, the resulting non-stationary model can be\nrepresented as the solution to another fractional SPDE on the deformed domain.\nBy defining the model in this way, the computational advantages of the SPDE\napproach can be combined with the deformation method's more intuitive\nparameterisation of non-stationarity. In particular it allows for independent\ncontrol over the non-stationary practical correlation range and the variance,\nwhich has not been possible with previously proposed non-stationary SPDE\nmodels.\n  The model is tested on spatial data of significant wave height, a\ncharacteristic of ocean surface conditions which is important when estimating\nthe wear and risks associated with a planned journey of a ship. The model\nparameters are estimated to data from the north Atlantic using a maximum\nlikelihood approach. The fitted model is used to compute wave height exceedance\nprobabilities and the distribution of accumulated fatigue damage for ships\ntraveling a popular shipping route. The model results agree well with the data,\nindicating that the model could be used for route optimization in naval\nlogistics.\n", "versions": [{"version": "v1", "created": "Thu, 14 Mar 2019 23:18:06 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 12:52:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Rychlik", "Igor", ""]]}, {"id": "1903.06422", "submitter": "Chuancun Yin", "authors": "Xuehua Yin, Xiuyan Sha, Chuancun Yin", "title": "The $CI$-index: a new index to characterize the scientific output of\n  researchers", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple new index, named the $CI$-index, based on the Choquet\nintegral to characterize the scientific output of researchers. This index is an\nimprovement of the $A$-index and $R$-index and has a notable feature that\nhighly cited papers have highly weights and lowly cited papers have lowly\nweights. In applications many researchers may have the same $h$-index,\n$g$-index or $R$-index. The $CI$-index can be provided an effective method of\ndistinguish among such researchers.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 09:23:27 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 13:18:08 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Yin", "Xuehua", ""], ["Sha", "Xiuyan", ""], ["Yin", "Chuancun", ""]]}, {"id": "1903.06488", "submitter": "Zachary Shahn", "authors": "Ellen C Caniglia, Eleanor J Murray, Miguel A Hernan and Zach Shahn", "title": "A Note on Estimating Optimal Dynamic Treatment Strategies Under Resource\n  Constraints Using Dynamic Marginal Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing strategies for determining the optimal treatment or monitoring\nstrategy typically assume unlimited access to resources. However, when a health\nsystem has resource constraints, such as limited funds, access to medication,\nor monitoring capabilities, medical decisions must balance impacts on both\nindividual and population health outcomes. That is, decisions should account\nfor competition between individuals in resource usage. One simple solution is\nto estimate the (counterfactual) resource usage under the possible\ninterventions and choose the optimal strategy for which resource usage is\nwithin acceptable limits. We propose a method to identify the optimal dynamic\nintervention strategy that leads to the best expected health outcome accounting\nfor a health system's resource constraints. We then apply this method to\ndetermine the optimal dynamic monitoring strategy for people living with HIV\nwhen resource limits on monitoring exist using observational data from the\nHIV-CAUSAL Collaboration.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:55:01 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Caniglia", "Ellen C", ""], ["Murray", "Eleanor J", ""], ["Hernan", "Miguel A", ""], ["Shahn", "Zach", ""]]}, {"id": "1903.06626", "submitter": "Paul Smith", "authors": "Paul A. Smith and Wesley Yung", "title": "A review and evaluation of the use of longitudinal approaches in\n  business surveys", "comments": null, "journal-ref": "Longitudinal and Life Course Studies 2019 vol 10 pp 491-511", "doi": "10.1332/175795919X15694142999134", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business surveys are not generally considered to be longitudinal by design.\nHowever, the largest businesses are almost always included in each wave of\nrecurrent surveys because they are essential for producing good estimates; and\nshort-period business surveys frequently make use of rotating panel designs to\nimprove the estimates of change by inducing sample overlaps between different\nperiods. These design features mean that business surveys share some\nmethodological challenges with longitudinal surveys. We review the longitudinal\nmethods and approaches which can be used to improve the design and operation of\nbusiness surveys, giving examples of their use. We also look in the other\ndirection, considering the aspects of longitudinal analysis which have the\npotential to improve the accuracy, relevance and interpretation of business\nsurvey outputs.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 20:21:46 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 12:34:13 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Smith", "Paul A.", ""], ["Yung", "Wesley", ""]]}, {"id": "1903.06666", "submitter": "Sumanta Das Dr", "authors": "Sumanta Kumar Das", "title": "Fitting Heterogeneous Lanchester Models on the Kursk Campaign", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The battle of Kursk between Soviet and German is known to be the biggest tank\nbattle in the history. The present paper uses the tank and artillery data from\nthe Kursk database for fitting both forms of homogeneous and heterogeneous\nLanchester model. Under homogeneous form the Soviet (or German) tank casualty\nis attributed to only the German(or Soviet) tank engagement. For heterogeneous\nform the tank casualty is attributed to both tank and artillery engagements. A\nset of differential equations using both forms have been developed, and the\ncommonly used least square estimation is compared with maximum likelihood\nestimation for attrition rates and exponent coefficients. For validating the\nmodels, different goodness-of-fit measures like R2, sum-of-square-residuals\n(SSR), root-mean-square error (RMSE), Kolmogorov-Smirnov (KS) and chi-square\nstatistics are used for comparison. Numerical results suggest the model is\nstatistically more accurate when each day of the battle is considered as a\nmini-battle. The distribution patterns of the SSR and likelihood values with\nvarying parameters are represented using contour plots and 3D surfaces.\n", "versions": [{"version": "v1", "created": "Tue, 12 Mar 2019 09:51:58 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Das", "Sumanta Kumar", ""]]}, {"id": "1903.06667", "submitter": "Leonardo Nascimento Ferreira", "authors": "Leonardo N. Ferreira and Didier A. Vega-Oliveros and Liang Zhao and\n  Manoel F. Cardoso and Elbert E. N. Macau", "title": "Global Fire Season Severity Analysis and Forecasting", "comments": "Computers & Geosciences", "journal-ref": null, "doi": "10.1016/j.cageo.2019.104339", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we divide the globe into a hexagonal grid and we extracted\ntime series of daily fire counts from each cell to estimate and analyze\nworldwide fire season severity (FSS), here defined as the accumulated fire\ndetections in a season. The central question here is evaluating the accuracy of\ntime series forecasting methods to estimate short-term (months) and medium-term\n(seasons) using only historical data of active fire detections. This approach\nis simple, fast, and use globally available data, making it easier for large\nscale prediction. Our results comprehend descriptive and predictive analyses of\nthe worldwide seasonal fire activity. We verified that in 99% of the cells, the\nfire seasons have lengths shorter than seven months and that 57% have their\nlengths decrease. We also observed a declining tendency in the number of active\nfire counts during the seasons in 61% cells. However, some regions like the\nNortheast Brazil and the West Coast of the USA present an increasing trend. We\nverified that the forecasting error is lower than the mean FSS in 95% of the\ncells, indicating clear predictability in the FSS.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 17:52:48 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 18:34:28 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 13:16:01 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Ferreira", "Leonardo N.", ""], ["Vega-Oliveros", "Didier A.", ""], ["Zhao", "Liang", ""], ["Cardoso", "Manoel F.", ""], ["Macau", "Elbert E. N.", ""]]}, {"id": "1903.06668", "submitter": "Ekaterina Abramova", "authors": "Ekaterina Abramova and Derek Bunn", "title": "Estimating Dynamic Conditional Spread Densities to Optimise Daily\n  Storage Trading of Electricity", "comments": "59 pages, 37 figures, CEMA 2019, POM Special Issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG econ.EM q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper formulates dynamic density functions, based upon skewed-t and\nsimilar representations, to model and forecast electricity price spreads\nbetween different hours of the day. This supports an optimal day ahead storage\nand discharge schedule, and thereby facilitates a bidding strategy for a\nmerchant arbitrage facility into the day-ahead auctions for wholesale\nelectricity. The four latent moments of the density functions are dynamic and\nconditional upon exogenous drivers, thereby permitting the mean, variance,\nskewness and kurtosis of the densities to respond hourly to such factors as\nweather and demand forecasts. The best specification for each spread is\nselected based on the Pinball Loss function, following the closed form\nanalytical solutions of the cumulative density functions. Those analytical\nproperties also allow the calculation of risk associated with the spread\narbitrages. From these spread densities, the optimal daily operation of a\nbattery storage facility is determined.\n", "versions": [{"version": "v1", "created": "Sat, 9 Mar 2019 20:33:21 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Abramova", "Ekaterina", ""], ["Bunn", "Derek", ""]]}, {"id": "1903.06669", "submitter": "Lily Xu", "authors": "Lily Xu, Shahrzad Gholami, Sara Mc Carthy, Bistra Dilkina, Andrew\n  Plumptre, Milind Tambe, Rohit Singh, Mustapha Nsubuga, Joshua Mabonga,\n  Margaret Driciru, Fred Wanyama, Aggrey Rwetsiba, Tom Okello, Eric Enyel", "title": "Stay Ahead of Poachers: Illegal Wildlife Poaching Prediction and Patrol\n  Planning Under Uncertainty with Field Test Evaluations", "comments": "12 pages, 11 figures. Short paper published in ICDE 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Illegal wildlife poaching threatens ecosystems and drives endangered species\ntoward extinction. However, efforts for wildlife protection are constrained by\nthe limited resources of law enforcement agencies. To help combat poaching, the\nProtection Assistant for Wildlife Security (PAWS) is a machine learning\npipeline that has been developed as a data-driven approach to identify areas at\nhigh risk of poaching throughout protected areas and compute optimal patrol\nroutes. In this paper, we take an end-to-end approach to the data-to-deployment\npipeline for anti-poaching. In doing so, we address challenges including\nextreme class imbalance (up to 1:200), bias, and uncertainty in wildlife\npoaching data to enhance PAWS, and we apply our methodology to three national\nparks with diverse characteristics. (i) We use Gaussian processes to quantify\npredictive uncertainty, which we exploit to improve robustness of our\nprescribed patrols and increase detection of snares by an average of 30%. We\nevaluate our approach on real-world historical poaching data from Murchison\nFalls and Queen Elizabeth National Parks in Uganda and, for the first time,\nSrepok Wildlife Sanctuary in Cambodia. (ii) We present the results of\nlarge-scale field tests conducted in Murchison Falls and Srepok Wildlife\nSanctuary which confirm that the predictive power of PAWS extends promisingly\nto multiple parks. This paper is part of an effort to expand PAWS to 800 parks\naround the world through integration with SMART conservation software.\n", "versions": [{"version": "v1", "created": "Fri, 8 Mar 2019 08:26:07 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 04:11:38 GMT"}, {"version": "v3", "created": "Wed, 6 Nov 2019 03:06:34 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Xu", "Lily", ""], ["Gholami", "Shahrzad", ""], ["Carthy", "Sara Mc", ""], ["Dilkina", "Bistra", ""], ["Plumptre", "Andrew", ""], ["Tambe", "Milind", ""], ["Singh", "Rohit", ""], ["Nsubuga", "Mustapha", ""], ["Mabonga", "Joshua", ""], ["Driciru", "Margaret", ""], ["Wanyama", "Fred", ""], ["Rwetsiba", "Aggrey", ""], ["Okello", "Tom", ""], ["Enyel", "Eric", ""]]}, {"id": "1903.06670", "submitter": "Ina Taralova", "authors": "V. Bondarenko (LS2N, ECN), Simona Petrakieva, Ina Taralova (LS2N,\n  ECN), Desislav Andreev", "title": "Forecasting Time Series for Power Consumption Data in Different\n  Buildings Using the Fractional Brownian Motion", "comments": null, "journal-ref": "International Journal of Circuits, Systems and Signal Processing,\n  North Atlantic University Union, 2018", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper will be discussed the problem related to the individual\nhousehold electric power consumption of objects in different areas-industry,\nfarmers, banks, hospitals, theaters, hostels, supermarkets, universities. The\nmain goal of the directed research is to estimate the active P and full S power\nconsumptions for all studied buildings. The defined goal is achieved by solving\nof the following three problems. The first problem studies which buildings\nincrease their power consumption. The second one finds which objects have the\ngreatest increase of power consumption. And the third problem regards if it is\npossible to make a short-term forecast, based on the solutions of previous two\nproblems. The present research and solving of the aforementioned problems is\nconducted using fractional Brownian motion theory. The applicability of this\napproach is illustrated on the example with 20 real objects in different areas.\nThe paper ends with conclusion notes about possibilities to make short-term\nforecasts about power consumption of the considered buildings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Mar 2019 08:08:45 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Bondarenko", "V.", "", "LS2N, ECN"], ["Petrakieva", "Simona", "", "LS2N,\n  ECN"], ["Taralova", "Ina", "", "LS2N,\n  ECN"], ["Andreev", "Desislav", ""]]}, {"id": "1903.06671", "submitter": "Benyamin Ghojogh", "authors": "Benyamin Ghojogh, Mark Crowley, Fakhri Karray", "title": "Addressing the Mystery of Population Decline of the Rose-Crested Blue\n  Pipit in a Nature Preserve using Data Visualization", "comments": "This paper can be useful for getting introduced to some data\n  visualization tools and techniques", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two main methods for exploring patterns in data are data visualization and\nmachine learning. The former relies on humans for investigating the patterns\nwhile the latter relies on machine learning algorithms. This paper tries to\nfind the patterns using only data visualization. It addresses the mystery of\npopulation decline of a bird, named Rose-Crested Blue Pipit, in a hypothetical\nnature preserve. Different visualization techniques are used and the reasons of\nthe problem are found and categorized. Finally, the solutions for preventing\nthe future similar problems are suggested. This paper can be useful for getting\nintroduced to some data visualization tools and techniques.\n", "versions": [{"version": "v1", "created": "Sun, 3 Mar 2019 17:35:03 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Ghojogh", "Benyamin", ""], ["Crowley", "Mark", ""], ["Karray", "Fakhri", ""]]}, {"id": "1903.06675", "submitter": "Bal\\'azs Dobi", "authors": "Bal\\'azs Dobi and Andr\\'as Zempl\\'eni", "title": "Markov Chain-based Cost-Optimal Control Charts for Healthcare Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control charts have traditionally been used in industrial statistics, but are\nconstantly seeing new areas of application, especially in the age of Industry\n4.0. This paper introduces a new method, which is suitable for applications in\nthe healthcare sector, especially for monitoring a health-characteristic of a\npatient. We adapt a Markov chain-based approach and develop a method in which\nnot only the shift size (i.e. the degradation of the patient's health) can be\nrandom, but the effect of the repair (i.e. treatment) and time between\nsamplings (i.e. visits) too. This means that we do not use many often-present\nassumptions which are usually not applicable for medical treatments. The\naverage cost of the protocol, which is determined by the time between samplings\nand the control limit, can be estimated using the stationary distribution of\nthe Markov chain.\n  Furthermore, we incorporate the standard deviation of the cost into the\noptimisation procedure, which is often very important from a process control\nviewpoint. The sensitivity of the optimal parameters and the resulting average\ncost and cost standard deviation on different parameter values is investigated.\nWe demonstrate the usefulness of the approach for real-life data of patients\ntreated in Hungary: namely the monitoring of cholesterol level of patients with\ncardiovascular event risk. The results showed that the optimal parameters from\nour approach can be somewhat different from the original medical parameters.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:01:06 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Dobi", "Bal\u00e1zs", ""], ["Zempl\u00e9ni", "Andr\u00e1s", ""]]}, {"id": "1903.06676", "submitter": "James Barrett", "authors": "James E. Barrett, Aylin Cakiroglu, Catey Bunce, Anoop Shah, Spiros\n  Denaxas", "title": "Selective recruitment designs for improving observational studies using\n  electronic health records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large scale electronic health records (EHRs) present an opportunity to\nquickly identify suitable individuals in order to directly invite them to\nparticipate in an observational study. EHRs can contain data from millions of\nindividuals, raising the question of how to optimally select a cohort of size n\nfrom a larger pool of size N. In this paper we propose a simple selective\nrecruitment protocol that selects a cohort in which covariates of interest tend\nto have a uniform distribution. We show that selectively recruited cohorts\npotentially offer greater statistical power and more accurate parameter\nestimates than randomly selected cohorts. Our protocol can be applied to\nstudies with multiple categorical and continuous covariates. We apply our\nprotocol to a numerically simulated prospective observational study using an\nEHR database of stable acute coronary disease patients from 82,089 individuals\nin the U.K. Selective recruitment designs require a smaller sample size,\nleading to more efficient and cost-effective studies.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:36:39 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Barrett", "James E.", ""], ["Cakiroglu", "Aylin", ""], ["Bunce", "Catey", ""], ["Shah", "Anoop", ""], ["Denaxas", "Spiros", ""]]}, {"id": "1903.06739", "submitter": "Annette M\\\"oller", "authors": "Annette M\\\"oller, J\\\"urgen Gro{\\ss}", "title": "Probabilistic Temperature Forecasting with a Heteroscedastic\n  Autoregressive Ensemble Postprocessing model", "comments": null, "journal-ref": null, "doi": "10.1002/qj.3667", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weather prediction today is performed with numerical weather prediction (NWP)\nmodels. These are deterministic simulation models describing the dynamics of\nthe atmosphere, and evolving the current conditions forward in time to obtain a\nprediction for future atmospheric states. To account for uncertainty in NWP\nmodels it has become common practice to employ ensembles of NWP forecasts.\nHowever, NWP ensembles often exhibit forecast biases and dispersion errors,\nthus require statistical postprocessing to improve reliability of the ensemble\nforecasts. This work proposes an extension of a recently developed\npostprocessing model utilizing autoregressive information present in the\nforecast error of the raw ensemble members. The original approach is modified\nto let the variance parameter depend on the ensemble spread, yielding a\ntwo-fold heteroscedastic model. Furthermore, an additional high-resolution\nforecast is included into the postprocessing model, yielding improved\npredictive performance. Finally, it is outlined how the autoregressive model\ncan be utilized to postprocess ensemble forecasts with higher forecast\nhorizons, without the necessity of making fundamental changes to the original\nmodel. We accompany the new methodology by an implementation within the R\npackage ensAR to make our method available for other researchers working in\nthis area. To illustrate the performance of the heteroscedastic extension of\nthe autoregressive model, and its use for higher forecast horizons we present a\ncase study for a data set containing 12 years of temperature forecasts and\nobservations over Germany. The case study indicates that the autoregressive\nmodel yields particularly strong improvements for forecast horizons beyond 24\nhours.\n", "versions": [{"version": "v1", "created": "Fri, 15 Mar 2019 18:35:58 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["M\u00f6ller", "Annette", ""], ["Gro\u00df", "J\u00fcrgen", ""]]}, {"id": "1903.06960", "submitter": "Michela Ottobre", "authors": "P.Dobson, I. Fursov, G. Lord and M. Ottobre", "title": "Reversible and non-reversible Markov Chain Monte Carlo algorithms for\n  reservoir simulation problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare numerically the performance of reversible and non-reversible\nMarkov Chain Monte Carlo algorithms for high dimensional oil reservoir\nproblems; because of the nature of the problem at hand, the target measures\nfrom which we sample are supported on bounded domains. We compare two\nstrategies to deal with bounded domains, namely reflecting proposals off the\nboundary and rejecting them when they fall outside of the domain. We observe\nthat for complex high dimensional problems reflection mechanisms outperform\nrejection approaches and that the advantage of introducing non-reversibility in\nthe Markov Chain employed for sampling is more and more visible as the\ndimension of the parameter space increases.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 17:13:40 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Dobson", "P.", ""], ["Fursov", "I.", ""], ["Lord", "G.", ""], ["Ottobre", "M.", ""]]}, {"id": "1903.06988", "submitter": "Dominik Sieradzki", "authors": "Dominik Sieradzki, Wojciech Zieli\\'nski", "title": "An example of application of optimal sample allocation in a finite\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimating a proportion of objects with particular attribute\nin a finite population is considered. This paper shows an example of the\napplication of estimation fraction using new proposed sample allocation in a\npopulation divided into two strata. Variance of estimator of proportion which\nuses proposed sample allocation is compared to variance of the standard one.\n", "versions": [{"version": "v1", "created": "Sat, 16 Mar 2019 21:24:53 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Sieradzki", "Dominik", ""], ["Zieli\u0144ski", "Wojciech", ""]]}, {"id": "1903.07218", "submitter": "Oliver Stevenson", "authors": "Oliver G. Stevenson, Brendon J. Brewer", "title": "Modelling Career Trajectories of Cricket Players Using Gaussian\n  Processes", "comments": "8 pages of content + 1 page of references. 2 figures. To be published\n  in BAYSM2018 conference proceedings titled: \"Bayesian Statistics: New\n  Challenges and New Generations - BAYSM 2018\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sport of cricket, variations in a player's batting ability can usually\nbe measured on one of two scales. Short-term changes in ability that are\nobserved during a single innings, and long-term changes that are witnessed\nbetween matches, over entire playing careers. To measure long-term variations,\nwe derive a Bayesian parametric model that uses a Gaussian process to measure\nand predict how the batting abilities of international cricketers fluctuate\nbetween innings. The model is fitted using nested sampling given its high\ndimensionality and for ease of model comparison. Generally speaking, the\nresults support an anecdotal description of a typical sporting career. Young\nplayers tend to begin their careers with some raw ability, which improves over\ntime as a result of coaching, experience and other external circumstances.\nEventually, players reach the peak of their career, after which ability tends\nto decline. The model provides more accurate quantifications of current and\nfuture player batting abilities than traditional cricketing statistics, such as\nthe batting average. The results allow us to identify which players are\nimproving or deteriorating in terms of batting ability, which has practical\nimplications in terms of player comparison, talent identification and team\nselection policy.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 01:14:51 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Stevenson", "Oliver G.", ""], ["Brewer", "Brendon J.", ""]]}, {"id": "1903.07270", "submitter": "Jingya Yan", "authors": "Jingya Yan", "title": "Extraction Urban Clusters from Geospatial Data: A Case Study from\n  Switzerland", "comments": "22 pages, 15 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different techniques were developed to extract urban agglomerations from a\nbig dataset. The urban agglomerations are used to understand the structure and\ngrowth of cities. However, the major challenge is to extract urban\nagglomerations from big data, which can reflect human activities. Community\nurban cluster refers to spatially clustered geographic events, such as human\nsettlements or activities. It provides a powerful and innovative insight to\nanalyze the structure and growth of the real city. In order to understand the\nshape and growth of urban agglomerations in Switzerland from spatial and\ntemporal aspects, this work identifies urban clusters from nighttime light data\nand street network data. Nighttime light data record lights emitted from human\nsettlements at night on the earth's surface. This work uses DMSP-OLS Nighttime\nlight data to extract urban clusters from 1992 to 2013. The street is one of\nthe most important factors to reflect human activities. Hence, urban clusters\nare also extracted from street network data to understand the structure of\ncities. Both of these data have a heavy-tailed distribution, which includes\npower laws as well as lognormal and exponential distributions. The head/tail\nbreaks is a classification method to find the hierarchy of data with a\nheavy-tailed distribution. This work uses head/tail breaks classification to\nextract urban clusters of Switzerland. At last, the power law distribution of\nall the urban clusters was detected at the country level.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 06:46:33 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yan", "Jingya", ""]]}, {"id": "1903.07401", "submitter": "Andr\\'e Beauducel", "authors": "Andr\\'e Beauducel and Norbert Hilger", "title": "Score predictor factor analysis: Reproducing observed covariances by\n  means of factor score predictors", "comments": null, "journal-ref": "Front. Psychol. 10:1895 (2019)", "doi": "10.3389/fpsyg.2019.01895", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model implied by factor score predictors does not reproduce the\nnon-diagonal elements of the observed covariance matrix as well as the factor\nloadings. It is therefore investigated whether it is possible to estimate\nfactor loadings for which the model implied by the factor score predictors\noptimally reproduces the non-diagonal elements of the observed covariance\nmatrix. Accordingly, loading estimates are proposed for which the model implied\nby the factor score predictors allows for a least-squares approximation of the\nnon-diagonal elements of the observed covariance matrix. This estimation method\nis termed Score predictor factor analysis and algebraically compared with\nMinres factor analysis as well as principal component analysis. A population\nbased and a sample based simulation study was performed in order to compare\nScore predictor factor analysis, Minres factor analysis, and principal\ncomponent analysis. It turns out that the non-diagonal elements of the observed\ncovariance matrix can more exactly be reproduced from the factor score\npredictors computed from Score predictor factor analysis than from the factor\nscore predictors computed from Minres factor analysis and from principal\ncomponents. Moreover, Score predictor factor analysis can be helpful to\nidentify factors when the factor model does not perfectly fit to the data\nbecause of model error.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 12:54:10 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Beauducel", "Andr\u00e9", ""], ["Hilger", "Norbert", ""]]}, {"id": "1903.07503", "submitter": "Jukka-Pekka Onnela", "authors": "Yingrui Yang, Ashley McKhann, Sixing Chen, Guy Harling, Jukka-Pekka\n  Onnela", "title": "Efficient vaccination strategies for epidemic control using network\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network-based interventions against epidemic spread are most powerful when\nthe full network structure is known. However, in practice, resource constraints\nrequire decisions to be made based on partial network information. We\ninvestigated how the accuracy of network data available at individual and\nvillage levels affected network-based vaccination effectiveness. We simulated a\nSusceptible-Infected-Recovered process on empirical social networks from 75\nvillages. First, we used regression to predict the percentage of individuals\never infected based on village-level network. Second, we simulated vaccinating\n10 percent of each of the 75 empirical village networks at baseline, selecting\nvaccinees through one of five network-based approaches: random individuals;\nrandom contacts of random individuals; random high-degree individuals; highest\ndegree individuals; or most central individuals. The first three approaches\nrequire only sample data; the latter two require full network data. We also\nsimulated imposing a limit on how many contacts an individual can nominate\n(Fixed Choice Design, FCD), which reduces the data collection burden but\ngenerates only partially observed networks. We found mean and standard\ndeviation of the degree distribution to strongly predict cumulative incidence.\nIn simulations, the Nomination method reduced cumulative incidence by one-sixth\ncompared to Random vaccination; full network methods reduced infection by\ntwo-thirds. The High Degree approach had intermediate effectiveness.\nSurprisingly, FCD truncating individuals' degrees at three was as effective as\nusing complete networks. Using even partial network information to prioritize\nvaccines at either the village or individual level substantially improved\nepidemic outcomes. Such approaches may be feasible and effective in outbreak\nsettings, and full ascertainment of network structure may not be required.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:22:09 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Yang", "Yingrui", ""], ["McKhann", "Ashley", ""], ["Chen", "Sixing", ""], ["Harling", "Guy", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1903.07509", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J. Reich and Dipankar Bandyopadhyay", "title": "A Spatial Bayesian Semiparametric Mixture Model for Positive Definite\n  Matrices with Applications to Diffusion Tensor Imaging", "comments": null, "journal-ref": null, "doi": "10.1002/cjs.11601", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion tensor imaging (DTI) is a popular magnetic resonance imaging\ntechnique used to characterize microstructural changes in the brain. DTI\nstudies quantify the diffusion of water molecules in a voxel using an estimated\n3x3 symmetric positive definite diffusion tensor matrix. Statistical analysis\nof DTI data is challenging because the data are positive definite matrices.\nMatrix-variate information is often summarized by a univariate quantity, such\nas the fractional anisotropy (FA), leading to a loss of information.\nFurthermore, DTI analyses often ignore the spatial association of neighboring\nvoxels, which can lead to imprecise estimates. Although the spatial modeling\nliterature is abundant, modeling spatially dependent positive definite matrices\nis challenging. To mitigate these issues, we propose a matrix-variate Bayesian\nsemiparametric mixture model, where the positive definite matrices are\ndistributed as a mixture of inverse Wishart distributions with the spatial\ndependence captured by a Markov model for the mixture component labels.\nConjugacy and the double Metropolis-Hastings algorithm result in fast and\nelegant Bayesian computing. Our simulation study shows that the proposed method\nis more powerful than non-spatial methods. We also apply the proposed method to\ninvestigate the effect of cocaine use on brain structure. The contribution of\nour work is to provide a novel statistical inference tool for DTI analysis by\nextending spatial statistics to matrix-variate data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:32:27 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J.", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "1903.07521", "submitter": "Christian Soerup", "authors": "Christian Michel S{\\o}rup, Daniel Alberto Sep\\'ulveda Estay, Peter\n  Jacobsen, Philip Dean Anderson", "title": "Simulating feedback mechanisms in patient flow and return visits in an\n  emergency department", "comments": "19 pages, 5 figures, presented as poster at the 6th Danish Emergency\n  Medicine Conference (DEMC6) in Odense, Denmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency department (ED) crowding has been an increasing problem worldwide.\nPrior research has identified factors that contribute to ED crowding. However,\nthe relationships between these remain incompletely understood. This study's\nobjective was to analyse the effects of initiating a local protocol to\nalleviate crowding situations at the expense of increasing returning patients\nthrough the development of a system dynamics (SD) simulation model. The SD\nstudy is from an academic care hospital in Boston, MA. Data sources include\ndirect observations, semi-structured interviews, archival data from October\n2013, and peer-reviewed literature from the domains of emergency medicine and\nmanagement science. The SD model shows interrelations between inpatient\ncapacity restraints and return visits due to potential premature discharges.\nThe model reflects the vulnerability of the ED system when exposed to\nunpredicted increases in demand. Default trigger values for the protocol are\ntested to determine a balance between increased patient flows and the number of\nreturning patients. Baseline simulation runs for generic variables assessment\nshowed high leverage potential in bed assignment- and transfer times.\n  A thorough understanding of the complex non-linear behaviour of causes and\neffects of ED crowding is enabled through the use of SD. The vulnerability of\nthe system lies in the crucial interaction between the physical constraints and\nthe expedited patient flows through protocol activation. This study is an\nexample of how hospital managers can benefit from virtual scenario testing\nwithin a safe simulation environment to immediately visualise the impacts of\npolicy adjustments.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 15:53:35 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["S\u00f8rup", "Christian Michel", ""], ["Estay", "Daniel Alberto Sep\u00falveda", ""], ["Jacobsen", "Peter", ""], ["Anderson", "Philip Dean", ""]]}, {"id": "1903.07639", "submitter": "Stephanie Hicks", "authors": "Stephanie C. Hicks and Roger D. Peng", "title": "Elements and Principles for Characterizing Variation between Data\n  Analyses", "comments": "14 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The data revolution has led to an increased interest in the practice of data\nanalysis. For a given problem, there can be significant or subtle differences\nin how a data analyst constructs or creates a data analysis, including\ndifferences in the choice of methods, tooling, and workflow. In addition, data\nanalysts can prioritize (or not) certain objective characteristics in a data\nanalysis, leading to differences in the quality or experience of the data\nanalysis, such as an analysis that is more or less reproducible or an analysis\nthat is more or less exhaustive. However, data analysts currently lack a formal\nmechanism to compare and contrast what makes analyses different from each\nother. To address this problem, we introduce a vocabulary to describe and\ncharacterize variation between data analyses. We denote this vocabulary as the\nelements and principles of data analysis, and we use them to describe the\nfundamental concepts for the practice and teaching of creating a data analysis.\nThis leads to two insights: it suggests a formal mechanism to evaluate data\nanalyses based on objective characteristics, and it provides a framework to\nteach students how to build data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 18:04:25 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 00:55:25 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Hicks", "Stephanie C.", ""], ["Peng", "Roger D.", ""]]}, {"id": "1903.07649", "submitter": "Wenna Xi", "authors": "Wenna Xi, Catherine A. Calder, Christopher R. Browning", "title": "Beyond Activity Space: Detecting Communities in Ecological Networks", "comments": "76 pages, 8 figures, 6 tables, and 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emerging research suggests that the extent to which activity spaces -- the\ncollection of an individual's routine activity locations -- overlap provides\nimportant information about the functioning of a city and its neighborhoods. To\nstudy patterns of overlapping activity spaces, we draw on the notion of an\necological network, a type of two-mode network with the two modes being\nindividuals and the geographic locations where individuals perform routine\nactivities. We describe a method for detecting \"ecological communities\" within\nthese networks based on shared activity locations among individuals.\nSpecifically, we identify latent activity pattern profiles, which, for each\ncommunity, summarize its members' probability distribution of going to each\nlocation, and community assignment vectors, which, for each individual,\nsummarize his/her probability distribution of belonging to each community.\nUsing data from the Adolescent Health and Development in Context (AHDC) Study,\nwe employ latent Dirichlet allocation (LDA) to identify activity pattern\nprofiles and communities. We then explore differences across neighborhoods in\nthe strength, and within-neighborhood consistency of community assignment. We\nhypothesize that these aspects of the neighborhood structure of ecological\ncommunity membership capture meaningful dimensions of neighborhood functioning\nlikely to co-vary with economic and racial composition. We discuss the\nimplications of a focus on ecological communities for the conduct of\n\"neighborhood effects\" research more broadly.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 18:20:01 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Xi", "Wenna", ""], ["Calder", "Catherine A.", ""], ["Browning", "Christopher R.", ""]]}, {"id": "1903.07720", "submitter": "Juan Felipe Restrepo Rinckoar", "authors": "Juan F. Restrepo, Diego M. Mateos and Gast\\'on Schlotthauer", "title": "Transfer Entropy Rate Through Lempel-Ziv Complexity", "comments": null, "journal-ref": "Phys. Rev. E 101, 052117 (2020)", "doi": "10.1103/PhysRevE.101.052117", "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present a methodology to estimate the Transfer Entropy\nRate between two systems through the Lempel-Ziv complexity. This methodology\ncarries a set of practical advantages: it can be estimated from two single\ndiscrete series of measures, it is not computationally expensive and it does\nnot assume any model for the data. The results of simulations over three\ndifferent unidirectional coupled systems, suggest that this methodology can be\nused to assess the direction and strength of the information flow between\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 21:07:07 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 20:16:01 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 18:15:43 GMT"}, {"version": "v4", "created": "Tue, 16 Apr 2019 17:30:10 GMT"}, {"version": "v5", "created": "Mon, 20 May 2019 12:46:24 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Restrepo", "Juan F.", ""], ["Mateos", "Diego M.", ""], ["Schlotthauer", "Gast\u00f3n", ""]]}, {"id": "1903.07755", "submitter": "Iavor Bojinov", "authors": "Iavor Bojinov, Ye Tu, Min Liu, Ya Xu", "title": "Causal inference from observational data: Estimating the effect of\n  contributions on visitation frequency atLinkedIn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments (A/B testings) have become the standard way for\nweb-facing companies to guide innovation, evaluate new products, and prioritize\nideas. There are times, however, when running an experiment is too complicated\n(e.g., we have not built the infrastructure), costly (e.g., the intervention\nwill have a substantial negative impact on revenue), and time-consuming (e.g.,\nthe effect may take months to materialize). Even if we can run an experiment,\nknowing the magnitude of the impact will significantly accelerate the product\ndevelopment life cycle by helping us prioritize tests and determine the\nappropriate traffic allocation for different treatment groups. In this setting,\nwe should leverage observational data to quickly and cost-efficiently obtain a\nreliable estimate of the causal effect. Although causal inference from\nobservational data has a long history, its adoption by data scientist in\ntechnology companies has been slow. In this paper, we rectify this by providing\na brief introduction to the vast field of causal inference with a specific\nfocus on the tools and techniques that data scientist can directly leverage. We\nillustrate how to apply some of these methodologies to measure the effect of\ncontributions (e.g., post, comment, like or send private messages) on\nengagement metrics. Evaluating the impact of contributions on engagement\nthrough an A/B test requires encouragement design and the development of\nnon-standard experimentation infrastructure, which can consume a tremendous\namount of time and financial resources. We present multiple efficient\nstrategies that exploit historical data to accurately estimate the\ncontemporaneous (or instantaneous) causal effect of a user's contribution on\nher own and her neighbors' (i.e., the users she is connected to) subsequent\nvisitation frequency. We apply these tools to LinkedIn data for several million\nmembers.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 23:04:43 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Bojinov", "Iavor", ""], ["Tu", "Ye", ""], ["Liu", "Min", ""], ["Xu", "Ya", ""]]}, {"id": "1903.07782", "submitter": "Yingrui Yang", "authors": "Yingrui Yang, Molin Wang", "title": "Semiparametric Methods for Exposure Misclassification in Propensity\n  Score-Based Time-to-Event Data Analysis", "comments": "Withdrawn due to grant related requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiology, identifying the effect of exposure variables in relation to\na time-to-event outcome is a classical research area of practical importance.\nIncorporating propensity score in the Cox regression model, as a measure to\ncontrol for confounding, has certain advantages when outcome is rare. However,\nin situations involving exposure measured with moderate to substantial error,\nidentifying the exposure effect using propensity score in Cox models remains a\nchallenging yet unresolved problem. In this paper, we propose an estimating\nequation method to correct for the exposure misclassification-caused bias in\nthe estimation of exposure-outcome associations. We also discuss the asymptotic\nproperties and derive the asymptotic variances of the proposed estimators. We\nconduct a simulation study to evaluate the performance of the proposed\nestimators in various settings. As an illustration, we apply our method to\ncorrect for the misclassification-caused bias in estimating the association of\nPM2.5 level with lung cancer mortality using a nationwide prospective cohort,\nthe Nurses' Health Study (NHS). The proposed methodology can be applied using\nour user-friendly R function published online.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 00:57:16 GMT"}, {"version": "v2", "created": "Sat, 8 Jun 2019 07:50:43 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Yang", "Yingrui", ""], ["Wang", "Molin", ""]]}, {"id": "1903.07847", "submitter": "Youzhi Liang", "authors": "Arturo Chavez, Dimitris Koutentakis, Youzhi Liang, Sonali Tripathy,\n  Jie Yun", "title": "Identify Statistical Similarities and Differences Between the Deadliest\n  Cancer Types Through Gene Expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prognostic genes have been well studied within each type of cancer. However,\ninvestigations of the similarities and differences across cancer types are\nrare. In view of the optimal course of treatment, the classification of cancers\ninto subtypes is critical to the diagnosis. We examined the properties in gene\nco-expression networks using a patient-to-patient correlation network analysis\nand a weighted gene correlation network analysis (WGCNA) for five cancer types\nusing data generated by UC Irvine. We further analyze and compare the degree,\ncentrality and betweenness of the network for each cancer type and apply a\nmultinomial logistic regression to identify the critical subset of genes. Given\nthe cancer types provided, our study presents a view of emergent similarities\nand differences across cancer types.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 05:43:27 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Chavez", "Arturo", ""], ["Koutentakis", "Dimitris", ""], ["Liang", "Youzhi", ""], ["Tripathy", "Sonali", ""], ["Yun", "Jie", ""]]}, {"id": "1903.07929", "submitter": "Johan Wahlstr\\\"om", "authors": "Johan Wahlstr\\\"om, Isaac Skog, Fredrik Gustafsson, Andrew Markham, and\n  Niki Trigoni", "title": "Zero-Velocity Detection - A Bayesian Approach to Adaptive Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian zero-velocity detector for foot-mounted inertial navigation\nsystems is presented. The detector extends existing zero-velocity detectors\nbased on the likelihood-ratio test, and allows, possibly time-dependent, prior\ninformation about the two hypotheses - the sensors being stationary or in\nmotion - to be incorporated into the test. It is also possible to incorporate\ninformation about the cost of a missed detection or a false alarm.\nSpecifically, we consider an hypothesis prior based on the velocity estimates\nprovided by the navigation system and an exponential model for how the cost of\na missed detection increases with the time since the last zero-velocity update.\nThereby, we obtain a detection threshold that adapts to the motion\ncharacteristics of the user. Thus, the proposed detection framework efficiently\nsolves one of the key challenges in current zero-velocity-aided inertial\nnavigation systems: the tuning of the zero-velocity detection threshold. A\nperformance evaluation on data with normal and fast gait demonstrates that the\nproposed detection framework outperforms any detector that chooses two separate\nfixed thresholds for the two gait speeds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 10:46:49 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 12:16:13 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wahlstr\u00f6m", "Johan", ""], ["Skog", "Isaac", ""], ["Gustafsson", "Fredrik", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1903.07976", "submitter": "Christof Seiler", "authors": "Christof Seiler, Lisa M. Kronstad, Laura J. Simpson, Mathieu Le Gars,\n  Elena Vendrame, Catherine A. Blish, Susan Holmes", "title": "Uncertainty Quantification in Multivariate Mixed Models for Mass\n  Cytometry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass cytometry technology enables the simultaneous measurement of over 40\nproteins on single cells. This has helped immunologists to increase their\nunderstanding of heterogeneity, complexity, and lineage relationships of white\nblood cells. Current statistical methods often collapse the rich single-cell\ndata into summary statistics before proceeding with downstream analysis,\ndiscarding the information in these multivariate datasets. In this article, our\naim is to exhibit the use of statistical analyses on the raw, uncompressed data\nthus improving replicability, and exposing multivariate patterns and their\nassociated uncertainty profiles. We show that multivariate generative models\nare a valid alternative to univariate hypothesis testing. We propose two\nmodels: a multivariate Poisson log-normal mixed model and a logistic linear\nmixed model. We show that these models are complementary and that either model\ncan account for different confounders. We use Hamiltonian Monte Carlo to\nprovide Bayesian uncertainty quantification. Our models applied to a recent\npregnancy study successfully reproduce key findings while quantifying increased\noverall protein-to-protein correlations between first and third trimester.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 13:12:56 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Seiler", "Christof", ""], ["Kronstad", "Lisa M.", ""], ["Simpson", "Laura J.", ""], ["Gars", "Mathieu Le", ""], ["Vendrame", "Elena", ""], ["Blish", "Catherine A.", ""], ["Holmes", "Susan", ""]]}, {"id": "1903.08028", "submitter": "Jason Poulos", "authors": "Jason Poulos", "title": "State-Building through Public Land Disposal? An Application of Matrix\n  Completion for Counterfactual Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN econ.EM q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How would the frontier have evolved in the absence of homestead policies? I\napply a matrix completion method to predict the counterfactual time-series of\nfrontier state capacity had there been no homesteading. In placebo tests, the\nmatrix completion method outperforms synthetic controls and other\nregression-based estimators in terms of minimizing prediction error. Causal\nestimates signify that homestead policies had significant and long-lasting\nnegative impacts on state government expenditure and revenue. These results are\nsimilar to difference-in-difference estimates that exploit variation in the\ntiming and intensity of homestead entries aggregated from 1.46 million\nindividual land patent records.\n", "versions": [{"version": "v1", "created": "Tue, 19 Mar 2019 14:49:22 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Poulos", "Jason", ""]]}, {"id": "1903.08421", "submitter": "Alexander Kreuzer", "authors": "Alexander Kreuzer, Luciana Dalla Valle, Claudia Czado", "title": "A Bayesian Non-linear State Space Copula Model to Predict Air Pollution\n  in Beijing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air pollution is a serious issue that currently affects many industrial\ncities in the world and can cause severe illness to the population. In\nparticular, it has been proven that extreme high levels of airborne\ncontaminants have dangerous short-term effects on human health, in terms of\nincreased hospital admissions for cardiovascular and respiratory diseases and\nincreased mortality risk. For these reasons, accurate estimation and prediction\nof airborne pollutant concentration is crucial. In this paper, we propose a\nflexible novel approach to model hourly measurements of fine particulate matter\nand meteorological data collected in Beijing in 2014. We show that the standard\nstate space model, based on Gaussian assumptions, does not correctly capture\nthe time dynamics of the observations. Therefore, we propose a non-linear\nnon-Gaussian state space model where both the observation and the state\nequations are defined by copula specifications, and we perform Bayesian\ninference using the Hamiltonian Monte Carlo method. The proposed copula state\nspace approach is very flexible, since it allows us to separately model the\nmarginals and to accommodate a wide variety of dependence structures in the\ndata dynamics. We show that the proposed approach allows us not only to predict\nparticulate matter measurements, but also to investigate the effects of user\nspecified climate scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 10:14:04 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 10:09:51 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Kreuzer", "Alexander", ""], ["Valle", "Luciana Dalla", ""], ["Czado", "Claudia", ""]]}, {"id": "1903.08426", "submitter": "Raju Rimal Mr", "authors": "Raju Rimal, Trygve Alm{\\o}y, Solve S{\\ae}b{\\o}", "title": "Comparison of Multi-response Prediction Methods", "comments": "22 pages, 13 figures", "journal-ref": null, "doi": "10.1016/j.chemolab.2019.05.004", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While data science is battling to extract information from the enormous\nexplosion of data, many estimators and algorithms are being developed for\nbetter prediction. Researchers and data scientists often introduce new methods\nand evaluate them based on various aspects of data. However, studies on the\nimpact of/on a model with multiple response variables are limited. This study\ncompares some newly-developed (envelope) and well-established (PLS, PCR)\nprediction methods based on real data and simulated data specifically designed\nby varying properties such as multicollinearity, the correlation between\nmultiple responses and position of relevant principal components of predictors.\nThis study aims to give some insight into these methods and help the researcher\nto understand and use them in further studies.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 10:34:44 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Rimal", "Raju", ""], ["Alm\u00f8y", "Trygve", ""], ["S\u00e6b\u00f8", "Solve", ""]]}, {"id": "1903.08460", "submitter": "Pietro Verzelli", "authors": "Pietro Verzelli and Laura Sacerdote", "title": "A study of dependency features of spike trains through copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simultaneous recordings from many neurons hide important information and the\nconnections characterizing the network remain generally undiscovered despite\nthe progresses of statistical and machine learning techniques. Discerning the\npresence of direct links between neuron from data is still a not completely\nsolved problem. To enlarge the number of tools for detecting the underlying\nnetwork structure, we propose here the use of copulas, pursuing on a research\ndirection we started in [1]. Here, we adapt their use to distinguish different\ntypes of connections on a very simple network. Our proposal consists in\nchoosing suitable random intervals in pairs of spike trains determining the\nshapes of their copulas. We show that this approach allows to detect different\ntypes of dependencies. We illustrate the features of the proposed method on\nsynthetic data from suitably connected networks of two or three formal neurons\ndirectly connected or influenced by the surrounding network. We show how a\nsmart choice of pairs of random times together with the use of empirical\ncopulas allows to discern between direct and un-direct interactions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 11:52:43 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Verzelli", "Pietro", ""], ["Sacerdote", "Laura", ""]]}, {"id": "1903.08648", "submitter": "Johan Elkink", "authors": "Johan A. Elkink and Thomas U. Grund", "title": "Modelling Diffusion through Statistical Network Analysis: A Simulation\n  Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of international relations by definition deals with\ninterdependencies among countries. One form of interdependence between\ncountries is the diffusion of country-level features, such as policies,\npolitical regimes, or conflict. In these studies, the outcome variable tends to\nbe categorical, and the primary concern is the clustering of the outcome\nvariable among connected countries. Statistically, such clustering is studied\nwith spatial econometric models. This paper instead proposes the use of a\nstatistical network approach to model diffusion with a binary outcome variable.\nUsing statistical network instead of spatial econometric models allows for a\nmore natural specification of the diffusion process, assuming autocorrelation\nin the outcomes rather than the corresponding latent variable, and it\nsimplifies the inclusion of temporal dynamics, higher level interdependencies\nand interactions between network ties and country-level features. In our\nsimulations, the performance of the Stochastic Actor-Oriented Model (SAOM)\nestimator is evaluated. Our simulation results show that spatial parameters and\ncoefficients on additional covariates in a static binary spatial autoregressive\nmodel are accurately recovered when using SAOM, albeit on a different scale. To\ndemonstrate the use of this model, the paper applies the model to the\ninternational diffusion of same-sex marriage.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 02:36:36 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Elkink", "Johan A.", ""], ["Grund", "Thomas U.", ""]]}, {"id": "1903.08716", "submitter": "Mark Schilling", "authors": "Mark F. Schilling", "title": "Is Basketball a Game of Runs?", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Basketball is often referred to as \"a game of runs.\" We investigate the\nappropriateness of this claim using data from the full NBA 2016-17 season,\ncomparing actual longest runs of scoring events to what long run theory\npredicts under the assumption that team \"momentum\" is not present. We provide\nseveral different variations of the analysis. Our results consistently indicate\nthat the lengths of longest runs in NBA games are no longer than those that\nwould occur naturally when scoring events are generated by a random process,\nrather than one that is influenced by \"momentum\".\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 20:01:09 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Schilling", "Mark F.", ""]]}, {"id": "1903.08747", "submitter": "Kenneth Hung", "authors": "Kenneth Hung, William Fithian", "title": "Statistical Methods for Replicability Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale replication studies like the Reproducibility Project: Psychology\n(RP:P) provide invaluable systematic data on scientific replicability, but most\nanalyses and interpretations of the data fail to agree on the definition of\n\"replicability\" and disentangle the inexorable consequences of known selection\nbias from competing explanations. We discuss three concrete definitions of\nreplicability based on (1) whether published findings about the signs of\neffects are mostly correct, (2) how effective replication studies are in\nreproducing whatever true effect size was present in the original experiment,\nand (3) whether true effect sizes tend to diminish in replication. We apply\ntechniques from multiple testing and post-selection inference to develop new\nmethods that answer these questions while explicitly accounting for selection\nbias. Our analyses suggest that the RP:P dataset is largely consistent with\npublication bias due to selection of significant effects. The methods in this\npaper make no distributional assumptions about the true effect sizes.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:15:12 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 00:04:45 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Hung", "Kenneth", ""], ["Fithian", "William", ""]]}, {"id": "1903.08755", "submitter": "Guillaume Saint-Jacques", "authors": "Guillaume Saint-Jacques, Maneesh Varshney, Jeremy Simpson, Ya Xu", "title": "Using Ego-Clusters to Measure Network Effects at LinkedIn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A network effect is said to take place when a new feature not only impacts\nthe people who receive it, but also other users of the platform, like their\nconnections or the people who follow them. This very common phenomenon violates\nthe fundamental assumption underpinning nearly all enterprise experimentation\nsystems, the stable unit treatment value assumption (SUTVA). When this\nassumption is broken, a typical experimentation platform, which relies on\nBernoulli randomization for assignment and two-sample t-test for assessment of\nsignificance, will not only fail to account for the network effect, but\npotentially give highly biased results.\n  This paper outlines a simple and scalable solution to measuring network\neffects, using ego-network randomization, where a cluster is comprised of an\n\"ego\" (a focal individual), and her \"alters\" (the individuals she is\nimmediately connected to). Our approach aims at maintaining representativity of\nclusters, avoiding strong modeling assumption, and significantly increasing\npower compared to traditional cluster-based randomization. In particular, it\ndoes not require product-specific experiment design, or high levels of\ninvestment from engineering teams, and does not require any changes to\nexperimentation and analysis platforms, as it only requires assigning treatment\nan individual level. Each user either has the feature or does not, and no\ncomplex manipulation of interactions between users is needed. It focuses on\nmeasuring the one-out network effect (i.e the effect of my immediate\nconnection's treatment on me), and gives reasonable estimates at a very low\nsetup cost, allowing us to run such experiments dozens of times a year.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 21:52:11 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Saint-Jacques", "Guillaume", ""], ["Varshney", "Maneesh", ""], ["Simpson", "Jeremy", ""], ["Xu", "Ya", ""]]}, {"id": "1903.08762", "submitter": "Min Liu", "authors": "Min Liu, Xiaohui Sun, Maneesh Varshney, Ya Xu", "title": "Large-Scale Online Experimentation with Quantile Metrics", "comments": null, "journal-ref": "In JSM Proceedings, Statistical Consulting Section. Alexandria,\n  VA: American Statistical Association. 2849-2860", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experimentation (or A/B testing) has been widely adopted in industry\nas the gold standard for measuring product impacts. Despite the wide adoption,\nfew literatures discuss A/B testing with quantile metrics. Quantile metrics,\nsuch as 90th percentile page load time, are crucial to A/B testing as many key\nperformance metrics including site speed and service latency are defined as\nquantiles. However, with LinkedIn's data size, quantile metric A/B testing is\nextremely challenging because there is no statistically valid and scalable\nvariance estimator for the quantile of dependent samples: the bootstrap\nestimator is statistically valid, but takes days to compute; the standard\nasymptotic variance estimate is scalable but results in order-of-magnitude\nunderestimation. In this paper, we present a statistically valid and scalable\nmethodology for A/B testing with quantiles that is fully generalizable to other\nA/B testing platforms. It achieves over 500 times speed up compared to\nbootstrap and has only $2\\%$ chance to differ from bootstrap estimates. Beyond\nmethodology, we also share the implementation of a data pipeline using this\nmethodology and insights on pipeline optimization.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 22:07:58 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Liu", "Min", ""], ["Sun", "Xiaohui", ""], ["Varshney", "Maneesh", ""], ["Xu", "Ya", ""]]}, {"id": "1903.08766", "submitter": "Guillaume Saint-Jacques", "authors": "Guillaume Saint-Jacques, James Eric Sorenson, Nanyu Chen, Ya Xu", "title": "A Method for Measuring Network Effects of One-to-One Communication\n  Features in Online A/B Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A/B testing is an important decision making tool in product development\nbecause can provide an accurate estimate of the average treatment effect of a\nnew features, which allows developers to understand how the business impact of\nnew changes to products or algorithms. However, an important assumption of A/B\ntesting, Stable Unit Treatment Value Assumption (SUTVA), is not always a valid\nassumption to make, especially for products that facilitate interactions\nbetween individuals. In contexts like one-to-one messaging we should expect\nnetwork interference; if an experimental manipulation is effective, behavior of\nthe treatment group is likely to influence members in the control group by\nsending them messages, violating this assumption. In this paper, we propose a\nnovel method that can be used to account for network effects when A/B testing\nchanges to one-to-one interactions. Our method is an edge-based analysis that\ncan be applied to standard Bernoulli randomized experiments to retrieve an\naverage treatment effect that is not influenced by network interference. We\ndevelop a theoretical model, and methods for computing point estimates and\nvariances of effects of interest via network-consistent permutation testing. We\nthen apply our technique to real data from experiments conducted on the\nmessaging product at LinkedIn. We find empirical support for our model, and\nevidence that the standard method of analysis for A/B tests underestimates the\nimpact of new features in one-to-one messaging contexts.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 22:29:26 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Saint-Jacques", "Guillaume", ""], ["Sorenson", "James Eric", ""], ["Chen", "Nanyu", ""], ["Xu", "Ya", ""]]}, {"id": "1903.08990", "submitter": "Antoine Aspeel", "authors": "Antoine Aspeel, Damien Dasnoy, Rapha\\\"el M. Jungers and Beno\\^it Macq", "title": "Optimal Intermittent Measurements for Tumor Tracking in X-ray Guided\n  Radiotherapy", "comments": "8 pages, 4 figures, 1 table", "journal-ref": "Proc. SPIE 10951, Medical Imaging 2019: Image-Guided Procedures,\n  Robotic Interventions, and Modeling, 109510C (8 March 2019);", "doi": "10.1117/12.2512859", "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In radiation therapy, tumor tracking is a challenging task that allows a\nbetter dose delivery. One practice is to acquire X-ray images in real-time\nduring treatment, that are used to estimate the tumor location. These\ninformations are used to predict the close future tumor trajectory. Kalman\nprediction is a classical approach for this task. The main drawback of X-ray\nacquisition is that it irradiates the patient, including its healthy tissues.\nIn the classical Kalman framework, X-ray measurements are taken regularly, i.e.\nat a constant rate. In this paper, we propose a new approach which relaxes this\nconstraint in order to take measurements when they are the most useful. Our aim\nis for a given budget of measurements to optimize the tracking process. This\nidea naturally brings to an optimal intermittent Kalman predictor for which\nmeasurement times are selected to minimize the mean squared prediction error\nover the complete fraction. This optimization problem can be solved directly\nwhen the respiratory model has been identified and the optimal sampling times\ncan be computed at once. These optimal measurement times are obtained by\nsolving a combinatorial optimization problem using a genetic algorithm. We\ncreated a test benchmark on trajectories validated on one patient. This new\nprediction method is compared to the regular Kalman predictor and a relative\nimprovement of 9:8% is observed on the root mean square position estimation\nerror.\n", "versions": [{"version": "v1", "created": "Wed, 20 Mar 2019 10:41:21 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Aspeel", "Antoine", ""], ["Dasnoy", "Damien", ""], ["Jungers", "Rapha\u00ebl M.", ""], ["Macq", "Beno\u00eet", ""]]}, {"id": "1903.09056", "submitter": "Taiyao Wang", "authors": "Taiyao Wang and Ioannis Ch. Paschalidis", "title": "Prescriptive Cluster-Dependent Support Vector Machines with an\n  Application to Reducing Hospital Readmissions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We augment linear Support Vector Machine (SVM) classifiers by adding three\nimportant features: (i) we introduce a regularization constraint to induce a\nsparse classifier; (ii) we devise a method that partitions the positive class\ninto clusters and selects a sparse SVM classifier for each cluster; and (iii)\nwe develop a method to optimize the values of controllable variables in order\nto reduce the number of data points which are predicted to have an undesirable\noutcome, which, in our setting, coincides with being in the positive class. The\nlatter feature leads to personalized prescriptions/recommendations. We apply\nour methods to the problem of predicting and preventing hospital readmissions\nwithin 30-days from discharge for patients that underwent a general surgical\nprocedure. To that end, we leverage a large dataset containing over 2.28\nmillion patients who had surgeries in the period 2011--2014 in the U.S. The\ndataset has been collected as part of the American College of Surgeons National\nSurgical Quality Improvement Program (NSQIP).\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 15:24:38 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Wang", "Taiyao", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1903.09113", "submitter": "Behnam Malmir", "authors": "Behnam Malmir", "title": "Exploratory studies of human gait changes using depth cameras and\n  considering measurement errors", "comments": "73 pages, 26 figures, a thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC stat.AP stat.CO", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This research aims to quantify human walking patterns through depth cameras\nto (1) detect walking pattern changes of a person with and without a\nmotion-restricting device or a walking aid, and to (2) identify distinct\nwalking patterns from different persons of similar physical attributes.\nMicrosoft Kinect devices, often used for video games, were used to provide and\ntrack coordinates of 25 different joints of people over time to form a human\nskeleton. Then multiple machine learning (ML) models were applied to the SE\ndatasets from ten college-age subjects - five males and five females. In\nparticular, ML models were applied to classify subjects into two categories:\nnormal walking and abnormal walking (i.e. with motion-restricting devices). The\nbest ML model (K-nearest neighborhood) was able to predict 97.3% accuracy using\n10-fold cross-validation. Finally, ML models were applied to classify five gait\nconditions: walking normally, walking while wearing the ankle brace, walking\nwhile wearing the ACL brace, walking while using a cane, and walking while\nusing a walker. The best ML model was again the K-nearest neighborhood\nperforming at 98.7% accuracy rate.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 17:06:40 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Malmir", "Behnam", ""]]}, {"id": "1903.09184", "submitter": "Christian Caama\\~no", "authors": "Christian Caama\\~no Carrillo and Sergio Contreras", "title": "Estimating the three-month series of the Chilean Gross Domestic Product", "comments": "14 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the methodology proponed by Cerqueira et al, 2008; is applied\nto estimate the three-month series of the Chilean Gross Domestic Product (GDP)\nin the period composed between 1965 and 2009. In First place, the equation of\nEngle- Granger is estimated using the data of the yearly GPD and related\nvariables. The estimated coeffcients of this regression are used to obtain a\nFirst estimation of the three-month GDP with measurements errors. Then a State\nSpace model is estimated through Benchmarking in order to improve the\npreliminary estimation of the GDP with the purpose of eliminating the maximum\nerror of measurement and in order that the sum of the three-month values\ncoincide with the yearly GDP.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 18:35:11 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Carrillo", "Christian Caama\u00f1o", ""], ["Contreras", "Sergio", ""]]}, {"id": "1903.09235", "submitter": "Taiyao Wang", "authors": "Taiyao Wang, Ioannis Ch. Paschalidis", "title": "Convergence of Parameter Estimates for Regularized Mixed Linear\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider {\\em Mixed Linear Regression (MLR)}, where training data have\nbeen generated from a mixture of distinct linear models (or clusters) and we\nseek to identify the corresponding coefficient vectors. We introduce a {\\em\nMixed Integer Programming (MIP)} formulation for MLR subject to regularization\nconstraints on the coefficient vectors. We establish that as the number of\ntraining samples grows large, the MIP solution converges to the true\ncoefficient vectors in the absence of noise. Subject to slightly stronger\nassumptions, we also establish that the MIP identifies the clusters from which\nthe training samples were generated. In the special case where training data\ncome from a single cluster, we establish that the corresponding MIP yields a\nsolution that converges to the true coefficient vector even when training data\nare perturbed by (martingale difference) noise. We provide a counterexample\nindicating that in the presence of noise, the MIP may fail to produce the true\ncoefficient vectors for more than one clusters. We also provide numerical\nresults testing the MIP solutions in synthetic examples with noise.\n", "versions": [{"version": "v1", "created": "Thu, 21 Mar 2019 20:44:20 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 15:30:59 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Wang", "Taiyao", ""], ["Paschalidis", "Ioannis Ch.", ""]]}, {"id": "1903.09366", "submitter": "Masanori Yamada", "authors": "Heecheol Kim, Masanori Yamada, Kosuke Miyoshi, Hiroshi Yamakawa", "title": "Macro Action Reinforcement Learning with Sequence Disentanglement using\n  Variational Autoencoder", "comments": "First and second authors equally contributed to this paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One problem in the application of reinforcement learning to real-world\nproblems is the curse of dimensionality on the action space. Macro actions, a\nsequence of primitive actions, have been studied to diminish the dimensionality\nof the action space with regard to the time axis. However, previous studies\nrelied on humans defining macro actions or assumed macro actions as repetitions\nof the same primitive actions. We present Factorized Macro Action Reinforcement\nLearning (FaMARL) which autonomously learns disentangled factor representation\nof a sequence of actions to generate macro actions that can be directly applied\nto general reinforcement learning algorithms. FaMARL exhibits higher scores\nthan other reinforcement learning algorithms on environments that require an\nextensive amount of search.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 05:54:27 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 01:46:52 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Kim", "Heecheol", ""], ["Yamada", "Masanori", ""], ["Miyoshi", "Kosuke", ""], ["Yamakawa", "Hiroshi", ""]]}, {"id": "1903.09478", "submitter": "Cristina Fernandes", "authors": "Luis Roque, Cristina A. C. Fernandes and Tony Silva", "title": "Optimal Combination Forecasts on Retail Multi-Dimensional Sales Data", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data in the retail world are particularly rich in terms of\ndimensionality, and these dimensions can be aggregated in groups or\nhierarchies. Valuable information is nested in these complex structures, which\nhelps to predict the aggregated time series data. From a portfolio of brands\nunder HUUB's monitoring, we selected two to explore their sales behaviour,\nleveraging the grouping properties of their product structure. Using\nstatistical models, namely SARIMA, to forecast each level of the hierarchy, an\noptimal combination approach was used to generate more consistent forecasts in\nthe higher levels. Our results show that the proposed methods can indeed\ncapture nested information in the more granular series, helping to improve the\nforecast accuracy of the aggregated series. The Weighted Least Squares (WLS)\nmethod surpasses all other methods proposed in the study, including the Minimum\nTrace (MinT) reconciliation.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 12:53:23 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Roque", "Luis", ""], ["Fernandes", "Cristina A. C.", ""], ["Silva", "Tony", ""]]}, {"id": "1903.09512", "submitter": "Baldur Magnusson", "authors": "Marc Vandemeulebroecke and Mark Baillie and Alison Margolskee and\n  Baldur Magnusson", "title": "Tutorial: Effective visual communication for the quantitative scientist", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective visual communication is a core competency for pharmacometricians,\nstatisticians, and more generally any quantitative scientist. It is essential\nin every step of a quantitative workflow, from scoping to execution and\ncommunicating results and conclusions. With this competency, we can better\nunderstand data and influence decisions towards appropriate actions. Without\nit, we can fool ourselves and others and pave the way to wrong conclusions and\nactions. The goal of this tutorial is to convey this competency. We posit three\nlaws of effective visual communication for the quantitative scientist: have a\nclear purpose, show the data clearly, and make the message obvious. A concise\n\"Cheat Sheet\", available on https://graphicsprinciples.github.io, distills more\ngranular recommendations for everyday practical use. Finally, these laws and\nrecommendations are illustrated in four case studies.\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 14:04:19 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Vandemeulebroecke", "Marc", ""], ["Baillie", "Mark", ""], ["Margolskee", "Alison", ""], ["Magnusson", "Baldur", ""]]}, {"id": "1903.09923", "submitter": "Fan Li", "authors": "Fan Li", "title": "Design and analysis considerations for cohort stepped wedge cluster\n  randomized trials with a decay correlation structure", "comments": "20 pages, 3 figures, 6 tables", "journal-ref": "Statistics in Medicine, 2020. 39(4), 438-455", "doi": "10.1002/sim.8415", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stepped wedge cluster randomized trial is a type of longitudinal cluster\ndesign that sequentially switches clusters to intervention over time until all\nclusters are treated. While the traditional posttest-only parallel design\nrequires adjustment for a single intraclass correlation coefficient, the\nstepped wedge design allows multiple outcome measurements from the same cluster\nand so additional correlation parameters are necessary to characterize the\nwithin-cluster correlation structure. Although a number of studies have\ndifferentiated between the concepts of within-period and between-period\ncorrelations, only a few studies have allowed the between-period correlation to\ndecay over time. In this article, we consider the proportional decay\ncorrelation structure for a cohort stepped wedge design, and provide a\nmatrix-adjusted quasi-least squares (MAQLS) approach to accurately estimate the\ncorrelation parameters along with the marginal intervention effect. We further\ndevelop the sample size and power procedures accounting for the correlation\ndecay, and investigate the accuracy of the power procedure with continuous\noutcomes in a simulation study. We show that the empirical power agrees well\nwith the prediction even with as few as 9 clusters, when data are analyzed with\nMAQLS concurrently with a suitable bias-corrected sandwich variance. Two trial\nexamples are provided to illustrate the new sample size procedure.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 04:47:34 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 17:27:11 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Li", "Fan", ""]]}, {"id": "1903.10032", "submitter": "Benjamin Seiyon Lee", "authors": "Ben Seiyon Lee, Murali Haran, Robert Fuller, David Pollard, and Klaus\n  Keller", "title": "A Fast Particle-Based Approach for Calibrating a 3-D Model of the\n  Antarctic Ice Sheet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the scientifically challenging and policy-relevant task of\nunderstanding the past and projecting the future dynamics of the Antarctic ice\nsheet. The Antarctic ice sheet has shown a highly nonlinear threshold response\nto past climate forcings. Triggering such a threshold response through\nanthropogenic greenhouse gas emissions would drive drastic and potentially fast\nsea level rise with important implications for coastal flood risks. Previous\nstudies have combined information from ice sheet models and observations to\ncalibrate model parameters. These studies have broken important new ground but\nhave either adopted simple ice sheet models or have limited the number of\nparameters to allow for the use of more complex models. These limitations are\nlargely due to the computational challenges posed by calibration as models\nbecome more computationally intensive or when the number of parameters\nincreases. Here we propose a method to alleviate this problem: a fast\nsequential Monte Carlo method that takes advantage of the massive\nparallelization afforded by modern high performance computing systems. We use\nsimulated examples to demonstrate how our sample-based approach provides\naccurate approximations to the posterior distributions of the calibrated\nparameters. The drastic reduction in computational times enables us to provide\nnew insights into important scientific questions, for example, the impact of\nPliocene era data and prior parameter information on sea level projections.\nThese studies would be computationally prohibitive with other computational\napproaches for calibration such as Markov chain Monte Carlo or emulation-based\nmethods. We also find considerable differences in the distributions of sea\nlevel projections when we account for a larger number of uncertain parameters.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 18:09:33 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 21:01:05 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Lee", "Ben Seiyon", ""], ["Haran", "Murali", ""], ["Fuller", "Robert", ""], ["Pollard", "David", ""], ["Keller", "Klaus", ""]]}, {"id": "1903.10073", "submitter": "Mehdi Korki", "authors": "Hadi Zayyani, Farzan Haddadi, Mehdi Korki", "title": "One Bit Spectrum Sensing in Cognitive Radio Sensor Networks", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a spectrum sensing algorithm from one bit measurements in\na cognitive radio sensor network. A likelihood ratio test (LRT) for the one bit\nspectrum sensing problem is derived. Different from the one bit spectrum\nsensing research work in the literature, the signal is assumed to be a discrete\nrandom correlated Gaussian process, where the correlation is only available\nwithin immediate successive samples of the received signal. The employed model\nfacilitates the design of a powerful detection criteria with measurable\nanalytical performance. One bit spectrum sensing criterion is derived for one\nsensor which is then generalized to multiple sensors. Performance of the\ndetector is analyzed by obtaining closed-form formulas for the probability of\nfalse alarm and the probability of detection. Simulation results corroborate\nthe theoretical findings and confirm the efficacy of the proposed detector in\nthe context of highly correlated signals and large number of sensors.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 22:52:47 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Zayyani", "Hadi", ""], ["Haddadi", "Farzan", ""], ["Korki", "Mehdi", ""]]}, {"id": "1903.10221", "submitter": "Christopher Pooley Dr", "authors": "C. M. Pooley, S. C. Bishop, A. Doeschl-Wilson and G. Marion", "title": "Posterior-based proposals for speeding up Markov chain Monte Carlo", "comments": "54 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) is widely used for Bayesian inference in\nmodels of complex systems. Performance, however, is often unsatisfactory in\nmodels with many latent variables due to so-called poor mixing, necessitating\ndevelopment of application specific implementations. This paper introduces\n\"posterior-based proposals\" (PBPs), a new type of MCMC update applicable to a\nhuge class of statistical models (whose conditional dependence structures are\nrepresented by directed acyclic graphs). PBPs generates large joint updates in\nparameter and latent variable space, whilst retaining good acceptance rates\n(typically 33%). Evaluation against other approaches (from standard Gibbs /\nrandom walk updates to state-of-the-art Hamiltonian and particle MCMC methods)\nwas carried out for widely varying model types: an individual-based model for\ndisease diagnostic test data, a financial stochastic volatility model, a mixed\nmodel used in statistical genetics and a population model used in ecology.\nWhilst different methods worked better or worse in different scenarios, PBPs\nwere found to be either near to the fastest or significantly faster than the\nnext best approach (by up to a factor of 10). PBPs therefore represent an\nadditional general purpose technique that can be usefully applied in a wide\nvariety of contexts.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 10:16:08 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 16:07:20 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Pooley", "C. M.", ""], ["Bishop", "S. C.", ""], ["Doeschl-Wilson", "A.", ""], ["Marion", "G.", ""]]}, {"id": "1903.10310", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok (PARIETAL), John Ioannidis", "title": "Exploration, inference and prediction in neuroscience and biomedicine", "comments": null, "journal-ref": "Trends in Neurosciences, Elsevier, 2019", "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decades saw dramatic progress in brain research. These advances were\noften buttressed by probing single variables to make circumscribed discoveries,\ntypically through null hypothesis significance testing. New ways for generating\nmassive data fueled tension between the traditional methodology, used to infer\nstatistically relevant effects in carefully-chosen variables, and\npattern-learning algorithms, used to identify predictive signatures by\nsearching through abundant information. In this article, we detail the\nantagonistic philosophies behind two quantitative approaches: certifying robust\neffects in understandable variables, and evaluating how accurately a built\nmodel can forecast future outcomes. We discourage choosing analysis tools via\ncategories like 'statistics' or 'machine learning'. Rather, to establish\nreproducible knowledge about the brain, we advocate prioritizing tools in view\nof the core motivation of each quantitative analysis: aiming towards\nmechanistic insight, or optimizing predictive accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 13:08:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Bzdok", "Danilo", "", "PARIETAL"], ["Ioannidis", "John", ""]]}, {"id": "1903.10443", "submitter": "Mattias Villani", "authors": "Olov Andersson, Per Sid\\'en, Johan Dahlin, Patrick Doherty and Mattias\n  Villani", "title": "Real-Time Robotic Search using Hierarchical Spatial Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aerial robots hold great potential for aiding Search and Rescue (SAR) efforts\nover large areas. Traditional approaches typically searches an area\nexhaustively, thereby ignoring that the density of victims varies based on\npredictable factors, such as the terrain, population density and the type of\ndisaster. We present a probabilistic model to automate SAR planning, with\nexplicit minimization of the expected time to discovery. The proposed model is\na hierarchical spatial point process with three interacting spatial fields for\ni) the point patterns of persons in the area, ii) the probability of detecting\npersons and iii) the probability of injury. This structure allows inclusion of\ninformative priors from e.g. geographic or cell phone traffic data, while\nfalling back to latent Gaussian processes when priors are missing or\ninaccurate. To solve this problem in real-time, we propose a combination of\nfast approximate inference using Integrated Nested Laplace Approximation\n(INLA), and a novel Monte Carlo tree search tailored to the problem.\nExperiments using data simulated from real world GIS maps show that the\nframework outperforms traditional search strategies, and finds up to ten times\nmore injured in the crucial first hours.\n", "versions": [{"version": "v1", "created": "Mon, 25 Mar 2019 16:24:45 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Andersson", "Olov", ""], ["Sid\u00e9n", "Per", ""], ["Dahlin", "Johan", ""], ["Doherty", "Patrick", ""], ["Villani", "Mattias", ""]]}, {"id": "1903.10565", "submitter": "Wenying Ji", "authors": "Wenying Ji", "title": "Simulation-Based Analytics for Fabrication Quality-Associated Decision\n  Support", "comments": null, "journal-ref": null, "doi": "10.7939/R3HX16598", "report-no": null, "categories": "cs.OH stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated, data-driven quality management systems, which facilitate the\ntransformation of data into useable information, are desired to enhance\ndecision-making processes. Integration of accurate, reliable, and\nstraightforward approaches that measure uncertainty of inspection processes are\ninstrumental for the successful implementation of automated, data-driven\nquality management systems. This research has addressed these needs by\nexploring and adapting Bayesian statistics-based approaches for fraction\nnonconforming posterior distribution derivation purposes. Using these accurate\nand reliable inputs, this research further develops novel, analytically-based\napproaches to improve the practical function of traditional construction\nfabrication quality management systems. Multiple descriptive and predictive\nanalytical functionalities are developed to support and augment\nquality-associated decision-making processes. Multi-relational databases (e.g.,\nquality management system, engineering design system, and cost management\nsystem) from an industrial company in Edmonton, Canada, are investigated and\nmapped to implement the novel system proposed. This research has contributed to\nacademic literature and practice by: (1) advancing decision-support systems for\nconstruction management by developing a dynamic simulation environment that\nuses real-time data to enhance simulation predictability; (2) developing\nintegrated analytical methods for improved modeling in fabrication\nquality-associated decision making; and (3) creating reliable and interpretable\ndecision-support metrics for quality performance measurement, complexity\nanalysis, and rework cost management to reduce the data interpretation load of\npractitioners and to uncover valuable knowledge and information from available\ndata sources.\n", "versions": [{"version": "v1", "created": "Mon, 18 Mar 2019 16:09:41 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Ji", "Wenying", ""]]}, {"id": "1903.10694", "submitter": "Anna Khudayarov", "authors": "Anna Khudayarov", "title": "Revising the Wilks Scoring System for pro RAW Powerlifting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: In powerlifting the total result is highly dependent on the athletes\nbodyweight. Powerlifting is divided to equipped and RAW types. Pro RAW\npowerlifting competitions use the Wilks scoring system to compare and rank\npowerlifting results across bodyweights, to choose the winners. The Wilks\nformulas are 5th order polynomials fitted to equipped powerlifting results from\nthe years 1987 - 1994. The Wilks formulas were meant to be updated every 2 to 5\nyears, but have never been revised. This study aims to update the Wilks scoring\nsystem for use in pro RAW powerlifting.\n  Methods: The 10 highest RAW powerlifting totals per weight class for\nbodyweights 60 - 175 kg for men, and 44 kg and up for women, were collected\nfrom the openpowerlifting.org database. Polynomials were fitted to these data\nseparately for men and women, using polynomial regression. The scores for men\nand women were calculated using these fits, and the results are assessed.\n  Results: 4th order polynomials were chosen for best describing the dependence\nof powerlifting total result on bodyweight. The mens scores were normalized to\n500 points, and womens to 455 points, on those fitted curves, to bring the top\nscores to the same level across weight classes of each sex. These scores\nappeared similar across the different weight classes for both men and women.\nThis even scoring across body weights is here considered the single most\nimportant characteristic of a scoring system for pro competitions.\n  Key words: Wilks score, revised prediction curve, polynomial regression,\npowerlifting\n", "versions": [{"version": "v1", "created": "Fri, 22 Mar 2019 17:20:16 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Khudayarov", "Anna", ""]]}, {"id": "1903.10726", "submitter": "Sourav Mishra", "authors": "Sourav Mishra, Toshihiko Yamasaki and Hideaki Imaizumi", "title": "Improving image classifiers for small datasets by learning rate\n  adaptations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our paper introduces an efficient combination of established techniques to\nimprove classifier performance, in terms of accuracy and training time. We\nachieve two-fold to ten-fold speedup in nearing state of the art accuracy, over\ndifferent model architectures, by dynamically tuning the learning rate. We find\nit especially beneficial in the case of a small dataset, where reliability of\nmachine reasoning is lower. We validate our approach by comparing our method\nversus vanilla training on CIFAR-10. We also demonstrate its practical\nviability by implementing on an unbalanced corpus of diagnostic images.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 08:22:01 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 17:15:30 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Mishra", "Sourav", ""], ["Yamasaki", "Toshihiko", ""], ["Imaizumi", "Hideaki", ""]]}, {"id": "1903.10889", "submitter": "Abdolnasser Sadeghkhani", "authors": "Abdolnasser Sadeghkhani and Syed Ejaz Ahmed", "title": "Predicting the scoring time in hockey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a Bayesian predictive density estimator to predict\nthe time until the r-th goal is scored in a hockey game, using ancillary\ninformation such as their performances in the past, points and specialists'\nopinions. To be more specific, we consider a gamma distribution as a waiting\nscoring model. The proposed density estimator belongs to an interesting new\nversion of weighted beta prime distribution and outperforms the other estimator\nin the literature. The efficiency of our estimator is evaluated using\nfrequentist risk along with measuring the prediction error from the old\ndataset, 2016-17, to the current season (2018-19) of the National Hockey\nLeague.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 13:42:53 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sadeghkhani", "Abdolnasser", ""], ["Ahmed", "Syed Ejaz", ""]]}, {"id": "1903.10905", "submitter": "Yury Garcia", "authors": "Yury E. Garc\\'ia, Oksana A. Chkrebtii, Marcos A. Capistr\\'an and,\n  Daniel E. Noyola", "title": "Inference for stochastic kinetic models from multiple data sources for\n  joint estimation of infection dynamics from aggregate reports and virological\n  data", "comments": "This is a reviced version of a previous paper \"Identifying Individual\n  Disease Dynamics in a Stochastic Multi-pathogen Model From Aggregated Reports\n  and Laboratory Data (arXiv:1710.10346)\" and it was upload as a new one\n  accidentally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influenza and respiratory syncytial virus (RSV) are the leading etiological\nagents of seasonal acute respiratory infections (ARI) around the world. Medical\ndoctors typically base the diagnosis of ARI on patients' symptoms alone and do\nnot always conduct virological tests necessary to identify individual viruses,\nwhich limits the ability to study the interaction between multiple pathogens\nand make public health recommendations. We consider a stochastic kinetic model\n(SKM) for two interacting ARI pathogens circulating in a large population and\nan empirically motivated background process for infections with other pathogens\ncausing similar symptoms. An extended marginal sampling approach based on the\nLinear Noise Approximation to the SKM integrates multiple data sources and\nadditional model components. We infer the parameters defining the pathogens'\ndynamics and interaction within a Bayesian hierarchical model and explore the\nposterior trajectories of infections for each illness based on aggregate\ninfection reports from six epidemic seasons collected by the state health\ndepartment, and a subset of virological tests from a sentinel program at a\ngeneral hospital in San Luis Potos\\'i, M\\'exico. We interpret the results based\non real and simulated data and make recommendations for future data collection\nstrategies. Supplementary materials and software are provided online.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 16:30:08 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 19:58:14 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Garc\u00eda", "Yury E.", ""], ["Chkrebtii", "Oksana A.", ""], ["and", "Marcos A. Capistr\u00e1n", ""], ["Noyola", "Daniel E.", ""]]}, {"id": "1903.10916", "submitter": "Adriaan Hilbers", "authors": "Adriaan P Hilbers, David J Brayshaw, Axel Gandy", "title": "Importance subsampling: improving power system planning under\n  climate-based uncertainty", "comments": null, "journal-ref": "Applied Energy 251 (2019), 113114", "doi": "10.1016/j.apenergy.2019.04.110", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies indicate that the effects of inter-annual climate-based\nvariability in power system planning are significant and that long samples of\ndemand & weather data (spanning multiple decades) should be considered. At the\nsame time, modelling renewable generation such as solar and wind requires high\ntemporal resolution to capture fluctuations in output levels. In many realistic\npower system models, using long samples at high temporal resolution is\ncomputationally unfeasible. This paper introduces a novel subsampling approach,\nreferred to as \"importance subsampling\", allowing the use of multiple decades\nof demand & weather data in power system planning models at reduced\ncomputational cost. The methodology can be applied in a wide class of\noptimisation-based power system simulations. A test case is performed on a\nmodel of the United Kingdom created using the open-source modelling framework\nCalliope and 36 years of hourly demand and wind data. Standard data reduction\napproaches such as using individual years or clustering into representative\ndays lead to significant errors in estimates of optimal system design.\nFurthermore, the resultant power systems lead to supply capacity shortages,\nraising questions of generation capacity adequacy. In contrast, \"importance\nsubsampling\" leads to accurate estimates of optimal system design at greatly\nreduced computational cost, with resultant power systems able to meet demand\nacross all 36 years of demand & weather scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:25:25 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Hilbers", "Adriaan P", ""], ["Brayshaw", "David J", ""], ["Gandy", "Axel", ""]]}, {"id": "1903.11143", "submitter": "Gregor Dumphart", "authors": "Gregor Dumphart, Henry Schulten, Bharat Bhatia, Christoph Sulser,\n  Armin Wittneben", "title": "Practical Accuracy Limits of Radiation-Aware Magneto-Inductive 3D\n  Localization", "comments": "To appear at the IEEE ICC 2019 Workshops. This work has been\n  submitted to the IEEE for possible publication. Copyright may be transferred\n  without notice, after which this version may no longer be accessible", "journal-ref": null, "doi": "10.1109/ICCW.2019.8757178", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key motivation for the low-frequency magnetic localization approach is\nthat magnetic near-fields are well predictable by a free-space model, which\nshould enable accurate localization. Yet, limited accuracy has been reported\nfor practical systems and it is unclear whether the inaccuracies are caused by\nfield distortion due to nearby conductors, unconsidered radiative propagation,\nor measurement noise. Hence, we investigate the practical performance limits by\nmeans of a calibrated magnetoinductive system which localizes an active\nsingle-coil agent with arbitrary orientation, using 4 mW transmit power at 500\nkHz. The system uses eight single-coil anchors around a 3m x 3m area in an\noffice room. We base the location estimation on a complex baseband model which\ncomprises both reactive and radiative propagation. The link coefficients, which\nserve as input data for location estimation, are measured with a multiport\nnetwork analyzer while the agent is moved with a positioner device. This\nestablishes a reliable ground truth for calibration and evaluation. The system\nachieves a median position error of 3.2 cm and a 90th percentile of 8.3 cm.\nAfter investigating the model error we conjecture that field distortion due to\nconducting building structures is the main cause of the performance bottleneck.\nThe results are complemented with predictions on the achievable accuracy in\nmore suitable circumstances using the Cram\\'er-Rao lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 20:20:32 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 21:17:45 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dumphart", "Gregor", ""], ["Schulten", "Henry", ""], ["Bhatia", "Bharat", ""], ["Sulser", "Christoph", ""], ["Wittneben", "Armin", ""]]}, {"id": "1903.11198", "submitter": "Caio Waisman", "authors": "Xiliang Lin and Harikesh S. Nair and Navdeep S. Sahni and Caio Waisman", "title": "Parallel Experimentation in a Competitive Advertising Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When multiple firms are simultaneously running experiments on a platform, the\ntreatment effects for one firm may depend on the experimentation policies of\nothers. This paper presents a set of causal estimands that are relevant to such\nan environment. We also present an experimental design that is suitable for\nfacilitating experimentation across multiple competitors in such an\nenvironment. Together, these can be used by a platform to run experiments \"as a\nservice,\" on behalf of its participating firms. We show that the causal\nestimands we develop are identified nonparametrically by the variation induced\nby the design, and present two scalable estimators that help measure them in\ntypical high-dimensional situations. We implement the design on the advertising\nplatform of JD.com, an eCommerce company, which is also a publisher of digital\nads in China. We discuss how the design is engineered within the platform's\nauction-driven ad-allocation system, which is typical of modern, digital\nadvertising marketplaces. Finally, we present results from a parallel\nexperiment involving 16 advertisers and millions of JD.com users. These results\nshowcase the importance of accommodating a role for interactions across\nexperimenters and demonstrates the viability of the framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:26:25 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 21:26:50 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Lin", "Xiliang", ""], ["Nair", "Harikesh S.", ""], ["Sahni", "Navdeep S.", ""], ["Waisman", "Caio", ""]]}, {"id": "1903.11200", "submitter": "Yangmei Zhou", "authors": "Yangmei Zhou, Weixin Yao", "title": "Maximum Likelihood Estimation of a Semiparametric Two-component Mixture\n  Model using Log-concave Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by studies in biological sciences to detect differentially\nexpressed genes, a semiparametric two-component mixture model with one known\ncomponent is being studied in this paper. Assuming the density of the unknown\ncomponent to be log-concave, which contains a very broad family of densities,\nwe develop a semiparametric maximum likelihood estimator and propose an EM\nalgorithm to compute it. Our new estimation method finds the mixing proportions\nand the distribution of the unknown component simultaneously. We establish the\nidentifiability of the proposed semiparametric mixture model and prove the\nexistence and consistency of the proposed estimators. We further compare our\nestimator with several existing estimators through simulation studies and apply\nour method to two real data sets from biological sciences and astronomy.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 00:32:57 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Zhou", "Yangmei", ""], ["Yao", "Weixin", ""]]}, {"id": "1903.11232", "submitter": "Tiffany Tang", "authors": "Yulia Baker, Tiffany M. Tang, Genevera I. Allen", "title": "Feature Selection for Data Integration with Mixed Multi-view Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data integration methods that analyze multiple sources of data simultaneously\ncan often provide more holistic insights than can separate inquiries of each\ndata source. Motivated by the advantages of data integration in the era of \"big\ndata\", we investigate feature selection for high-dimensional multi-view data\nwith mixed data types (e.g. continuous, binary, count-valued). This\nheterogeneity of multi-view data poses numerous challenges for existing feature\nselection methods. However, after critically examining these issues through\nempirical and theoretically-guided lenses, we develop a practical solution, the\nBlock Randomized Adaptive Iterative Lasso (B-RAIL), which combines the\nstrengths of the randomized Lasso, adaptive weighting schemes, and stability\nselection. B-RAIL serves as a versatile data integration method for sparse\nregression and graph selection, and we demonstrate the effectiveness of B-RAIL\nthrough extensive simulations and a case study to infer the ovarian cancer gene\nregulatory network. In this case study, B-RAIL successfully identifies\nwell-known biomarkers associated with ovarian cancer and hints at novel\ncandidates for future ovarian cancer research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 02:56:26 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 16:40:47 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Baker", "Yulia", ""], ["Tang", "Tiffany M.", ""], ["Allen", "Genevera I.", ""]]}, {"id": "1903.11372", "submitter": "Neo Christopher Chung", "authors": "Neo Christopher Chung, B{\\l}a\\.zej Miasojedow, Micha{\\l} Startek, Anna\n  Gambin", "title": "Jaccard/Tanimoto similarity test and estimation methods", "comments": null, "journal-ref": "BMC Bioinformatics (2019) 20(Suppl 15): 644", "doi": "10.1186/s12859-019-3118-5", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary data are used in a broad area of biological sciences. Using binary\npresence-absence data, we can evaluate species co-occurrences that help\nelucidate relationships among organisms and environments. To summarize\nsimilarity between occurrences of species, we routinely use the\nJaccard/Tanimoto coefficient, which is the ratio of their intersection to their\nunion. It is natural, then, to identify statistically significant\nJaccard/Tanimoto coefficients, which suggest non-random co-occurrences of\nspecies. However, statistical hypothesis testing using this similarity\ncoefficient has been seldom used or studied.\n  We introduce a hypothesis test for similarity for biological presence-absence\ndata, using the Jaccard/Tanimoto coefficient. Several key improvements are\npresented including unbiased estimation of expectation and centered\nJaccard/Tanimoto coefficients, that account for occurrence probabilities. We\nderived the exact and asymptotic solutions and developed the bootstrap and\nmeasurement concentration algorithms to compute statistical significance of\nbinary similarity. Comprehensive simulation studies demonstrate that our\nproposed methods produce accurate p-values and false discovery rates. The\nproposed estimation methods are orders of magnitude faster than the exact\nsolution. The proposed methods are implemented in an open source R package\ncalled jaccard (https://cran.r-project.org/package=jaccard).\n  We introduce a suite of statistical methods for the Jaccard/Tanimoto\nsimilarity coefficient, that enable straightforward incorporation of\nprobabilistic measures in analysis for species co-occurrences. Due to their\ngenerality, the proposed methods and implementations are applicable to a wide\nrange of binary data arising from genomics, biochemistry, and other areas of\nscience.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 12:22:31 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chung", "Neo Christopher", ""], ["Miasojedow", "B\u0142a\u017cej", ""], ["Startek", "Micha\u0142", ""], ["Gambin", "Anna", ""]]}, {"id": "1903.11454", "submitter": "Milena \\v{C}uki\\'c Dr", "authors": "Milena Cukic Radenkovic", "title": "Machine learning approaches in Detecting the Depression from\n  Resting-state Electroencephalogram (EEG): A Review Study", "comments": "31 pages, 4 Figures. arXiv admin note: text overlap with\n  arXiv:1803.05985 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aimed at reviewing several different approaches present\ntoday in the search for more accurate diagnostic and treatment management in\nmental healthcare. Our focus is on mood disorders, and in particular on the\nmajor depressive disorder (MDD). We are reviewing and discussing findings based\non neuroimaging studies (MRI and fMRI) first to get the impression of the body\nof knowledge about the anatomical and functional differences in depression.\nThen, we are focusing on less expensive data-driven approach, applicable for\neveryday clinical practice, in particular, those based on\nelectroencephalographic (EEG) recordings. Among those studies utilizing EEG, we\nare discussing a group of applications used for detecting of depression based\non the resting state EEG (detection studies) and interventional studies (using\nstimulus in their protocols or aiming to predict the outcome of therapy). We\nconclude with a discussion and review of guidelines to improve the reliability\nof developed models that could serve improvement of diagnostic of depression in\npsychiatry.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:45:43 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Radenkovic", "Milena Cukic", ""]]}, {"id": "1903.11577", "submitter": "Thomas Staudt", "authors": "Thomas Staudt, Timo Aspelmeier, Oskar Laitenberger, Claudia Geisler,\n  Alexander Egner, Axel Munk", "title": "Statistical Molecule Counting in Super-Resolution Fluorescence\n  Microscopy: Towards Quantitative Nanoscopy", "comments": "49 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution microscopy is rapidly gaining importance as an analytical\ntool in the life sciences. A compelling feature is the ability to label\nbiological units of interest with fluorescent markers in living cells and to\nobserve them with considerably higher resolution than conventional microscopy\npermits. The images obtained this way, however, lack an absolute intensity\nscale in terms of numbers of fluorophores observed. We provide an elaborate\nmodel to estimate this information from the raw data. To this end we model the\nentire process of photon generation in the fluorophore, their passage trough\nthe microscope, detection and photo electron amplification in the camera, and\nextraction of time series from the microscopic images. At the heart of these\nmodeling steps is a careful description of the fluorophore dynamics by a novel\nhidden Markov model that operates on two time scales (HTMM). Besides the\nfluorophore number, information about the kinetic transition rates of the\nfluorophore's internal states is also inferred during estimation. We comment on\ncomputational issues that arise when applying our model to simulated or\nmeasured fluorescence traces and illustrate our methodology on simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Mar 2019 22:44:46 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 07:13:21 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 15:14:10 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Staudt", "Thomas", ""], ["Aspelmeier", "Timo", ""], ["Laitenberger", "Oskar", ""], ["Geisler", "Claudia", ""], ["Egner", "Alexander", ""], ["Munk", "Axel", ""]]}, {"id": "1903.11683", "submitter": "Vasileios Tzoumas", "authors": "Vasileios Tzoumas, Pasquale Antonante, Luca Carlone", "title": "Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms,\n  and Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG cs.RO cs.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial perception is the backbone of many robotics applications, and spans a\nbroad range of research problems, including localization and mapping, point\ncloud alignment, and relative pose estimation from camera images. Robust\nspatial perception is jeopardized by the presence of incorrect data\nassociation, and in general, outliers. Although techniques to handle outliers\ndo exist, they can fail in unpredictable manners (e.g., RANSAC, robust\nestimators), or can have exponential runtime (e.g., branch-and-bound). In this\npaper, we advance the state of the art in outlier rejection by making three\ncontributions. First, we show that even a simple linear instance of outlier\nrejection is inapproximable: in the worst-case one cannot design a\nquasi-polynomial time algorithm that computes an approximate solution\nefficiently. Our second contribution is to provide the first per-instance\nsub-optimality bounds to assess the approximation quality of a given outlier\nrejection outcome. Our third contribution is to propose a simple\ngeneral-purpose algorithm, named adaptive trimming, to remove outliers. Our\nalgorithm leverages recently-proposed global solvers that are able to solve\noutlier-free problems, and iteratively removes measurements with large errors.\nWe demonstrate the proposed algorithm on three spatial perception problems: 3D\nregistration, two-view geometry, and SLAM. The results show that our algorithm\noutperforms several state-of-the-art methods across applications while being a\ngeneral-purpose method.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:12:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 19:46:50 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Tzoumas", "Vasileios", ""], ["Antonante", "Pasquale", ""], ["Carlone", "Luca", ""]]}, {"id": "1903.11696", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Caroline \\\"Ubelh\\\"or, Steven W. Mes, Roland\n  Martens, Thomas Koopman, Pim de Graaf, Floris H.P. van Velden, Ronald\n  Boellaard, Jonas A. Castelijns, Dennis E. te Beest, Martijn W. Heymans, Mark\n  A. van de Wiel", "title": "Stable prediction with radiomics data", "comments": "52 pages: 14 pages Main Text and 38 pages of Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.IV q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Radiomics refers to the high-throughput mining of quantitative\nfeatures from radiographic images. It is a promising field in that it may\nprovide a non-invasive solution for screening and classification. Standard\nmachine learning classification and feature selection techniques, however, tend\nto display inferior performance in terms of (the stability of) predictive\nperformance. This is due to the heavy multicollinearity present in radiomic\ndata. We set out to provide an easy-to-use approach that deals with this\nproblem.\n  Results: We developed a four-step approach that projects the original\nhigh-dimensional feature space onto a lower-dimensional latent-feature space,\nwhile retaining most of the covariation in the data. It consists of (i)\npenalized maximum likelihood estimation of a redundancy filtered correlation\nmatrix. The resulting matrix (ii) is the input for a maximum likelihood factor\nanalysis procedure. This two-stage maximum-likelihood approach can be used to\n(iii) produce a compact set of stable features that (iv) can be directly used\nin any (regression-based) classifier or predictor. It outperforms other\nclassification (and feature selection) techniques in both external and internal\nvalidation settings regarding survival in squamous cell cancers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Mar 2019 20:45:58 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["\u00dcbelh\u00f6r", "Caroline", ""], ["Mes", "Steven W.", ""], ["Martens", "Roland", ""], ["Koopman", "Thomas", ""], ["de Graaf", "Pim", ""], ["van Velden", "Floris H. P.", ""], ["Boellaard", "Ronald", ""], ["Castelijns", "Jonas A.", ""], ["Beest", "Dennis E. te", ""], ["Heymans", "Martijn W.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1903.11865", "submitter": "Jasper G. Franke", "authors": "Jasper G. Franke and Reik V. Donner", "title": "Correlating Paleoclimate Time Series: Sources of Uncertainty and\n  Potential Pitfalls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing paleoclimate time series is complicated by a variety of typical\nfeatures, including irregular sampling, age model uncertainty (e.g., errors due\nto interpolation between radiocarbon sampling points) and time uncertainty\n(uncertainty in calibration), which, taken together, result in unequal and\nuncertain observation times of the individual time series to be correlated.\nSeveral methods have been proposed to approximate the joint probability\ndistribution needed to estimate correlations, most of which rely either on\ninterpolation or temporal downsampling.\n  Here, we compare the performance of some popular approximation methods using\nsynthetic data resembling common properties of real world marine sediment\nrecords. Correlations are determined by estimating the parameters of a\nbivariate Gaussian model from the data using Markov Chain Monte Carlo sampling.\nWe complement our pseudoproxy experiments by applying the same methodology to a\npair of marine benthic oxygen records from the Atlantic Ocean.\n  We find that methods based upon interpolation yield better results in terms\nof precision and accuracy than those which reduce the number of observations.\nIn all cases, the specific characteristics of the studied time series are,\nhowever, more important than the choice of a particular interpolation method.\nRelevant features include the number of observations, the persistence of each\nrecord, and the imposed coupling strength between the paired series. In most of\nour pseudoproxy experiments, uncertainty in observation times introduces less\nadditional uncertainty than unequal sampling and errors in observation times\ndo. Thus, it can be reasonable to rely on published time scales as long as\ncalibration uncertainties are not known.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 09:55:06 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Franke", "Jasper G.", ""], ["Donner", "Reik V.", ""]]}, {"id": "1903.12012", "submitter": "Jing Chen", "authors": "Xiangyan Tang, Liang Wang, Jieren Cheng, Jing Chen", "title": "Forecasting model based on information-granulated GA-SVR and ARIMA for\n  producer price index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of predicting the Producer Price Index (PPI) plays an\nindispensable role in government economic work. However, it is difficult to\nforecast the PPI. In our research, we first propose an unprecedented hybrid\nmodel based on fuzzy information granulation that integrates the GA-SVR and\nARIMA (Autoregressive Integrated Moving Average Model) models. The\nfuzzy-information-granulation-based GA-SVR-ARIMA hybrid model is intended to\ndeal with the problem of imprecision in PPI estimation. The proposed model\nadopts the fuzzy information-granulation algorithm to\npre-classification-process monthly training samples of the PPI, and produced\nthree different sequences of fuzzy information granules, whose Support Vector\nRegression (SVR) machine forecast models were separately established for their\nGenetic Algorithm (GA) optimization parameters. Finally, the residual errors of\nthe GA-SVR model were rectified through ARIMA modeling, and the PPI estimate\nwas reached. Research shows that the PPI value predicted by this hybrid model\nis more accurate than that predicted by other models, including ARIMA, GRNN,\nand GA-SVR, following several comparative experiments. Research also indicates\nthe precision and validation of the PPI prediction of the hybrid model and\ndemonstrates that the model has consistent ability to leverage the forecasting\nadvantage of GA-SVR in non-linear space and of ARIMA in linear space.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 14:40:43 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Tang", "Xiangyan", ""], ["Wang", "Liang", ""], ["Cheng", "Jieren", ""], ["Chen", "Jing", ""]]}, {"id": "1903.12067", "submitter": "Kristina Rognlien Dahl", "authors": "Kristina Rognlien Dahl and Arne Bang Huseby", "title": "Buffered environmental contours", "comments": null, "journal-ref": "Safety and Reliability - Safe Societies in a Changing World.\n  Proceedings of ESREL 2018, June 17-21, 2018. Taylor & Francis. ISBN\n  9781351174657. 281. s 2285 - 2292", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of this paper is to use the notion of buffered failure\nprobability from probabilistic structural design, to introduce buffered\nenvironmental contours. Classical environmental contours are used in structural\ndesign in order to obtain upper bounds on the failure probabilities of a large\nclass of designs. The purpose of buffered failure probabilities is the same.\nHowever, in constrast to classical environmental contours, this new concept\ndoes not just take into account failure vs. functioning, but also to which\nextent the system is failing. For example, this is relevant when considering\nthe risk of flooding: We are not just interested in knowing whether a river has\nflooded. The damages caused by the flooding greatly depends on how much the\nwater has risen above the standard level.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 12:22:21 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Dahl", "Kristina Rognlien", ""], ["Huseby", "Arne Bang", ""]]}, {"id": "1903.12078", "submitter": "Ziyu Liu", "authors": "Ziyu Liu, Shihong Wei, James C. Spall", "title": "Error Analysis for the Particle Filter: Methods and Theoretical Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The particle filter is a popular Bayesian filtering algorithm for use in\ncases where the state-space model is nonlinear and/or the random terms (initial\nstate or noises) are non-Gaussian distributed. We study the behavior of the\nerror in the particle filter algorithm as the number of particles gets large.\nAfter a decomposition of the error into two terms, we show that the difference\nbetween the estimator and the conditional mean is asymptotically normal when\nthe resampling is done at every step in the filtering process. Two\nnonlinear/non-Gaussian examples are tested to verify this conclusion.\n", "versions": [{"version": "v1", "created": "Tue, 26 Mar 2019 14:45:22 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Liu", "Ziyu", ""], ["Wei", "Shihong", ""], ["Spall", "James C.", ""]]}, {"id": "1903.12127", "submitter": "Emilia Apostolova PhD", "authors": "Tony Wang, Tim Tschampel, Emilia Apostolova and Tom Velez", "title": "Using Latent Class Analysis to Identify ARDS Sub-phenotypes for Enhanced\n  Machine Learning Predictive Performance", "comments": "Work in progress, preliminary results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we utilize Machine Learning for early recognition of patients\nat high risk of acute respiratory distress syndrome (ARDS), which is critical\nfor successful prevention strategies for this devastating syndrome. The\ndifficulty in early ARDS recognition stems from its complex and heterogenous\nnature. In this study, we integrate knowledge of the heterogeneity of ARDS\npatients into predictive model building. Using MIMIC-III data, we first apply\nlatent class analysis (LCA) to identify homogeneous sub-groups in the ARDS\npopulation, and then build predictive models on the partitioned data. The\nresults indicate that significantly improved performances of prediction can be\nobtained for two of the three identified sub-phenotypes of ARDS. Experiments\nsuggests that identifying sub-phenotypes is beneficial for building predictive\nmodel for ARDS.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 17:12:35 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Wang", "Tony", ""], ["Tschampel", "Tim", ""], ["Apostolova", "Emilia", ""], ["Velez", "Tom", ""]]}]