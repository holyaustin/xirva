[{"id": "1205.0243", "submitter": "Andino Maseleno", "authors": "Andino Maseleno, Md. Mahmud Hasan", "title": "Poultry Diseases Expert System using Dempster-Shafer Theory", "comments": "Brunei International Conference and Engineering Technology 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Based on World Health Organization (WHO) fact sheet in the 2011, outbreaks of\npoultry diseases especially Avian Influenza in poultry may raise global public\nhealth concerns due to their effect on poultry populations, their potential to\ncause serious disease in people, and their pandemic potential. In this\nresearch, we built a Poultry Diseases Expert System using Dempster-Shafer\nTheory. In this Poultry Diseases Expert System We describe five symptoms which\ninclude depression, combs, wattle, bluish face region, swollen face region,\nnarrowness of eyes, and balance disorders. The result of the research is that\nPoultry Diseases Expert System has been successfully identifying poultry\ndiseases.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2012 00:19:47 GMT"}, {"version": "v2", "created": "Sun, 6 May 2012 04:11:42 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Maseleno", "Andino", ""], ["Hasan", "Md. Mahmud", ""]]}, {"id": "1205.0540", "submitter": "Weimao Ke", "authors": "Weimao Ke", "title": "A Fitness Model for Scholarly Impact Analysis", "comments": "19 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DL cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model to analyze citation growth and influences of fitness\n(competitiveness) factors in an evolving citation network. Applying the\nproposed method to modeling citations to papers and scholars in the InfoVis\n2004 data, a benchmark collection about a 31-year history of information\nvisualization, leads to findings consistent with citation distributions in\ngeneral and observations of the domain in particular. Fitness variables based\non prior impacts and the time factor have significant influences on citation\noutcomes. We find considerably large effect sizes from the fitness modeling,\nwhich suggest inevitable bias in citation analysis due to these factors. While\nraw citation scores offer little insight into the growth of InfoVis,\nnormalization of the scores by influences of time and prior fitness offers a\nreasonable depiction of the field's development. The analysis demonstrates the\nproposed model's ability to produce results consistent with observed data and\nto support meaningful comparison of citation scores over time.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2012 19:50:45 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Ke", "Weimao", ""]]}, {"id": "1205.0680", "submitter": "Lutz Bornmann Dr.", "authors": "Lutz Bornmann and Loet Leydesdorff", "title": "Citation impact of papers published from six prolific countries: A\n  national comparison based on InCites data", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the InCites tool of Thomson Reuters, this study compares normalized\ncitation impact values calculated for China, Japan, France, Germany, United\nStates, and the UK throughout the time period from 1981 to 2010. The citation\nimpact values are normalized to four subject areas: natural sciences;\nengineering and technology; medical and health sciences; and agricultural\nsciences. The results show an increasing trend in citation impact values for\nFrance, the UK and especially for Germany across the last thirty years in all\nsubject areas. The citation impact of papers from China is still at a\nrelatively low level (mostly below the world average), but the country follows\nan increasing trend line. The USA exhibits a relatively stable pattern of high\ncitation impact values across the years. With small impact differences between\nthe publication years, the US trend is increasing in engineering and technology\nbut decreasing in medical and health sciences as well as in agricultural\nsciences. Similar to the USA, Japan follows increasing as well as decreasing\ntrends in different subject areas, but the variability across the years is\nsmall. In most of the years, papers from Japan perform below or approximately\nat the world average in each subject area.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 11:40:56 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Bornmann", "Lutz", ""], ["Leydesdorff", "Loet", ""]]}, {"id": "1205.0686", "submitter": "Erika Cule", "authors": "Erika Cule and Maria De Iorio", "title": "A semi-automatic method to guide the choice of ridge parameter in ridge\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the application of a popular penalised regression method, Ridge\nRegression, to data with very high dimensions and many more covariates than\nobservations. Our motivation is the problem of out-of-sample prediction and the\nsetting is high-density genotype data from a genome-wide association or\nresequencing study. Ridge regression has previously been shown to offer\nimproved performance for prediction when compared with other penalised\nregression methods. One problem with ridge regression is the choice of an\nappropriate parameter for controlling the amount of shrinkage of the\ncoefficient estimates. Here we propose a method for choosing the ridge\nparameter based on controlling the variance of the predicted observations in\nthe model.\n  Using simulated data, we demonstrate that our method outperforms subset\nselection based on univariate tests of association and another penalised\nregression method, HyperLasso regression, in terms of improved prediction\nerror. We extend our approach to regression problems when the outcomes are\nbinary (representing cases and controls, as is typically the setting for\ngenome-wide association studies) and demonstrate the method on a real data\nexample consisting of case-control and genotype data on Bipolar Disorder, taken\nfrom the Wellcome Trust Case Control Consortium and the Genetic Association\nInformation Network.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 11:54:13 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Cule", "Erika", ""], ["De Iorio", "Maria", ""]]}, {"id": "1205.0741", "submitter": "Dmitry Kobak", "authors": "Dmitry Kobak, Sergey Shpilkin and Maxim S. Pshenichnikov", "title": "Statistical anomalies in 2011-2012 Russian elections revealed by 2D\n  correlation analysis", "comments": "12 pages, 5 figures; Methods slightly expanded", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we perform a statistical analysis of the official data from recent\nRussian parliamentary and presidential elections (held on December 4th, 2011\nand March 4th, 2012, respectively). A number of anomalies are identified that\npersistently skew the results in favour of the pro-government party, United\nRussia (UR), and its leader Vladimir Putin. The main irregularities are: (i)\nremarkably high correlation between turnout and voting results; (ii) a large\nnumber of polling stations where the UR/Putin results are given by a round\nnumber of percent; (iii) constituencies showing improbably low or (iv)\nanomalously high dispersion of results across polling stations; (v) substantial\ndifference between results at paper-based and electronic polling stations.\nThese anomalies, albeit less prominent in the presidential elections, hardly\nconform to the assumptions of fair and free voting. The approaches proposed\nhere can be readily extended to quantify fingerprints of electoral fraud in any\nother problematic elections.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 15:46:04 GMT"}, {"version": "v2", "created": "Thu, 17 May 2012 09:38:11 GMT"}], "update_date": "2012-05-18", "authors_parsed": [["Kobak", "Dmitry", ""], ["Shpilkin", "Sergey", ""], ["Pshenichnikov", "Maxim S.", ""]]}, {"id": "1205.0793", "submitter": "Jennifer Listgarten", "authors": "Jennifer Listgarten, Christoph Lippert, Eun Yong Kang, Jing Xiang,\n  Carl M. Kadie and David Heckerman", "title": "A powerful and efficient set test for genetic markers that handles\n  confounders", "comments": "* denotes equal contributions", "journal-ref": null, "doi": "10.1093/bioinformatics/btt177", "report-no": null, "categories": "q-bio.GN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches for testing sets of variants, such as a set of rare or common\nvariants within a gene or pathway, for association with complex traits are\nimportant. In particular, set tests allow for aggregation of weak signal within\na set, can capture interplay among variants, and reduce the burden of multiple\nhypothesis testing. Until now, these approaches did not address confounding by\nfamily relatedness and population structure, a problem that is becoming more\nimportant as larger data sets are used to increase power.\n  Results: We introduce a new approach for set tests that handles confounders.\nOur model is based on the linear mixed model and uses two random effects-one to\ncapture the set association signal and one to capture confounders. We also\nintroduce a computational speedup for two-random-effects models that makes this\napproach feasible even for extremely large cohorts. Using this model with both\nthe likelihood ratio test and score test, we find that the former yields more\npower while controlling type I error. Application of our approach to richly\nstructured GAW14 data demonstrates that our method successfully corrects for\npopulation structure and family relatedness, while application of our method to\na 15,000 individual Crohn's disease case-control cohort demonstrates that it\nadditionally recovers genes not recoverable by univariate analysis.\n  Availability: A Python-based library implementing our approach is available\nat http://mscompbio.codeplex.com\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2012 19:05:38 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2012 16:49:49 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 04:30:32 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Listgarten", "Jennifer", ""], ["Lippert", "Christoph", ""], ["Kang", "Eun Yong", ""], ["Xiang", "Jing", ""], ["Kadie", "Carl M.", ""], ["Heckerman", "David", ""]]}, {"id": "1205.1078", "submitter": "Wentian Li", "authors": "Wentian Li", "title": "Analyses of Baby Name Popularity Distribution in U.S. for the Last 131\n  Years", "comments": "6 figures", "journal-ref": "Complexity, 18(1):44-50 (2012)", "doi": "10.1002/cplx.21409", "report-no": null, "categories": "nlin.AO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the complete dataset of baby name popularity collected by U.S.\nSocial Security Administration for the last 131 years (1880-2010). The ranked\nbaby name popularity can be fitted empirically by a piecewise function\nconsisting of Beta function for the high-ranking names and power-law function\nfor low-ranking names, but not power-law (Zipf's law) or Beta function by\nitself.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2012 22:29:44 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Li", "Wentian", ""]]}, {"id": "1205.1107", "submitter": "Jean-Noel Bacro", "authors": "Jean-Noel Bacro, Carlo Gaetan", "title": "Estimation of spatial max-stable models using threshold exceedances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric inference for spatial max-stable processes is difficult since the\nrelated likelihoods are unavailable. A composite likelihood approach based on\nthe bivariate distribution of block maxima has been recently proposed in the\nliterature. However modeling block maxima is a wasteful approach provided that\nother information is available. Moreover an approach based on block, typically\nannual, maxima is unable to take into account the fact that maxima occur or not\nsimultaneously. If time series of, say, daily data are available, then\nestimation procedures based on exceedances of a high threshold could mitigate\nsuch problems. In this paper we focus on two approaches for composing\nlikelihoods based on pairs of exceedances. The first one comes from the tail\napproximation for bivariate distribution proposed by Ledford and Tawn (1996)\nwhen both pairs of observations exceed the fixed threshold. The second one uses\nthe bivariate extension (Rootzen and Tajvidi, 2006) of the generalized Pareto\ndistribution which allows to model exceedances when at least one of the\ncomponents is over the threshold. The two approaches are compared through a\nsimulation study according to different degrees of spatial dependency. Results\nshow that both the strength of the spatial dependencies and the threshold\nchoice play a fundamental role in determining which is the best estimating\nprocedure.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2012 06:47:58 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Bacro", "Jean-Noel", ""], ["Gaetan", "Carlo", ""]]}, {"id": "1205.1287", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Tzyy-Ping Jung, Scott Makeig, Bhaskar D. Rao", "title": "Compressed Sensing for Energy-Efficient Wireless Telemonitoring of\n  Noninvasive Fetal ECG via Block Sparse Bayesian Learning", "comments": "The code and the data can be downloaded from the first author's\n  homepage: http://sites.google.com/site/researchbyzhang/bsbl, or\n  http://dsp.ucsd.edu/~zhilin/BSBL.html", "journal-ref": null, "doi": "10.1109/TBME.2012.2226175", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fetal ECG (FECG) telemonitoring is an important branch in telemedicine. The\ndesign of a telemonitoring system via a wireless body-area network with low\nenergy consumption for ambulatory use is highly desirable. As an emerging\ntechnique, compressed sensing (CS) shows great promise in\ncompressing/reconstructing data with low energy consumption. However, due to\nsome specific characteristics of raw FECG recordings such as non-sparsity and\nstrong noise contamination, current CS algorithms generally fail in this\napplication.\n  This work proposes to use the block sparse Bayesian learning (BSBL) framework\nto compress/reconstruct non-sparse raw FECG recordings. Experimental results\nshow that the framework can reconstruct the raw recordings with high quality.\nEspecially, the reconstruction does not destroy the interdependence relation\namong the multichannel recordings. This ensures that the independent component\nanalysis decomposition of the reconstructed recordings has high fidelity.\nFurthermore, the framework allows the use of a sparse binary sensing matrix\nwith much fewer nonzero entries to compress recordings. Particularly, each\ncolumn of the matrix can contain only two nonzero entries. This shows the\nframework, compared to other algorithms such as current CS algorithms and\nwavelet algorithms, can greatly reduce code execution in CPU in the data\ncompression stage.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 06:15:15 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2012 16:04:28 GMT"}, {"version": "v3", "created": "Sun, 29 Jul 2012 12:41:03 GMT"}, {"version": "v4", "created": "Wed, 19 Sep 2012 20:05:41 GMT"}, {"version": "v5", "created": "Sun, 23 Sep 2012 22:29:43 GMT"}, {"version": "v6", "created": "Fri, 19 Oct 2012 12:24:39 GMT"}, {"version": "v7", "created": "Sun, 2 Nov 2014 05:38:50 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Zhang", "Zhilin", ""], ["Jung", "Tzyy-Ping", ""], ["Makeig", "Scott", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1205.1461", "submitter": "Neretin Yurii A.", "authors": "Yury Neretin", "title": "On statistical researches of parliament elections in Russian Federation,\n  04.12.2011", "comments": "29pp, 26 figures", "journal-ref": null, "doi": null, "report-no": "ESI-2349", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lot of statistical researches of Russian elections 04.12.2011. The\npurpose of this activity is to give a mathematical proof of large\nfalsifications and to estimate possible 'real results of elections'. My purpose\nis to show that\n  1. Statistical argumentation allows to prove existence of falsifications and\nto give a lower estimate of falsification, near 1-2 percents.\n  2. Statistical proofs of stronger statements are incorrect from both points\nof view of mathematics and of natural sciences.\n  3. This problem is not a problem of pure mathematics (since it includes\nstrong indeterminacy of sociological nature).\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 14:22:50 GMT"}], "update_date": "2012-05-08", "authors_parsed": [["Neretin", "Yury", ""]]}, {"id": "1205.1492", "submitter": "Serguei Saavedra", "authors": "Serguei Saavedra, Satyam Mukherjee, James P. Bagrow", "title": "Is coaching experience associated with effective use of timeouts in\n  basketball?", "comments": "Scientific Reports 2, Article number: 676 (2012)", "journal-ref": null, "doi": "10.1038/srep00676", "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience is an important asset in almost any professional activity. In\nbasketball, there is believed to be a positive association between coaching\nexperience and effective use of team timeouts. Here, we analyze both the extent\nto which a team's change in scoring margin per possession after timeouts\ndeviate from the team's average scoring margin per possession---what we called\ntimeout factor, and the extent to which this performance measure is associated\nwith coaching experience across all teams in the National Basketball\nAssociation over the 2009-2012 seasons. We find that timeout factor plays a\nminor role in the scoring dynamics of basketball. Surprisingly, we find that\ntimeout factor is negatively associated with coaching experience. Our findings\nsupport empirical studies showing that, under certain conditions, mentors early\nin their careers can have a stronger positive impact on their teams than later\nin their careers.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2012 19:45:21 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 20:26:00 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Saavedra", "Serguei", ""], ["Mukherjee", "Satyam", ""], ["Bagrow", "James P.", ""]]}, {"id": "1205.1564", "submitter": "Wentian Li", "authors": "Wentian Li", "title": "Characterizing Ranked Chinese Syllable-to-Character Mapping Spectrum: A\n  Bridge Between the Spoken and Written Chinese Language", "comments": "15 pages, 4 figures", "journal-ref": "Journal of Quantitative Linguistics, 20, 153-167 (2013)", "doi": "10.1080/09296174.2013.773140", "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important aspect of the relationship between spoken and written Chinese\nis the ranked syllable-to-character mapping spectrum, which is the ranked list\nof syllables by the number of characters that map to the syllable. Previously,\nthis spectrum is analyzed for more than 400 syllables without distinguishing\nthe four intonations. In the current study, the spectrum with 1280 toned\nsyllables is analyzed by logarithmic function, Beta rank function, and\npiecewise logarithmic function. Out of the three fitting functions, the\ntwo-piece logarithmic function fits the data the best, both by the smallest sum\nof squared errors (SSE) and by the lowest Akaike information criterion (AIC)\nvalue. The Beta rank function is the close second. By sampling from a Poisson\ndistribution whose parameter value is chosen from the observed data, we\nempirically estimate the $p$-value for testing the\ntwo-piece-logarithmic-function being better than the Beta rank function\nhypothesis, to be 0.16. For practical purposes, the piecewise logarithmic\nfunction and the Beta rank function can be considered a tie.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 00:29:48 GMT"}], "update_date": "2013-08-29", "authors_parsed": [["Li", "Wentian", ""]]}, {"id": "1205.1746", "submitter": "Brian Macdonald", "authors": "Brian Macdonald, Craig Lennon, Rodney Sturdivant", "title": "Evaluating NHL Goalies, Skaters, and Teams Using Weighted Shots", "comments": "19 pages, 10 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a logistic regression model to estimate the\nprobability that a particular shot in an NHL game will result in a goal, and\nuse the results to evaluate the performance of NHL skaters, goalies, and teams.\nWe weight each shot based on the estimated probabilities obtained from our\nmodel, call this statistic \"weighted shots\", and use advanced statistics based\non weighted shots as the basis of our evaluation. We also analyze whether\nadvanced statistics based on weighted shots outperform traditional statistics\nas an indicator of future performance of skaters, goalies, and teams. In\ngeneral, statistics based on weighted shots perform well, but not better than\ntraditional statistics. We conclude that weighted shots should not be viewed as\na replacement for those statistics, but can be used in conjunction with those\nstatistics. Finally, we use weighted shots as the dependent variable in an\nadjusted plus-minus model. The results are estimates of each player's offensive\nand defensive contribution to his team's weighted shots during even strength,\npower play, and short handed situations, independent of the strength of his\nteammates, the strength of his opponents, and the zone in which his shifts\nbegin.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 17:08:46 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Macdonald", "Brian", ""], ["Lennon", "Craig", ""], ["Sturdivant", "Rodney", ""]]}, {"id": "1205.1819", "submitter": "Juli Atherton", "authors": "Juli Atherton, Nathan Boley, Ben Brown, Nobuo Ogawa, Stuart M.\n  Davidson, Michael B. Eisen, Mark D. Biggin, Peter Bickel", "title": "A model for sequential evolution of ligands by exponential enrichment\n  (SELEX) data", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS537 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 928-949", "doi": "10.1214/12-AOAS537", "report-no": "IMS-AOAS-AOAS537", "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Systematic Evolution of Ligands by EXponential enrichment (SELEX)\nexperiment begins in round one with a random pool of oligonucleotides in\nequilibrium solution with a target. Over a few rounds, oligonucleotides having\na high affinity for the target are selected. Data from a high throughput SELEX\nexperiment consists of lists of thousands of oligonucleotides sampled after\neach round. Thus far, SELEX experiments have been very good at suggesting the\nhighest affinity oligonucleotide, but modeling lower affinity recognition site\nvariants has been difficult. Furthermore, an alignment step has always been\nused prior to analyzing SELEX data. We present a novel model, based on a\nbiochemical parametrization of SELEX, which allows us to use data from all\nrounds to estimate the affinities of the oligonucleotides. Most notably, our\nmodel also aligns the oligonucleotides. We use our model to analyze a SELEX\nexperiment containing double stranded DNA oligonucleotides and the\ntranscription factor Bicoid as the target. Our SELEX model outperformed other\npublished methods for predicting putative binding sites for Bicoid as indicated\nby the results of an in-vivo ChIP-chip experiment.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2012 20:34:47 GMT"}, {"version": "v2", "created": "Thu, 27 Sep 2012 12:02:51 GMT"}], "update_date": "2012-09-28", "authors_parsed": [["Atherton", "Juli", ""], ["Boley", "Nathan", ""], ["Brown", "Ben", ""], ["Ogawa", "Nobuo", ""], ["Davidson", "Stuart M.", ""], ["Eisen", "Michael B.", ""], ["Biggin", "Mark D.", ""], ["Bickel", "Peter", ""]]}, {"id": "1205.1989", "submitter": "Seunghak Lee", "authors": "Seunghak Lee and Eric P. Xing", "title": "Structured Input-Output Lasso, with Application to eQTL Mapping, and a\n  Thresholding Algorithm for Fast Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a high-dimensional multi-task regression\nmodel, under sparsity constraints induced by presence of grouping structures on\nthe input covariates and on the output predictors. This problem is primarily\nmotivated by expression quantitative trait locus (eQTL) mapping, of which the\ngoal is to discover genetic variations in the genome (inputs) that influence\nthe expression levels of multiple co-expressed genes (outputs), either\nepistatically, or pleiotropically, or both. A structured input-output lasso\n(SIOL) model based on an intricate l1/l2-norm penalty over the regression\ncoefficient matrix is employed to enable discovery of complex sparse\ninput/output relationships; and a highly efficient new optimization algorithm\ncalled hierarchical group thresholding (HiGT) is developed to solve the\nresultant non-differentiable, non-separable, and ultra high-dimensional\noptimization problem. We show on both simulation and on a yeast eQTL dataset\nthat our model leads to significantly better recovery of the structured sparse\nrelationships between the inputs and the outputs, and our algorithm\nsignificantly outperforms other optimization techniques under the same model.\nAdditionally, we propose a novel approach for efficiently and effectively\ndetecting input interactions by exploiting the prior knowledge available from\nbiological experiments.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 14:14:22 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Lee", "Seunghak", ""], ["Xing", "Eric P.", ""]]}, {"id": "1205.2034", "submitter": "Ting-Li Chen", "authors": "Ting-Li Chen, Dai-Ni Hsieh, Hung Hung, I-Ping Tu, Pei-Shien Wu,\n  Yi-Ming Wu, Wei-Hau Chang, Su-Yun Huang", "title": "$\\gamma$-SUP: A clustering algorithm for cryo-electron microscopy images\n  of asymmetric particles", "comments": "Published in at http://dx.doi.org/10.1214/13-AOAS680 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 1, 259-285", "doi": "10.1214/13-AOAS680", "report-no": "IMS-AOAS-AOAS680", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) has recently emerged as a powerful tool\nfor obtaining three-dimensional (3D) structures of biological macromolecules in\nnative states. A minimum cryo-EM image data set for deriving a meaningful\nreconstruction is comprised of thousands of randomly orientated projections of\nidentical particles photographed with a small number of electrons. The\ncomputation of 3D structure from 2D projections requires clustering, which aims\nto enhance the signal to noise ratio in each view by grouping similarly\noriented images. Nevertheless, the prevailing clustering techniques are often\ncompromised by three characteristics of cryo-EM data: high noise content, high\ndimensionality and large number of clusters. Moreover, since clustering\nrequires registering images of similar orientation into the same pixel\ncoordinates by 2D alignment, it is desired that the clustering algorithm can\nlabel misaligned images as outliers. Herein, we introduce a clustering\nalgorithm $\\gamma$-SUP to model the data with a $q$-Gaussian mixture and adopt\nthe minimum $\\gamma$-divergence for estimation, and then use a self-updating\nprocedure to obtain the numerical solution. We apply $\\gamma$-SUP to the\ncryo-EM images of two benchmark macromolecules, RNA polymerase II and ribosome.\nIn the former case, simulated images were chosen to decouple clustering from\nalignment to demonstrate $\\gamma$-SUP is more robust to misalignment outliers\nthan the existing clustering methods used in the cryo-EM community. In the\nlatter case, the clustering of real cryo-EM data by our $\\gamma$-SUP method\neliminates noise in many views to reveal true structure features of ribosome at\nthe projection level.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 16:59:36 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 08:46:32 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2013 15:25:00 GMT"}, {"version": "v4", "created": "Fri, 25 Apr 2014 13:40:44 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Chen", "Ting-Li", ""], ["Hsieh", "Dai-Ni", ""], ["Hung", "Hung", ""], ["Tu", "I-Ping", ""], ["Wu", "Pei-Shien", ""], ["Wu", "Yi-Ming", ""], ["Chang", "Wei-Hau", ""], ["Huang", "Su-Yun", ""]]}, {"id": "1205.2064", "submitter": "Eric D. Feigelson", "authors": "Eric D. Feigelson and G. Jogesh Babu", "title": "Statistical Methods for Astronomy", "comments": "48 pages, 2 figures. Adapted from `Statistical Methods for Astronomy'\n  to appear in `Astronomical Techniques, Software, and Data', volume 2 (Howard\n  Bond, editor) of `Planets, Stars, and Stellar Systems' (Terry Ostwalt,\n  editor) to be published by Springer Science+Business Media", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review outlines concepts of mathematical statistics, elements of\nprobability theory, hypothesis tests and point estimation for use in the\nanalysis of modern astronomical data. Least squares, maximum likelihood, and\nBayesian approaches to statistical inference are treated. Resampling methods,\nparticularly the bootstrap, provide valuable procedures when distributions\nfunctions of statistics are not known. Several approaches to model selection\nand good- ness of fit are considered. Applied statistics relevant to\nastronomical research are briefly discussed: nonparametric methods for use when\nlittle is known about the behavior of the astronomical populations or\nprocesses; data smoothing with kernel density estimation and nonparametric\nregression; unsupervised clustering and supervised classification procedures\nfor multivariate problems; survival analysis for astronomical datasets with\nnondetections; time- and frequency-domain times series analysis for light\ncurves; and spatial statistics to interpret the spatial distributions of points\nin low dimensions. Two types of resources are presented: about 40 recommended\ntexts and monographs in various fields of statistics, and the public domain R\nsoftware system for statistical analysis. Together with its \\sim 3500 (and\ngrowing) add-on CRAN packages, R implements a vast range of statistical\nprocedures in a coherent high-level language with advanced graphics.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 18:42:32 GMT"}], "update_date": "2012-05-10", "authors_parsed": [["Feigelson", "Eric D.", ""], ["Babu", "G. Jogesh", ""]]}, {"id": "1205.2109", "submitter": "Suliadi Sufahani", "authors": "Suliadi F. Sufahani, Siti N. A. Mohd Razali, Mohammad F. Mormin, Azme\n  Khamis", "title": "An Analysis of the Prevalence of Pneumonia for Children under 12 Year\n  Old in Tawau General Hospital, Malaysia", "comments": "Presented at the International Seminar on the Application of Science\n  & Mathematics 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pneumonia is one of the serious illnesses, which involves lung infection\nspecifically alveoli. Nearly 40,000 to 70,000 people die each year in United\nState because of pneumonia. Therefore, it is not a surprise that pneumonia is\none of the most critical illnesses for children under 12 years old in many\nparts of the world, including Malaysia and particularly in Tawau, Sabah,\nMalaysia. The objectives of this study are: to develop a summary on the\nprevalence of pneumonia in Tawau General Hospital, to analyze the best practice\nto prevent this illness and lastly to determine an overview of which area that\nis widely affected by pneumonia. The results can assist doctors and the\ngovernment to take major precautions and preventive measures efficiently to the\nfull extent. This paper presents a descriptive analysis of the data, which are\nretrieved from the medical reports at the Tawau General Hospital. Through the\nfindings, pneumonia is widely spread among young children under 12 years old.\nThere are more than one major factor that leads to this critical illness, such\nas family background, genetic and environment. Therefore, the government,\ndoctors and parents should take major steps to prevent children suffering from\npneumonia.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2012 21:28:13 GMT"}], "update_date": "2012-05-11", "authors_parsed": [["Sufahani", "Suliadi F.", ""], ["Razali", "Siti N. A. Mohd", ""], ["Mormin", "Mohammad F.", ""], ["Khamis", "Azme", ""]]}, {"id": "1205.2240", "submitter": "Markus Haltmeier", "authors": "Markus Haltmeier and Axel Munk", "title": "Extreme Value Analysis of Empirical Frame Coefficients and Implications\n  for Denoising by Soft-Thresholding", "comments": "[Content: 39 pages, 4 figures] Note that in this version 4 we have\n  slightely changed the title of the paper and we have rewritten parts of the\n  introduction. Except for corrected typos the other parts of the paper are the\n  same as the original versions 1", "journal-ref": "Appl. Comput. Harmon. Anal. 36 (2014) 434-460", "doi": "10.1016/j.acha.2013.07.004", "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Denoising by frame thresholding is one of the most basic and efficient\nmethods for recovering a discrete signal or image from data that are corrupted\nby additive Gaussian white noise. The basic idea is to select a frame of\nanalyzing elements that separates the data in few large coefficients due to the\nsignal and many small coefficients mainly due to the noise \\epsilon_n. Removing\nall data coefficients being in magnitude below a certain threshold yields a\nreconstruction of the original signal. In order to properly balance the amount\nof noise to be removed and the relevant signal features to be kept, a precise\nunderstanding of the statistical properties of thresholding is important. For\nthat purpose we derive the asymptotic distribution of max_{\\omega \\in \\Omega_n}\n|<\\phi_\\omega^n,\\epsilon_n>| for a wide class of redundant frames\n(\\phi_\\omega^n: \\omega \\in \\Omega_n}. Based on our theoretical results we give\na rationale for universal extreme value thresholding techniques yielding\nasymptotically sharp confidence regions and smoothness estimates corresponding\nto prescribed significance levels. The results cover many frames used in\nimaging and signal recovery applications, such as redundant wavelet systems,\ncurvelet frames, or unions of bases. We show that `generically' a standard\nGumbel law results as it is known from the case of orthonormal wavelet bases.\nHowever, for specific highly redundant frames other limiting laws may occur. We\nindeed verify that the translation invariant wavelet transform shows a\ndifferent asymptotic behaviour.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2012 11:59:31 GMT"}, {"version": "v2", "created": "Mon, 28 May 2012 20:05:31 GMT"}, {"version": "v3", "created": "Sat, 13 Oct 2012 12:24:59 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2013 10:52:47 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Haltmeier", "Markus", ""], ["Munk", "Axel", ""]]}, {"id": "1205.2417", "submitter": "Giovanni Montana", "authors": "Christopher Minas and Giovanni Montana", "title": "Distance-based analysis of variance: approximate inference and an\n  application to genome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In several modern applications, ranging from genetics to genomics and\nneuroimaging, there is a need to compare observations across different\npopulations, such as groups of healthy and diseased individuals. The interest\nis in detecting a group effect. When the observations are vectorial,\nreal-valued and follow a multivariate Normal distribution, multivariate\nanalysis of variance (MANOVA) tests are routinely applied. However, such\ntraditional procedures are not suitable when dealing with more complex data\nstructures such as functional (e.g. curves) or graph-structured (e.g. trees and\nnetworks) objects, where the required distributional assumptions may be\nviolated. In this paper we discuss a distance-based MANOVA-like approach, the\nDBF test, for detecting differences between groups for a wider range of data\ntypes. The test statistic, analogously to other distance-based statistics, only\nrelies on a suitably chosen distance measure that captures the pairwise\ndissimilarity among all available samples. An approximate null probability\ndistribution of the DBF statistic is proposed thus allowing inferences to be\ndrawn without the need for costly permutation procedures. Through extensive\nsimulations we provide evidence that the proposed methodology works well for a\nrange of data types and distances, and generalizes the traditional MANOVA\ntests. We also report on an application of the proposed methodology for the\nanalysis of a multi-locus genome-wide association study of Alzheimer's disease,\nwhich has been carried out using several genetic distance measures.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 01:59:28 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Minas", "Christopher", ""], ["Montana", "Giovanni", ""]]}, {"id": "1205.2501", "submitter": "Alex Lenkoski", "authors": "Alexander Jordan and Alex Lenkoski", "title": "Tobit Bayesian Model Averaging and the Determinants of Foreign Direct\n  Investment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully Bayesian, computationally efficient framework for\nincorporating model uncertainty into Type II Tobit models and apply this to the\ninvestigation of the determinants of Foreign Direct Investment (FDI). While\ndirect evaluation of modelprobabilities is intractable in this setting, we show\nthat by using conditional Bayes Factors, which nest model moves inside a Gibbs\nsampler, we are able to incorporate model uncertainty in a straight-forward\nfashion. We conclude with a study of global FDI flows between 1988-2000.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2012 12:40:45 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Jordan", "Alexander", ""], ["Lenkoski", "Alex", ""]]}, {"id": "1205.2777", "submitter": "Antonino Abbruzzo AA", "authors": "Antonino Abbruzzo and Ernst Wit", "title": "Modelling slowly changing dynamic gene-regulatory networks", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic gene-regulatory networks are complex since the number of potential\ncomponents involved in the system is very large. Estimating dynamic networks is\nan important task because they compromise valuable information about\ninteractions among genes. Graphical models are a powerful class of models to\nestimate conditional independence among random variables, e.g. interactions in\ndynamic systems. Indeed, these interactions tend to vary over time. However,\nthe literature has been focused on static networks, which can only reveal\noverall structures. Time-course experiments are performed in order to tease out\nsignificant changes in networks. It is typically reasonable to assume that\nchanges in genomic networks are few because systems in biology tend to be\nstable. We introduce a new model for estimating slowly changes in dynamic\ngene-regulatory networks which is suitable for a high-dimensional dataset, e.g.\ntime-course genomic data. Our method is based on i) the penalized likelihood\nwith $\\ell_1$-norm, ii) the penalized differences between conditional\nindependence elements across time points and iii) the heuristic search strategy\nto find optimal smoothing parameters. We implement a set of linear constraints\nnecessary to estimate sparse graphs and penalized changing in dynamic networks.\nThese constraints are not in the linear form. For this reason, we introduce\nslack variables to re-write our problem into a standard convex optimization\nproblem subject to equality linear constraints. We show that GL$_\\Delta$\nperforms well in a simulation study. Finally, we apply the proposed model to a\ntime-course genetic dataset T-cell.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2012 12:36:09 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Abbruzzo", "Antonino", ""], ["Wit", "Ernst", ""]]}, {"id": "1205.2911", "submitter": "Antonino Abbruzzo AA", "authors": "E. C. Wit, A. Abbruzzo", "title": "Factorial graphical lasso for dynamic networks", "comments": "30 pp, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic networks models describe a growing number of important scientific\nprocesses, from cell biology and epidemiology to sociology and finance. There\nare many aspects of dynamical networks that require statistical considerations.\nIn this paper we focus on determining network structure. Estimating dynamic\nnetworks is a difficult task since the number of components involved in the\nsystem is very large. As a result, the number of parameters to be estimated is\nbigger than the number of observations. However, a characteristic of many\nnetworks is that they are sparse. For example, the molecular structure of genes\nmake interactions with other components a highly-structured and therefore\nsparse process.\n  Penalized Gaussian graphical models have been used to estimate sparse\nnetworks. However, the literature has focussed on static networks, which lack\nspecific temporal constraints. We propose a structured Gaussian dynamical\ngraphical model, where structures can consist of specific time dynamics, known\npresence or absence of links and block equality constraints on the parameters.\nThus, the number of parameters to be estimated is reduced and accuracy of the\nestimates, including the identification of the network, can be tuned up. Here,\nwe show that the constrained optimization problem can be solved by taking\nadvantage of an efficient solver, logdetPPA, developed in convex optimization.\nMoreover, model selection methods for checking the sensitivity of the inferred\nnetworks are described. Finally, synthetic and real data illustrate the\nproposed methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2012 22:02:28 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Wit", "E. C.", ""], ["Abbruzzo", "A.", ""]]}, {"id": "1205.3217", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Stephen E. Fienberg", "title": "A Generalized Fellegi-Sunter Framework for Multiple Record Linkage With\n  Application to Homicide Record Systems", "comments": "Several changes with respect to previous version. Accepted in the\n  Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic method for linking multiple datafiles. This task\nis not trivial in the absence of unique identifiers for the individuals\nrecorded. This is a common scenario when linking census data to coverage\nmeasurement surveys for census coverage evaluation, and in general when\nmultiple record-systems need to be integrated for posterior analysis. Our\nmethod generalizes the Fellegi-Sunter theory for linking records from two\ndatafiles and its modern implementations. The multiple record linkage goal is\nto classify the record K-tuples coming from K datafiles according to the\ndifferent matching patterns. Our method incorporates the transitivity of\nagreement in the computation of the data used to model matching probabilities.\nWe use a mixture model to fit matching probabilities via maximum likelihood\nusing the EM algorithm. We present a method to decide the record K-tuples\nmembership to the subsets of matching patterns and we prove its optimality. We\napply our method to the integration of three Colombian homicide record systems\nand we perform a simulation study in order to explore the performance of the\nmethod under measurement error and different scenarios. The proposed method\nworks well and opens some directions for future research.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2012 23:01:28 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2013 18:58:00 GMT"}], "update_date": "2013-02-07", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1205.3641", "submitter": "Duncan Lee", "authors": "Duncan Lee and Richard Mitchell", "title": "Locally adaptive spatial smoothing using conditional autoregressive\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional autoregressive (CAR) models are commonly used to capture spatial\ncorrelation in areal unit data, and are typically specified as a prior\ndistribution for a set of random effects, as part of a hierarchical Bayesian\nmodel. The spatial correlation structure induced by these models is determined\nby geographical adjacency, so that two areas have correlated random effects if\nthey share a common border. However, this correlation structure is too\nsimplistic for real data, which are instead likely to include sub-regions of\nstrong correlation as well as locations at which the response exhibits a\nstep-change. Therefore this paper proposes an extension to CAR priors, which\ncan capture such localised spatial correlation. The proposed approach takes the\nform of an iterative algorithm, which sequentially updates the spatial\ncorrelation structure in the data as well as estimating the remaining model\nparameters. The efficacy of the approach is assessed by simulation, and its\nutility is illustrated in a disease mapping context, using data on respiratory\ndisease risk in Greater Glasgow, Scotland.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2012 11:52:37 GMT"}], "update_date": "2012-05-17", "authors_parsed": [["Lee", "Duncan", ""], ["Mitchell", "Richard", ""]]}, {"id": "1205.4097", "submitter": "Van Hanh Nguyen", "authors": "Van Hanh Nguyen (SG, LM-Orsay), Catherine Matias (SG)", "title": "On efficient estimators of the proportion of true null hypotheses in a\n  multiple testing setup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the proportion $\\theta$ of true null\nhypotheses in a multiple testing context. The setup is classically modeled\nthrough a semiparametric mixture with two components: a uniform distribution on\ninterval $[0,1]$ with prior probability $\\theta$ and a nonparametric density\n$f$. We discuss asymptotic efficiency results and establish that two different\ncases occur whether $f$ vanishes on a set with non null Lebesgue measure or\nnot. In the first case, we exhibit estimators converging at parametric rate,\ncompute the optimal asymptotic variance and conjecture that no estimator is\nasymptotically efficient (i.e. attains the optimal asymptotic variance). In the\nsecond case, we prove that the quadratic risk of any estimator does not\nconverge at parametric rate. We illustrate those results on simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2012 07:02:06 GMT"}, {"version": "v2", "created": "Tue, 8 Jan 2013 17:49:54 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Nguyen", "Van Hanh", "", "SG, LM-Orsay"], ["Matias", "Catherine", "", "SG"]]}, {"id": "1205.4345", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi", "title": "Involving copula functions in Conditional Tail Expectation", "comments": "Submitted Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.RM stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Our goal in this paper is to propose an alternative risk measure which takes\ninto account the fluctuations of losses and possible correlations between\nrandom variables. This new notion of risk measures, that we call Copula\nConditional Tail Expectation describes the expected amount of risk that can be\nexperienced given that a potential bivariate risk exceeds a bivariate threshold\nvalue, and provides an important measure for right-tail risk. An application to\nreal financial data is given.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2012 16:18:33 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2013 14:40:35 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2013 20:34:51 GMT"}, {"version": "v4", "created": "Wed, 2 Apr 2014 15:07:30 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Brahimi", "Brahim", ""]]}, {"id": "1205.4418", "submitter": "Alberto Baccini", "authors": "Alberto Baccini, Lucio Barabesi, Marzia Marcheselli, Luca Pratelli", "title": "Statistical inference on the h-index with an application to\n  top-scientist performance", "comments": "14 pages, 3 tables", "journal-ref": "Journal of Informetrics, Volume 6, Issue 4, October 2012, Pages\n  721 - 728", "doi": "10.1016/j.joi.2012.07.009", "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the huge amount of literature on h-index, few papers have been\ndevoted to the statistical analysis of h-index when a probabilistic\ndistribution is assumed for citation counts. The present contribution relies on\nshowing the available inferential techniques, by providing the details for\nproper point and set estimation of the theoretical h-index. Moreover, some\nissues on simultaneous inference - aimed to produce suitable scholar\ncomparisons - are carried out. Finally, the analysis of the citation dataset\nfor the Nobel Laureates (in the last five years) and for the Fields medallists\n(from 2002 onward) is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2012 13:30:26 GMT"}], "update_date": "2014-07-29", "authors_parsed": [["Baccini", "Alberto", ""], ["Barabesi", "Lucio", ""], ["Marcheselli", "Marzia", ""], ["Pratelli", "Luca", ""]]}, {"id": "1205.4476", "submitter": "Deniz Akdemir", "authors": "Deniz Akdemir and Nicolas Heslot", "title": "Soft Rule Ensembles for Statistical Learning", "comments": "arXiv admin note: text overlap with arXiv:1112.3699", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article supervised learning problems are solved using soft rule\nensembles. We first review the importance sampling learning ensembles (ISLE)\napproach that is useful for generating hard rules. The soft rules are then\nobtained with logistic regression from the corresponding hard rules. In order\nto deal with the perfect separation problem related to the logistic regression,\nFirth's bias corrected likelihood is used. Various examples and simulation\nresults show that soft rule ensembles can improve predictive performance over\nhard rule ensembles.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 01:46:04 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2012 16:14:45 GMT"}, {"version": "v3", "created": "Fri, 22 Feb 2013 17:03:20 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Akdemir", "Deniz", ""], ["Heslot", "Nicolas", ""]]}, {"id": "1205.4750", "submitter": "Kevin Dayaratna", "authors": "Kevin D. Dayaratna and Steven J. Miller", "title": "First Order Approximations of the Pythagorean Won-Loss Formula for\n  Predicting MLB Teams' Winning Percentages", "comments": "7 pages, 1 Table, Appendix with Alternative Proof; By the Numbers 21,\n  2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We mathematically prove that an existing linear predictor of baseball teams'\nwinning percentages (Jones and Tappin 2005) is simply just a first-order\napproximation to Bill James' Pythagorean Won-Loss formula and can thus be\nwritten in terms of the formula's well-known exponent. We estimate the linear\nmodel on twenty seasons of Major League Baseball data and are able to verify\nthat the resulting coefficient estimate, with 95% confidence, is virtually\nidentical to the empirically accepted value of 1.82. Our work thus helps\nexplain why this simple and elegant model is such a strong linear predictor.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2012 21:04:36 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Dayaratna", "Kevin D.", ""], ["Miller", "Steven J.", ""]]}, {"id": "1205.4840", "submitter": "Beno\\^ite de Saporta", "authors": "Beno\\^ite de Saporta, Anne G\\'egout Petit, Laurence Marsalle", "title": "Statistical study of asymmetry in cell lineage data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rigorous methodology is proposed to study cell division data consisting in\nseveral observed genealogical trees of possibly different shapes. The procedure\ntakes into account missing observations, data from different trees, as well as\nthe dependence structure within genealogical trees. Its main new feature is the\njoint use of all available information from several data sets instead of single\ndata set estimation, to avoid the drawbacks of low accuracy for estimators or\nlow power for tests on small single-trees. The data is modeled by an asymmetric\nbifurcating autoregressive process and possibly missing observations are taken\ninto account by modeling the genealogies with a two-type Galton-Watson process.\nLeast-squares estimators of the unknown parameters of the processes are given\nand symmetry tests are derived. Results are applied on real data of Escherichia\ncoli division and an empirical study of the convergence rates of the estimators\nand power of the tests is conducted on simulated data.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 08:44:59 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2013 14:06:02 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2013 08:45:16 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["de Saporta", "Beno\u00eete", ""], ["Petit", "Anne G\u00e9gout", ""], ["Marsalle", "Laurence", ""]]}, {"id": "1205.4955", "submitter": "Ajay Jasra", "authors": "Alberto Cozzini, Ajay Jasra and Giovanni Montana", "title": "A Bayesian Mixture of Lasso Regressions with t-Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by a challenging problem in financial trading we are presented with\na mixture of regressions with variable selection problem. In this regard, one\nis faced with data which possess outliers, skewness and, simultaneously, due to\nthe nature of financial trading, one would like to be able to construct\nclusters with specific predictors that are fairly sparse. We develop a Bayesian\nmixture of lasso regressions with $t-$errors to reflect these specific demands.\nThe resulting model is necessarily complex and to fit the model to real data,\nwe develop a state-of-the-art Particle Markov chain Monte Carlo (PMCMC)\nalgorithm based upon sequential Monte Carlo (SMC) methods. The model and\nalgorithm are investigated on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2012 15:59:54 GMT"}], "update_date": "2012-05-23", "authors_parsed": [["Cozzini", "Alberto", ""], ["Jasra", "Ajay", ""], ["Montana", "Giovanni", ""]]}, {"id": "1205.5082", "submitter": "Dominic Lee", "authors": "Dominic S. Lee and Carey E. Priebe", "title": "Bayesian Vertex Nomination", "comments": "25 pages, 10 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an attributed graph whose vertices are colored green or red, but\nonly a few are observed to be red. The color of the other vertices is\nunobserved. Typically, the unknown total number of red vertices is small. The\nvertex nomination problem is to nominate one of the unobserved vertices as\nbeing red. The edge set of the graph is a subset of the set of unordered pairs\nof vertices. Suppose that each edge is also colored green or red and this is\nobserved for all edges. The context statistic of a vertex is defined as the\nnumber of observed red vertices connected to it, and its content statistic is\nthe number of red edges incident to it. Assuming that these statistics are\nindependent between vertices and that red edges are more likely between red\nvertices, Coppersmith and Priebe (2012) proposed a likelihood model based on\nthese statistics. Here, we formulate a Bayesian model using the proposed\nlikelihood together with prior distributions chosen for the unknown parameters\nand unobserved vertex colors. From the resulting posterior distribution, the\nnominated vertex is the one with the highest posterior probability of being\nred. Inference is conducted using a Metropolis-within-Gibbs algorithm, and\nperformance is illustrated by a simulation study. Results show that (i) the\nBayesian model performs significantly better than chance; (ii) the probability\nof correct nomination increases with increasing posterior probability that the\nnominated vertex is red; and (iii) the Bayesian model either matches or\nperforms better than the method in Coppersmith and Priebe. An application\nexample is provided using the Enron email corpus, where vertices represent\nEnron employees and their associates, observed red vertices are known\nfraudsters, red edges represent email communications perceived as fraudulent,\nand we wish to identify one of the latent vertices as most likely to be a\nfraudster.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2012 01:19:37 GMT"}], "update_date": "2012-05-24", "authors_parsed": [["Lee", "Dominic S.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1205.5560", "submitter": "Francisco Kitaura", "authors": "Francisco-Shu Kitaura, Pirin Erdogdu, Sebastian E. Nuza, Arman\n  Khalatyan, Raul E. Angulo, Yehuda Hoffman and Stefan Gottloeber", "title": "Cosmic Structure and Dynamics of the Local Universe", "comments": "6 pages, 5 figures; accepted at MNRAS after minor corrections", "journal-ref": "2012MNRAS.tmpL.528K", "doi": "10.1111/j.1745-3933.2012.01340.x", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a cosmography analysis of the Local Universe based on the recently\nreleased Two-Micron All-Sky Redshift Survey (2MRS). Our method is based on a\nBayesian Networks Machine Learning algorithm (the Kigen-code) which\nself-consistently samples the initial density fluctuations compatible with the\nobserved galaxy distribution and a structure formation model given by second\norder Lagrangian perturbation theory (2LPT). From the initial conditions we\nobtain an ensemble of reconstructed density and peculiar velocity fields which\ncharacterize the local cosmic structure with high accuracy unveiling nonlinear\nstructures like filaments and voids in detail. Coherent redshift space\ndistortions are consistently corrected within 2LPT. From the ensemble of\ncross-correlations between the reconstructions and the galaxy field and the\nvariance of the recovered density fields we find that our method is extremely\naccurate up to k ~ 1 h Mpc^-1 and still yields reliable results down to scales\nof about 3-4 h^-1 Mpc. The motion of the local group we obtain within ~ 80 h^-1\nMpc (v_LG=522+-86 km s^-1, l_LG=291^o +- 16^o, b_LG=34^o+-8^o) is in good\nagreement with measurements derived from the CMB and from direct observations\nof peculiar motions and is consistent with the predictions of LambdaCDM.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2012 20:26:31 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 14:29:46 GMT"}], "update_date": "2012-11-08", "authors_parsed": [["Kitaura", "Francisco-Shu", ""], ["Erdogdu", "Pirin", ""], ["Nuza", "Sebastian E.", ""], ["Khalatyan", "Arman", ""], ["Angulo", "Raul E.", ""], ["Hoffman", "Yehuda", ""], ["Gottloeber", "Stefan", ""]]}, {"id": "1205.5651", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, \\'Alvaro Corral, Mari\\'an Bogu\\~n\\'a, Mart\\'in Haro,\n  Josep Lluis Arcos", "title": "Measuring the evolution of contemporary western popular music", "comments": "Supplementary materials not included. Please see the journal\n  reference or contact the authors", "journal-ref": "Scientific Reports 2, 521 (2012)", "doi": "10.1038/srep00521", "report-no": null, "categories": "cs.SD cs.IR cs.MM physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Popular music is a key cultural expression that has captured listeners'\nattention for ages. Many of the structural regularities underlying musical\ndiscourse are yet to be discovered and, accordingly, their historical evolution\nremains formally unknown. Here we unveil a number of patterns and metrics\ncharacterizing the generic usage of primary musical facets such as pitch,\ntimbre, and loudness in contemporary western popular music. Many of these\npatterns and metrics have been consistently stable for a period of more than\nfifty years, thus pointing towards a great degree of conventionalism.\nNonetheless, we prove important changes or trends related to the restriction of\npitch transitions, the homogenization of the timbral palette, and the growing\nloudness levels. This suggests that our perception of the new would be rooted\non these changing characteristics. Hence, an old tune could perfectly sound\nnovel and fashionable, provided that it consisted of common harmonic\nprogressions, changed the instrumentation, and increased the average loudness.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2012 09:54:24 GMT"}], "update_date": "2016-05-18", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Corral", "\u00c1lvaro", ""], ["Bogu\u00f1\u00e1", "Mari\u00e1n", ""], ["Haro", "Mart\u00edn", ""], ["Arcos", "Josep Lluis", ""]]}, {"id": "1205.5906", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, George V. Moustakides, Xiaodong Wang", "title": "Channel-aware Decentralized Detection via Level-triggered Sampling", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2222401", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider decentralized detection through distributed sensors that perform\nlevel-triggered sampling and communicate with a fusion center via noisy\nchannels. Each sensor computes its local log-likelihood ratio (LLR), samples it\nusing the level-triggered sampling, and upon sampling transmits a single bit to\nthe FC. Upon receiving a bit from a sensor, the FC updates the global LLR and\nperforms a sequential probability ratio test (SPRT) step. We derive the fusion\nrules under various types of channels. We further provide an asymptotic\nanalysis on the average detection delay for the proposed channel-aware scheme,\nand show that the asymptotic detection delay is characterized by a KL\ninformation number. The delay analysis facilitates the choice of appropriate\nsignaling schemes under different channel types for sending the 1-bit\ninformation from sensors to the FC.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2012 18:32:56 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2012 03:44:31 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Moustakides", "George V.", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1205.6158", "submitter": "Cedric Ginestet", "authors": "Arnaud P. Fournel, Emanuelle Reynaud, Michael J. Brammer, Andrew\n  Simmons and Cedric E. Ginestet", "title": "Group Analysis of Self-organizing Maps based on Functional MRI using\n  Restricted Frechet Means", "comments": "23 pages, 5 figures, 4 tables. Submitted to Neuroimage", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Studies of functional MRI data are increasingly concerned with the estimation\nof differences in spatio-temporal networks across groups of subjects or\nexperimental conditions. Unsupervised clustering and independent component\nanalysis (ICA) have been used to identify such spatio-temporal networks. While\nthese approaches have been useful for estimating these networks at the\nsubject-level, comparisons over groups or experimental conditions require\nfurther methodological development. In this paper, we tackle this problem by\nshowing how self-organizing maps (SOMs) can be compared within a Frechean\ninferential framework. Here, we summarize the mean SOM in each group as a\nFrechet mean with respect to a metric on the space of SOMs. We consider the use\nof different metrics, and introduce two extensions of the classical sum of\nminimum distance (SMD) between two SOMs, which take into account the\nspatio-temporal pattern of the fMRI data. The validity of these methods is\nillustrated on synthetic data. Through these simulations, we show that the\nthree metrics of interest behave as expected, in the sense that the ones\ncapturing temporal, spatial and spatio-temporal aspects of the SOMs are more\nlikely to reach significance under simulated scenarios characterized by\ntemporal, spatial and spatio-temporal differences, respectively. In addition, a\nre-analysis of a classical experiment on visually-triggered emotions\ndemonstrates the usefulness of this methodology. In this study, the\nmultivariate functional patterns typical of the subjects exposed to pleasant\nand unpleasant stimuli are found to be more similar than the ones of the\nsubjects exposed to emotionally neutral stimuli. Taken together, these results\nindicate that our proposed methods can cast new light on existing data by\nadopting a global analytical perspective on functional MRI paradigms.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2012 16:53:38 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2012 11:00:46 GMT"}], "update_date": "2012-08-14", "authors_parsed": [["Fournel", "Arnaud P.", ""], ["Reynaud", "Emanuelle", ""], ["Brammer", "Michael J.", ""], ["Simmons", "Andrew", ""], ["Ginestet", "Cedric E.", ""]]}, {"id": "1205.6310", "submitter": "Alberto Sorrentino", "authors": "Alberto Sorrentino, Adam M. Johansen, John A. D. Aston, Thomas E.\n  Nichols, Wilfrid S. Kendall", "title": "Dynamic filtering of static dipoles in magnetoencephalography", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS611 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2013, Vol. 7, No. 2, 955-988", "doi": "10.1214/12-AOAS611", "report-no": "IMS-AOAS-AOAS611", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating neural activity from measurements of\nthe magnetic fields recorded by magnetoencephalography. We exploit the temporal\nstructure of the problem and model the neural current as a collection of\nevolving current dipoles, which appear and disappear, but whose locations are\nconstant throughout their lifetime. This fully reflects the physiological\ninterpretation of the model. In order to conduct inference under this proposed\nmodel, it was necessary to develop an algorithm based around state-of-the-art\nsequential Monte Carlo methods employing carefully designed importance\ndistributions. Previous work employed a bootstrap filter and an artificial\ndynamic structure where dipoles performed a random walk in space, yielding\nnonphysical artefacts in the reconstructions; such artefacts are not observed\nwhen using the proposed model. The algorithm is validated with simulated data,\nin which it provided an average localisation error which is approximately half\nthat of the bootstrap filter. An application to complex real data derived from\na somatosensory experiment is presented. Assessment of model fit via marginal\nlikelihood showed a clear preference for the proposed model and the associated\nreconstructions show better localisation.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 09:29:51 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2012 12:20:42 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2013 12:01:18 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Sorrentino", "Alberto", ""], ["Johansen", "Adam M.", ""], ["Aston", "John A. D.", ""], ["Nichols", "Thomas E.", ""], ["Kendall", "Wilfrid S.", ""]]}, {"id": "1205.6339", "submitter": "Lionel Barnett", "authors": "Lionel Barnett and Terry Bossomaier", "title": "Transfer Entropy as a Log-likelihood Ratio", "comments": null, "journal-ref": null, "doi": "10.1103/PhysRevLett.109.138105", "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy, an information-theoretic measure of time-directed\ninformation transfer between joint processes, has steadily gained popularity in\nthe analysis of complex stochastic dynamics in diverse fields, including the\nneurosciences, ecology, climatology and econometrics. We show that for a broad\nclass of predictive models, the log-likelihood ratio test statistic for the\nnull hypothesis of zero transfer entropy is a consistent estimator for the\ntransfer entropy itself. For finite Markov chains, furthermore, no explicit\nmodel is required. In the general case, an asymptotic chi-squared distribution\nis established for the transfer entropy estimator. The result generalises the\nequivalence in the Gaussian case of transfer entropy and Granger causality, a\nstatistical notion of causal influence based on prediction via vector\nautoregression, and establishes a fundamental connection between directed\ninformation transfer and causality in the Wiener-Granger sense.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 11:58:15 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2012 15:39:57 GMT"}, {"version": "v3", "created": "Fri, 27 Jul 2012 15:37:40 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Barnett", "Lionel", ""], ["Bossomaier", "Terry", ""]]}, {"id": "1205.6439", "submitter": "Kevin Dayaratna", "authors": "Kevin D. Dayaratna and P. K. Kannan", "title": "A Mathematical Reformulation of the Reference Price", "comments": "22 pages, 2 tables. Forthcoming in Marketing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reference prices have long been studied in applied economics and business\nresearch. One of the classic formulations of the reference price is in terms of\nan iterative function of past prices. There are a number of limitations of such\na formulation, however. Such limitations include burdensome computational time\nto estimate parameters, an inability to truly account for customer\nheterogeneity, and an estimation procedure that implies a misspecified model.\nManagerial recommendations based on inferences from such a model can be quite\nmisleading. We mathematically reformulate the reference price by developing a\nclosed-form expansion that addresses the aforementioned issues, enabling one to\nelicit truly meaningful managerial advice from the model. We estimate our model\non a real world data set to illustrate the efficacy of our approach. Our work\nis not only useful from a modeling perspective, but also has important\nbehavioral and managerial implications, which modelers and non-modelers alike\nwould find useful.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2012 18:05:14 GMT"}], "update_date": "2012-05-30", "authors_parsed": [["Dayaratna", "Kevin D.", ""], ["Kannan", "P. K.", ""]]}, {"id": "1205.6741", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Sequential Cross-Validated Bandwidth Selection Under Dependence and\n  Anscombe-Type Extensions to Random Time Horizons", "comments": null, "journal-ref": "Sequential Analysis, Volume 31, 2012 - Issue 3, 326-350", "doi": "10.1080/07474946.2012.694347", "report-no": null, "categories": "math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To detect changes in the mean of a time series, one may use previsible\ndetection procedures based on nonparametric kernel prediction smoothers which\ncover various classic detection statistics as special cases. Bandwidth\nselection, particularly in a data-adaptive way, is a serious issue and not well\nstudied for detection problems. To ensure data adaptation, we select the\nbandwidth by cross-validation, but in a sequential way leading to a functional\nestimation approach. This article provides the asymptotic theory for the method\nunder fairly weak assumptions on the dependence structure of the error terms,\nwhich cover, e.g., GARCH($p,q$) processes, by establishing (sequential)\nfunctional central limit theorems for the cross-validation objective function\nand the associated bandwidth selector. It turns out that the proof can be based\nin a neat way on \\cite{KurtzProtter1996}'s results on the weak convergence of\n\\ito integrals and a diagonal argument.\n  Our gradual change-point model covers multiple change-points in that it\nallows for a nonlinear regression function after the first change-point\npossibly with further jumps and Lipschitz continuous between those\ndiscontinuities.\n  In applications, the time horizon where monitoring stops latest is often\ndetermined by a random experiment, e.g. a first-exit stopping time applied to a\ncumulated cost process or a risk measure, possibly stochastically dependent\nfrom the monitored time series. Thus, we also study that case and establish\nrelated limit theorems in the spirit of \\citet{Anscombe1952}'s result. The\nresult has various applications including statistical parameter estimation and\nmonitoring financial investment strategies with risk-controlled early\ntermination, which are briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 16:11:20 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1205.6986", "submitter": "Barbara Rakitsch", "authors": "Barbara Rakitsch, Christoph Lippert, Oliver Stegle, Karsten Borgwardt", "title": "LMM-Lasso: A Lasso Multi-Marker Mixed Model for Association Mapping with\n  Population Structure Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploring the genetic basis of heritable traits remains one of the central\nchallenges in biomedical research. In simple cases, single polymorphic loci\nexplain a significant fraction of the phenotype variability. However, many\ntraits of interest appear to be subject to multifactorial control by groups of\ngenetic loci instead. Accurate detection of such multivariate associations is\nnontrivial and often hindered by limited power. At the same time, confounding\ninfluences such as population structure cause spurious association signals that\nresult in false positive findings if they are not accounted for in the model.\nHere, we propose LMM-Lasso, a mixed model that allows for both, multi-locus\nmapping and correction for confounding effects. Our approach is simple and free\nof tuning parameters, effectively controls for population structure and scales\nto genome-wide datasets. We show practical use in genome-wide association\nstudies and linkage mapping through retrospective analyses. In data from\nArabidopsis thaliana and mouse, our method is able to find a genetic cause for\nsignificantly greater fractions of phenotype variation in 91% of the phenotypes\nconsidered. At the same time, our model dissects this variability into\ncomponents that result from individual SNP effects and population structure. In\naddition to this increase of genetic heritability, enrichment of known\ncandidate genes suggests that the associations retrieved by LMM-Lasso are more\nlikely to be genuine.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2012 05:43:35 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2012 12:52:36 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Rakitsch", "Barbara", ""], ["Lippert", "Christoph", ""], ["Stegle", "Oliver", ""], ["Borgwardt", "Karsten", ""]]}]