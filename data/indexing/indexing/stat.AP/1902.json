[{"id": "1902.00203", "submitter": "Robert R. Junker", "authors": "Robert R. Junker, Florian Griessenberger, Wolfgang Trutschnig", "title": "A copula-based measure for quantifying asymmetry in dependence and\n  associations", "comments": "9 pages. 4 figures, 4 supporting information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymmetry is an inherent property of bivariate associations and therefore\nmust not be ignored. The currently applicable dependence measures mask the\npotential asymmetry of the underlying dependence structure by implicitly\nassuming that quantity Y is equally dependent on quantity X, and vice versa,\nwhich is generally not true. We introduce the copula-based dependence measure\nqad that quantifies asymmetry. Specifically, qad is applicable in general\nsituations, is sensitive to noise in data, detects asymmetry in dependence and\nreliably quantifies the information gain/predictability of quantity Y given\nknowledge of quantity X, and vice versa. Using real-world data sets, we\ndemonstrate the relevance of asymmetry in associations. Asymmetry in dependence\nis a novel category of information that provides substantial information gain\nin analyses of bivariate associations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 06:56:39 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 08:51:33 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Junker", "Robert R.", ""], ["Griessenberger", "Florian", ""], ["Trutschnig", "Wolfgang", ""]]}, {"id": "1902.00382", "submitter": "Morteza Taiebat", "authors": "Morteza Taiebat, Samuel Stolper, Ming Xu", "title": "Forecasting the Impact of Connected and Automated Vehicles on Energy Use\n  A Microeconomic Study of Induced Travel and Energy Rebound", "comments": null, "journal-ref": "Applied Energy, 2019, 247, 297-308", "doi": "10.1016/j.apenergy.2019.03.174", "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected and automated vehicles (CAVs) are expected to yield significant\nimprovements in safety, energy efficiency, and time utilization. However, their\nnet effect on energy and environmental outcomes is unclear. Higher fuel economy\nreduces the energy required per mile of travel, but it also reduces the fuel\ncost of travel, incentivizing more travel and causing an energy \"rebound\neffect.\" Moreover, CAVs are predicted to vastly reduce the time cost of travel,\ninducing further increases in travel and energy use. In this paper, we forecast\nthe induced travel and rebound from CAVs using data on existing travel\nbehavior. We develop a microeconomic model of vehicle miles traveled (VMT)\nchoice under income and time constraints; then we use it to estimate\nelasticities of VMT demand with respect to fuel and time costs, with fuel cost\ndata from the 2017 United States National Household Travel Survey (NHTS) and\nwage-derived predictions of travel time cost. Our central estimate of the\ncombined price elasticity of VMT demand is -0.4, which differs substantially\nfrom previous estimates. We also find evidence that wealthier households have\nmore elastic demand, and that households at all income levels are more\nsensitive to time costs than to fuel costs. We use our estimated elasticities\nto simulate VMT and energy use impacts of full, private CAV adoption under a\nrange of possible changes to the fuel and time costs of travel. We forecast a\n2-47% increase in travel demand for an average household. Our results indicate\nthat backfire - i.e., a net rise in energy use - is a possibility, especially\nin higher income groups. This presents a stiff challenge to policy goals for\nreductions in not only energy use but also traffic congestion and local and\nglobal air pollution, as CAV use increases.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 17:19:33 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 23:55:58 GMT"}, {"version": "v3", "created": "Thu, 18 Apr 2019 18:47:24 GMT"}, {"version": "v4", "created": "Thu, 9 May 2019 20:16:04 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Taiebat", "Morteza", ""], ["Stolper", "Samuel", ""], ["Xu", "Ming", ""]]}, {"id": "1902.00450", "submitter": "Ioana Bica", "authors": "Ioana Bica, Ahmed M. Alaa, Mihaela van der Schaar", "title": "Time Series Deconfounder: Estimating Treatment Effects over Time in the\n  Presence of Hidden Confounders", "comments": null, "journal-ref": "In Proc. 37th International Conference on Machine Learning (ICML\n  2020)", "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of treatment effects is a pervasive problem in medicine.\nExisting methods for estimating treatment effects from longitudinal\nobservational data assume that there are no hidden confounders, an assumption\nthat is not testable in practice and, if it does not hold, leads to biased\nestimates. In this paper, we develop the Time Series Deconfounder, a method\nthat leverages the assignment of multiple treatments over time to enable the\nestimation of treatment effects in the presence of multi-cause hidden\nconfounders. The Time Series Deconfounder uses a novel recurrent neural network\narchitecture with multitask output to build a factor model over time and infer\nlatent variables that render the assigned treatments conditionally independent;\nthen, it performs causal inference using these latent variables that act as\nsubstitutes for the multi-cause unobserved confounders. We provide a\ntheoretical analysis for obtaining unbiased causal effects of time-varying\nexposures using the Time Series Deconfounder. Using both simulated and real\ndata we show the effectiveness of our method in deconfounding the estimation of\ntreatment responses over time.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 16:49:51 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 17:43:22 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 20:42:06 GMT"}, {"version": "v4", "created": "Fri, 18 Sep 2020 12:43:27 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Bica", "Ioana", ""], ["Alaa", "Ahmed M.", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1902.00490", "submitter": "Artem Lutov", "authors": "Artem Lutov, Soheil Roshankish, Mourad Khayati and Philippe\n  Cudr\\'e-Mauroux", "title": "StaTIX - Statistical Type Inference on Linked Data", "comments": "Application sources and executables:\n  https://github.com/eXascaleInfolab/StaTIX", "journal-ref": "2018 IEEE International Conference on Big Data", "doi": "10.1109/BigData.2018.8622285", "report-no": null, "categories": "stat.AP cs.DS cs.SI physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large knowledge bases typically contain data adhering to various schemas with\nincomplete and/or noisy type information. This seriously complicates further\nintegration and post-processing efforts, as type information is crucial in\ncorrectly handling the data. In this paper, we introduce a novel statistical\ntype inference method, called StaTIX, to effectively infer instance types in\nLinked Data sets in a fully unsupervised manner. Our inference technique\nleverages a new hierarchical clustering algorithm that is robust, highly\neffective, and scalable. We introduce a novel approach to reduce the processing\ncomplexity of the similarity matrix specifying the relations between various\ninstances in the knowledge base. This approach speeds up the inference process\nwhile also improving the correctness of the inferred types due to the noise\nattenuation in the input data. We further optimize the clustering process by\nintroducing a dedicated hash function that speeds up the inference process by\norders of magnitude without negatively affecting its accuracy. Finally, we\ndescribe a new technique to identify representative clusters from the\nmulti-scale output of our clustering algorithm to further improve the accuracy\nof the inferred types. We empirically evaluate our approach on several\nreal-world datasets and compare it to the state of the art. Our results show\nthat StaTIX is more efficient than existing methods (both in terms of speed and\nmemory consumption) as well as more effective. StaTIX reduces the F1-score\nerror of the predicted types by about 40% on average compared to the state of\nthe art and improves the execution time by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:25:08 GMT"}, {"version": "v2", "created": "Sat, 16 Feb 2019 09:48:23 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Lutov", "Artem", ""], ["Roshankish", "Soheil", ""], ["Khayati", "Mourad", ""], ["Cudr\u00e9-Mauroux", "Philippe", ""]]}, {"id": "1902.00509", "submitter": "Letizia Angeli", "authors": "Letizia Angeli, Stefan Grosskinsky, Adam M. Johansen", "title": "Limit theorems for cloning algorithms", "comments": "42 pages", "journal-ref": "Stoch. Proc. Appl. 138, 117-152 (2021)", "doi": "10.1016/j.spa.2021.04.007", "report-no": null, "categories": "math.PR cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large deviations for additive path functionals of stochastic processes have\nattracted significant research interest, in particular in the context of\nstochastic particle systems and statistical physics. Efficient numerical\n`cloning' algorithms have been developed to estimate the scaled cumulant\ngenerating function, based on importance sampling via cloning of rare event\ntrajectories. So far, attempts to study the convergence properties of these\nalgorithms in continuous time have only led to partial results for particular\ncases. Adapting previous results from the literature of particle filters and\nsequential Monte Carlo methods, we establish a first comprehensive and fully\nrigorous approach to bound systematic and random errors of cloning algorithms\nin continuous time. To this end we develop a method to compare different\nalgorithms for particular classes of observables, based on the martingale\ncharacterization of stochastic processes. Our results apply to a large class of\njump processes on compact state space, and do not involve any time\ndiscretization in contrast to previous approaches. This provides a robust and\nrigorous framework that can also be used to evaluate and improve the efficiency\nof algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 18:59:40 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 15:14:13 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 13:00:47 GMT"}, {"version": "v4", "created": "Wed, 14 Apr 2021 08:37:23 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Angeli", "Letizia", ""], ["Grosskinsky", "Stefan", ""], ["Johansen", "Adam M.", ""]]}, {"id": "1902.00563", "submitter": "T\\'arik S. Salem", "authors": "T\\'arik S. Salem, Karan Kathuria, Heri Ramampiaro, Helge Langseth", "title": "Forecasting Intra-Hour Imbalances in Electric Power Systems", "comments": "Accepted at the Thirty-First Annual Conference on Innovative\n  Applications of Artificial Intelligence (IAAI-19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keeping the electricity production in balance with the actual demand is\nbecoming a difficult and expensive task in spite of an involvement of\nexperienced human operators. This is due to the increasing complexity of the\nelectric power grid system with the intermittent renewable production as one of\nthe contributors. A beforehand information about an occurring imbalance can\nhelp the transmission system operator to adjust the production plans, and thus\nensure a high security of supply by reducing the use of costly balancing\nreserves, and consequently reduce undesirable fluctuations of the 50 Hz power\nsystem frequency. In this paper, we introduce the relatively new problem of an\nintra-hour imbalance forecasting for the transmission system operator (TSO). We\nfocus on the use case of the Norwegian TSO, Statnett. We present a\ncomplementary imbalance forecasting tool that is able to support the TSO in\ndetermining the trend of future imbalances, and show the potential to\nproactively alleviate imbalances with a higher accuracy compared to the\ncontemporary solution.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 20:58:51 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Salem", "T\u00e1rik S.", ""], ["Kathuria", "Karan", ""], ["Ramampiaro", "Heri", ""], ["Langseth", "Helge", ""]]}, {"id": "1902.00567", "submitter": "Zekun Xu", "authors": "Zekun Xu, Deovrat Kakde, Arin Chaudhuri", "title": "Automatic Hyperparameter Tuning Method for Local Outlier Factor, with\n  Applications to Anomaly Detection", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": "10.1109/BigData47090.2019.9006151", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been many practical applications of anomaly\ndetection such as in predictive maintenance, detection of credit fraud, network\nintrusion, and system failure. The goal of anomaly detection is to identify in\nthe test data anomalous behaviors that are either rare or unseen in the\ntraining data. This is a common goal in predictive maintenance, which aims to\nforecast the imminent faults of an appliance given abundant samples of normal\nbehaviors. Local outlier factor (LOF) is one of the state-of-the-art models\nused for anomaly detection, but the predictive performance of LOF depends\ngreatly on the selection of hyperparameters. In this paper, we propose a novel,\nheuristic methodology to tune the hyperparameters in LOF. A tuned LOF model\nthat uses the proposed method shows good predictive performance in both\nsimulations and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 21:32:21 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Xu", "Zekun", ""], ["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1902.00770", "submitter": "S. Stanley Young", "authors": "S. Stanley Young, Mithun Kumar Acharjee, Kumer Das", "title": "The reliability of an environmental epidemiology meta-analysis, a case\n  study", "comments": "19 pages, 3 tables, 3 figures", "journal-ref": "Regulatory Toxicology and Pharmacology, 2019", "doi": "10.1016/j.yrtph.2018.12.013", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary\n  Background Claims made in science papers are coming under increased scrutiny\nwith many claims failing to replicate. Meta-analysis studies that use\nunreliable observational studies should be in question. We examine the\nreliability of the base studies used in an air quality/heart attack\nmeta-analysis and the resulting meta-analysis.\n  Methods A meta-analysis study that includes 14 observational air\nquality/heart attack studies is examined for its statistical reliability. We\nuse simple counting to evaluate the reliability of the base papers and a\np-value plot of the p-values from the base studies to examine study\nheterogeneity.\n  Findings We find that the based papers have massive multiple testing and\nmultiple modeling with no statistical adjustments. Statistics coming from the\nbase papers are not guaranteed to be unbiased, a requirement for a valid\nmeta-analysis. There is study heterogeneity for the base papers with strong\nevidence for so called p-hacking.\n  Interpretation We make two observations: there are many claims at issue in\neach of the 14 base studies so uncorrected multiple testing is a serious issue.\nWe find the base papers and the resulting meta-analysis are unreliable.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 19:09:44 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Young", "S. Stanley", ""], ["Acharjee", "Mithun Kumar", ""], ["Das", "Kumer", ""]]}, {"id": "1902.01011", "submitter": "Pritam Ranjan", "authors": "Feng Yang, C. Devon Lin, Pritam Ranjan", "title": "Global Fitting of the Response Surface via Estimating Multiple Contours\n  of a Simulator", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer simulators are nowadays widely used to understand complex physical\nsystems in many areas such as aerospace, renewable energy, climate modeling,\nand manufacturing. One fundamental issue in the study of computer simulators is\nknown as experimental design, that is, how to select the input settings where\nthe computer simulator is run and the corresponding response is collected.\nExtra care should be taken in the selection process because computer simulators\ncan be computationally expensive to run. The selection shall acknowledge and\nachieve the goal of the analysis. This article focuses on the goal of producing\nmore accurate prediction which is important for risk assessment and decision\nmaking. We propose two new methods of design approaches that sequentially\nselect input settings to achieve this goal. The approaches make novel\napplications of simultaneous and sequential contour estimations. Numerical\nexamples are employed to demonstrate the effectiveness of the proposed\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 02:29:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Yang", "Feng", ""], ["Lin", "C. Devon", ""], ["Ranjan", "Pritam", ""]]}, {"id": "1902.01015", "submitter": "Jingyu He", "authors": "Guanhao Feng and Jingyu He", "title": "Factor Investing: A Bayesian Hierarchical Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates asset allocation problems when returns are\npredictable. We introduce a market-timing Bayesian hierarchical (BH) approach\nthat adopts heterogeneous time-varying coefficients driven by lagged\nfundamental characteristics. Our approach includes a joint estimation of\nconditional expected returns and covariance matrix and considers estimation\nrisk for portfolio analysis. The hierarchical prior allows modeling different\nassets separately while sharing information across assets. We demonstrate the\nperformance of the U.S. equity market. Though the Bayesian forecast is slightly\nbiased, our BH approach outperforms most alternative methods in point and\ninterval prediction. Our BH approach in sector investment for the recent twenty\nyears delivers a 0.92\\% average monthly returns and a 0.32\\% significant\nJensen`s alpha. We also find technology, energy, and manufacturing are\nimportant sectors in the past decade, and size, investment, and short-term\nreversal factors are heavily weighted. Finally, the stochastic discount factor\nconstructed by our BH approach explains most anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 02:48:03 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2020 04:26:10 GMT"}, {"version": "v3", "created": "Thu, 17 Sep 2020 06:58:35 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Feng", "Guanhao", ""], ["He", "Jingyu", ""]]}, {"id": "1902.01032", "submitter": "Taewoon Kong", "authors": "Taewoon Kong and Brani Vidakovic", "title": "Non-decimated Complex Wavelet Spectral Tools with Applications", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose spectral tools based on non-decimated complex\nwavelet transforms implemented by their matrix formulation. This non-decimated\ncomplex wavelet spectra utilizes both real and imaginary parts of\ncomplex-valued wavelet coefficients via their modulus and phases. A structural\nredundancy in non-decimated wavelets and a componential redundancy in complex\nwavelets act in a synergy when extracting wavelet-based informative\ndescriptors. In particular, we suggest an improved way of separating signals\nand images based on their scaling indices in terms of spectral slopes and\ninformation contained in the phase in order to improve performance of\nclassification. We show that performance of the proposed method is\nsignificantly improved when compared with procedures based on standard versions\nof wavelet transforms or on real-valued wavelets. It is also worth mentioning\nthat the matrix-based non-decimated wavelet transform can handle signals of an\narbitrary size and in 2-D case, rectangular images of possibly different and\nnon-dyadic dimensions. This is in contrast to the standard wavelet transforms\nwhere algorithms for handling objects of non-dyadic dimensions requires either\ndata preprocessing or customized algorithm adjustments. To demonstrate the use\nof defined spectral methodology we provide two examples of application on\nreal-data problems: classification of visual acuity using scaling in pupil\ndiameter dynamic in time and diagnostic and classification of digital mammogram\nimages using the fractality of digitized images of the background tissue. The\nproposed tools are contrasted with the traditional wavelet based counterparts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 05:01:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Kong", "Taewoon", ""], ["Vidakovic", "Brani", ""]]}, {"id": "1902.01052", "submitter": "Kazuhiko Nishimura", "authors": "Satoshi Nakano and Kazuhiko Nishimura", "title": "Restoration and extrapolation of structural transformation by dynamical\n  general equilibrium feedbacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model sectoral production by serially nesting (cascading) binary\ncompounding processes. The sequence of processes is discovered in a\nself-similar hierarchical structure stylized in macroscopic input-output\ntransactions. The feedback system of unit cost functions, with recursively\nestimated nest-wise CES parameters, is calibrated for sectoral productivities\nto replicate two temporally distant cost share structures, observed in a set of\nlinked input--output tables. We model representative households by multifactor\nCES, with parameters estimated by fixed effects regressions. By the integrated\ndynamic general equilibrium model, we extrapolate potential structural\ntransformations, and measure the associated welfare changes, caused by\nexogenous sectoral productivity shocks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 06:57:54 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 06:25:01 GMT"}, {"version": "v3", "created": "Wed, 27 Mar 2019 07:11:46 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Nakano", "Satoshi", ""], ["Nishimura", "Kazuhiko", ""]]}, {"id": "1902.01333", "submitter": "Barbara McGillivray", "authors": "Barbara McGillivray (The Alan Turing Institute and University of\n  Cambridge) and Mathias Astell (Hindawi Limited)", "title": "The relationship between usage and citations in an open access mega\n  journal", "comments": "22 pages, 7 figures. Scientometrics (2019)", "journal-ref": null, "doi": "10.1007/s11192-019-03228-3", "report-no": null, "categories": "cs.DL stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How do the level of usage of an article, the timeframe of its usage and its\nsubject area relate to the number of citations it accrues? This paper aims to\nanswer this question through an observational study of usage and citation data\ncollected about the multidisciplinary, open access mega-journal Scientific\nReports. This observational study answers these questions using the following\nmethods: an overlap analysis of most read and top-cited articles; Spearman\ncorrelation tests between total citation counts over two years and usage over\nvarious timeframes; a comparison of first months of citation for most read and\nall articles; a Wilcoxon test on the distribution of total citations of early\ncited articles and the distribution of total citations of all other articles.\nAll analyses were performed using the programming language R. As Scientific\nReports is a multidisciplinary journal covering all natural and clinical\nsciences, we also looked at the differences across subjects. We found a\nmoderate correlation between usage in the first year and citations in the first\ntwo years since publication, and that articles with high usage in the first 6\nmonths are more likely to have their first citation earlier (Wilcoxon=1811500,\np < 0.0001), which is also related to higher citations in the first two years\n(Wilcoxon=8071200, p < 0.0001). As this final assertion is inferred based on\nthe results of the other elements of this paper, it requires further analysis.\nMoreover, our choice of a 2 year window for our analysis did not consider the\narticles' citation half-life, and our use of Scientific Reports (a journal that\nis atypical compared to most academic journals) as the source of the articles\nanalysed has likely played a role in our findings, and so analysing a longer\ntimeframe and carrying out similar analysis on a different journal (or group of\njournals) may lead to different conclusions.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:44:20 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 15:06:23 GMT"}, {"version": "v3", "created": "Thu, 12 Sep 2019 17:55:16 GMT"}, {"version": "v4", "created": "Fri, 4 Oct 2019 17:07:07 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["McGillivray", "Barbara", "", "The Alan Turing Institute and University of\n  Cambridge"], ["Astell", "Mathias", "", "Hindawi Limited"]]}, {"id": "1902.01377", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford, Olga Morozova, Ashley L. Buchanan, and Donna\n  Spiegelman", "title": "Interpretation of the individual effect under treatment spillover", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some interventions may include important spillover or dissemination effects\nbetween study participants. For example, vaccines, cash transfers, and\neducation programs may exert a causal effect on participants beyond those to\nwhom individual treatment is assigned. In a recent paper, Buchanan et al.\nprovide a causal definition of the \"individual effect\" of an intervention in\nnetworks of people who inject drugs. In this short note, we discuss the\ninterpretation of the individual effect when a spillover or dissemination\neffect exists.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:44:36 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Morozova", "Olga", ""], ["Buchanan", "Ashley L.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1902.01399", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean, Maxime Chamberland, Max A. Viergever, Alexander\n  Leemans", "title": "Reducing variability in along-tract analysis with diffusion profile\n  realignment", "comments": "v4: peer-reviewed round 2 v3 : deleted some old text from before\n  peer-review which was mistakenly included v2 : peer-reviewed version v1:\n  preprint as submitted to journal NeuroImage", "journal-ref": "NeuroImage, 2019, ISSN 1053-8119", "doi": "10.1016/j.neuroimage.2019.06.016", "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion weighted MRI (dMRI) provides a non invasive virtual reconstruction\nof the brain's white matter structures through tractography. Analyzing dMRI\nmeasures along the trajectory of white matter bundles can provide a more\nspecific investigation than considering a region of interest or tract-averaged\nmeasurements. However, performing group analyses with this along-tract strategy\nrequires correspondence between points of tract pathways across subjects. This\nis usually achieved by creating a new common space where the representative\nstreamlines from every subject are resampled to the same number of points. If\nthe underlying anatomy of some subjects was altered due to, e.g. disease or\ndevelopmental changes, such information might be lost by resampling to a fixed\nnumber of points. In this work, we propose to address the issue of possible\nmisalignment, which might be present even after resampling, by realigning the\nrepresentative streamline of each subject in this 1D space with a new method,\ncoined diffusion profile realignment (DPR). Experiments on synthetic datasets\nshow that DPR reduces the coefficient of variation for the mean diffusivity,\nfractional anisotropy and apparent fiber density when compared to the unaligned\ncase. Using 100 in vivo datasets from the HCP, we simulated changes in mean\ndiffusivity, fractional anisotropy and apparent fiber density. Pairwise\nStudent's t-tests between these altered subjects and the original subjects\nindicate that regional changes are identified after realignment with the DPR\nalgorithm, while preserving differences previously detected in the unaligned\ncase. This new correction strategy contributes to revealing effects of interest\nwhich might be hidden by misalignment and has the potential to improve the\nspecificity in longitudinal population studies beyond the traditional region of\ninterest based analysis and along-tract analysis workflows.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 17:45:34 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 15:17:57 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 10:24:05 GMT"}, {"version": "v4", "created": "Wed, 8 May 2019 08:47:05 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["St-Jean", "Samuel", ""], ["Chamberland", "Maxime", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1902.01659", "submitter": "Michael Moor", "authors": "Michael Moor and Max Horn and Bastian Rieck and Damian Roqueiro and\n  Karsten Borgwardt", "title": "Early Recognition of Sepsis with Gaussian Process Temporal Convolutional\n  Networks and Dynamic Time Warping", "comments": "Accepted at the Machine Learning for Healthcare 2019 Conference\n  (MLHC). Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sepsis is a life-threatening host response to infection associated with high\nmortality, morbidity, and health costs. Its management is highly time-sensitive\nsince each hour of delayed treatment increases mortality due to irreversible\norgan damage. Meanwhile, despite decades of clinical research, robust\nbiomarkers for sepsis are missing. Therefore, detecting sepsis early by\nutilizing the affluence of high-resolution intensive care records has become a\nchallenging machine learning problem. Recent advances in deep learning and data\nmining promise to deliver a powerful set of tools to efficiently address this\ntask. This empirical study proposes two novel approaches for the early\ndetection of sepsis: a deep learning model and a lazy learner based on time\nseries distances. Our deep learning model employs a temporal convolutional\nnetwork that is embedded in a Multi-task Gaussian Process Adapter framework,\nmaking it directly applicable to irregularly-spaced time series data. Our lazy\nlearner, by contrast, is an ensemble approach that employs dynamic time\nwarping. We frame the timely detection of sepsis as a supervised time series\nclassification task. For this, we derive the most recent sepsis definition in\nan hourly resolution to provide the first fully accessible early sepsis\ndetection environment. Seven hours before sepsis onset, our methods improve\narea under the precision--recall curve from 0.25 to 0.35/0.40 over the state of\nthe art. This demonstrates that they are well-suited for detecting sepsis in\nthe crucial earlier stages when management is most effective.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 12:35:54 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 12:53:23 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 13:18:53 GMT"}, {"version": "v4", "created": "Thu, 15 Oct 2020 12:44:49 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Moor", "Michael", ""], ["Horn", "Max", ""], ["Rieck", "Bastian", ""], ["Roqueiro", "Damian", ""], ["Borgwardt", "Karsten", ""]]}, {"id": "1902.01923", "submitter": "Mouloud Belbahri", "authors": "Ali Vahdat, Mouloud Belbahri, Vahid Partovi Nia", "title": "Active Learning for High-Dimensional Binary Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Erbium-doped fiber amplifier (EDFA) is an optical amplifier/repeater device\nused to boost the intensity of optical signals being carried through a fiber\noptic communication system. A highly accurate EDFA model is important because\nof its crucial role in optical network management and optimization. The input\nchannels of an EDFA device are treated as either on or off, hence the input\nfeatures are binary. Labeled training data is very expensive to collect for\nEDFA devices, therefore we devise an active learning strategy suitable for\nbinary variables to overcome this issue. We propose to take advantage of sparse\nlinear models to simplify the predictive model. This approach simultaneously\nimproves prediction and accelerates active learning query generation. We show\nthe performance of our proposed active learning strategies on simulated data\nand real EDFA data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 21:46:53 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 15:23:31 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Vahdat", "Ali", ""], ["Belbahri", "Mouloud", ""], ["Nia", "Vahid Partovi", ""]]}, {"id": "1902.02020", "submitter": "David Yu", "authors": "David Yu, Christopher Boucher, Luke Bornn, Mehrsan Javan", "title": "Playing Fast Not Loose: Evaluating team-level pace of play in ice hockey\n  using spatio-temporal possession data", "comments": "Accepted Paper for the 2019 Sloan Sports Analytics Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pace of play is an important characteristic in hockey as well as other team\nsports. We provide the first comprehensive study of pace within the sport of\nhockey, focusing on how teams and players impact pace in different regions of\nthe ice, and the resultant effect on other aspects of the game.\n  First we examined how pace of play varies across the surface of the rink,\nacross different periods, at different manpower situations, between different\nprofessional leagues, and through time between seasons. Our analysis of pace by\nzone helps to explain some of the counter-intuitive results reported in prior\nstudies. For instance, we show that the negative correlation between attacking\nspeed and shots/goals is likely due to a large decline in attacking speed in\nthe OZ.\n  We also studied how pace impacts the outcomes of various events. We found\nthat pace is positively-correlated with both high-danger zone entries (e.g.\nodd-man rushes) and higher shot quality. However, we find that passes with\nfailed receptions occur at higher speeds than successful receptions. These\nfindings suggest that increased pace is beneficial, but perhaps only up to a\ncertain extent. Higher pace can create breakdowns in defensive structure and\nlead to better scoring chances but can also lead to more turnovers.\n  Finally, we analyzed team and player-level pace in the NHL, highlighting the\nconsiderable variability in how teams and players attack and defend against\npace. Taken together, our results demonstrate that measures of team-level pace\nderived from spatio-temporal data are informative metrics in hockey and should\nprove useful in other team sports.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 04:38:19 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Yu", "David", ""], ["Boucher", "Christopher", ""], ["Bornn", "Luke", ""], ["Javan", "Mehrsan", ""]]}, {"id": "1902.02021", "submitter": "Yu Wang", "authors": "Yu Wang, Somit Gupta, Jiannan Lu, Ali Mahmoudzadeh, Sophia Liu", "title": "On Heavy-user Bias in A/B Testing", "comments": "5 pages, CIKM'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On-line experimentation (also known as A/B testing) has become an integral\npart of software development. To timely incorporate user feedback and\ncontinuously improve products, many software companies have adopted the culture\nof agile deployment, requiring online experiments to be conducted and concluded\non limited sets of users for a short period. While conceptually efficient, the\nresult observed during the experiment duration can deviate from what is seen\nafter the feature deployment, which makes the A/B test result biased. In this\npaper, we provide theoretical analysis to show that heavy-users can contribute\nsignificantly to the bias, and propose a re-sampling estimator for bias\nadjustment.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 04:39:10 GMT"}, {"version": "v2", "created": "Sat, 10 Aug 2019 17:22:24 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Wang", "Yu", ""], ["Gupta", "Somit", ""], ["Lu", "Jiannan", ""], ["Mahmoudzadeh", "Ali", ""], ["Liu", "Sophia", ""]]}, {"id": "1902.02026", "submitter": "Michael Donohue", "authors": "Dan Li, Samuel Iddi, Paul S. Aisen, Wesley K. Thompson, Michael C.\n  Donohue", "title": "The relative efficiency of time-to-progression and continuous measures\n  of cognition in pre-symptomatic Alzheimer's", "comments": "16 pages, 4 figures", "journal-ref": "Alzheimer's & Dementia: Translational Research & Clinical\n  Interventions (2019)", "doi": "10.1016/j.trci.2019.04.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-symptomatic (or Preclinical) Alzheimer's Disease is defined by biomarker\nevidence of fibrillar amyloid beta pathology in the absence of clinical\nsymptoms. Clinical trials in this early phase of disease are challenging due to\nthe slow rate of disease progression as measured by periodic cognitive\nperformance tests or by transition to a diagnosis of Mild Cognitive Impairment.\nIn a multisite study, experts provide diagnoses by central chart review without\nthe benefit of in-person assessment. We use a simulation study to demonstrate\nthat models of repeated cognitive assessments detect treatment effects more\nefficiently compared to models of time-to-progression to an endpoint such as\nchange in diagnosis. Multivariate continuous data are simulated from a Bayesian\njoint mixed effects model fit to data from the Alzheimer's Disease Neuroimaging\nInitiative. Simulated progression events are algorithmically derived from the\ncontinuous assessments using a random forest model fit to the same data. We\nfind that power is approximately doubled with models of repeated continuous\noutcomes compared to the time-to-progression analysis. The simulations also\ndemonstrate that a plausible informative missing data pattern can induce a bias\nwhich inflates treatment effects, yet 5% Type I error is maintained.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 05:20:00 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Li", "Dan", ""], ["Iddi", "Samuel", ""], ["Aisen", "Paul S.", ""], ["Thompson", "Wesley K.", ""], ["Donohue", "Michael C.", ""]]}, {"id": "1902.02061", "submitter": "Phil Scarf", "authors": "Phil Scarf, Mansour Shrahili, Naif Alotaibi, Simon Jobson, and Louis\n  Passfield", "title": "Modelling the effect of training on performance in road cycling:\n  estimation of the Banister model parameters using field data", "comments": "14 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suppose that performance is a random variable whose expectation is related\nto training inputs, and we study four performance measures in a statistical\nmodel that relates performance to training. Our aim is to carry out a robust\nstatistical analysis of the training-performance models that are used in\nproprietary software to plan training, and thereby put them on a firmer\nfooting. The performance measures we consider are calculated using power output\nand heart rate data collected in the field by road cyclists. We find that\nparameter estimates in the training-performance models that we study differ\nacross riders and across performance measures within riders. We conclude\ntherefore that models and their estimates must be specific, both to the\nindividual and to the quality (e.g. speed or endurance) that the individual\nseeks to train. While the parameter estimates we obtain may be useful for\ncomparing given training programmes, we show that the underlying models\nthemselves are not appropriate for the optimisation of a training schedule in\nadvance of competition.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 08:29:24 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Scarf", "Phil", ""], ["Shrahili", "Mansour", ""], ["Alotaibi", "Naif", ""], ["Jobson", "Simon", ""], ["Passfield", "Louis", ""]]}, {"id": "1902.02173", "submitter": "Igor Barahona Dr", "authors": "Dalina Aidee Villa, Igor Barahona, Luis Javier \\'Alvarez", "title": "Dise\\~no de un espacio sem\\'antico sobre la base de la Wikipedia. Una\n  propuesta de an\\'alisis de la sem\\'antica latente para el idioma espa\\~nol", "comments": "14 pages, in Spanish, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Semantic Analysis (LSA) was initially conceived by the cognitive\npsychology at the 90s decade. Since its emergence, the LSA has been used to\nmodel cognitive processes, pointing out academic texts, compare literature\nworks and analyse political speeches, among other applications. Taking as\nstarting point multivariate method for dimensionality reduction, this paper\npropose a semantic space for Spanish language. Out results include a document\ntext matrix with dimensions 1.3 x10^6 and 5.9x10^6, which later is decomposed\ninto singular values. Those singular values are used to semantically words or\ntext.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 21:39:23 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Villa", "Dalina Aidee", ""], ["Barahona", "Igor", ""], ["\u00c1lvarez", "Luis Javier", ""]]}, {"id": "1902.02270", "submitter": "Nooshin Yousefi", "authors": "Nooshin Yousefi, Ahmad Sobhani, Leila Moslemi Naeni, Kenneth R. Currie", "title": "Using statistical control charts to monitor duration-based performance\n  of project", "comments": null, "journal-ref": null, "doi": "10.19255/jmpm415", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring of project performance is a crucial task of project managers that\nsignificantly affect the project success or failure. Earned Value Management\n(EVM) is a well-known tool to evaluate project performance and effective\ntechnique for identifying delays and proposing appropriate corrective actions.\nThe original EVM analysis is a monetary-based method and it can be misleading\nin the evaluation of the project schedule performance and estimation of the\nproject duration. Earned Duration Management (EDM) is a more recent method\nwhich introduces metrics for the project schedule performance evaluation and\nimproves EVM analysis. In this paper, we apply statistical control charts on\nEDM indices to better investigate the variations of project schedule\nperformance. Control charts are decision support tools to detect the out of\ncontrol performance. Usually project performance measurements are\nauto-correlated and not following the normal distribution. Hence, in this\npaper, a two-step adjustment framework is proposed to make the control charts\napplicable to non-normal and auto-correlated measurements. The case study\nproject illustrates how the new method can be implemented in practice. The\nnumerical results conclude that that employing control chart method along with\nanalyzing the actual values of EDM indices increase the capability of project\nmanagement teams to detect cost and schedule problems on time\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 16:53:17 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Yousefi", "Nooshin", ""], ["Sobhani", "Ahmad", ""], ["Naeni", "Leila Moslemi", ""], ["Currie", "Kenneth R.", ""]]}, {"id": "1902.02397", "submitter": "David Yu", "authors": "Nick Czuzoj-Shulman, David Yu, Christopher Boucher, Luke Bornn,\n  Mehrsan Javan", "title": "Winning Is Not Everything: A contextual analysis of hockey face-offs", "comments": "Accepted paper for the 2019 Sloan Sports Analytics Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes a different approach to evaluating face-offs in ice hockey.\nInstead of looking at win percentages, the de facto measure of successful\nface-off takers for decades, focuses on the game events following the face-off\nand how directionality, clean wins, and player handedness play a significant\nrole in creating value. This will demonstrate how not all face-off wins are\nmade equal: some players consistently create post-face-off value through clean\nwins and by directing the puck to high-value areas of the ice. As a result, we\npropose an expected events face-off model as well as a wins above expected\nmodel that take into account the value added on a face-off by targeting the\npuck to specific areas on the ice in various contexts, as well as the impact\nthis has on subsequent game events.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 20:55:01 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Czuzoj-Shulman", "Nick", ""], ["Yu", "David", ""], ["Boucher", "Christopher", ""], ["Bornn", "Luke", ""], ["Javan", "Mehrsan", ""]]}, {"id": "1902.02412", "submitter": "Quinten Meertens", "authors": "Q. A. Meertens, C. G. H. Diks, H. J. van den Herik, F W Takes", "title": "A Bayesian Approach for Accurate Classification-Based Aggregates", "comments": "9 pages, 5 figures, accepted conference paper, SIAM International\n  Conference on Data Mining 2019 (SDM19)", "journal-ref": null, "doi": "10.1137/1.9781611975673.35", "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the accuracy of values aggregated over classes\npredicted by a classification algorithm. The problem is that the resulting\naggregates (e.g., sums of a variable) are known to be biased. The bias can be\nlarge even for highly accurate classification algorithms, in particular when\ndealing with class-imbalanced data. To correct this bias, the algorithm's\nclassification error rates have to be estimated. In this estimation, two issues\narise when applying existing bias correction methods. First, inaccuracies in\nestimating classification error rates have to be taken into account. Second,\nimpermissible estimates, such as a negative estimate for a positive value, have\nto be dismissed. We show that both issues are relevant in applications where\nthe true labels are known only for a small set of data points. We propose a\nnovel bias correction method using Bayesian inference. The novelty of our\nmethod is that it imposes constraints on the model parameters. We show that our\nmethod solves the problem of biased classification-based aggregates as well as\nthe two issues above, in the general setting of multi-class classification. In\nthe empirical evaluation, using a binary classifier on a real-world dataset of\ncompany tax returns, we show that our method outperforms existing methods in\nterms of mean squared error.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 22:18:42 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meertens", "Q. A.", ""], ["Diks", "C. G. H.", ""], ["Herik", "H. J. van den", ""], ["Takes", "F W", ""]]}, {"id": "1902.02509", "submitter": "Quentin Bertrand", "authors": "Quentin Bertrand (PARIETAL), Mathurin Massias (PARIETAL), Alexandre\n  Gramfort (PARIETAL), Joseph Salmon (IMAG)", "title": "Handling correlated and repeated measurements with the smoothed\n  multivariate square-root Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity promoting norms are frequently used in high dimensional regression.\nA limitation of such Lasso-type estimators is that the optimal regularization\nparameter depends on the unknown noise level. Estimators such as the\nconcomitant Lasso address this dependence by jointly estimating the noise level\nand the regression coefficients. Additionally, in many applications, the data\nis obtained by averaging multiple measurements: this reduces the noise\nvariance, but it dramatically reduces sample sizes and prevents refined noise\nmodeling. In this work, we propose a concomitant estimator that can cope with\ncomplex noise structure by using non-averaged measurements. The resulting\noptimization problem is convex and amenable, thanks to smoothing theory, to\nstate-of-the-art optimization techniques that leverage the sparsity of the\nsolutions. Practical benefits are demonstrated on toy datasets, realistic\nsimulated data and real neuroimaging data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 07:44:54 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 07:49:35 GMT"}, {"version": "v3", "created": "Thu, 19 Sep 2019 09:21:11 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2020 17:03:49 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bertrand", "Quentin", "", "PARIETAL"], ["Massias", "Mathurin", "", "PARIETAL"], ["Gramfort", "Alexandre", "", "PARIETAL"], ["Salmon", "Joseph", "", "IMAG"]]}, {"id": "1902.02533", "submitter": "Michael Sachs", "authors": "Michael C Sachs, Andrea Discacciati, {\\AA}sa Everhov, Ola Ol\\'en, Erin\n  E Gabriel", "title": "Ensemble Prediction of Time to Event Outcomes with Competing Risks: A\n  Case Study of Surgical Complications in Crohn's Disease", "comments": "25 pages, 3 figures, submitted to JRSS-C", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel algorithm to predict the occurrence of major abdominal\nsurgery within 5 years following Crohn's disease diagnosis using a panel of 29\nbaseline covariates from the Swedish population registers. We model\npseudo-observations based on the Aalen-Johansen estimator of the cause-specific\ncumulative incidence with an ensemble of modern machine learning approaches.\nPseudo-observation pre-processing easily extends all existing or new machine\nlearning procedures to right-censored event history data. We propose\npseudo-observation based estimators for the area under the time varying ROC\ncurve, for optimizing the ensemble, and the predictiveness curve, for\nevaluating and summarizing predictive performance.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 09:14:40 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Sachs", "Michael C", ""], ["Discacciati", "Andrea", ""], ["Everhov", "\u00c5sa", ""], ["Ol\u00e9n", "Ola", ""], ["Gabriel", "Erin E", ""]]}, {"id": "1902.02860", "submitter": "Saeed Khaki", "authors": "Saeed Khaki and Lizhi Wang", "title": "Crop Yield Prediction Using Deep Neural Networks", "comments": "9 pages, Presented at 2018 INFORMS Conference on Business Analytics\n  and Operations Research (Baltimore, MD, USA). One of the winning solutions to\n  the 2018 Syngenta Crop Challenge", "journal-ref": "Frontiers in Plant Science, 10:621, 2019", "doi": "10.3389/fpls.2019.00621", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crop yield is a highly complex trait determined by multiple factors such as\ngenotype, environment, and their interactions. Accurate yield prediction\nrequires fundamental understanding of the functional relationship between yield\nand these interactive factors, and to reveal such relationship requires both\ncomprehensive datasets and powerful algorithms. In the 2018 Syngenta Crop\nChallenge, Syngenta released several large datasets that recorded the genotype\nand yield performances of 2,267 maize hybrids planted in 2,247 locations\nbetween 2008 and 2016 and asked participants to predict the yield performance\nin 2017. As one of the winning teams, we designed a deep neural network (DNN)\napproach that took advantage of state-of-the-art modeling and solution\ntechniques. Our model was found to have a superior prediction accuracy, with a\nroot-mean-square-error (RMSE) being 12% of the average yield and 50% of the\nstandard deviation for the validation dataset using predicted weather data.\nWith perfect weather data, the RMSE would be reduced to 11% of the average\nyield and 46% of the standard deviation. We also performed feature selection\nbased on the trained DNN model, which successfully decreased the dimension of\nthe input space without significant drop in the prediction accuracy. Our\ncomputational results suggested that this model significantly outperformed\nother popular methods such as Lasso, shallow neural networks (SNN), and\nregression tree (RT). The results also revealed that environmental factors had\na greater effect on the crop yield than genotype.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 21:54:00 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 21:53:26 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 18:20:24 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Khaki", "Saeed", ""], ["Wang", "Lizhi", ""]]}, {"id": "1902.03000", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (UFC), Abdoulkarim Ilmi Amir (LMB)", "title": "Distribution of residual autocorrelations for multiplicative seasonal\n  ARMA models with uncorrelated but non-independent error terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider portmanteau tests for testing the adequacy of\nmultiplicative seasonal autoregressive moving-average (SARMA) models under the\nassumption that the errors are uncorrelated but not necessarily independent.We\nrelax the standard independence assumption on the error term in order to extend\nthe range of application of the SARMA models.We study the asymptotic\ndistributions of residual and normalized residual empirical autocovariances and\nautocorrelations underweak assumptions on the noise. We establish the\nasymptotic behaviour of the proposed statistics. A set of Monte Carlo\nexperiments and an application to monthly mean total sunspot number are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 10:07:55 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "UFC"], ["Amir", "Abdoulkarim Ilmi", "", "LMB"]]}, {"id": "1902.03093", "submitter": "Alfredo Kalaitzis", "authors": "Laure Delisle, Alfredo Kalaitzis, Krzysztof Majewski, Archy de Berker,\n  Milena Marin, Julien Cornebise", "title": "A large-scale crowdsourced analysis of abuse against women journalists\n  and politicians on Twitter", "comments": "Workshop on AI for Social Good, NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report the first, to the best of our knowledge, hand-in-hand collaboration\nbetween human rights activists and machine learners, leveraging crowd-sourcing\nto study online abuse against women on Twitter. On a technical front, we\ncarefully curate an unbiased yet low-variance dataset of labeled tweets,\nanalyze it to account for the variability of abuse perception, and establish\nbaselines, preparing it for release to community research efforts. On a social\nimpact front, this study provides the technical backbone for a media campaign\naimed at raising public and deciders' awareness and elevating the standards\nexpected from social media companies.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 16:59:01 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Delisle", "Laure", ""], ["Kalaitzis", "Alfredo", ""], ["Majewski", "Krzysztof", ""], ["de Berker", "Archy", ""], ["Marin", "Milena", ""], ["Cornebise", "Julien", ""]]}, {"id": "1902.03097", "submitter": "Georgios Giasemidis Dr", "authors": "Georgios Giasemidis and Nikolaos Kaplis and Ioannis Agrafiotis and\n  Jason R. C. Nurse", "title": "A semi-supervised approach to message stance classification", "comments": "33 pages, 8 figures, 1 table", "journal-ref": "IEEE Transactions on Knowledge and Data Engineering, November 2018", "doi": "10.1109/TKDE.2018.2880192", "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media communications are becoming increasingly prevalent; some useful,\nsome false, whether unwittingly or maliciously. An increasing number of rumours\ndaily flood the social networks. Determining their veracity in an autonomous\nway is a very active and challenging field of research, with a variety of\nmethods proposed. However, most of the models rely on determining the\nconstituent messages' stance towards the rumour, a feature known as the \"wisdom\nof the crowd\". Although several supervised machine-learning approaches have\nbeen proposed to tackle the message stance classification problem, these have\nnumerous shortcomings. In this paper we argue that semi-supervised learning is\nmore effective than supervised models and use two graph-based methods to\ndemonstrate it. This is not only in terms of classification accuracy, but\nequally important, in terms of speed and scalability. We use the Label\nPropagation and Label Spreading algorithms and run experiments on a dataset of\n72 rumours and hundreds of thousands messages collected from Twitter. We\ncompare our results on two available datasets to the state-of-the-art to\ndemonstrate our algorithms' performance regarding accuracy, speed and\nscalability for real-time applications.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2019 11:57:53 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Giasemidis", "Georgios", ""], ["Kaplis", "Nikolaos", ""], ["Agrafiotis", "Ioannis", ""], ["Nurse", "Jason R. C.", ""]]}, {"id": "1902.03132", "submitter": "Adam Charles", "authors": "Gal Mishne and Adam S. Charles", "title": "Learning spatially-correlated temporal dictionaries for calcium imaging", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calcium imaging has become a fundamental neural imaging technique, aiming to\nrecover the individual activity of hundreds of neurons in a cortical region.\nCurrent methods (mostly matrix factorization) are aimed at detecting neurons in\nthe field-of-view and then inferring the corresponding time-traces. In this\npaper, we reverse the modeling and instead aim to minimize the spatial\ninference, while focusing on finding the set of temporal traces present in the\ndata. We reframe the problem in a dictionary learning setting, where the\ndictionary contains the time-traces and the sparse coefficient are spatial\nmaps. We adapt dictionary learning to calcium imaging by introducing\nconstraints on the norms and correlations of the time-traces, and incorporating\na hierarchical spatial filtering model that correlates the time-trace usage\nover the field-of-view. We demonstrate on synthetic and real data that our\nsolution has advantages regarding initialization, implicitly inferring number\nof neurons and simultaneously detecting different neuronal types.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:56:35 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Mishne", "Gal", ""], ["Charles", "Adam S.", ""]]}, {"id": "1902.03188", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Mahdi Imani, Ulisses Braga-Neto, Xiaoning Qian,\n  and Edward R Dougherty", "title": "Scalable optimal Bayesian classification of single-cell trajectories\n  under regulatory model uncertainty", "comments": null, "journal-ref": "BMC Genomics 2019", "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell gene expression measurements offer opportunities in deriving\nmechanistic understanding of complex diseases, including cancer. However, due\nto the complex regulatory machinery of the cell, gene regulatory network (GRN)\nmodel inference based on such data still manifests significant uncertainty. The\ngoal of this paper is to develop optimal classification of single-cell\ntrajectories accounting for potential model uncertainty. Partially-observed\nBoolean dynamical systems (POBDS) are used for modeling gene regulatory\nnetworks observed through noisy gene-expression data. We derive the exact\noptimal Bayesian classifier (OBC) for binary classification of single-cell\ntrajectories. The application of the OBC becomes impractical for large GRNs,\ndue to computational and memory requirements. To address this, we introduce a\nparticle-based single-cell classification method that is highly scalable for\nlarge GRNs with much lower complexity than the optimal solution. The\nperformance of the proposed particle-based method is demonstrated through\nnumerical experiments using a POBDS model of the well-known T-cell large\ngranular lymphocyte (T-LGL) leukemia network with noisy time-series\ngene-expression data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 16:51:34 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Imani", "Mahdi", ""], ["Braga-Neto", "Ulisses", ""], ["Qian", "Xiaoning", ""], ["Dougherty", "Edward R", ""]]}, {"id": "1902.03271", "submitter": "Supreeth Prajwal Shashikumar", "authors": "Russell Jeter, Christopher Josef, Supreeth Shashikumar and Shamim\n  Nemati", "title": "Does the \"Artificial Intelligence Clinician\" learn optimal treatment\n  strategies for sepsis in intensive care?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  From 2017 to 2018 the number of scientific publications found via PubMed\nsearch using the keyword \"Machine Learning\" increased by 46% (4,317 to 6,307).\nThe results of studies involving machine learning, artificial intelligence\n(AI), and big data have captured the attention of healthcare practitioners,\nhealthcare managers, and the public at a time when Western medicine grapples\nwith unmitigated cost increases and public demands for accountability. The\ncomplexity involved in healthcare applications of machine learning and the size\nof the associated data sets has afforded many researchers an uncontested\nopportunity to satisfy these demands with relatively little oversight. In a\nrecent Nature Medicine article, \"The Artificial Intelligence Clinician learns\noptimal treatment strategies for sepsis in intensive care,\" Komorowski and his\ncoauthors propose methods to train an artificial intelligence clinician to\ntreat sepsis patients with vasopressors and IV fluids. In this post, we will\nclosely examine the claims laid out in this paper. In particular, we will study\nthe individual treatment profiles suggested by their AI Clinician to gain\ninsight into how their AI Clinician intends to treat patients on an individual\nlevel.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 19:54:49 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Jeter", "Russell", ""], ["Josef", "Christopher", ""], ["Shashikumar", "Supreeth", ""], ["Nemati", "Shamim", ""]]}, {"id": "1902.03293", "submitter": "Stefania Russo", "authors": "Stefania Russo, Guangyu Li, Kris Villez", "title": "Automatic dimensionality selection for principal component analysis\n  models with the ignorance score", "comments": "Submitted to American Chemical Society Journal, Industrial &\n  Engineering Chemistry Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is by far the most widespread tool for\nunsupervised learning with high-dimensional data sets. Its application is\npopularly studied for the purpose of exploratory data analysis and online\nprocess monitoring. Unfortunately, fine-tuning PCA models and particularly the\nnumber of components remains a challenging task. Today, this selection is often\nbased on a combination of guiding principles, experience, and process\nunderstanding. Unlike the case of regression, where cross-validation of the\nprediction error is a widespread and trusted approach for model selection,\nthere are no tools for PCA model selection which reach this level of\nacceptance. In this work, we address this challenge and evaluate the utility of\nthe cross-validated ignorance score with both simulated and experimental data\nsets. Application of this method is based on the interpretation of PCA as a\ndensity model, as in probabilistic principal component analysis, and is shown\nto be a valuable tool to identify an optimal number of principal components.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 21:17:02 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Russo", "Stefania", ""], ["Li", "Guangyu", ""], ["Villez", "Kris", ""]]}, {"id": "1902.03350", "submitter": "Nicholas James Mr", "authors": "Nick James, Roman Marchant, Richard Gerlach, Sally Cripps", "title": "Bayesian Nonparametric Adaptive Spectral Density Estimation for\n  Financial Time Series", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrimination between non-stationarity and long-range dependency is a\ndifficult and long-standing issue in modelling financial time series. This\npaper uses an adaptive spectral technique which jointly models the\nnon-stationarity and dependency of financial time series in a non-parametric\nfashion assuming that the time series consists of a finite, but unknown number,\nof locally stationary processes, the locations of which are also unknown. The\nmodel allows a non-parametric estimate of the dependency structure by modelling\nthe auto-covariance function in the spectral domain. All our estimates are made\nwithin a Bayesian framework where we use aReversible Jump Markov Chain Monte\nCarlo algorithm for inference. We study the frequentist properties of our\nestimates via a simulation study, and present a novel way of generating time\nseries data from a nonparametric spectrum. Results indicate that our techniques\nperform well across a range of data generating processes. We apply our method\nto a number of real examples and our results indicate that several financial\ntime series exhibit both long-range dependency and non-stationarity.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 01:58:48 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["James", "Nick", ""], ["Marchant", "Roman", ""], ["Gerlach", "Richard", ""], ["Cripps", "Sally", ""]]}, {"id": "1902.03488", "submitter": "Yoshihiko Suhara", "authors": "Yoshihiko Suhara, Mohsen Bahrami, Bur\\c{c}in Bozkaya, Alex `Sandy'\n  Pentland", "title": "Validating Gravity-Based Market Share Models Using Large-Scale\n  Transactional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer patronage behavior has been widely studied in market share modeling\ncontexts, which is an essential step towards modeling and solving competitive\nfacility location problems. Existing studies have conducted surveys to estimate\nmerchants' market share and factors of attractiveness to use in various\nproposed mathematical models. Recent trends in Big Data analysis allow us to\nbetter understand human behavior and decision making, potentially leading to\nlocation models with more realistic assumptions. In this paper, we propose a\nnovel approach for validating Huff gravity market share model, using a\nlarge-scale transactional dataset that describes customer patronage behavior in\na regional scale. Although the Huff model has been well-studied and widely used\nin the context of competitive facility location and demand allocation, this\npaper is the first in validating the Huff model with a real dataset. Our\napproach helps to easily apply the model in different regions and with\ndifferent merchant categories. Experimental results show that the Huff model\nfits well when modeling customer shopping behavior for a number of shopping\ncategories including grocery stores, clothing stores, gas stations, and\nrestaurants. We also conduct regression analysis to show that certain features\nsuch as gender diversity and marital status diversity lead to stronger\nvalidation of the Huff model. We believe we provide strong evidence, with the\nhelp of real world data, that gravity-based market share models are viable\nassumptions for competitive facility location models.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 20:56:17 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Suhara", "Yoshihiko", ""], ["Bahrami", "Mohsen", ""], ["Bozkaya", "Bur\u00e7in", ""], ["Pentland", "Alex `Sandy'", ""]]}, {"id": "1902.03714", "submitter": "Achraf Bahamou", "authors": "Achraf Bahamou, Maud Doumergue, Philippe Donnat", "title": "Hawkes processes for credit indices time series analysis: How random are\n  trades arrival times?", "comments": "ITISE 2018 International Conference on Time Series and Forecasting\n  accepted paper", "journal-ref": "Proceedings - International Conference on Time Series and\n  Forecasting, ITISE 2018. Granada: University of Granada, pp. 1178-1192", "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST q-fin.TR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Targeting a better understanding of credit market dynamics, the authors have\nstudied a stochastic model named the Hawkes process. Describing trades arrival\ntimes, this kind of model allows for the capture of self-excitement and mutual\ninteractions phenomena. The authors propose here a simple yet conclusive method\nfor fitting multidimensional Hawkes processes with exponential kernels, based\non a maximum likelihood non-convex optimization. The method was successfully\ntested on simulated data, then used on new publicly available real trading data\nfor three European credit indices, thus enabling quantification of\nself-excitement as well as volume impacts or cross indices influences.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 03:39:20 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Bahamou", "Achraf", ""], ["Doumergue", "Maud", ""], ["Donnat", "Philippe", ""]]}, {"id": "1902.04186", "submitter": "Hiroyuki Kasai", "authors": "Hiroyuki Kasai and Bamdev Mishra", "title": "Riemannian joint dimensionality reduction and dictionary learning on\n  symmetric positive definite manifold", "comments": "European Signal Processing Conference (EUSIPCO 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dictionary leaning (DL) and dimensionality reduction (DR) are powerful tools\nto analyze high-dimensional noisy signals. This paper presents a proposal of a\nnovel Riemannian joint dimensionality reduction and dictionary learning\n(R-JDRDL) on symmetric positive definite (SPD) manifolds for classification\ntasks. The joint learning considers the interaction between dimensionality\nreduction and dictionary learning procedures by connecting them into a unified\nframework. We exploit a Riemannian optimization framework for solving DL and DR\nproblems jointly. Finally, we demonstrate that the proposed R-JDRDL outperforms\nexisting state-of-the-arts algorithms when used for image classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 23:49:03 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Kasai", "Hiroyuki", ""], ["Mishra", "Bamdev", ""]]}, {"id": "1902.04242", "submitter": "Zhiqing Xu", "authors": "Zhiqing Xu, Balgobin Nandram, Binod Manandhar", "title": "Bayesian Inference of a Finite Population Mean Under Length-Biased\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a robust Bayesian method to analyze forestry data when samples are\nselected with probability proportional to length from a finite population of\nunknown size. Specifically, we use Bayesian predictive inference to estimate\nthe finite population mean of shrub widths in a limestone quarry dominated by\nre-growth of mountain mahogany. The data on shrub widths are collected using\ntransect sampling and it is assumed that the probability that a shrub is\nselected is proportional to its width; this is length-biased sampling. In this\ntype of sampling, the population size is also unknown and this creates an\nadditional challenge. The quantity of interest is average finite population\nshrub width and the total shrub area of the quarry can be estimated. Our method\nis assisted by using the three-parameter generalized gamma distribution,\nthereby robustifying our procedure against a possible model failure. Using\nconditional predictive ordinates, we show that the model, which accommodates\nlength bias, performs better than the model that does not. In the Bayesian\ncomputation, we overcome a technical problem associated with Gibbs sampling by\nusing a random sampler.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 05:10:57 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Xu", "Zhiqing", ""], ["Nandram", "Balgobin", ""], ["Manandhar", "Binod", ""]]}, {"id": "1902.04303", "submitter": "Jun Jie Sim", "authors": "Jun Jie Sim, Fook Mun Chan, Shibin Chen, Benjamin Hong Meng Tan, Khin\n  Mi Mi Aung", "title": "Achieving GWAS with Homomorphic Encryption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CR q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One way of investigating how genes affect human traits would be with a\ngenome-wide association study (GWAS). Genetic markers, known as\nsingle-nucleotide polymorphism (SNP), are used in GWAS. This raises privacy and\nsecurity concerns as these genetic markers can be used to identify individuals\nuniquely. This problem is further exacerbated by a large number of SNPs needed,\nwhich produce reliable results at a higher risk of compromising the privacy of\nparticipants.\n  We describe a method using homomorphic encryption (HE) to perform GWAS in a\nsecure and private setting. This work is based on a proposed algorithm. Our\nsolution mainly involves homomorphically encrypted matrix operations and\nsuitable approximations that adapts the semi-parallel GWAS algorithm for HE. We\nleverage the complex space of the CKKS encryption scheme to increase the number\nof SNPs that can be packed within a ciphertext. We have also developed a cache\nmodule that manages ciphertexts, reducing the memory footprint.\n  We have implemented our solution over two HE open source libraries, HEAAN and\nSEAL. Our best implementation took $24.70$ minutes for a dataset with $245$\nsamples, over $4$ covariates and $10643$ SNPs.\n  We demonstrate that it is possible to achieve GWAS with homomorphic\nencryption with suitable approximations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 09:51:25 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 02:47:00 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 09:54:44 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Sim", "Jun Jie", ""], ["Chan", "Fook Mun", ""], ["Chen", "Shibin", ""], ["Tan", "Benjamin Hong Meng", ""], ["Aung", "Khin Mi Mi", ""]]}, {"id": "1902.04337", "submitter": "Jose Vilar", "authors": "Jose M. G. Vilar", "title": "Winning the Big Data Technologies Horizon Prize: Fast and reliable\n  forecasting of electricity grid traffic by identification of recurrent\n  fluctuations", "comments": "Approach and methodology used in winning the European Union Big Data\n  Technologies Horizon Prize\n  (https://ec.europa.eu/research/horizonprize/index.cfm?pg=prizes)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cond-mat.stat-mech cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a description of the approach and methodology I used in\nwinning the European Union Big Data Technologies Horizon Prize on data-driven\nprediction of electricity grid traffic. The methodology relies on identifying\ntypical short-term recurrent fluctuations, which is subsequently refined\nthrough a regression-of-fluctuations approach. The key points and strategic\nconsiderations that led to selecting or discarding different methodological\naspects are also discussed. The criteria include adaptability to changing\nconditions, reliability with outliers and missing data, robustness to noise,\nand efficiency in implementation.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 11:38:25 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Vilar", "Jose M. G.", ""]]}, {"id": "1902.04350", "submitter": "Gregor Dumphart", "authors": "Gregor Dumphart, Marc Kuhn, Armin Wittneben, Florian Tr\\\"osch", "title": "Inter-Node Distance Estimation from Multipath Delay Differences of\n  Channels to Observer Nodes", "comments": "To appear at IEEE ICC 2019. This work has been submitted to the IEEE\n  for possible publication. Copyright may be transferred without notice, after\n  which this version may no longer be accessible", "journal-ref": null, "doi": "10.1109/ICC.2019.8761943", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the estimation of distance d between two wireless nodes by means of\ntheir wideband channels to a third node, called observer. The motivating\nprinciple is that the channel impulse responses are similar for small d and\ndrift apart when d increases. Following this idea we propose specific distance\nestimators based on the differences of path delays of the extractable multipath\ncomponents. In particular, we derive such estimators for rich multipath\nenvironments and various important cases: with and without clock\nsynchronization as well as errors on the extracted path delays (e.g. due to\nlimited bandwidth). The estimators readily support (and benefit from) the\npresence of multiple observers. We present an error analysis and, using ray\ntracing in an exemplary indoor environment, show that the estimators perform\nwell in realistic conditions. We describe possible localization applications of\nthe proposed scheme and highlight its major advantages: it requires neither\nprecise synchronization nor line-of-sight connection. This could make wireless\nuser tracking feasible in dynamic indoor settings.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 12:07:27 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dumphart", "Gregor", ""], ["Kuhn", "Marc", ""], ["Wittneben", "Armin", ""], ["Tr\u00f6sch", "Florian", ""]]}, {"id": "1902.04499", "submitter": "Ehsan Hajiramezanali", "authors": "Seyyed Hamed Fouladi, Ehsan Hajiramezanali", "title": "Non-Linear Non-Stationary Heteroscedasticity Volatility for Tracking of\n  Jump Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new jump process modeling which involves a\nparticular kind of non-Gaussian stochastic processes with random jumps at\nrandom time points. The main goal of this study is to provide an accurate\ntracking technique based on non-linear non-stationary heteroscedasticity (NNH)\ntime series. It is, in fact, difficult to track jump processes regarding the\nfact that non-Gaussianity is an inherent feature in these processes. The\nproposed NNH model is conditionally Gaussian whose conditional variance is\ntime-varying. Therefore, we use Kalman filter for state tracking. We show\nanalytically that the proposed NNH model is superior to the traditional\nmethods. Furthermore, to validate the findings, simulations are performed.\nFinally, the comparison between the proposed method and other alternatives\ntechniques has been made.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 17:04:06 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Fouladi", "Seyyed Hamed", ""], ["Hajiramezanali", "Ehsan", ""]]}, {"id": "1902.04671", "submitter": "Ehsan Hajiramezanali", "authors": "Ehsan Hajiramezanali, Seyyed Hamed Fouladi, Hamidreza Amindavar", "title": "A Novel Maneuvering Target Tracking Approach by Stochastic Volatility\n  GARCH Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new single model maneuvering target tracking\napproach using stochastic differential equation (SDE) based on GARCH\nvolatility. The traditional input estimation (IE) techniques assume constant\nacceleration level which do not cover all the possible acceleration\nquintessence. In contrast, the multiple model (MM) algorithms that take care of\nsome IE's shortcomings, are sensitive to the transition probability matrices.\nIn this paper, an innovative model is proposed to overcome these drawbacks by\nusing a new generalized dynamic modeling of acceleration and a Bayesian filter.\nWe utilize SDE to model Markovian jump acceleration of a maneuvering target\nthrough GARCH process as the SDE volatility. In the proposed scheme, the\noriginal state and stochastic volatility (SV) are estimated simultaneously by a\nbootstrap particle filter (PF). We introduce the bootstrap resampling to obtain\nthe statistical properties of a GARCH density. Due to the heavy-tailed nature\nof the GARCH distribution, the bootstrap PF is more effective in the presence\nof large errors that can occur in the state equation. We show analytically that\nthe target tracking performance is improved by considering GARCH acceleration\nmodel. Finally, the effectiveness and capabilities of our proposed strategy\n(PF-AR-GARCH) are demonstrated and validated through simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 23:24:44 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Hajiramezanali", "Ehsan", ""], ["Fouladi", "Seyyed Hamed", ""], ["Amindavar", "Hamidreza", ""]]}, {"id": "1902.04727", "submitter": "Michael LuValle", "authors": "M. LuValle", "title": "A simple statistical approach to prediction in open high dimensional\n  chaotic systems", "comments": "19 pages, 5 figures, 5 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two recent papers on prediction of chaotic systems, one on multi-view\nembedding1 , and the second on prediction in projection2 provide empirical\nevidence to support particular prediction methods for chaotic systems.\nMulti-view embedding1 is a method of using several multivariate time series to\ncome up with an improved embedding based predictor of a chaotic time series.\nPrediction in projection2 discusses how much smaller embeddings can provide\nuseful prediction even though they may not be able to resolve the dynamics of\nthe system. Both papers invoke a nearest neighbor3, or Lorenz method of\nAnalogue (LMA)4 approach to estimation. However with open high dimensional\nchaotic systems there may be no very close nearest neighbor trajectories in a\nhistory, so in this paper we add in the thread of linear response theory5,\nalthough our approach is quite simple assuming that linear regressions6 on\nmultiple embeddings5 will be sufficient. The approach is thus to create linear\nresponse models5 of multiple1 low dimensional embeddings2 to provide practical\nmethods of predicting in high dimensional, open chaotic systems. Some theory is\ndeveloped, mostly around two unproven conjectures to suggest methods of\nprediction, and they are applied to prediction in the earth's climate system\n(predicting regional rainfall in a small region) multiple seasons ahead and to\npower a simulated automated trading program applied to three stock indexes, the\nDow Jones industrial average, the Dow Jones transportation average, and the Dow\nJones utility average, based on data from 1929 through 20077\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 03:36:16 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["LuValle", "M.", ""]]}, {"id": "1902.04732", "submitter": "Michael LuValle", "authors": "Parsa Rastin, Michael LuValle", "title": "Statistical Failure Mechanism Analysis of Earthquakes Revealing Time\n  Relationships", "comments": "8 pages, 3 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If we assume that earthquakes are chaotic, and influenced locally then chaos\ntheory suggests that there should be a temporal association between earthquakes\nin a local region that should be revealed with statistical examination. To date\nno strong relationship has been shown (refs not prediction). However,\nearthquakes are basically failures of structured material systems, and when\nmultiple failure mechanisms are present, prediction of failure is strongly\ninhibited without first separating the mechanisms. Here we show that by\nseparating earthquakes statistically, based on their central tensor moment\nstructure, along lines first suggested by a separation into mechanisms\naccording to depth of the earthquake, a strong indication of temporal\nassociation appears. We show this in earthquakes above 200 Km along the pacific\nring of fire, with a positive association in time between earthquakes of the\nsame statistical type and a negative association in time between earthquakes of\ndifferent types. Whether this can reveal either useful mechanistic information\nto seismologists, or can result in useful forecasts remains to be seen.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 03:49:24 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Rastin", "Parsa", ""], ["LuValle", "Michael", ""]]}, {"id": "1902.04931", "submitter": "Guillaume Damblin", "authors": "Guillaume Damblin, Pierre Gaillard", "title": "Bayesian inference and non-linear extensions of the CIRCE method for\n  quantifying the uncertainty of closure relationships integrated into\n  thermal-hydraulic system codes", "comments": "37 pages, 5 figures", "journal-ref": "Nuclear Engineering and Design, 2020, Volume 359, 1 April 2020,\n  110391", "doi": "10.1016/j.nucengdes.2019.110391", "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty Quantification of closure relationships integrated into\nthermal-hydraulic system codes is a critical prerequisite in applying the\nBest-Estimate Plus Uncertainty (BEPU) methodology for nuclear safety and\nlicensing processes.The purpose of the CIRCE method is to estimate the\n(log)-Gaussian probability distribution of a multiplicative factor applied to a\nreference closure relationship in order to assess its uncertainty. Even though\nthis method has been implemented with success in numerous physical scenarios,\nit can still suffer from substantial limitations such as the linearity\nassumption and the difficulty of properly taking into account the inherent\nstatistical uncertainty. In the paper, we will extend the CIRCE method in two\naspects. On the one hand, we adopt the Bayesian setting putting prior\nprobability distributions on the parameters of the (log)-Gaussian distribution.\nThe posterior distribution of the parameters is then computed with respect to\nan experimental database by means of Markov Chain Monte Carlo (MCMC)\nalgorithms. On the other hand, we tackle the more general setting where the\nsimulations do not move linearly against the multiplicative factor(s). MCMC\nalgorithms then become time-prohibitive when the thermal-hydraulic simulations\nexceed a few minutes. This handicap is overcome by using Gaussian process (GP)\nemulators which can yield both reliable and fast predictions of the\nsimulations. The GP-based MCMC algorithms will be applied to quantify the\nuncertainty of two condensation closure relationships at a safety injection\nwith respect to a database of experimental tests. The thermal-hydraulic\nsimulations will be run with the CATHARE 2 computer code.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 14:43:40 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 08:51:16 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Damblin", "Guillaume", ""], ["Gaillard", "Pierre", ""]]}, {"id": "1902.04944", "submitter": "Trivik Verma", "authors": "T. Verma, L. Rebelo, N. A. M. Ara\\'ujo", "title": "Impact of Inter-Country Distances on International Tourism", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0225315", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tourism is a worldwide practice with international tourism revenues\nincreasing from US\\$495 billion in 2000 to US\\$1340 billion in 2017. Its\nrelevance to the economy of many countries is obvious. Even though the World\nAirline Network (WAN) is global and has a peculiar construction, the\nInternational Tourism Network (ITN) is very similar to a random network and\nbarely global in its reach. To understand the impact of global distances on\nlocal flows, we map the flow of tourists around the world onto a complex\nnetwork and study its topological and dynamical balance. We find that although\nthe WAN serves as infrastructural support for the ITN, the flow of tourism does\nnot correlate strongly with the extent of flight connections worldwide.\nInstead, unidirectional flows appear locally forming communities that shed\nlight on global travelling behaviour inasmuch as there is only a 15%\nprobability of finding bidirectional tourism between a pair of countries. We\nconjecture that this is a consequence of one-way cyclic tourism by analyzing\nthe triangles that are formed by the network of flows in the ITN. Finally, we\nfind that most tourists travel to neighbouring countries and mainly cover\nlarger distances when there is a direct flight, irrespective of the time it\ntakes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:06:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Verma", "T.", ""], ["Rebelo", "L.", ""], ["Ara\u00fajo", "N. A. M.", ""]]}, {"id": "1902.04964", "submitter": "Hidetoshi Shimodaira", "authors": "Hidetoshi Shimodaira and Yoshikazu Terada", "title": "Selective Inference for Testing Trees and Edges in Phylogenetics", "comments": null, "journal-ref": "Frontiers in Ecology and Evolution 7:174, 2019", "doi": "10.3389/fevo.2019.00174", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective inference is considered for testing trees and edges in phylogenetic\ntree selection from molecular sequences. This improves the previously proposed\napproximately unbiased test by adjusting the selection bias when testing many\ntrees and edges at the same time. The newly proposed selective inference\n$p$-value is useful for testing selected edges to claim that they are\nsignificantly supported if $p>1-\\alpha$, whereas the non-selective $p$-value is\nstill useful for testing candidate trees to claim that they are rejected if\n$p<\\alpha$. The selective $p$-value controls the type-I error conditioned on\nthe selection event, whereas the non-selective $p$-value controls it\nunconditionally. The selective and non-selective approximately unbiased\n$p$-values are computed from two geometric quantities called signed distance\nand mean curvature of the region representing tree or edge of interest in the\nspace of probability distributions. These two geometric quantities are\nestimated by fitting a model of scaling-law to the non-parametric multiscale\nbootstrap probabilities. Our general method is applicable to a wider class of\nproblems; phylogenetic tree selection is an example of model selection, and it\nis interpreted as the variable selection of multiple regression, where each\nedge corresponds to each predictor. Our method is illustrated in a previously\ncontroversial phylogenetic analysis of human, rabbit and mouse.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:48:22 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 15:24:40 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Shimodaira", "Hidetoshi", ""], ["Terada", "Yoshikazu", ""]]}, {"id": "1902.05015", "submitter": "Konstantinos Pelechrinis", "authors": "Sara Daraei, Konstantinos Pelechrinis, Daniele Quercia", "title": "A Data-Driven Approach for Assessing Biking Safety in Cities", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the focus that cities around the world have put on sustainable\ntransportation during the past few years, biking has become one of the foci for\nlocal governments around the world. Cities all over the world invest in bike\ninfrastructure, including bike lanes, bike parking racks, shared (dockless)\nbike systems etc. However, one of the critical factors in converting\ncity-dwellers to (regular) bike users/commuters is safety. In this work, we\nutilize bike accident data from different cities to model the biking safety\nbased on street-level (geographical and infrastructural) features. Our\nevaluations indicate that our model provides well-calibrated probabilities that\naccurately capture the risk of a biking accident. We further perform cross-city\ncomparisons in order to explore whether there are universal features that\nrelate to cycling safety. Finally, we discuss and showcase how our model can be\nutilized to explore \"what-if\" scenarios and facilitate policy decision making.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 17:18:27 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Daraei", "Sara", ""], ["Pelechrinis", "Konstantinos", ""], ["Quercia", "Daniele", ""]]}, {"id": "1902.05083", "submitter": "Nicolas Le Roux", "authors": "Nicolas Le Roux", "title": "Anytime Tail Averaging", "comments": "Added a specific section on the case of multiple accumulators when\n  k_t is a constant", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tail averaging consists in averaging the last examples in a stream. Common\ntechniques either have a memory requirement which grows with the number of\nsamples to average, are not available at every timestep or do not accomodate\ngrowing windows. We propose two techniques with a low constant memory cost that\nperform tail averaging with access to the average at every time step. We also\nshow how one can improve the accuracy of that average at the cost of increased\nmemory consumption.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 19:00:25 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 19:00:20 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Roux", "Nicolas Le", ""]]}, {"id": "1902.05173", "submitter": "Syed Rahman", "authors": "Syed Rahman, Kshitij Khare, George Michailidis, Carlos Martinez and\n  Juan Carulla", "title": "Estimation of Gaussian directed acyclic graphs using partial ordering\n  information with an application to dairy cattle data", "comments": "23 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a directed acyclic graph (DAG) from observational data represents\na canonical learning problem and has generated a lot of interest in recent\nyears. Research has focused mostly on the following two cases: when no\ninformation regarding the ordering of the nodes in the DAG is available, and\nwhen a domain-specific complete ordering of the nodes is available. In this\npaper, motivated by a recent application in dairy science, we develop a method\nfor DAG estimation for the middle scenario, where partition based partial\nordering of the nodes is known based on domain specific knowledge.We develop an\nefficient algorithm that solves the posited problem, coined Partition-DAG.\nThrough extensive simulations using the DREAM3 Yeast data, we illustrate that\nPartition-DAG effectively incorporates the partial ordering information to\nimprove both speed and accuracy. We then illustrate the usefulness of\nPartition-DAG by applying it to recently collected dairy cattle data, and\ninferring relationships between various variables involved in dairy\nagroecosystems.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 00:31:26 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Rahman", "Syed", ""], ["Khare", "Kshitij", ""], ["Michailidis", "George", ""], ["Martinez", "Carlos", ""], ["Carulla", "Juan", ""]]}, {"id": "1902.05189", "submitter": "Kevin Keys", "authors": "Hua Zhou, Janet S. Sinsheimer, Christopher A. German, Sarah S. Ji,\n  Douglas M. Bates, Benjamin B. Chu, Kevin L. Keys, Juhyun Kim, Seyoon Ko,\n  Gordon D. Mosher, Jeanette C. Papp, Eric M. Sobel, Jing Zhai, Jin J. Zhou and\n  Kenneth Lange", "title": "OPENMENDEL: A Cooperative Programming Project for Statistical Genetics", "comments": "16 pages, 2 figures, 2 tables", "journal-ref": "Human Genetics, pp 1-11, 2019 Mar 26", "doi": "10.1007/s00439-019-02001-z", "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods for genomewide association studies (GWAS) continue to\nimprove. However, the increasing volume and variety of genetic and genomic data\nmake computational speed and ease of data manipulation mandatory in future\nsoftware. In our view, a collaborative effort of statistical geneticists is\nrequired to develop open source software targeted to genetic epidemiology. Our\nattempt to meet this need is called the OPENMENDELproject\n(https://openmendel.github.io). It aims to (1) enable interactive and\nreproducible analyses with informative intermediate results, (2) scale to big\ndata analytics, (3) embrace parallel and distributed computing, (4) adapt to\nrapid hardware evolution, (5) allow cloud computing, (6) allow integration of\nvaried genetic data types, and (7) foster easy communication between\nclinicians, geneticists, statisticians, and computer scientists. This article\nreviews and makes recommendations to the genetic epidemiology community in the\ncontext of the OPENMENDEL project.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 02:09:32 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Zhou", "Hua", ""], ["Sinsheimer", "Janet S.", ""], ["German", "Christopher A.", ""], ["Ji", "Sarah S.", ""], ["Bates", "Douglas M.", ""], ["Chu", "Benjamin B.", ""], ["Keys", "Kevin L.", ""], ["Kim", "Juhyun", ""], ["Ko", "Seyoon", ""], ["Mosher", "Gordon D.", ""], ["Papp", "Jeanette C.", ""], ["Sobel", "Eric M.", ""], ["Zhai", "Jing", ""], ["Zhou", "Jin J.", ""], ["Lange", "Kenneth", ""]]}, {"id": "1902.05499", "submitter": "Crystal Nguyen", "authors": "Crystal T. Nguyen (1), Daniel J. Luckett (1), Anna R. Kahkoska (2),\n  Grace E. Shearrer (2), Donna Spruijt-Metz (3), Jaimie N. Davis (4), and\n  Michael R. Kosorok (1) ((1) Department of Biostatistics, University of North\n  Carolina, Chapel Hill, North Carolina, U.S.A., (2) Department of Nutrition,\n  University of North Carolina, Chapel Hill, U.S.A., (3) Center of Economic and\n  Social Research, University of Southern California, Los Angeles, California,\n  U.S.A., (4) Department of Nutrition, University of Texas, Austin, Texas,\n  U.S.A.)", "title": "Estimating Individualized Treatment Regimes from Crossover Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of precision medicine aims to tailor treatment based on\npatient-specific factors in a reproducible way. To this end, estimating an\noptimal individualized treatment regime (ITR) that recommends treatment\ndecisions based on patient characteristics to maximize the mean of a\npre-specified outcome is of particular interest. Several methods have been\nproposed for estimating an optimal ITR from clinical trial data in the parallel\ngroup setting where each subject is randomized to a single intervention.\nHowever, little work has been done in the area of estimating the optimal ITR\nfrom crossover study designs. Such designs naturally lend themselves to\nprecision medicine, because they allow for observing the response to multiple\ntreatments for each patient. In this paper, we introduce a method for\nestimating the optimal ITR using data from a 2x2 crossover study with or\nwithout carryover effects. The proposed method is similar to policy search\nmethods such as outcome weighted learning; however, we take advantage of the\ncrossover design by using the difference in responses under each treatment as\nthe observed reward. We establish Fisher and global consistency, present\nnumerical experiments, and analyze data from a feeding trial to demonstrate the\nimproved performance of the proposed method compared to standard methods for a\nparallel study design.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 02:28:33 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Nguyen", "Crystal T.", ""], ["Luckett", "Daniel J.", ""], ["Kahkoska", "Anna R.", ""], ["Shearrer", "Grace E.", ""], ["Spruijt-Metz", "Donna", ""], ["Davis", "Jaimie N.", ""], ["Kosorok", "Michael R.", ""]]}, {"id": "1902.05527", "submitter": "Lorenzo Cappello", "authors": "Lorenzo Cappello and Julia A. Palacios", "title": "Sequential importance sampling for multi-resolution Kingman-Tajima\n  coalescent counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference of evolutionary parameters from molecular sequence data\nrelies on coalescent models to account for the shared genealogical ancestry of\nthe samples. However, inferential algorithms do not scale to available data\nsets. A strategy to improve computational efficiency is to rely on simpler\ncoalescent and mutation models, resulting in smaller hidden state spaces. An\nestimate of the cardinality of the state-space of genealogical trees at\ndifferent resolutions is essential to decide the best modeling strategy for a\ngiven dataset. To our knowledge, there is neither an exact nor approximate\nmethod to determine these cardinalities. We propose a sequential importance\nsampling algorithm to estimate the cardinality of the space of genealogical\ntrees under different coalescent resolutions. Our sampling scheme proceeds\nsequentially across the set of combinatorial constraints imposed by the data.\nWe analyse the cardinality of different genealogical tree spaces on simulations\nto study the settings that favor coarser resolutions. We estimate the\ncardinality of genealogical tree spaces from mtDNA data from the 1000 genomes\nand a sample from a Melanesian population to illustrate the settings in which\nit is advantageous to employ coarser resolutions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 18:07:26 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2019 21:56:15 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Cappello", "Lorenzo", ""], ["Palacios", "Julia A.", ""]]}, {"id": "1902.05680", "submitter": "Yanxun Xu", "authors": "Yuliang Li, Dipankar Bandyopadhyay, Fangzheng Xie, Yanxun Xu", "title": "BAREB: A Bayesian repulsive biclustering model for periodontal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventing periodontal diseases (PD) and maintaining the structure and\nfunction of teeth are important goals for personal oral care. To understand the\nheterogeneity in patients with diverse PD patterns, we develop BAREB, a\nBayesian repulsive biclustering method that can simultaneously cluster the PD\npatients and their tooth sites after taking the patient- and site- level\ncovariates into consideration. BAREB uses the determinantal point process (DPP)\nprior to induce diversity among different biclusters to facilitate parsimony\nand interpretability. Since PD progression is hypothesized to be\nspatially-referenced, BAREB factors in the spatial dependence among tooth\nsites. In addition, since PD is the leading cause for tooth loss, the missing\ndata mechanism is non-ignorable. Such nonrandom missingness is incorporated\ninto BAREB. For the posterior inference, we design an efficient reversible jump\nMarkov chain Monte Carlo sampler. Simulation studies show that BAREB is able to\naccurately estimate the biclusters, and compares favorably to alternatives. For\nreal world application, we apply BAREB to a dataset from a clinical PD study,\nand obtain desirable and interpretable results. A major contribution of this\npaper is the Rcpp implementation of BAREB, available at\nhttps://github.com/YanxunXu/ BAREB.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 04:10:23 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 09:47:58 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Li", "Yuliang", ""], ["Bandyopadhyay", "Dipankar", ""], ["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1902.05764", "submitter": "Pejman Farhadi Ghalati", "authors": "Pejman F. Ghalati, Satya S. Samal, Jayesh S. Bhat, Robert Deisz,\n  Gernot Marx and Andreas Schuppert", "title": "Critical Transitions in Intensive Care Units: A Sepsis Case Study", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": "Scientific Reports, 9(1) (2019)", "doi": "10.1038/s41598-019-49006-2", "report-no": null, "categories": "q-bio.QM cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progression of complex human diseases is associated with critical\ntransitions across dynamical regimes. These transitions often spawn\nearly-warning signals and provide insights into the underlying disease-driving\nmechanisms. In this paper, we propose a computational method based on surprise\nloss (SL) to discover data-driven indicators of such transitions in a\nmultivariate time series dataset of septic shock and non-sepsis patient cohorts\n(MIMIC-III database). The core idea of SL is to train a mathematical model on\ntime series in an unsupervised fashion and to quantify the deterioration of the\nmodel's forecast (out-of-sample) performance relative to its past (in-sample)\nperformance. Considering the highest value of the moving average of SL as a\ncritical transition, our retrospective analysis revealed that critical\ntransitions occurred at a median of over 35 hours before the onset of septic\nshock, which suggests the applicability of our method as an early-warning\nindicator. Furthermore, we show that clinical variables at critical-transition\nregions are significantly different between septic shock and non-sepsis\ncohorts. Therefore, our paper contributes a critical-transition-based\ndata-sampling strategy that can be utilized for further analysis, such as\npatient classification. Moreover, our method outperformed other indicators of\ncritical transition in complex systems, such as temporal autocorrelation and\nvariance.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 11:00:51 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 09:34:47 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Ghalati", "Pejman F.", ""], ["Samal", "Satya S.", ""], ["Bhat", "Jayesh S.", ""], ["Deisz", "Robert", ""], ["Marx", "Gernot", ""], ["Schuppert", "Andreas", ""]]}, {"id": "1902.05977", "submitter": "Mark Risser", "authors": "Mark D. Risser, Christopher J. Paciorek, Travis A. O'Brien, Michael F.\n  Wehner, and William D. Collins", "title": "Detected changes in precipitation extremes at their native scales\n  derived from in situ measurements", "comments": null, "journal-ref": null, "doi": "10.1175/JCLI-D-19-0077.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gridding of daily accumulated precipitation -- especially extremes --\nfrom ground-based station observations is problematic due to the fractal nature\nof precipitation, and therefore estimates of long period return values and\ntheir changes based on such gridded daily data sets are generally\nunderestimated. In this paper, we characterize high-resolution changes in\nobserved extreme precipitation from 1950 to 2017 for the contiguous United\nStates (CONUS) based on in situ measurements only. Our analysis utilizes\nspatial statistical methods that allow us to derive gridded estimates that do\nnot smooth extreme daily measurements and are consistent with statistics from\nthe original station data while increasing the resulting signal to noise ratio.\nFurthermore, we use a robust statistical technique to identify significant\npointwise changes in the climatology of extreme precipitation while carefully\ncontrolling the rate of false positives. We present and discuss seasonal\nchanges in the statistics of extreme precipitation: the largest and most\nspatially-coherent pointwise changes are in fall (SON), with approximately 33%\nof CONUS exhibiting significant changes (in an absolute sense). Other seasons\ndisplay very few meaningful pointwise changes (in either a relative or absolute\nsense), illustrating the difficulty in detecting pointwise changes in extreme\nprecipitation based on in situ measurements. While our main result involves\nseasonal changes, we also present and discuss annual changes in the statistics\nof extreme precipitation. In this paper we only seek to detect changes over\ntime and leave attribution of the underlying causes of these changes for future\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 19:53:43 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 19:51:01 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 15:38:27 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Risser", "Mark D.", ""], ["Paciorek", "Christopher J.", ""], ["O'Brien", "Travis A.", ""], ["Wehner", "Michael F.", ""], ["Collins", "William D.", ""]]}, {"id": "1902.05979", "submitter": "Benjamin Jamroz", "authors": "Michael Frey, Benjamin F. Jamroz, Amanda Koepke, Jacob D. Rezac, Dylan\n  Williams", "title": "Monte Carlo Sampling Bias in the Microwave Uncertainty Framework", "comments": null, "journal-ref": null, "doi": "10.1088/1681-7575/ab2c18", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty propagation software can have unknown, inadvertent biases\nintroduced by various means. This work is a case study in bias identification\nand reduction in one such software package, the Microwave Uncertainty Framework\n(MUF). The general purpose of the MUF is to provide automated multivariate\nstatistical uncertainty propagation and analysis on a Monte Carlo (MC) basis.\nCombine is a key module in the MUF, responsible for merging data, raw or\ntransformed, to accurately reflect the variability in the data and in its\ncentral tendency. In this work the performance of Combine's MC replicates is\nanalytically compared against its stated design goals. An alternative\nconstruction is proposed for Combine's MC replicates and its performance is\ncompared, too, against Combine's design goals. These comparisons are made\nwithin an archetypal two-stage scenario in which received data are first\ntransformed in conjunction with shared systematic error and then combined to\nproduce summary information. These comparisons reveal the limited conditions\nunder which Combine's uncertainty results are unbiased and the extent of these\nbiases when these conditions are not met. For small MC sample sizes neither\nconstruction, current or alternative, fully meets Combine's design goals, nor\ndoes either construction consistently outperform the other. However, for large\nMC sample sizes the bias in the proposed alternative construction is\nasymptotically zero, and this construction is recommended.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 20:11:23 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Frey", "Michael", ""], ["Jamroz", "Benjamin F.", ""], ["Koepke", "Amanda", ""], ["Rezac", "Jacob D.", ""], ["Williams", "Dylan", ""]]}, {"id": "1902.06078", "submitter": "Bernard Silverman", "authors": "Bernard W. Silverman", "title": "Model fitting in Multiple Systems Analysis for the quantification of\n  Modern Slavery: Classical and Bayesian approaches", "comments": "31 pages. Version 3. (Original version July 2018)", "journal-ref": "Journal of the Royal Statistical Society, Series A. (2020) with\n  Discussion", "doi": "10.1111/rssa.12505", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple systems estimation is a key approach for quantifying hidden\npopulations such as the number of victims of modern slavery. The UK Government\npublished an estimate of 10,000 to 13,000 victims, constructed by the present\nauthor, as part of the strategy leading to the Modern Slavery Act 2015. This\nestimate was obtained by a stepwise multiple systems method based on six lists.\nFurther investigation shows that a small proportion of the possible models give\nrather different answers, and that other model fitting approaches may choose\none of these. Three data sets collected in the field of modern slavery,\ntogether with a data set about the death toll in the Kosovo conflict, are used\nto investigate the stability and robustness of various multiple systems\nestimate methods. The crucial aspect is the way that interactions between lists\nare modelled, because these can substantially affect the results. Model\nselection and Bayesian approaches are considered in detail, in particular to\nassess their stability and robustness when applied to real modern slavery data.\nA new Markov Chain Monte Carlo Bayesian approach is developed; overall, this\ngives robust and stable results at least for the examples considered. The\nsoftware and datasets are freely and publicly available to facilitate wider\nimplementation and further research.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 10:01:20 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 10:15:30 GMT"}, {"version": "v3", "created": "Sun, 11 Aug 2019 21:36:43 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Silverman", "Bernard W.", ""]]}, {"id": "1902.06175", "submitter": "Jason Anquandah", "authors": "Jason S. Anquandah and Leonid V. Bogachev", "title": "Optimal Stopping and Utility in a Simple Model of Unemployment Insurance", "comments": "45 pages, 8 figures", "journal-ref": "Risks, vol. 7 (2019), issue 3, paper #94, pages 1-41; Special\n  Issue \"Applications of Stochastic Optimal Control to Economics and Finance\"", "doi": "10.3390/risks7030094", "report-no": null, "categories": "q-fin.ST math.OC q-fin.MF stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Managing unemployment is one of the key issues in social policies.\nUnemployment insurance schemes are designed to cushion the financial and morale\nblow of loss of job but also to encourage the unemployed to seek new jobs more\npro-actively due to the continuous reduction of benefit payments. In the\npresent paper, a simple model of unemployment insurance is proposed with a\nfocus on optimality of the individual's entry to the scheme. The corresponding\noptimal stopping problem is solved, and its similarity and differences with the\nperpetual American call option are discussed. Beyond a purely financial point\nof view, we argue that in the actuarial context the optimal decisions should\ntake into account other possible preferences through a suitable utility\nfunction. Some examples in this direction are worked out.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 23:28:25 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 14:15:05 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 09:24:33 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Anquandah", "Jason S.", ""], ["Bogachev", "Leonid V.", ""]]}, {"id": "1902.06183", "submitter": "Arnab Chakraborty", "authors": "Arnab Chakraborty and Soumendra Nath Lahiri and Alyson Wilson", "title": "A Statistical Analysis of Noisy Crowdsourced Weather Data", "comments": "Submitted to Annals of Applied Statistics", "journal-ref": "A statistical analysis of noisy crowdsourced weather data, Annals\n  of Applied Statistics 2020, Vol. 14, No. 1, 116-142", "doi": "10.1214/19-AOAS1290", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction of weather-elements like temperature, precipitation, and\nbarometric pressure are generally based on satellite imagery or data collected\nat ground-stations. None of these data provide information at a more granular\nor \"hyper-local\" resolution. On the other hand, crowdsourced weather data,\nwhich are captured by sensors installed on mobile devices and gathered by\nweather-related mobile apps like WeatherSignal and AccuWeather, can serve as\npotential data sources for analyzing environmental processes at a hyper-local\nresolution. However, due to the low quality of the sensors and the\nnon-laboratory environment, the quality of the observations in crowdsourced\ndata is compromised. This paper describes methods to improve hyper-local\nspatial prediction using this varying-quality noisy crowdsourced information.\nWe introduce a reliability metric, namely Veracity Score (VS), to assess the\nquality of the crowdsourced observations using a coarser, but high-quality,\nreference data. A VS-based methodology to analyze noisy spatial data is\nproposed and evaluated through extensive simulations. The merits of the\nproposed approach are illustrated through case studies analyzing crowdsourced\ndaily average ambient temperature readings for one day in the contiguous United\nStates.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 01:31:58 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 15:37:44 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chakraborty", "Arnab", ""], ["Lahiri", "Soumendra Nath", ""], ["Wilson", "Alyson", ""]]}, {"id": "1902.06315", "submitter": "Paulo Hubert", "authors": "Paulo Hubert and Rebecca Killick and Alexandra Chung and Linilson\n  Padovese", "title": "A Bayesian binary algorithm for RMS-based acoustic signal segmentation", "comments": null, "journal-ref": null, "doi": "10.1121/1.5126522", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changepoint analysis (also known as segmentation analysis) aims at analyzing\nan ordered, one-dimensional vector, in order to find locations where some\ncharacteristic of the data changes. Many models and algorithms have been\nstudied under this theme, including models for changes in mean and / or\nvariance, changes in linear regression parameters, etc. In this work, we are\ninterested in an algorithm for the segmentation of long duration acoustic\nsignals; the segmentation is based on the change of the RMS power of the\nsignal. We investigate a Bayesian model with two possible parameterizations,\nand propose a binary algorithm in two versions, using non-informative or\ninformative priors. We apply our algorithm to the segmentation of annotated\nacoustic signals from the Alcatrazes marine preservation park in Brazil.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 19:59:36 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 00:17:53 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Hubert", "Paulo", ""], ["Killick", "Rebecca", ""], ["Chung", "Alexandra", ""], ["Padovese", "Linilson", ""]]}, {"id": "1902.06351", "submitter": "Priyanga Dilini Talagala", "authors": "Priyanga Dilini Talagala, Rob J. Hyndman, Catherine Leigh, Kerrie\n  Mengersen, Kate Smith-Miles", "title": "A feature-based framework for detecting technical outliers in\n  water-quality data from in situ sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outliers due to technical errors in water-quality data from in situ sensors\ncan reduce data quality and have a direct impact on inference drawn from\nsubsequent data analysis. However, outlier detection through manual monitoring\nis unfeasible given the volume and velocity of data the sensors produce. Here,\nwe proposed an automated framework that provides early detection of outliers in\nwater-quality data from in situ sensors caused by technical issues.The\nframework was used first to identify the data features that differentiate\noutlying instances from typical behaviours. Then statistical transformations\nwere applied to make the outlying instances stand out in transformed data\nspace. Unsupervised outlier scoring techniques were then applied to the\ntransformed data space and an approach based on extreme value theory was used\nto calculate a threshold for each potential outlier. Using two data sets\nobtained from in situ sensors in rivers flowing into the Great Barrier Reef\nlagoon, Australia, we showed that the proposed framework successfully\nidentified outliers involving abrupt changes in turbidity, conductivity and\nriver level, including sudden spikes, sudden isolated drops and level shifts,\nwhile maintaining very low false detection rates. We implemented this framework\nin the open source R package oddwater.\n", "versions": [{"version": "v1", "created": "Sun, 17 Feb 2019 23:46:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Talagala", "Priyanga Dilini", ""], ["Hyndman", "Rob J.", ""], ["Leigh", "Catherine", ""], ["Mengersen", "Kerrie", ""], ["Smith-Miles", "Kate", ""]]}, {"id": "1902.06453", "submitter": "Raoul Heese", "authors": "Raoul Heese, Michal Walczak, Tobias Seidel, Norbert Asprion, Michael\n  Bortz", "title": "Optimized data exploration applied to the simulation of a chemical\n  process", "comments": "45 pages, 6 figures", "journal-ref": "Computers & Chemical Engineering, 2019", "doi": "10.1016/j.compchemeng.2019.01.007", "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex simulation environments, certain parameter space regions may\nresult in non-convergent or unphysical outcomes. All parameters can therefore\nbe labeled with a binary class describing whether or not they lead to valid\nresults. In general, it can be very difficult to determine feasible parameter\nregions, especially without previous knowledge. We propose a novel algorithm to\nexplore such an unknown parameter space and improve its feasibility\nclassification in an iterative way. Moreover, we include an additional\noptimization target in the algorithm to guide the exploration towards regions\nof interest and to improve the classification therein. In our method we make\nuse of well-established concepts from the field of machine learning like kernel\nsupport vector machines and kernel ridge regression. From a comparison with a\nKriging-based exploration approach based on recently published results we can\nshow the advantages of our algorithm in a binary feasibility classification\nscenario with a discrete feasibility constraint violation. In this context, we\nalso propose an improvement of the Kriging-based exploration approach. We apply\nour novel method to a fully realistic, industrially relevant chemical process\nsimulation to demonstrate its practical usability and find a comparably good\napproximation of the data space topology from relatively few data points.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 08:26:10 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Heese", "Raoul", ""], ["Walczak", "Michal", ""], ["Seidel", "Tobias", ""], ["Asprion", "Norbert", ""], ["Bortz", "Michael", ""]]}, {"id": "1902.06688", "submitter": "Alma Rosa M\\'endez Rodriguez", "authors": "W. Marques Jr. and A. R. Mendez and R. M. Velasco", "title": "Fundamental Diagram of Traffic Flow from Prigogine-Herman-Enskog\n  Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications of a new methodology to measure fundamental traffic\nrelations on freeways shows that many of the critical parameters of the\nflow-density and speed-spacing diagrams depend on vehicle length. In response\nto this fact, we present in this work a generalization of the Prigogine-Herman\ntraffic equation for aggressive drivers which takes into account the fact that\nvehicles are not point-like objects but have an effective length. Our approach\nis similar to that introduced by Enskog for dense gases and provides the\nconstruction of fundamental diagrams which are in excellent agreement with\nempirical traffic data.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 05:11:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Marques", "W.", "Jr."], ["Mendez", "A. R.", ""], ["Velasco", "R. M.", ""]]}, {"id": "1902.07133", "submitter": "Craig Tutterow", "authors": "Craig Tutterow and Guillaume Saint-Jacques", "title": "Estimating Network Effects Using Naturally Occurring Peer Notification\n  Queue Counterfactuals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments, or A/B tests are used to estimate the causal impact\nof a feature on the behavior of users by creating two parallel universes in\nwhich members are simultaneously assigned to treatment and control. However, in\nsocial network settings, members interact, such that the impact of a feature is\nnot always contained within the treatment group. Researchers have developed a\nnumber of experimental designs to estimate network effects in social settings.\nAlternatively, naturally occurring exogenous variation, or 'natural\nexperiments,' allow researchers to recover causal estimates of peer effects\nfrom observational data in the absence of experimental manipulation. Natural\nexperiments trade off the engineering costs and some of the ethical concerns\nassociated with network randomization with the search costs of finding\nsituations with natural exogenous variation. To mitigate the search costs\nassociated with discovering natural counterfactuals, we identify a common\nengineering requirement used to scale massive online systems, in which natural\nexogenous variation is likely to exist: notification queueing. We identify two\nnatural experiments on the LinkedIn platform based on the order of notification\nqueues to estimate the causal impact of a received message on the engagement of\na recipient. We show that receiving a message from another member significantly\nincreases a member's engagement, but that some popular observational\nspecifications, such as fixed-effects estimators, overestimate this effect by\nas much as 2.7x. We then apply the estimated network effect coefficients to a\nlarge body of past experiments to quantify the extent to which it changes our\ninterpretation of experimental results. The study points to the benefits of\nusing messaging queues to discover naturally occurring counterfactuals for the\nestimation of causal effects without experimenter intervention.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 16:44:08 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Tutterow", "Craig", ""], ["Saint-Jacques", "Guillaume", ""]]}, {"id": "1902.07276", "submitter": "Tellen Bennett", "authors": "Tellen Bennett, Seth Russell, James King, Lisa Schilling, Chan Voong,\n  Nancy Rogers, Bonnie Adrian, Nicholas Bruce, Debashis Ghosh", "title": "Accuracy of the Epic Sepsis Prediction Model in a Regional Health System", "comments": "Presented at AMIA Symposium 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in an electronic health record-based computational model that can\naccurately predict a patient's risk of sepsis at a given point in time has\ngrown rapidly in the last several years. Like other EHR vendors, the Epic\nSystems Corporation has developed a proprietary sepsis prediction model (ESPM).\nEpic developed the model using data from three health systems and penalized\nlogistic regression. Demographic, comorbidity, vital sign, laboratory,\nmedication, and procedural variables contribute to the model. The objective of\nthis project was to compare the predictive performance of the ESPM with a\nregional health system's current Early Warning Score-based sepsis detection\nprogram.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 20:48:43 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Bennett", "Tellen", ""], ["Russell", "Seth", ""], ["King", "James", ""], ["Schilling", "Lisa", ""], ["Voong", "Chan", ""], ["Rogers", "Nancy", ""], ["Adrian", "Bonnie", ""], ["Bruce", "Nicholas", ""], ["Ghosh", "Debashis", ""]]}, {"id": "1902.07355", "submitter": "Kirk Bansak", "authors": "Avidit Acharya, Kirk Bansak, Jens Hainmueller", "title": "Combining Outcome-Based and Preference-Based Matching: A Constrained\n  Priority Mechanism", "comments": "This manuscript has been accepted for publication by Political\n  Analysis and will appear in a revised form subject to peer review and/or\n  input from the journal's editor. End-users of this manuscript may only make\n  use of it for private research and study and may not distribute it further", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.LG q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a constrained priority mechanism that combines outcome-based\nmatching from machine-learning with preference-based allocation schemes common\nin market design. Using real-world data, we illustrate how our mechanism could\nbe applied to the assignment of refugee families to host country locations, and\nkindergarteners to schools. Our mechanism allows a planner to first specify a\nthreshold $\\bar g$ for the minimum acceptable average outcome score that should\nbe achieved by the assignment. In the refugee matching context, this score\ncorresponds to the predicted probability of employment, while in the student\nassignment context it corresponds to standardized test scores. The mechanism is\na priority mechanism that considers both outcomes and preferences by assigning\nagents (refugee families, students) based on their preferences, but subject to\nmeeting the planner's specified threshold. The mechanism is both strategy-proof\nand constrained efficient in that it always generates a matching that is not\nPareto dominated by any other matching that respects the planner's threshold.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 00:17:33 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 01:41:28 GMT"}, {"version": "v3", "created": "Tue, 11 Aug 2020 17:22:34 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Acharya", "Avidit", ""], ["Bansak", "Kirk", ""], ["Hainmueller", "Jens", ""]]}, {"id": "1902.07378", "submitter": "Martin Ingram", "authors": "Martin Ingram", "title": "Gaussian Process Priors for Dynamic Paired Comparison Modelling", "comments": "Code is available here:\n  https://github.com/martiningram/paired-comparison-gp-laplace", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic paired comparison models, such as Elo and Glicko, are frequently used\nfor sports prediction and ranking players or teams. We present an alternative\ndynamic paired comparison model which uses a Gaussian Process (GP) as a prior\nfor the time dynamics rather than the Markovian dynamics usually assumed. In\naddition, we show that the GP model can easily incorporate covariates. We\nderive an efficient approximate Bayesian inference procedure based on the\nLaplace Approximation and sparse linear algebra. We select hyperparameters by\nmaximising their marginal likelihood using Bayesian Optimisation, comparing the\nresults against random search. Finally, we fit and evaluate the model on the\n2018 season of ATP tennis matches, where it performs competitively,\noutperforming Elo and Glicko on log loss, particularly when surface covariates\nare included.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 02:27:55 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Ingram", "Martin", ""]]}, {"id": "1902.07634", "submitter": "Chelsea Zhang", "authors": "Chelsea Zhang, Sean J. Taylor, Curtiss Cobb, and Jasjeet Sekhon", "title": "Active Matrix Factorization for Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Amid historically low response rates, survey researchers seek ways to reduce\nrespondent burden while measuring desired concepts with precision. We propose\nto ask fewer questions of respondents and impute missing responses via\nprobabilistic matrix factorization. A variance-minimizing active learning\ncriterion chooses the most informative questions per respondent. In simulations\nof our matrix sampling procedure on real-world surveys, as well as a Facebook\nsurvey experiment, we find active question selection achieves efficiency gains\nover baselines. The reduction in imputation error is heterogeneous across\nquestions, and depends on the latent concepts they capture. The imputation\nprocedure can benefit from incorporating respondent side information, modeling\nresponses as ordered logit rather than Gaussian, and accounting for order\neffects. With our method, survey researchers obtain principled suggestions of\nquestions to retain and, if desired, can automate the design of shorter\ninstruments.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 16:46:18 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 17:04:03 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Zhang", "Chelsea", ""], ["Taylor", "Sean J.", ""], ["Cobb", "Curtiss", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1902.07685", "submitter": "Mohammad Gheshlaghi Azar", "authors": "Mohammad Gheshlaghi Azar and Bilal Piot and Bernardo Avila Pires and\n  Jean-Bastien Grill and Florent Altch\\'e and R\\'emi Munos", "title": "World Discovery Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As humans we are driven by a strong desire for seeking novelty in our world.\nAlso upon observing a novel pattern we are capable of refining our\nunderstanding of the world based on the new information---humans can discover\ntheir world. The outstanding ability of the human mind for discovery has led to\nmany breakthroughs in science, art and technology. Here we investigate the\npossibility of building an agent capable of discovering its world using the\nmodern AI technology. In particular we introduce NDIGO, Neural Differential\nInformation Gain Optimisation, a self-supervised discovery model that aims at\nseeking new information to construct a global view of its world from partial\nand noisy observations. Our experiments on some controlled 2-D navigation tasks\nshow that NDIGO outperforms state-of-the-art information-seeking methods in\nterms of the quality of the learned representation. The improvement in\nperformance is particularly significant in the presence of white or structured\nnoise where other information-seeking methods follow the noise instead of\ndiscovering their world.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:07:18 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 15:21:34 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 20:25:58 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Azar", "Mohammad Gheshlaghi", ""], ["Piot", "Bilal", ""], ["Pires", "Bernardo Avila", ""], ["Grill", "Jean-Bastien", ""], ["Altch\u00e9", "Florent", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1902.07711", "submitter": "Isobel Claire Gormley Dr.", "authors": "Isobel Claire Gormley, Yuxin Bai and Lorraine Brennan", "title": "Combining biomarker and self-reported dietary intake data: a review of\n  the state of the art and an exposition of concepts", "comments": "To appear in Statistical Methods in Medical Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical approaches to assessing dietary intake are associated with\nmeasurement error. In an effort to address inherent measurement error in\ndietary self-reported data there is increased interest in the use of dietary\nbiomarkers as objective measures of intake. Furthermore, there is a growing\nconsensus of the need to combine dietary biomarker data with self-reported\ndata.\n  A review of state of the art techniques employed when combining biomarker and\nself-reported data is conducted. Two predominant methods, the calibration\nmethod and the method of triads, emerge as relevant techniques used when\ncombining biomarker and self-reported data to account for measurement errors in\ndietary intake assessment. Both methods crucially assume measurement error\nindependence. To expose and understand the performance of these methods in a\nrange of realistic settings, their underpinning statistical concepts are\nunified and delineated, and thorough simulation studies conducted.\n  Results show that violation of the methods' assumptions negatively impacts\nresulting inference but that this impact is mitigated when the variation of the\nbiomarker around the true intake is small. Thus there is much scope for the\nfurther development of biomarkers and models in tandem to achieve the ultimate\ngoal of accurately assessing dietary intake.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 14:43:17 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Gormley", "Isobel Claire", ""], ["Bai", "Yuxin", ""], ["Brennan", "Lorraine", ""]]}, {"id": "1902.07788", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Integer-Valued Functional Data Analysis for Measles Forecasting", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13110", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measles presents a unique and imminent challenge for epidemiologists and\npublic health officials: the disease is highly contagious, yet vaccination\nrates are declining precipitously in many localities. Consequently, the risk of\na measles outbreak continues to rise. To improve preparedness, we study\nhistorical measles data both pre- and post-vaccine, and design new methodology\nto forecast measles counts with uncertainty quantification. We propose to model\nthe disease counts as an integer-valued functional time series: measles counts\nare a function of time-of-year and time-ordered by year. The counts are modeled\nusing a negative-binomial distribution conditional on a real-valued latent\nprocess, which accounts for the overdispersion observed in the data. The latent\nprocess is decomposed using an unknown basis expansion, which is learned from\nthe data, with dynamic basis coefficients. The resulting framework provides\nenhanced capability to model complex seasonality, which varies dynamically from\nyear-to-year, and offers improved multi-month ahead point forecasts and\nsubstantially tighter forecast intervals (with correct coverage) compared to\nexisting forecasting models. Importantly, the fully Bayesian approach provides\nwell-calibrated and precise uncertainty quantification for epi-relevent\nfeatures, such as the future value and time of the peak measles count in a\ngiven year. An R package is available online.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 21:49:12 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "1902.07791", "submitter": "Yicheng Li", "authors": "Yicheng Li and Adrian E. Raftery", "title": "Estimating and Forecasting the Smoking-Attributable Mortality Fraction\n  for Both Genders Jointly in Over 60 Countries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoking is one of the preventable threats to human health and is a major risk\nfactor for lung cancer, upper aero-digestive cancer, and chronic obstructive\npulmonary disease. Estimating and forecasting the smoking attributable fraction\n(SAF) of mortality can yield insights into smoking epidemics and also provide a\nbasis for more accurate mortality and life expectancy projection. Peto et al.\n(1992) proposed a method to estimate the SAF using the lung cancer mortality\nrate as an indicator of exposure to smoking in the population of interest. Here\nwe use the same method to estimate the all-age SAF (ASAF) for both genders for\nover 60 countries. We document a strong and cross-nationally consistent pattern\nof the evolution of the SAF over time. We use this as the basis for a new\nBayesian hierarchical model to project future male and female ASAF from over 60\ncountries simultaneously. This gives forecasts as well as predictive\ndistributions that can be used to find uncertainty intervals for any quantities\nof interest. We assess the model using out-of-sample predictive validation, and\nfind that it provides good forecasts and well calibrated forecast intervals.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 22:00:38 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 05:43:40 GMT"}, {"version": "v3", "created": "Sun, 27 Oct 2019 02:09:42 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Li", "Yicheng", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1902.07905", "submitter": "Roel Ceballos", "authors": "Marites F. Carillo, Fe F. Largo and Roel F. Ceballos", "title": "Principal Component Analysis on the Philippine Health Data", "comments": null, "journal-ref": "International Journal of Ecological Economics and Statistics,\n  39(3), 91-97, 2018", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study was conducted to determine the structures of a set of n correlated\nvariables and creates a new set of uncorrelated indices which are the\nunderlying components of the Philippine health data.The data utilized in this\nstudy was the 2009 Philippine Health Data which was made available by National\nStatistical Coordination Board(NSCB) in its 2009 publication.The publication\ncontains the health data of 81 provinces of the Philippines consisting of ten\nsystem-related determinants which was considered as the variables in this\nstudy. From the ten health system-related determinants, it was found out that\nthere are three significant underlying components that could summarize the\nPhilippine health data. The first component was named as importance of safe\nwater supply and emphasis on child heat while the second and third component\nwere named as importance of Barangay Health Stations, government health workers\nand emphasis on pregnant women's health and emphasis on women's health,\nrespectively. These three components jointly account for a total of 73.01% of\nthe total variance explained by the component.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 08:04:41 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Carillo", "Marites F.", ""], ["Largo", "Fe F.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1902.07953", "submitter": "Roel Ceballos", "authors": "Empha Grace Perez and Roel F. Ceballos", "title": "Malaria Incidence in the Philippines: Prediction using the\n  Autoregressive Moving Average Models", "comments": null, "journal-ref": "International Journal of Engineering and Future Technology, 16(4),\n  2019", "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study was conducted to develop an appropriate model that could predict\nthe weekly reported Malaria incidence in the Philippines using the Box-Jenkins\nmethod.The data were retrieved from the Department of Health(DOH) website in\nthe Philippines. It contains 70 data points of which 60 data points were used\nin model building and the remaining 10 data points were used for forecast\nevaluation. The R Statistical Software was used to do all the necessary\ncomputations in the study. Box-Cox Transformation and Differencing was done to\nmake the series stationary. Based on the results of the analysis, ARIMA (2, 1,\n0) is the appropriate model for the weekly Malaria incidence in the\nPhilippines.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:47:46 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Perez", "Empha Grace", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1902.08081", "submitter": "Konstantinos Pelechrinis", "authors": "Anthony Sicilia, Konstantinos Pelechrinis, Kirk Goldsberry", "title": "DeepHoops: Evaluating Micro-Actions in Basketball Using Deep Feature\n  Representations of Spatio-Temporal Data", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How much is an on-ball screen worth? How much is a backdoor cut away from the\nball worth? Basketball is one of a number of sports which, within the past\ndecade, have seen an explosion in quantitative metrics and methods for\nevaluating players and teams. However, it is still challenging to evaluate\nindividual off-ball events in terms of how they contribute to the success of a\npossession. In this study, we develop an end-to-end deep learning architecture\nDeepHoops to process a unique dataset composed of spatio-temporal tracking data\nfrom NBA games in order to generate a running stream of predictions on the\nexpected points to be scored as a possession progresses. We frame the problem\nas a multi-class sequence classification problem in which our model estimates\nprobabilities of terminal actions taken by players (e.g. take field goal,\nturnover, foul etc.) at each moment of a possession based on a sequence of ball\nand player court locations preceding the said moment. Each of these terminal\nactions is associated with an expected point value, which is used to estimate\nthe expected points to be scored. One of the challenges associated with this\nproblem is the high imbalance in the action classes. To solve this problem, we\nparameterize a downsampling scheme for the training phase. We demonstrate that\nDeepHoops is well-calibrated, estimating accurately the probabilities of each\nterminal action and we further showcase the model's capability to evaluate\nindividual actions (potentially off-ball) within a possession that are not\ncaptured by boxscore statistics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 14:54:06 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Sicilia", "Anthony", ""], ["Pelechrinis", "Konstantinos", ""], ["Goldsberry", "Kirk", ""]]}, {"id": "1902.08127", "submitter": "Marta Pelizzola", "authors": "Kerstin Spitzer, Marta Pelizzola and Andreas Futschik", "title": "Modifying the Chi-square and the CMH test for population genetic\n  inference: adapting to over-dispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evolve and resequence studies provide a popular approach to simulate\nevolution in the lab and explore its genetic basis. In this context, the\nchi-square test, Fishers exact test, as well as the Cochran-Mantel-Haenszel\ntest are commonly used to infer genomic positions affected by selection from\ntemporal changes in allele frequency. However, the null model associated with\nthese tests does not match the null hypothesis of actual interest. Indeed due\nto genetic drift and possibly other additional noise components such as pool\nsequencing, the null variance in the data can be substantially larger than\naccounted forby these common test statistics. This leads to p-values that are\nsystematically too small and therefore a huge number of false positive results.\nEven, if the ranking rather than the actual p-values is of interest, a naive\napplication of the mentioned tests will give misleading results, as the amount\nof over-dispersion varies from locus to locus. We therefore propose adjusted\nstatistics that take the over-dispersion into account while keeping the\nformulas simple. This is particularly useful in genome-wide applications, where\nmillions of SNPs can be handled with little computational effort. We then apply\nthe adapted test statistics to real data fromDrosophila, and investigate how\nin-formation from intermediate generations can be included when avail-able. The\nobtained formulas may also be useful in other situations, provided that the\nnull variance either is known or can be estimated.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 16:26:05 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Spitzer", "Kerstin", ""], ["Pelizzola", "Marta", ""], ["Futschik", "Andreas", ""]]}, {"id": "1902.08289", "submitter": "Dennis Feehan", "authors": "Dennis M. Feehan and Curtiss Cobb", "title": "Using an online sample to learn about an offline population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online data sources offer tremendous promise to demography and other social\nsciences, but researchers worry that the group of people who are represented in\nonline datasets can be different from the general population. We show that by\nsampling and anonymously interviewing people who are online, researchers can\nlearn about both people who are online and people who are offline. Our approach\nis based on the insight that people everywhere are connected through in-person\nsocial networks, such as kin, friendship, and contact networks. We illustrate\nhow this insight can be used to derive an estimator for tracking the *digital\ndivide* in access to the internet, an increasingly important dimension of\npopulation inequality in the modern world. We conducted a large-scale empirical\ntest of our approach, using an online sample to estimate internet adoption in\nfive countries ($n \\approx 15,000$). Our test embedded a randomized experiment\nwhose results can help design future studies. Our approach could be adapted to\nmany other settings, offering one way to overcome some of the major challenges\nfacing demographers in the information age.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 22:27:11 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 00:30:18 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Feehan", "Dennis M.", ""], ["Cobb", "Curtiss", ""]]}, {"id": "1902.08486", "submitter": "Ron Sarafian", "authors": "Ron Sarafian, Itai Kloog, Allan C. Just, Johnathan D. Rosenblatt", "title": "Gaussian Markov Random Fields versus Linear Mixed Models for\n  satellite-based PM2.5 assessment: Evidence from the Northeastern USA", "comments": null, "journal-ref": null, "doi": "10.1016/j.atmosenv.2019.02.025", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the effects of air-pollution on health is a key area in\nenvironmental epidemiology. An accurate estimation of air-pollution effects\nrequires spatio-temporally resolved datasets of air-pollution, especially, Fine\nParticulate Matter (PM). Satellite-based technology has greatly enhanced the\nability to provide PM assessments in locations where direct measurement is\nimpossible.\n  Indirect PM measurement is a statistical prediction problem. The\nspatio-temporal statistical literature offer various predictive models:\nGaussian Random Fields (GRF) and Linear Mixed Models (LMM), in particular. GRF\nemphasize the spatio-temporal structure in the data, but are computationally\ndemanding to fit. LMMs are computationally easier to fit, but require some\ntampering to deal with space and time.\n  Recent advances in the spatio-temporal statistical literature propose to\nalleviate the computation burden of GRFs by approximating them with Gaussian\nMarkov Random Fields (GMRFs). Since LMMs and GMRFs are both computationally\nfeasible, the question arises: which is statistically better? We show that\ndespite the great popularity of LMMs in environmental monitoring and pollution\nassessment, LMMs are statistically inferior to GMRF for measuring PM in the\nNortheastern USA.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 13:37:47 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Sarafian", "Ron", ""], ["Kloog", "Itai", ""], ["Just", "Allan C.", ""], ["Rosenblatt", "Johnathan D.", ""]]}, {"id": "1902.08593", "submitter": "Joshua Reid", "authors": "Larkin Liu, Richard Downe and Joshua Reid", "title": "Multi-Armed Bandit Strategies for Non-Stationary Reward Distributions\n  and Delayed Feedback Processes", "comments": "13 pages, manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey is performed of various Multi-Armed Bandit (MAB) strategies in order\nto examine their performance in circumstances exhibiting non-stationary\nstochastic reward functions in conjunction with delayed feedback. We run\nseveral MAB simulations to simulate an online eCommerce platform for grocery\npick up, optimizing for product availability. In this work, we evaluate several\npopular MAB strategies, such as $\\epsilon$-greedy, UCB1, and Thompson Sampling.\nWe compare the respective performances of each MAB strategy in the context of\nregret minimization. We run the analysis in the scenario where the reward\nfunction is non-stationary. Furthermore, the process experiences delayed\nfeedback, where the reward function is not immediately responsive to the arm\nplayed. We devise a new adaptive technique (AG1) tailored for non-stationary\nreward functions in the delayed feedback scenario. The results of the\nsimulation show show superior performance in the context of regret minimization\ncompared to traditional MAB strategies.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 18:19:17 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 17:50:34 GMT"}, {"version": "v3", "created": "Mon, 29 Jul 2019 18:16:49 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Liu", "Larkin", ""], ["Downe", "Richard", ""], ["Reid", "Joshua", ""]]}, {"id": "1902.08831", "submitter": "Thorsten Gl\u00c3\u00bcsenkamp", "authors": "Thorsten Gl\\\"usenkamp", "title": "A unified perspective on modified Poisson likelihoods for limited Monte\n  Carlo data", "comments": "30 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1748-0221/15/01/P01035", "report-no": null, "categories": "astro-ph.IM hep-ex physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting experiments often rely on Monte Carlo simulations for predictions of\nPoisson expectations. The accompanying uncertainty from the finite Monte Carlo\nsample size can be incorporated into parameter estimation by modifying the\nPoisson likelihood. We first review previous Frequentist methods of this type\nby Barlow et al, Bohm et al, and Chirkin, as well as recently proposed\nprobabilistic methods by the author and Arg\\\"uelles et al. We show that all\nthese approaches can be understood in a unified way: they all approximate the\nunderlying probability distribution of the sum of weights in a given bin, the\ncompound Poisson distribution (CPD). The Probabilistic methods marginalize the\nPoisson mean with a distribution that approximates the CPD, while the\nFrequentist counterparts optimize the same integrand treating the mean as a\nnuisance parameter. With this viewpoint we can motivate three new probabilistic\nlikelihoods based on generalized gamma-Poisson mixture distributions which we\nderive in analytic form. Afterwards, we test old and new formulas in different\nparameter estimation settings consisting of a \"background\" and \"signal\"\ndataset. The probablistic counterpart to the Ansatz by Barlow et al.\noutperforms all other existing approaches in various scenarios. We further find\na surprising outcome: usage of the exact CPD is actually bad for parameter\nestimation. A continuous approximation performs much better and in principle\nallows to perform bias-free inference at any level of simulated livetime if the\nfirst two moments of the CPD of each dataset are known exactly. Finally, we\nalso discuss the situation where new Monte Carlo simulation is produced for a\ngiven parameter choice which leads to fluctuations in the computed likelihood\nvalue. Two of the new formulas allow to include this Poisson uncertainty\ndirectly into the likelihood which substantially decreases these fluctuations.\n", "versions": [{"version": "v1", "created": "Sat, 23 Feb 2019 19:21:18 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 16:24:30 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Gl\u00fcsenkamp", "Thorsten", ""]]}, {"id": "1902.08935", "submitter": "Karla DiazOrdaz", "authors": "Karla DiazOrdaz and Richard Grieve", "title": "Non-compliance and missing data in health economic evaluation", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic evaluations face the issues of non-compliance and missing\ndata. Here, non-compliance is defined as non-adherence to a specific treatment,\nand occurs within randomised controlled trials (RCTs) when participants depart\nfrom their random assignment. Missing data arises if, for example, there is\nloss to follow-up, survey non-response, or the information available from\nroutine data sources is incomplete. Appropriate statistical methods for\nhandling non-compliance and missing data have been developed, but they have\nrarely been applied in health economics studies. Here, we illustrate the issues\nand outline some of the appropriate methods to handle these with an application\nto a health economic evaluation that uses data from an RCT.\n  In an RCT the random assignment can be used as an instrument for treatment\nreceipt, to obtain consistent estimates of the complier average causal effect,\nprovided the underlying assumptions are met. Instrumental variable methods can\naccommodate essential features of the health economic context such as the\ncorrelation between individuals' costs and outcomes in cost-effectiveness\nstudies. Methodological guidance for handling missing data encourages\napproaches such as multiple imputation or inverse probability weighting, that\nassume the data are Missing At Random, but also sensitivity analyses that\nrecognise the data may be missing according to the true, unobserved values,\nthat is, Missing Not at Random.\n  Future studies should subject the assumptions behind methods for handling\nnon-compliance and missing data to thorough sensitivity analyses. Modern\nmachine learning methods can help reduce reliance on correct model\nspecification. Further research is required to develop flexible methods for\nhandling more complex forms of non-compliance and missing data.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 12:26:29 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["DiazOrdaz", "Karla", ""], ["Grieve", "Richard", ""]]}, {"id": "1902.09026", "submitter": "Jaroslav Hor\\'a\\v{c}ek", "authors": "Jaroslav Hor\\'a\\v{c}ek, V\\'aclav Kouck\\'y and Milan Hlad\\'ik", "title": "Contribution of Interval Linear Algebra to the Ongoing Discussions on\n  Multiple Breath Washout Test", "comments": "The paper summarizes our hypotheses relevant to the area of the\n  Multiple Breath Washout test, which are based on application of interval\n  linear algebra", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper the interval least squares approach to estimate/fit data with\ninterval uncertainties is introduced. The solution of this problem is discussed\nfrom the perspective of interval linear algebra. Using the interval linear\nalgebra carefully, it is possible to significantly speed up the computation in\nspecialized cases. The interval least squares approach is then applied to lung\nfunction testing method - Multiple breath washout test (MBW). It is used for\nalgebraic handling of uncertainties arising during the measurement.\nSurprisingly, it sheds new light on various aspects of this procedure - it\nshows that the precision of currently used sensors does not allow verified\nprediction. Moreover, it proved the most commonly used curve to model the\nnitrogen washout process from lung to be wrong. Such insight contributes to the\nongoing discussions on the possibility to predict clinically relevant indices\n(e.g., LCI).\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 22:22:19 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Hor\u00e1\u010dek", "Jaroslav", ""], ["Kouck\u00fd", "V\u00e1clav", ""], ["Hlad\u00edk", "Milan", ""]]}, {"id": "1902.09292", "submitter": "Michael Lebacher", "authors": "Michael Lebacher, Paul W. Thurner and G\\\"oran Kauermann", "title": "Censored Regression for Modelling International Small Arms Trading and\n  its \"Forensic\" Use for Exploring Unreported Trades", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use a censored regression model to investigate data on the\ninternational trade of small arms and ammunition (SAA) provided by the\nNorwegian Initiative on Small Arms Transfers (NISAT). Taking a network based\nview on the transfers, we not only rely on exogenous covariates but also\nestimate endogenous network effects. We apply a spatial autocorrelation (SAR)\nmodel with multiple weight matrices. The likelihood is maximized employing the\nMonte Carlo Expectation Maximization (MCEM) algorithm. Our approach reveals\nstrong and stable endogenous network effects. Furthermore, we find evidence for\na substantial path dependence as well as a close connection between exports of\ncivilian and military small arms. The model is then used in a \"forensic\" manner\nto analyse latent network structures and thereby to identify countries with\nhigher or lower tendency to export or import than reflected in the data. The\napproach is also validated using a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:34:23 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 11:44:47 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 12:07:05 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lebacher", "Michael", ""], ["Thurner", "Paul W.", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "1902.09302", "submitter": "Philip Chodrow", "authors": "Philip S. Chodrow", "title": "Configuration Models of Random Hypergraphs", "comments": "Major revisions to all text and figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.SI physics.data-an physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many empirical networks are intrinsically polyadic, with interactions\noccurring within groups of agents of arbitrary size. There are, however, few\nflexible null models that can support statistical inference for such polyadic\nnetworks. We define a class of null random hypergraphs that hold constant both\nthe node degree and edge dimension sequences, generalizing the classical dyadic\nconfiguration model. We provide a Markov Chain Monte Carlo scheme for sampling\nfrom these models, and discuss connections and distinctions between our\nproposed models and previous approaches. We then illustrate these models\nthrough a triplet of applications. We start with two classical network topics\n-- triadic clustering and degree-assortativity. In each, we emphasize the\nimportance of randomizing over hypergraph space rather than projected graph\nspace, showing that this choice can dramatically alter statistical inference\nand study findings. We then define and study the edge intersection profile of a\nhypergraph as a measure of higher-order correlation between edges, and derive\nasymptotic approximations under the stub-labeled null. Our experiments\nemphasize the ability of explicit, statistically-grounded polyadic modeling to\nsignificantly enhance the toolbox of network data science. We close with\nsuggestions for multiple avenues of future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:43:57 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 13:33:29 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 18:23:47 GMT"}, {"version": "v4", "created": "Tue, 9 Jul 2019 02:54:03 GMT"}, {"version": "v5", "created": "Fri, 13 Dec 2019 19:18:22 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chodrow", "Philip S.", ""]]}, {"id": "1902.09304", "submitter": "Leah Comment", "authors": "Leah Comment, Fabrizia Mealli, Sebastien Haneuse, Corwin Zigler", "title": "Survivor average causal effects for continuous time: a principal\n  stratification approach to causal inference with semicompeting risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semicompeting risks problems, nonterminal time-to-event outcomes such as\ntime to hospital readmission are subject to truncation by death. These settings\nare often modeled with illness-death models for the hazards of the terminal and\nnonterminal events, but evaluating causal treatment effects with hazard models\nis problematic due to conditioning on survival (a post-treatment outcome) that\nis embedded in the definition of a hazard. Extending an existing survivor\naverage causal effect (SACE) estimand, we frame the evaluation of treatment\neffects in the context of semicompeting risks with principal stratification and\nintroduce two new causal estimands: the time-varying survivor average causal\neffect (TV-SACE) and the restricted mean survivor average causal effect\n(RM-SACE). These principal causal effects are defined among units that would\nsurvive regardless of assigned treatment. We adopt a Bayesian estimation\nprocedure that parameterizes illness-death models for both treatment arms. We\noutline a frailty specification that can accommodate within-person correlation\nbetween nonterminal and terminal event times, and we discuss potential avenues\nfor adding model flexibility. The method is demonstrated in the context of\nhospital readmission among late-stage pancreatic cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 21:22:22 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Comment", "Leah", ""], ["Mealli", "Fabrizia", ""], ["Haneuse", "Sebastien", ""], ["Zigler", "Corwin", ""]]}, {"id": "1902.09386", "submitter": "Dipankar Bandyopadhyay", "authors": "Jing Xu, Dipankar Bandyopadhyay, Sedigheh Mirzaei, Bryan Michalowicz,\n  Bibhas Chakraaborty", "title": "SMARTp: A SMART design for non-surgical treatments of chronic\n  periodontitis with spatially-referenced and non-randomly missing skewed\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes dynamic treatment regimes for choosing individualized\neffective treatment strategies of chronic periodontal disease. R codes for\nimplementing the proposed sample size formula are available in GitHub.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 15:51:21 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Xu", "Jing", ""], ["Bandyopadhyay", "Dipankar", ""], ["Mirzaei", "Sedigheh", ""], ["Michalowicz", "Bryan", ""], ["Chakraaborty", "Bibhas", ""]]}, {"id": "1902.09578", "submitter": "Zeinab Takbiri", "authors": "Zeinab Takbiri, Ardeshir Ebtehaj, Efi Foufoula-Georgiou,\n  Pierre-Emmanuel Kirstetter, and F. Joseph Turk", "title": "A Nested K-Nearest Prognostic Approach for Microwave Precipitation Phase\n  Detection over Snow Cover", "comments": null, "journal-ref": "J. Hydrometeorology (2019) 20, 251-274", "doi": "10.1175/JHM-D-18-0021.1", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring changes of precipitation phase from space is important for\nunderstanding the mass balance of Earth's cryosphere in a changing climate.\nThis paper examines a Bayesian nearest neighbor approach for prognostic\ndetection of precipitation and its phase using passive microwave observations\nfrom the Global Precipitation Measurement (GPM) satellite. The method uses the\nweighted Euclidean distance metric to search through an a priori database\npopulated with coincident GPM radiometer and radar observations as well as\nancillary snow-cover data. The algorithm performance is evaluated using data\nfrom GPM official precipitation products, ground-based radars, and\nhigh-fidelity simulations from the Weather Research and Forecasting model.\nUsing the presented approach, we demonstrate that the hit probability of\nterrestrial precipitation detection can reach to 0.80, while the probability of\nfalse alarm remains below 0.11. The algorithm demonstrates higher skill in\ndetecting snowfall than rainfall, on average by 10 percent. In particular, the\nprobability of precipitation detection and its solid phase increases by 11 and\n8 percent, over dry snow cover, when compared to other surface types. The main\nreason is found to be related to the ability of the algorithm in capturing the\nsignal of increased liquid water content in snowy clouds over radiometrically\ncold snow-covered surfaces\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 19:24:45 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Takbiri", "Zeinab", ""], ["Ebtehaj", "Ardeshir", ""], ["Foufoula-Georgiou", "Efi", ""], ["Kirstetter", "Pierre-Emmanuel", ""], ["Turk", "F. Joseph", ""]]}, {"id": "1902.09653", "submitter": "Indranil Sahoo", "authors": "Indranil Sahoo, Joseph Guinness and Brian J. Reich", "title": "Estimating Atmospheric Motion Winds from Satellite Image Data using\n  Space-time Drift Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geostationary satellites collect high-resolution weather data comprising a\nseries of images which can be used to estimate wind speed and direction at\ndifferent altitudes. The Derived Motion Winds (DMW) Algorithm is commonly used\nto process these data and estimate atmospheric winds by tracking features in\nimages taken by the GOES-R series of the NOAA geostationary meteorological\nsatellites. However, the wind estimates from the DMW Algorithm are sparse and\ndo not come with uncertainty measures. This motivates us to statistically model\nwind motions as a spatial process drifting in time. We propose a covariance\nfunction that depends on spatial and temporal lags and a drift parameter to\ncapture the wind speed and wind direction. We estimate the parameters by local\nmaximum likelihood. Our method allows us to compute standard errors of the\nestimates, enabling spatial smoothing of the estimates using a Gaussian kernel\nweighted by the inverses of the estimated variances. We conduct extensive\nsimulation studies to determine the situations where our method performs well.\nThe proposed method is applied to the GOES-15 brightness temperature data over\nColorado and reduces prediction error of brightness temperature compared to the\nDMW Algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 23:08:16 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 05:12:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 16:05:08 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Sahoo", "Indranil", ""], ["Guinness", "Joseph", ""], ["Reich", "Brian J.", ""]]}, {"id": "1902.09694", "submitter": "Shahin Boluki", "authors": "Shahin Boluki, Siamak Zamani Dadaneh, Xiaoning Qian, Edward R.\n  Dougherty", "title": "Optimal Clustering with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing values frequently arise in modern biomedical studies due to various\nreasons, including missing tests or complex profiling technologies for\ndifferent omics measurements. Missing values can complicate the application of\nclustering algorithms, whose goals are to group points based on some similarity\ncriterion. A common practice for dealing with missing values in the context of\nclustering is to first impute the missing values, and then apply the clustering\nalgorithm on the completed data.\n  We consider missing values in the context of optimal clustering, which finds\nan optimal clustering operator with reference to an underlying random labeled\npoint process (RLPP). We show how the missing-value problem fits neatly into\nthe overall framework of optimal clustering by incorporating the missing value\nmechanism into the random labeled point process and then marginalizing out the\nmissing-value process. In particular, we demonstrate the proposed framework for\nthe Gaussian model with arbitrary covariance structures. Comprehensive\nexperimental studies on both synthetic and real-world RNA-seq data show the\nsuperior performance of the proposed optimal clustering with missing values\nwhen compared to various clustering approaches. Optimal clustering with missing\nvalues obviates the need for imputation-based pre-processing of the data, while\nat the same time possessing smaller clustering errors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 01:40:13 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Boluki", "Shahin", ""], ["Dadaneh", "Siamak Zamani", ""], ["Qian", "Xiaoning", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1902.09703", "submitter": "Kevin Cummiskey", "authors": "Kevin Cummiskey, Chanmin Kim, Christine Choirat, Lucas R.F. Henneman,\n  Joel Schwartz, Corwin Zigler", "title": "A Source-Oriented Approach to Coal Power Plant Emissions Health Effects", "comments": "26 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is increasing focus on whether air pollution originating from different\nsources has different health implications. In particular, recent evidence\nsuggests that fine particulate matter (PM2.5) with chemical tracers suggesting\ncoal combustion origins is especially harmful. Augmenting this knowledge with\nestimates from causal inference methods to identify the health impacts of PM2.5\nderived from specific point sources of coal combustion would be an important\nstep towards informing specific, targeted interventions. We investigated the\neffect of high-exposure to coal combustion emissions from 783 coal-fired power\ngenerating units on ischemic heart disease (IHD) hospitalizations in over 19\nmillion Medicare beneficiaries residing at 21,351 ZIP codes in the eastern\nUnited States. We used InMAP, a newly-developed, reduced-complexity air quality\nmodel to classify each ZIP code as either a high-exposed or control location.\nOur health outcomes analysis uses a causal inference method - propensity score\nmatching - to adjust for potential confounders of the relationship between\nexposure and IHD. We fit separate Poisson regression models to the matched data\nin each geographic region to estimate the incidence rate ratio for IHD\ncomparing high-exposed to control locations. High exposure to coal power plant\nemissions and IHD were positively associated in the Northeast (IRR = 1.08, 95%\nCI = 1.06, 1.09) and the Southeast (IRR = 1.06, 95% CI = 1.04, 1.08). No\nsignificant association was found in the Industrial Midwest (IRR = 1.02, 95% CI\n= 1.00, 1.04), likely the result of small exposure contrasts between\nhigh-exposed and control ZIP codes in that region. This study provides targeted\nevidence of the association between emissions from specific coal power plants\nand IHD hospitalizations among Medicare beneficiaries.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 02:06:00 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Cummiskey", "Kevin", ""], ["Kim", "Chanmin", ""], ["Choirat", "Christine", ""], ["Henneman", "Lucas R. F.", ""], ["Schwartz", "Joel", ""], ["Zigler", "Corwin", ""]]}, {"id": "1902.09745", "submitter": "Inon Peled", "authors": "Inon Peled, Kelvin Lee, Yu Jiang, Justin Dauwels, Francisco C. Pereira", "title": "Online Predictive Optimization Framework for Stochastic\n  Demand-Responsive Transit Services", "comments": "34 pages, 12 figures, 5 tables", "journal-ref": "2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n  Auckland, New Zealand, 2019, pp. 3043-3048", "doi": "10.1109/ITSC.2019.8916878", "report-no": null, "categories": "stat.ML cs.LG math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops an online predictive optimization framework for\ndynamically operating a transit service in an area of crowd movements. The\nproposed framework integrates demand prediction and supply optimization to\nperiodically redesign the service routes based on recently observed demand. To\npredict demand for the service, we use Quantile Regression to estimate the\nmarginal distribution of movement counts between each pair of serviced\nlocations. The framework then combines these marginals into a joint demand\ndistribution by constructing a Gaussian copula, which captures the structure of\ncorrelation between the marginals. For supply optimization, we devise a linear\nprogramming model, which simultaneously determines the route structure and the\nservice frequency according to the predicted demand. Importantly, our framework\nboth preserves the uncertainty structure of future demand and leverages this\nfor robust route optimization, while keeping both components decoupled. We\nevaluate our framework using a real-world case study of autonomous mobility in\na university campus in Denmark. The results show that our framework often\nobtains the ground truth optimal solution, and can outperform conventional\nmethods for route optimization, which do not leverage full predictive\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 05:56:05 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 10:27:20 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Peled", "Inon", ""], ["Lee", "Kelvin", ""], ["Jiang", "Yu", ""], ["Dauwels", "Justin", ""], ["Pereira", "Francisco C.", ""]]}, {"id": "1902.09862", "submitter": "Marco Marani", "authors": "Marco Marani and Enrico Zorzetto", "title": "Doubly stochastic distributions of extreme events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of block maxima of sequences of independent and\nidentically-distributed random variables is used to model extreme values in\nmany disciplines. The traditional extreme value (EV) theory derives a\nclosed-form expression for the distribution of block maxima under asymptotic\nassumptions, and is generally fitted using annual maxima or excesses over a\nhigh threshold, thereby discarding a large fraction of the available\nobservations. The recently-introduced Metastatistical Extreme Value\nDistribution (MEVD), a non-asymptotic formulation based on doubly stochastic\ndistributions, has been shown to offer several advantages compared to the\ntraditional EV theory. In particular, MEVD explicitly accounts for the\nvariability of the process generating the extreme values, and uses all the\navailable information to perform high-quantile inferences. Here we review the\nderivation of the MEVD, analyzing its assumptions in detail, and show that its\ngeneral formulation includes other doubly stochastic approaches to extreme\nvalue analysis that have been recently proposed.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 11:16:26 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Marani", "Marco", ""], ["Zorzetto", "Enrico", ""]]}, {"id": "1902.10060", "submitter": "Lingjiao Zhang", "authors": "Lingjiao Zhang, Xiruo Ding, Yanyuan Ma, Naveen Muthu, Imran Ajmal,\n  Jason H. Moore, Daniel S. Herman, Jinbo Chen", "title": "Electronic Health Record Phenotyping with Internally Assessable\n  Performance (PhIAP) using Anchor-Positive and Unlabeled Patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building phenotype models using electronic health record (EHR) data\nconventionally requires manually labeled cases and controls. Assigning labels\nis labor intensive and, for some phenotypes, identifying gold-standard controls\nis prohibitive. To facilitate comprehensive clinical decision support and\nresearch, we sought to develop an accurate EHR phenotyping approach that\nassesses its performance without a validation set. Our framework relies on\nspecifying a random subset of cases, potentially using an anchor variable that\nhas excellent positive predictive value and sensitivity that is independent of\npredictors. We developed a novel maximum likelihood approach that efficiently\nleverages data from anchor-positive and unlabeled patients to develop logistic\nregression phenotyping models. Additionally, we described novel statistical\nmethods for estimating phenotyping prevalence and assessing model calibration\nand predictive performance measures. Theoretical and simulation studies\nindicated our method generates accurate predicted probabilities, leading to\nexcellent discrimination and calibration, and consistent estimates of phenotype\nprevalence and anchor sensitivity. The method appeared robust to minor\nlack-of-fit and the proposed calibration assessment detected major lack-of-fit.\nWe applied our method to EHR data to develop a preliminary model for\nidentifying patients with primary aldosteronism, which achieved an AUC of 0.99\nand PPV of 0.8. We developed novel statistical methods for accurate model\ndevelopment and validation with minimal manual labeling, facilitating\ndevelopment of scalable, transferable, semi-automated case labeling and\npractice-specific models. Our EHR phenotyping approach decreases\nlabor-intensive manual phenotyping and annotation, which should enable broader\nmodel development and dissemination for EHR clinical decision support and\nresearch.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2019 17:26:18 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zhang", "Lingjiao", ""], ["Ding", "Xiruo", ""], ["Ma", "Yanyuan", ""], ["Muthu", "Naveen", ""], ["Ajmal", "Imran", ""], ["Moore", "Jason H.", ""], ["Herman", "Daniel S.", ""], ["Chen", "Jinbo", ""]]}, {"id": "1902.10061", "submitter": "Benedikt Zacher", "authors": "Benedikt Zacher and Irina Czogiel", "title": "Supervised learning improves disease outbreak detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The early detection of infectious disease outbreaks is a crucial task to\nprotect population health. To this end, public health surveillance systems have\nbeen established to systematically collect and analyse infectious disease data.\nA variety of statistical tools are available, which detect potential outbreaks\nas abberations from an expected endemic level using these data. Here, we\ndevelop the first supervised learning approach based on hidden Markov models\nfor disease outbreak detection, which leverages data that is routinely\ncollected within a public health surveillance system. We evaluate our model\nusing real Salmonella and Campylobacter data, as well as simulations. In\ncomparison to a state-of-the-art approach, which is applied in multiple\nEuropean countries including Germany, our proposed model reduces the false\npositive rate by up to 50% while retaining the same sensitivity. We see our\nsupervised learning approach as a significant step to further develop machine\nlearning applications for disease outbreak detection, which will be\ninstrumental to improve public health surveillance systems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 13:45:07 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Zacher", "Benedikt", ""], ["Czogiel", "Irina", ""]]}, {"id": "1902.10066", "submitter": "Alexey Shutov", "authors": "A.V. Shutov, A.A. Kaygorodtseva", "title": "Parameter identification in elasto-plasticity: distance between\n  parameters and impact of measurement errors", "comments": "11 pages, 2 figures, 4 tables", "journal-ref": "ZAMM, 2019", "doi": "10.1002/zamm.201800340", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A special aspect of parameter identification in finite-strain\nelasto-plasticity is considered. Namely, we analyze the impact of the\nmeasurement errors on the resulting set of material parameters. In order to\ndefine the sensitivity of parameters with respect to the measurement errors, a\nmechanics-based distance between two sets of parameters is introduced. Using\nthis distance function, we assess the reliability of certain parameter\nidentification procedures. The assessment involves introduction of artificial\nnoise to the experimental data; the noise can be both correlated and\nuncorrelated. An analytical procedure to speed up Monte Carlo simulations is\npresented. As a result, a simple tool for estimating the robustness of\nparameter identification is obtained. The efficiency of the approach is\nillustrated using a model of finite-strain elasto-plasticity, which accounts\nfor combined isotropic and kinematic hardening. It is shown that dealing with\ncorrelated measurement errors, most stable identification results are obtained\nfor non-diagonal weighting matrix. At the same time, there is a conflict\nbetween the stability and accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 16:55:08 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shutov", "A. V.", ""], ["Kaygorodtseva", "A. A.", ""]]}, {"id": "1902.10067", "submitter": "Luis A. Mateos", "authors": "Luis A. Mateos", "title": "Does the winning team always covers the point spread? A study in\n  professional basketball since 1990", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sports betting it is easier to predict the winner of a game match than the\nteam that covers the bet. Since, a winner team might not cover a bet.\n  This study focuses on the relation of the variable win to the betting\nvariable cover the point spread. The study is performed with data from\nprofessional basketball (betting lines and scores) and tries to answer the\nquestion: Does the winning team always covers the point spread?. In order to\nanswer this question, a regression analysis is performed taking into account\nthe most and less winning teams, together with their betting variables since\nthe 1990-1991 NBA season. The regression results are inserted in the SPXS\nexpert system revealing an indirect factor analysis that correlates betting\nvariables with teams winning percentages.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2019 18:57:37 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Mateos", "Luis A.", ""]]}, {"id": "1902.10106", "submitter": "Sameer Deshpande", "authors": "Timothy G. Gaulton and Sameer K. Deshpande and Dylan S. Small and Mark\n  D. Neuman", "title": "Protocol for an Observational Study of the Association of High School\n  Football Participation on Health in Late Adulthood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American football is the most popular high school sport and is among the\nleading cause of injury among adolescents. While there has been considerable\nrecent attention on the link between football and cognitive decline, there is\nalso evidence of higher than expected rates of pain, obesity, and lower quality\nof life among former professional players, either as a result of repetitive\nhead injury or through different mechanisms. Previously hidden downstream\neffects of playing football may have far-reaching public health implications\nfor participants in youth and high school football programs.\n  Our proposed study is a retrospective observational study that compares 1,153\nhigh school males who played varsity football with 2,751 male students who did\nnot. 1,951 of the control subjects did not play any sport and the remaining 800\ncontrols played a non-contact sport. Our primary outcome is self-rated health\nmeasured at age 65. To control for potential confounders, we adjust for\npre-exposure covariates with matching and model-based covariance adjustment. We\nwill conduct an ordered testing procedure designed to use the full pool of\n2,751 controls while also controlling for possible unmeasured differences\nbetween students who played sports and those who did not. We will\nquantitatively assess the sensitivity of the results to potential unmeasured\nconfounding. The study will also assess secondary outcomes of pain, difficulty\nwith activities of daily living, and obesity, as these are both important to\nindividual well-being and have public health relevance.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 18:33:39 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Gaulton", "Timothy G.", ""], ["Deshpande", "Sameer K.", ""], ["Small", "Dylan S.", ""], ["Neuman", "Mark D.", ""]]}, {"id": "1902.10261", "submitter": "Kristoffer Glover", "authors": "Kristoffer Glover", "title": "Optimally Stopping a Brownian Bridge with an Unknown Pinning Time: A\n  Bayesian Approach", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of optimally stopping a Brownian bridge with an\nunknown pinning time so as to maximise the value of the process upon stopping.\nAdopting a Bayesian approach, we assume the stopper has a general continuous\nprior and is allowed to update their belief about the value of the pinning time\nthrough sequential observations of the process. Uncertainty in the pinning time\ninfluences both the conditional dynamics of the process and the expected\n(random) horizon of the optimal stopping problem. We analyse certain gamma and\nbeta distributed priors in detail. Remarkably, the optimal stopping problem in\nthe gamma case becomes time homogeneous and is completely solvable in closed\nform. Moreover, in the beta case we find that the optimal stopping boundary\ntakes on a square-root form, similar to the classical solution with a known\npinning time.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 23:06:50 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 09:13:27 GMT"}, {"version": "v3", "created": "Fri, 19 Jul 2019 22:12:50 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 22:53:26 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Glover", "Kristoffer", ""]]}, {"id": "1902.10283", "submitter": "Behnam Malmir", "authors": "Behnam Malmir and Shing I Chang", "title": "Gait Change Detection Using Parameters Generated from Microsoft Kinect\n  Coordinates", "comments": "This article is an updated version of a paper entitled 'Gait Change\n  Detection Using Parameters Generated from Microsoft Kinect Coordinates'\n  presented at the 2016 Industrial and Systems Engineering Research Conference\n  (ISERC) in Anaheim, California (May 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes a method to convert Microsoft Kinect coordinates into\ngait parameters in order to detect a person's gait change. The proposed method\ncan help quantify the progress of physical therapy. Microsoft Kinect, a popular\nplatform for video games, was used to generate 25 joints to form a human\nskeleton, and then the proposed method converted the coordinates of selected\nKinect joints into gait parameters such as spine tilt, hip tilt, and shoulder\ntilt, which were tracked over time. Sample entropy measure was then applied to\nquantify the variability of each gait parameter. Male and female subjects\nwalked a three-meter path multiple times in initial experiments, and their\nwalking patterns were recorded via the proposed Kinect device through the\nfrontal plane. Time series of the gait parameters were generated for subjects\nwith and without knee braces. Sample entropy was used to transform these time\nseries into numerical values for comparison of these two conditions.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 00:49:31 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Malmir", "Behnam", ""], ["Chang", "Shing I", ""]]}, {"id": "1902.10579", "submitter": "Erla Sturludottir", "authors": "Erla Sturludottir, Gudjon Mar Sigurdsson, and Gunnar Stefansson", "title": "Evaluation of a length-based method to estimate discard rate and the\n  effect of sampling size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common fisheries policy aims at eliminating discarding which has been\npart of fisheries for centuries. It is important to monitor the compliance with\nthe new regulations but estimating the discard rate is a challenging task,\nespecially where the practise is illegal. The aim of this study was to review a\nlength-based method that has been used to estimate the discard rate in\nIcelandic waters and explore the effects of different monitoring schemes. The\nlength-based method estimates the minimum discard rate and the method of\nbootstrapping can be used to determine the uncertainty of the estimate. This\nstudy showed that the number of ships is the most important factor to consider\nin order to decrease the uncertainty.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 15:18:19 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Sturludottir", "Erla", ""], ["Sigurdsson", "Gudjon Mar", ""], ["Stefansson", "Gunnar", ""]]}, {"id": "1902.10613", "submitter": "Leah Comment", "authors": "Leah Comment, Brent A. Coull, Corwin Zigler, Linda Valeri", "title": "Bayesian data fusion for unmeasured confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian causal inference offers a principled approach to policy evaluation\nof proposed interventions on mediators or time-varying exposures. We outline a\ngeneral approach to the estimation of causal quantities for settings with\ntime-varying confounding, such as exposure-induced mediator-outcome\nconfounders. We further extend this approach to propose two Bayesian data\nfusion (BDF) methods for unmeasured confounding. Using informative priors on\nquantities relating to the confounding bias parameters, our methods incorporate\ndata from an external source where the confounder is measured in order to make\ninferences about causal estimands in the main study population. We present\nresults from a simulation study comparing our data fusion methods to two common\nfrequentist correction methods for unmeasured confounding bias in the mediation\nsetting. We also demonstrate our method with an investigation of the role of\nstage at cancer diagnosis in contributing to Black-White colorectal cancer\nsurvival disparities.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 16:12:00 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Comment", "Leah", ""], ["Coull", "Brent A.", ""], ["Zigler", "Corwin", ""], ["Valeri", "Linda", ""]]}, {"id": "1902.10787", "submitter": "Maria Josefsson", "authors": "Maria Josefsson and Michael J. Daniels", "title": "Bayesian semi-parametric G-computation for causal inference in a cohort\n  study with MNAR dropout and death", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference with observational longitudinal data and time-varying\nexposures is often complicated by time-dependent confounding and attrition. The\nG-computation formula is one approach for estimating a causal effect in this\nsetting. The parametric modeling approach typically used in practice relies on\nstrong modeling assumptions for valid inference, and moreover depends on an\nassumption of missing at random, which is not appropriate when the missingness\nis missing not at random (MNAR) or due to death. In this work we develop a\nflexible Bayesian semi-parametric G-computation approach for assessing the\ncausal effect on the subpopulation that would survive irrespective of exposure,\nin a setting with MNAR dropout. The approach is to specify models for the\nobserved data using Bayesian additive regression trees, and then use\nassumptions with embedded sensitivity parameters to identify and estimate the\ncausal effect. The proposed approach is motivated by a longitudinal cohort\nstudy on cognition, health, and aging, and we apply our approach to study the\neffect of becoming a widow on memory. We also compare our approach to several\nstandard methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 21:05:26 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 10:33:39 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2019 06:26:51 GMT"}, {"version": "v4", "created": "Mon, 12 Oct 2020 19:38:59 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Josefsson", "Maria", ""], ["Daniels", "Michael J.", ""]]}, {"id": "1902.10981", "submitter": "Martina Vittorietti", "authors": "Martina Vittorietti, Piet J.J. Kok, Jilt Sietsma, Wei Li, Geurt\n  Jongbloed", "title": "General framework for testing Poisson-Voronoi assumption for real\n  microstructures", "comments": "29 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling microstructures is an interesting problem not just in Materials\nScience but also in Mathematics and Statistics. The most basic model for steel\nmicrostructure is the Poisson-Voronoi diagram. It has mathematically attractive\nproperties and it has been used in the approximation of single phase steel\nmicrostructures. The aim of this paper is to develop methods that can be used\nto test whether a real steel microstructure can be approximated by such a\nmodel. Therefore, a general framework for testing the Poisson-Voronoi\nassumption based on images of 2D sections of real metals is set out. Following\ntwo different approaches, according to the use or not of periodic boundary\nconditions, three different model tests are proposed. The first two are based\non the coefficient of variation and the cumulative distribution function of the\ncells area. The third exploits tools from to Topological Data Analysis, such as\npersistence landscapes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 09:57:42 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Vittorietti", "Martina", ""], ["Kok", "Piet J. J.", ""], ["Sietsma", "Jilt", ""], ["Li", "Wei", ""], ["Jongbloed", "Geurt", ""]]}]