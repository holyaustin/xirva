[{"id": "0706.0073", "submitter": "Yiping Dou", "authors": "Yiping Dou, Nhu D Le, and James V Zidek", "title": "Modeling Hourly Ozone Concentration Fields", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": null, "abstract": "  This paper presents a dynamic linear model for modeling hourly ozone\nconcentrations over the eastern United States. That model, which is developed\nwithin an Bayesian hierarchical framework, inherits the important feature of\nsuch models that its coefficients, treated as states of the process, can change\nwith time. Thus the model includes a time--varying site invariant mean field as\nwell as time varying coefficients for 24 and 12 diurnal cycle components. This\ncost of this model's great flexibility comes at the cost of computational\ncomplexity, forcing us to use an MCMC approach and to restrict application of\nour model domain to a small number of monitoring sites. We critically assess\nthis model and discover some of its weaknesses in this type of application.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2007 06:05:44 GMT"}], "update_date": "2007-06-04", "authors_parsed": [["Dou", "Yiping", ""], ["Le", "Nhu D", ""], ["Zidek", "James V", ""]]}, {"id": "0706.0096", "submitter": "William Rey Dr.", "authors": "William Rey", "title": "Total singular value decomposition. Robust SVD, regression and\n  location-scale", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": null, "abstract": "  Singular Value Decomposition (SVD) is the basic body of many statistical\nalgorithms and few users question whether SVD is properly handling its job.\n  SVD aims at evaluating the decomposition that best approximates a data\nmatrix, given some rank restriction. However often we are interested in the\nbest components of the decomposition rather than in the best approximation .\nThis conflict of objectives leads us to introduce {\\em Total SVD}, where the\nword \"Total\" is taken as in \"Total\" least squares.\n  SVD is a least squares method and, therefore, is very sensitive to gross\nerrors in the data matrix. We make SVD robust by imposing a weight to each of\nthe matrix entries. Breakdown properties are excellent.\n  Algorithmic aspects are handled; they rely on high dimension fixed point\ncomputations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2007 08:44:54 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2007 15:29:52 GMT"}], "update_date": "2007-09-06", "authors_parsed": [["Rey", "William", ""]]}, {"id": "0706.1062", "submitter": "Aaron Clauset", "authors": "Aaron Clauset, Cosma Rohilla Shalizi, M. E. J. Newman", "title": "Power-law distributions in empirical data", "comments": "43 pages, 11 figures, 7 tables, 4 appendices; code available at\n  http://www.santafe.edu/~aaronc/powerlaws/", "journal-ref": "SIAM Review 51, 661-703 (2009)", "doi": "10.1137/070710111", "report-no": null, "categories": "physics.data-an cond-mat.dis-nn stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Power-law distributions occur in many situations of scientific interest and\nhave significant consequences for our understanding of natural and man-made\nphenomena. Unfortunately, the detection and characterization of power laws is\ncomplicated by the large fluctuations that occur in the tail of the\ndistribution -- the part of the distribution representing large but rare events\n-- and by the difficulty of identifying the range over which power-law behavior\nholds. Commonly used methods for analyzing power-law data, such as\nleast-squares fitting, can produce substantially inaccurate estimates of\nparameters for power-law distributions, and even in cases where such methods\nreturn accurate answers they are still unsatisfactory because they give no\nindication of whether the data obey a power law at all. Here we present a\nprincipled statistical framework for discerning and quantifying power-law\nbehavior in empirical data. Our approach combines maximum-likelihood fitting\nmethods with goodness-of-fit tests based on the Kolmogorov-Smirnov statistic\nand likelihood ratios. We evaluate the effectiveness of the approach with tests\non synthetic data and give critical comparisons to previous approaches. We also\napply the proposed methods to twenty-four real-world data sets from a range of\ndifferent disciplines, each of which has been conjectured to follow a power-law\ndistribution. In some cases we find these conjectures to be consistent with the\ndata while in others the power law is ruled out.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2007 19:33:07 GMT"}, {"version": "v2", "created": "Mon, 2 Feb 2009 17:49:43 GMT"}], "update_date": "2009-11-12", "authors_parsed": [["Clauset", "Aaron", ""], ["Shalizi", "Cosma Rohilla", ""], ["Newman", "M. E. J.", ""]]}, {"id": "0706.1154", "submitter": "Debbarh Mohammed", "authors": "Mohammed Debbarh and Bertrand Maillot", "title": "Additive Regression Model for Continuous Time Processes", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": null, "abstract": "  In the setting of additive regression model for continuous time process, we\nestablish the optimal uniform convergence rates and optimal asymptotic\nquadratic error of additive regression. To build our estimate, we use the\nmarginal integration method.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2007 10:34:05 GMT"}], "update_date": "2007-06-11", "authors_parsed": [["Debbarh", "Mohammed", ""], ["Maillot", "Bertrand", ""]]}, {"id": "0706.1161", "submitter": "Debbarh Mohammed", "authors": "Mohammed Debbarh", "title": "Some Uniform Limit Results in Additive Regression Model", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.AP stat.TH", "license": null, "abstract": "  We establish some uniform limit results in the setting of additive regression\nmodel estimation. Our results allow to give an asymptotic 100% confidence bands\nfor these components. These results are stated in the framework of i.i.d random\nvectors when the marginal integration estimation method is used.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2007 11:24:01 GMT"}], "update_date": "2007-06-11", "authors_parsed": [["Debbarh", "Mohammed", ""]]}, {"id": "0706.1401", "submitter": "J.R. Lockwood", "authors": "J.R. Lockwood, Daniel F. McCaffrey", "title": "Controlling for individual heterogeneity in longitudinal models, with\n  applications to student achievement", "comments": "Published at http://dx.doi.org/10.1214/07-EJS057 in the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2007, Vol. 1, 223-252", "doi": "10.1214/07-EJS057", "report-no": "IMS-EJS-EJS_2007_57", "categories": "stat.AP", "license": null, "abstract": "  Longitudinal data tracking repeated measurements on individuals are highly\nvalued for research because they offer controls for unmeasured individual\nheterogeneity that might otherwise bias results. Random effects or mixed models\napproaches, which treat individual heterogeneity as part of the model error\nterm and use generalized least squares to estimate model parameters, are often\ncriticized because correlation between unobserved individual effects and other\nmodel variables can lead to biased and inconsistent parameter estimates.\nStarting with an examination of the relationship between random effects and\nfixed effects estimators in the standard unobserved effects model, this article\ndemonstrates through analysis and simulation that the mixed model approach has\na ``bias compression'' property under a general model for individual\nheterogeneity that can mitigate bias due to uncontrolled differences among\nindividuals. The general model is motivated by the complexities of longitudinal\nstudent achievement measures, but the results have broad applicability to\nlongitudinal modeling.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2007 05:57:04 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Lockwood", "J. R.", ""], ["McCaffrey", "Daniel F.", ""]]}, {"id": "0706.1776", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui", "title": "Coherence and phase synchronization: generalization to pairs of\n  multivariate time series, and removal of zero-lag contributions", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": null, "abstract": "  Coherence and phase synchronization between time series corresponding to\ndifferent spatial locations are usually interpreted as indicators of the\nconnectivity between locations. In neurophysiology, time series of electric\nneuronal activity are essential for studying brain interconnectivity. Such\nsignals can either be invasively measured from depth electrodes, or computed\nfrom very high time resolution, non-invasive, extracranial recordings of scalp\nelectric potential differences (EEG: electroencephalogram) and magnetic fields\n(MEG: magnetoencephalogram) by means of a tomography such as sLORETA\n(standardized low resolution brain electromagnetic tomography). There are two\nproblems in this case. First, in the usual situation of unknown cortical\ngeometry, the estimated signal at each brain location is a vector with three\ncomponents (i.e. a current density vector), which means that coherence and\nphase synchronization must be generalized to pairs of multivariate time series.\nSecond, the inherent low spatial resolution of the EEG/MEG tomography\nintroduces artificially high zero-lag coherence and phase synchronization. In\nthis report, solutions to both problems are presented. Two additional\ngeneralizations are briefly mentioned: (1) conditional coherence and phase\nsynchronization; and (2) non-stationary time-frequency analysis. Finally, a\nnon-parametric randomization method for connectivity significance testing is\noutlined. The new connectivity measures proposed here can be applied to pairs\nof univariate EEG/MEG signals, as is traditional in the published literature.\nHowever, these calculations cannot be interpreted as connectivity, since it is\nin general incorrect to associate an extracranial electrode or sensor to the\nunderlying cortex.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2007 19:48:30 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2007 11:35:14 GMT"}, {"version": "v3", "created": "Thu, 12 Jul 2007 12:24:44 GMT"}], "update_date": "2007-07-12", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""]]}, {"id": "0706.2024", "submitter": "Eben Kenah", "authors": "Eben Kenah, Marc Lipsitch, James M. Robins", "title": "Generation interval contraction and epidemic data analysis", "comments": "20 pages, 5 figures; to appear in Mathematical Biosciences", "journal-ref": "Mathematical Biosciences 213(1): 71-79 (May, 2008)", "doi": null, "report-no": null, "categories": "q-bio.QM math.PR stat.AP", "license": null, "abstract": "  The generation interval is the time between the infection time of an infected\nperson and the infection time of his or her infector. Probability density\nfunctions for generation intervals have been an important input for epidemic\nmodels and epidemic data analysis. In this paper, we specify a general\nstochastic SIR epidemic model and prove that the mean generation interval\ndecreases when susceptible persons are at risk of infectious contact from\nmultiple sources. The intuition behind this is that when a susceptible person\nhas multiple potential infectors, there is a ``race'' to infect him or her in\nwhich only the first infectious contact leads to infection. In an epidemic, the\nmean generation interval contracts as the prevalence of infection increases. We\ncall this global competition among potential infectors. When there is rapid\ntransmission within clusters of contacts, generation interval contraction can\nbe caused by a high local prevalence of infection even when the global\nprevalence is low. We call this local competition among potential infectors.\nUsing simulations, we illustrate both types of competition.\n  Finally, we show that hazards of infectious contact can be used instead of\ngeneration intervals to estimate the time course of the effective reproductive\nnumber in an epidemic. This approach leads naturally to partial likelihoods for\nepidemic data that are very similar to those that arise in survival analysis,\nopening a promising avenue of methodological research in infectious disease\nepidemiology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2007 02:00:03 GMT"}, {"version": "v2", "created": "Fri, 7 Dec 2007 02:02:05 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2008 00:44:47 GMT"}], "update_date": "2008-07-02", "authors_parsed": [["Kenah", "Eben", ""], ["Lipsitch", "Marc", ""], ["Robins", "James M.", ""]]}, {"id": "0706.3434", "submitter": "Shuheng Zhou", "authors": "Avrim Blum, Amin Coja-Oghlan, Alan Frieze, Shuheng Zhou", "title": "Separating populations with wide data: A spectral analysis", "comments": "Published in at http://dx.doi.org/10.1214/08-EJS289 the Electronic\n  Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Electronic Journal of Statistics 2009, Vol. 3, 76-113", "doi": "10.1214/08-EJS289", "report-no": "IMS-EJS-EJS_2008_289", "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of partitioning a small data sample\ndrawn from a mixture of $k$ product distributions. We are interested in the\ncase that individual features are of low average quality $\\gamma$, and we want\nto use as few of them as possible to correctly partition the sample. We analyze\na spectral technique that is able to approximately optimize the total data\nsize--the product of number of data points $n$ and the number of features\n$K$--needed to correctly perform this partitioning as a function of $1/\\gamma$\nfor $K>n$. Our goal is motivated by an application in clustering individuals\naccording to their population of origin using markers, when the divergence\nbetween any two of the populations is small.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2007 08:03:25 GMT"}, {"version": "v2", "created": "Thu, 29 Jan 2009 11:31:54 GMT"}], "update_date": "2017-11-17", "authors_parsed": [["Blum", "Avrim", ""], ["Coja-Oghlan", "Amin", ""], ["Frieze", "Alan", ""], ["Zhou", "Shuheng", ""]]}, {"id": "0706.3443", "submitter": "John Aston", "authors": "Jyh-Ying Peng, John A. D. Aston", "title": "The SSM Toolbox for Matlab", "comments": "Software available from authors", "journal-ref": null, "doi": null, "report-no": "C-2007-02", "categories": "stat.CO stat.AP", "license": null, "abstract": "  State Space Models (SSM) is a MATLAB 7.0 software toolbox for doing time\nseries analysis by state space methods. The software features fully interactive\nconstruction and combination of models, with support for univariate and\nmultivariate models, complex time-varying (dynamic) models, non-Gaussian\nmodels, and various standard models such as ARIMA and structural time-series\nmodels. The software includes standard functions for Kalman filtering and\nsmoothing, simulation smoothing, likelihood evaluation, parameter estimation,\nsignal extraction and forecasting, with incorporation of exact initialization\nfor filters and smoothers, and support for missing observations and multiple\ntime series input with common analysis structure. The software also includes\nimplementations of TRAMO model selection and Hillmer-Tiao decomposition for\nARIMA models. The software will provide a general toolbox for doing time series\nanalysis on the MATLAB platform, allowing users to take advantage of its\nreadily available graph plotting and general matrix computation capabilities.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2007 09:45:48 GMT"}], "update_date": "2007-06-26", "authors_parsed": [["Peng", "Jyh-Ying", ""], ["Aston", "John A. D.", ""]]}, {"id": "0706.3985", "submitter": "John Aston", "authors": "John A. D. Aston, Donald E. K. Martin", "title": "Distributions associated with general runs and patterns in hidden Markov\n  models", "comments": "Published in at http://dx.doi.org/10.1214/07-AOAS125 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2007, Vol. 1, No. 2, 585-611", "doi": "10.1214/07-AOAS125", "report-no": "IMS-AOAS-AOAS125", "categories": "stat.ME stat.AP stat.CO", "license": null, "abstract": "  This paper gives a method for computing distributions associated with\npatterns in the state sequence of a hidden Markov model, conditional on\nobserving all or part of the observation sequence. Probabilities are computed\nfor very general classes of patterns (competing patterns and generalized later\npatterns), and thus, the theory includes as special cases results for a large\nclass of problems that have wide application. The unobserved state sequence is\nassumed to be Markovian with a general order of dependence. An auxiliary Markov\nchain is associated with the state sequence and is used to simplify the\ncomputations. Two examples are given to illustrate the use of the methodology.\nWhereas the first application is more to illustrate the basic steps in applying\nthe theory, the second is a more detailed application to DNA sequences, and\nshows that the methods can be adapted to include restrictions related to\nbiological knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2007 09:15:55 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2007 14:49:46 GMT"}], "update_date": "2007-12-18", "authors_parsed": [["Aston", "John A. D.", ""], ["Martin", "Donald E. K.", ""]]}, {"id": "0706.4108", "submitter": "John Rice", "authors": "Peter Bickel, Bas Kleijn, and John Rice", "title": "Event Weighted Tests for Detecting Periodicity in Photon Arrival Times", "comments": null, "journal-ref": null, "doi": "10.1086/590399", "report-no": null, "categories": "stat.ME astro-ph stat.AP", "license": null, "abstract": "  This paper treats the problem of detecting periodicity in a sequence of\nphoton arrival times, which occurs, for example, in attempting to detect\ngamma-ray pulsars. A particular focus is on how auxiliary information,\ntypically source intensity, background intensity, and incidence angles and\nenergies associated with each photon arrival should be used to maximize the\ndetection power. We construct a class of likelihood-based tests, score tests,\nwhich give rise to event weighting in a principled and natural way, and derive\nexpressions quantifying the power of the tests. These results can be used to\ncompare the efficacies of different weight functions, including cuts in energy\nand incidence angle. The test is targeted toward a template for the periodic\nlightcurve, and we quantify how deviation from that template affects the power\nof detection.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2007 22:00:46 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Bickel", "Peter", ""], ["Kleijn", "Bas", ""], ["Rice", "John", ""]]}]