[{"id": "2012.00077", "submitter": "Anusha Lalitha", "authors": "Anusha Lalitha and Tara Javidi", "title": "On Error Exponents of Almost-Fixed-Length Channel Codes and Hypothesis\n  Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new class of channel coding strategies, and hypothesis tests\nreferred to as almost-fixed-length strategies that have little flexibility in\nthe stopping time over fixed-length strategies. The stopping time of these\nstrategies is allowed to be slightly large only on a rare set of sample paths\nwith an exponentially small probability. We show that almost-fixed-length\nchannel coding strategies can achieve Burnashev's optimal error exponent.\nSimilarly, almost-fixed length hypothesis tests are shown to bridge the gap\nbetween hypothesis testing with fixed sample size and sequential hypothesis\ntesting and improve the trade-off between type-I and type-II error exponents.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 20:01:58 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Lalitha", "Anusha", ""], ["Javidi", "Tara", ""]]}, {"id": "2012.00110", "submitter": "Andrew Miller", "authors": "Jeffrey Chan, Andrew C. Miller, Emily B. Fox", "title": "Representing and Denoising Wearable ECG Recordings", "comments": "ML for Mobile Health Workshop, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern wearable devices are embedded with a range of noninvasive biomarker\nsensors that hold promise for improving detection and treatment of disease. One\nsuch sensor is the single-lead electrocardiogram (ECG) which measures\nelectrical signals in the heart. The benefits of the sheer volume of ECG\nmeasurements with rich longitudinal structure made possible by wearables come\nat the price of potentially noisier measurements compared to clinical ECGs,\ne.g., due to movement. In this work, we develop a statistical model to simulate\na structured noise process in ECGs derived from a wearable sensor, design a\nbeat-to-beat representation that is conducive for analyzing variation, and\ndevise a factor analysis-based method to denoise the ECG. We study synthetic\ndata generated using a realistic ECG simulator and a structured noise model. At\nvarying levels of signal-to-noise, we quantitatively measure an upper bound on\nperformance and compare estimates from linear and non-linear models. Finally,\nwe apply our method to a set of ECGs collected by wearables in a mobile health\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 21:33:11 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Chan", "Jeffrey", ""], ["Miller", "Andrew C.", ""], ["Fox", "Emily B.", ""]]}, {"id": "2012.00180", "submitter": "John R.J. Thompson", "authors": "John R.J. Thompson, W. John Braun", "title": "Anisotropic local constant smoothing for change-point regression\n  function estimation", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding forest fire spread in any region of Canada is critical to\npromoting forest health, and protecting human life and infrastructure.\nQuantifying fire spread from noisy images, where regions of a fire are\nseparated by change-point boundaries, is critical to faithfully estimating fire\nspread rates. In this research, we develop a statistically consistent smooth\nestimator that allows us to denoise fire spread imagery from micro-fire\nexperiments. We develop an anisotropic smoothing method for change-point data\nthat uses estimates of the underlying data generating process to inform\nsmoothing. We show that the anisotropic local constant regression estimator is\nconsistent with convergence rate $O\\left(n^{-1/{(q+2)}}\\right)$. We demonstrate\nits effectiveness on simulated one- and two-dimensional change-point data and\nfire spread imagery from micro-fire experiments.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 00:11:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Thompson", "John R. J.", ""], ["Braun", "W. John", ""]]}, {"id": "2012.00229", "submitter": "Bing Xu", "authors": "Bing Xu, Jinfeng Wang, Zhongjie Li, Chengdong Xu, Yilan Liao, Maogui\n  Hu, Jing Yang, Shengjie Lai, Liping Wang, Weizhong Yang", "title": "Seasonal association between viral causes of hospitalised acute lower\n  respiratory infections and meteorological factors in China: a retrospective\n  study", "comments": "6 figures and tables", "journal-ref": "The Lancet Planetary Health, 2021", "doi": "10.1016/S2542-5196(20)30297-7", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Acute lower respiratory infections caused by respiratory viruses are common\nand persistent infectious diseases worldwide and in China, which have\npronounced seasonal patterns. Meteorological factors have important roles in\nthe seasonality of some major viruses. Our aim was to identify the dominant\nmeteorological factors and to model their effects on common respiratory viruses\nin different regions of China. We analysed monthly virus data on patients from\n81 sentinel hospitals in 22 provinces in mainland China from 2009 to 2013. The\ngeographical detector method was used to quantify the explanatory power of each\nmeteorological factor, individually and interacting in pairs. 28369\nhospitalised patients with ALRI were tested, 10387 were positive for at least\none virus, including RSV, influenza virus, PIV, ADV, hBoV, hCoV and hMPV. RSV\nand influenza virus had annual peaks in the north and biannual peaks in the\nsouth. PIV and hBoV had higher positive rates in the spring summer months. hMPV\nhad an annual peak in winter spring, especially in the north. ADV and hCoV\nexhibited no clear annual seasonality. Temperature, atmospheric pressure,\nvapour pressure, and rainfall had most explanatory power on most respiratory\nviruses in each region. Relative humidity was only dominant in the north, but\nhad no significant explanatory power for most viruses in the south. Hours of\nsunlight had significant explanatory power for RSV and influenza virus in the\nnorth, and for most viruses in the south. Wind speed was the only factor with\nsignificant explanatory power for human coronavirus in the south. For all\nviruses, interactions between any two of the paired factors resulted in\nenhanced explanatory power, either bivariately or non-linearly.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 02:59:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 05:40:51 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xu", "Bing", ""], ["Wang", "Jinfeng", ""], ["Li", "Zhongjie", ""], ["Xu", "Chengdong", ""], ["Liao", "Yilan", ""], ["Hu", "Maogui", ""], ["Yang", "Jing", ""], ["Lai", "Shengjie", ""], ["Wang", "Liping", ""], ["Yang", "Weizhong", ""]]}, {"id": "2012.00368", "submitter": "Angela Andreella", "authors": "Angela Andreella, Jesse Hemerik, Wouter Weeda, Livio Finos, Jelle\n  Goeman", "title": "Permutation-based true discovery proportions for fMRI cluster analysis", "comments": "16 pages, 11 figures, submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general permutation-based closed testing method to compute a\nsimultaneous lower confidence bound for the true discovery proportions of all\npossible subsets of a hypothesis testing problem. It is particularly useful in\nfunctional Magnetic Resonance Imaging cluster analysis, where it is of interest\nto select a cluster of voxels and to provide a confidence statement on the\npercentage of truly activated voxels within that cluster, avoiding the\nwell-known spatial specificity paradox. We offer a user-friendly tool to find\nthe percentage of true discoveries for each cluster while controlling the\nfamilywise error rate for multiple testing and taking into account that the\ncluster was chosen in a data-driven way. Permutation theory adapts to the\nspatial correlation structure that characterizes functional Magnetic Resonance\nImaging data and therefore gains power over parametric approaches.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 09:51:57 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Andreella", "Angela", ""], ["Hemerik", "Jesse", ""], ["Weeda", "Wouter", ""], ["Finos", "Livio", ""], ["Goeman", "Jelle", ""]]}, {"id": "2012.00394", "submitter": "Swapnil Mishra", "authors": "Samir Bhatt, Neil Ferguson, Seth Flaxman, Axel Gandy, Swapnil Mishra,\n  James A. Scott", "title": "Semi-Mechanistic Bayesian Modeling of COVID-19 with Renewal Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a general Bayesian approach to modeling epidemics such as\nCOVID-19. The approach grew out of specific analyses conducted during the\npandemic, in particular an analysis concerning the effects of\nnon-pharmaceutical interventions (NPIs) in reducing COVID-19 transmission in 11\nEuropean countries. The model parameterizes the time varying reproduction\nnumber $R_t$ through a regression framework in which covariates can e.g be\ngovernmental interventions or changes in mobility patterns. This allows a joint\nfit across regions and partial pooling to share strength. This innovation was\ncritical to our timely estimates of the impact of lockdown and other NPIs in\nthe European epidemics, whose validity was borne out by the subsequent course\nof the epidemic. Our framework provides a fully generative model for latent\ninfections and observations deriving from them, including deaths, cases,\nhospitalizations, ICU admissions and seroprevalence surveys. One issue\nsurrounding our model's use during the COVID-19 pandemic is the confounded\nnature of NPIs and mobility. We use our framework to explore this issue. We\nhave open sourced an R package epidemia implementing our approach in Stan.\nVersions of the model are used by New York State, Tennessee and Scotland to\nestimate the current situation and make policy decisions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:51:09 GMT"}, {"version": "v2", "created": "Tue, 29 Dec 2020 09:00:05 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Bhatt", "Samir", ""], ["Ferguson", "Neil", ""], ["Flaxman", "Seth", ""], ["Gandy", "Axel", ""], ["Mishra", "Swapnil", ""], ["Scott", "James A.", ""]]}, {"id": "2012.00397", "submitter": "Xinyu Wang", "authors": "Xinyu Wang, Lu Yang, Hong Zhang, Zhouwang Yang and Catherine Liu", "title": "Forecasting confirmed cases of the COVID-19 pandemic with a\n  migration-based epidemiological model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unprecedented coronavirus disease 2019 (COVID-19) pandemic is still a\nworldwide threat to human life since its invasion into the daily lives of the\npublic in the first several months of 2020. Predicting the size of confirmed\ncases is important for countries and communities to make proper prevention and\ncontrol policies so as to effectively curb the spread of COVID-19. Different\nfrom the 2003 SARS epidemic and the worldwide 2009 H1N1 influenza pandemic,\nCOVID-19 has unique epidemiological characteristics in its infectious and\nrecovered compartments. This drives us to formulate a new infectious dynamic\nmodel for forecasting the COVID-19 pandemic within the human mobility network,\nnamed the SaucIR-model in the sense that the new compartmental model extends\nthe benchmark SIR model by dividing the flow of people in the infected state\ninto asymptomatic, pathologically infected but unconfirmed, and confirmed.\nFurthermore, we employ dynamic modeling of population flow in the model in\norder that spatial effects can be incorporated effectively. We forecast the\nspread of accumulated confirmed cases in some provinces of mainland China and\nother countries that experienced severe infection during the time period from\nlate February to early May 2020. The novelty of incorporating the geographic\nspread of the pandemic leads to a surprisingly good agreement with published\nconfirmed case reports. The numerical analysis validates the high degree of\npredictability of our proposed SaucIR model compared to existing resemblance.\nThe proposed forecasting SaucIR model is implemented in Python. A web-based\napplication is also developed by Dash (under construction).\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 10:54:18 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Wang", "Xinyu", ""], ["Yang", "Lu", ""], ["Zhang", "Hong", ""], ["Yang", "Zhouwang", ""], ["Liu", "Catherine", ""]]}, {"id": "2012.00513", "submitter": "S{\\o}ren B. Vilsen", "authors": "S{\\o}ren B. Vilsen, Torben Tvedebrink, and Poul Svante Eriksen", "title": "DNA mixture deconvolution using an evolutionary algorithm with multiple\n  populations, hill-climbing, and guided mutation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  DNA samples crime cases analysed in forensic genetics, frequently contain DNA\nfrom multiple contributors. These occur as convolutions of the DNA profiles of\nthe individual contributors to the DNA sample. Thus, in cases where one or more\nof the contributors were unknown, an objective of interest would be the\nseparation, often called deconvolution, of these unknown profiles. In order to\nobtain deconvolutions of the unknown DNA profiles, we introduced a multiple\npopulation evolutionary algorithm (MEA). We allowed the mutation operator of\nthe MEA to utilise that the fitness is based on a probabilistic model and guide\nit by using the deviations between the observed and the expected value for\nevery element of the encoded individual. This guided mutation operator (GM) was\ndesigned such that the larger the deviation the higher probability of mutation.\nFurthermore, the GM was inhomogeneous in time, decreasing to a specified lower\nbound as the number of iterations increased. We analysed 102 two-person DNA\nmixture samples in varying mixture proportions. The samples were quantified\nusing two different DNA prep. kits: (1) Illumina ForenSeq Panel B (30 samples),\nand (2) Applied Biosystems Precision ID Globalfiler NGS STR panel (72 samples).\nThe DNA mixtures were deconvoluted by the MEA and compared to the true DNA\nprofiles of the sample. We analysed three scenarios where we assumed: (1) the\nDNA profile of the major contributor was unknown, (2) DNA profile of the minor\nwas unknown, and (3) both DNA profiles were unknown. Furthermore, we conducted\na series of sensitivity experiments on the ForenSeq panel by varying the\nsub-population size, comparing a completely random homogeneous mutation\noperator to the guided operator with varying mutation decay rates, and allowing\nfor hill-climbing of the parent population.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 14:23:55 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Vilsen", "S\u00f8ren B.", ""], ["Tvedebrink", "Torben", ""], ["Eriksen", "Poul Svante", ""]]}, {"id": "2012.00538", "submitter": "Zihuan Liu", "authors": "Zihuan Liu, Tapabrate Maiti and Andrew R.Bender", "title": "A Role for Prior Knowledge in Statistical Classification of the\n  Transition from MCI to Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The transition from mild cognitive impairment (MCI) to Alzheimer's disease\n(AD) is of great interest to clinical researchers. This phenomenon also serves\nas a valuable data source for quantitative methodological researchers\ndeveloping new approaches for classification. However, the growth of machine\nlearning (ML) approaches for classification may falsely lead many clinical\nresearchers to underestimate the value of logistic regression (LR), yielding\nequivalent or superior classification accuracy over other ML methods. Further,\nin applications with many features that could be used for classifying the\ntransition, clinical researchers are often unaware of the relative value of\ndifferent selection procedures. In the present study, we sought to investigate\nthe use of automated and theoretically-guided feature selection techniques, and\nas well as the L-1 norm when applying different classification techniques for\npredicting conversion from MCI to AD in a highly characterized and studied\nsample from the Alzheimer's Disease Neuroimaging Initiative (ADNI). We propose\nan alternative pre-selection technique that utilizes an efficient feature\nselection based on clinical knowledge of brain regions involved in AD. The\npresent findings demonstrate how similar performance can be achieved using\nuser-guided pre-selection versus algorithmic feature selection techniques.\nFinally, we compare the performance of a support vector machine (SVM) with that\nof logistic regression on multi-modal data from ADNI. The present findings show\nthat although SVM and other ML techniques are capable of relatively accurate\nclassification, similar or higher accuracy can often be achieved by LR,\nmitigating SVM's necessity or value for many clinical researchers.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2020 18:15:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Liu", "Zihuan", ""], ["Maiti", "Tapabrate", ""], ["Bender", "Andrew R.", ""]]}, {"id": "2012.00629", "submitter": "Antonio Maria Scarfone", "authors": "Giorgio Kaniadakis, Mauro M. Baldi, Thomas S. Deisboeck, Giulia\n  Grisolia, Dionissios T. Hristopulos, Antonio M. Scarfone, Amelia Sparavigna,\n  Tatsuaki Wada and Umberto Lucia", "title": "The k-statistics approach to epidemiology", "comments": "15 pages, 1 table, 5 figures", "journal-ref": "Scientific Report (2020) 10:19949", "doi": "10.1038/s41598-020-76673-3", "report-no": null, "categories": "q-bio.PE nlin.AO physics.bio-ph physics.soc-ph stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  A great variety of complex physical, natural and artificial systems are\ngoverned by statistical distributions, which often follow a standard\nexponential function in the bulk, while their tail obeys the Pareto power law.\nThe recently introduced $\\kappa$-statistics framework predicts distribution\nfunctions with this feature. A growing number of applications in different\nfields of investigation are beginning to prove the relevance and effectiveness\nof $\\kappa$-statistics in fitting empirical data. In this paper, we use\n$\\kappa$-statistics to formulate a statistical approach for epidemiological\nanalysis. We validate the theoretical results by fitting the derived\n$\\kappa$-Weibull distributions with data from the plague pandemic of 1417 in\nFlorence as well as data from the COVID-19 pandemic in China over the entire\ncycle that concludes in April 16, 2020. As further validation of the proposed\napproach we present a more systematic analysis of COVID-19 data from countries\nsuch as Germany, Italy, Spain and United Kingdom, obtaining very good agreement\nbetween theoretical predictions and empirical observations. For these countries\nwe also study the entire first cycle of the pandemic which extends until the\nend of July 2020. The fact that both the data of the Florence plague and those\nof the Covid-19 pandemic are successfully described by the same theoretical\nmodel, even though the two events are caused by different diseases and they are\nseparated by more than 600 years, is evidence that the $\\kappa$-Weibull model\nhas universal features.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2020 16:15:24 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Kaniadakis", "Giorgio", ""], ["Baldi", "Mauro M.", ""], ["Deisboeck", "Thomas S.", ""], ["Grisolia", "Giulia", ""], ["Hristopulos", "Dionissios T.", ""], ["Scarfone", "Antonio M.", ""], ["Sparavigna", "Amelia", ""], ["Wada", "Tatsuaki", ""], ["Lucia", "Umberto", ""]]}, {"id": "2012.00657", "submitter": "Carmen Armero", "authors": "Carmen Armero and Gonzalo Garc\\'ia-Donato and Joaqu\\'in\n  Jim\\'enez-Puerto and Salvador Pardo-Gord\\'o and Joan Bernabeu", "title": "Bayesian classification for dating archaeological sites via projectile\n  points", "comments": "12 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dating is a key element for archaeologists. We propose a Bayesian approach to\nprovide chronology to sites that have neither radiocarbon dating nor clear\nstratigraphy and whose only information comes from lithic arrowheads. This\nclassifier is based on the Dirichlet-multinomial inferential process and\nposterior predictive distributions. The procedure is applied to predict the\nperiod of a set of undated sites located in the east of the Iberian Peninsula\nduring the IVth and IIIrd millennium cal. BC.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:31:24 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Armero", "Carmen", ""], ["Garc\u00eda-Donato", "Gonzalo", ""], ["Jim\u00e9nez-Puerto", "Joaqu\u00edn", ""], ["Pardo-Gord\u00f3", "Salvador", ""], ["Bernabeu", "Joan", ""]]}, {"id": "2012.00662", "submitter": "George Mohler", "authors": "Martin B. Short and George O. Mohler", "title": "A Fully Bayesian, Logistic Regression Tracking Algorithm for Mitigating\n  Disparate Misclassification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully Bayesian, logistic tracking algorithm with the purpose of\nproviding classification results that are unbiased when applied uniformly to\nindividuals with differing sensitive variable values. Here, we consider bias in\nthe form of differences in false prediction rates between the different\nsensitive variable groups. Given that the method is fully Bayesian, it is well\nsuited for situations where group parameters or logistic regression\ncoefficients are dynamic quantities. We illustrate our method, in comparison to\nothers, on both simulated datasets and the well-known ProPublica COMPAS\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 24 Sep 2020 18:42:48 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Short", "Martin B.", ""], ["Mohler", "George O.", ""]]}, {"id": "2012.00667", "submitter": "Krzysztof Kotowski", "authors": "W. Sommer, K. Stapor, G. Konczak, K. Kotowski, P. Fabian, J. Ochab, A.\n  Beres, G. Slusarczyk", "title": "Single trial ERP amplitudes reveal the time course of acquiring\n  representations of novel faces in individual participants", "comments": "36 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The neural correlates of face individuation - the acquisition of memory\nrepresentations for novel faces - have been studied only in coarse detail and\ndisregarding individual differences between learners. In their seminal study,\n(Tanaka, Curran, Porterfield, & Collins, 2006) required the identification of a\nparticular novel face across 70 trials and found that the N250 component in the\nERP became more negative from the first to the second half of the experiment,\nwhere it reached a similar amplitude as a well-known face. We were unable to\ndirectly replicate this finding in our study when we used the original split of\ntrials. However, when we applied a different split of trials we observed very\nsimilar changes in N250 amplitude. Then, we developed and applied a new\ntwo-step explorative-confirmative non-parametric method based on permutation\ntesting to determine the time course of face individuation in individual\nparticipants based on single-trial N250 amplitudes. We show that the assumption\nof a steep initial increase of N250 amplitude across multiple presentations of\nthe target face, followed by a plateau, yields plausible results in fitting\nlinear trends for most participants. The transition point from initial\nacquisition to the plateau phase differed strongly between participants and\ntended to be earlier when performance in target face recognition was better.\nHence, face individuation may be accounted for by a biphasic process of early,\nfast acquisition, followed by a slower, asymptotic consolidation or maintenance\nphase. The current approach might be fruitfully applied to further\ninvestigations into face individuation and their neural correlates\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 17:39:40 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Sommer", "W.", ""], ["Stapor", "K.", ""], ["Konczak", "G.", ""], ["Kotowski", "K.", ""], ["Fabian", "P.", ""], ["Ochab", "J.", ""], ["Beres", "A.", ""], ["Slusarczyk", "G.", ""]]}, {"id": "2012.00673", "submitter": "Andrzej Jaszkiewicz", "authors": "Andrzej Jaszkiewicz", "title": "Modified Dorfman procedure for pool tests with dilution -- COVID-19 case\n  study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The outbreak of the global COVID-19 pandemic results in unprecedented demand\nfor fast and efficient testing of large numbers of patients for the presence of\nSARS-CoV-2 coronavirus. Beside technical improvements of the cost and speed of\nindividual tests, pool testing may be used to improve efficiency and throughput\nof a population test. Dorfman pool testing procedure is one of the best known\nand studied methods of this kind. This procedure is, however, based on\nunrealistic assumptions that the pool test has perfect sensitivity and the only\nobjective is to minimize the number of tests, and is not well adapted to the\ncase of imperfect pool tests. We propose and analyze a simple modification of\nthis procedure in which test of a pool with negative result is independently\nrepeated up to several times. The proposed procedure is evaluated in a\ncomputational study using recent data about dilution effect for SARS-CoV-2 PCR\ntests, showing that the proposed approach significantly reduces the number of\nfalse negatives with a relatively small increase of the number of tests,\nespecially for small prevalence rates. For example, for prevalence rate 0.001\nthe number of tests could be reduced to 22.1% of individual tests, increasing\nthe expected number of false negatives by no more than 1%, and to 16.8% of\nindividual tests increasing the expected number of false negatives by no more\nthan 10%. At the same time, a similar reduction of the expected number of tests\nin the standard Dorfman procedure would yield 675% and 821% increase of the\nexpected number of false negatives, respectively. This makes the proposed\nprocedure an interesting choice for screening tests in the case of diseases\nlike COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2020 08:53:08 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 08:15:32 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Jaszkiewicz", "Andrzej", ""]]}, {"id": "2012.00722", "submitter": "Oscar Claveria", "authors": "Oscar Claveria", "title": "Measuring and assessing economic uncertainty", "comments": "22 pages, 6 figures", "journal-ref": "IREA Working Papers, 2020/11", "doi": null, "report-no": "IREA Working Papers, 2020/11", "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper evaluates the dynamic response of economic activity to shocks in\nuncertainty as percieved by agents.The study focuses on the comparison between\nthe perception of economic uncertainty by manufacturers and consumers.Since\nuncertainty is not directly observable, we approximate it using the geometric\ndiscrepancy indicator of Claveria et al.(2019).This approach allows us\nquantifying the proportion of disagreement in business and consumer\nexpectations of eleven European countries and the Euro Area.First, we compute\nthree independent indices of discrepancy corresponding to three dimensions of\nuncertainty (economic, inflation and employment) and we average them to obtain\naggregate disagreement measures for businesses and for consumers.Next, we use a\nbivariate Bayesian vector autoregressive framework to estimate the impulse\nresponse functions to innovations in disagreement in every country.We find that\nthe effect on economic activity of shocks to the perception of uncertainty\ndiffer markedly between manufacturers and consumers.On the one hand, shocks to\nconsumer discrepancy tend to be of greater magnitude and duration than those to\nmanufacturer discrepancy.On the other hand, innovations in disagreement between\nthe two collectives have an opposite effect on economic activity:shocks to\nmanufacturer discrepancy lead to a decrease in economic activity, as opposed to\nshocks to consumer discrepancy.This finding is of particular relevance to\nresearchers when using cross-sectional dispersion of survey-based expectations,\nsince the effect on economic growth of shocks to disagreement depend on the\ntype of agent.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 18:32:32 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Claveria", "Oscar", ""]]}, {"id": "2012.00789", "submitter": "Adam Sykulski Dr", "authors": "Sarah Oscroft, Adam M. Sykulski, Jeffrey J. Early", "title": "Separating Mesoscale and Submesoscale Flows from Clustered Drifter\n  Trajectories", "comments": "Accepted in Fluids", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph eess.SP physics.data-an physics.flu-dyn stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Drifters deployed in close proximity collectively provide a unique\nobservational data set with which to separate mesoscale and submesoscale flows.\nIn this paper we provide a principled approach for doing so by fitting observed\nvelocities to a local Taylor expansion of the velocity flow field. We\ndemonstrate how to estimate mesoscale and submesoscale quantities that evolve\nslowly over time, as well as their associated statistical uncertainty. We show\nthat in practice the mesoscale component of our model can explain much first\nand second-moment variability in drifter velocities, especially at low\nfrequencies. This results in much lower and more meaningful measures of\nsubmesoscale diffusivity, which would otherwise be contaminated by unresolved\nmesoscale flow. We quantify these effects theoretically via computing\nLagrangian frequency spectra, and demonstrate the usefulness of our methodology\nthrough simulations as well as with real observations from the LatMix\ndeployment of drifters. The outcome of this method is a full Lagrangian\ndecomposition of each drifter trajectory into three components that represent\nthe background, mesoscale, and submesoscale flow.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 19:35:11 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 15:07:14 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Oscroft", "Sarah", ""], ["Sykulski", "Adam M.", ""], ["Early", "Jeffrey J.", ""]]}, {"id": "2012.00838", "submitter": "Gourab Ghoshal", "authors": "Hugo Barbosa, Surendra Hazarie, Brian Dickinson, Aleix Bassolas, Adam\n  Frank, Henry Kautz, Adam Sadilek, Jose J. Ramasco, Gourab Ghoshal", "title": "Uncovering the socioeconomic facets of human mobility", "comments": "main manuscript 25 page, 7 Figures. Supplementary material 11 pages,\n  8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the rapid recent trend of urbanization, a better understanding of how\nurban infrastructure mediates socioeconomic interactions and economic systems\nis of vital importance. While the accessibility of location-enabled devices as\nwell as large-scale datasets of human activities, has fueled significant\nadvances in our understanding, there is little agreement on the linkage between\nsocioeconomic status and its influence on movement patterns, in particular, the\nrole of inequality. Here, we analyze a heavily aggregated and anonymized\nsummary of global mobility and investigate the relationships between\nsocioeconomic status and mobility across a hundred cities in the US and Brazil.\nWe uncover two types of relationships, finding either a clear connection or\nlittle-to-no interdependencies. The former tend to be characterized by low\nlevels of public transportation usage, inequitable access to basic amenities\nand services, and segregated clusters of communities in terms of income, with\nthe latter class showing the opposite trends. Our findings provide useful\nlessons in designing urban habitats that serve the larger interests of all\ninhabitants irrespective of their economic status.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:30:10 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Barbosa", "Hugo", ""], ["Hazarie", "Surendra", ""], ["Dickinson", "Brian", ""], ["Bassolas", "Aleix", ""], ["Frank", "Adam", ""], ["Kautz", "Henry", ""], ["Sadilek", "Adam", ""], ["Ramasco", "Jose J.", ""], ["Ghoshal", "Gourab", ""]]}, {"id": "2012.00860", "submitter": "Siyu Heng", "authors": "Siyu Heng, Wendy P. O'Meara, Ryan A. Simmons, Dylan S. Small", "title": "Relationship between changing malaria burden and low birth weight in\n  sub-Saharan Africa: a difference-in-differences study via a pair-of-pairs\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although interventional studies demonstrate that preventing malaria during\npregnancy can reduce the low birth weight (i.e., child's birth weight $<$ 2,500\ngrams) rate, it remains unknown whether natural changes in parasite\ntransmission and malaria burden can improve birth outcomes. We conduct an\nobservational study of the effect of changing malaria burden on low birth\nweight using data from 18,112 births in 19 countries in sub-Saharan African\ncountries during the years 2000--2015. A malaria prevalence decline from a high\nrate (Plasmodium falciparum parasite rate in children aged 2-up-to-10 (i.e.,\n$Pf\\text{PR}_{2-10}$) $>$ 0.4) to a low rate ($Pf\\text{PR}_{2-10}$ $<$ 0.2) is\nestimated to reduce the rate of low birth weight by 1.48 percentage points (95%\nconfidence interval: 3.70 percentage points reduction, 0.74 percentage points\nincrease), which is a 17% reduction in the low birth weight rate compared to\nthe average (8.6%) in our study population with observed birth weight records\n(1.48/8.6 $\\approx$ 17%). When focusing on first pregnancies, a decline in\nmalaria prevalence from high to low is estimated to have a greater impact on\nthe low birth weight rate than for all births: 3.73 percentage points (95%\nconfidence interval: 9.11 percentage points reduction, 1.64 percentage points\nincrease). Although the confidence intervals cannot rule out the possibility of\nno effect at the 95% confidence level, the concurrence between our primary\nanalysis, secondary analyses, and sensitivity analyses, and the magnitude of\nthe effect size, contribute to the weight of the evidence suggesting that\ndeclining malaria burden has an important effect on birth weight at the\npopulation level. The novel statistical methodology developed in this article,\na pair-of-pairs approach to a difference-in-differences study, could be useful\nfor many settings in which the units observed are different at different times.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 21:57:39 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 02:09:56 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 23:12:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Heng", "Siyu", ""], ["O'Meara", "Wendy P.", ""], ["Simmons", "Ryan A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "2012.01099", "submitter": "Steven Nijman", "authors": "Steven WJ Nijman, Jeroen Hoogland, T Katrien J Groenhof, Menno\n  Brandjes, John JL Jacobs, Michiel L Bots, Folkert W Asselbergs, Karel GM\n  Moons, Thomas PA Debray", "title": "Real-time imputation of missing predictor values in clinical practice", "comments": "17 pages, 6 figures, to be published in European Heart Journal -\n  Digital Health, accepted for MEMTAB 2020 conference", "journal-ref": null, "doi": "10.1093/ehjdh/ztaa016", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Use of prediction models is widely recommended by clinical guidelines, but\nusually requires complete information on all predictors that is not always\navailable in daily practice. We describe two methods for real-time handling of\nmissing predictor values when using prediction models in practice. We compare\nthe widely used method of mean imputation (M-imp) to a method that personalizes\nthe imputations by taking advantage of the observed patient characteristics.\nThese characteristics may include both prediction model variables and other\ncharacteristics (auxiliary variables). The method was implemented using\nimputation from a joint multivariate normal model of the patient\ncharacteristics (joint modeling imputation; JMI). Data from two different\ncardiovascular cohorts with cardiovascular predictors and outcome were used to\nevaluate the real-time imputation methods. We quantified the prediction model's\noverall performance (mean squared error (MSE) of linear predictor),\ndiscrimination (c-index), calibration (intercept and slope) and net benefit\n(decision curve analysis). When compared with mean imputation, JMI\nsubstantially improved the MSE (0.10 vs. 0.13), c-index (0.70 vs 0.68) and\ncalibration (calibration-in-the-large: 0.04 vs. 0.06; calibration slope: 1.01\nvs. 0.92), especially when incorporating auxiliary variables. When the\nimputation method was based on an external cohort, calibration deteriorated,\nbut discrimination remained similar. We recommend JMI with auxiliary variables\nfor real-time imputation of missing values, and to update imputation models\nwhen implementing them in new settings or (sub)populations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 11:35:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Nijman", "Steven WJ", ""], ["Hoogland", "Jeroen", ""], ["Groenhof", "T Katrien J", ""], ["Brandjes", "Menno", ""], ["Jacobs", "John JL", ""], ["Bots", "Michiel L", ""], ["Asselbergs", "Folkert W", ""], ["Moons", "Karel GM", ""], ["Debray", "Thomas PA", ""]]}, {"id": "2012.01149", "submitter": "Qiwei Li", "authors": "Cong Zhang and Guanghua Xiao and Chul Moon and Min Chen and Qiwei Li", "title": "Bayesian Landmark-based Shape Analysis of Tumor Pathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical imaging is a form of technology that has revolutionized the medical\nfield in the past century. In addition to radiology imaging of tumor tissues,\ndigital pathology imaging, which captures histological details in high spatial\nresolution, is fast becoming a routine clinical procedure for cancer diagnosis\nsupport and treatment planning. Recent developments in deep-learning methods\nfacilitate the segmentation of tumor regions at almost the cellular level from\ndigital pathology images. The traditional shape features that were developed\nfor characterizing tumor boundary roughness in radiology are not applicable.\nReliable statistical approaches to modeling tumor shape in pathology images are\nin urgent need. In this paper, we consider the problem of modeling a tumor\nboundary with a closed polygonal chain. A Bayesian landmark-based shape\nanalysis (BayesLASA) model is proposed to partition the polygonal chain into\nmutually exclusive segments to quantify the boundary roughness piecewise. Our\nfully Bayesian inference framework provides uncertainty estimates of both the\nnumber and locations of landmarks. The BayesLASA outperforms a recently\ndeveloped landmark detection model for planar elastic curves in terms of\naccuracy and efficiency. We demonstrate how this model-based analysis can lead\nto sharper inferences than ordinary approaches through a case study on the 246\npathology images from 143 non-small cell lung cancer patients. The case study\nshows that the heterogeneity of tumor boundary roughness predicts patient\nprognosis (p-value < 0.001). This statistical methodology not only presents a\nnew model for characterizing a digitized object's shape features by using its\nlandmarks, but also provides a new perspective for understanding the role of\ntumor surface in cancer progression.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2020 00:04:09 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Cong", ""], ["Xiao", "Guanghua", ""], ["Moon", "Chul", ""], ["Chen", "Min", ""], ["Li", "Qiwei", ""]]}, {"id": "2012.01224", "submitter": "Hyunji Moon", "authors": "Hyunji Moon, Jinwoo Choi", "title": "Hierarchical spline for time series forecasting: An application to Naval\n  ship engine failure rate", "comments": null, "journal-ref": "2021 Applied AI Letters", "doi": "10.22541/au.159969715.57074848", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predicting equipment failure is important because it could improve\navailability and cut down the operating budget. Previous literature has\nattempted to model failure rate with bathtub-formed function, Weibull\ndistribution, Bayesian network, or AHP. But these models perform well with a\nsufficient amount of data and could not incorporate the two salient\ncharacteristics; imbalanced category and sharing structure. Hierarchical model\nhas the advantage of partial pooling. The proposed model is based on Bayesian\nhierarchical B-spline. Time series of the failure rate of 99 Republic of Korea\nNaval ships are modeled hierarchically, where each layer corresponds to ship\nengine, engine type, and engine archetype. As a result of the analysis, the\nsuggested model predicted the failure rate of an entire lifetime accurately in\nmultiple situational conditions, such as prior knowledge of the engine.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 14:13:35 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Moon", "Hyunji", ""], ["Choi", "Jinwoo", ""]]}, {"id": "2012.01265", "submitter": "Rossana Mastrandrea", "authors": "Francesco Picciolo, Franco Ruzzenenti, Rossana Mastrandrea", "title": "Random walk patterns to identify weighted motifs", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last two decades, network theory has shown to be a fruitful paradigm\nin understanding the organization and functioning of real complex systems.\nParticularly relevant in this sense appears the identification of significant\nsubgraphs that can shed light onto the underlying evolutionary processes. Such\npatterns, called motifs, have received much attention in binary networks, but a\nsimilar deep investigation for weighted networks is still lagging behind. Here,\nwe proposed a novel methodology based on a random walker and a fixed maximum\nnumber of steps to study weighted motifs of limited size. The novelty is\nrepresented by the introduction of a sink node to balance the network and allow\nthe detection of configurations within an a priori fixed number of steps for\nthe random walker. We applied this approach to different real networks and\nselected a specific benchmark model based on maximum-entropy to test the\nsignificance of weighted motifs occurrence. We found that identified\nsimilarities enable the classifications of systems according to functioning\nmechanisms associated with specific configurations.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 15:14:46 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Picciolo", "Francesco", ""], ["Ruzzenenti", "Franco", ""], ["Mastrandrea", "Rossana", ""]]}, {"id": "2012.01349", "submitter": "Abhinav Prakash", "authors": "Abhinav Prakash, Rui Tuo and Yu Ding", "title": "The temporal overfitting problem with applications in wind power curve\n  modeling", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with a nonparametric regression problem in which the\nindependence assumption of the input variables and the residuals is no longer\nvalid. Using existing model selection methods, like cross validation, the\npresence of temporal autocorrelation in the input variables and the error terms\nleads to model overfitting. This phenomenon is referred to as temporal\noverfitting, which causes loss of performance while predicting responses for a\ntime domain different from the training time domain. We propose a new method to\ntackle the temporal overfitting problem. Our nonparametric model is partitioned\ninto two parts -- a time-invariant component and a time-varying component, each\nof which is modeled through a Gaussian process regression. The key in our\ninference is a thinning-based strategy, an idea borrowed from Markov chain\nMonte Carlo sampling, to estimate the two components, respectively. Our\nspecific application in this paper targets the power curve modeling in wind\nenergy. In our numerical studies, we compare extensively our proposed method\nwith both existing power curve models and available ideas for handling temporal\noverfitting. Our approach yields significant improvement in prediction both in\nand outside the time domain covered by the training data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Dec 2020 17:39:57 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Prakash", "Abhinav", ""], ["Tuo", "Rui", ""], ["Ding", "Yu", ""]]}, {"id": "2012.01618", "submitter": "Yang Chen", "authors": "Hu Sun, Zhijun Hua, Jiaen Ren, Shasha Zou, Yuekai Sun, Yang Chen", "title": "Matrix Completion Methods for the Total Electron Content Video\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The total electron content (TEC) maps can be used to estimate the signal\ndelay of GPS due to the ionospheric electron content between a receiver and\nsatellite. This delay can result in GPS positioning error. Thus it is important\nto monitor the TEC maps. The observed TEC maps have big patches of missingness\nin the ocean and scattered small areas of missingness on the land. In this\npaper, we propose several extensions of existing matrix completion algorithms\nto achieve TEC map reconstruction, accounting for spatial smoothness and\ntemporal consistency while preserving important structures of the TEC maps. We\ncall the proposed method Video Imputation with SoftImpute, Temporal smoothing\nand Auxiliary data (VISTA). Numerical simulations that mimic patterns of real\ndata are given. We show that our proposed method achieves better reconstructed\nTEC maps as compared to existing methods in literature. Our proposed\ncomputational algorithm is general and can be readily applied for other\nproblems besides TEC map reconstruction.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:55:21 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sun", "Hu", ""], ["Hua", "Zhijun", ""], ["Ren", "Jiaen", ""], ["Zou", "Shasha", ""], ["Sun", "Yuekai", ""], ["Chen", "Yang", ""]]}, {"id": "2012.01619", "submitter": "Nicholas Tierney", "authors": "Nicholas J Tierney, Dianne Cook, Tania Prvan", "title": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and\n  Analytically in R", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal (panel) data provide the opportunity to examine temporal\npatterns of individuals, because measurements are collected on the same person\nat different, and often irregular, time points. The data is typically\nvisualised using a \"spaghetti plot\", where a line plot is drawn for each\nindividual. When overlaid in one plot, it can have the appearance of a bowl of\nspaghetti. With even a small number of subjects, these plots are too overloaded\nto be read easily. The interesting aspects of individual differences are lost\nin the noise. Longitudinal data is often modelled with a hierarchical linear\nmodel to capture the overall trends, and variation among individuals, while\naccounting for various levels of dependence. However, these models can be\ndifficult to fit, and can miss unusual individual patterns. Better visual tools\ncan help to diagnose longitudinal models, and better capture the individual\nexperiences. This paper introduces the R package, brolgar (BRowse over\nLongitudinal data Graphically and Analytically in R), which provides tools to\nidentify and summarise interesting individual patterns in longitudinal data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 00:55:42 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Tierney", "Nicholas J", ""], ["Cook", "Dianne", ""], ["Prvan", "Tania", ""]]}, {"id": "2012.01912", "submitter": "Michel Besserve", "authors": "Michel Besserve, Simon Buchholz and Bernhard Sch\\\"olkopf", "title": "Assaying Large-scale Testing Models to Interpret COVID-19 Case Numbers", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale testing is considered key to assess the state of the current\nCOVID-19 pandemic. Yet, the link between the reported case numbers and the true\nstate of the pandemic remains elusive. We develop mathematical models based on\ncompeting hypotheses regarding this link, thereby providing different\nprevalence estimates based on case numbers, and validate them by predicting\nSARS-CoV-2-attributed death rate trajectories. Assuming that individuals were\ntested based solely on a predefined risk of being infectious implies the\nabsolute case numbers reflect the prevalence, but turned out to be a poor\npredictor, consistently overestimating growth rates at the beginning of two\nCOVID-19 epidemic waves. In contrast, assuming that testing capacity is fully\nexploited performs better. This leads to using the percent-positive rate as a\nmore robust indicator of epidemic dynamics, however we find it is subject to a\nsaturation phenomenon that needs to be accounted for as the number of tests\nbecomes larger.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 13:50:21 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2021 09:05:53 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Besserve", "Michel", ""], ["Buchholz", "Simon", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2012.01937", "submitter": "Rajdip Nayek", "authors": "Rajdip Nayek, Ramon Fuentes, Keith Worden, Elizabeth J. Cross", "title": "On spike-and-slab priors for Bayesian equation discovery of nonlinear\n  dynamical systems via sparse linear regression", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2021.107986", "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents the use of spike-and-slab (SS) priors for discovering\ngoverning differential equations of motion of nonlinear structural dynamic\nsystems. The problem of discovering governing equations is cast as that of\nselecting relevant variables from a predetermined dictionary of basis variables\nand solved via sparse Bayesian linear regression. The SS priors, which belong\nto a class of discrete-mixture priors and are known for their strong\nsparsifying (or shrinkage) properties, are employed to induce sparse solutions\nand select relevant variables. Three different variants of SS priors are\nexplored for performing Bayesian equation discovery. As the posteriors with SS\npriors are analytically intractable, a Markov chain Monte Carlo (MCMC)-based\nGibbs sampler is employed for drawing posterior samples of the model\nparameters; the posterior samples are used for variable selection and parameter\nestimation in equation discovery. The proposed algorithm has been applied to\nfour systems of engineering interest, which include a baseline linear system,\nand systems with cubic stiffness, quadratic viscous damping, and Coulomb\ndamping. The results demonstrate the effectiveness of the SS priors in\nidentifying the presence and type of nonlinearity in the system. Additionally,\ncomparisons with the Relevance Vector Machine (RVM) - that uses a Student's-t\nprior - indicate that the SS priors can achieve better model selection\nconsistency, reduce false discoveries, and derive models that have superior\npredictive accuracy. Finally, the Silverbox experimental benchmark is used to\nvalidate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 14:14:31 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:45:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Nayek", "Rajdip", ""], ["Fuentes", "Ramon", ""], ["Worden", "Keith", ""], ["Cross", "Elizabeth J.", ""]]}, {"id": "2012.02021", "submitter": "S. Yaser Samadi", "authors": "Hadi Safari-Katesari, S. Yaser Samadi, Samira Zaroudi", "title": "Modeling Count Data via Copulas", "comments": "33 pages", "journal-ref": "Statistics 2020", "doi": "10.1080/02331888.2020.1867140", "report-no": "2020", "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Copula models have been widely used to model the dependence between\ncontinuous random variables, but modeling count data via copulas has recently\nbecome popular in the statistics literature. Spearman's rho is an appropriate\nand effective tool to measure the degree of dependence between two random\nvariables. In this paper, we derived the population version of Spearman's rho\ncorrelation via copulas when both random variables are discrete. The\nclosed-form expressions of the Spearman correlation are obtained for some\ncopulas of simple structure such as Archimedean copulas with different marginal\ndistributions. We derive the upper bound and the lower bound of the Spearman's\nrho for Bernoulli random variables. Then, the proposed Spearman's rho\ncorrelations are compared with their corresponding Kendall's tau values. We\ncharacterize the functional relationship between these two measures of\ndependence in some special cases. An extensive simulation study is conducted to\ndemonstrate the validity of our theoretical results. Finally, we propose a\nbivariate copula regression model to analyze the count data of a \\emph{cervical\ncancer} dataset.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 16:04:51 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Safari-Katesari", "Hadi", ""], ["Samadi", "S. Yaser", ""], ["Zaroudi", "Samira", ""]]}, {"id": "2012.02074", "submitter": "Xinyue Qi", "authors": "Xinyue Qi, Shouhao Zhou, Martyn Plummer", "title": "A Note on Bayesian Modeling Specification of Censored Data in JAGS", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just Another Gibbs Sampling (JAGS) is a convenient tool to draw posterior\nsamples using Markov Chain Monte Carlo for Bayesian modeling. However, the\nbuilt-in function dinterval() to model censored data misspecifies the\ncomputation of deviance function, which may limit its usage to perform\nlikelihood based model comparison. To establish an automatic approach to\nspecify the correct deviance function in JAGS, we propose a simple alternative\nmodeling strategy to implement Bayesian model selection for analysis of\ncensored outcomes. The proposed approach is applicable to a broad spectrum of\ndata types, which include survival data and many other right-, left- and\ninterval-censored Bayesian model structures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 17:06:11 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Qi", "Xinyue", ""], ["Zhou", "Shouhao", ""], ["Plummer", "Martyn", ""]]}, {"id": "2012.02099", "submitter": "Rory Bunker", "authors": "Rory Bunker and Kirsten Spencer", "title": "Performance Indicators Contributing To Success At The Group And Play-Off\n  Stages Of The 2019 Rugby World Cup", "comments": null, "journal-ref": null, "doi": "10.14198/jhse.2022.173.18", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance indicators that contributed to success at the group stage and\nplay-off stages of the 2019 Rugby World Cup were analysed using publicly\navailable data obtained from the official tournament website using both a\nnon-parametric statistical technique, Wilcoxon's signed rank test, and a\ndecision rules technique from machine learning called RIPPER. Our statistical\nresults found that ball carry effectiveness (percentage of ball carries that\npenetrated the opposition gain-line) and total metres gained (kick metres plus\ncarry metres) were found to contribute to success at both stages of the\ntournament and that indicators that contributed to success during the group\nstages (dominating possession, making more ball carries, making more passes,\nwinning more rucks, and making less tackles) did not contribute to success at\nthe play-off stage. Our results using RIPPER found that low ball carries and a\nlow lineout success percentage jointly contributed to losing at the group\nstage, while winning a low number of rucks and carrying over the gain-line a\nsufficient number of times contributed to winning at the play-off stage of the\ntournament. The results emphasise the need for teams to adapt their playing\nstrategies from the group stage to the play-off stage at tournament in order to\nbe successful.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 06:26:32 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Bunker", "Rory", ""], ["Spencer", "Kirsten", ""]]}, {"id": "2012.02100", "submitter": "Mikael Mieskolainen", "authors": "Mikael Mieskolainen, Robert Bainbridge, Oliver Buchmueller, Louis\n  Lyons, Nicholas Wardle", "title": "Statistical techniques to estimate the SARS-CoV-2 infection fatality\n  rate", "comments": "50 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The determination of the infection fatality rate (IFR) for the novel\nSARS-CoV-2 coronavirus is a key aim for many of the field studies that are\ncurrently being undertaken in response to the pandemic. The IFR together with\nthe basic reproduction number $R_0$, are the main epidemic parameters\ndescribing severity and transmissibility of the virus, respectively. The IFR\ncan be also used as a basis for estimating and monitoring the number of\ninfected individuals in a population, which may be subsequently used to inform\npolicy decisions relating to public health interventions and lockdown\nstrategies. The interpretation of IFR measurements requires the calculation of\nconfidence intervals. We present a number of statistical methods that are\nrelevant in this context and develop an inverse problem formulation to\ndetermine correction factors to mitigate time-dependent effects that can lead\nto biased IFR estimates. We also review a number of methods to combine IFR\nestimates from multiple independent studies, provide example calculations\nthroughout this note and conclude with a summary and \"best practice\"\nrecommendations. The developed code is available online.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2020 18:15:13 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Mieskolainen", "Mikael", ""], ["Bainbridge", "Robert", ""], ["Buchmueller", "Oliver", ""], ["Lyons", "Louis", ""], ["Wardle", "Nicholas", ""]]}, {"id": "2012.02101", "submitter": "Christoph Schumacher", "authors": "Christoph Schumacher and Matthias T\\\"aufer", "title": "The Statistics of Noisy One-Stage Group Testing in Outbreaks", "comments": "30 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.DM q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In one-stage or non-adaptive group testing, instead of testing every sample\nunit individually, they are split, bundled in pools, and simultaneously tested.\nThe results are then decoded to infer the states of the individual items. This\ncombines advantages of adaptive pooled testing, i. e. saving resources and\nhigher throughput, with those of individual testing, e. g. short detection time\nand lean laboratory organisation, and might be suitable for screening during\noutbreaks. We study the COMP and NCOMP decoding algorithms for non-adaptive\npooling strategies based on maximally disjunct pooling matrices with constant\nrow and column sums in the linear prevalence regime and in the presence of\nnoisy measurements motivated by PCR tests. We calculate sensitivity,\nspecificity, the probabilities of Type I and II errors, and the expected number\nof items with a positive result as well as the expected number of false\npositives and false negatives. We further provide estimates on the variance of\nthe number of positive and false positive results. We conduct a thorough\ndiscussion of the calculations and bounds derived. Altogether, the article\nprovides blueprints for screening strategies and tools to help decision makers\nto appropriately tune them in an outbreak.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2020 16:03:35 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Schumacher", "Christoph", ""], ["T\u00e4ufer", "Matthias", ""]]}, {"id": "2012.02102", "submitter": "Souvik Banerjee", "authors": "Atanu Bhattacharjee, Gajendra K. Vishwakarma, Souvik Banerjee", "title": "A modified risk detection approach of biomarkers by frailty effect on\n  multiple time to event data", "comments": "21 pages, 2 figures,7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple indications of disease progression found in a cancer patient by\nloco-regional relapse, distant metastasis and death. Early identification of\nthese indications is necessary to change the treatment strategy. Biomarkers\nplay an essential role in this aspect. The survival chance of a patient is\ndependent on the biomarker, and the treatment strategy also differs\naccordingly, e.g., the survival prediction of breast cancer patients diagnosed\nwith HER2 positive status is different from the same with HER2 negative status.\nThis results in a different treatment strategy. So, the heterogeneity of the\nbiomarker statuses or levels should be taken into consideration while modelling\nthe survival outcome. This heterogeneity factor which is often unobserved, is\ncalled frailty. When multiple indications are present simultaneously, the\nscenario becomes more complex as only one of them can occur, which will censor\nthe occurrence of other events. Incorporating independent frailties of each\nbiomarker status for every cause of indications will not depict the complete\npicture of heterogeneity. The events indicating cancer progression are likely\nto be inter-related. So, the correlation should be incorporated through the\nfrailties of different events. In our study, we considered a multiple events or\nrisks model with a heterogeneity component. Based on the estimated variance of\nthe frailty, the threshold levels of a biomarker are utilised as early\ndetection tool of the disease progression or death. Additive-gamma frailty\nmodel is considered to account the correlation between different frailty\ncomponents and estimation of parameters are performed using\nExpectation-Maximization Algorithm. With the extensive algorithm in R, we have\nobtained the threshold levels of activity of a biomarker in a multiple events\nscenario.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2020 13:14:12 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 14:43:01 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bhattacharjee", "Atanu", ""], ["Vishwakarma", "Gajendra K.", ""], ["Banerjee", "Souvik", ""]]}, {"id": "2012.02103", "submitter": "Tim Friede", "authors": "Jan Beyersmann and Tim Friede and Claudia Schmoor", "title": "Design aspects of COVID-19 treatment trials: Improving probability and\n  time of favourable events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As a reaction to the pandemic of the severe acute respiratory syndrome\ncoronavirus 2 (SARS-CoV-2), a multitude of clinical trials for the treatment of\nSARS-CoV-2 or the resulting corona disease (COVID-19) are globally at various\nstages from planning to completion. Although some attempts were made to\nstandardize study designs, this was hindered by the ferocity of the pandemic\nand the need to set up trials quickly. We take the view that a successful\ntreatment of COVID-19 patients (i) increases the probability of a recovery or\nimprovement within a certain time interval, say 28 days; (ii) aims to expedite\nfavourable events within this time frame; and (iii) does not increase mortality\nover this time period. On this background we discuss the choice of endpoint and\nits analysis. Furthermore, we consider consequences of this choice for other\ndesign aspects including sample size and power and provide some guidance on the\napplication of adaptive designs in this particular context.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 12:35:15 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Beyersmann", "Jan", ""], ["Friede", "Tim", ""], ["Schmoor", "Claudia", ""]]}, {"id": "2012.02105", "submitter": "Guido Sanguinetti", "authors": "Guido Sanguinetti", "title": "Systematic errors in estimates of $R_t$ from symptomatic cases in the\n  presence of observation bias", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating the reproduction number $R_t$ of an\nepidemic for populations where the probability of detection of cases depends on\na known covariate. We argue that in such cases the normal empirical estimator\ncan fail when the prevalence of cases among groups changes with time. We\npropose a Bayesian strategy to resolve the problem, as well as a simple\nsolution in the case of large number of cases. We illustrate the issue and its\nsolution on a simple yet realistic simulation study, and discuss the general\nrelevance of the issue to the current covid19 pandemic.\n", "versions": [{"version": "v1", "created": "Tue, 1 Dec 2020 11:05:34 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Sanguinetti", "Guido", ""]]}, {"id": "2012.02168", "submitter": "Andres Christen", "authors": "Marcos A. Capistr\\'an, Antonio Capella and J. Andr\\'es Christen", "title": "Filtering and improved Uncertainty Quantification in the dynamic\n  estimation of effective reproduction numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The effective reproduction number $R_t$ measures an infectious disease's\ntransmissibility as the number of secondary infections in one reproduction time\nin a population having both susceptible and non-susceptible hosts. Current\napproaches do not quantify the uncertainty correctly in estimating $R_t$, as\nexpected by the observed variability in contagion patterns. We elaborate on the\nBayesian estimation of $R_t$ by improving on the Poisson sampling model of Cori\net al. (2013). By adding an autoregressive latent process, we build a Dynamic\nLinear Model on the log of observed $R_t$s, resulting in a filtering type\nBayesian inference. We use a conjugate analysis, and all calculations are\nexplicit. Results show an improved uncertainty quantification on the estimation\nof $R_t$'s, with a reliable method that could safely be used by non-experts and\nwithin other forecasting systems. We illustrate our approach with recent data\nfrom the current COVID19 epidemic in Mexico.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 18:48:46 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Capistr\u00e1n", "Marcos A.", ""], ["Capella", "Antonio", ""], ["Christen", "J. Andr\u00e9s", ""]]}, {"id": "2012.02196", "submitter": "Chibuzor Nnanatu", "authors": "Chibuzor C. Nnanatu, Murray S. A. Thompson, Michael A. Spence, Elena\n  Couce, Jeroen van der Kooij and Christopher P. Lynam", "title": "Bayesian hierarchical space-time models to improve multispecies\n  assessment by combining observations from disparate fish surveys", "comments": "32 pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many wild species affected by human activities require multiple surveys with\ndiffering designs to capture behavioural response to wide ranging habitat\nconditions and map and quantify them. While data from for example intersecting\nbut disparate fish surveys using different gear, are widely available,\ndifferences in design and methodology often limit their integration. Novel\nstatistical approaches which can draw on observations from diverse sources\ncould enhance our understanding of multiple species distributions\nsimultaneously and thus provide vital evidence needed to conserve their\npopulations and biodiversity at large. Using a novel Bayesian hierarchical\nbinomial-lognormal hurdle modelling approach within the INLA-SPDE framework, we\ncombined and analysed acoustic and bottom trawl survey data for herring, sprat\nand northeast Atlantic mackerel in the North Sea. These models were implemented\nusing INLA-SPDE techniques. By accounting for gear-specific efficiencies across\nsurveys in addition to increased spatial coverage, we gained larger statistical\npower with greatly minimised uncertainties in estimation. Our statistical\napproach provides a methodological development to improve the evidence base for\nmultispecies assessment and marine ecosystem-based management. And on a broader\nscale, it could be readily applied where disparate biological surveys and\nsampling methods intersect, e.g. to provide information on biodiversity\npatterns using global datasets of species distributions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:04:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Nnanatu", "Chibuzor C.", ""], ["Thompson", "Murray S. A.", ""], ["Spence", "Michael A.", ""], ["Couce", "Elena", ""], ["van der Kooij", "Jeroen", ""], ["Lynam", "Christopher P.", ""]]}, {"id": "2012.02262", "submitter": "Alan Rezazadeh", "authors": "Alan Rezazadeh", "title": "Toe-Heal-Air-Injection Thermal Recovery Production Prediction and\n  Modelling Using Quadratic Poisson Polynomial Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research paper explores application of multivariable regression models\nusing only reservoir temperatures for predicting oil and gas production in a\nToe-Heal-Air-Injection (THAI) enhanced oil recovery process. This paper\ndiscusses effects of statistical interaction between thermocouples by using\nsecond degree quadratic polynomials, which showed significant production\nforecast accuracy. Interactions among thermocouples statistically include\ntemperature of larger reservoir areas, hence improving the predictive models.\nThe interaction of two thermocouples can be interpreted as temperature gradient\nof combustion zone as moving forward during THAI operations life cycle.\n  Second degree polynomial regression including interactions showed major\nprediction improvement for both oil and natural gas productions compare to\nsimple regression models. Application of Poisson regression slightly improved\nprediction accuracy for oil production and was less effective on improving\nnatural gas production predictions. Quadratic Poisson regression models showed\nrealistic production prediction method for both oil and gas production values,\ndue to the nature of Poisson probability distribution which is non-negative for\nrates and count values.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2020 05:45:59 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Rezazadeh", "Alan", ""]]}, {"id": "2012.02302", "submitter": "Cai Li", "authors": "Cai Li, Luo Xiao, Sheng Luo", "title": "Joint Model for Survival and Multivariate Sparse Functional Data with\n  Application to a Study of Alzheimer's Disease", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13427", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Studies of Alzheimer's disease (AD) often collect multiple longitudinal\nclinical outcomes, which are correlated and predictive of AD progression. It is\nof great scientific interest to investigate the association between the\noutcomes and time to AD onset. We model the multiple longitudinal outcomes as\nmultivariate sparse functional data and propose a functional joint model\nlinking multivariate functional data to event time data. In particular, we\npropose a multivariate functional mixed model (MFMM) to identify the shared\nprogression pattern and outcome-specific progression patterns of the outcomes,\nwhich enables more interpretable modeling of associations between outcomes and\nAD onset. The proposed method is applied to the Alzheimer's Disease\nNeuroimaging Initiative study (ADNI) and the functional joint model sheds new\nlight on inference of five longitudinal outcomes and their associations with AD\nonset. Simulation studies also confirm the validity of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Dec 2020 22:04:04 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Li", "Cai", ""], ["Xiao", "Luo", ""], ["Luo", "Sheng", ""]]}, {"id": "2012.02359", "submitter": "Paul Pu Liang", "authors": "Terrance Liu, Paul Pu Liang, Michal Muszynski, Ryo Ishii, David Brent,\n  Randy Auerbach, Nicholas Allen, Louis-Philippe Morency", "title": "Multimodal Privacy-preserving Mood Prediction from Mobile Data: A\n  Preliminary Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental health conditions remain under-diagnosed even in countries with common\naccess to advanced medical care. The ability to accurately and efficiently\npredict mood from easily collectible data has several important implications\ntowards the early detection and intervention of mental health disorders. One\npromising data source to help monitor human behavior is from daily smartphone\nusage. However, care must be taken to summarize behaviors without identifying\nthe user through personal (e.g., personally identifiable information) or\nprotected attributes (e.g., race, gender). In this paper, we study behavioral\nmarkers or daily mood using a recent dataset of mobile behaviors from high-risk\nadolescent populations. Using computational models, we find that multimodal\nmodeling of both text and app usage features is highly predictive of daily mood\nover each modality alone. Furthermore, we evaluate approaches that reliably\nobfuscate user identity while remaining predictive of daily mood. By combining\nmultimodal representations with privacy-preserving learning, we are able to\npush forward the performance-privacy frontier as compared to unimodal\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 01:44:22 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Liu", "Terrance", ""], ["Liang", "Paul Pu", ""], ["Muszynski", "Michal", ""], ["Ishii", "Ryo", ""], ["Brent", "David", ""], ["Auerbach", "Randy", ""], ["Allen", "Nicholas", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2012.02378", "submitter": "Ying Yuan", "authors": "Liyun Jiang, Lei Nie, Fangrong Yan, and Ying Yuan", "title": "Optimal Bayesian hierarchical model to accelerate the development of\n  tissue-agnostic drugs and basket trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tissue-agnostic trials enroll patients based on their genetic biomarkers, not\ntumor type, in an attempt to determine if a new drug can successfully treat\ndisease conditions based on biomarkers. The Bayesian hierarchical model (BHM)\nprovides an attractive approach to design phase II tissue-agnostic trials by\nallowing information borrowing across multiple disease types. In this article,\nwe elucidate two intrinsic and inevitable issues that may limit the use of BHM\nto tissue-agnostic trials: sensitivity to the prior specification of the\nshrinkage parameter and the competing \"interest\" among disease types in\nincreasing power and controlling type I error. To address these issues, we\npropose the optimal BHM (OBHM) approach. With OBHM, we first specify a flexible\nutility function to quantify the tradeoff between type I error and power across\ndisease type based on the study objectives, and then we select the prior of the\nshrinkage parameter to optimize the utility function of clinical and regulatory\ninterest. OBMH effectively balances type I and II errors, addresses the\nsensitivity of the prior selection, and reduces the \"unwarranted\" subjectivity\nin the prior selection. Simulation study shows that the resulting OBHM and its\nextensions, clustered OBHM (COBHM) and adaptive OBHM (AOBHM), have desirable\noperating characteristics, outperforming some existing methods with better\nbalanced power and type I error control. Our method provides a systematic,\nrigorous way to apply BHM and solve the common problem of blindingly using a\nnon-informative inverse-gamma prior (with a large variance) or priors\narbitrarily chosen that may lead to pathological statistical properties.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 03:12:09 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Jiang", "Liyun", ""], ["Nie", "Lei", ""], ["Yan", "Fangrong", ""], ["Yuan", "Ying", ""]]}, {"id": "2012.02397", "submitter": "Kexin Chen", "authors": "Kexin Chen, Chi Seng Pun and Hoi Ying Wong", "title": "Efficient Social Distancing for COVID-19: An Integration of Economic\n  Health and Public Health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social distancing has been the only effective way to contain the spread of an\ninfectious disease prior to the availability of the pharmaceutical treatment.\nIt can lower the infection rate of the disease at the economic cost. A pandemic\ncrisis like COVID-19, however, has posed a dilemma to the policymakers since a\nlong-term restrictive social distancing or even lockdown will keep economic\ncost rising. This paper investigates an efficient social distancing policy to\nmanage the integrated risk from economic health and public health issues for\nCOVID-19 using a stochastic epidemic modeling with mobility controls. The\nsocial distancing is to restrict the community mobility, which was recently\naccessible with big data analytics. This paper takes advantage of the community\nmobility data to model the COVID-19 processes and infer the COVID-19 driven\neconomic values from major market index price, which allow us to formulate the\nsearch of the efficient social distancing policy as a stochastic control\nproblem. We propose to solve the problem with a deep-learning approach. By\napplying our framework to the US data, we empirically examine the efficiency of\nthe US social distancing policy and offer recommendations generated from the\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 04:20:34 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Chen", "Kexin", ""], ["Pun", "Chi Seng", ""], ["Wong", "Hoi Ying", ""]]}, {"id": "2012.02500", "submitter": "Nicola Melillo PhD", "authors": "Nicola Melillo, Adam S. Darwich", "title": "A latent variable approach to account for correlated inputs in global\n  sensitivity analysis with cases from pharmacological systems modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In pharmaceutical research and development decision-making related to drug\ncandidate selection, efficacy and safety is commonly supported through\nmodelling and simulation (M\\&S). Among others, physiologically-based\npharmacokinetic models are used to describe drug absorption, distribution and\nmetabolism in human. Global sensitivity analysis (GSA) is gaining interest in\nthe pharmacological M\\&S community as an important element for quality\nassessment of model-based inference. Physiological models often present\ninter-correlated parameters. The inclusion of correlated factors in GSA and the\nsensitivity indices interpretation has proven an issue for these models. Here\nwe devise and evaluate a latent variable approach for dealing with correlated\nfactors in GSA. This approach describes the correlation between two model\ninputs through the causal relationship of three independent factors: the latent\nvariable and the unique variances of the two correlated parameters. Then, GSA\nis performed with the classical variance-based method. We applied the latent\nvariable approach to a set of algebraic models and a case from\nphysiologically-based pharmacokinetics. Then, we compared our approach to\nSobol's GSA assuming no correlations, Sobol's GSA with groups and the\nKucherenko approach. The relative ease of implementation and interpretation\nmakes this a simple approach for carrying out GSA for models with correlated\ninput factors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 10:03:55 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Melillo", "Nicola", ""], ["Darwich", "Adam S.", ""]]}, {"id": "2012.02717", "submitter": "Zhimei Ren", "authors": "Zhimei Ren, Yuting Wei and Emmanuel Cand\\`es", "title": "Derandomizing Knockoffs", "comments": "35 pages, 32 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-X knockoffs is a general procedure that can leverage any feature\nimportance measure to produce a variable selection algorithm, which discovers\ntrue effects while rigorously controlling the number or fraction of false\npositives. Model-X knockoffs is a randomized procedure which relies on the\none-time construction of synthetic (random) variables. This paper introduces a\nderandomization method by aggregating the selection results across multiple\nruns of the knockoffs algorithm. The derandomization step is designed to be\nflexible and can be adapted to any variable selection base procedure to yield\nstable decisions without compromising statistical power. When applied to the\nbase procedure of Janson et al. (2016), we prove that derandomized knockoffs\ncontrols both the per family error rate (PFER) and the k family-wise error rate\n(k-FWER). Further, we carry out extensive numerical studies demonstrating tight\ntype-I error control and markedly enhanced power when compared with alternative\nvariable selection algorithms. Finally, we apply our approach to multi-stage\ngenome-wide association studies of prostate cancer and report locations on the\ngenome that are significantly associated with the disease. When\ncross-referenced with other studies, we find that the reported associations\nhave been replicated.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 17:06:24 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Ren", "Zhimei", ""], ["Wei", "Yuting", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2012.02755", "submitter": "Kevin Raina", "authors": "Kevin Raina", "title": "Statistical inference of the inter-sample Dice distribution for\n  discriminative CNN brain lesion segmentation models", "comments": "Proceedings of the 14th International Joint Conference on Biomedical\n  Engineering Systems and Technologies, Volume 2: BIOIMAGING 2021", "journal-ref": null, "doi": "10.5220/0010286201680173", "report-no": null, "categories": "eess.IV cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discriminative convolutional neural networks (CNNs), for which a voxel-wise\nconditional Multinoulli distribution is assumed, have performed well in many\nbrain lesion segmentation tasks. For a trained discriminative CNN to be used in\nclinical practice, the patient's radiological features are inputted into the\nmodel, in which case a conditional distribution of segmentations is produced.\nCapturing the uncertainty of the predictions can be useful in deciding whether\nto abandon a model, or choose amongst competing models. In practice, however,\nwe never know the ground truth segmentation, and therefore can never know the\ntrue model variance. In this work, segmentation sampling on discriminative CNNs\nis used to assess a trained model's robustness by analyzing the inter-sample\nDice distribution on a new patient solely based on their magnetic resonance\n(MR) images. Furthermore, by demonstrating the inter-sample Dice observations\nare independent and identically distributed with a finite mean and variance\nunder certain conditions, a rigorous confidence based decision rule is proposed\nto decide whether to reject or accept a CNN model for a particular patient.\nApplied to the ISLES 2015 (SISS) dataset, the model identified 7 predictions as\nnon-robust, and the average Dice coefficient calculated on the remaining brains\nimproved by 12 percent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 18:18:24 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 14:45:24 GMT"}], "update_date": "2021-02-22", "authors_parsed": [["Raina", "Kevin", ""]]}, {"id": "2012.02807", "submitter": "Pedro L. C. Rodrigues", "authors": "Pedro L. C. Rodrigues, Alexandre Gramfort", "title": "Learning summary features of time series for likelihood free inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been an increasing interest from the scientific community in using\nlikelihood-free inference (LFI) to determine which parameters of a given\nsimulator model could best describe a set of experimental data. Despite\nexciting recent results and a wide range of possible applications, an important\nbottleneck of LFI when applied to time series data is the necessity of defining\na set of summary features, often hand-tailored based on domain knowledge. In\nthis work, we present a data-driven strategy for automatically learning summary\nfeatures from univariate time series and apply it to signals generated from\nautoregressive-moving-average (ARMA) models and the Van der Pol Oscillator. Our\nresults indicate that learning summary features from data can compete and even\noutperform LFI methods based on hand-crafted values such as autocorrelation\ncoefficients even in the linear case.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 19:21:37 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Rodrigues", "Pedro L. C.", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2012.02845", "submitter": "Zhichao Jiang", "authors": "Kosuke Imai, Zhichao Jiang, James Greiner, Ryan Halen, Sooahn Shin", "title": "Experimental Evaluation of Algorithm-Assisted Human Decision-Making:\n  Application to Pretrial Public Safety Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite an increasing reliance on fully-automated algorithmic decision making\nin our day-to-day lives, human beings still make highly consequential\ndecisions. As frequently seen in business, healthcare, and public policy,\nrecommendations produced by algorithms are provided to human decision-makers in\norder to guide their decisions. While there exists a fast-growing literature\nevaluating the bias and fairness of such algorithmic recommendations, an\noverlooked question is whether they help humans make better decisions. We\ndevelop a statistical methodology for experimentally evaluating the causal\nimpacts of algorithmic recommendations on human decisions. We also show how to\nexamine whether algorithmic recommendations improve the fairness of human\ndecisions and derive the optimal decisions under various settings. We apply the\nproposed methodology to the first-ever randomized controlled trial that\nevaluates the pretrial Public Safety Assessment (PSA) in the criminal justice\nsystem. A goal of the PSA is to help judges decide which arrested individuals\nshould be released. We find that the PSA provision has little overall impact on\nthe judge's decisions and subsequent arrestee behavior. However, our analysis\nprovides some potentially suggestive evidence that the PSA may help avoid\nunnecessarily harsh decisions for female arrestees regardless of their risk\nlevels while it encourages the judge to make stricter decisions for male\narrestees who are deemed to be risky. In terms of fairness, the PSA appears to\nincrease the gender bias against males while having little effect on the\nexisting racial biases of the judge's decisions against non-white males.\nFinally, we find that the PSA's recommendations might be too severe unless the\ncost of a new crime is sufficiently higher than the cost of a decision that may\nresult in an unnecessary incarceration.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 20:48:44 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Imai", "Kosuke", ""], ["Jiang", "Zhichao", ""], ["Greiner", "James", ""], ["Halen", "Ryan", ""], ["Shin", "Sooahn", ""]]}, {"id": "2012.02847", "submitter": "Paolo Bertolotti", "authors": "Paolo Bertolotti and Ali Jadbabaie", "title": "Network Group Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying infected individuals in a population\nof size $N$. Group testing provides an approach to test the entire population\nusing significantly fewer than $N$ tests when infection prevalence is low. The\noriginal and most commonly utilized form of group testing, called Dorfman\ntesting, treats each individual's infection probability as independent and\nhomogenous. However, as communicable diseases spread from individual to\nindividual through underlying social networks, an individual's network location\naffects their infection probability. In this work, we utilize network\ninformation to improve group testing. Specifically, we group individuals by\ncommunity and demonstrate the performance gain over Dorfman testing. After\nintroducing a network and epidemic model, we derive the number of tests used\nunder network grouping. We prove the expected number of tests is upper bounded\nby Dorfman testing. In addition, we demonstrate network grouping successfully\nachieves the theoretical lower bound for two-stage testing procedures when\nnetworks have strong community structure. On the other hand, network grouping\nis equivalent to Dorfman testing when networks have no structure. We end by\ndemonstrating network grouping outperforms Dorfman testing in the scenario of a\nuniversity testing its population for COVID-19 cases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 20:55:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Bertolotti", "Paolo", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "2012.02901", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii M. Ostrovskii, Mohamed Ndaoud, Adel Javanmard, Meisam\n  Razaviyayn", "title": "Near-Optimal Procedures for Model Discrimination with Non-Disclosure\n  Properties", "comments": "52 pages, 2 figures; corrected the proof of the lower bound; added\n  new applications and the Fisher information-based argument in Appendix F", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Let $\\theta_0,\\theta_1 \\in \\mathbb{R}^d$ be the population risk minimizers\nassociated to some loss $\\ell:\\mathbb{R}^d\\times \\mathcal{Z}\\to\\mathbb{R}$ and\ntwo distributions $\\mathbb{P}_0,\\mathbb{P}_1$ on $\\mathcal{Z}$. The models\n$\\theta_0,\\theta_1$ are unknown, and $\\mathbb{P}_0,\\mathbb{P}_1$ can be\naccessed by drawing i.i.d samples from them. Our work is motivated by the\nfollowing model discrimination question: \"What sizes of the samples from\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ allow to distinguish between the two\nhypotheses $\\theta^*=\\theta_0$ and $\\theta^*=\\theta_1$ for given\n$\\theta^*\\in\\{\\theta_0,\\theta_1\\}$?\" Making the first steps towards answering\nit in full generality, we first consider the case of a well-specified linear\nmodel with squared loss. Here we provide matching upper and lower bounds on the\nsample complexity as given by $\\min\\{1/\\Delta^2,\\sqrt{r}/\\Delta\\}$ up to a\nconstant factor; here $\\Delta$ is a measure of separation between\n$\\mathbb{P}_0$ and $\\mathbb{P}_1$ and $r$ is the rank of the design covariance\nmatrix. We then extend this result in two directions: (i) for general\nparametric models in asymptotic regime; (ii) for generalized linear models in\nsmall samples ($n\\le r$) under weak moment assumptions. In both cases we derive\nsample complexity bounds of a similar form while allowing for model\nmisspecification. In fact, our testing procedures only access $\\theta^*$ via a\ncertain functional of empirical risk. In addition, the number of observations\nthat allows us to reach statistical confidence does not allow to \"resolve\" the\ntwo models $-$ that is, recover $\\theta_0,\\theta_1$ up to $O(\\Delta)$\nprediction accuracy. These two properties allow to use our framework in applied\ntasks where one would like to $\\textit{identify}$ a prediction model, which can\nbe proprietary, while guaranteeing that the model cannot be actually\n$\\textit{inferred}$ by the identifying agent.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 23:52:54 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 04:56:43 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 12:46:22 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Ostrovskii", "Dmitrii M.", ""], ["Ndaoud", "Mohamed", ""], ["Javanmard", "Adel", ""], ["Razaviyayn", "Meisam", ""]]}, {"id": "2012.02966", "submitter": "Martin Huber", "authors": "Daria Loginova, Marco Portmann, Martin Huber", "title": "Assessing the effects of seasonal tariff-rate quotas on vegetable prices\n  in Switzerland", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal estimation of the short-term effects of tariff-rate quotas (TRQs) on\nvegetable producer prices is hampered by the large variety and different\ngrowing seasons of vegetables and is therefore rarely performed. We quantify\nthe effects of Swiss seasonal TRQs on domestic producer prices of a variety of\nvegetables based on a difference-in-differences estimation using a novel\ndataset of weekly producer prices for Switzerland and neighbouring countries.\nWe find that TRQs increase prices of most vegetables by more than 20% above the\nprices in neighbouring countries during the main harvest time for most\nvegetables and even more than 50% for some vegetables. The effects are stronger\nfor more perishable vegetables and for conventionally produced ones compared\nwith organic vegetables. However, we do not find clear-cut effects of TRQs on\nthe week-to-week price volatility of vegetables although the overall lower\nprice volatility in Switzerland compared with neighbouring countries might be a\nresult of the TRQ system in place.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 07:24:52 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Loginova", "Daria", ""], ["Portmann", "Marco", ""], ["Huber", "Martin", ""]]}, {"id": "2012.03120", "submitter": "Andrey Tremba", "authors": "Andrey Tremba", "title": "Mixed robustness: Analysis of systems with uncertain deterministic and\n  random parameters using the example of linear systems", "comments": "text in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness of linear systems with constant coefficients is considered. There\nexist methods and tools for analyzing the stability of systems with random or\ndeterministic uncertainties. At the same time, there are no approaches for the\nanalysis of systems containing both types of parametric uncertainty. The types\nof robustness are reviewed and new type of \"mixed parametric robustness\" is\nintroduced. It includes several variations. The proposed formulations of mixed\nrobustness problems can be considered as intermediate type between the\nclassical deterministic and probabilistic approaches to robustness. Several\ncases are listed in which the tasks are easily solved. In general, tests of the\nstability of robust systems using the scenario approach are applicable, but\nthese tests can be computationally complex. To calculate the desired stability\nprobability, a simple graphical approach based on a robust D-partition is\nproposed. This method is suitable for the case of a small number of random\nparameters. The final estimate of the probability of stability is calculated in\na deterministic way and can be found with arbitrary precision. Approximate ways\nof solving the assigned tasks are described. Examples and generalization of\nmixed robustness to other types of systems are given.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 21:02:15 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Tremba", "Andrey", ""]]}, {"id": "2012.03129", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Hieu Pham and Lizhi Wang", "title": "Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data\n  Using Deep Transfer Learning", "comments": "14 pages, 8 figures, 7 tables", "journal-ref": "Scientific Reports, 11(1), 1-14 (2021)", "doi": "10.1038/s41598-021-89779-z", "report-no": null, "categories": "cs.CV cs.LG eess.IV q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large-scale crop yield estimation is, in part, made possible due to the\navailability of remote sensing data allowing for the continuous monitoring of\ncrops throughout their growth cycle. Having this information allows\nstakeholders the ability to make real-time decisions to maximize yield\npotential. Although various models exist that predict yield from remote sensing\ndata, there currently does not exist an approach that can estimate yield for\nmultiple crops simultaneously, and thus leads to more accurate predictions. A\nmodel that predicts the yield of multiple crops and concurrently considers the\ninteraction between multiple crop yields. We propose a new convolutional neural\nnetwork model called YieldNet which utilizes a novel deep learning framework\nthat uses transfer learning between corn and soybean yield predictions by\nsharing the weights of the backbone feature extractor. Additionally, to\nconsider the multi-target response variable, we propose a new loss function. We\nconduct our experiment using data from 1,132 counties for corn and 1,076\ncounties for soybean across the United States. Numerical results demonstrate\nthat our proposed method accurately predicts corn and soybean yield from one to\nfour months before the harvest with a MAE being 8.74% and 8.70% of the average\nyield, respectively, and is competitive to other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 22:09:07 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 18:10:16 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 15:20:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Khaki", "Saeed", ""], ["Pham", "Hieu", ""], ["Wang", "Lizhi", ""]]}, {"id": "2012.03217", "submitter": "Themistoklis Botsas", "authors": "Themistoklis Botsas, Jonathan A. Cumming and Ian H. Jermyn", "title": "A Bayesian approach to deconvolution in well test analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In petroleum well test analysis, deconvolution is used to obtain information\nabout the reservoir system. This information is contained in the response\nfunction, which can be estimated by solving an inverse problem in the pressure\nand flow rate measurements. Our Bayesian approach to this problem is based upon\na parametric physical model of reservoir behaviour, derived from the solution\nfor fluid flow in a general class of reservoirs. This permits joint parametric\nBayesian inference for both the reservoir parameters and the true pressure and\nrate values, which is essential due to the typical levels of observation error.\nUsing a set of flexible priors for the reservoir parameters to restrict the\nsolution space to physical behaviours, samples from the posterior are generated\nusing MCMC. Summaries and visualisations of the reservoir parameters'\nposterior, response, and true pressure and rate values can be produced,\ninterpreted, and model selection can be performed. The method is validated\nthrough a synthetic application, and applied to a field data set. The results\nare comparable to the state of the art solution, but through our method we gain\naccess to system parameters, we can incorporate prior knowledge that excludes\nnon-physical results, and we can quantify parameter uncertainty.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 08:46:31 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Botsas", "Themistoklis", ""], ["Cumming", "Jonathan A.", ""], ["Jermyn", "Ian H.", ""]]}, {"id": "2012.03248", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio", "title": "Modeling animal movement with directional persistence and attractive\n  points", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  GPS technology is more accessible to researchers and, nowadays, animal\nmovement data are widely available. To analyze such data, different approaches\nhave been proposed, and among them, hidden Markov models with the\nOrnstein-Uhlenbeck or the step-and-turn emission distribution are the most\ncommonly used. The former characterizes movement with the use of a center of\nattraction, while the latter has directional persistence. In this work we\npropose a new emission distribution that posses the defining characteristics of\nthe two aforementioned approaches, and at any given time, an animal can exhibit\na different degree of directional persistence and attraction to a point in\nspace. Hidden Markov models based on our proposal, the Ornstein-Uhlenbeck, and\nthe step-and-turn, are estimated on a real data example, where GPS locations of\na Maremma Sheepdog are recorded.We show that our proposal has the richest\noutput and better describes the data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 12:31:43 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Mastrantonio", "Gianluca", ""]]}, {"id": "2012.03326", "submitter": "Qiwei Li", "authors": "Qiwei Li, Minzhe Zhang, Yang Xie, Guanghua Xiao", "title": "Bayesian Modeling of Spatial Molecular Profiling Data via Gaussian\n  Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The location, timing, and abundance of gene expression (both mRNA and\nproteins) within a tissue define the molecular mechanisms of cell functions.\nRecent technology breakthroughs in spatial molecular profiling, including\nimaging-based technologies and sequencing-based technologies, have enabled the\ncomprehensive molecular characterization of single cells while preserving their\nspatial and morphological contexts. This new bioinformatics scenario calls for\neffective and robust computational methods to identify genes with spatial\npatterns. We represent a novel Bayesian hierarchical model to analyze spatial\ntranscriptomics data, with several unique characteristics. It models the\nzero-inflated and over-dispersed counts by deploying a zero-inflated negative\nbinomial model that greatly increases model stability and robustness. Besides,\nthe Bayesian inference framework allows us to borrow strength in parameter\nestimation in a de novo fashion. As a result, the proposed model shows\ncompetitive performances in accuracy and robustness over existing methods in\nboth simulation studies and two real data applications. The related R/C++\nsource code is available at https://github.com/Minzhe/BOOST-GP.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2020 17:15:55 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Li", "Qiwei", ""], ["Zhang", "Minzhe", ""], ["Xie", "Yang", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2012.03554", "submitter": "Andreas Meid", "authors": "Andreas D. Meid", "title": "Teaching reproducible research for medical students and postgraduate\n  pharmaceutical scientists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many academic settings, medical students start their scientific work\nalready during their studies. Like at our institution, they often work in\ninterdisciplinary teams with more or less experienced (postgraduate)\nresearchers of pharmaceutical sciences, natural sciences in general, or\nbiostatistics. All of them should be taught good research practices as an\nintegral part of their education, especially in terms of statistical analysis.\nThis includes reproducibility as a central aspect of modern research.\nAcknowledging that even educators might be unfamiliar with necessary aspects of\na perfectly reproducible workflow, I agreed to give a lecture series on\nreproducible research (RR) for medical students and postgraduate pharmacists\ninvolved in several areas of clinical research. Thus, I designed a piloting\nlecture series to highlight definitions of RR, reasons for RR, potential merits\nof RR, and ways to work accordingly. In trying to actually reproduce a\npublished analysis, I encountered several practical obstacles. In this article,\nI focus on this working example to emphasize the manifold facets of RR, to\nprovide possible explanations and solutions, and argue that harmonized\ncurricula for (quantitative) clinical researchers should include RR principles.\nI therefore hope these experiences are helpful to raise awareness among\neducators and students. RR working habits are not only beneficial for ourselves\nor our students, but also for other researchers within an institution, for\nscientific partners, for the scientific community, and eventually for the\npublic profiting from research findings.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:44:23 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Meid", "Andreas D.", ""]]}, {"id": "2012.03786", "submitter": "Jack Bowden Professor", "authors": "Jack Bowden, Bjoern Bornkamp, Ekkehard Glimm and Frank Bretz", "title": "Connecting Instrumental Variable methods for causal inference to the\n  Estimand Framework", "comments": "29 pages, 9 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal inference methods are gaining increasing prominence in pharmaceutical\ndrug development in light of the recently published addendum on estimands and\nsensitivity analysis in clinical trials to the E9 guideline of the\nInternational Council for Harmonisation. The E9 addendum emphasises the need to\naccount for post-randomization or `intercurrent' events that can potentially\ninfluence the interpretation of a treatment effect estimate at a trial's\nconclusion. Instrumental Variables (IV) methods have been used extensively in\neconomics, epidemiology and academic clinical studies for `causal inference',\nbut less so in the pharmaceutical industry setting until now. In this tutorial\npaper we review the basic tools for causal inference, including graphical\ndiagrams and potential outcomes, as well as several conceptual frameworks that\nan IV analysis can sit within. We discuss in detail how to map these approaches\nto the Treatment Policy, Principal Stratum and Hypothetical `estimand\nstrategies' introduced in the E9 addendum, and provide details of their\nimplementation using standard regression models. Specific attention is given to\ndiscussing the assumptions each estimation strategy relies on in order to be\nconsistent, the extent to which they can be empirically tested and sensitivity\nanalyses in which specific assumptions can be relaxed. We finish by applying\nthe methods described to simulated data closely matching two recent\npharmaceutical trials to further motivate and clarify the ideas\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 15:28:39 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 06:53:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bowden", "Jack", ""], ["Bornkamp", "Bjoern", ""], ["Glimm", "Ekkehard", ""], ["Bretz", "Frank", ""]]}, {"id": "2012.03830", "submitter": "Alexandre Sava", "authors": "Fei Huang (LCOMS, HYIT), Alexandre Sava (LCOMS), Kondo H. Adjallah\n  (LCOMS), Wang Zhouhang (LCOMS)", "title": "Bearings degradation monitoring indicators based on discarded projected\n  space information and piecewise linear representation", "comments": null, "journal-ref": "International Journal of Mechatronics and Automation, 2020, 7 (1),\n  pp.23-31", "doi": "10.1504/IJMA.2020.108185", "report-no": null, "categories": "cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Condition-based maintenance of rotating mechanics requests efficient bearings\ndegradation monitoring. The accuracy of bearings degradation measure depends\nlargely on degradation indicators. To extract efficient indicators, in this\npaper we propose a method based on the discarded projected space information\nand piecewise linear representation (PLR) to build three bearings degradation\nmonitoring indicators which are named SDHT2, VSDHT2 and NVSDHT2. The discarded\nprojected space information is measured by the segmented discarded Hotelling T\nsquare we propose in this paper. For illustration, the IEEE PHM 2012 benchmark\ndataset is used in this paper. The results show that the three new indicators\nare all sensitive and monotonic during the bearings whole lifecycle. They\ndescribe the whole degradation process history and carry the real-time\ninformation of bearings degradation. And NVSDHT2 is the generalised version of\nVSDHT2, which is promising to monitor bearings degradation.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:26:48 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Huang", "Fei", "", "LCOMS, HYIT"], ["Sava", "Alexandre", "", "LCOMS"], ["Adjallah", "Kondo H.", "", "LCOMS"], ["Zhouhang", "Wang", "", "LCOMS"]]}, {"id": "2012.03854", "submitter": "Fotios Petropoulos", "authors": "Fotios Petropoulos, Daniele Apiletti, Vassilios Assimakopoulos,\n  Mohamed Zied Babai, Devon K. Barrow, Souhaib Ben Taieb, Christoph Bergmeir,\n  Ricardo J. Bessa, Jakub Bijak, John E. Boylan, Jethro Browell, Claudio\n  Carnevale, Jennifer L. Castle, Pasquale Cirillo, Michael P. Clements, Clara\n  Cordeiro, Fernando Luiz Cyrino Oliveira, Shari De Baets, Alexander\n  Dokumentov, Joanne Ellison, Piotr Fiszeder, Philip Hans Franses, David T.\n  Frazier, Michael Gilliland, M. Sinan G\\\"on\\\"ul, Paul Goodwin, Luigi Grossi,\n  Yael Grushka-Cockayne, Mariangela Guidolin, Massimo Guidolin, Ulrich Gunter,\n  Xiaojia Guo, Renato Guseo, Nigel Harvey, David F. Hendry, Ross Hollyman, Tim\n  Januschowski, Jooyoung Jeon, Victor Richmond R. Jose, Yanfei Kang, Anne B.\n  Koehler, Stephan Kolassa, Nikolaos Kourentzes, Sonia Leva, Feng Li,\n  Konstantia Litsiou, Spyros Makridakis, Gael M. Martin, Andrew B. Martinez,\n  Sheik Meeran, Theodore Modis, Konstantinos Nikolopoulos, Dilek \\\"Onkal,\n  Alessia Paccagnini, Anastasios Panagiotelis, Ioannis Panapakidis, Jose M.\n  Pav\\'ia, Manuela Pedio, Diego J. Pedregal, Pierre Pinson, Patr\\'icia Ramos,\n  David E. Rapach, J. James Reade, Bahman Rostami-Tabar, Micha{\\l} Rubaszek,\n  Georgios Sermpinis, Han Lin Shang, Evangelos Spiliotis, Aris A. Syntetos,\n  Priyanga Dilini Talagala, Thiyanga S. Talagala, Len Tashman, Dimitrios\n  Thomakos, Thordis Thorarinsdottir, Ezio Todini, Juan Ram\\'on Trapero Arenas,\n  Xiaoqian Wang, Robert L. Winkler, Alisa Yusupova, Florian Ziel", "title": "Forecasting: theory and practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting has always been at the forefront of decision making and planning.\nThe uncertainty that surrounds the future is both exciting and challenging,\nwith individuals and organisations seeking to minimise risks and maximise\nutilities. The large number of forecasting applications calls for a diverse set\nof forecasting methods to tackle real-life challenges. This article provides a\nnon-systematic review of the theory and the practice of forecasting. We provide\nan overview of a wide range of theoretical, state-of-the-art models, methods,\nprinciples, and approaches to prepare, produce, organise, and evaluate\nforecasts. We then demonstrate how such theoretical concepts are applied in a\nvariety of real-life contexts.\n  We do not claim that this review is an exhaustive list of methods and\napplications. However, we wish that our encyclopedic presentation will offer a\npoint of reference for the rich work that has been undertaken over the last\ndecades, with some key insights for the future of forecasting theory and\npractice. Given its encyclopedic nature, the intended mode of reading is\nnon-linear. We offer cross-references to allow the readers to navigate through\nthe various topics. We complement the theoretical concepts and applications\ncovered by large lists of free or open-source software implementations and\npublicly-available databases.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2020 16:56:44 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 11:53:37 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Petropoulos", "Fotios", ""], ["Apiletti", "Daniele", ""], ["Assimakopoulos", "Vassilios", ""], ["Babai", "Mohamed Zied", ""], ["Barrow", "Devon K.", ""], ["Taieb", "Souhaib Ben", ""], ["Bergmeir", "Christoph", ""], ["Bessa", "Ricardo J.", ""], ["Bijak", "Jakub", ""], ["Boylan", "John E.", ""], ["Browell", "Jethro", ""], ["Carnevale", "Claudio", ""], ["Castle", "Jennifer L.", ""], ["Cirillo", "Pasquale", ""], ["Clements", "Michael P.", ""], ["Cordeiro", "Clara", ""], ["Oliveira", "Fernando Luiz Cyrino", ""], ["De Baets", "Shari", ""], ["Dokumentov", "Alexander", ""], ["Ellison", "Joanne", ""], ["Fiszeder", "Piotr", ""], ["Franses", "Philip Hans", ""], ["Frazier", "David T.", ""], ["Gilliland", "Michael", ""], ["G\u00f6n\u00fcl", "M. Sinan", ""], ["Goodwin", "Paul", ""], ["Grossi", "Luigi", ""], ["Grushka-Cockayne", "Yael", ""], ["Guidolin", "Mariangela", ""], ["Guidolin", "Massimo", ""], ["Gunter", "Ulrich", ""], ["Guo", "Xiaojia", ""], ["Guseo", "Renato", ""], ["Harvey", "Nigel", ""], ["Hendry", "David F.", ""], ["Hollyman", "Ross", ""], ["Januschowski", "Tim", ""], ["Jeon", "Jooyoung", ""], ["Jose", "Victor Richmond R.", ""], ["Kang", "Yanfei", ""], ["Koehler", "Anne B.", ""], ["Kolassa", "Stephan", ""], ["Kourentzes", "Nikolaos", ""], ["Leva", "Sonia", ""], ["Li", "Feng", ""], ["Litsiou", "Konstantia", ""], ["Makridakis", "Spyros", ""], ["Martin", "Gael M.", ""], ["Martinez", "Andrew B.", ""], ["Meeran", "Sheik", ""], ["Modis", "Theodore", ""], ["Nikolopoulos", "Konstantinos", ""], ["\u00d6nkal", "Dilek", ""], ["Paccagnini", "Alessia", ""], ["Panagiotelis", "Anastasios", ""], ["Panapakidis", "Ioannis", ""], ["Pav\u00eda", "Jose M.", ""], ["Pedio", "Manuela", ""], ["Pedregal", "Diego J.", ""], ["Pinson", "Pierre", ""], ["Ramos", "Patr\u00edcia", ""], ["Rapach", "David E.", ""], ["Reade", "J. James", ""], ["Rostami-Tabar", "Bahman", ""], ["Rubaszek", "Micha\u0142", ""], ["Sermpinis", "Georgios", ""], ["Shang", "Han Lin", ""], ["Spiliotis", "Evangelos", ""], ["Syntetos", "Aris A.", ""], ["Talagala", "Priyanga Dilini", ""], ["Talagala", "Thiyanga S.", ""], ["Tashman", "Len", ""], ["Thomakos", "Dimitrios", ""], ["Thorarinsdottir", "Thordis", ""], ["Todini", "Ezio", ""], ["Arenas", "Juan Ram\u00f3n Trapero", ""], ["Wang", "Xiaoqian", ""], ["Winkler", "Robert L.", ""], ["Yusupova", "Alisa", ""], ["Ziel", "Florian", ""]]}, {"id": "2012.04009", "submitter": "Asmaa Boujibar", "authors": "Asmaa Boujibar, Samantha Howell, Shuang Zhang, Grethe Hystad, Anirudh\n  Prabhu, Nan Liu, Thomas Stephan, Shweta Narkar, Ahmed Eleish, Shaunna M.\n  Morrison, Robert M. Hazen, Larry R. Nittler", "title": "Cluster analysis of presolar silicon carbide grains: evaluation of their\n  classification and astrophysical implications", "comments": "24 pages, 10 figures and 1 table", "journal-ref": null, "doi": "10.3847/2041-8213/abd102", "report-no": null, "categories": "astro-ph.SR astro-ph.EP astro-ph.GA stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster analysis of presolar silicon carbide grains based on literature data\nfor 12C/13C, 14N/15N, {\\delta}30Si/28Si, and {\\delta}29Si/28Si including or not\ninferred initial 26Al/27Al data, reveals nine clusters agreeing with previously\ndefined grain types but also highlighting new divisions. Mainstream grains\nreside in three clusters probably representing different parent star\nmetallicities. One of these clusters has a compact core, with a narrow range of\ncomposition, pointing to an enhanced production of SiC grains in asymptotic\ngiant branch (AGB) stars with a narrow range of masses and metallicities. The\naddition of 26Al/27Al data highlights a cluster of mainstream grains, enriched\nin 15N and 26Al, which cannot be explained by current AGB models. We defined\ntwo AB grain clusters, one with 15N and 26Al excesses, and the other with 14N\nand smaller 26Al excesses, in agreement with recent studies. Their definition\ndoes not use the solar N isotopic ratio as a divider, and the contour of the\n26Al-rich AB cluster identified in this study is in better agreement with\ncore-collapse supernova models. We also found a cluster with a mixture of\nputative nova and AB grains, which may have formed in supernova or nova\nenvironments. X grains make up two clusters, having either strongly correlated\nSi isotopic ratios or deviating from the 2/3 slope line in the Si 3-isotope\nplot. Finally, most Y and Z grains are jointly clustered, suggesting that the\nprevious use of 12C/13C= 100 as a divider for Y grains was arbitrary. Our\nresults show that cluster analysis is a powerful tool to interpret the data in\nlight of stellar evolution and nucleosynthesis modelling and highlight the need\nof more multi-element isotopic data for better classification.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 19:24:31 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Boujibar", "Asmaa", ""], ["Howell", "Samantha", ""], ["Zhang", "Shuang", ""], ["Hystad", "Grethe", ""], ["Prabhu", "Anirudh", ""], ["Liu", "Nan", ""], ["Stephan", "Thomas", ""], ["Narkar", "Shweta", ""], ["Eleish", "Ahmed", ""], ["Morrison", "Shaunna M.", ""], ["Hazen", "Robert M.", ""], ["Nittler", "Larry R.", ""]]}, {"id": "2012.04062", "submitter": "Chris Forest", "authors": "Kristina R. Colbert, Frank C. Errickson, David Anthoff, Chris E.\n  Forest", "title": "Including climate system feedbacks in calculations of the social cost of\n  methane", "comments": "13 pages, 2 figures, Supplement (18 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph stat.AP stat.OT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Integrated assessment models (IAMs) are valuable tools that consider the\ninteractions between socioeconomic systems and the climate system.\nDecision-makers and policy analysts employ IAMs to calculate the marginalized\nmonetary cost of climate damages resulting from an incremental emission of a\ngreenhouse gas. Used within the context of regulating anthropogenic methane\nemissions, this metric is called the social cost of methane (SC-CH$_4$).\nBecause several key IAMs used for social cost estimation contain a simplified\nmodel structure that prevents the endogenous modeling of non-CO$_2$ greenhouse\ngases, very few estimates of the SC-CH$_4$ exist. For this reason, IAMs should\nbe updated to better represent methane cycle dynamics that are consistent with\ncomprehensive Earth System Models. We include feedbacks of climate change on\nthe methane cycle to estimate the SC-CH$_4$. Our expected value for the\nSC-CH$_4$ is \\$1163/t-CH$_4$ under a constant 3.0% discount rate. This\nrepresents a 44% increase relative to a mean estimate without feedbacks on the\nmethane cycle.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 21:07:50 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Colbert", "Kristina R.", ""], ["Errickson", "Frank C.", ""], ["Anthoff", "David", ""], ["Forest", "Chris E.", ""]]}, {"id": "2012.04181", "submitter": "Kyungsub Lee", "authors": "Hyun Jin Jang, Kiseop Lee, Kyungsub Lee", "title": "Systemic Risk in Market Microstructure of Crude Oil and Gasoline Futures\n  Prices: A Hawkes Flocking Model Approach", "comments": null, "journal-ref": "Journal of Futures Markets, 40, 2020, 247-275", "doi": "10.1002/fut.22048", "report-no": null, "categories": "q-fin.TR q-fin.RM q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the Hawkes flocking model that assesses systemic risk in\nhigh-frequency processes at the two perspectives -- endogeneity and\ninteractivity. We examine the futures markets of WTI crude oil and gasoline for\nthe past decade, and perform a comparative analysis with conditional\nvalue-at-risk as a benchmark measure. In terms of high-frequency structure, we\nderive the empirical findings. The endogenous systemic risk in WTI was\nsignificantly higher than that in gasoline, and the level at which gasoline\naffects WTI was constantly higher than in the opposite case. Moreover, although\nthe relative influence's degree was asymmetric, its difference has gradually\nreduced.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 02:54:05 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Jang", "Hyun Jin", ""], ["Lee", "Kiseop", ""], ["Lee", "Kyungsub", ""]]}, {"id": "2012.04200", "submitter": "Yan Li", "authors": "Yan Li, Kun Chen, Jun Yan, Xuebin Zhang", "title": "Regularized Fingerprinting in Detection and Attribution of Climate\n  Change with Weight Matrix Optimizing the Efficiency in Scaling Factor\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal fingerprinting method for detection and attribution of climate\nchange is based on a multiple regression where each covariate has measurement\nerror whose covariance matrix is the same as that of the regression error up to\na known scale. Inferences about the regression coefficients are critical not\nonly for making statements about detection and attribution but also for\nquantifying the uncertainty in important outcomes derived from detection and\nattribution analyses. When there is no errors-in-variables (EIV), the optimal\nweight matrix in estimating the regression coefficients is the precision matrix\nof the regression error which, in practice, is never known and has to be\nestimated from climate model simulations. We construct a weight matrix by\ninverting a nonlinear shrinkage estimate of the error covariance matrix that\nminimizes loss functions directly targeting the uncertainty of the resulting\nregression coefficient estimator. The resulting estimator of the regression\ncoefficients is asymptotically optimal as the sample size of the climate model\nsimulations and the matrix dimension go to infinity together with a limiting\nratio. When EIVs are present, the estimator of the regression coefficients\nbased on the proposed weight matrix is asymptotically more efficient than that\nbased on the inverse of the existing linear shrinkage estimator of the error\ncovariance matrix. The performance of the method is confirmed in finite sample\nsimulation studies mimicking realistic situations in terms of the length of the\nconfidence intervals and empirical coverage rates for the regression\ncoefficients. An application to detection and attribution analyses of the mean\ntemperature at different spatial scales illustrates the utility of the method.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 04:07:59 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Li", "Yan", ""], ["Chen", "Kun", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "2012.04277", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Comparisons of multiple treatment groups with a negative control or\n  placebo group: Dunnett test vs. closed test procedur", "comments": "Appendix with simulation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Several treatments are usually compared with a control using the Dunnett\ntest. As an alternative, three variants of the closed testing approach are\nconsidered, one with ANOVA-F-tests, one with MCT-GrandMean and one with global\nDunnett-tests in the partition hypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 08:43:31 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2012.04452", "submitter": "Kairui Feng", "authors": "Kairui Feng, Ouyang Min, Ning Lin", "title": "Hurricane-blackout-heatwave Compound Hazard Risk and Resilience in a\n  Changing Climate", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hurricanes have caused power outages and blackouts, affecting millions of\ncustomers and inducing severe social and economic impacts. The impacts of\nhurricane-caused blackouts may worsen due to increased heat extremes and\npossibly increased hurricanes under climate change. We apply hurricane and\nheatwave projections with power outage and recovery process analysis to\ninvestigate how the emerging hurricane-blackout-heatwave compound hazard may\nvary in a changing climate, for Harris County in Texas (including major part of\nHouston City) as an example. We find that, under the high-emissions scenario\nRCP8.5, the expected percent of customers experiencing at least one\nlonger-than-5-day hurricane-induced power outage in a 20-year period would\nincrease significantly from 14% at the end of the 20th century to 44% at the\nend of the 21st century in Harris County. The expected percent of customers who\nmay experience at least one longer-than-5-day heatwave without power (to\nprovide air conditioning) would increase alarmingly, from 0.8% to 15.5%. These\nincreases of risk may be largely avoided if the climate is well controlled\nunder the stringent mitigation scenario RCP2.6. We also reveal that a moderate\nenhancement of critical sectors of the distribution network can significantly\nimprove the resilience of the entire power grid and mitigate the risk of the\nfuture compound hazard. Together these findings suggest that, in addition to\nclimate mitigation, climate adaptation actions are urgently needed to improve\nthe resilience of coastal power systems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 14:40:06 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Feng", "Kairui", ""], ["Min", "Ouyang", ""], ["Lin", "Ning", ""]]}, {"id": "2012.04455", "submitter": "Giulio D'Agostini", "authors": "Giulio D'Agostini", "title": "Ratio of counts vs ratio of rates in Poisson processes", "comments": "73 pages, 24 figures. The scripts of Appendix B are available for\n  download from https://www.roma1.infn.it/~dagos/prob+stat.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The often debated issue of `ratios of small numbers of events' is approached\nfrom a probabilistic perspective, making a clear distinction between the\npredictive problem (forecasting numbers of events we might count under well\nstated assumptions, and therefore of their ratios) and inferential problem\n(learning about the relevant parameters of the related probability\ndistribution, in the light of the observed number of events). The quantities of\ninterests and their relations are visualized in a graphical model (`Bayesian\nnetwork'), very useful to understand how to approach the problem following the\nrules of probability theory. In this paper, written with didactic intent, we\ndiscuss in detail the basic ideas, however giving some hints of how real life\ncomplications, like (uncertain) efficiencies and possible background and\nsystematics, can be included in the analysis, as well as the possibility that\nthe ratio of rates might depend on some physical quantity. The simple models\nconsidered in this paper allow to obtain, under reasonable assumptions, closed\nexpressions for the rates and their ratios. Monte Carlo methods are also used,\nboth to cross check the exact results and to evaluate by sampling the ratios of\ncounts in the cases in which large number approximation does not hold. In\nparticular it is shown how to make approximate inferences using a Markov Chain\nMonte Carlo using JAGS/rjags. Some examples of R and JAGS code are provided.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 16:23:02 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["D'Agostini", "Giulio", ""]]}, {"id": "2012.04549", "submitter": "Ruo-Qian Wang", "authors": "Behzad Golparvar, Petros Papadopoulos, Ahmed Aziz Ezzat, Ruo-Qian Wang", "title": "A Surrogate-model-based Approach for Estimating the First and\n  Second-order Moments of Offshore Wind Power", "comments": null, "journal-ref": null, "doi": "10.1016/j.apenergy.2021.117286", "report-no": null, "categories": "stat.AP physics.flu-dyn", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Power curve is widely used in the wind industry to estimate power output for\nplanning and operational purposes. Existing methods for power curve estimation\nhave three main limitations: (i) they mostly rely on wind speed as the sole\ninput, thus ignoring the secondary, yet possibly significant effects of other\nenvironmental factors, (ii) they largely overlook the complex marine\nenvironment in which offshore turbines operate, potentially compromising their\nvalue in offshore wind energy applications, and (ii) they solely focus on the\nfirst-order properties of wind power, with little (or null) information about\nthe variation around the mean behavior, which is important for ensuring\nreliable grid integration, asset health monitoring, and energy storage, among\nothers. This study investigates the impact of several wind- and wave-related\nfactors on offshore wind power variability, with the ultimate goal of\naccurately predicting its first two moments. Our approach couples OpenFAST with\nGaussian Process (GP) regression to reveal the underlying relationships\ngoverning offshore weather-to-power conversion. We first find that a\nmulti-input power curve which captures the combined impact of wind speed,\ndirection, and air density, can provide double-digit improvements relative to\nunivariate methods which rely on wind speed as the sole explanatory variable\n(e.g. the standard method of bins). Wave-related variables are found not\nimportant for predicting the average power output, but interestingly, appear to\nbe extremely relevant in describing the fluctuation of the offshore power\naround its mean. Tested on real-world data collected at the New York/New Jersey\nbight, our proposed multi-input models demonstrate a high explanatory power in\npredicting the first two moments of offshore wind generation, testifying their\npotential value to the offshore wind industry.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 16:42:13 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 00:24:13 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Golparvar", "Behzad", ""], ["Papadopoulos", "Petros", ""], ["Ezzat", "Ahmed Aziz", ""], ["Wang", "Ruo-Qian", ""]]}, {"id": "2012.04596", "submitter": "Gustau Camps-Valls", "authors": "Manuel Campos-Taberner, Franciso Javier Garc\\'ia-Haro, \\'Alvaro\n  Moreno, Mar\\'ia Amparo Gilabert, Sergio S\\'anchez-Ruiz, Beatriz Mart\\'inez,\n  and Gustau Camps-Valls", "title": "Mapping Leaf Area Index with a Smartphone and Gaussian Processes", "comments": null, "journal-ref": "IEEE Geoscience and Remote Sensing Letters, vol. 12, no. 12, pp.\n  2501-2505, Dec. 2015", "doi": "10.1109/LGRS.2015.2488682", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Leaf area index (LAI) is a key biophysical parameter used to determine\nfoliage cover and crop growth in environmental studies. Smartphones are\nnowadays ubiquitous sensor devices with high computational power, moderate\ncost, and high-quality sensors. A smartphone app, called PocketLAI, was\nrecently presented and tested for acquiring ground LAI estimates. In this\nletter, we explore the use of state-of-the-art nonlinear Gaussian process\nregression (GPR) to derive spatially explicit LAI estimates over rice using\nground data from PocketLAI and Landsat 8 imagery. GPR has gained popularity in\nrecent years because of their solid Bayesian foundations that offers not only\nhigh accuracy but also confidence intervals for the retrievals. We show the\nfirst LAI maps obtained with ground data from a smartphone combined with\nadvanced machine learning. This work compares LAI predictions and confidence\nintervals of the retrievals obtained with PocketLAI to those obtained with\nclassical instruments, such as digital hemispheric photography (DHP) and LI-COR\nLAI-2000. This letter shows that all three instruments got comparable result\nbut the PocketLAI is far cheaper. The proposed methodology hence opens a wide\nrange of possible applications at moderate cost.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:21:13 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Campos-Taberner", "Manuel", ""], ["Garc\u00eda-Haro", "Franciso Javier", ""], ["Moreno", "\u00c1lvaro", ""], ["Gilabert", "Mar\u00eda Amparo", ""], ["S\u00e1nchez-Ruiz", "Sergio", ""], ["Mart\u00ednez", "Beatriz", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04598", "submitter": "Gustau Camps-Valls", "authors": "Jorge Vicent, Luis Alonso, Luca Martino, Neus Sabater, Jochem\n  Verrelst, Gustau Camps-Valls", "title": "Gradient-based Automatic Look-Up Table Generator for Atmospheric\n  Radiative Transfer Models", "comments": null, "journal-ref": "IEEE Transactions on Geoscience and Remote Sensing, vol. 57, no.\n  2, pp. 1040-1048, Feb. 2019", "doi": "10.1109/TGRS.2018.2864517", "report-no": null, "categories": "eess.SP cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Atmospheric correction of Earth Observation data is one of the most critical\nsteps in the data processing chain of a satellite mission for successful remote\nsensing applications. Atmospheric Radiative Transfer Models (RTM) inversion\nmethods are typically preferred due to their high accuracy. However, the\nexecution of RTMs on a pixel-per-pixel basis is impractical due to their high\ncomputation time, thus large multi-dimensional look-up tables (LUTs) are\nprecomputed for their later interpolation. To further reduce the RTM\ncomputation burden and the error in LUT interpolation, we have developed a\nmethod to automatically select the minimum and optimal set of nodes to be\nincluded in a LUT. We present the gradient-based automatic LUT generator\nalgorithm (GALGA) which relies on the notion of an acquisition function that\nincorporates (a) the Jacobian evaluation of an RTM, and (b) information about\nthe multivariate distribution of the current nodes. We illustrate the\ncapabilities of GALGA in the automatic construction and optimization of\nMODerate resolution atmospheric TRANsmission (MODTRAN) LUTs for several input\ndimensions. Our results indicate that, when compared to a pseudo-random\nhomogeneous distribution of the LUT nodes, GALGA reduces (1) the LUT size by\n$\\sim$75\\% and (2) the maximum interpolation relative errors by 0.5\\% It is\nconcluded that automatic LUT design might benefit from the methodology proposed\nin GALGA to reduce computation time and interpolation errors.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:33:34 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Vicent", "Jorge", ""], ["Alonso", "Luis", ""], ["Martino", "Luca", ""], ["Sabater", "Neus", ""], ["Verrelst", "Jochem", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.04602", "submitter": "Samuel Duffield", "authors": "Samuel Duffield, Sumeetpal S. Singh", "title": "Online Particle Smoothing with Application to Map-matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a novel method for online smoothing in state-space models based\non a fixed-lag approximation. Unlike classical fixed-lag smoothing we\napproximate the joint posterior distribution rather than just the marginals. By\nonly partially resampling particles, our online particle smoothing technique\navoids path degeneracy as the length of the state-space model increases. We\ndemonstrate the utility of our method in the context of map-matching, the task\nof inferring a vehicle's trajectory given a road network and noisy GPS\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 18:02:42 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Duffield", "Samuel", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "2012.04765", "submitter": "James Matuk", "authors": "James Matuk, Oksana Chkrebtii, Stephen Niezgoda", "title": "Bayesian Inference for Polycrystalline Materials", "comments": null, "journal-ref": "Stat. 2020;e340", "doi": "10.1002/sta4.340", "report-no": null, "categories": "stat.AP cond-mat.mtrl-sci", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Polycrystalline materials, such as metals, are comprised of heterogeneously\noriented crystals. Observed crystal orientations are modelled as a sample from\nan orientation distribution function (ODF), which determines a variety of\nmaterial properties and is therefore of great interest to practitioners.\nObservations consist of quaternions, 4-dimensional unit vectors reflecting both\norientation and rotation of a single crystal. Thus, an ODF must account for\nknown crystal symmetries as well as satisfy the unit length constraint. A\npopular method for estimating ODFs non-parametrically is symmetrized kernel\ndensity estimation. However, disadvantages of this approach include difficulty\nin interpreting results quantitatively, as well as in quantifying uncertainty\nin the ODF. We propose to use a mixture of symmetric Bingham distributions as a\nflexible parametric ODF model, inferring the number of mixture components, the\nmixture weights, and scale and location parameters based on crystal orientation\ndata. Furthermore, our Bayesian approach allows for structured uncertainty\nquantification of the parameters of interest. We discuss details of the\nsampling methodology and conclude with analyses of various orientation\ndatasets, interpretations of parameters of interest, and comparison with kernel\ndensity estimation methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 22:11:35 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Matuk", "James", ""], ["Chkrebtii", "Oksana", ""], ["Niezgoda", "Stephen", ""]]}, {"id": "2012.04825", "submitter": "Helen Zhou", "authors": "Cheng Cheng, Helen Zhou, Jeremy C. Weiss, Zachary C. Lipton", "title": "Unpacking the Drop in COVID-19 Case Fatality Rates: A Study of National\n  and Florida Line-Level Data", "comments": "24 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the COVID-19 pandemic first reached the United States, the case\nfatality rate has fallen precipitously. Several possible explanations have been\nfloated, including greater detection of mild cases due to expanded testing,\nshifts in age distribution among the infected, lags between confirmed cases and\nreported deaths, improvements in treatment, mutations in the virus, and\ndecreased viral load as a result of mask-wearing. Using both Florida line-level\ndata and recently released (but incomplete) national line level data from April\n1, 2020 to November 1, 2020 on cases, hospitalizations, and deaths--each\nstratified by age--we unpack the drop in case fatality rate (CFR). Under the\nhypothesis that improvements in treatment efficacy should correspond to\ndecreases in hospitalization fatality rate (HFR), we find that improvements in\nthe national data do not always match the story told by Florida data. In the\nnational data, treatment improvements between the first wave and the second\nwave appear substantial, but modest when compared to the drop in aggregate CFR.\nBy contrast, possibly due to constrained resources in a much larger second\npeak, Florida data suggests comparatively little difference between the first\nand second wave, with HFR slightly increasing in every age group. However, by\nNovember 1st, both Florida and national data suggest significant decreases in\nage-stratified HFR since April 1st. By accounting for several confounding\nfactors, our analysis shows how age-stratified HFR can provide a more realistic\npicture of treatment improvements than CFR. One key limitation of our analysis\nis that the national line-level data remains incomplete and plagued by\nartifacts. Our analysis highlights the crucial role that this data can play but\nalso the pressing need for public, complete, and high-quality age-stratified\nline-level data for both cases, hospitalizations, and deaths for all states.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:25:48 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 07:41:11 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Cheng", "Cheng", ""], ["Zhou", "Helen", ""], ["Weiss", "Jeremy C.", ""], ["Lipton", "Zachary C.", ""]]}, {"id": "2012.04831", "submitter": "Corwin Zigler", "authors": "Corwin Zigler, Laura Forastiere, Fabrizia Mealli", "title": "Bipartite Interference and Air Pollution Transport: Estimating Health\n  Effects of Power Plant Interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating air quality interventions is confronted with the challenge of\ninterference since interventions at a particular pollution source likely impact\nair quality and health at distant locations and air quality and health at any\ngiven location are likely impacted by interventions at many sources. The\nstructure of interference in this context is dictated by complex atmospheric\nprocesses governing how pollution emitted from a particular source is\ntransformed and transported across space, and can be cast with a bipartite\nstructure reflecting the two distinct types of units: 1) interventional units\non which treatments are applied or withheld to change pollution emissions; and\n2) outcome units on which outcomes of primary interest are measured. We propose\nnew estimands for bipartite causal inference with interference that construe\ntwo components of treatment: a \"key-associated\" (or \"individual\") treatment and\nan \"upwind\" (or \"neighborhood\") treatment. Estimation is carried out using a\nsemi-parametric adjustment approach based on joint propensity scores. A\nreduced-complexity atmospheric model is deployed to characterize the structure\nof the interference network by modeling the movement of air parcels through\ntime and space. The new methods are deployed to evaluate the effectiveness of\ninstalling flue-gas desulfurization scrubbers on 472 coal-burning power plants\n(the interventional units) in reducing Medicare hospitalizations among\n22,603,597 Medicare beneficiaries residing across 23,675 ZIP codes in the\nUnited States (the outcome units).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 02:40:17 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Zigler", "Corwin", ""], ["Forastiere", "Laura", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2012.04878", "submitter": "Qiwei Li", "authors": "Esteban Fern\\'andez Morales and Cong Zhang and Guanghua Xiao and Chul\n  Moon and Qiwei Li", "title": "Discovering Clinically Meaningful Shape Features for the Analysis of\n  Tumor Pathology Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advanced imaging technology, digital pathology imaging of tumor\ntissue slides is becoming a routine clinical procedure for cancer diagnosis.\nThis process produces massive imaging data that capture histological details in\nhigh resolution. Recent developments in deep-learning methods have enabled us\nto automatically detect and characterize the tumor regions in pathology images\nat large scale. From each identified tumor region, we extracted 30 well-defined\ndescriptors that quantify its shape, geometry, and topology. We demonstrated\nhow those descriptor features were associated with patient survival outcome in\nlung adenocarcinoma patients from the National Lung Screening Trial (n=143).\nBesides, a descriptor-based prognostic model was developed and validated in an\nindependent patient cohort from The Cancer Genome Atlas Program program\n(n=318). This study proposes new insights into the relationship between tumor\nshape, geometrical, and topological features and patient prognosis. We provide\nsoftware in the form of R code on GitHub:\nhttps://github.com/estfernandez/Slide_Image_Segmentation_and_Extraction.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 05:56:41 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Morales", "Esteban Fern\u00e1ndez", ""], ["Zhang", "Cong", ""], ["Xiao", "Guanghua", ""], ["Moon", "Chul", ""], ["Li", "Qiwei", ""]]}, {"id": "2012.05088", "submitter": "Apostolos Chalkis", "authors": "Apostolos Chalkis and Ioannis Z. Emiris", "title": "Modeling asset allocation strategies and a new portfolio performance\n  score", "comments": "16 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.CG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a powerful, geometric representation of financial portfolios and\nstock markets, which identifies the space of portfolios with the points lying\nin a simplex convex polytope. The ambient space has dimension equal to the\nnumber of stocks, or assets. Although our statistical tools are quite general,\nin this paper we focus on the problem of portfolio scoring. Our contribution is\nto introduce an original computational framework to model portfolio allocation\nstrategies, which is of independent interest for computational finance. To\nmodel asset allocation strategies, we employ log-concave distributions centered\non portfolio benchmarks. Our approach addresses the crucial question of\nevaluating portfolio management, and is relevant to the individual private\ninvestors as well as financial organizations. We evaluate the performance of an\nallocation, in a certain time period, by providing a new portfolio score, based\non the aforementioned framework and concepts. In particular, it relies on the\nexpected proportion of actually invested portfolios that it outperforms when a\ncertain set of strategies take place in that time period. We also discuss how\nthis set of strategies -- and the knowledge one may have about them -- could\nvary in our framework, and we provide additional versions of our score in order\nto obtain a more complete picture of its performance. In all cases, we show\nthat the score computations can be performed efficiently. Last but not least,\nwe expect this framework to be useful in portfolio optimization and in\nautomatically identifying extreme phenomena in a stock market.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 14:44:02 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 11:02:45 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Chalkis", "Apostolos", ""], ["Emiris", "Ioannis Z.", ""]]}, {"id": "2012.05108", "submitter": "Hugo Flores-Arguedas", "authors": "Hugo Flores-Arguedas, Marcos A. Capistr\\'an", "title": "Bayesian Analysis of Glucose Dynamics during the Oral Glucose Tolerance\n  Test (OGTT)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a model of the dynamics of the blood glucose level\nduring an Oral Glucose Tolerance Test (OGTT). This dynamic includes the action\nof insulin and glucagon in the glucose homeostasis process as the reaction of\nan oral stimulus. We propose a Bayesian approach in the inference of five\nparameters related to insulin secretion, glucagon secretion, gastrointestinal\nemptying, and basal glucose level. Two insulin indicators related to the\nglucose level in blood and in the gastrointestinal tract allow us to suggest a\nclassification for patients with impaired insulin sensitivity.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 13:50:30 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 13:46:51 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Flores-Arguedas", "Hugo", ""], ["Capistr\u00e1n", "Marcos A.", ""]]}, {"id": "2012.05127", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Principled selection of effect modifiers: Comments on 'Matching-adjusted\n  indirect comparisons: Application to time-to-event data'", "comments": "14 pages, submitted to Statistics in Medicine. Response to\n  `Matching-adjusted indirect comparisons: Application to time-to-event data'\n  by Aouni, Gaudel-Dedieu and Sebastien, published in Statistics in Medicine\n  (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this commentary, we raise our concerns about a recent simulation study\nconducted by Aouni, Gaudel-Dedieu and Sebastien, evaluating the performance of\ndifferent versions of matching-adjusted indirect comparison (MAIC). The\nfollowing points are highlighted: (1) making a clear distinction between\nprognostic and effect-modifying covariates is important; (2) in the anchored\nsetting, MAIC is necessary where there are cross-trial imbalances in effect\nmodifiers; (3) the standard indirect comparison provides greater precision and\naccuracy than MAIC if there are no effect modifiers in imbalance; (4) while the\ntarget estimand of the simulation study is a conditional treatment effect, MAIC\ntargets a marginal or population-average treatment effect; (5) in MAIC,\nvariable selection is a problem of low dimensionality and sparsity-inducing\nmethods like the LASSO may induce bias; and (6) individual studies are\nunderpowered to detect interactions and data-driven approaches do not obviate\nthe necessity for subject matter knowledge when selecting effect modifiers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:01:53 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 10:41:26 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2012.05140", "submitter": "Feliu Serra-Burriel", "authors": "Feliu Serra-Burriel and Pedro Delicado and Andrew T. Prata and\n  Fernando M. Cucchietti", "title": "Estimating heterogeneous wildfire effects using synthetic controls and\n  satellite remote sensing", "comments": "29 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wildfires have become one of the biggest natural hazards for environments\nworldwide. The effects of wildfires are heterogeneous, meaning that the\nmagnitude of their effects depends on many factors such as geographical region,\nclimate and land cover/vegetation type. Yet, which areas are more affected by\nthese events remains unclear. Here we present a novel application of the\nGeneralised Synthetic Control (GSC) method that enables quantification and\nprediction of vegetation changes due to wildfires through a time-series\nanalysis of in situ and satellite remote sensing data. We apply this method to\nmedium to large wildfires ($>$ 1000 acres) in California throughout a time-span\nof two decades (1996--2016). The method's ability for estimating counterfactual\nvegetation characteristics for burned regions is explored in order to quantify\nabrupt system changes. We find that the GSC method is better at predicting\nvegetation changes than the more traditional approach of using nearby regions\nto assess wildfire impacts. We evaluate the GSC method by comparing its\npredictions of spectral vegetation indices to observations during pre-wildfire\nperiods and find improvements in correlation coefficient from $R^2 = 0.66$ to\n$R^2 = 0.93$ in Normalised Difference Vegetation Index (NDVI), from $R^2 =\n0.48$ to $R^2 = 0.81$ for Normalised Burn Ratio (NBR), and from $R^2 = 0.49$ to\n$R^2 = 0.85$ for Normalised Difference Moisture Index (NDMI). Results show\ngreater changes in NDVI, NBR, and NDMI post-fire on regions classified as\nhaving a lower Burning Index. The GSC method also reveals that wildfire effects\non vegetation can last for more than a decade post-wildfire, and in some cases\nnever return to their previous vegetation cycles within our study period.\nLastly, we discuss the usefulness of using GSC in remote sensing analyses.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 16:22:16 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2020 09:38:02 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 10:42:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Serra-Burriel", "Feliu", ""], ["Delicado", "Pedro", ""], ["Prata", "Andrew T.", ""], ["Cucchietti", "Fernando M.", ""]]}, {"id": "2012.05216", "submitter": "Furkan Gursoy", "authors": "Furkan Gursoy, Bertan Badur", "title": "Extracting the signed backbone of intrinsically dense weighted networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Networks provide useful tools for analyzing diverse complex systems from\nnatural, social, and technological domains. Growing size and variety of data\nsuch as more nodes and links and associated weights, directions, and signs can\nprovide accessory information. Link and weight abundance, on the other hand,\nresults in denser networks with noisy, insignificant, or otherwise redundant\ndata. Moreover, typical network analysis and visualization techniques\npresuppose sparsity and are not appropriate or scalable for dense and weighted\nnetworks. As a remedy, network backbone extraction methods aim to retain only\nthe important links while preserving the useful and elucidative structure of\nthe original networks for further analyses. Here, we provide the first methods\nfor extracting signed network backbones from intrinsically dense unsigned\nunipartite weighted networks. Utilizing a null model based on statistical\ntechniques, the proposed significance filter and vigor filter allow inferring\nedge signs. Empirical analysis on migration, voting, temporal interaction, and\nspecies similarity networks reveals that the proposed filters extract\nmeaningful and sparse signed backbones while preserving the multiscale nature\nof the network. The resulting backbones exhibit characteristics typically\nassociated with signed networks such as reciprocity, structural balance, and\ncommunity structure. The developed tool is provided as a free, open-source\nsoftware package.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 18:27:11 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 10:57:12 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gursoy", "Furkan", ""], ["Badur", "Bertan", ""]]}, {"id": "2012.05285", "submitter": "Fernando Castro-Prado", "authors": "Fernando Castro-Prado, Javier Costas, Wenceslao Gonz\\'alez-Manteiga,\n  David R. Penas", "title": "Searching for genetic interactions in complex disease by using distance\n  correlation", "comments": "24 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.GN stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding epistasis (genetic interaction) may shed some light on the\ngenomic basis of common diseases, including disorders of maximum interest due\nto their high socioeconomic burden, like schizophrenia. Distance correlation is\nan association measure that characterises general statistical independence\nbetween random variables, not only the linear one. Here, we propose distance\ncorrelation as a novel tool for the detection of epistasis from case-control\ndata of single nucleotide polymorphisms (SNPs). This approach will be developed\nboth theoretically (mathematical statistics, in a context of high-dimensional\nstatistical inference) and from an applied point of view (simulations and real\ndatasets).\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 19:50:54 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Castro-Prado", "Fernando", ""], ["Costas", "Javier", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Penas", "David R.", ""]]}, {"id": "2012.05294", "submitter": "Imelda Trejo", "authors": "Imelda Trejo and Nicolas Hengartner", "title": "A modified Susceptible-Infected-Recovered model for observed\n  under-reported incidence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fitting Susceptible-Infected-Recovered (SIR) models to incidence data is\nproblematic when a fraction $q$ of the infected individuals are not reported.\nAssuming an underlying SIR model with general but known distribution for the\ntime to recovery, this paper derives the implied differential-integral\nequations for observed incidence data when a fixed fraction $q$ of newly\ninfected individuals are not observed. The parameters of the resulting system\nof differential equations are identifiable. Using these differential equations,\nwe develop a stochastic model for the conditional distribution of current\ndisease incidence given the entire past history of incidences. This results in\nan epidemic model that can track complex epidemic dynamics, such as outbreaks\nwith multiple waves. We propose to estimate of model parameters using Bayesian\nMonte-Carlo Markov Chain sampling of the posterior distribution. We apply our\nmodel to estimate the infection rate and fraction of asymptomatic individuals\nfor the current Coronavirus 2019 outbreak in eight countries in North and South\nAmerica. Our analysis reveals that consistently, about 70-90\\% of infected\nindividuals were not observed in the American outbreaks.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:05:49 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Trejo", "Imelda", ""], ["Hengartner", "Nicolas", ""]]}, {"id": "2012.05301", "submitter": "David Norris", "authors": "David C. Norris", "title": "What Were They Thinking? Pharmacologic priors implicit in a choice of\n  3+3 dose-escalation design", "comments": "5 pages, 3 figures, 1 table, 13 references with added hyperlinks.\n  Version 2 restores to the exhibit on page 2 a DCG clause which v1 had\n  mistakenly cropped; additionally, footnote 6 is expanded and a few minor\n  typographical or grammatical improvements are made", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  If explicit, formal consideration of clinical pharmacology at all informs the\ndesign and conduct of modern oncology dose-finding trials, the designs\nthemselves hardly attest to this. Yet in conducting a trial, investigators\naffirm that they hold reasonable expectations of participant safety -\nexpectations that necessarily depend on beliefs about how certain pharmacologic\nparameters are distributed in the study population. Thus, these beliefs are\nimplicit in a trial's presumed conformance to a community standard of safety,\nand may therefore to some extent be reverse-engineered from trial designs. For\none popular form of dose-escalation trial design, I demonstrate here how this\nmay be done.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:33:38 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 14:18:38 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Norris", "David C.", ""]]}, {"id": "2012.05313", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas and Han Lin Shang", "title": "A partial least squares approach for function-on-function interaction\n  regression", "comments": "34 pages, 6 figures, 2 tables, to appear at Computational Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A partial least squares regression is proposed for estimating the\nfunction-on-function regression model where a functional response and multiple\nfunctional predictors consist of random curves with quadratic and interaction\neffects. The direct estimation of a function-on-function regression model is\nusually an ill-posed problem. To overcome this difficulty, in practice, the\nfunctional data that belong to the infinite-dimensional space are generally\nprojected into a finite-dimensional space of basis functions. The\nfunction-on-function regression model is converted to a multivariate regression\nmodel of the basis expansion coefficients. In the estimation phase of the\nproposed method, the functional variables are approximated by a\nfinite-dimensional basis function expansion method. We show that the partial\nleast squares regression constructed via a functional response, multiple\nfunctional predictors, and quadratic/interaction terms of the functional\npredictors is equivalent to the partial least squares regression constructed\nusing basis expansions of functional variables. From the partial least squares\nregression of the basis expansions of functional variables, we provide an\nexplicit formula for the partial least squares estimate of the coefficient\nfunction of the function-on-function regression model. Because the true forms\nof the models are generally unspecified, we propose a forward procedure for\nmodel selection. The finite sample performance of the proposed method is\nexamined using several Monte Carlo experiments and two empirical data analyses,\nand the results were found to compare favorably with an existing method.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 20:51:00 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""]]}, {"id": "2012.05346", "submitter": "Xiaoyue Niu", "authors": "Jacob Parsons, Xiaoyue Niu, Le Bao", "title": "A Bayesian hierarchical modeling approach to combining multiple data\n  sources: A case study in size estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To combat the HIV/AIDS pandemic effectively, targeted interventions among\ncertain key populations play a critical role. Examples of such key populations\ninclude sex workers, people who inject drugs, and men who have sex with men.\nWhile having accurate estimates for the size of these key populations is\nimportant, any attempt to directly contact or count members of these\npopulations is difficult. As a result, indirect methods are used to produce\nsize estimates. Multiple approaches for estimating the size of such populations\nhave been suggested but often give conflicting results. It is therefore\nnecessary to have a principled way to combine and reconcile these estimates. To\nthis end, we present a Bayesian hierarchical model for estimating the size of\nkey populations that combines multiple estimates from different sources of\ninformation. The proposed model makes use of multiple years of data and\nexplicitly models the systematic error in the data sources used. We use the\nmodel to estimate the size of people who inject drugs in Ukraine. We evaluate\nthe appropriateness of the model and compare the contribution of each data\nsource to the final estimates.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2020 22:15:36 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:44:23 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Parsons", "Jacob", ""], ["Niu", "Xiaoyue", ""], ["Bao", "Le", ""]]}, {"id": "2012.05488", "submitter": "Billy Pik Lik Lau", "authors": "Nipun Wijerathne, Billy Pik Lik Lau, Benny Kai Kiat Ng, Chau Yuen", "title": "Urban Space Insights Extraction using Acoustic Histogram Information", "comments": "Accepted at IEEE Systems Journal", "journal-ref": null, "doi": "10.1109/JSYST.2020.3044325", "report-no": null, "categories": "stat.AP cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Urban data mining can be identified as a highly potential area that can\nenhance the smart city services towards better sustainable development\nespecially in the urban residential activity tracking. While existing human\nactivity tracking systems have demonstrated the capability to unveil the hidden\naspects of citizens' behavior, they often come with a high implementation cost\nand require a large communication bandwidth. In this paper, we study the\nimplementation of low-cost analogue sound sensors to detect outdoor activities\nand estimate the raining period in an urban residential area. The analogue\nsound sensors are transmitted to the cloud every 5 minutes in histogram format,\nwhich consists of sound data sampled every 100ms (10Hz). We then use wavelet\ntransformation (WT) and principal component analysis (PCA) to generate a more\nrobust and consistent feature set from the histogram. After that, we performed\nunsupervised clustering and attempt to understand the individual\ncharacteristics of each cluster to identify outdoor residential activities. In\naddition, on-site validation has been conducted to show the effectiveness of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 07:21:34 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 06:02:50 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Wijerathne", "Nipun", ""], ["Lau", "Billy Pik Lik", ""], ["Ng", "Benny Kai Kiat", ""], ["Yuen", "Chau", ""]]}, {"id": "2012.05648", "submitter": "Katharina Gruber", "authors": "Katharina Gruber, Peter Regner, Sebastian Wehrle, Marianne Zeyringer,\n  Johannes Schmidt", "title": "Towards a global dynamic wind atlas: A multi-country validation of wind\n  power simulation from MERRA-2 and ERA-5 reanalyses bias-corrected with the\n  Global Wind Atlas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reanalysis data are widely used for simulating renewable energy and in\nparticular wind power generation. While MERRA-2 has been a de-facto standard in\nmany studies, the newer ERA5- reanalysis recently gained importance. Here, we\nuse these two datasets to simulate wind power generation and evaluate the\nrespective quality in terms of correlations and errors when validated against\nhistorical wind power generation. However, due to their coarse spatial\nresolution, reanalyses fail to adequately represent local climatic conditions.\nWe therefore additionally apply mean bias correction with two versions of the\nGlobal Wind Atlas (GWA) and assess the respective quality of resulting\nsimulations. Potential users of the dataset can also benefit from our analysis\nof the impact of spatial and temporal aggregation on simulation quality\nindicators. While similar studies have been conducted, they mainly cover\nlimited areas in Europe. In contrast, we look into regions, which globally\ndiffer significantly in terms of the prevailing climate: the US, Brazil,\nSouth-Africa, and New Zealand. Our principal findings are that (i) ERA5\noutperforms MERRA-2, (ii) no major improvements can be expected by using\nbias-correction with GWA2, while GWA3 even reduces simulation quality, and\n(iii) temporal aggregation increases correlations and reduces errors, while\nspatial aggregation does so only consistently when comparing very low and very\nhigh aggregation levels.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 13:12:05 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Gruber", "Katharina", ""], ["Regner", "Peter", ""], ["Wehrle", "Sebastian", ""], ["Zeyringer", "Marianne", ""], ["Schmidt", "Johannes", ""]]}, {"id": "2012.05724", "submitter": "Cristi\\'an Bravo", "authors": "David Barrera Ferro, Sally Brailsford, Cristi\\'an Bravo, Honora Smith", "title": "Improving healthcare access management by predicting patient no-show\n  behaviour", "comments": "v4 - 26 pages", "journal-ref": "Decision Support Systems 138: 113398 (2020)", "doi": "10.1016/j.dss.2020.113398", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Low attendance levels in medical appointments have been associated with poor\nhealth outcomes and efficiency problems for service providers. To address this\nproblem, healthcare managers could aim at improving attendance levels or\nminimizing the operational impact of no-shows by adapting resource allocation\npolicies. However, given the uncertainty of patient behaviour, generating\nrelevant information regarding no-show probabilities could support the\ndecision-making process for both approaches. In this context many researchers\nhave used multiple regression models to identify patient and appointment\ncharacteristics than can be used as good predictors for no-show probabilities.\nThis work develops a Decision Support System (DSS) to support the\nimplementation of strategies to encourage attendance, for a preventive care\nprogram targeted at underserved communities in Bogot\\'a, Colombia. Our\ncontribution to literature is threefold. Firstly, we assess the effectiveness\nof different machine learning approaches to improve the accuracy of regression\nmodels. In particular, Random Forest and Neural Networks are used to model the\nproblem accounting for non-linearity and variable interactions. Secondly, we\npropose a novel use of Layer-wise Relevance Propagation in order to improve the\nexplainability of neural network predictions and obtain insights from the\nmodelling step. Thirdly, we identify variables explaining no-show probabilities\nin a developing context and study its policy implications and potential for\nimproving healthcare access. In addition to quantifying relationships reported\nin previous studies, we find that income and neighbourhood crime statistics\naffect no-show probabilities. Our results will support patient prioritization\nin a pilot behavioural intervention and will inform appointment planning\ndecisions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:57:25 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Ferro", "David Barrera", ""], ["Brailsford", "Sally", ""], ["Bravo", "Cristi\u00e1n", ""], ["Smith", "Honora", ""]]}, {"id": "2012.05820", "submitter": "Ben Moews", "authors": "Ben Moews, Romeel Dav\\'e, Sourav Mitra, Sultan Hassan, Weiguang Cui", "title": "Hybrid analytic and machine-learned baryonic property insertion into\n  galactic dark matter haloes", "comments": "13 pages, 8 figures, preprint submitted to MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab1120", "report-no": null, "categories": "astro-ph.GA astro-ph.IM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While cosmological dark matter-only simulations relying solely on\ngravitational effects are comparably fast to compute, baryonic properties in\nsimulated galaxies require complex hydrodynamic simulations that are\ncomputationally costly to run. We explore the merging of an extended version of\nthe equilibrium model, an analytic formalism describing the evolution of the\nstellar, gas, and metal content of galaxies, into a machine learning framework.\nIn doing so, we are able to recover more properties than the analytic formalism\nalone can provide, creating a high-speed hydrodynamic simulation emulator that\npopulates galactic dark matter haloes in N-body simulations with baryonic\nproperties. While there exists a trade-off between the reached accuracy and the\nspeed advantage this approach offers, our results outperform an approach using\nonly machine learning for a subset of baryonic properties. We demonstrate that\nthis novel hybrid system enables the fast completion of dark matter-only\ninformation by mimicking the properties of a full hydrodynamic suite to a\nreasonable degree, and discuss the advantages and disadvantages of hybrid\nversus machine learning-only frameworks. In doing so, we offer an acceleration\nof commonly deployed simulations in cosmology.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 16:50:33 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Moews", "Ben", ""], ["Dav\u00e9", "Romeel", ""], ["Mitra", "Sourav", ""], ["Hassan", "Sultan", ""], ["Cui", "Weiguang", ""]]}, {"id": "2012.05905", "submitter": "Jordi Mu\\~noz-Mar\\'i", "authors": "Anna Mateo-Sanchis, Maria Piles, Jordi Mu\\~noz-Mar\\'i, Jose E.\n  Adsuara, Adri\\'an P\\'erez-Suay, Gustau Camps-Valls", "title": "Synergistic Integration of Optical and Microwave Satellite Data for Crop\n  Yield Estimation", "comments": "53 pages, 8 figures, 3 tables", "journal-ref": "Remote Sensing of Environment Volume 234, 1 December 2019, 111460", "doi": "10.1016/j.rse.2019.111460", "report-no": null, "categories": "eess.SP physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Developing accurate models of crop stress, phenology and productivity is of\nparamount importance, given the increasing need of food. Earth observation\nremote sensing data provides a unique source of information to monitor crops in\na temporally resolved and spatially explicit way. In this study, we propose the\ncombination of multisensor (optical and microwave) remote sensing data for crop\nyield estimation and forecasting using two novel approaches. We first propose\nthe lag between Enhanced Vegetation Index derived from MODIS and Vegetation\nOptical Depth derived from SMAP as a new joint metric combining the information\nfrom the two satellite sensors in a unique feature or descriptor. Our second\napproach avoids summarizing statistics and uses machine learning to combine\nfull time series of EVI and VOD. This study considers two statistical methods,\na regularized linear regression and its nonlinear extension called kernel ridge\nregression to directly estimate the county-level surveyed total production, as\nwell as individual yields of the major crops grown in the region: corn, soybean\nand wheat. The study area includes the US Corn Belt, and we use agricultural\nsurvey data from the National Agricultural Statistics Service (USDA-NASS) for\nyear 2015 for quantitative assessment.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:15:52 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Mateo-Sanchis", "Anna", ""], ["Piles", "Maria", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Adsuara", "Jose E.", ""], ["P\u00e9rez-Suay", "Adri\u00e1n", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.05907", "submitter": "Xinyu He", "authors": "Xinyu He, Fang He, Xinting Zhu, Lishuai Li", "title": "Data-driven Method for Estimating Aircraft Mass from Quick Access\n  Recorder using Aircraft Dynamics and Multilayer Perceptron Neural Network", "comments": "20 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate aircraft-mass estimation is critical to airlines from the\nsafety-management and performance-optimization viewpoints. Overloading an\naircraft with passengers and baggage might result in a safety hazard. In\ncontrast, not fully utilizing an aircraft's payload-carrying capacity\nundermines its operational efficiency and airline profitability. However,\naccurate determination of the aircraft mass for each operating flight is not\nfeasible because it is impractical to weigh each aircraft component, including\nthe payload. The existing methods for aircraft-mass estimation are dependent on\nthe aircraft- and engine-performance parameters, which are usually considered\nproprietary information. Moreover, the values of these parameters vary under\ndifferent operating conditions while those of others might be subject to large\nestimation errors. This paper presents a data-driven method involving use of\nthe quick access recorder (QAR)-a digital flight-data recorder-installed on all\naircrafts to record the initial aircraft climb mass during each flight. The\nmethod requires users to select appropriate parameters among several thousand\nothers recorded by the QAR using physical models. The selected data are\nsubsequently processed and provided as input to a multilayer perceptron neural\nnetwork for building the model for initial-climb aircraft-mass prediction.\nThus, the proposed method offers the advantages of both the model-based and\ndata-driven approaches for aircraft-mass estimation. Because this method does\nnot explicitly rely on any aircraft or engine parameter, it is universally\napplicable to all aircraft types. In this study, the proposed method was\napplied to a set of Boeing 777-300ER aircrafts, the results of which\ndemonstrated reasonable accuracy. Airlines can use this tool to better utilize\naircraft's payload.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 04:44:47 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["He", "Xinyu", ""], ["He", "Fang", ""], ["Zhu", "Xinting", ""], ["Li", "Lishuai", ""]]}, {"id": "2012.05938", "submitter": "Pauline Barmby", "authors": "Dayi Li (1 and 2), Pauline Barmby (2) ((1) Univ. Toronto, (2) Western\n  Univ.)", "title": "Gibbs Point Process Model for Young Star Clusters in M33", "comments": "MNRAS in press; 22 pages", "journal-ref": "2021, MNRAS, 501, 3472", "doi": "10.1093/mnras/staa3908", "report-no": null, "categories": "astro-ph.GA astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate the power of Gibbs point process models from the spatial\nstatistics literature when applied to studies of resolved galaxies. We conduct\na rigorous analysis of the spatial distributions of objects in the star\nformation complexes of M33, including giant molecular clouds (GMCs) and young\nstellar cluster candidates (YSCCs). We choose a hierarchical model structure\nfrom GMCs to YSCCs based on the natural formation hierarchy between them. This\napproach circumvents the limitations of the empirical two-point correlation\nfunction analysis by naturally accounting for the inhomogeneity present in the\ndistribution of YSCCs. We also investigate the effects of GMCs' properties on\ntheir spatial distributions. We confirm that the distribution of GMCs and YSCCs\nare highly correlated. We found that the spatial distributions of YSCCs reaches\na peak of clustering pattern at ~250 pc scale compared to a Poisson process.\nThis clustering mainly occurs in regions where the galactocentric distance\n>~4.5 kpc. Furthermore, the galactocentric distance of GMCs and their mass have\nstrong positive effects on the correlation strength between GMCs and YSCCs. We\noutline some possible implications of these findings for our understanding of\nthe cluster formation process.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 19:33:45 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Li", "Dayi", "", "1 and 2"], ["Barmby", "Pauline", ""]]}, {"id": "2012.05967", "submitter": "Matthias Katzfuss", "authors": "Brian Kidd, Matthias Katzfuss", "title": "Bayesian nonstationary and nonparametric covariance estimation for large\n  spatial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spatial statistics, it is often assumed that the spatial field of interest\nis stationary and its covariance has a simple parametric form, but these\nassumptions are not appropriate in many applications. Given replicate\nobservations of a Gaussian spatial field, we propose nonstationary and\nnonparametric Bayesian inference on the spatial dependence. Instead of\nestimating the quadratic (in the number of spatial locations) entries of the\ncovariance matrix, the idea is to infer a near-linear number of nonzero entries\nin a sparse Cholesky factor of the precision matrix. Our prior assumptions are\nmotivated by recent results on the exponential decay of the entries of this\nCholesky factor for Matern-type covariances under a specific ordering scheme.\nOur methods are highly scalable and parallelizable. We conduct numerical\ncomparisons and apply our methodology to climate-model output, enabling\nstatistical emulation of an expensive physical model.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:48:43 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Kidd", "Brian", ""], ["Katzfuss", "Matthias", ""]]}, {"id": "2012.05968", "submitter": "Yan-Cheng Chao", "authors": "Yan-Cheng Chao (1), Thomas M. Braun (1), Roy N. Tamura (2), Kelley M.\n  Kidwell (1) ((1) Department of Biostatistics, School of Public Health,\n  University of Michigan, Ann Arbor, USA, (2) Health Informatics Institute,\n  University of South Florida, Tampa, USA)", "title": "Power prior models for treatment effect estimation in a small n,\n  sequential, multiple assignment, randomized trial", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A small n, sequential, multiple assignment, randomized trial (snSMART) is a\nsmall sample, two-stage design where participants receive up to two treatments\nsequentially, but the second treatment depends on response to the first\ntreatment. The treatment effect of interest in an snSMART is the first-stage\nresponse rate, but outcomes from both stages can be used to obtain more\ninformation from a small sample. A novel way to incorporate the outcomes from\nboth stages applies power prior models, in which first stage outcomes from an\nsnSMART are regarded as the primary data and second stage outcomes are regarded\nas supplemental. We apply existing power prior models to snSMART data, and we\nalso develop new extensions of power prior models. All methods are compared to\neach other and to the Bayesian joint stage model (BJSM) via simulation studies.\nBy comparing the biases and the efficiency of the response rate estimates among\nall proposed power prior methods, we suggest application of Fisher's exact test\nor the Bhattacharyya's overlap measure to an snSMART to estimate the treatment\neffect in an snSMART, which both have performance mostly as good or better than\nthe BJSM. We describe the situations where each of these suggested approaches\nis preferred.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 20:48:55 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Chao", "Yan-Cheng", ""], ["Braun", "Thomas M.", ""], ["Tamura", "Roy N.", ""], ["Kidwell", "Kelley M.", ""]]}, {"id": "2012.06038", "submitter": "Luk Arnaut", "authors": "Luk R. Arnaut", "title": "Excess Power, Energy and Intensity of Stochastic Fields in Quasi-Static\n  and Dynamic Environments", "comments": "(10 pages, 11 figures, accepted for IEEE Trans. Electromagn. Compat.,\n  Dec. 2020)", "journal-ref": null, "doi": "10.1109/TEMC.2020.3039376", "report-no": null, "categories": "physics.app-ph physics.class-ph physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The excess power, energy and intensity of a random electromagnetic field\nabove a high threshold level are characterized based on a Slepian--Kac model\nfor upcrossings. For quasi-static fields, the probability distribution of the\nexcess intensity in its regression approximation evolves from $\\chi^2_3$ to\n$\\chi^2_2$ when the threshold level increases. The excursion area associated\nwith excess energy exhibits a chi-cubed ($\\chi^3_2$) distribution above\nasymptotically high thresholds, where excursions are parabolic. For dynamic\nfields, the dependence of the electrical and environmental modulations of the\nexcess power on the hybrid modulation index and threshold level are\nestablished. The normalized effective power relative to the quasi-static power\nincreases non-monotonically when this index increases. The mean and standard\ndeviation of the dynamic excess power are obtained in closed form and validated\nby Monte Carlo simulation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Dec 2020 19:14:11 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Arnaut", "Luk R.", ""]]}, {"id": "2012.06077", "submitter": "Stuart Lee", "authors": "Stuart Lee, Ursula Laa, Dianne Cook", "title": "Casting Multiple Shadows: High-Dimensional Interactive Data\n  Visualisation with Tours and Embeddings", "comments": "25 pages, 7 figures, submitted to JDSSV", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Non-linear dimensionality reduction (NLDR) methods such as t-distributed\nstochastic neighbour embedding (t-SNE) are ubiquitous in the natural sciences,\nhowever, the appropriate use of these methods is difficult because of their\ncomplex parameterisations; analysts must make trade-offs in order to identify\nstructure in the visualisation of an NLDR technique. We present visual\ndiagnostics for the pragmatic usage of NLDR methods by combining them with a\ntechnique called the tour. A tour is a sequence of interpolated linear\nprojections of multivariate data onto a lower dimensional space. The sequence\nis displayed as a dynamic visualisation, allowing a user to see the shadows the\nhigh-dimensional data casts in a lower dimensional view. By linking the tour to\nan NLDR view, we can preserve global structure and through user interactions\nlike linked brushing observe where the NLDR view may be misleading. We display\nseveral case studies from both simulations and single cell transcriptomics,\nthat shows our approach is useful for cluster orientation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 01:50:26 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Lee", "Stuart", ""], ["Laa", "Ursula", ""], ["Cook", "Dianne", ""]]}, {"id": "2012.06267", "submitter": "Robin Upham", "authors": "Robin E. Upham, Michael L. Brown and Lee Whittaker", "title": "Sufficiency of a Gaussian power spectrum likelihood for accurate\n  cosmology from upcoming weak lensing surveys", "comments": "15 pages, 19 figures, matches version accepted by MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stab522", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate whether a Gaussian likelihood is sufficient to obtain accurate\nparameter constraints from a Euclid-like combined tomographic power spectrum\nanalysis of weak lensing, galaxy clustering and their cross-correlation.\nTesting its performance on the full sky against the Wishart distribution, which\nis the exact likelihood under the assumption of Gaussian fields, we find that\nthe Gaussian likelihood returns accurate parameter constraints. This accuracy\nis robust to the choices made in the likelihood analysis, including the choice\nof fiducial cosmology, the range of scales included, and the random noise\nlevel. We extend our results to the cut sky by evaluating the additional\nnon-Gaussianity of the joint cut-sky likelihood in both its marginal\ndistributions and dependence structure. We find that the cut-sky likelihood is\nmore non-Gaussian than the full-sky likelihood, but at a level insufficient to\nintroduce significant inaccuracy into parameter constraints obtained using the\nGaussian likelihood. Our results should not be affected by the assumption of\nGaussian fields, as this approximation only becomes inaccurate on small scales,\nwhich in turn corresponds to the limit in which any non-Gaussianity of the\nlikelihood becomes negligible. We nevertheless compare against N-body weak\nlensing simulations and find no evidence of significant additional\nnon-Gaussianity in the likelihood. Our results indicate that a Gaussian\nlikelihood will be sufficient for robust parameter constraints with power\nspectra from Stage IV weak lensing surveys.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 12:01:12 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2021 14:14:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Upham", "Robin E.", ""], ["Brown", "Michael L.", ""], ["Whittaker", "Lee", ""]]}, {"id": "2012.06391", "submitter": "Suryanarayana Maddu", "authors": "Suryanarayana Maddu, Bevan L. Cheeseman, Christian L. M\\\"uller, Ivo F.\n  Sbalzarini", "title": "Learning physically consistent mathematical models from data using group\n  sparsity", "comments": null, "journal-ref": "Phys. Rev. E 103, 042310 (2021)", "doi": "10.1103/PhysRevE.103.042310", "report-no": null, "categories": "cs.LG q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a statistical learning framework based on group-sparse regression\nthat can be used to 1) enforce conservation laws, 2) ensure model equivalence,\nand 3) guarantee symmetries when learning or inferring differential-equation\nmodels from measurement data. Directly learning $\\textit{interpretable}$\nmathematical models from data has emerged as a valuable modeling approach.\nHowever, in areas like biology, high noise levels, sensor-induced correlations,\nand strong inter-system variability can render data-driven models nonsensical\nor physically inconsistent without additional constraints on the model\nstructure. Hence, it is important to leverage $\\textit{prior}$ knowledge from\nphysical principles to learn \"biologically plausible and physically consistent\"\nmodels rather than models that simply fit the data best. We present a novel\ngroup Iterative Hard Thresholding (gIHT) algorithm and use stability selection\nto infer physically consistent models with minimal parameter tuning. We show\nseveral applications from systems biology that demonstrate the benefits of\nenforcing $\\textit{priors}$ in data-driven modeling.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 14:45:38 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Maddu", "Suryanarayana", ""], ["Cheeseman", "Bevan L.", ""], ["M\u00fcller", "Christian L.", ""], ["Sbalzarini", "Ivo F.", ""]]}, {"id": "2012.06417", "submitter": "Alvaro Moreno", "authors": "Alvaro Moreno-Martinez, Gustau Camps-Valls, Jens Kattge, Nathaniel\n  Robinson, Markus Reichstein, Peter van Bodegom, Koen Kramer, J. Hans C.\n  Cornelissen, Peter Reich, Michael Bahn, \u007fUlo Niinemets, Josep Pe\\~nuelas,\n  Joseph Craine, Bruno E.L. Cerabolini, Vanessa Minden, Daniel C. Laughlin,\n  Lawren Sack, Brady Allred, Christopher Baraloto, Chaeho Byun, Nadejda A.\n  Soudzilovskaia, Steven W. Running", "title": "A Methodology to Derive Global Maps of Leaf Traits Using Remote Sensing\n  and Climate Data", "comments": null, "journal-ref": "Remote Sensing of Environment, Volume 218, 1 December 2018, Pages\n  69-88", "doi": "10.1016/j.rse.2018.09.006", "report-no": null, "categories": "stat.AP physics.app-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a modular processing chain to derive global\nhigh-resolution maps of leaf traits. In particular, we present global maps at\n500 m resolution of specific leaf area, leaf dry matter content, leaf nitrogen\nand phosphorus content per dry mass, and leaf nitrogen/phosphorus ratio. The\nprocessing chain exploits machine learning techniques along with optical remote\nsensing data (MODIS/Landsat) and climate data for gap filling and up-scaling of\nin-situ measured leaf traits. The chain first uses random forests regression\nwith surrogates to fill gaps in the database ($> 45 \\% $ of missing entries)\nand maximize the global representativeness of the trait dataset. Along with the\nestimated global maps of leaf traits, we provide associated uncertainty\nestimates derived from the regression models. The process chain is modular, and\ncan easily accommodate new traits, data streams (traits databases and remote\nsensing data), and methods. The machine learning techniques applied allow\nattribution of information gain to data input and thus provide the opportunity\nto understand trait-environment relationships at the plant and ecosystem\nscales.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 15:20:35 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Moreno-Martinez", "Alvaro", ""], ["Camps-Valls", "Gustau", ""], ["Kattge", "Jens", ""], ["Robinson", "Nathaniel", ""], ["Reichstein", "Markus", ""], ["van Bodegom", "Peter", ""], ["Kramer", "Koen", ""], ["Cornelissen", "J. Hans C.", ""], ["Reich", "Peter", ""], ["Bahn", "Michael", ""], ["Niinemets", "\u007fUlo", ""], ["Pe\u00f1uelas", "Josep", ""], ["Craine", "Joseph", ""], ["Cerabolini", "Bruno E. L.", ""], ["Minden", "Vanessa", ""], ["Laughlin", "Daniel C.", ""], ["Sack", "Lawren", ""], ["Allred", "Brady", ""], ["Baraloto", "Christopher", ""], ["Byun", "Chaeho", ""], ["Soudzilovskaia", "Nadejda A.", ""], ["Running", "Steven W.", ""]]}, {"id": "2012.06564", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Paulo F\\'elix, Carlos Meijide-Garcia and Francisco\n  Gude", "title": "Glucose values prediction five years ahead with a new framework of\n  missing responses in reproducing kernel Hilbert spaces, and the use of\n  continuous glucose monitoring technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AEGIS study possesses unique information on longitudinal changes in\ncirculating glucose through continuous glucose monitoring technology (CGM).\nHowever, as usual in longitudinal medical studies, there is a significant\namount of missing data in the outcome variables. For example, 40 percent of\nglycosylated hemoglobin (A1C) biomarker data are missing five years ahead. With\nthe purpose to reduce the impact of this issue, this article proposes a new\ndata analysis framework based on learning in reproducing kernel Hilbert spaces\n(RKHS) with missing responses that allows to capture non-linear relations\nbetween variable studies in different supervised modeling tasks. First, we\nextend the Hilbert-Schmidt dependence measure to test statistical independence\nin this context introducing a new bootstrap procedure, for which we prove\nconsistency. Next, we adapt or use existing models of variable selection,\nregression, and conformal inference to obtain new clinical findings about\nglucose changes five years ahead with the AEGIS data. The most relevant\nfindings are summarized below: i) We identify new factors associated with\nlong-term glucose evolution; ii) We show the clinical sensibility of CGM data\nto detect changes in glucose metabolism; iii) We can improve clinical\ninterventions based on our algorithms' expected glucose changes according to\npatients' baseline characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 11 Dec 2020 18:51:44 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 18:47:42 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Matabuena", "Marcos", ""], ["F\u00e9lix", "Paulo", ""], ["Meijide-Garcia", "Carlos", ""], ["Gude", "Francisco", ""]]}, {"id": "2012.06577", "submitter": "Wolfgang Karl H\\\"ardle", "authors": "Kainat Khowaja, Danial Saef, Sergej Sizov, Wolfgang Karl H\\\"ardle", "title": "Data Analytics Driven Controlling: bridging statistical modeling and\n  managerial intuition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Strategic planning in a corporate environment is often based on experience\nand intuition, although internal data is usually available and can be a\nvaluable source of information. Predicting merger & acquisition (M&A) events is\nat the heart of strategic management, yet not sufficiently motivated by data\nanalytics driven controlling. One of the main obstacles in using e.g. count\ndata time series for M&A seems to be the fact that the intensity of M&A is time\nvarying at least in certain business sectors, e.g. communications. We propose a\nnew automatic procedure to bridge this obstacle using novel statistical\nmethods. The proposed approach allows for a selection of adaptive windows in\ncount data sets by detecting significant changes in the intensity of events. We\ntest the efficacy of the proposed method on a simulated count data set and put\nit into action on various M&A data sets. It is robust to aberrant behaviour and\ngenerates accurate forecasts for the evaluated business sectors. It also\nprovides guidance for an a-priori selection of fixed windows for forecasting.\nFurthermore, it can be generalized to other business lines, e.g. for managing\nsupply chains, sales forecasts, or call center arrivals, thus giving managers\nnew ways for incorporating statistical modeling in strategic planning\ndecisions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Dec 2020 14:08:20 GMT"}], "update_date": "2020-12-14", "authors_parsed": [["Khowaja", "Kainat", ""], ["Saef", "Danial", ""], ["Sizov", "Sergej", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2012.06715", "submitter": "Yishu Xue", "authors": "Guanyu Hu, Hou-Cheng Yang, Yishu Xue, Dipak K. Dey", "title": "Zero Inflated Poisson Model with Clustered Regression Coefficients: an\n  Application to Heterogeneity Learning of Field Goal Attempts of Professional\n  Basketball Players", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although basketball is a dynamic process sport, with 5 plus 5 players\ncompeting on both offense and defense simultaneously, learning some static\ninformation is predominant for professional players, coaches and team mangers.\nIn order to have a deep understanding of field goal attempts among different\nplayers, we propose a zero inflated Poisson model with clustered regression\ncoefficients to learn the shooting habits of different players over the court\nand the heterogeneity among them. Specifically, the zero inflated model\nrecovers the large proportion of the court with zero field goal attempts, and\nthe mixture of finite mixtures model learn the heterogeneity among different\nplayers based on clustered regression coefficients and inflated probabilities.\nBoth theoretical and empirical justification through simulation studies\nvalidate our proposed method. We apply our proposed model to the National\nBasketball Association (NBA), for learning players' shooting habits and\nheterogeneity among different players over the 2017--2018 regular season. This\nillustrates our model as a way of providing insights from different aspects.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 03:43:15 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 00:38:28 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hu", "Guanyu", ""], ["Yang", "Hou-Cheng", ""], ["Xue", "Yishu", ""], ["Dey", "Dipak K.", ""]]}, {"id": "2012.06819", "submitter": "Marco Antonio Aquino-L\\'opez", "authors": "Marco A Aquino-L\\'opez and Nicole K. Sanderson and Maarten Blaauw and\n  Joan-Albert Sanchez-Cabeza and Ana Carolina Ruiz-Fernandez and J Andr\\'es\n  Christen Marco A Aquino-L\\'opez, Nicole K. Sanderson, Maarten Blaauw,\n  Joan-Albert Sanchez-Cabeza, Ana Carolina Ruiz-Fernandez, J Andr\\'es Christen", "title": "A simulation study to compare 210Pb dating data analyses", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The increasing interest in understanding anthropogenic impacts on the\nenvironment have led to a considerable number of studies focusing on\nsedimentary records for the last $\\sim$ 100 - 200 years. Dating this period is\noften complicated by the poor resolution and large errors associated with\nradiocarbon (14C) ages, which is the most popular dating technique. To improve\nage-depth model resolution for the recent period, sediment dating with lead-210\n($^{210}$Pb) is widely used as it provides absolute and continuous dates for\nthe last $\\sim$ 100 - 150 years. The $^{210}$Pb dating method has traditionally\nrelied on the Constant Rate of Supply (CRS, also known as Constant Flux - CF)\nmodel which uses the radioactive decay equation as an age-depth relationship\nresulting in a restrictive model to approximate dates. In this work, we compare\nthe classical approach to $^{210}$Pb dating (CRS) and its Bayesian alternative\n(\\textit{Plum}). To do so, we created simulated $^{210}$Pb profiles following\nthree different sedimentation processes, complying with the assumptions imposed\nby the CRS model, and analysed them using both approaches. Results indicate\nthat the CRS model does not capture the true values even with a high dating\nresolution for the sediment, nor improves does its accuracy improve as more\ninformation is available. On the other hand, the Bayesian alternative\n(\\textit{Plum}) provides consistently more accurate results even with few\nsamples, and its accuracy and precision constantly improves as more information\nis available.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 13:49:37 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Aquino-L\u00f3pez", "Marco A", ""], ["Sanderson", "Nicole K.", ""], ["Blaauw", "Maarten", ""], ["Sanchez-Cabeza", "Joan-Albert", ""], ["Ruiz-Fernandez", "Ana Carolina", ""], ["Aquino-L\u00f3pez", "J Andr\u00e9s Christen Marco A", ""], ["Sanderson", "Nicole K.", ""], ["Blaauw", "Maarten", ""], ["Sanchez-Cabeza", "Joan-Albert", ""], ["Ruiz-Fernandez", "Ana Carolina", ""], ["Christen", "J Andr\u00e9s", ""]]}, {"id": "2012.06865", "submitter": "Falco J. Bargagli Stoffi", "authors": "Francesca Dominici and Falco J. Bargagli-Stoffi and Fabrizia Mealli", "title": "From controlled to undisciplined data: estimating causal effects in the\n  era of data science using a potential outcome framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses the fundamental principles of causal inference - the\narea of statistics that estimates the effect of specific occurrences,\ntreatments, interventions, and exposures on a given outcome from experimental\nand observational data. We explain the key assumptions required to identify\ncausal effects, and highlight the challenges associated with the use of\nobservational data. We emphasize that experimental thinking is crucial in\ncausal inference. The quality of the data (not necessarily the quantity), the\nstudy design, the degree to which the assumptions are met, and the rigor of the\nstatistical analysis allow us to credibly infer causal effects. Although we\nadvocate leveraging the use of big data and the application of machine learning\n(ML) algorithms for estimating causal effects, they are not a substitute of\nthoughtful study design. Concepts are illustrated via examples.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2020 17:15:46 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Dominici", "Francesca", ""], ["Bargagli-Stoffi", "Falco J.", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "2012.07117", "submitter": "Ogun Yurdakul", "authors": "Ogun Yurdakul, Andreas Meyer, Fikret Sivrikaya, and Sahin Albayrak", "title": "Forecasting Daily Primary Three-Hour Net Load Ramps in the CAISO System", "comments": "duck curve, long short-term memory (LSTM), net load, power system\n  flexibility, ramp forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deepening penetration of variable energy resources creates unprecedented\nchallenges for system operators (SOs). An issue that merits special attention\nis the precipitous net load ramps, which require SOs to have flexible capacity\nat their disposal so as to maintain the supply-demand balance at all times. In\nthe judicious procurement and deployment of flexible capacity, a tool that\nforecasts net load ramps may be of great assistance to SOs. To this end, we\npropose a methodology to forecast the magnitude and start time of daily primary\nthree-hour net load ramps. We perform an extensive analysis so as to identify\nthe factors that influence net load and draw on the identified factors to\ndevelop a forecasting methodology that harnesses the long short-term memory\nmodel. We demonstrate the effectiveness of the proposed methodology on the\nCAISO system using comparative assessments with selected benchmarks based on\nvarious evaluation metrics.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2020 17:55:03 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Yurdakul", "Ogun", ""], ["Meyer", "Andreas", ""], ["Sivrikaya", "Fikret", ""], ["Albayrak", "Sahin", ""]]}, {"id": "2012.07188", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein and Frederik Rehbach and Olaf Mersmann and Eva\n  Bartz", "title": "Hospital Capacity Planning Using Discrete Event Simulation Under Special\n  Consideration of the COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a resource-planning tool for hospitals under special consideration\nof the COVID-19 pandemic, called babsim.hospital. It provides many advantages\nfor crisis teams, e.g., comparison with their own local planning, simulation of\nlocal events, simulation of several scenarios (worst / best case). There are\nbenefits for medical professionals, e.g, analysis of the pandemic at local,\nregional, state and federal level, the consideration of special risk groups,\ntools for validating the length of stays and transition probabilities. Finally,\nthere are potential advantages for administration, management, e.g., assessment\nof the situation of individual hospitals taking local events into account,\nconsideration of relevant resources such as beds, ventilators, rooms,\nprotective clothing, and personnel planning, e.g., medical and nursing staff.\nbabsim.hospital combines simulation, optimization, statistics, and artificial\nintelligence processes in a very efficient way. The core is a discrete,\nevent-based simulation model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 00:17:26 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""], ["Rehbach", "Frederik", ""], ["Mersmann", "Olaf", ""], ["Bartz", "Eva", ""]]}, {"id": "2012.07190", "submitter": "Yuxuan Zhang Tgd", "authors": "Yuxuan Zhang (1,6), Chen Gong (2), Dawei Li (3), Zhi-Wei Wang (4,5),\n  Shengda D Pu (2), Alex W Robertson (2), Hong Yu (6), John Parrington (1) (1.\n  Department of Pharmacology, University of Oxford, Oxford, United Kingdom. 2.\n  Department of Materials, University of Oxford, Parks Road, Oxford, United\n  Kingdom. 3. Department of Physics, University of California, San Diego, La\n  Jolla, CA, United States. 4. Computer Science, University of York, York,\n  United Kingdom. 5. College of Physics, Jilin University, Changchun, People's\n  Republic of China. 6. Shanghai Chest Hospital, Shanghai Jiao Tong University,\n  Shanghai, People's Republic of China. Yuxuan Zhang, Chen Gong, and Dawei Li\n  contributed equally to this work.)", "title": "A prognostic dynamic model applicable to infectious diseases providing\n  easily visualized guides -- A case study of COVID-19 in the UK", "comments": "Errors appears in Results, data changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reasonable prediction of infectious diseases transmission process under\ndifferent disease control strategies is an important reference point for policy\nmakers. Here we established a dynamic transmission model via Python and\nrealized comprehensive regulation of disease control measures. We classified\ngovernment interventions into three categories and introduced three parameters\nas descriptions for the key points in disease control, these being\nintraregional growth rate, interregional communication rate, and detection rate\nof infectors. Our simulation predicts the infection by COVID-19 in the UK would\nbe out of control in 73 days without any interventions; at the same time, herd\nimmunity acquisition will begin from the epicentre. After we introduced\ngovernment interventions, single intervention is effective in disease control\nbut at huge expense while combined interventions would be more efficient, among\nwhich, enhancing detection number is crucial in control strategy of COVID-19.\nIn addition, we calculated requirements for the most effective vaccination\nstrategy based on infection number in real situation. Our model was programmed\nwith iterative algorithms, and visualized via cellular automata, it can be\napplied to similar epidemics in other regions if the basic parameters are\ninputted, and is able to synthetically mimick the effect of multiple factors in\ninfectious disease control.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 00:26:25 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 11:31:19 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zhang", "Yuxuan", ""], ["Gong", "Chen", ""], ["Li", "Dawei", ""], ["Wang", "Zhi-Wei", ""], ["Pu", "Shengda D", ""], ["Robertson", "Alex W", ""], ["Yu", "Hong", ""], ["Parrington", "John", ""]]}, {"id": "2012.07224", "submitter": "Brian Mark", "authors": "Hanoch Lev-Ari, Yariv Ephraim, Brian L. Mark", "title": "Traffic Rate Network Tomography with Higher-Order Cumulants", "comments": "10 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PF cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network tomography aims at estimating source-destination traffic rates from\nlink traffic measurements. This inverse problem was formulated by Vardi in 1996\nfor Poisson traffic over networks operating under deterministic as well as\nrandom routing regimes. In this paper we expand Vardi's second-order moment\nmatching rate estimation approach to higher-order cumulant matching with the\ngoal of increasing the column rank of the mapping and consequently improving\nthe rate estimation accuracy. We develop a systematic set of linear cumulant\nmatching equations and express them compactly in terms of the Khatri-Rao\nproduct. Both least squares estimation and iterative minimum I-divergence\nestimation are considered. We develop an upper bound on the mean squared error\n(MSE) in least squares rate estimation from empirical cumulants. We demonstrate\nfor the NSFnet that supplementing Vardi's approach with third-order empirical\ncumulant reduces its averaged normalized MSE relative to the theoretical\nminimum of the second-order moment matching approach by about 12%-18%. This\nminimum MSE is obtained when Vardi's second-order moment matching approach is\nbased on the theoretical rather than the empirical moments.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 02:51:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lev-Ari", "Hanoch", ""], ["Ephraim", "Yariv", ""], ["Mark", "Brian L.", ""]]}, {"id": "2012.07278", "submitter": "Jean Feng", "authors": "Jean Feng", "title": "Learning how to approve updates to machine learning algorithms in\n  non-stationary settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning algorithms in healthcare have the potential to continually\nlearn from real-world data generated during healthcare delivery and adapt to\ndataset shifts. As such, the FDA is looking to design policies that can\nautonomously approve modifications to machine learning algorithms while\nmaintaining or improving the safety and effectiveness of the deployed models.\nHowever, selecting a fixed approval strategy, a priori, can be difficult\nbecause its performance depends on the stationarity of the data and the quality\nof the proposed modifications. To this end, we investigate a\nlearning-to-approve approach (L2A) that uses accumulating monitoring data to\nlearn how to approve modifications. L2A defines a family of strategies that\nvary in their \"optimism''---where more optimistic policies have faster approval\nrates---and searches over this family using an exponentially weighted average\nforecaster. To control the cumulative risk of the deployed model, we give L2A\nthe option to abstain from making a prediction and incur some fixed abstention\ncost instead. We derive bounds on the average risk of the model deployed by\nL2A, assuming the distributional shifts are smooth. In simulation studies and\nempirical analyses, L2A tailors the level of optimism for each problem-setting:\nIt learns to abstain when performance drops are common and approve beneficial\nmodifications quickly when the distribution is stable.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 05:54:55 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Feng", "Jean", ""]]}, {"id": "2012.07485", "submitter": "Jai-Hua Yen", "authors": "Jai-Hua Yen, Chun-Huo Chiu", "title": "Richness estimation with species identity error", "comments": "6 pages, ISI WSC 2019 conference", "journal-ref": "Proceedings 62th International Statistical Institute (ISI) World\n  Statistics Congress, Volume 6, 401-408 (2020)", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Richness estimation of an interesting area is always a challenge statistical\nwork due to small sample size or species identity error. In the literatures,\nmost richness estimators were only proposed to tackle the underestimation of\nthe size-limited sample. However, species identity error almost occurs in each\nspecies survey and seriously reduces the accuracy of observed, singleton, and\ndoubleton richness in turns to influence the behavior of richness estimator.\nTherefore, to estimate the true richness, the biased collected data due to\nspecies identity error should be modified before processing the richness\nestimation work. In the manuscript, we propose a new approach to correct the\nbias of richness estimation due to species identity error. First, a species\nlist inventory from a subplot obtained by the investigator was used to estimate\nthe species identity error rate. Then, we can correct the biased observed,\nsingleton, and doubleton richness of the raw sampling data from the interesting\narea. Finally, the richness estimators proposed in the literature could be\nsupplied to get the more correct estimates based on adjusted observed data. To\ninvestigate the behavior of the proposed method, we performed simulations by\ngenerating data sets from various species models with different species\nidentity error rates. For the purpose of illustration, the real data was\nsupplied to demonstrate our proposed approach. A presence/absence weeds species\nwas surveyed in the organic farmland located at Soft Bridge County in the North\nof Taiwan.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:04:13 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 13:04:46 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Yen", "Jai-Hua", ""], ["Chiu", "Chun-Huo", ""]]}, {"id": "2012.07487", "submitter": "Yamila Barrera", "authors": "Yamila Barrera, Leonardo Boechi, Matthieu Jonckheere, Vincent Lefieux,\n  Dominique Picard, Ezequiel Smucler, Agustin Somacal, Alfredo Umfurer", "title": "Clustering high dimensional meteorological scenarios: results and\n  performance index", "comments": "19 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reseau de Transport d'Electricit\\'e (RTE) is the French main electricity\nnetwork operational manager and dedicates large number of resources and efforts\ntowards understanding climate time series data. We discuss here the problem and\nthe methodology of grouping and selecting representatives of possible climate\nscenarios among a large number of climate simulations provided by RTE. The data\nused is composed of temperature times series for 200 different possible\nscenarios on a grid of geographical locations in France. These should be\nclustered in order to detect common patterns regarding temperatures curves and\nhelp to choose representative scenarios for network simulations, which in turn\ncan be used for energy optimisation. We first show that the choice of the\ndistance used for the clustering has a strong impact on the meaning of the\nresults: depending on the type of distance used, either spatial or temporal\npatterns prevail. Then we discuss the difficulty of fine-tuning the distance\nchoice (combined with a dimension reduction procedure) and we propose a\nmethodology based on a carefully designed index.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 13:06:41 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Barrera", "Yamila", ""], ["Boechi", "Leonardo", ""], ["Jonckheere", "Matthieu", ""], ["Lefieux", "Vincent", ""], ["Picard", "Dominique", ""], ["Smucler", "Ezequiel", ""], ["Somacal", "Agustin", ""], ["Umfurer", "Alfredo", ""]]}, {"id": "2012.07574", "submitter": "James Walsh", "authors": "Chance Haycock, Edward Thorpe-Woods, James Walsh, Patrick O'Hara,\n  Oscar Giles, Neil Dhir, Theodoros Damoulas", "title": "An Expectation-Based Network Scan Statistic for a COVID-19 Early Warning\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the Greater London Authority's (GLA) response to the COVID-19 pandemic\nbrings together multiple large-scale and heterogeneous datasets capturing\nmobility, transportation and traffic activity over the city of London to better\nunderstand 'busyness' and enable targeted interventions and effective\npolicy-making. As part of Project Odysseus we describe an early-warning system\nand introduce an expectation-based scan statistic for networks to help the GLA\nand Transport for London, understand the extent to which populations are\nfollowing government COVID-19 guidelines. We explicitly treat the case of\ngeographically fixed time-series data located on a (road) network and primarily\nfocus on monitoring the dynamics across large regions of the capital.\nAdditionally, we also focus on the detection and reporting of significant\nspatio-temporal regions. Our approach is extending the Network Based Scan\nStatistic (NBSS) by making it expectation-based (EBP) and by using stochastic\nprocesses for time-series forecasting, which enables us to quantify metric\nuncertainty in both the EBP and NBSS frameworks. We introduce a variant of the\nmetric used in the EBP model which focuses on identifying space-time regions in\nwhich activity is quieter than expected.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2020 19:35:17 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Haycock", "Chance", ""], ["Thorpe-Woods", "Edward", ""], ["Walsh", "James", ""], ["O'Hara", "Patrick", ""], ["Giles", "Oscar", ""], ["Dhir", "Neil", ""], ["Damoulas", "Theodoros", ""]]}, {"id": "2012.07601", "submitter": "Atul Pokharel", "authors": "Atul Pokharel (1), Robert Soul\\'e (2), Avi Silberschatz (2) ((1) New\n  York University, (2) Yale University)", "title": "A case for location based contact tracing", "comments": "14 pages, 7 figures, submitted to Healthcare Management Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an evaluation of the effectiveness of manual contact tracing\ncompared to bulletin board contact tracing. We show that bulletin board contact\ntracing gives comparable results in terms of the reproductive number, duration,\nprevalence and incidence but is less resource intensive, easier to implement\nand offers a wider range of privacy options. Classical contact tracing focuses\non contacting individuals whom an infectious person has been in proximity to. A\nbulletin board approach focuses on identifying locations visited by an\ninfectious person, and then contacting those who were at those locations. We\npresent results comparing their effects on the overall reproductive number as\nwell as the incidence and prevalence of disease. We evaluate them by building a\nnew discrete time stochastic model based on the Susceptible Exposed Infectious\nand Recovered (SEIR) framework for disease spread. We conduct simulation\nexperiments to quantify the effectiveness of these two models of contact\ntracing by calibrating the model to be compatible with SARS-CoV-2. Our\nexperiments show that location-based bulletin board contact tracing can improve\nmanual contact tracing.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2020 17:19:48 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Pokharel", "Atul", ""], ["Soul\u00e9", "Robert", ""], ["Silberschatz", "Avi", ""]]}, {"id": "2012.07824", "submitter": "Marcos Peres", "authors": "Marcos Vinicius de Oliveira Peres, Ricardo Puziol de Oliveira, Jorge\n  Alberto Achcar and Edson Zangiacomi Martinez", "title": "The Bivariate Defective Gompertz Distribution Based on Clayton Copula\n  with Applications to Medical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical studies, it is common the presence of a fraction of patients who\ndo not experience the event of interest. These patients are people who are not\nat risk of the event or are patients who were cured during the research. The\nproportion of immune or cured patients is known in the literature as cure rate.\nIn general, the traditional existing lifetime statistical models are not\nappropriate to model data sets with cure rate, including bivariate lifetimes.\nIn this paper, it is proposed a bivariate model based on a defective Gompertz\ndistribution and also using a Clayton copula function to capture the possible\ndependence structure between the lifetimes. An extensive simulation study was\ncarried out in order to evaluate the biases and the mean squared errors for the\nmaximum likelihood estimators of the parameters associated to the proposed\ndistribution. Some applications using medical data are presented to show the\nusefulness of the proposed model.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 18:57:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Peres", "Marcos Vinicius de Oliveira", ""], ["de Oliveira", "Ricardo Puziol", ""], ["Achcar", "Jorge Alberto", ""], ["Martinez", "Edson Zangiacomi", ""]]}, {"id": "2012.07915", "submitter": "Yili Hong", "authors": "Li Xu, Thomas Lux, Tyler Chang, Bo Li, Yili Hong, Layne Watson, Ali\n  Butt, Danfeng Yao, and Kirk Cameron", "title": "Prediction of High-Performance Computing Input/Output Variability and\n  Its Application to Optimization for System Configurations", "comments": "29 pages, 8 figures", "journal-ref": "Quality Engineering, 2021", "doi": null, "report-no": null, "categories": "cs.DC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance variability is an important measure for a reliable high\nperformance computing (HPC) system. Performance variability is affected by\ncomplicated interactions between numerous factors, such as CPU frequency, the\nnumber of input/output (IO) threads, and the IO scheduler. In this paper, we\nfocus on HPC IO variability. The prediction of HPC variability is a challenging\nproblem in the engineering of HPC systems and there is little statistical work\non this problem to date. Although there are many methods available in the\ncomputer experiment literature, the applicability of existing methods to HPC\nperformance variability needs investigation, especially, when the objective is\nto predict performance variability both in interpolation and extrapolation\nsettings. A data analytic framework is developed to model data collected from\nlarge-scale experiments. Various promising methods are used to build predictive\nmodels for the variability of HPC systems. We evaluate the performance of the\nmethods by measuring prediction accuracy at previously unseen system\nconfigurations. We also discuss a methodology for optimizing system\nconfigurations that uses the estimated variability map. The findings from\nmethod comparisons and developed tool sets in this paper yield new insights\ninto existing statistical methods and can be beneficial for the practice of HPC\nvariability management. This paper has supplementary materials online.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 19:56:52 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Xu", "Li", ""], ["Lux", "Thomas", ""], ["Chang", "Tyler", ""], ["Li", "Bo", ""], ["Hong", "Yili", ""], ["Watson", "Layne", ""], ["Butt", "Ali", ""], ["Yao", "Danfeng", ""], ["Cameron", "Kirk", ""]]}, {"id": "2012.07921", "submitter": "Johannes Breidenbach", "authors": "Johannes Breidenbach, Janis Ivanovs, Annika Kangas, Thomas\n  Nord-Larsen, Mats Nilson, Rasmus Astrup", "title": "Improving living biomass C-stock loss estimates by combining optical\n  satellite, airborne laser scanning, and NFI data", "comments": "30 pages, 6 figures. Accepted to Canadian Journal of Forest Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy measures and management decisions aiming at enhancing the role of\nforests in mitigating climate-change require reliable estimates of C-stock\ndynamics in greenhouse gas inventories (GHGIs). Aim of this study was to\nassemble design-based estimators to provide estimates relevant for GHGIs using\nnational forest inventory (NFI) data. We improve basic expansion (BE) estimates\nof living-biomass C-stock loss using field-data only, by leveraging with\nremotely-sensed auxiliary data in model-assisted (MA) estimates. Our case\nstudies from Norway, Sweden, Denmark, and Latvia covered an area of >70 Mha.\nLandsat-based Forest Cover Loss (FCL) and one-time wall-to-wall airborne laser\nscanning (ALS) data served as auxiliary data. ALS provided information on the\nC-stock before a potential disturbance indicated by FCL. The use of FCL in MA\nestimators resulted in considerable efficiency gains which in most cases were\nfurther increased by using ALS in addition. A doubling of efficiency was\npossible for national estimates and even larger efficiencies were observed at\nthe sub-national level. Average annual estimates were considerably more precise\nthan pooled estimates using NFI data from all years at once. The combination of\nremotely-sensed with NFI field data yields reliable estimates which is not\nnecessarily the case when using remotely-sensed data without reference\nobservations.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2020 20:16:42 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 19:05:06 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Breidenbach", "Johannes", ""], ["Ivanovs", "Janis", ""], ["Kangas", "Annika", ""], ["Nord-Larsen", "Thomas", ""], ["Nilson", "Mats", ""], ["Astrup", "Rasmus", ""]]}, {"id": "2012.07986", "submitter": "Gustau Camps-Valls", "authors": "Gustau Camps-Valls, Luca Martino, Daniel H. Svendsen, Manuel\n  Campos-Taberner, Jordi Mu\\~noz-Mar\\'i, Valero Laparra, David Luengo,\n  Francisco Javier Garc\\'ia-Haro", "title": "Physics-Aware Gaussian Processes in Remote Sensing", "comments": null, "journal-ref": "Applied Soft Computing Volume 68, July 2018, Pages 69-82", "doi": "10.1016/j.asoc.2018.03.021", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Earth observation from satellite sensory data poses challenging problems,\nwhere machine learning is currently a key player. In recent years, Gaussian\nProcess (GP) regression has excelled in biophysical parameter estimation tasks\nfrom airborne and satellite observations. GP regression is based on solid\nBayesian statistics and generally yields efficient and accurate parameter\nestimates. However, GPs are typically used for inverse modeling based on\nconcurrent observations and in situ measurements only. Very often a forward\nmodel encoding the well-understood physical relations between the state vector\nand the radiance observations is available though and could be useful to\nimprove predictions and understanding. In this work, we review three GP models\nthat respect and learn the physics of the underlying processes in the context\nof both forward and inverse modeling. After reviewing the traditional\napplication of GPs for parameter retrieval, we introduce a Joint GP (JGP) model\nthat combines in situ measurements and simulated data in a single GP model.\nThen, we present a latent force model (LFM) for GP modeling that encodes\nordinary differential equations to blend data-driven modeling and physical\nconstraints of the system governing equations. The LFM performs multi-output\nregression, adapts to the signal characteristics, is able to cope with missing\ndata in the time series, and provides explicit latent functions that allow\nsystem analysis and evaluation. Finally, we present an Automatic Gaussian\nProcess Emulator (AGAPE) that approximates the forward physical model using\nconcepts from Bayesian optimization and at the same time builds an optimally\ncompact look-up-table for inversion. We give empirical evidence of the\nperformance of these models through illustrative examples of vegetation\nmonitoring and atmospheric modeling.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 11:44:11 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Camps-Valls", "Gustau", ""], ["Martino", "Luca", ""], ["Svendsen", "Daniel H.", ""], ["Campos-Taberner", "Manuel", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Laparra", "Valero", ""], ["Luengo", "David", ""], ["Garc\u00eda-Haro", "Francisco Javier", ""]]}, {"id": "2012.08028", "submitter": "Zhiqing Yang", "authors": "Zhiqing Yang, Youngjun Choe, Matthew Martell", "title": "COVID-19 Economic Policy Effects on Consumer Spending and Foot Traffic\n  in the U.S", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To battle with economic challenges during the COVID-19 pandemic, the US\ngovernment implemented various measures to mitigate economic loss. From\nissuance of stimulus checks to reopening businesses, consumers had to\nconstantly alter their behavior in response to government policies. Using\nanonymized card transactions and mobile device-based location tracking data, we\nanalyze the factors that contribute to these behavior changes, focusing on\nstimulus check issuance and state-wide reopening. Our finding suggests that\nstimulus payment has a significant immediate effect of boosting spending, but\nit typically does not reverse a downward trend. State-wide reopening had a\nsmall effect on spending. Foot traffic increased gradually after stimulus check\nissuance, but only increased slightly after reopening, which also coincided or\npreceded several policy changes and confounding events (e.g., protests) in the\nUS. We also find differences in the reaction to these policies in different\nregions in the US. Our results may be used to inform future economic recovery\npolicies and their potential consumer response.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 01:20:09 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 01:34:22 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yang", "Zhiqing", ""], ["Choe", "Youngjun", ""], ["Martell", "Matthew", ""]]}, {"id": "2012.08036", "submitter": "Marius Hofert", "authors": "Marius Hofert, Avinash Prasad, Mu Zhu", "title": "Applications of multivariate quasi-random sampling with neural networks", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching networks (GMMNs) are suggested for modeling the\ncross-sectional dependence between stochastic processes. The stochastic\nprocesses considered are geometric Brownian motions and ARMA-GARCH models.\nGeometric Brownian motions lead to an application of pricing American basket\ncall options under dependence and ARMA-GARCH models lead to an application of\nsimulating predictive distributions. In both types of applications the benefit\nof using GMMNs in comparison to parametric dependence models is highlighted and\nthe fact that GMMNs can produce dependent quasi-random samples with no\nadditional effort is exploited to obtain variance reduction.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 01:42:23 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Hofert", "Marius", ""], ["Prasad", "Avinash", ""], ["Zhu", "Mu", ""]]}, {"id": "2012.08089", "submitter": "Pratishtha Batra", "authors": "Pratishtha Batra, Neil A. Spencer and Pritam Ranjan", "title": "IsoCheck: An R Package to check Isomorphism for Two-level Factorial\n  Designs with Randomization Restrictions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factorial designs are often used in various industrial and sociological\nexperiments to identify significant factors and factor combinations that may\naffect the process response. In the statistics literature, several studies have\ninvestigated the analysis, construction, and isomorphism of factorial and\nfractional factorial designs. When there are multiple choices for a design, it\nis helpful to have an easy-to-use tool for identifying which are distinct, and\nwhich of those can be efficiently analyzed/has good theoretical properties. For\nthis task, we present an R library called IsoCheck that checks the isomorphism\nof multi-stage 2^n factorial experiments with randomization restrictions.\nThrough representing the factors and their combinations as a finite projective\ngeometry, IsoCheck recasts the problem of searching over all possible\nrelabelings as a search over collineations, then exploits projective geometric\nproperties of the space to make the search much more efficient. Furthermore, a\nbitstring representation of the factorial effects is used to characterize all\npossible rearrangements of designs, thus facilitating quick comparisons after\nrelabeling. We present several examples with R code to illustrate the usage of\nthe main functions in IsoCheck. Besides checking equivalence and isomorphism of\n2^n multi-stage factorial designs, we demonstrate how the functions of the\npackage can be used to create a catalog of all non-isomorphic designs, and\nsubsequently rank these designs based on a suitably defined ranking criterion.\nIsoCheck is free software and distributed under the General Public License and\navailable from the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 04:55:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Batra", "Pratishtha", ""], ["Spencer", "Neil A.", ""], ["Ranjan", "Pritam", ""]]}, {"id": "2012.08235", "submitter": "Gabriel Turinici", "authors": "Gabriel Turinici", "title": "Architectures of epidemic models: accommodating constraints from\n  empirical and clinical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.NA math.NA q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deterministic compartmental models have been used extensively in modeling\nepidemic propagation. These models are required to fit available data and\nnumerical procedures are often implemented to this end. But not every model\narchitecture is able to fit the data because the structure of the model imposes\nhard constraints on the solutions. We investigate in this work two such\nsituations: first the distribution of transition times from a compartment to\nanother may impose a variable number of intermediary states; secondly, a\nnon-linear relationship between time-dependent measures of compartments sizes\nmay indicate the need for structurations (i.e., considering several groups of\nindividuals of heterogeneous characteristics).\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 11:59:38 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Turinici", "Gabriel", ""]]}, {"id": "2012.08246", "submitter": "Cornelius Fritz", "authors": "Cornelius Fritz, Marius Mehrl, Paul W. Thurner, G\\\"oran Kauermann", "title": "The Role of Governmental Weapons Procurements in Forecasting Monthly\n  Fatalities in Intrastate Conflicts: A Semiparametric Hierarchical Hurdle\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate and interpretable forecasting models predicting spatially and\ntemporally fine-grained changes in the numbers of intrastate conflict\ncasualties are of crucial importance for policymakers and international\nnon-governmental organisations (NGOs). Using a count data approach, we propose\na hierarchical hurdle regression model to address the corresponding prediction\nchallenge at the monthly PRIO-grid level. More precisely, we model the\nintensity of local armed conflict at a specific point in time as a three-stage\nprocess. Stages one and two of our approach estimate whether we will observe\nany casualties at the country- and grid-cell-level, respectively, while stage\nthree applies a regression model for truncated data to predict the number of\nsuch fatalities conditional upon the previous two stages. Within this modelling\nframework, we focus on the role of governmental arms imports as a processual\nfactor allowing governments to intensify or deter from fighting. We further\nargue that a grid cell's geographic remoteness is bound to moderate the effects\nof these military buildups. Out-of-sample predictions corroborate the\neffectiveness of our parsimonious and theory-driven model, which enables full\ntransparency combined with accuracy in the forecasting process.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 12:25:55 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Fritz", "Cornelius", ""], ["Mehrl", "Marius", ""], ["Thurner", "Paul W.", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2012.08397", "submitter": "Yunyi Shen", "authors": "Yunyi Shen and Claudia Solis-Lemus", "title": "Bayesian Conditional Auto-Regressive LASSO Models to Learn Sparse\n  Microbial Networks with Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Microbiome data analyses require statistical models that can simultaneously\ndecode microbes' reactions to the environment and interactions among microbes.\nWhile a multiresponse linear regression model seems like a straightforward\nsolution, we argue that treating it as a graphical model is flawed given that\nthe regression coefficient matrix does not encode the conditional dependence\nstructure between response and predictor nodes because it does not represent\nthe adjacency matrix. This observation is especially important in biological\nsettings when we have prior knowledge on the edges from specific experimental\ninterventions that can only be properly encoded under a conditional dependence\nmodel. Here, we propose a chain graph model with two sets of nodes (predictors\nand responses) whose solution yields a graph with edges that indeed represent\nconditional dependence and thus, agrees with the experimenter's intuition on\nthe average behavior of nodes under treatment. The solution to our model is\nsparse via Bayesian LASSO and is also guaranteed to be the sparse solution to a\nConditional Auto-Regressive (CAR) model. In addition, we propose an adaptive\nextension so that different shrinkage can be applied to different edges to\nincorporate edge-specific prior knowledge. Our model is computationally\ninexpensive through an efficient Gibbs sampling algorithm and can account for\nbinary, counting, and compositional responses via appropriate hierarchical\nstructure. We apply our model to a human gut and a soil microbial compositional\ndatasets and we highlight that CAR-LASSO can estimate biologically meaningful\nnetwork structures in the data. The CAR-LASSO software is available as an R\npackage at https://github.com/YunyiShen/CAR-LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 16:12:54 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2021 23:02:19 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2021 22:04:03 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 06:54:30 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 15:19:42 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Shen", "Yunyi", ""], ["Solis-Lemus", "Claudia", ""]]}, {"id": "2012.08453", "submitter": "Soumaila Dembele", "authors": "Tagbo Innocent Aroh, Ousman Saine, Soumaila Demb\\'el\\'e, Gane Samb Lo", "title": "A Supervised Hybrid Statistical Catch-up System Built on Gabece Gambian\n  Data", "comments": "34 pages", "journal-ref": null, "doi": "10.16929/ajas/2020.829.244", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we want to find a statistical rule that assigns a passing or\nfailing grade to students who undertook at least three exams out of four in a\nnational exam, instead of completely dismissing them students. While it is\ncruel to declare them as failing, especially if the reason for their absence it\nnot intentional, they should have demonstrated enough merit in the three exams\ntaken to deserve a chance to be declared passing. We use a special\nclassification method and nearest neighbors methods based on the average grade\nand on the most modal grade to build a statistical rule in a supervised\nlearning process. The study is built on the national GABECE educational data\nwhich is a considerable data covering seven years and all the six regions of\nthe Gambia.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 17:46:11 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Aroh", "Tagbo Innocent", ""], ["Saine", "Ousman", ""], ["Demb\u00e9l\u00e9", "Soumaila", ""], ["Lo", "Gane Samb", ""]]}, {"id": "2012.08488", "submitter": "Cheikh Haidara Mohamed", "authors": "Ousmane Saine, Soumaila Demb\\'el\\'e, Gane Samb Lo, Mohamed Cheikh\n  Haidara", "title": "Overview description of the Gambian GABECE Educational Data and\n  associated algorithms and unsupervized learning process", "comments": "23 pages; 5 figures", "journal-ref": null, "doi": "10.16929/ajas/2020.451.245", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As the first paper of a series of exploratory analysis and statistical\ninvestigation works on the Gambian \\textit{GABECE} data based on a variety of\nstatistical tools, we wish to begin with a thorough unsupervised learning\nprocess through descriptive and exploratory methods. This will lead to a\nvariety of discoveries and hypotheses that will direct future research works\nrelated to this data.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 18:33:42 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Saine", "Ousmane", ""], ["Demb\u00e9l\u00e9", "Soumaila", ""], ["Lo", "Gane Samb", ""], ["Haidara", "Mohamed Cheikh", ""]]}, {"id": "2012.08591", "submitter": "Brian Karrer", "authors": "Brian Karrer, Liang Shi, Monica Bhole, Matt Goldman, Tyrone Palmer,\n  Charlie Gelman, Mikael Konutgan, Feng Sun", "title": "Network experimentation at scale", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe our framework, deployed at Facebook, that accounts for\ninterference between experimental units through cluster-randomized experiments.\nWe document this system, including the design and estimation procedures, and\ndetail insights we have gained from the many experiments that have used this\nsystem at scale. We introduce a cluster-based regression adjustment that\nsubstantially improves precision for estimating global treatment effects as\nwell as testing for interference as part of our estimation procedure. With this\nregression adjustment, we find that imbalanced clusters can better account for\ninterference than balanced clusters without sacrificing accuracy. In addition,\nwe show how logging exposure to a treatment can be used for additional variance\nreduction. Interference is a widely acknowledged issue with online field\nexperiments, yet there is less evidence from real-world experiments\ndemonstrating interference in online settings. We fill this gap by describing\ntwo case studies that capture significant network effects and highlight the\nvalue of this experimentation framework.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 20:02:19 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Karrer", "Brian", ""], ["Shi", "Liang", ""], ["Bhole", "Monica", ""], ["Goldman", "Matt", ""], ["Palmer", "Tyrone", ""], ["Gelman", "Charlie", ""], ["Konutgan", "Mikael", ""], ["Sun", "Feng", ""]]}, {"id": "2012.08640", "submitter": "Gustau Camps-Valls", "authors": "Jochem Verrelst, Juan Pablo Rivera, Anatoly Gitelson, Jesus Delegido,\n  Jos\\'e Moreno, Gustau Camps-Valls", "title": "Spectral band selection for vegetation properties retrieval using\n  Gaussian processes regression", "comments": null, "journal-ref": "International Journal of Applied Earth Observation and\n  Geoinformation Volume 52, October 2016, Pages 554-567", "doi": "10.1016/j.jag.2016.07.016", "report-no": null, "categories": "cs.CV eess.IV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With current and upcoming imaging spectrometers, automated band analysis\ntechniques are needed to enable efficient identification of most informative\nbands to facilitate optimized processing of spectral data into estimates of\nbiophysical variables. This paper introduces an automated spectral band\nanalysis tool (BAT) based on Gaussian processes regression (GPR) for the\nspectral analysis of vegetation properties. The GPR-BAT procedure sequentially\nbackwards removes the least contributing band in the regression model for a\ngiven variable until only one band is kept. GPR-BAT is implemented within the\nframework of the free ARTMO's MLRA (machine learning regression algorithms)\ntoolbox, which is dedicated to the transforming of optical remote sensing\nimages into biophysical products. GPR-BAT allows (1) to identify the most\ninformative bands in relating spectral data to a biophysical variable, and (2)\nto find the least number of bands that preserve optimized accurate predictions.\nThis study concludes that a wise band selection of hyperspectral data is\nstrictly required for optimal vegetation properties mapping.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 09:28:33 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Verrelst", "Jochem", ""], ["Rivera", "Juan Pablo", ""], ["Gitelson", "Anatoly", ""], ["Delegido", "Jesus", ""], ["Moreno", "Jos\u00e9", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.08652", "submitter": "Xu Liang", "authors": "German A. Villalba, Xu Liang, and Yao Liang", "title": "Selection of multiple donor gauges via Graphical Lasso for estimation of\n  daily streamflow time series", "comments": "arXiv admin note: substantial text overlap with arXiv:2004.01373", "journal-ref": null, "doi": "10.1029/2020WR028936", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in estimations of daily streamflow time series at\nsites with incomplete records is how to effectively and efficiently select\nreference or donor gauges from an existing gauge network to infer the missing\ndata. While research on estimating missing streamflow time series is not new,\nthe existing approaches either use a single reference streamflow gauge or\nemploy a set of \"ad-hoc\" reference gauges, leaving a systematic selection of\nreference gauges as a long-standing open question. In this work, a novel method\nis introduced that facilitates systematical selection of multiple reference\ngauges from any given streamflow network. The idea is to mathematically\ncharacterize the network-wise correlation structure of a streamflow network via\ngraphical Markov modeling, and further transforms a dense network into a\nsparsely connected one. The resulted underlying sparse graph from the graphical\nmodel encodes conditional independence conditions among all reference gauges\nfrom the streamflow network, allowing determination of an optimum subset of the\ndonor gauges. The sparsity is discovered by using the Graphical Lasso algorithm\nwith an L1-norm regularization parameter and a thresholding parameter. These\ntwo parameters are determined by a multi-objective optimization process.\nFurthermore, the graphical modeling approach is employed to solve another open\nproblem in gauge removal planning decision (e.g., due to operation budget\nconstraints): which gauges to remove would statistically guarantee the least\nloss of information by estimations from the remaining gauges? Our graphical\nmodel-based method is demonstrated with daily streamflow data from a network of\n34 gauges over the Ohio River basin.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2020 22:30:35 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 04:08:21 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Villalba", "German A.", ""], ["Liang", "Xu", ""], ["Liang", "Yao", ""]]}, {"id": "2012.08724", "submitter": "Min Liu", "authors": "Min Liu, Jialiang Mao, Kang Kang", "title": "Trustworthy Online Marketplace Experimentation with Budget-split Design", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experimentation, also known as A/B testing, is the gold standard for\nmeasuring product impacts and making business decisions in the tech industry.\nThe validity and utility of experiments, however, hinge on unbiasedness and\nsufficient power. In two-sided online marketplaces, both requirements are\ncalled into question. The Bernoulli randomized experiments are biased because\ntreatment units interfere with control units through market competition and\nviolate the \"stable unit treatment value assumption\"(SUTVA). The experimental\npower on at least one side of the market is often insufficient because of\ndisparate sample sizes on the two sides. Despite the important of online\nmarketplaces to the online economy and the crucial role experimentation plays\nin product improvement, there lacks an effective and practical solution to the\nbias and low power problems in marketplace experimentation. Our paper fills\nthis gap by proposing an experimental design that is unbiased in any\nmarketplace where buyers have a defined budget, which could be finite or\ninfinite. We show that it is more powerful than all other unbiased designs in\nliterature. We then provide generalizable system architecture for deploying\nthis design to online marketplaces. Finally, we confirm our findings with\nempirical performance from experiments run in two real-world online\nmarketplaces.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 03:48:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Liu", "Min", ""], ["Mao", "Jialiang", ""], ["Kang", "Kang", ""]]}, {"id": "2012.08910", "submitter": "Amandine Pierrot", "authors": "Amandine Pierrot, Pierre Pinson", "title": "Adaptive Generalized Logit-Normal Distributions for Wind Power\n  Short-Term Forecasting", "comments": "6 pages, 6 figures, submitted to the 14th IEEE PowerTech 2021\n  Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is increasing interest in very short-term and higher-resolution wind\npower forecasting (from minutes to hours ahead), especially offshore.\nStatistical methods are of utmost relevance, since weather forecasts cannot be\ninformative for those lead times. Those approaches ought to account for the\nfact that wind power generation as a stochastic process is nonstationary,\ndouble-bounded (by zero and the nominal power of the turbine) and non-linear.\nAccommodating those aspects may lead to improving both point and probabilistic\nforecasts. We propose here to focus on generalized logit-normal distributions,\nwhich are naturally suitable and flexible for double-bounded and non-linear\nprocesses. Relevant parameters are estimated via maximum likelihood inference.\nBoth batch and online versions of the estimation approach are described -- the\nonline version permitting to additionally handle non-stationarity through the\nvariation of distribution parameters. The approach is applied and analysed on\nthe test case of the Anholt offshore wind farm in Denmark, with emphasis placed\non 10-min-ahead point and probabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 12:41:17 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 09:27:31 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Pierrot", "Amandine", ""], ["Pinson", "Pierre", ""]]}, {"id": "2012.09096", "submitter": "Bastien Mallein", "authors": "Emilien Joly and Bastien Mallein", "title": "A tractable non-adaptative group testing method for non-binary\n  measurements", "comments": "16 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original problem of group testing consists in the identification of\ndefective items in a collection, by applying tests on groups of items that\ndetect the presence of at least one defective item in the group. The aim is\nthen to identify all defective items of the collection with as few tests as\npossible. This problem is relevant in several fields, among which biology and\ncomputer sciences. In the present article we consider that the tests applied to\ngroups of items returns a \\emph{load}, measuring how defective the most\ndefective item of the group is. In this setting, we propose a simple\nnon-adaptative algorithm allowing the detection of all defective items of the\ncollection. This method improves on classical group testing algorithms using\nonly the binary response of the test.\n  Group testing recently gained attraction as a potential tool to solve a\nshortage of COVID-19 test kits, in particular for RT-qPCR. These tests return\nthe viral load of the sample and the viral load varies greatly among\nindividuals. Therefore our model presents some of the key features of this\nproblem. We aim at using the extra piece of information that represents the\nviral load to construct a one-stage pool testing algorithm on this idealized\nversion. We show that under the right conditions, the total number of tests\nneeded to detect contaminated samples can be drastically diminished.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 17:40:30 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:21:34 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Joly", "Emilien", ""], ["Mallein", "Bastien", ""]]}, {"id": "2012.09258", "submitter": "Samuel Ackerman", "authors": "Samuel Ackerman, Eitan Farchi, Orna Raz, Marcel Zalmanovici, Parijat\n  Dube", "title": "Detection of data drift and outliers affecting machine learning model\n  performance over time", "comments": "In: JSM Proceedings, Nonparametric Statistics Section, 20202.\n  Philadelphia, PA: American Statistical Association. 144--160", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A trained ML model is deployed on another `test' dataset where target feature\nvalues (labels) are unknown. Drift is distribution change between the training\nand deployment data, which is concerning if model performance changes. For a\ncat/dog image classifier, for instance, drift during deployment could be rabbit\nimages (new class) or cat/dog images with changed characteristics (change in\ndistribution). We wish to detect these changes but can't measure accuracy\nwithout deployment data labels. We instead detect drift indirectly by\nnonparametrically testing the distribution of model prediction confidence for\nchanges. This generalizes our method and sidesteps domain-specific feature\nrepresentation.\n  We address important statistical issues, particularly Type-1 error control in\nsequential testing, using Change Point Models (CPMs; see Adams and Ross 2012).\nWe also use nonparametric outlier methods to show the user suspicious\nobservations for model diagnosis, since the before/after change confidence\ndistributions overlap significantly. In experiments to demonstrate robustness,\nwe train on a subset of MNIST digit classes, then insert drift (e.g., unseen\ndigit class) in deployment data in various settings (gradual/sudden changes in\nthe drift proportion). A novel loss function is introduced to compare the\nperformance (detection delay, Type-1 and 2 errors) of a drift detector under\ndifferent levels of drift class contamination.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2020 20:50:12 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 09:31:46 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Ackerman", "Samuel", ""], ["Farchi", "Eitan", ""], ["Raz", "Orna", ""], ["Zalmanovici", "Marcel", ""], ["Dube", "Parijat", ""]]}, {"id": "2012.09449", "submitter": "Sebastian Kersting", "authors": "Sebastian Kersting and Michael Kohler", "title": "Uncertainty Quantification in Case of Imperfect Models: A Review", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.SY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Uncertainty quantification of complex technical systems is often based on a\ncomputer model of the system. As all models such a computer model is always\nwrong in the sense that it does not describe the reality perfectly. The purpose\nof this article is to give a review of techniques which use observed values of\nthe technical systems in order to take into account the inadequacy of a\ncomputer model in uncertainty quantification. The techniques reviewed in this\narticle are illustrated and compared by applying them to applications in\nmechanical engineering.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 08:52:10 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Kersting", "Sebastian", ""], ["Kohler", "Michael", ""]]}, {"id": "2012.09598", "submitter": "Owen G. Ward", "authors": "Owen G. Ward, Jing Wu, Tian Zheng, Anna L. Smith, James P. Curley", "title": "Network Hawkes Process Models for Exploring Latent Hierarchy in Social\n  Animal Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Group-based social dominance hierarchies are of essential interest in animal\nbehavior research. Studies often record aggressive interactions observed over\ntime, and models that can capture such dynamic hierarchy are therefore crucial.\nTraditional ranking methods summarize interactions across time, using only\naggregate counts. Instead, we take advantage of the interaction timestamps,\nproposing a series of network point process models with latent ranks. We\ncarefully design these models to incorporate important characteristics of\nanimal interaction data, including the winner effect, bursting and pair-flip\nphenomena. Through iteratively constructing and evaluating these models we\narrive at the final cohort Markov-Modulated Hawkes process (C-MMHP), which best\ncharacterizes all aforementioned patterns observed in interaction data. We\ncompare all models using simulated and real data. Using statistically developed\ndiagnostic perspectives, we demonstrate that the C-MMHP model outperforms other\nmethods, capturing relevant latent ranking structures that lead to meaningful\npredictions for real data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 14:16:16 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Ward", "Owen G.", ""], ["Wu", "Jing", ""], ["Zheng", "Tian", ""], ["Smith", "Anna L.", ""], ["Curley", "James P.", ""]]}, {"id": "2012.09821", "submitter": "Ritabrata Dutta", "authors": "Sherman Lo and Peter Watson and Peter Dueben and Ritabrata Dutta", "title": "High-resolution Probabilistic Precipitation Prediction for use in\n  Climate Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate prediction of precipitation is important to allow for reliable\nwarnings of flood or drought risk in a changing climate. However, to make\ntrust-worthy predictions of precipitation, at a local scale, is one of the most\ndifficult challenges for today's weather and climate models. This is because\nimportant features, such as individual clouds and high-resolution topography,\ncannot be resolved explicitly within simulations due to the significant\ncomputational cost of high-resolution simulations. Climate models are typically\nrun at $\\sim$50-100 km resolution which is insufficient to represent local\nprecipitation events in satisfying detail. Here, we develop a method to make\nprobabilistic precipitation predictions based on features that climate models\ncan resolve well and that is not highly sensitive to the approximations used in\nindividual models. To predict, we will use a temporal compound Poisson\ndistribution dependent on the output of climate models at a location. We use\nthe output of Earth System models at coarse resolution $\\sim$50 km as input and\ntrain the statistical models towards precipitation observations over Wales at\n$\\sim$10 km resolution. A Bayesian inferential scheme is provided so that the\ncompound-Poisson model can be inferred using a\nGibbs-within-Metropolis-Elliptic-Slice sampling scheme which enables us to\nquantify the uncertainty of our predictions. In addition, we use a Gaussian\nprocess regressor on the posterior samples of the model parameters, to infer a\nspatially coherent model and hence to produce spatially coherent rainfall\nprediction. We illustrate the prediction performance of our model by training\nover 5 years of the data up to 31st December 1999 and predicting precipitation\nfor 20 years afterwards for Cardiff and Wales.\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 18:42:48 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2021 10:11:56 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Lo", "Sherman", ""], ["Watson", "Peter", ""], ["Dueben", "Peter", ""], ["Dutta", "Ritabrata", ""]]}, {"id": "2012.09932", "submitter": "Edward Raff", "authors": "Edward Raff", "title": "Research Reproducibility as a Survival Analysis", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been increasing concern within the machine learning community that\nwe are in a reproducibility crisis. As many have begun to work on this problem,\nall work we are aware of treat the issue of reproducibility as an intrinsic\nbinary property: a paper is or is not reproducible. Instead, we consider\nmodeling the reproducibility of a paper as a survival analysis problem. We\nargue that this perspective represents a more accurate model of the underlying\nmeta-science question of reproducible research, and we show how a survival\nanalysis allows us to draw new insights that better explain prior longitudinal\ndata. The data and code can be found at\nhttps://github.com/EdwardRaff/Research-Reproducibility-Survival-Analysis\n", "versions": [{"version": "v1", "created": "Thu, 17 Dec 2020 20:56:53 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Raff", "Edward", ""]]}, {"id": "2012.10006", "submitter": "Xianghao Zhan", "authors": "Xianghao Zhan, Yiheng Li, Yuzhe Liu, August G. Domel, Hossein Vahid\n  Alizadeh, Samuel J. Raymond, Jesse Ruan, Saeed Barbat, Stephen Tiernan,\n  Olivier Gevaert, Michael Zeineh, Gerald Grant, David B. Camarillo", "title": "Relationship between brain injury criteria and brain strain across\n  different types of head impacts can be different", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO cs.LG physics.data-an q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple brain injury criteria (BIC) are developed to quickly quantify brain\ninjury risks after head impacts. These BIC originated from different types of\nhead impacts (e.g., sports and car crashes) are widely used in risk evaluation.\nHowever, the accuracy of using the BIC on brain injury risk estimation across\ndifferent types of head impacts has not been evaluated. Physiologically, brain\nstrain is often considered the key parameter of brain injury. To evaluate the\nBIC's risk estimation accuracy across five datasets comprising different head\nimpact types, linear regression was used to model 95% maximum principal strain,\n95% maximum principal strain at the corpus callosum, and cumulative strain\ndamage (15%) on each of 18 BIC respectively. The results show a significant\ndifference in the relationship between BIC and brain strain across datasets,\nindicating the same BIC value may suggest different brain strain in different\nhead impact types. The accuracy of brain strain regression is generally\ndecreasing if the BIC regression models are fit on a dataset with a different\ntype of head impact rather than on the dataset with the same type. Given this\nfinding, this study raises concerns for applying BIC to estimate the brain\ninjury risks for head impacts different from the head impacts on which the BIC\nwas developed.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 01:41:41 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:51:02 GMT"}, {"version": "v3", "created": "Mon, 19 Apr 2021 06:51:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhan", "Xianghao", ""], ["Li", "Yiheng", ""], ["Liu", "Yuzhe", ""], ["Domel", "August G.", ""], ["Alizadeh", "Hossein Vahid", ""], ["Raymond", "Samuel J.", ""], ["Ruan", "Jesse", ""], ["Barbat", "Saeed", ""], ["Tiernan", "Stephen", ""], ["Gevaert", "Olivier", ""], ["Zeineh", "Michael", ""], ["Grant", "Gerald", ""], ["Camarillo", "David B.", ""]]}, {"id": "2012.10167", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen and Tom Heskes", "title": "Inferring the Direction of a Causal Link and Estimating Its Effect via a\n  Bayesian Mendelian Randomization Approach", "comments": "26 pages, 22 figures, published in Statistical Methods in Medical\n  Research", "journal-ref": "Statistical Methods in Medical Research, Vol 29, Issue 4, 2020", "doi": "10.1177/0962280219851817", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of genetic variants as instrumental variables - an approach known as\nMendelian randomization - is a popular epidemiological method for estimating\nthe causal effect of an exposure (phenotype, biomarker, risk factor) on a\ndisease or health-related outcome from observational data. Instrumental\nvariables must satisfy strong, often untestable assumptions, which means that\nfinding good genetic instruments among a large list of potential candidates is\nchallenging. This difficulty is compounded by the fact that many genetic\nvariants influence more than one phenotype through different causal pathways, a\nphenomenon called horizontal pleiotropy. This leads to errors not only in\nestimating the magnitude of the causal effect but also in inferring the\ndirection of the putative causal link. In this paper, we propose a Bayesian\napproach called BayesMR that is a generalization of the Mendelian randomization\ntechnique in which we allow for pleiotropic effects and, crucially, for the\npossibility of reverse causation. The output of the method is a posterior\ndistribution over the target causal effect, which provides an immediate and\neasily interpretable measure of the uncertainty in the estimation. More\nimportantly, we use Bayesian model averaging to determine how much more likely\nthe inferred direction is relative to the reverse direction.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 11:01:52 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "2012.10320", "submitter": "Odalric-Ambrym Maillard", "authors": "Maillard Odalric-Ambrym", "title": "Local Dvoretzky-Kiefer-Wolfowitz confidence bands", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we revisit the concentration inequalities for the supremum of\nthe cumulative distribution function (CDF) of a real-valued continuous\ndistribution as established by Dvoretzky, Kiefer, Wolfowitz and revisited later\nby Massart in in two seminal papers. We focus on the concentration of the\n\\textit{local} supremum over a sub-interval, rather than on the full domain.\nThat is, denoting $U$ the CDF of the uniform distribution over $[0,1]$ and\n$U_n$ its empirical version built from $n$ samples, we study\n$\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U_n(u)-U(u) > \\epsilon\\Big)$ for different\nvalues of $\\uu,\\ou\\in[0,1]$. Such local controls naturally appear for instance\nwhen studying estimation error of spectral risk-measures (such as the\nconditional value at risk), where $[\\uu,\\ou]$ is typically $[0,\\alpha]$ or\n$[1-\\alpha,1]$ for a risk level $\\alpha$, after reshaping the CDF $F$ of the\nconsidered distribution into $U$ by the general inverse transform $F^{-1}$.\nExtending a proof technique from Smirnov, we provide exact expressions of the\nlocal quantities $\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U_n(u)-U(u) > \\epsilon\\Big)$\nand $\\Pr\\Big(\\sup_{u\\in [\\uu,\\ou]} U(u)-U_n(u) > \\epsilon\\Big)$ for each\n$n,\\epsilon,\\uu,\\ou$. Interestingly these quantities, seen as a function of\n$\\epsilon$, can be easily inverted numerically into functions of the\nprobability level $\\delta$. Although not explicit, they can be computed and\ntabulated. We plot such expressions and compare them to the classical bound\n$\\sqrt{\\frac{\\ln(1/\\delta)}{2n}}$ provided by Massart inequality. Last, we\nextend the local concentration results holding individually for each $n$ to\ntime-uniform concentration inequalities holding simultaneously for all $n$,\nrevisiting a reflection inequality by James, which is of independent interest\nfor the study of sequential decision making strategies.\n", "versions": [{"version": "v1", "created": "Fri, 18 Dec 2020 16:04:00 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:35:09 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Odalric-Ambrym", "Maillard", ""]]}, {"id": "2012.10392", "submitter": "Gustau Camps-Valls", "authors": "Jorge Vicent, Jochem Verrelst, Juan Pablo Rivera-Caicedo, Neus\n  Sabater, Jordi Mu\\~noz-Mar\\'i, Gustau Camps-Valls, Jos\\'e Moreno", "title": "Emulation as an Accurate Alternative to Interpolation in Sampling\n  Radiative Transfer Codes", "comments": null, "journal-ref": "IEEE Journal of Selected Topics in Applied Earth Observations and\n  Remote Sensing, vol. 11, no. 12, pp. 4918-4931, Dec. 2018", "doi": "10.1109/JSTARS.2018.2875330", "report-no": null, "categories": "physics.ao-ph cs.LG physics.comp-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computationally expensive Radiative Transfer Models (RTMs) are widely used}\nto realistically reproduce the light interaction with the Earth surface and\natmosphere. Because these models take long processing time, the common practice\nis to first generate a sparse look-up table (LUT) and then make use of\ninterpolation methods to sample the multi-dimensional LUT input variable space.\nHowever, the question arise whether common interpolation methods perform most\naccurate. As an alternative to interpolation, this work proposes to use\nemulation, i.e., approximating the RTM output by means of statistical learning.\nTwo experiments were conducted to assess the accuracy in delivering spectral\noutputs using interpolation and emulation: (1) at canopy level, using PROSAIL;\nand (2) at top-of-atmosphere level, using MODTRAN. Various interpolation\n(nearest-neighbour, inverse distance weighting, piece-wice linear) and\nemulation (Gaussian process regression (GPR), kernel ridge regression, neural\nnetworks) methods were evaluated against a dense reference LUT. In all\nexperiments, the emulation methods clearly produced more accurate output\nspectra than classical interpolation methods. GPR emulation performed up to ten\ntimes more accurately than the best performing interpolation method, and this\nwith a speed that is competitive with the faster interpolation methods. It is\nconcluded that emulation can function as a fast and more accurate alternative\nto commonly used interpolation methods for reconstructing RTM spectral data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 10:04:12 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Vicent", "Jorge", ""], ["Verrelst", "Jochem", ""], ["Rivera-Caicedo", "Juan Pablo", ""], ["Sabater", "Neus", ""], ["Mu\u00f1oz-Mar\u00ed", "Jordi", ""], ["Camps-Valls", "Gustau", ""], ["Moreno", "Jos\u00e9", ""]]}, {"id": "2012.10559", "submitter": "Tyler McCormick", "authors": "Shane Lubold, Arun G. Chandrasekhar, Tyler H. McCormick", "title": "Identifying the latent space geometry of network models through analysis\n  of curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.GT stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistically modeling networks, across numerous disciplines and contexts, is\nfundamentally challenging because of (often high-order) dependence between\nconnections. A common approach assigns each person in the graph to a position\non a low-dimensional manifold. Distance between individuals in this (latent)\nspace is inversely proportional to the likelihood of forming a connection. The\nchoice of the latent geometry (the manifold class, dimension, and curvature)\nhas consequential impacts on the substantive conclusions of the model. More\npositive curvature in the manifold, for example, encourages more and tighter\ncommunities; negative curvature induces repulsion among nodes. Currently,\nhowever, the choice of the latent geometry is an a priori modeling assumption\nand there is limited guidance about how to make these choices in a data-driven\nway. In this work, we present a method to consistently estimate the manifold\ntype, dimension, and curvature from an empirically relevant class of latent\nspaces: simply connected, complete Riemannian manifolds of constant curvature.\nOur core insight comes by representing the graph as a noisy distance matrix\nbased on the ties between cliques. Leveraging results from statistical\ngeometry, we develop hypothesis tests to determine whether the observed\ndistances could plausibly be embedded isometrically in each of the candidate\ngeometries. We explore the accuracy of our approach with simulations and then\napply our approach to data-sets from economics and sociology as well as\nneuroscience.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 00:35:29 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 22:40:45 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 21:47:53 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lubold", "Shane", ""], ["Chandrasekhar", "Arun G.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "2012.10590", "submitter": "Sanjib Sharma", "authors": "Mahkameh Zarekarizi, K. Joel Roop-Eckart, Sanjib Sharma, Klaus Keller", "title": "The FLOod Probability Interpolation Tool (FLOPIT): Improving Spatial\n  Flood Probability Quantification and Communication Through Higher Resolution\n  Mapping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding flood probabilities is essential to making sound decisions\nabout flood-risk management. Many people rely on flood probability maps to\ninform decisions about purchasing flood insurance, buying or selling\nreal-estate, flood-proofing a house, or managing floodplain development.\nCurrent flood probability maps typically use flood zones (for example the 1 in\n100 or 1 in 500-year flood zones) to communicate flooding probabilities.\nHowever, this choice of communication format can miss important details and\nlead to biased risk assessments. Here we develop, test, and demonstrate the\nFLOod Probability Interpolation Tool (FLOPIT). FLOPIT interpolates flood\nprobabilities between water surface elevation to produce continuous\nflood-probability maps. We show that FLOPIT can be relatively easily applied to\nexisting datasets used to create flood zones. Using publicly available data\nfrom the Federal Emergency Management Agency (FEMA) flood risk databases as\nwell as state and national datasets, we produce continuous flood-probability\nmaps at three example locations in the United States: Houston (TX), Muncy (PA),\nand Selinsgrove (PA). We find that the discrete flood zones generally\ncommunicate substantially lower flood probabilities than the continuous\nestimates.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 04:17:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zarekarizi", "Mahkameh", ""], ["Roop-Eckart", "K. Joel", ""], ["Sharma", "Sanjib", ""], ["Keller", "Klaus", ""]]}, {"id": "2012.10629", "submitter": "Matthieu Marbac", "authors": "Amay SM Cheam, Marc Fredette, Matthieu Marbac, and Fabien Navarro", "title": "Translation-invariant functional clustering on COVID-19 deaths adjusted\n  on population risk factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The COVID-19 pandemic has taken the world by storm with its high infection\nrate. Investigating its geographical disparities has paramount interest in\norder to gauge its relationships with political decisions, economic indicators,\nor mental health. This paper focuses on clustering the daily death rates\nreported in several regions of Europe and the United States over eight months.\nSeveral methods have been developed to cluster such functional data. However,\nthese methods are not translation-invariant and thus cannot handle different\ntimes of arrivals of the disease, nor can they consider external covariates and\nso are unable to adjust for the population risk factors of each region. We\npropose a novel three-step clustering method to circumvent these issues. As a\nfirst step, feature extraction is performed by translation-invariant wavelet\ndecomposition which permits to deal with the different onsets. As a second\nstep, single-index regression is used to neutralize disparities caused by\npopulation risk factors. As a third step, a nonparametric mixture is fitted on\nthe regression residuals to achieve the region clustering. Supplementary\nmaterials for this article, including a standardized description of the\nmaterials available for reproducing the work, are available online.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 08:40:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cheam", "Amay SM", ""], ["Fredette", "Marc", ""], ["Marbac", "Matthieu", ""], ["Navarro", "Fabien", ""]]}, {"id": "2012.10746", "submitter": "Philipp Otto", "authors": "Andreas Piter, Philipp Otto, Hamza Alkhatib", "title": "A Spatiotemporal Functional Model for Bike-Sharing Systems -- An Example\n  based on the City of Helsinki", "comments": "28 pages, 11 figures, submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the usage patterns for bike-sharing systems is essential in\nterms of supporting and enhancing operational planning for such schemes.\nStudies have demonstrated how factors such as weather conditions influence the\nnumber of bikes that should be available at bike-sharing stations at certain\ntimes during the day. However, the influences of these factors usually vary\nover the course of a day, and if there is good temporal resolution, there could\nalso be significant effects only for some hours/minutes (rush hours, the hours\nwhen shops are open, and so forth). Thus, in this paper, an analysis of\nHelsinki's bike-sharing data from 2017 is conducted that considers full\ntemporal and spatial resolutions. Moreover, the data are available at a very\nhigh frequency. Hence, the station hire data is analysed in a spatiotemporal\nfunctional setting, where the number of bikes at a station is defined as a\ncontinuous function of the time of day. For this completely novel approach, we\napply a functional spatiotemporal hierarchical model to investigate the effect\nof environmental factors and the magnitude of the spatial and temporal\ndependence. Challenges in computational complexity are faced using a\nbootstrapping approach. The results show the necessity of splitting the\nbike-sharing stations into two clusters based on the similarity of their\nspatiotemporal functional observations in order to model the station hire data\nof Helsinki's bike-sharing system effectively. The estimated functional\ninfluences of the proposed factors are different for the two clusters.\nMoreover, the estimated parameters reveal high random effects in the data that\nare not explained by the mean of the process. In this random-effects model, the\ntemporal autoregressive parameter dominates the spatial dependence.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 18:08:37 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Piter", "Andreas", ""], ["Otto", "Philipp", ""], ["Alkhatib", "Hamza", ""]]}, {"id": "2012.10763", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Ruofan Xu", "title": "Functional time series forecasting of extreme values", "comments": "21 pages, 4 figures, 1 table, to appear at Communication in\n  Statistics: Case Studies and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider forecasting functional time series of extreme values within a\ngeneralised extreme value distribution (GEV). The GEV distribution can be\ncharacterised using the three parameters (location, scale and shape). As a\nresult, the forecasts of the GEV density can be accomplished by forecasting\nthese three latent parameters. Depending on the underlying data structure, some\nof the three parameters can either be modelled as scalars or functions. We\nprovide two forecasting algorithms to model and forecast these parameters. To\nassess the forecast uncertainty, we apply a sieve bootstrap method to construct\npointwise and simultaneous prediction intervals of the forecasted extreme\nvalues. Illustrated by a daily maximum temperature dataset, we demonstrate the\nadvantages of modelling these parameters as functions. Further, the\nfinite-sample performance of our methods is quantified using several\nMonte-Carlo simulated data under a range of scenarios.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2020 19:31:05 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Shang", "Han Lin", ""], ["Xu", "Ruofan", ""]]}, {"id": "2012.11114", "submitter": "Tingting Zhang", "authors": "Huazhang Li, Yaotian Wang, Guofen Yan, Yinge Sun, Seiji Tanabe,\n  Chang-Chia Liu, Mark Quigg, Tingting Zhang", "title": "A Bayesian State-Space Approach to Mapping Directional Brain Networks", "comments": "35 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human brain is a directional network system of brain regions involving\ndirectional connectivity. Seizures are a directional network phenomenon as\nabnormal neuronal activities start from a seizure onset zone (SOZ) and\npropagate to otherwise healthy regions. To localize the SOZ of an epileptic\npatient, clinicians use iEEG to record the patient's intracranial brain\nactivity in many small regions. iEEG data are high-dimensional multivariate\ntime series. We build a state-space multivariate autoregression (SSMAR) for\niEEG data to model the underlying directional brain network. To produce\nscientifically interpretable network results, we incorporate into the SSMAR the\nscientific knowledge that the underlying brain network tends to have a cluster\nstructure. Specifically, we assign to the SSMAR parameters a\nstochastic-blockmodel-motivated prior, which reflects the cluster structure. We\ndevelop a Bayesian framework to estimate the SSMAR, infer directional\nconnections, and identify clusters for the unobserved network edges. The new\nmethod is robust to violations of model assumptions and outperforms existing\nnetwork methods. By applying the new method to an epileptic patient's iEEG\ndata, we reveal seizure initiation and propagation in the patient's brain\nnetwork. Our method can also accurately localize the SOZ. Overall, this paper\nprovides a tool to study the human brain network.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 04:44:57 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Li", "Huazhang", ""], ["Wang", "Yaotian", ""], ["Yan", "Guofen", ""], ["Sun", "Yinge", ""], ["Tanabe", "Seiji", ""], ["Liu", "Chang-Chia", ""], ["Quigg", "Mark", ""], ["Zhang", "Tingting", ""]]}, {"id": "2012.11124", "submitter": "Pritam Ranjan", "authors": "Pritam Ranjan, M. Harshvardhan", "title": "The Evolution of Dynamic Gaussian Process Model with Applications to\n  Malaria Vaccine Coverage Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process (GP) based statistical surrogates are popular, inexpensive\nsubstitutes for emulating the outputs of expensive computer models that\nsimulate real-world phenomena or complex systems. Here, we discuss the\nevolution of dynamic GP model - a computationally efficient statistical\nsurrogate for a computer simulator with time series outputs. The main idea is\nto use a convolution of standard GP models, where the weights are guided by a\nsingular value decomposition (SVD) of the response matrix over the time\ncomponent. The dynamic GP model also adopts a localized modeling approach for\nbuilding a statistical model for large datasets.\n  In this chapter, we use several popular test function based computer\nsimulators to illustrate the evolution of dynamic GP models. We also use this\nmodel for predicting the coverage of Malaria vaccine worldwide. Malaria is\nstill affecting more than eighty countries concentrated in the tropical belt.\nIn 2019 alone, it was the cause of more than 435,000 deaths worldwide. The\nmalice is easy to cure if diagnosed in time, but the common symptoms make it\ndifficult. We focus on a recently discovered reliable vaccine called Mos-Quirix\n(RTS,S) which is currently going under human trials. With the help of publicly\navailable data on dosages, efficacy, disease incidence and communicability of\nother vaccines obtained from the World Health Organisation, we predict vaccine\ncoverage for 78 Malaria-prone countries.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 05:27:42 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Ranjan", "Pritam", ""], ["Harshvardhan", "M.", ""]]}, {"id": "2012.11135", "submitter": "Kungang Zhang", "authors": "Kungang Zhang, Daniel W. Apley, Wei Chen", "title": "Nonstationarity Analysis of Materials Microstructures via Fisher Score\n  Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microstructures are critical to the physical properties of materials.\nStochastic microstructures are commonly observed in many kinds of materials and\ntraditional descriptor-based image analysis of them can be challenging. In this\npaper, we introduce a powerful and versatile score-based framework for\nanalyzing nonstationarity in stochastic materials microstructures. The\nframework involves training a parametric supervised learning model to predict a\npixel value using neighboring pixels in images of microstructures~(as known as\nmicrographs), and this predictive model provides an implicit characterization\nof the stochastic nature of the microstructure. The basis for our approach is\nthe Fisher score vector, defined as the gradient of the log-likelihood with\nrespect to the parameters of the predictive model, at each micrograph pixel. A\nfundamental property of the score vector is that it is zero-mean if the\npredictive relationship in the vicinity of that pixel remains unchanged, which\nwe equate with the local stochastic nature of the microstructure remaining\nunchanged. Conversely, if the local stochastic nature changes, then the mean of\nthe score vector generally differs from zero. Our framework analyzes how the\nlocal mean of the score vector varies across one or more image samples to: (1)\nmonitor for nonstationarity by indicating whether new samples are statistically\ndifferent than reference samples and where they may differ and (2) diagnose\nnonstationarity by identifying the distinct types of stochastic microstructures\nand labeling accordingly the corresponding regions of the samples. Unlike\nfeature-based methods, our approach is almost completely general and requires\nno prior knowledge of the nature of the nonstationarities. Using a number of\nreal and simulated micrographs, including polymer composites and multiphase\nalloys, we demonstrate the power and versatility of the approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 06:24:54 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Zhang", "Kungang", ""], ["Apley", "Daniel W.", ""], ["Chen", "Wei", ""]]}, {"id": "2012.11361", "submitter": "Kieran Kalair", "authors": "Kieran Kalair, Colm Connaughton", "title": "Anomaly detection and classification in traffic flow data from\n  fluctuations in the flow-density relationship", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and validate a novel data-driven approach to the real time\ndetection and classification of traffic anomalies based on the identification\nof atypical fluctuations in the relationship between density and flow. For\naggregated data under stationary conditions, flow and density are related by\nthe fundamental diagram. However, high resolution data obtained from modern\nsensor networks is generally non-stationary and disaggregated. Such data\nconsequently show significant statistical fluctuations. These fluctuations are\nbest described using a bivariate probability distribution in the density-flow\nplane. By applying kernel density estimation to high-volume data from the UK\nNational Traffic Information Service (NTIS), we empirically construct these\ndistributions for London's M25 motorway. Curves in the density-flow plane are\nthen constructed, analogous to quantiles of univariate distributions. These\ncurves quantitatively separate atypical fluctuations from typical traffic\nstates. Although the algorithm identifies anomalies in general rather than\nspecific events, we find that fluctuations outside the 95\\% probability curve\ncorrelate strongly with the spikes in travel time associated with significant\ncongestion events. Moreover, the size of an excursion from the typical region\nprovides a simple, real-time measure of the severity of detected anomalies. We\nvalidate the algorithm by benchmarking its ability to identify labelled events\nin historical NTIS data against some commonly used methods from the literature.\nDetection rate, time-to-detect and false alarm rate are used as metrics and\nfound to be generally comparable except in situations when the speed\ndistribution is bi-modal. In such situations, the new algorithm achieves a much\nlower false alarm rate without suffering significant degradation on the other\nmetrics. This method has the additional advantage of being self-calibrating.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 14:14:08 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Kalair", "Kieran", ""], ["Connaughton", "Colm", ""]]}, {"id": "2012.11411", "submitter": "Diana Cesar", "authors": "Diana Cesar", "title": "An Approach to Gender Pay Equity Analysis Using Bayesian Hierarchical\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diversity and inclusion, or D and I, is a topic that sparks the interest of\ncompanies, research groups, and individuals alike. Recently in the United\nStates, renewed focus has been placed on fair and equitable pay practices,\nwhich are a key component of promoting diversity in the workplace. Despite the\nincreased demand for reliable pay equity analysis, the challenges of conducting\nthis type of analysis on industry data have not been adequately addressed. This\npaper explains a few limitations of current approaches to pay equity analysis\nby gender and improves on them with a Bayesian hierarchical regression model.\nUsing global workforce data from a large U.S. semiconductor company, Micron\nTechnology, Inc., the paper demonstrates how the model provides a holistic view\nof gender pay equity across the organization, while overcoming issues more\ncommon in industry data, such as small sample size and poor gender\nrepresentation. When compared to a prior analysis of Micron's U.S. workforce,\nthis approach decreased the amount of manual review required, enabling decision\nmakers to finalize pay adjustments across a workforce of 31,738 people within\nfour weeks of receiving preliminary model results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 15:15:10 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Cesar", "Diana", ""]]}, {"id": "2012.11542", "submitter": "Sean Elliott", "authors": "Sean Elliott and Christian Gourieroux", "title": "Uncertainty on the Reproduction Ratio in the SIR Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this paper is to understand the extreme variability on the\nestimated reproduction ratio $R_0$ observed in practice. For expository purpose\nwe consider a discrete time stochastic version of the\nSusceptible-Infected-Recovered (SIR) model, and introduce different approximate\nmaximum likelihood (AML) estimators of $R_0$. We carefully discuss the\nproperties of these estimators and illustrate by a Monte-Carlo study the width\nof confidence intervals on $R_0$.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 18:21:36 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Elliott", "Sean", ""], ["Gourieroux", "Christian", ""]]}, {"id": "2012.11649", "submitter": "Francis Diebold", "authors": "Francis X. Diebold, Minchul Shin, and Boyuan Zhang", "title": "On the Aggregation of Probability Assessments: Regularized Mixtures of\n  Predictive Densities for Eurozone Inflation and Real Interest Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methods for constructing regularized mixtures of density\nforecasts. We explore a variety of objectives and regularization penalties, and\nwe use them in a substantive exploration of Eurozone inflation and real\ninterest rate density forecasts. All individual inflation forecasters (even the\nex post best forecaster) are outperformed by our regularized mixtures. From the\nGreat Recession onward, the optimal regularization tends to move density\nforecasts' probability mass from the centers to the tails, correcting for\noverconfidence.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 19:22:29 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 20:27:37 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Diebold", "Francis X.", ""], ["Shin", "Minchul", ""], ["Zhang", "Boyuan", ""]]}, {"id": "2012.11678", "submitter": "Elena Badillo-Goicoechea", "authors": "Elena Badillo-Goicoechea, Ting-Hsuan Chang, Esther Kim, Sarah LaRocca,\n  Katherine Morris, Xiaoyi Deng, Samantha Chiu, Adrianne Bradford, Andres\n  Garcia, Christoph Kern, Curtiss Cobb, Frauke Kreuter, Elizabeth A. Stuart", "title": "Global Trends and Predictors of Face Mask Usage During the COVID-19\n  Pandemic", "comments": "39 pages, 2 mian figures, Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Guidelines and recommendations from public health authorities\nrelated to face masks have been essential in containing the COVID-19 pandemic.\nWe assessed the prevalence and correlates of mask usage during the pandemic.\n  Methods: We examined a total of 13,723,810 responses to a daily\ncross-sectional representative online survey in 38 countries who completed from\nApril 23, 2020 to October 31, 2020 and reported having been in public at least\nonce during the last seven days. The outcome was individual face mask usage in\npublic settings, and the predictors were country fixed effects, country-level\nmask policy stringency, calendar time, individual sociodemographic factors, and\nhealth prevention behaviors. Associations were modelled using survey-weighted\nmultivariable logistic regression.\n  Findings: Mask-wearing varied over time and across the 38 countries. While\nsome countries consistently showed high prevalence throughout, in other\ncountries mask usage increased gradually, and a few other countries remained at\nlow prevalence. Controlling for time and country fixed effects,\nsociodemographic factors (older age, female gender, education, urbanicity) and\nstricter mask-related policies were significantly associated with higher mask\nusage in public settings, while social behaviors considered risky in the\ncontext of the pandemic (going out to large events, restaurants, shopping\ncenters, and socializing outside of the household) were associated with lower\nmask use.\n  Interpretation: The decision to wear a face mask in public settings is\nsignificantly associated with sociodemographic factors, risky social behaviors,\nand mask policies. This has important implications for health prevention\npolicies and messaging, including the potential need for more targeted policy\nand messaging design.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2020 20:52:30 GMT"}, {"version": "v2", "created": "Sat, 9 Jan 2021 01:21:13 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Badillo-Goicoechea", "Elena", ""], ["Chang", "Ting-Hsuan", ""], ["Kim", "Esther", ""], ["LaRocca", "Sarah", ""], ["Morris", "Katherine", ""], ["Deng", "Xiaoyi", ""], ["Chiu", "Samantha", ""], ["Bradford", "Adrianne", ""], ["Garcia", "Andres", ""], ["Kern", "Christoph", ""], ["Cobb", "Curtiss", ""], ["Kreuter", "Frauke", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2012.11757", "submitter": "Yujia Pan", "authors": "Yujia Pan and Johann A. Gagnon-Bartsch", "title": "Separating and reintegrating latent variables to improve classification\n  of genomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genomic datasets contain the effects of various unobserved biological\nvariables in addition to the variable of primary interest. These latent\nvariables often affect a large number of features (e.g., genes) and thus give\nrise to dense latent variation, which presents both challenges and\nopportunities for classification. Some of these latent variables may be\npartially correlated with the phenotype of interest and therefore helpful,\nwhile others may be uncorrelated and thus merely contribute additional noise.\nMoreover, whether potentially helpful or not, these latent variables may\nobscure weaker effects that impact only a small number of features but more\ndirectly capture the signal of primary interest. We propose the\ncross-residualization classifier to better account for the latent variables in\ngenomic data. Through an adjustment and ensemble procedure, the\ncross-residualization classifier essentially estimates the latent variables and\nresidualizes out their effects, trains a classifier on the residuals, and then\nre-integrates the the latent variables in a final ensemble classifier. Thus,\nthe latent variables are accounted for without discarding any potentially\npredictive information that they may contribute. We apply the method to\nsimulated data as well as a variety of genomic datasets from multiple\nplatforms. In general, we find that the cross-residualization classifier\nperforms well relative to existing classifiers and sometimes offers substantial\ngains.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 00:06:17 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Pan", "Yujia", ""], ["Gagnon-Bartsch", "Johann A.", ""]]}, {"id": "2012.11796", "submitter": "Zann Koh", "authors": "Zann Koh, Yuren Zhou, Billy Pik Lik Lau, Chau Yuen, Bige Tuncer, and\n  Keng Hua Chong", "title": "Multiple-Perspective Clustering of Passive Wi-Fi Sensing Trajectory Data", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": "10.1109/TBDATA.2020.3045154", "report-no": null, "categories": "cs.LG stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Information about the spatiotemporal flow of humans within an urban context\nhas a wide plethora of applications. Currently, although there are many\ndifferent approaches to collect such data, there lacks a standardized framework\nto analyze it. The focus of this paper is on the analysis of the data collected\nthrough passive Wi-Fi sensing, as such passively collected data can have a wide\ncoverage at low cost. We propose a systematic approach by using unsupervised\nmachine learning methods, namely k-means clustering and hierarchical\nagglomerative clustering (HAC) to analyze data collected through such a passive\nWi-Fi sniffing method. We examine three aspects of clustering of the data,\nnamely by time, by person, and by location, and we present the results obtained\nby applying our proposed approach on a real-world dataset collected over five\nmonths.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 02:30:16 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Koh", "Zann", ""], ["Zhou", "Yuren", ""], ["Lau", "Billy Pik Lik", ""], ["Yuen", "Chau", ""], ["Tuncer", "Bige", ""], ["Chong", "Keng Hua", ""]]}, {"id": "2012.11836", "submitter": "Ritwik Bhattacharya", "authors": "Narayanaswamy Balakrishnan and Ritwik Bhattacharya", "title": "D-optimal joint best linear unbiased prediction of order statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In life-testing experiments, it is often of interest to predict unobserved\nfuture failure times based on observed early failure times. A point best linear\nunbiased predictor (BLUP) has been developed in this context by Kaminsky and\nNelson (1975). In this article, we develop joint BLUPs of two future failure\ntimes based on early failure times by minimizing the determinant of the\nvariance-covariance matrix of the predictors. The advantage of applying joint\nprediction is demonstrated by using a real data set. The non-existence of joint\nBLUPs in certain setups is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 05:23:22 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Balakrishnan", "Narayanaswamy", ""], ["Bhattacharya", "Ritwik", ""]]}, {"id": "2012.11915", "submitter": "Andreas Kryger Jensen", "authors": "Claus Thorn Ekstr{\\o}m and Andreas Kryger Jensen", "title": "Having a Ball: evaluating scoring streaks and game excitement using\n  in-match trend estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many popular sports involve matches between two teams or players where each\nteam have the possibility of scoring points throughout the match. While the\noverall match winner and result is interesting, it conveys little information\nabout the underlying scoring trends throughout the match. Modeling approaches\nthat accommodate a finer granularity of the score difference throughout the\nmatch is needed to evaluate in-game strategies, discuss scoring streaks, teams\nstrengths, and other aspects of the game.\n  We propose a latent Gaussian process to model the score difference between\ntwo teams and introduce the Trend Direction Index as an easily interpretable\nprobabilistic measure of the current trend in the match as well as a measure of\npost-game trend evaluation. In addition we propose the Excitement Trend Index -\nthe expected number of monotonicity changes in the running score difference -\nas a measure of overall game excitement.\n  Our proposed methodology is applied to all 1143 matches from the 2019-2020\nNational Basketball Association (NBA) season. We show how the trends can be\ninterpreted in individual games and how the excitement score can be used to\ncluster teams according to how exciting they are to watch.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:30:12 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", ""], ["Jensen", "Andreas Kryger", ""]]}, {"id": "2012.11920", "submitter": "Anis Mohamed Haddouche", "authors": "Anis M. Haddouche, Dominique Fourdrinier and Fatiha Mezoued", "title": "Covariance matrix estimation under data-based loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the $p\\times p$ scale\nmatrix $\\Sigma$ of a multivariate linear regression model $Y=X\\,\\beta +\n\\mathcal{E}\\,$ when the distribution of the observed matrix $Y$ belongs to a\nlarge class of elliptically symmetric distributions. After deriving the\ncanonical form $(Z^T U^T)^T$ of this model, any estimator $\\hat{ \\Sigma}$ of\n$\\Sigma$ is assessed through the data-based loss tr$(S^{+}\\Sigma\\,\n(\\Sigma^{-1}\\hat{\\Sigma} - I_p)^2 )\\,$ where $S=U^T U$ is the sample covariance\nmatrix and $S^{+}$ is its Moore-Penrose inverse. We provide alternative\nestimators to the usual estimators $a\\,S$, where $a$ is a positive constant,\nwhich present smaller associated risk. Compared to the usual quadratic loss\ntr$(\\Sigma^{-1}\\hat{\\Sigma} - I_p)^2$, we obtain a larger class of estimators\nand a wider class of elliptical distributions for which such an improvement\noccurs. A numerical study illustrates the theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 10:41:35 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Haddouche", "Anis M.", ""], ["Fourdrinier", "Dominique", ""], ["Mezoued", "Fatiha", ""]]}, {"id": "2012.12085", "submitter": "Khue-Dung Dang", "authors": "Khue-Dung Dang, Louise M. Ryan, Tugba Akkaya-Hocagil, Richard J. Cook,\n  Gale A. Richardson, Nancy L. Day, Claire D. Coles, Heather Carmichael Olson,\n  Sandra W. Jacobson, Joseph L. Jacobson", "title": "Bayesian structural equation modeling for data from multiple cohorts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While it is well known that high levels of prenatal alcohol exposure (PAE)\nresult in significant cognitive deficits in children, the exact nature of the\ndose response is less well understood. In particular, there is a pressing need\nto identify the levels of PAE associated with an increased risk of clinically\nsignificant adverse effects. To address this issue, data have been combined\nfrom six longitudinal birth cohort studies in the United States that assessed\nthe effects of PAE on cognitive outcomes measured from early school age through\nadolescence. Structural equation models (SEMs) are commonly used to capture the\nassociation among multiple observed outcomes in order to characterise the\nunderlying variable of interest (in this case, cognition) and then relate it to\nPAE. However, it was not possible to apply classic SEM software in our context\nbecause different outcomes were measured in the six studies. In this paper we\nshow how a Bayesian approach can be used to fit a multi-group multi-level\nstructural model that maps cognition to a broad range of observed variables\nmeasured at multiple ages. These variables map to several different cognitive\nsubdomains and are examined in relation to PAE after adjusting for confounding\nusing propensity scores. The model also tests the possibility of a change point\nin the dose-response function.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 15:26:28 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Ryan", "Louise M.", ""], ["Akkaya-Hocagil", "Tugba", ""], ["Cook", "Richard J.", ""], ["Richardson", "Gale A.", ""], ["Day", "Nancy L.", ""], ["Coles", "Claire D.", ""], ["Olson", "Heather Carmichael", ""], ["Jacobson", "Sandra W.", ""], ["Jacobson", "Joseph L.", ""]]}, {"id": "2012.12099", "submitter": "Gustau Camps-Valls", "authors": "Daniel Heestermans Svendsen, Pablo Morales-\\'Alvarez, Rafael Molina,\n  Gustau Camps-Valls", "title": "Deep Gaussian Processes for geophysical parameter retrieval", "comments": "Preprint, Paper published in IGARSS 2018 - 2018 IEEE International\n  Geoscience and Remote Sensing Symposium, Valencia, 2018, pp. 6175-6178", "journal-ref": null, "doi": "10.1109/IGARSS.2018.8517647", "report-no": null, "categories": "physics.geo-ph cs.LG eess.SP stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces deep Gaussian processes (DGPs) for geophysical\nparameter retrieval. Unlike the standard full GP model, the DGP accounts for\ncomplicated (modular, hierarchical) processes, provides an efficient solution\nthat scales well to large datasets, and improves prediction accuracy over\nstandard full and sparse GP models. We give empirical evidence of performance\nfor estimation of surface dew point temperature from infrared sounding data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 14:44:04 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Svendsen", "Daniel Heestermans", ""], ["Morales-\u00c1lvarez", "Pablo", ""], ["Molina", "Rafael", ""], ["Camps-Valls", "Gustau", ""]]}, {"id": "2012.12102", "submitter": "Chul Moon", "authors": "Chul Moon, Qiwei Li, Guanghua Xiao", "title": "Using Persistent Homology Topological Features to Characterize Medical\n  Images: Case Studies on Lung and Brain Cancers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Tumor shape is a key factor that affects tumor growth and metastasis. This\npaper proposes a topological feature computed by persistent homology to\ncharacterize tumor progression from digital pathology and radiology images and\nexamines its effect on the time-to-event data. The proposed topological\nfeatures are invariant to scale-preserving transformation and can summarize\nvarious tumor shape patterns. The topological features are represented in\nfunctional space and used as functional predictors in a functional Cox\nproportional hazards model. The proposed model enables interpretable inference\nabout the association between topological shape features and survival risks.\nTwo case studies are conducted using consecutive 143 lung cancer and 77 brain\ntumor patients. The results of both studies show that the topological features\npredict survival prognosis after adjusting clinical variables, and the\npredicted high-risk groups have significantly (at the level of 0.01) worse\nsurvival outcomes than the low-risk groups. Also, the topological shape\nfeatures found to be positively associated with survival hazards are irregular\nand heterogeneous shape patterns, which are known to be related to tumor\nprogression.\n", "versions": [{"version": "v1", "created": "Mon, 7 Dec 2020 18:16:59 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 18:48:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Moon", "Chul", ""], ["Li", "Qiwei", ""], ["Xiao", "Guanghua", ""]]}, {"id": "2012.12130", "submitter": "Md Saiful Islam", "authors": "Md Saiful Islam, Md Sarowar Morshed, and Md. Noor-E-Alam", "title": "Algorithms for Solving Nonlinear Binary Optimization Problems in Robust\n  Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying cause-effect relation among variables is a key step in the\ndecision-making process. While causal inference requires randomized\nexperiments, researchers and policymakers are increasingly using observational\nstudies to test causal hypotheses due to the wide availability of observational\ndata and the infeasibility of experiments. The matching method is the most used\ntechnique to make causal inference from observational data. However, the pair\nassignment process in one-to-one matching creates uncertainty in the inference\nbecause of different choices made by the experimenter. Recently, discrete\noptimization models are proposed to tackle such uncertainty. Although a robust\ninference is possible with discrete optimization models, they produce nonlinear\nproblems and lack scalability. In this work, we propose greedy algorithms to\nsolve the robust causal inference test instances from observational data with\ncontinuous outcomes. We propose a unique framework to reformulate the nonlinear\nbinary optimization problems as feasibility problems. By leveraging the\nstructure of the feasibility formulation, we develop greedy schemes that are\nefficient in solving robust test problems. In many cases, the proposed\nalgorithms achieve global optimal solution. We perform experiments on three\nreal-world datasets to demonstrate the effectiveness of the proposed algorithms\nand compare our result with the state-of-the-art solver. Our experiments show\nthat the proposed algorithms significantly outperform the exact method in terms\nof computation time while achieving the same conclusion for causal tests. Both\nnumerical experiments and complexity analysis demonstrate that the proposed\nalgorithms ensure the scalability required for harnessing the power of big data\nin the decision-making process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:12:11 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Islam", "Md Saiful", ""], ["Morshed", "Md Sarowar", ""], ["Noor-E-Alam", "Md.", ""]]}, {"id": "2012.12135", "submitter": "Sarath Yasodharan", "authors": "Siva Athreya, Giridhara R. Babu, Aniruddha Iyer, Mohammed Minhaas\n  B.S., Nihesh Rathod, Sharad Shriram, Rajesh Sundaresan, Nidhin Koshy\n  Vaidhiyan, Sarath Yasodharan", "title": "COVID-19: Optimal Design of Serosurveys for Disease Burden Estimation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a methodology by which an epidemiologist may arrive at an optimal\ndesign for a survey whose goal is to estimate the disease burden in a\npopulation. For serosurveys with a given budget of $C$ rupees, a specified set\nof tests with costs, sensitivities, and specificities, we show the existence of\noptimal designs in four different contexts, including the well known c-optimal\ndesign. Usefulness of the results are illustrated via numerical examples. Our\nresults are applicable to a wide range of epidemiological surveys under the\nassumptions that the estimate's Fisher-information matrix satisfies a uniform\npositive definite criterion.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 16:14:37 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Athreya", "Siva", ""], ["Babu", "Giridhara R.", ""], ["Iyer", "Aniruddha", ""], ["S.", "Mohammed Minhaas B.", ""], ["Rathod", "Nihesh", ""], ["Shriram", "Sharad", ""], ["Sundaresan", "Rajesh", ""], ["Vaidhiyan", "Nidhin Koshy", ""], ["Yasodharan", "Sarath", ""]]}, {"id": "2012.12196", "submitter": "Guanhua Fang", "authors": "Guanhua Fang, Xin Xu, Jinxin Guo, Zhiliang Ying, Susu Zhang", "title": "Identifiability of Bifactor Models", "comments": "89 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The bifactor model and its extensions are multidimensional latent variable\nmodels, under which each item measures up to one subdimension on top of the\nprimary dimension(s). Despite their wide applications to educational and\npsychological assessments, this type of multidimensional latent variable models\nmay suffer from non-identifiability, which can further lead to inconsistent\nparameter estimation and invalid inference. The current work provides a\nrelatively complete characterization of identifiability for the linear and\ndichotomous bifactor models and the linear extended bifactor model with\ncorrelated subdimensions. In addition, similar results for the two-tier models\nare also developed. Illustrative examples are provided on checking model\nidentifiability through inspecting the factor loading structure. Simulation\nstudies are reported that examine estimation consistency when the\nidentifiability conditions are/are not satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 17:34:09 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Fang", "Guanhua", ""], ["Xu", "Xin", ""], ["Guo", "Jinxin", ""], ["Ying", "Zhiliang", ""], ["Zhang", "Susu", ""]]}, {"id": "2012.12246", "submitter": "Wolfgang Waltenberger", "authors": "Wolfgang Waltenberger, Andr\\'e Lessa, Sabine Kraml", "title": "Artificial Proto-Modelling: Building Precursors of a Next Standard Model\n  from Simplified Model Results", "comments": "54 pages, 14 figures, two references added in v2. v3 accepted for\n  publication in JHEP. Homepage: https://smodels.github.io/protomodels", "journal-ref": "Journal of High Energy Physics 2021, 207 (2021)", "doi": "10.1007/JHEP03(2021)207", "report-no": null, "categories": "hep-ph hep-ex stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel algorithm to identify potential dispersed signals of new\nphysics in the slew of published LHC results. It employs a random walk\nalgorithm to introduce sets of new particles, dubbed \"proto-models\", which are\ntested against simplified-model results from ATLAS and CMS (exploiting the\nSModelS software framework). A combinatorial algorithm identifies the set of\nanalyses and/or signal regions that maximally violates the SM hypothesis, while\nremaining compatible with the entirety of LHC constraints in our database.\nDemonstrating our method by running over the experimental results in the\nSModelS database, we find as currently best-performing proto-model a top\npartner, a light-flavor quark partner, and a lightest neutral new particle with\nmasses of the order of 1.2 TeV, 700 GeV and 160 GeV, respectively. The\ncorresponding global p-value for the SM hypothesis is approximately 0.19; by\nconstruction no look-elsewhere effect applies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 18:45:22 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 12:05:20 GMT"}, {"version": "v3", "created": "Mon, 15 Mar 2021 11:21:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Waltenberger", "Wolfgang", ""], ["Lessa", "Andr\u00e9", ""], ["Kraml", "Sabine", ""]]}, {"id": "2012.12345", "submitter": "Jos\\'e Luis Sainz-Pardo Au\\~n\\'on", "authors": "Jos\\'e Luis Sainz-Pardo, Jos\\'e Valero", "title": "COVID-19 and other viruses: holding back its expansion by massive\n  testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The experience of Singapur and South Korea makes it clear that under certain\ncircumstances massive testing is an effective way for containing the advance of\nthe COVID-19. In this paper, we propose a modified SEIR model which takes into\naccount tracing and massive testing, proving theoretically that more tracing\nand testing implies a reduction of the total number of infected people in the\nlong run. We apply this model to the spread of the first wave of the disease in\nSpain, obtaining numerical results. After that, we introduce a heuristic\napproach in order to minimize the COVID-19 spreading by planning effective test\ndistributions among the populations of a region over a period of time. As an\napplication, the impact of distributing tests among the counties of New York\naccording to this method is computed in terms of the number of saved infected\nindividuals.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 20:37:14 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Sainz-Pardo", "Jos\u00e9 Luis", ""], ["Valero", "Jos\u00e9", ""]]}, {"id": "2012.12390", "submitter": "Adam Kleczkowski", "authors": "Jonathan Wells and Chris Robertson and Vincent Marmara and Alan Yeung\n  and Adam Kleczkowski", "title": "Modelling a novel Coronavirus (COVID-19): A stochastic SEIR-HCD\n  approach, with real-time parameter estimation & forecasting for Scotland", "comments": "30 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Faced with the 2020 SARS-CoV2 epidemic, public health officials have been\nseeking models that could be used to predict not only the number of new cases\nbut also the levels of hospitalisation, critical care and deaths. In this paper\nwe present a stochastic compartmental model capable of real-time monitoring and\nforecasting of the pandemic incorporating multiple streams of real-world data,\nreported cases, testing intensity, deaths, hospitalisations and critical care\noccupancy. Model parameters are estimated via a Bayesian particle filtering\ntechnique. The model successfully tracks the key variables (reported cases,\ncritical care and deaths) throughout the two waves (March-June and\nSeptember-November 2020) of the COVID-19 outbreak in Scotland. The model\nhospitalisation predictions in Summer 2020 are consistently lower than the\nrecorded data, but consistent with the change to the reporting criteria by the\nHealth Protection Scotland on 15th September. Most parameter estimates were\nconstant over the two waves, but the infection rate and consequently the\nreproductive number decrease in the later stages of the first wave and increase\nagain from July 2020. The death rates are initially high but decrease over\nSummer 2020 before rising again in November. The model can also be used to\nprovide short-term predictions. We show that the 2-week predictability is very\ngood for the period from March to June 2020, even at early stages of the\npandemic. The model has been slower to pick up the increase in the case numbers\nin September 2020 but forecasting improves again in the later stages of the\nepidemic.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2020 22:27:30 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Wells", "Jonathan", ""], ["Robertson", "Chris", ""], ["Marmara", "Vincent", ""], ["Yeung", "Alan", ""], ["Kleczkowski", "Adam", ""]]}, {"id": "2012.12802", "submitter": "Marcelo Medeiros", "authors": "Ricardo P. Masini, Marcelo C. Medeiros and Eduardo F. Mendes", "title": "Machine Learning Advances for Time Series Forecasting", "comments": "42 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we survey the most recent advances in supervised machine\nlearning and high-dimensional models for time series forecasting. We consider\nboth linear and nonlinear alternatives. Among the linear methods we pay special\nattention to penalized regressions and ensemble of models. The nonlinear\nmethods considered in the paper include shallow and deep neural networks, in\ntheir feed-forward and recurrent versions, and tree-based methods, such as\nrandom forests and boosted trees. We also consider ensemble and hybrid models\nby combining ingredients from different alternatives. Tests for superior\npredictive ability are briefly reviewed. Finally, we discuss application of\nmachine learning in economics and finance and provide an illustration with\nhigh-frequency financial data.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 17:01:56 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2021 22:38:39 GMT"}, {"version": "v3", "created": "Fri, 9 Apr 2021 11:24:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Masini", "Ricardo P.", ""], ["Medeiros", "Marcelo C.", ""], ["Mendes", "Eduardo F.", ""]]}, {"id": "2012.12895", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "A Modern Analysis of Hutchinson's Trace Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper establishes the new state-of-art in the accuracy analysis of\nHutchinson's trace estimator. Leveraging tools that have not been previously\nused in this context, particularly hypercontractive inequalities and\nconcentration properties of sub-gamma distributions, we offer an elegant and\nmodular analysis, as well as numerically superior bounds. Besides these\nimprovements, this work aims to better popularize the aforementioned techniques\nwithin the CS community.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 18:58:01 GMT"}], "update_date": "2020-12-24", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2012.12961", "submitter": "Brian Cleary", "authors": "Brian Cleary and Aviv Regev", "title": "The necessity and power of random, under-sampled experiments in biology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A vast array of transformative technologies developed over the past decade\nhas enabled measurement and perturbation at ever increasing scale, yet our\nunderstanding of many systems remains limited by experimental capacity.\nOvercoming this limitation is not simply a matter of reducing costs with\nexisting approaches; for complex biological systems it will likely never be\npossible to comprehensively measure and perturb every combination of variables\nof interest. There is, however, a growing body of work - much of it\nfoundational and precedent setting - that extracts a surprising amount of\ninformation from highly under sampled data. For a wide array of biological\nquestions, especially the study of genetic interactions, approaches like these\nwill be crucial to obtain a comprehensive understanding. Yet, there is no\ncoherent framework that unifies these methods, provides a rigorous mathematical\nfoundation to understand their limitations and capabilities, allows us to\nunderstand through a common lens their surprising successes, and suggests how\nwe might crystalize the key concepts to transform experimental biology. Here,\nwe review prior work on this topic - both the biology and the mathematical\nfoundations of randomization and low dimensional inference - and propose a\ngeneral framework to make data collection in a wide array of studies vastly\nmore efficient using random experiments and composite experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 20:38:33 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Cleary", "Brian", ""], ["Regev", "Aviv", ""]]}, {"id": "2012.13027", "submitter": "Deniz Sargun", "authors": "Deniz Sargun and C. Emre Koksal", "title": "Quickest Detection over Sensor Networks with Unknown Post-Change\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a quickest change detection problem over sensor networks where\nboth the subset of sensors undergoing a change and the local post-change\ndistributions are unknown. Each sensor in the network observes a local discrete\ntime random process over a finite alphabet. Initially, the observations are\nindependent and identically distributed (i.i.d.) with known pre-change\ndistributions independent from other sensors. At a fixed but unknown change\npoint, a fixed but unknown subset of the sensors undergo a change and start\nobserving samples from an unknown distribution. We assume the change can be\nquantified using concave (or convex) local statistics over the space of\ndistributions. We propose an asymptotically optimal and computationally\ntractable stopping time for Lorden's criterion. Under this scenario, our\nproposed method uses a concave global cumulative sum (CUSUM) statistic at the\nfusion center and suppresses the most likely false alarms using information\nprojection. Finally, we show some numerical results of the simulation of our\nalgorithm for the problem described.\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2020 23:55:18 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 19:30:23 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Sargun", "Deniz", ""], ["Koksal", "C. Emre", ""]]}, {"id": "2012.13112", "submitter": "David Walsh", "authors": "David Walsh, Alejandro Schuler, Diana Hall, Jon Walsh, Charles Fisher", "title": "Bayesian prognostic covariate adjustment", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historical data about disease outcomes can be integrated into the analysis of\nclinical trials in many ways. We build on existing literature that uses\nprognostic scores from a predictive model to increase the efficiency of\ntreatment effect estimates via covariate adjustment. Here we go further,\nutilizing a Bayesian framework that combines prognostic covariate adjustment\nwith an empirical prior distribution learned from the predictive performances\nof the prognostic model on past trials. The Bayesian approach interpolates\nbetween prognostic covariate adjustment with strict type I error control when\nthe prior is diffuse, and a single-arm trial when the prior is sharply peaked.\nThis method is shown theoretically to offer a substantial increase in\nstatistical power, while limiting the type I error rate under reasonable\nconditions. We demonstrate the utility of our method in simulations and with an\nanalysis of a past Alzheimer's disease clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 05:19:03 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Walsh", "David", ""], ["Schuler", "Alejandro", ""], ["Hall", "Diana", ""], ["Walsh", "Jon", ""], ["Fisher", "Charles", ""]]}, {"id": "2012.13267", "submitter": "Matteo Iacopini", "authors": "Matteo Iacopini and Carlo R.M.A. Santagiustina", "title": "Filtering the intensity of public concern from social media count data\n  with jumps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Count time series obtained from online social media data, such as Twitter,\nhave drawn increasing interest among academics and market analysts over the\npast decade. Transforming Web activity records into counts yields time series\nwith peculiar features, including the coexistence of smooth paths and sudden\njumps, as well as cross-sectional and temporal dependence. Using Twitter posts\nabout country risks for the United Kingdom and the United States, this paper\nproposes an innovative state space model for multivariate count data with\njumps. We use the proposed model to assess the impact of public concerns in\nthese countries on market systems. To do so, public concerns inferred from\nTwitter data are unpacked into country-specific persistent terms, risk social\namplification events, and co-movements of the country series. The identified\ncomponents are then used to investigate the existence and magnitude of\ncountry-risk spillovers and social amplification effects on the volatility of\nfinancial markets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Dec 2020 14:18:04 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Iacopini", "Matteo", ""], ["Santagiustina", "Carlo R. M. A.", ""]]}, {"id": "2012.13546", "submitter": "Maxim Bakaev", "authors": "Maxim Bakaev, Sebastian Heil, Martin Gaedke", "title": "Distributional Ground Truth: Non-Redundant Crowdsourcing Data Quality\n  Control in UI Labeling Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.LG cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  HCI increasingly employs Machine Learning and Image Recognition, in\nparticular for visual analysis of user interfaces (UIs). A popular way for\nobtaining human-labeled training data is Crowdsourcing, typically using the\nquality control methods ground truth and majority consensus, which necessitate\nredundancy in the outcome. In our paper we propose a non-redundant method for\nprediction of crowdworkers' output quality in web UI labeling tasks, based on\nhomogeneity of distributions assessed with two-sample Kolmogorov-Smirnov test.\nUsing a dataset of about 500 screenshots with over 74,000 UI elements located\nand classified by 11 trusted labelers and 298 Amazon Mechanical Turk\ncrowdworkers, we demonstrate the advantage of our approach over the baseline\nmodel based on mean Time-on-Task. Exploring different dataset partitions, we\nshow that with the trusted set size of 17-27% UIs our \"distributional ground\ntruth\" model can achieve R2s of over 0.8 and help to obviate the ancillary work\neffort and expenses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Dec 2020 09:06:10 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Bakaev", "Maxim", ""], ["Heil", "Sebastian", ""], ["Gaedke", "Martin", ""]]}, {"id": "2012.13677", "submitter": "Song-Kyoo Amang Kim Ph.D.", "authors": "Song-Kyoo (Amang) Kim", "title": "Toward Compact Data from Big Data", "comments": "This paper has been accepted in the 2020 IEEE-ICITIS Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Bigdata is a dataset of which size is beyond the ability of handling a\nvaluable raw material that can be refined and distilled into valuable specific\ninsights. Compact data is a method that optimizes the big dataset that gives\nbest assets without handling complex bigdata. The compact dataset contains the\nmaximum knowledge patterns at fine grained level for effective and personalized\nutilization of bigdata systems without bigdata. The compact data method is a\ntailor-made design which depends on problem situations. Various compact data\ntechniques have been demonstrated into various data-driven research area in the\npaper.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2020 04:45:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Song-Kyoo", "", "", "Amang"], ["Kim", "", ""]]}, {"id": "2012.13926", "submitter": "Michael Crowther", "authors": "Caroline E. Weibull, Paul C. Lambert, Sandra Eloranta, Therese M.L.\n  Andersson, Paul W. Dickman, Michael J. Crowther", "title": "A multi-state model incorporating estimation of excess hazards and\n  multiple time scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As cancer patient survival improves, late effects from treatment are becoming\nthe next clinical challenge. Chemotherapy and radiotherapy, for example,\npotentially increase the risk of both morbidity and mortality from second\nmalignancies and cardiovascular disease. To provide clinically relevant\npopulation-level measures of late effects, it is of importance to (1)\nsimultaneously estimate the risks of both morbidity and mortality, (2)\npartition these risks into the component expected in the absence of cancer and\nthe component due to the cancer and its treatment, and (3) incorporate the\nmultiple time scales of attained age, calendar time, and time since diagnosis.\nMulti-state models provide a framework for simultaneously studying morbidity\nand mortality, but do not solve the problem of partitioning the risks. However,\nthis partitioning can be achieved by applying a relative survival framework, by\nallowing is to directly quantify the excess risk. This paper proposes a\ncombination of these two frameworks, providing one approach to address (1)-(3).\nUsing recently developed methods in multi-state modeling, we incorporate\nestimation of excess hazards into a multi-state model. Both intermediate and\nabsorbing state risks can be partitioned and different transitions are allowed\nto have different and/or multiple time scales. We illustrate our approach using\ndata on Hodgkin lymphoma patients and excess risk of diseases of the\ncirculatory system, and provide user-friendly Stata software with accompanying\nexample code.\n", "versions": [{"version": "v1", "created": "Sun, 27 Dec 2020 11:49:40 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Weibull", "Caroline E.", ""], ["Lambert", "Paul C.", ""], ["Eloranta", "Sandra", ""], ["Andersson", "Therese M. L.", ""], ["Dickman", "Paul W.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "2012.14081", "submitter": "Pedro Ramos", "authors": "Eduardo Ramos, Osafu A. Egbon, Pedro L. Ramos, Francisco A. Rodrigues,\n  Francisco Louzada", "title": "Objective Bayesian Analysis for the Differential Entropy of the Gamma\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of entropy related concepts goes from physics, such as in statistical\nmechanics, to evolutionary biology. The Shannon entropy is a measure used to\nquantify the amount of information in a system, and its estimation is usually\nmade under the frequentist approach. In the present paper, we introduce an\nfully objective Bayesian analysis to obtain this measure's posterior\ndistribution. Notably, we consider the Gamma distribution, which describes many\nnatural phenomena in physics, engineering, and biology. We reparametrize the\nmodel in terms of entropy, and different objective priors are derived, such as\nJeffreys prior, reference prior, and matching priors. Since the obtained priors\nare improper, we prove that the obtained posterior distributions are proper and\ntheir respective posterior means are finite. An intensive simulation study is\nconducted to select the prior that returns better results in terms of bias,\nmean square error, and coverage probabilities. The proposed approach is\nillustrated in two datasets, where the first one is related to the Achaemenid\ndynasty reign period, and the second data describes the time to failure of an\nelectronic component in the sugarcane harvest machine.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 03:47:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Ramos", "Eduardo", ""], ["Egbon", "Osafu A.", ""], ["Ramos", "Pedro L.", ""], ["Rodrigues", "Francisco A.", ""], ["Louzada", "Francisco", ""]]}, {"id": "2012.14192", "submitter": "Lucia Russo", "authors": "Konstantinos Kaloudis, George A. Kevrekidis, Helena C. Maltezou, Cleo\n  Anastassopoulou, Athanasios Tsakris, Lucia Russo", "title": "Estimation of the effective reproduction number for SARS-CoV-2 infection\n  during the first epidemic wave in the metropolitan area of Athens, Greece", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Herein, we provide estimations for the effective reproduction number $R_e$\nfor the greater metropolitan area of Athens, Greece during the first wave of\nthe pandemic (February 26-May 15, 2020). For our calculations, we implemented,\nin a comparative approach, the two most widely used methods for the estimation\nof $R_e$, that by Wallinga and Teunis and by Cori et al. Data were retrieved\nfrom the national database of SARS-CoV-2 infections in Greece. Our analysis\nrevealed that the expected value of Re dropped below 1 around March 15, shortly\nafter the suspension of the operation of educational institutions of all levels\nnationwide on March 10, and the closing of all retail activities (cafes, bars,\nmuseums, shopping centres, sports facilities and restaurants) on March 13. On\nMay 4, the date on which the gradual relaxation of the strict lockdown\ncommenced, the expected value of $R_e$ was slightly below 1, however with\nrelatively high levels of uncertainty due to the limited number of notified\ncases during this period. Finally, we discuss the limitations and pitfalls of\nthe methods utilized for the estimation of the $R_e$, highlighting that the\nresults of such analyses should be considered only as indicative by policy\nmakers.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 11:08:51 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Kaloudis", "Konstantinos", ""], ["Kevrekidis", "George A.", ""], ["Maltezou", "Helena C.", ""], ["Anastassopoulou", "Cleo", ""], ["Tsakris", "Athanasios", ""], ["Russo", "Lucia", ""]]}, {"id": "2012.14289", "submitter": "Seyedeh Azadeh Fallah Mortezanejad Dr", "authors": "A. F. Mortezanejad, G. M. Borzadaran, B. S. Gildeh", "title": "Profile control chart based on maximum entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monitoring a process over time is so important in manufacturing processes to\nreduce the wastage of money and time. The purpose of this article is to monitor\nprofile coefficients instead of a process mean. In this paper, two methods are\nproposed for monitoring the intercept and slope of the simple linear profile,\nsimultaneously. The first one is linear regression, and another one is the\nmaximum entropy principle. A simulation study is applied to compare the two\nmethods in terms of the second type of error and average run length. Finally,\ntwo real examples are presented to demonstrate the ability of the proposed\nchart.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 15:34:05 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Mortezanejad", "A. F.", ""], ["Borzadaran", "G. M.", ""], ["Gildeh", "B. S.", ""]]}, {"id": "2012.14372", "submitter": "Stefano M. Iacus", "authors": "Tiziana Carpi, Airo Hino, Stefano Maria Iacus, Giuseppe Porro", "title": "On a Japanese Subjective Well-Being Indicator Based on Twitter data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study presents for the first time the SWB-J index, a subjective\nwell-being indicator for Japan based on Twitter data. The index is composed by\neight dimensions of subjective well-being and is estimated relying on Twitter\ndata by using human supervised sentiment analysis. The index is then compared\nwith the analogous SWB-I index for Italy, in order to verify possible analogies\nand cultural differences. Further, through structural equation models, a causal\nassumption is tested to see whether the economic and health conditions of the\ncountry influence the well-being latent variable and how this latent dimension\naffects the SWB-J and SWB-I indicators. It turns out that, as expected, the\neconomic and health welfare is only one aspect of the multidimensional\nwell-being that is captured by the Twitter-based indicator.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 17:28:30 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Carpi", "Tiziana", ""], ["Hino", "Airo", ""], ["Iacus", "Stefano Maria", ""], ["Porro", "Giuseppe", ""]]}, {"id": "2012.14465", "submitter": "David K A Mordecai", "authors": "Ryan C. Saxe, Samantha Kappagoda, David K.A. Mordecai", "title": "Classification of Pathological and Normal Gait: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Gait recognition is a term commonly referred to as an identification problem\nwithin the Computer Science field. There are a variety of methods and models\ncapable of identifying an individual based on their pattern of ambulatory\nlocomotion. By surveying the current literature on gait recognition, this paper\nseeks to identify appropriate metrics, devices, and algorithms for collecting\nand analyzing data regarding patterns and modes of ambulatory movement across\nindividuals. Furthermore, this survey seeks to motivate interest in a broader\nscope of longitudinal analysis regarding the perturbations in gait across\nstates (i.e. physiological, emotive, and/or cognitive states). More broadly,\ninferences to normal versus pathological gait patterns can be attributed, based\non both longitudinal and non-longitudinal forms of classification. This may\nindicate promising research directions and experimental designs, such as\ncreating algorithmic metrics for the quantification of fatigue, or models for\nforecasting episodic disorders. Furthermore, in conjunction with other\nmeasurements of physiological and environmental conditions, pathological gait\nclassification might be applicable to inference for syndromic surveillance of\ninfectious disease states or cognitive impairment.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2020 19:56:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saxe", "Ryan C.", ""], ["Kappagoda", "Samantha", ""], ["Mordecai", "David K. A.", ""]]}, {"id": "2012.14559", "submitter": "Bhav Jain", "authors": "Bhav Jain, Sean Elliott", "title": "Correlation Across Environments Encoded by Hippocampal Place Cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The hippocampus is often attributed to episodic memory formation and storage\nin the mammalian brain; in particular, Alme et al. showed that hippocampal area\nCA3 forms statistically independent representations across a large number of\nenvironments, even if the environments share highly similar features. This lack\nof overlap between spatial maps indicates the large capacity of the CA3\ncircuitry. In this paper, we support the argument for the large capacity of the\nCA3 network. To do so, we replicate the key findings of Alme et al. and extend\nthe results by perturbing the neural activity encodings with noise and\nconducting representation similarity analysis (RSA). We find that the\ncorrelations between firing rates are partially resistant to noise, and that\nthe spatial representations across cells show similar patterns, even across\ndifferent environments. Finally, we discuss some theoretical and practical\nimplications of our results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 01:41:31 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Jain", "Bhav", ""], ["Elliott", "Sean", ""]]}, {"id": "2012.14589", "submitter": "Guanyu Hu", "authors": "Qingyang Liu, Guanyu Hu, Binqi Ye, Susan Wang, Yaoshi Wu", "title": "Sample Size Re-estimation Design in Phase II Dose Finding Study with\n  Multiple Dose Groups: Frequentist and Bayesian Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unblinded sample size re-estimation (SSR) is often planned in a clinical\ntrial when there is large uncertainty about the true treatment effect. For\nProof-of Concept (PoC) in a Phase II dose finding study, contrast test can be\nadopted to leverage information from all treatment groups. In this article, we\npropose two-stage SSR designs using frequentist conditional power and Bayesian\nposterior predictive power for both single and multiple contrast tests. The\nBayesian SSR can be implemented under a wide range of prior settings to\nincorporate different prior knowledge. Taking the adaptivity into account, all\ntype I errors of final analysis in this paper are rigorously protected.\nSimulation studies are carried out to demonstrate the advantages of unblinded\nSSR in multi-arm trials.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 03:48:42 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 02:04:29 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Liu", "Qingyang", ""], ["Hu", "Guanyu", ""], ["Ye", "Binqi", ""], ["Wang", "Susan", ""], ["Wu", "Yaoshi", ""]]}, {"id": "2012.14759", "submitter": "Seyedeh Azadeh Fallah Mortezanejad Dr", "authors": "S. A. F. Mortezanejad, G. M. Borzadaran and B. S. Gildeh", "title": "New statistical control limits using maximum copula entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Statistical quality control methods are noteworthy to produced standard\nproduction in manufacturing processes. In this regard, there are many classical\nmanners to control the process. Many of them have a global assumption around\ndistributions of the process data. They are supposed to be normal, which is\nclear that it is not always valid for all processes. Such control charts made\nsome false decisions that waste funds. So, the main question while working with\nmultivariate data set is how to find the multivariate distribution of the data\nset, which saves the original dependency between variables. Up to our\nknowledge, a copula function guarantees the dependence on the result function.\nBut it is not enough when there is no other functional information about the\nstatistical society, and we have just a data set. Therefore, we apply the\nmaximum entropy concept to deal with this situation. In this paper, first of\nall, we find out the joint distribution of a data set, which is from a\nmanufacturing process that needs to be control while running the production\nprocess. Then, we get an elliptical control limit via the maximum copula\nentropy. In the final step, we represent a practical example using the stated\nmethod. Average run lengths are calculated for some means and shifts to show\nthe ability of the maximum copula entropy. In the end, two real data examples\nare presented.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 14:15:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Mortezanejad", "S. A. F.", ""], ["Borzadaran", "G. M.", ""], ["Gildeh", "B. S.", ""]]}, {"id": "2012.14817", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Spatial Resolution Enhancement of Oversampled Images Using Regression\n  Decomposition and Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new statistical model designed for regression analysis with a sparse design\nmatrix is proposed. This new model utilizes the positions of the limited\nnon-zero elements in the design matrix to decompose the regression model into\nsub-regression models. Statistical inferences are further made on the values of\nthese limited non-zero elements to provide a reference for synthesizing these\nsub-regression models. With this concept of the regression decomposition and\nsynthesis, the information on the structure of the design matrix can be\nincorporated into the regression analysis to provide a more reliable\nestimation. The proposed model is then applied to resolve the spatial\nresolution enhancement problem for spatially oversampled images. To\nsystematically evaluate the performance of the proposed model in enhancing the\nspatial resolution, the proposed approach is applied to the oversampled images\nthat are reproduced via random field simulations. These application results\nbased on different generated scenarios then conclude the effectiveness and the\nfeasibility of the proposed approach in enhancing the spatial resolution of\nspatially oversampled images.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 15:55:11 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2012.14949", "submitter": "Luke Benz", "authors": "Luke S. Benz and Michael J. Lopez", "title": "Estimating the change in soccer's home advantage during the Covid-19\n  pandemic using bivariate Poisson regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In wake of the Covid-19 pandemic, 2019-2020 soccer seasons across the world\nwere postponed and eventually made up during the summer months of 2020.\nResearchers from a variety of disciplines jumped at the opportunity to compare\nthe rescheduled games, played in front of empty stadia, to previous games,\nplayed in front of fans. To date, most of this post-Covid soccer research has\nused linear regression models, or versions thereof, to estimate potential\nchanges to the home advantage. But because soccer outcomes are non-linear, we\nargue that leveraging the Poisson distribution would be more appropriate. We\nbegin by using simulations to show that bivariate Poisson regression reduces\nabsolute bias when estimating the home advantage benefit in a single season of\nsoccer games, relative to linear regression, by almost 85 percent. Next, with\ndata from 17 professional soccer leagues, we extend bivariate Poisson models\nestimate the change in home advantage due to games being played without fans.\nIn contrast to current research that overwhelmingly suggests a drop in the home\nadvantage, our findings are mixed; in some leagues, evidence points to a\ndecrease, while in others, the home advantage may have risen. Altogether, this\nsuggests a more complex causal mechanism for the impact of fans on sporting\nevents.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2020 21:39:19 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:25:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Benz", "Luke S.", ""], ["Lopez", "Michael J.", ""]]}, {"id": "2012.15035", "submitter": "Minkyu Shin", "authors": "Minkyu Shin, Jin Kim, Minkyung Kim", "title": "Measuring Human Adaptation to AI in Decision Making: Application to\n  Evaluate Changes after AlphaGo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC econ.GN q-fin.EC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Across a growing number of domains, human experts are expected to learn from\nand adapt to AI with superior decision making abilities. But how can we\nquantify such human adaptation to AI? We develop a simple measure of human\nadaptation to AI and test its usefulness in two case studies. In Study 1, we\nanalyze 1.3 million move decisions made by professional Go players and find\nthat a positive form of adaptation to AI (learning) occurred after the players\ncould observe the reasoning processes of AI, rather than mere actions of AI.\nThese findings based on our measure highlight the importance of explainability\nfor human learning from AI. In Study 2, we test whether our measure is\nsufficiently sensitive to capture a negative form of adaptation to AI (cheating\naided by AI), which occurred in a match between professional Go players. We\ndiscuss our measure's applications in domains other than Go, especially in\ndomains in which AI's decision making ability will likely surpass that of human\nexperts.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 04:34:46 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 18:57:08 GMT"}, {"version": "v3", "created": "Mon, 1 Feb 2021 01:33:29 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Shin", "Minkyu", ""], ["Kim", "Jin", ""], ["Kim", "Minkyung", ""]]}, {"id": "2012.15093", "submitter": "Ludwig Hothorn", "authors": "Ludwig A. Hothorn", "title": "Closed test procedures for the comparison of dose groups against a\n  negative control group or placebo", "comments": "1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Dose groups are compared with a control assuming an order restriction usually\nby the Williams trend test. Here, as an alternative, two variants of the closed\ntesting procedure are considered, one where global Williams tests are used in\nthe partition hypotheses, and another where pairwise contrast tests are used\nfor this purpose. Related R software is provided.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 09:30:20 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Hothorn", "Ludwig A.", ""]]}, {"id": "2012.15112", "submitter": "Marcos Oliveira", "authors": "Juhi Kulshrestha, Marcos Oliveira, Orkut Karacalik, Denis Bonnay,\n  Claudia Wagner", "title": "Web Routineness and Limits of Predictability: Investigating Demographic\n  and Behavioral Differences Using Web Tracking Data", "comments": "12 pages, 8 figures. To be published in the proceedings of the\n  International AAAI Conference on Web and Social Media (ICWSM) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.IT math.IT physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding human activities and movements on the Web is not only important\nfor computational social scientists but can also offer valuable guidance for\nthe design of online systems for recommendations, caching, advertising, and\npersonalization. In this work, we demonstrate that people tend to follow\nroutines on the Web, and these repetitive patterns of web visits increase their\nbrowsing behavior's achievable predictability. We present an\ninformation-theoretic framework for measuring the uncertainty and theoretical\nlimits of predictability of human mobility on the Web. We systematically assess\nthe impact of different design decisions on the measurement. We apply the\nframework to a web tracking dataset of German internet users. Our empirical\nresults highlight that individual's routines on the Web make their browsing\nbehavior predictable to 85% on average, though the value varies across\nindividuals. We observe that these differences in the users' predictabilities\ncan be explained to some extent by their demographic and behavioral attributes.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 11:19:10 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Kulshrestha", "Juhi", ""], ["Oliveira", "Marcos", ""], ["Karacalik", "Orkut", ""], ["Bonnay", "Denis", ""], ["Wagner", "Claudia", ""]]}, {"id": "2012.15130", "submitter": "Addison Hu", "authors": "Addison J. Hu, Mikael Kuusela, Ann B. Lee, Donata Giglio, Kimberly M.\n  Wood", "title": "Spatio-temporal methods for estimating subsurface ocean thermal response\n  to tropical cyclones", "comments": "33 pages, 14 figures; supplement and code at\n  https://github.com/huisaddison/tc-ocean-methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tropical cyclones (TCs), driven by heat exchange between the air and sea,\npose a substantial risk to many communities around the world. Accurate\ncharacterization of the subsurface ocean thermal response to TC passage is\ncrucial for accurate TC intensity forecasts and for an understanding of the\nrole that TCs play in the global climate system. However, that characterization\nis complicated by the high-noise ocean environment, correlations inherent in\nspatio-temporal data, relative scarcity of in situ observations, and the\nentanglement of the TC-induced signal with seasonal signals. We present a\ngeneral methodological framework that addresses these difficulties, integrating\nexisting techniques in seasonal mean field estimation, Gaussian process\nmodeling, and nonparametric regression into a functional ANOVA model.\nImportantly, we improve upon past work by properly handling seasonality,\nproviding rigorous uncertainty quantification, and treating time as a\ncontinuous variable, rather than producing estimates that are binned in time.\nThis functional ANOVA model is estimated using in situ subsurface temperature\nprofiles from the Argo fleet of autonomous floats through a multi-step\nprocedure, which (1) characterizes the upper ocean seasonal shift during the TC\nseason; (2) models the variability in the temperature observations; (3) fits a\nthin plate spline using the variability estimates to account for\nheteroskedasticity and correlation between the observations. This spline fit\nreveals the ocean thermal response to TC passage. Through this framework, we\nobtain new scientific insights into the interaction between TCs and the ocean\non a global scale, including a three-dimensional characterization of the\nnear-surface and subsurface cooling along the TC storm track and the\nmixing-induced subsurface warming on the track's right side.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 12:35:32 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 06:33:19 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 04:03:14 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Hu", "Addison J.", ""], ["Kuusela", "Mikael", ""], ["Lee", "Ann B.", ""], ["Giglio", "Donata", ""], ["Wood", "Kimberly M.", ""]]}, {"id": "2012.15302", "submitter": "Yesim Guney", "authors": "Ye\\c{s}im G\\\"uney, Yetkin Tua\\c{c}, \\c{S}enay \\\"Ozdemir, Fulya\n  G\\\"okalp Yavuz, Olcay Arslan", "title": "An analysis to identify the structural breaks of COVID-19 in Turkey", "comments": "23 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In countries with a severe outbreak of COVID-19, most governments are\nconsidering whether anti-transmission measures are worth social and economic\ncosts. The seriousness of economic costs such as the closure of some\nworkplaces, unemployment, reduction in production, and social costs such as\nschool closures, disruptions in education could be observable. However, the\neffect of the measures taken on the spread of the epidemic, such as the number\nof delayed or prevented cases, could not be observed. For this reason, the\ndirect effects of the measures taken on health, that is, the effects on the\ncourse of the epidemic, are important research subjects. For this purpose, in\nthis study, the breakpoint linear regression analysis is performed to analyze\nthe trends of daily active cases, recovered, and deaths in Turkey. The analysis\nreveals that there has been a remarkable impact on lockdown and other\nprecautions. Using the breakpoint regression model, we also analyze the active\ncases' trajectory for eight affected countries and compare the patterns in\nthese countries with Turkey.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:48:04 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["G\u00fcney", "Ye\u015fim", ""], ["Tua\u00e7", "Yetkin", ""], ["\u00d6zdemir", "\u015eenay", ""], ["Yavuz", "Fulya G\u00f6kalp", ""], ["Arslan", "Olcay", ""]]}, {"id": "2012.15306", "submitter": "Theodore Papamarkou", "authors": "Adam Spannaus, Theodore Papamarkou, Samantha Erwin, J. Blair Christian", "title": "Bayesian state space modelling for COVID-19: with Tennessee and New York\n  case studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian inferential framework for the spread of COVID-19 using\nmechanistic epidemiological models, such as SIR or SEIR, and allow the\neffective contact rate to vary in time. A novel aspect of our approach is the\nincorporation of a time-varying reporting rate accounting for the initial phase\nof the pandemic before testing was widely available. By varying both the\nreporting rate and the effective contact rate in time, our models can capture\nchanges in the data induced by external influences, such as public health\nintervention measures, for example. We view COVID-19 incidence data as the\nobserved measurements of a hidden Markov model, with latent space represented\nby the underlying epidemiological model, and employ a particle Markov chain\nMonte Carlo (PMCMC) sampling scheme for Bayesian inference. Parameter inference\nis performed via PMCMC on incidence data collated by the New York Times from\nthe states of New York and Tennessee from March 1, 2020 through August 30,\n2020. Lastly, we perform Bayesian model selection on the different formulations\nof the epidemiological models, make predictions from our fitted models, and\nvalidate our predictions against the true incidence data for the week between\nAugust 31, 2020 and September 7, 2020.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 19:52:22 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 07:52:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Spannaus", "Adam", ""], ["Papamarkou", "Theodore", ""], ["Erwin", "Samantha", ""], ["Christian", "J. Blair", ""]]}, {"id": "2012.15339", "submitter": "Florian Gerber", "authors": "Florian Gerber and Douglas W. Nychka", "title": "Fast covariance parameter estimation of spatial Gaussian process models\n  using neural networks", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are a popular model for spatially referenced data\nand allow descriptive statements, predictions at new locations, and simulation\nof new fields. Often a few parameters are sufficient to parameterize the\ncovariance function, and maximum likelihood (ML) methods can be used to\nestimate these parameters from data. ML methods, however, are computationally\ndemanding. For example, in the case of local likelihood estimation, even\nfitting covariance models on modest size windows can overwhelm typical\ncomputational resources for data analysis. This limitation motivates the idea\nof using neural network (NN) methods to approximate ML estimates. We train NNs\nto take moderate size spatial fields or variograms as input and return the\nrange and noise-to-signal covariance parameters. Once trained, the NNs provide\nestimates with a similar accuracy compared to ML estimation and at a speedup by\na factor of 100 or more. Although we focus on a specific covariance estimation\nproblem motivated by a climate science application, this work can be easily\nextended to other, more complex, spatial problems and provides a\nproof-of-concept for this use of machine learning in computational statistics.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 22:06:26 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Gerber", "Florian", ""], ["Nychka", "Douglas W.", ""]]}, {"id": "2012.15368", "submitter": "Marcos Oliveira", "authors": "Marcos Oliveira", "title": "More crime in cities? On the scaling laws of crime and the inadequacy of\n  per capita rankings -- a cross-country study", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objectives: To evaluate the relationship between population size and number\nof crimes in cities across twelve countries and assess the impact of per capita\nmeasurements on crime analyses, depending on offense type.\n  Methods: We use data on burglaries and thefts at the city level and evaluate\nthe relationship between crime numbers and population size using probabilistic\nscaling analysis. We estimate the growth exponent of each offense type and use\nKendall rank correlation to assess the impact of a linear growth assumption\n(i.e., per-capita analysis) on cities rankings.\n  Result: In nine out of eleven countries, theft increases superlinearly with\npopulation size; in two of them, it increases linearly. In eight out of ten\ncountries, burglary increases linearly with population size; in two of them, it\nincreases superlinearly. In nonlinear scenarios, using per capita rates to rank\ncities produces substantially different rankings from rankings adjusted for\npopulation size.\n  Conclusions: Comparing cities using per capita crime rates (e.g., crime per\n100,000 people per year) assumes that crime increases linearly with population\nsize. Our findings indicate, however, that this assumption is unfounded,\nimplying that one should be cautious when using per capita rankings. When crime\nincreases nonlinearly with population, per capita rates do not remove\npopulation effects. The contrasting crime growth of burglary and theft also\nsuggests that different crime dynamics at the local level lead to different\nmacro-level features in cities.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2020 23:36:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Oliveira", "Marcos", ""]]}]