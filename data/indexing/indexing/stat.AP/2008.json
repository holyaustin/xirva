[{"id": "2008.00048", "submitter": "Emiliano Valdez", "authors": "Guojun Gan and Emiliano A. Valdez", "title": "Analysis of Prescription Drug Utilization with Beta Regression Models", "comments": "26 pages, 10 Figures, 11 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The healthcare sector in the U.S. is complex and is also a large sector that\ngenerates about 20% of the country's gross domestic product. Healthcare\nanalytics has been used by researchers and practitioners to better understand\nthe industry. In this paper, we examine and demonstrate the use of Beta\nregression models to study the utilization of brand name drugs in the U.S. to\nunderstand the variability of brand name drug utilization across different\nareas. The models are fitted to public datasets obtained from the Medicare &\nMedicaid Services and the Internal Revenue Service. Integrated Nested Laplace\nApproximation (INLA) is used to perform the inference. The numerical results\nshow that Beta regression models can fit the brand name drug claim rates well\nand including spatial dependence improves the performance of the Beta\nregression models. Such models can be used to reflect the effect of\nprescription drug utilization when updating an insured's health risk in a risk\nscoring model.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:28:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Gan", "Guojun", ""], ["Valdez", "Emiliano A.", ""]]}, {"id": "2008.00049", "submitter": "Qi Yang", "authors": "Qi Yang, Khizar Qureshi, Tauhid Zaman", "title": "Mitigating the Backfire Effect Using Pacing and Leading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks create echo-chambers where people are infrequently\nexposed to opposing opinions. Even if such exposure occurs, the persuasive\neffect may be minimal or nonexistent. Recent studies have shown that exposure\nto opposing opinions causes a backfire effect, where people become more\nsteadfast in their original beliefs. We conducted a longitudinal field\nexperiment on Twitter to test methods that mitigate the backfire effect while\nexposing people to opposing opinions. Our subjects were Twitter users with\nanti-immigration sentiment. The backfire effect was defined as an increase in\nthe usage frequency of extreme anti-immigration language in the subjects'\nposts. We used automated Twitter accounts, or bots, to apply different\ntreatments to the subjects. One bot posted only pro-immigration content, which\nwe refer to as arguing. Another bot initially posted anti-immigration content,\nthen gradually posted more pro-immigration content, which we refer to as pacing\nand leading. We also applied a contact treatment in conjunction with the\nmessaging based methods, where the bots liked the subjects' posts. We found\nthat the most effective treatment was a combination of pacing and leading with\ncontact. The least effective treatment was arguing with contact. In fact,\narguing with contact consistently showed a backfire effect relative to a\ncontrol group. These findings have many limitations, but they still have\nimportant implications for the study of political polarization, the backfire\neffect, and persuasion in online social networks.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:32:58 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Yang", "Qi", ""], ["Qureshi", "Khizar", ""], ["Zaman", "Tauhid", ""]]}, {"id": "2008.00127", "submitter": "Jinghao Sun", "authors": "Jinghao Sun (1), Luk Van Baelen (2), Els Plettinckx (2), Forrest W.\n  Crawford (1 and 3 and 4 and 5) ((1) Department of Biostatistics, Yale School\n  of Public Health, (2) Directorate of Epidemiology and Public Health,\n  Sciensano, Belgium, (3) Department of Statistics and Data Science, Yale\n  University, (4) Department of Ecology and Evolutionary Biology, Yale\n  University, (5) Yale School of Management)", "title": "Partial identification and dependence-robust confidence intervals for\n  capture-recapture surveys", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Capture-recapture (CRC) surveys are widely used to estimate the size of a\npopulation whose members cannot be enumerated directly. When $k$ capture\nsamples are obtained, counts of unit captures in subsets of samples are\nrepresented naturally by a $2^k$ contingency table in which one element -- the\nnumber of individuals appearing in none of the samples -- remains unobserved.\nIn the absence of additional assumptions, the population size is not\npoint-identified. Assumptions about independence between samples are often used\nto achieve point-identification. However, real-world CRC surveys often use\nconvenience samples in which independence cannot be guaranteed, and population\nsize estimates under independence assumptions may lack empirical credibility.\nIn this work, we apply the theory of partial identification to show that weak\nassumptions or qualitative knowledge about the nature of dependence between\nsamples can be used to characterize a non-trivial set in which the true\npopulation size lies with high probability. We construct confidence sets for\nthe population size under bounds on pairwise capture probabilities, and bounds\non the highest order interaction term in a log-linear model using two methods:\ntest inversion bootstrap confidence intervals, and profile likelihood\nconfidence intervals. We apply these methods to recent survey data to estimate\nthe number of people who inject drugs in Brussels, Belgium.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 23:56:23 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Sun", "Jinghao", "", "1 and 3 and 4 and 5"], ["Van Baelen", "Luk", "", "1 and 3 and 4 and 5"], ["Plettinckx", "Els", "", "1 and 3 and 4 and 5"], ["Crawford", "Forrest W.", "", "1 and 3 and 4 and 5"]]}, {"id": "2008.00235", "submitter": "Alessandra Cabassi", "authors": "Alessandra Cabassi, Denis Seyres, Mattia Frontini, Paul D. W. Kirk", "title": "Two-step penalised logistic regression for multi-omic data with an\n  application to cardiometabolic syndrome", "comments": "Manuscript: 22 pages, 6 figures. Supplement: 24 pages, 20 figures.\n  For associated R code, see\n  https://github.com/acabassi/logistic-regression-for-multi-omic-data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building classification models that predict a binary class label on the basis\nof high dimensional multi-omics datasets poses several challenges, due to the\ntypically widely differing characteristics of the data layers in terms of\nnumber of predictors, type of data, and levels of noise. Previous research has\nshown that applying classical logistic regression with elastic-net penalty to\nthese datasets can lead to poor results (Liu et al., 2018). We implement a\ntwo-step approach to multi-omic logistic regression in which variable selection\nis performed on each layer separately and a predictive model is then built\nusing the variables selected in the first step. Here, our approach is compared\nto other methods that have been developed for the same purpose, and we adapt\nexisting software for multi-omic linear regression (Zhao and Zucknick, 2020) to\nthe logistic regression setting. Extensive simulation studies show that our\napproach should be preferred if the goal is to select as many relevant\npredictors as possible, as well as achieving prediction performances comparable\nto those of the best competitors. Our motivating example is a cardiometabolic\nsyndrome dataset comprising eight 'omic data types for 2 extreme phenotype\ngroups (10 obese and 10 lipodystrophy individuals) and 185 blood donors. Our\nproposed approach allows us to identify features that characterise\ncardiometabolic syndrome at the molecular level. R code is available at\nhttps://github.com/acabassi/logistic-regression-for-multi-omic-data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:36:27 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Seyres", "Denis", ""], ["Frontini", "Mattia", ""], ["Kirk", "Paul D. W.", ""]]}, {"id": "2008.00253", "submitter": "Kevin McKinney", "authors": "Kevin L. McKinney and John M. Abowd", "title": "Male Earnings Volatility in LEHD before, during, and after the Great\n  Recession", "comments": "This versions has minor edits to the introduction, conclusion and\n  references to clarify the context of the four coordinated papers on this\n  subject", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is part of a coordinated collection of papers on prime-age male\nearnings volatility. Each paper produces a similar set of statistics for the\nsame reference population using a different primary data source. Our primary\ndata source is the Census Bureau's Longitudinal Employer-Household Dynamics\n(LEHD) infrastructure files. Using LEHD data from 1998 to 2016, we create a\nwell-defined population frame to facilitate accurate estimation of temporal\nchanges comparable to designed longitudinal samples of people. We show that\nearnings volatility, excluding increases during recessions, has declined over\nthe analysis period, a finding robust to various sensitivity analyses. Although\nwe find volatility is declining, the effect is not homogeneous, particularly\nfor workers with tenuous labor force attachment for whom volatility is\nincreasing. These \"not stable\" workers have earnings volatility approximately\n30 times larger than stable workers, but more important for earnings volatility\ntrends we observe a large increase in the share of stable employment from 60%\nin 1998 to 67% in 2016, which we show to largely be responsible for the decline\nin overall earnings volatility. To further emphasize the importance of not\nstable and/or low earning workers we also conduct comparisons with the PSID and\nshow how changes over time in the share of workers at the bottom tail of the\ncross-sectional earnings distributions can produce either declining or\nincreasing earnings volatility trends.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 11:58:43 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 17:46:55 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["McKinney", "Kevin L.", ""], ["Abowd", "John M.", ""]]}, {"id": "2008.00262", "submitter": "Amanda Fern\\'andez-Fontelo Dr.", "authors": "Amanda Fern\\'andez-Fontelo, David Mori\\~na, Alejandra Caba\\~na,\n  Argimiro Arratia, Pere Puig", "title": "Estimating the real burden of disease under a pandemic situation: The\n  SARS-CoV2 case", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0242956", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper introduces a new model used to study and analyse the severe\nacute respiratory syndrome coronavirus 2 (SARS-CoV2) epidemic-reported-data\nfrom Spain. This is a Hidden Markov Model whose hidden layer is a regeneration\nprocess with Poisson immigration, Po-INAR(1), together with a mechanism that\nallows the estimation of the under-reporting in non-stationary count time\nseries. A novelty of the model is that the expectation of the innovations in\nthe unobserved process is a time-dependent function defined in such a way that\ninformation about the spread of an epidemic, as modelled through a\nSusceptible-Infectious-Removed dynamical system, is incorporated into the\nmodel. In addition, the parameter controlling the intensity of the\nunder-reporting is also made to vary with time to adjust to possible\nseasonality or trend in the data. Maximum likelihood methods are used to\nestimate the parameters of the model.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 13:18:48 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Fern\u00e1ndez-Fontelo", "Amanda", ""], ["Mori\u00f1a", "David", ""], ["Caba\u00f1a", "Alejandra", ""], ["Arratia", "Argimiro", ""], ["Puig", "Pere", ""]]}, {"id": "2008.00298", "submitter": "Daniel Sacks", "authors": "Daniel W. Sacks, Nir Menachemi, Peter Embi, Coady Wing", "title": "What can we learn about SARS-CoV-2 prevalence from testing and hospital\n  data?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Measuring the prevalence of active SARS-CoV-2 infections in the general\npopulation is difficult because tests are conducted on a small and non-random\nsegment of the population. However, people admitted to the hospital for\nnon-COVID reasons are tested at very high rates, even though they do not appear\nto be at elevated risk of infection. This sub-population may provide valuable\nevidence on prevalence in the general population. We estimate upper and lower\nbounds on the prevalence of the virus in the general population and the\npopulation of non-COVID hospital patients under weak assumptions on who gets\ntested, using Indiana data on hospital inpatient records linked to SARS-CoV-2\nvirological tests. The non-COVID hospital population is tested fifty times as\noften as the general population, yielding much tighter bounds on prevalence. We\nprovide and test conditions under which this non-COVID hospitalization bound is\nvalid for the general population. The combination of clinical testing data and\nhospital records may contain much more information about the state of the\nepidemic than has been previously appreciated. The bounds we calculate for\nIndiana could be constructed at relatively low cost in many other states.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 17:13:28 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 19:15:41 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sacks", "Daniel W.", ""], ["Menachemi", "Nir", ""], ["Embi", "Peter", ""], ["Wing", "Coady", ""]]}, {"id": "2008.00339", "submitter": "Albert Lee III", "authors": "Albert. H. Lee III, Edward L Boone, Roy T. Sabo, and Erin Donahue", "title": "Application of Bayesian Dynamic Linear Models to Random Allocation\n  Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random allocation models used in clinical trials aid researchers in\ndetermining which of a particular treatment provides the best results by\nreducing bias between groups. Often however, this determination leaves\nresearchers battling ethical issues of providing patients with unfavorable\ntreatments. Many methods such as Play the Winner and Randomized Play the Winner\nRule have historically been utilized to determine patient allocation, however,\nthese methods are prone to the increased assignment of unfavorable treatments.\nRecently a new Bayesian Method using Decreasingly Informative Priors has been\nproposed by \\citep{sabo2014adaptive}, and later \\citep{donahue2020allocation}.\nYet this method can be time consuming if MCMC methods are required. We propose\nthe use of a new method which uses Dynamic Linear Model (DLM)\n\\citep{harrison1999bayesian} to increase allocation speed while also decreasing\npatient allocation samples necessary to identify the more favorable treatment.\nFurthermore, a sensitivity analysis is conducted on multiple parameters.\nFinally, a Bayes Factor is calculated to determine the proportion of unused\npatient budget remaining at a specified cut off and this will be used to\ndetermine decisive evidence in favor of the better treatment.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 20:47:08 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Lee", "Albert. H.", "III"], ["Boone", "Edward L", ""], ["Sabo", "Roy T.", ""], ["Donahue", "Erin", ""]]}, {"id": "2008.00375", "submitter": "Hamid Eftekhari", "authors": "Hamid Eftekhari, Debarghya Mukherjee, Moulinath Banerjee, Ya'acov\n  Ritov", "title": "Markovian And Non-Markovian Processes with Active Decision Making\n  Strategies For Addressing The COVID-19 Pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study and predict the evolution of Covid-19 in six US states from the\nperiod May 1 through August 31 using a discrete compartment-based model and\nprescribe active intervention policies, like lockdowns, on the basis of\nminimizing a loss function, within the broad framework of partially observed\nMarkov decision processes. For each state, Covid-19 data for 40 days (starting\nfrom May 1 for two northern states and June 1 for four southern states) are\nanalyzed to estimate the transition probabilities between compartments and\nother parameters associated with the evolution of the epidemic. These\nquantities are then used to predict the course of the epidemic in the given\nstate for the next 50 days (test period) under various policy allocations,\nleading to different values of the loss function over the training horizon. The\noptimal policy allocation is the one corresponding to the smallest loss. Our\nanalysis shows that none of the six states need lockdowns over the test period,\nthough the no lockdown prescription is to be interpreted with caution:\nresponsible mask use and social distancing of course need to be continued. The\ncaveats involved in modeling epidemic propagation of this sort are discussed at\nlength. A sketch of a non-Markovian formulation of Covid-19 propagation (and\nmore general epidemic propagation) is presented as an attractive avenue for\nfuture research in this area.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 01:14:18 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 17:47:27 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Eftekhari", "Hamid", ""], ["Mukherjee", "Debarghya", ""], ["Banerjee", "Moulinath", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "2008.00400", "submitter": "Jialiang Mao", "authors": "Jialiang Mao, Li Ma", "title": "Dirichlet-tree multinomial mixtures for clustering microbiome\n  compositions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the human microbiome has gained substantial interest in recent\nyears, and a common task in the analysis of these data is to cluster microbiome\ncompositions into subtypes. This subdivision of samples into subgroups serves\nas an intermediary step in achieving personalized diagnosis and treatment. In\napplying existing clustering methods to modern microbiome studies including the\nAmerican Gut Project (AGP) data, we found that this seemingly standard task,\nhowever, is very challenging in the microbiome composition context due to\nseveral key features of such data. Standard distance-based clustering\nalgorithms generally do not produce reliable results as they do not take into\naccount the heterogeneity of the cross-sample variability among the bacterial\ntaxa, while existing model-based approaches do not allow sufficient flexibility\nfor the identification of complex within-cluster variation from cross-cluster\nvariation. Direct applications of such methods generally lead to overly\ndispersed clusters in the AGP data and such phenomenon is common for other\nmicrobiome data. To overcome these challenges, we introduce Dirichlet-tree\nmultinomial mixtures (DTMM) as a Bayesian generative model for clustering\namplicon sequencing data in microbiome studies. DTMM models the microbiome\npopulation with a mixture of Dirichlet-tree kernels that utilizes the\nphylogenetic tree to offer a more flexible covariance structure in\ncharacterizing within-cluster variation, and it provides a means for\nidentifying a subset of signature taxa that distinguish the clusters. We\nperform extensive simulation studies to evaluate the performance of DTMM and\ncompare it to state-of-the-art model-based and distance-based clustering\nmethods in the microbiome context. Finally, we report a case study on the fecal\ndata from the AGP to identify compositional clusters among individuals with\ninflammatory bowel disease and diabetes.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 05:17:02 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 07:11:36 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Mao", "Jialiang", ""], ["Ma", "Li", ""]]}, {"id": "2008.00522", "submitter": "Baolei Wei", "authors": "Baolei Wei, Naiming Xie", "title": "On unified framework for continuous-time grey models: an integral\n  matching perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since most of the research about grey forecasting models is focused on\ndeveloping novel models and improving accuracy, relatively limited attention\nhas been paid to the modelling mechanism and relationships among diverse kinds\nof models. This paper aims to unify and reconstruct continuous-time grey\nmodels, highlighting the differences and similarities among different models.\nFirst, the unified form of grey models is proposed and simplified into a\nreduced-order ordinary differential equation. Then, the integral matching that\nconsists of integral transformation and least squares, is proposed to estimate\nthe structural parameter and initial value simultaneously. The cumulative sum\noperator, an essential element in grey modelling, proves to be the discrete\napproximation of the integral transformation formula. Next, grey models are\nreconstructed by the integral matching-based ordinary differential equations.\nFinally, the existing grey models are compared with the reconstructed models\nthrough extensive simulation studies, and a real-world example shows how to\napply and further verify the reconstructed model.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 17:00:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wei", "Baolei", ""], ["Xie", "Naiming", ""]]}, {"id": "2008.00547", "submitter": "Arvind Krishna", "authors": "Arvind Krishna (1), V. Roshan Joseph (1), Shan Ba (2), William A.\n  Brenneman (3), William R. Myers (3) ((1) Georgia Institute of Technology, (2)\n  LinkedIn Corporation, (3) Procter & Gamble Company)", "title": "Robust Experimental Designs for Model Calibration", "comments": "25 pages, 10 figures", "journal-ref": null, "doi": "10.1080/00224065.2021.1930618", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A computer model can be used for predicting an output only after specifying\nthe values of some unknown physical constants known as calibration parameters.\nThe unknown calibration parameters can be estimated from real data by\nconducting physical experiments. This paper presents an approach to optimally\ndesign such a physical experiment. The problem of optimally designing physical\nexperiment, using a computer model, is similar to the problem of finding\noptimal design for fitting nonlinear models. However, the problem is more\nchallenging than the existing work on nonlinear optimal design because of the\npossibility of model discrepancy, that is, the computer model may not be an\naccurate representation of the true underlying model. Therefore, we propose an\noptimal design approach that is robust to potential model discrepancies. We\nshow that our designs are better than the commonly used physical experimental\ndesigns that do not make use of the information contained in the computer model\nand other nonlinear optimal designs that ignore potential model discrepancies.\nWe illustrate our approach using a toy example and a real example from\nindustry.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 19:31:18 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Krishna", "Arvind", ""], ["Joseph", "V. Roshan", ""], ["Ba", "Shan", ""], ["Brenneman", "William A.", ""], ["Myers", "William R.", ""]]}, {"id": "2008.00559", "submitter": "Chris von Csefalvay", "authors": "Chris von Csefalvay", "title": "Vector quantisation and partitioning of COVID-19 temporal dynamics in\n  the United States", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical dynamics of a pathogen within a population depend on a range\nof factors: population density, the effectiveness and investment into social\ndistancing, public policy measures and non-pharmaceutical interventions (NPIs)\nare only some examples of factors that influence the number of cases over time\nby state. This paper outlines an analysis of time series vector quantisation\nand paritioning of COVID-19 cases in the United States, using a soft-DTW\n(Dynamic Time Warping) k-means clustering and a k-shape based clustering\nalgorithm to identify internally consistent clusters of case counts over time.\nThe identification of characteristic types of time-dependent variations can\nlead to the identification of patterns within sets of time series. This, in\nturn, can help discern the future of infectious dynamics in an area and,\nthrough identifying the most likely cluster-wise trajectory by calculating the\ncluster barycenter, inform public health decision-making.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 20:12:14 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["von Csefalvay", "Chris", ""]]}, {"id": "2008.00568", "submitter": "Patrick Toman", "authors": "Patrick Toman, Jingyue Zhang, Nalini Ravishanker, Karthik Konduri", "title": "Spatiotemporal Analysis of Ridesourcing and Taxi Demand by Taxi zones in\n  New York City", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The burst of demand for TNCs has significantly changed the transportation\nlandscape and dramatically disrupted the Vehicle for Hire (VFH) market that\nused to be dominated by taxicabs for many years. Since first being introduced\nby Uber in 2009, ridesourcing companies have rapidly penetrated the market.\nThis paper aims to investigate temporal and spatial patterns in taxi and TNC\nusage based on data at the taxi zone level in New York City. Specifically, we\nfit suitable time series models to estimate the temporal patterns. Next, we\nfilter out the temporal effects and investigate spatial dependence in the\nresiduals using global and local Moran's I statistics. We discuss the\nrelationship between the spatial correlations and the demographic and land use\neffects at the taxi zone level. Estimating and removing these effects via a\nmultiple linear regression (MLR) model and recomputing the Moran's I statistics\non the resulting residuals enables us to investigate spatial dependence after\naccounting for these effects. Our analysis indicates interesting patterns in\nspatial correlations between taxi zones in NYC and over time, indicating that\npredictive modeling of ridesourcing usage must incorporate both temporal and\nspatial dependence.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 21:26:13 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Toman", "Patrick", ""], ["Zhang", "Jingyue", ""], ["Ravishanker", "Nalini", ""], ["Konduri", "Karthik", ""]]}, {"id": "2008.00615", "submitter": "Jinjian Mu", "authors": "Jinjian Mu, Qingyang Liu, Lynn Kuo, Guanyu Hu", "title": "Bayesian Variable Selection for Cox Regression Model with Spatially\n  Varying Coefficients with Applications to Louisiana Respiratory Cancer Data", "comments": null, "journal-ref": null, "doi": "10.1002/bimj.202000047", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox regression model is a commonly used model in survival analysis. In\npublic health studies, clinical data are often collected from medical service\nproviders of different locations. There are large geographical variations in\nthe covariate effects on survival rates from particular diseases. In this\npaper, we focus on the variable selection issue for the Cox regression model\nwith spatially varying coefficients. We propose a Bayesian hierarchical model\nwhich incorporates a horseshoe prior for sparsity and a point mass mixture\nprior to determine whether a regression coefficient is spatially varying. An\nefficient two-stage computational method is used for posterior inference and\nvariable selection. It essentially applies the existing method for maximizing\nthe partial likelihood for the Cox model by site independently first, and then\napplying an MCMC algorithm for variable selection based on results of the first\nstage. Extensive simulation studies are carried out to examine the empirical\nperformance of the proposed method. Finally, we apply the proposed methodology\nto analyzing a real data set on respiratory cancer in Louisiana from the SEER\nprogram.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 02:32:03 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 20:53:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Mu", "Jinjian", ""], ["Liu", "Qingyang", ""], ["Kuo", "Lynn", ""], ["Hu", "Guanyu", ""]]}, {"id": "2008.00718", "submitter": "Ekaterina Krymova", "authors": "Denis Belomestny, Ekaterina Krymova, Andrey Polbin", "title": "Estimating TVP-VAR models with time invariant long-run multipliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The main goal of this paper is to develop a methodology for estimating time\nvarying parameter vector auto-regression (TVP-VAR) models with a timeinvariant\nlong-run relationship between endogenous variables and changes in exogenous\nvariables. We propose a Gibbs sampling scheme for estimation of model\nparameters as well as time-invariant long-run multiplier parameters. Further we\ndemonstrate the applicability of the proposed method by analyzing examples of\nthe Norwegian and Russian economies based on the data on real GDP, real\nexchange rate and real oil prices. Our results show that incorporating the time\ninvariance constraint on the long-run multipliers in TVP-VAR model helps to\nsignificantly improve the forecasting performance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 08:45:01 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Belomestny", "Denis", ""], ["Krymova", "Ekaterina", ""], ["Polbin", "Andrey", ""]]}, {"id": "2008.00903", "submitter": "Charlie Pilgrim", "authors": "Charlie Pilgrim, Thomas T Hills", "title": "Bias in Zipf's Law Estimators", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevailing maximum likelihood estimators for inferring power law models\nfrom rank-frequency data are biased. The source of this bias is an\ninappropriate likelihood function. The correct likelihood function is derived\nand shown to be computationally intractable. A more computationally efficient\nmethod of approximate Bayesian computation (ABC) is explored. This method is\nshown to have less bias for data generated from idealised rank-frequency\nZipfian distributions. However, the existing estimators and the ABC estimator\ndescribed here assume that words are drawn from a simple probability\ndistribution, while language is a much more complex process. We show that this\nfalse assumption leads to continued biases when applying any of these methods\nto natural language to estimate Zipf exponents. We recommend that researchers\nbe aware of these biases when investigating power laws in rank-frequency data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 14:33:21 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2021 16:18:37 GMT"}, {"version": "v3", "created": "Wed, 12 May 2021 15:17:01 GMT"}, {"version": "v4", "created": "Mon, 26 Jul 2021 10:50:39 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Pilgrim", "Charlie", ""], ["Hills", "Thomas T", ""]]}, {"id": "2008.00991", "submitter": "Matthew Martell", "authors": "Matthew Martell (1), Scott Miles (2), Youngjun Choe (1) ((1)\n  University of Washington Department of Industrial and Systems Engineering,(2)\n  University of Washington Department of Human Centered Design and Engineering)", "title": "Modeling of Lifeline Infrastructure Restoration Using Empirical\n  Quantitative Data", "comments": "29 pages (42 including citations), 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disaster recovery is widely regarded as the least understood phase of the\ndisaster cycle. In particular, the literature around lifeline infrastructure\nrestoration modeling frequently mentions the lack of empirical quantitative\ndata available. Despite limitations, there is a growing body of research on\nmodeling lifeline infrastructure restoration, often developed using empirical\nquantitative data. This study reviews this body of literature and identifies\nthe data collection and usage patterns present across modeling approaches to\ninform future efforts using empirical quantitative data. We classify the\nmodeling approaches into simulation, optimization, and statistical modeling.\nThe number of publications in this domain has increased over time with the most\nrapid growth of statistical modeling. Electricity infrastructure restoration is\nmost frequently modeled, followed by the restoration of multiple\ninfrastructures, water infrastructure, and transportation infrastructure.\nInterdependency between multiple infrastructures is increasingly considered in\nrecent literature. Researchers gather the data from a variety of sources,\nincluding collaborations with utility companies, national databases, and\npost-event damage and restoration reports. This study provides discussion and\nrecommendations around data usage practices within the lifeline restoration\nmodeling field. Following the recommendations would facilitate the development\nof a community of practice around restoration modeling and provide greater\nopportunities for future data sharing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:20:36 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Martell", "Matthew", ""], ["Miles", "Scott", ""], ["Choe", "Youngjun", ""]]}, {"id": "2008.01013", "submitter": "Parker Lamb", "authors": "Parker Lamb, Alexander Millar, Ramon Fuentes", "title": "Swipe dynamics as a means of authentication: results from a Bayesian\n  unsupervised approach", "comments": "9 pages, 7 figures; Layout and editing improved", "journal-ref": null, "doi": "10.1109/IJCB48548.2020.9304876", "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of behavioural biometrics stands as an appealing alternative to\nmore traditional biometric systems due to the ease of use from a user\nperspective and potential robustness to presentation attacks. This paper\nfocuses its attention to a specific type of behavioural biometric utilising\nswipe dynamics, also referred to as touch gestures. In touch gesture\nauthentication, a user swipes across the touchscreen of a mobile device to\nperform an authentication attempt. A key characteristic of touch gesture\nauthentication and new behavioural biometrics in general is the lack of\navailable data to train and validate models. From a machine learning\nperspective, this presents the classic curse of dimensionality problem and the\nmethodology presented here focuses on Bayesian unsupervised models as they are\nwell suited to such conditions. This paper presents results from a set of\nexperiments consisting of 38 sessions with labelled victim as well as blind and\nover-the-shoulder presentation attacks. Three models are compared using this\ndataset; two single-mode models: a shrunk covariance estimate and a Bayesian\nGaussian distribution, as well as a Bayesian non-parametric infinite mixture of\nGaussians, modelled as a Dirichlet Process. Equal error rates (EER) for the\nthree models are compared and attention is paid to how these vary across the\ntwo single-mode models at differing numbers of enrolment samples.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jul 2020 16:53:28 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 13:04:11 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lamb", "Parker", ""], ["Millar", "Alexander", ""], ["Fuentes", "Ramon", ""]]}, {"id": "2008.01019", "submitter": "Zoe Guan", "authors": "Zoe Guan, Theodore Huang, Anne Marie McCarthy, Kevin S. Hughes, Alan\n  Semine, Hajime Uno, Lorenzo Trippa, Giovanni Parmigiani, Danielle Braun", "title": "Combining Breast Cancer Risk Prediction Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate risk stratification is key to reducing cancer morbidity through\ntargeted screening and preventative interventions. Numerous breast cancer risk\nprediction models have been developed, but they often give predictions with\nconflicting clinical implications. Integrating information from different\nmodels may improve the accuracy of risk predictions, which would be valuable\nfor both clinicians and patients. BRCAPRO and BCRAT are two widely used models\nbased on largely complementary sets of risk factors. BRCAPRO is a Bayesian\nmodel that uses detailed family history information to estimate the probability\nof carrying a BRCA1/2 mutation, as well as future risk of breast and ovarian\ncancer, based on mutation prevalence and penetrance (age-specific probability\nof developing cancer given genotype). BCRAT uses a relative hazard model based\non first-degree family history and non-genetic risk factors. We consider two\napproaches for combining BRCAPRO and BCRAT: 1) modifying the penetrance\nfunctions in BRCAPRO using relative hazard estimates from BCRAT, and 2)\ntraining an ensemble model that takes as input BRCAPRO and BCRAT predictions.\nWe show that the combination models achieve performance gains over BRCAPRO and\nBCRAT in simulations and data from the Cancer Genetics Network.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 17:36:21 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Guan", "Zoe", ""], ["Huang", "Theodore", ""], ["McCarthy", "Anne Marie", ""], ["Hughes", "Kevin S.", ""], ["Semine", "Alan", ""], ["Uno", "Hajime", ""], ["Trippa", "Lorenzo", ""], ["Parmigiani", "Giovanni", ""], ["Braun", "Danielle", ""]]}, {"id": "2008.01030", "submitter": "Farzali Izadi", "authors": "Farzali Izadi", "title": "Generalized additive models to capture the death rates in Canada\n  COVID-19", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  To capture the death rates and strong weekly, biweekly and probably monthly\npatterns in the Canada COVID-19, we utilize the generalized additive models in\nthe absence of direct statistically based measurement of infection rates. By\nexamining the death rates of Canada in general and Quebec, Ontario and Alberta\nin particular, one can easily figured out that there are substantial\noverdispersion relative to the Poisson so that the negative binomial\ndistribution is an appropriate choice for the analysis. Generalized additive\nmodels (GAMs) are one of the main modeling tools for data analysis. GAMs can\nefficiently combine different types of fixed, random and smooth terms in the\nlinear predictor of a regression model to account for different types of\neffects. GAMs are a semi-parametric extension of the generalized linear models\n(GLMs), used often for the case when there is no a priori reason for choosing a\nparticular response function such as linear, quadratic, etc. and need the data\nto 'speak for themselves'. GAMs do this via the smoothing functions and take\neach predictor variable in the model and separate it into sections delimited by\n'knots', and then fit polynomial functions to each section separately, with the\nconstraint that there are no links at the knots - second derivatives of the\nseparate functions are equal at the knots.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jul 2020 05:14:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Izadi", "Farzali", ""]]}, {"id": "2008.01176", "submitter": "Ezequiel Alvarez", "authors": "Ezequiel Alvarez (ICAS, Argentina) and Franco Marsico (Health\n  Ministry, Buenos Aires)", "title": "COVID-19 mild cases determination from correlating COVID-line calls to\n  reported cases", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": "ICAS 051/20", "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: One of the most challenging keys to understand COVID-19 evolution\nis to have a measure on those mild cases which are never tested because their\nfew symptoms are soft and/or fade away soon. The problem is not only that they\nare difficult to identify and test, but also that it is believed that they may\nconstitute the bulk of the cases and could be crucial in the pandemic equation.\n  Methods: We present a novel algorithm to extract the number of these mild\ncases by correlating a COVID-line calls to reported cases in given districts.\nThe key assumption is to realize that, being a highly contagious disease, the\nnumber of calls by mild cases should be proportional to the number of reported\ncases. Whereas a background of calls not related to infected people should be\nproportional to the district population.\n  Results: We find that for Buenos Aires Province, in addition to the\nbackground, there are in signal 6.6 +/- 0.4 calls per each reported COVID-19\ncase. Using this we estimate in Buenos Aires Province 20 +/- 2 COVID-19\nsymptomatic cases for each one reported.\n  Conclusions: A very simple algorithm that models the COVID-line calls as sum\nof signal plus background allows to estimate the crucial number of the rate of\nsymptomatic to reported COVID-19 cases in a given district. The result from\nthis method is an early and inexpensive estimate and should be contrasted to\nother methods such as serology and/or massive testing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 20:17:53 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Alvarez", "Ezequiel", "", "ICAS, Argentina"], ["Marsico", "Franco", "", "Health\n  Ministry, Buenos Aires"]]}, {"id": "2008.01200", "submitter": "Han Yu", "authors": "Han Yu, Alan D. Hutson", "title": "A Robust Spearman Correlation Coefficient Permutation Test", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we show that Spearman's correlation coefficient test about\n$H_0:\\rho_s=0$ found in most statistical software packages is theoretically\nincorrect and performs poorly when bivariate normality assumptions are not met\nor the sample size is small. The historical works about these tests make an\nunverifiable assumption that the approximate bivariate normality of original\ndata justifies using classic approximations. In general, there is common\nmisconception that the tests about $\\rho_s=0$ are robust to deviations from\nbivariate normality. In fact, we found under certain scenarios violation of the\nbivariate normality assumption has severe effects on type I error control for\nthe most commonly utilized tests. To address this issue, we developed a robust\npermutation test for testing the general hypothesis $H_0: \\rho_s=0$. The\nproposed test is based on an appropriately studentized statistic. We will show\nthat the test is theoretically asymptotically valid in the general setting when\ntwo paired variables are uncorrelated but dependent. This desired property was\ndemonstrated across a range of distributional assumptions and sample sizes in\nsimulation studies, where the proposed test exhibits robust type I error\ncontrol across a variety of settings, even when the sample size is small. We\ndemonstrated the application of this test in real world examples of\ntranscriptomic data of the TCGA breast cancer patients and a data set of PSA\nlevels and age.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 21:17:59 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Yu", "Han", ""], ["Hutson", "Alan D.", ""]]}, {"id": "2008.01479", "submitter": "Ingrid Kockum", "authors": "Jesse Huang, Ingrid Kockum, Pernilla Stridh", "title": "Interaction between two exposures: determining odds ratios and\n  confidence intervals for risk estimates", "comments": "9 pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological research, it is common to investigate the interaction\nbetween risk factors for an outcome such as a disease and hence to estimate the\nrisk associated with being exposed for either or both of two risk factors under\ninvestigation. Interactions can be estimated both on the additive and\nmultiplicative scale using the same regression model. We here present a review\nfor calculating interaction and estimating the risk and confidence interval of\ntwo exposures using a single regression model and the relationship between\nmeasures, particularly the standard error for the combined exposure risk group.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:11:30 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Huang", "Jesse", ""], ["Kockum", "Ingrid", ""], ["Stridh", "Pernilla", ""]]}, {"id": "2008.01485", "submitter": "Jose Fontanari", "authors": "Sandro M. Reia and Jos\\'e F. Fontanari", "title": "Wisdom of crowds: much ado about nothing", "comments": null, "journal-ref": null, "doi": "10.1088/1742-5468/abfa1f", "report-no": null, "categories": "stat.AP cs.MA physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The puzzling idea that the combination of independent estimates of the\nmagnitude of a quantity results in a very accurate prediction, which is\nsuperior to any or, at least, to most of the individual estimates is known as\nthe wisdom of crowds. Here we use the Federal Reserve Bank of Philadelphia's\nSurvey of Professional Forecasters database to confront the statistical and\npsychophysical explanations of this phenomenon. Overall we find that the data\ndo not support any of the proposed explanations of the wisdom of crowds. In\nparticular, we find a positive correlation between the variance (or diversity)\nof the estimates and the crowd error in disagreement with some interpretations\nof the diversity prediction theorem. In addition, contra the predictions of the\npsychophysical augmented quincunx model, we find that the skew of the estimates\noffers no information about the crowd error. More importantly, we find that the\ncrowd beats all individuals in less than 2% of the forecasts and beats most\nindividuals in less than 70% of the forecasts, which means that there is a\nsporting chance that an individual selected at random will perform better than\nthe crowd. These results contrast starkly with the performance of non-natural\ncrowds composed of unbiased forecasters which beat most individuals in\npractically all forecasts. The moderate statistical advantage of a real-world\ncrowd over its members does not justify the ado about its wisdom, which is most\nlikely a product of the selective attention fallacy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 12:26:15 GMT"}, {"version": "v2", "created": "Tue, 23 Feb 2021 19:52:03 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Reia", "Sandro M.", ""], ["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "2008.01650", "submitter": "Constantine Kontokosta", "authors": "Boyeong Hong, Bartosz Bonczak, Arpit Gupta, Lorna Thorpe, and\n  Constantine E. Kontokosta", "title": "Exposure Density and Neighborhood Disparities in COVID-19 Infection\n  Risk: Using Large-scale Geolocation Data to Understand Burdens on Vulnerable\n  Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study develops a new method to quantify neighborhood activity levels at\nhigh spatial and temporal resolutions and test whether, and to what extent,\nbehavioral responses to social distancing policies vary with socioeconomic and\ndemographic characteristics. We define exposure density as a measure of both\nthe localized volume of activity in a defined area and the proportion of\nactivity occurring in non-residential and outdoor land uses. We utilize this\napproach to capture inflows/outflows of people as a result of the pandemic and\nchanges in mobility behavior for those that remain. First, we develop a\ngeneralizable method for assessing neighborhood activity levels by land use\ntype using smartphone geolocation data over a three-month period covering more\nthan 12 million unique users within the Greater New York area. Second, we\nmeasure and analyze disparities in community social distancing by identifying\npatterns in neighborhood activity levels and characteristics before and after\nthe stay-at-home order. Finally, we evaluate the effect of social distancing in\nneighborhoods on COVID-19 infection rates and outcomes associated with\nlocalized demographic, socioeconomic, and infrastructure characteristics in\norder to identify disparities in health outcomes related to exposure risk. Our\nfindings provide insight into the timely evaluation of the effectiveness of\nsocial distancing for individual neighborhoods and support a more equitable\nallocation of resources to support vulnerable and at-risk communities. Our\nfindings demonstrate distinct patterns of activity pre- and post-COVID across\nneighborhoods. The variation in exposure density has a direct and measurable\nimpact on the risk of infection.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 15:41:24 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Hong", "Boyeong", ""], ["Bonczak", "Bartosz", ""], ["Gupta", "Arpit", ""], ["Thorpe", "Lorna", ""], ["Kontokosta", "Constantine E.", ""]]}, {"id": "2008.01714", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe, Maxime Leroux, Dalibor Stevanovic,\n  St\\'ephane Surprenant", "title": "Macroeconomic Data Transformations Matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a low-dimensional linear regression setup, considering linear\ntransformations/combinations of predictors does not alter predictions. However,\nwhen the forecasting technology either uses shrinkage or is nonlinear, it does.\nThis is precisely the fabric of the machine learning (ML) macroeconomic\nforecasting environment. Pre-processing of the data translates to an alteration\nof the regularization -- explicit or implicit -- embedded in ML algorithms. We\nreview old transformations and propose new ones, then empirically evaluate\ntheir merits in a substantial pseudo-out-sample exercise. It is found that\ntraditional factors should almost always be included as predictors and moving\naverage rotations of the data can provide important gains for various\nforecasting targets. Also, we note that while predicting directly the average\ngrowth rate is equivalent to averaging separate horizon forecasts when using\nOLS-based techniques, the latter can substantially improve on the former when\nregularization and/or nonparametric nonlinearities are involved.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:34:43 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 16:37:56 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["Leroux", "Maxime", ""], ["Stevanovic", "Dalibor", ""], ["Surprenant", "St\u00e9phane", ""]]}, {"id": "2008.01778", "submitter": "Shane Jensen", "authors": "Wichinpong Park Sinchaisri and Shane T. Jensen", "title": "Community Vibrancy and its Relationship with Safety in Philadelphia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To what extent can the strength of a local urban community impact\nneighborhood safety? We constructed measures of community vibrancy based on a\nunique dataset of block party permit approvals from the City of Philadelphia.\nWe use both regression modeling and propensity score matching to control for\nthe economic, demographic and land use characteristics of the surrounding\nneighborhood when examining the relationship betweeen crime and our measures of\ncommunity vibrancy. We conduct our analysis on aggregate levels of crime and\ncommunity vibrancy from 2006 to 2015 as well as the trends in community\nvibrancy and crime over this time period. We find that neighborhoods with a\nhigher number of block parties have a significantly higher crime rate, while\nthose holding a greater proportion of spontaneous block party events have a\nsignificantly lower crime rate. We also find that neighborhoods with an\nincrease in the ratio spontaneous block parties over time are significantly\nmore likely to have a decreasing trend in total crime incidence over that same\ntime period.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 19:29:52 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Sinchaisri", "Wichinpong Park", ""], ["Jensen", "Shane T.", ""]]}, {"id": "2008.01830", "submitter": "Richard Schweickert Ph. D.", "authors": "Richard Schweickert (1), Xiaofang Zheng (1) ((1) Purdue University)", "title": "Tree Inference: Response Time in a Binary Multinomial Processing Tree,\n  Representation and Uniqueness of Parameters", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multinomial Processing Tree (MPT) is a directed tree with a probability\nassociated with each arc. Here we consider an additional parameter associated\nwith each arc, a measure such as the time required to select the arc. MPTs are\noften used as models of tasks. Each vertex represents a process and an arc\ndescending from a vertex represents selection of an outcome of the process. A\nsource vertex represents processing that begins when a stimulus is presented\nand a terminal vertex represents making a response. Responses are partitioned\ninto classes. An experimental factor selectively influences a vertex if\nchanging the level of the factor changes parameter values on arcs descending\nfrom that vertex and on no others. Earlier work shows that if each of two\nexperimental factors selectively influences a different vertex in an arbitrary\nMPT it is equivalent for the factors to one of two relatively simple MPTs.\nWhich of the two applies depends on whether the two selectively influenced\nvertices are ordered by the factors or not. A special case, the Standard Binary\nTree for Ordered Processes, arises if the vertices are so ordered and the\nfactor selectively influencing the first vertex changes parameter values on\nonly two arcs descending from that vertex. Here we derive necessary and\nsufficient conditions for the probability and measure associated with a\nparticular response class to be accounted for by this special case. Parameter\nvalues are not unique and we give admissible transformations for transforming\none set of parameter values to another. When an experiment with two factors is\nconducted, the number of observations and parameters to be estimated depend on\nthe number of levels of each factor; we provide degrees of freedom.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 21:05:20 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Schweickert", "Richard", "", "Purdue University"], ["Zheng", "Xiaofang", "", "Purdue University"]]}, {"id": "2008.01875", "submitter": "Hussein Ammar", "authors": "Hussein A. Ammar, Raviraj Adve, Shahram Shahbazpanahi, and Gary\n  Boudreau", "title": "Statistical Analysis of Downlink Zero-Forcing Beamforming", "comments": "To appear in IEEE Wireless Communications Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.SY eess.SP eess.SY math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the mean and the variance of the useful signal and interference\npowers in a multi-cell network using zero-forcing beamforming (ZF-BF) with two\nbeamformer normalization approaches. While the mean has been the main focus in\nearlier studies on ZF-BF, analysis of the variance has not been tackled. Our\nanalysis provides a complete statistical study, sheds light on the importance\nof the variance by deriving closed-form expressions for the signals' two\nmoments, and provides a practical use for these expressions; we use the gamma\nor lognormal distribution for the interference power to analytically calculate\nthe outage.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 23:16:29 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Ammar", "Hussein A.", ""], ["Adve", "Raviraj", ""], ["Shahbazpanahi", "Shahram", ""], ["Boudreau", "Gary", ""]]}, {"id": "2008.01944", "submitter": "Weiyu Xu", "authors": "Jirong Yi, Myung Cho, Xiaodong Wu, Raghu Mudumbai, Weiyu Xu", "title": "Optimal Pooling Matrix Design for Group Testing with Dilution (Row\n  Degree) Constraints", "comments": "group testing design, COVID-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.IT eess.SP math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of designing optimal pooling matrix\nfor group testing (for example, for COVID-19 virus testing) with the constraint\nthat no more than $r>0$ samples can be pooled together, which we call \"dilution\nconstraint\". This problem translates to designing a matrix with elements being\neither 0 or 1 that has no more than $r$ '1's in each row and has a certain\nperformance guarantee of identifying anomalous elements. We explicitly give\npooling matrix designs that satisfy the dilution constraint and have\nperformance guarantees of identifying anomalous elements, and prove their\noptimality in saving the largest number of tests, namely showing that the\ndesigned matrices have the largest width-to-height ratio among all\nconstraint-satisfying 0-1 matrices.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:32:21 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Yi", "Jirong", ""], ["Cho", "Myung", ""], ["Wu", "Xiaodong", ""], ["Mudumbai", "Raghu", ""], ["Xu", "Weiyu", ""]]}, {"id": "2008.02148", "submitter": "Andrej Srakar", "authors": "Andrej Srakar (1 and 2), Marilena Vecco (3), Miroslav Verbi\\v{c} (2\n  and 1), Montserrat Gonzalez Garibay (1) and Jo\\v{z}e Sambt (2) ((1) Institute\n  for Economic Research (IER), (2) School of Economics and Business, University\n  of Ljubljana, (3) Burgundy School of Business - Universit\\'e Bourgogne\n  Franche-Comt\\'e)", "title": "MIMIC modelling with instrumental variables: A 2SLS-MIMIC approach", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Indicators Multiple Causes (MIMIC) models are type of structural\nequation models, a theory-based approach to confirm the influence of a set of\nexogenous causal variables on the latent variable, and also the effect of the\nlatent variable on observed indicator variables. In a common MIMIC model,\nmultiple indicators reflect the underlying latent variables/factors, and the\nmultiple causes (observed predictors) affect latent variables/factors. Basic\nassumptions of MIMIC are clearly violated in case of a variable being both an\nindicator and a cause, i.e. in the presence of reverse causality. Furthermore,\nthe model is then unidentified. To resolve the situation, which can arise\nfrequently, and as MIMIC estimation lacks closed form solutions for parameters\nwe utilize a version of Bollen's (1996) 2SLS estimator for structural equation\nmodels combined with J\\\"oreskog (1970)'s method of the analysis of covariance\nstructures to derive a new, 2SLS estimator for MIMIC models. Our 2SLS empirical\nestimation is based on static MIMIC specification but we point also to\ndynamic/error-correction MIMIC specification and 2SLS solution for it. We\nderive basic asymptotic theory for static 2SLS-MIMIC, present a simulation\nstudy and apply findings to an interesting empirical case of estimating\nprecarious status of older workers (using dataset of Survey of Health, Ageing\nand Retirement in Europe) which solves an important issue of the definition of\nprecarious work as a multidimensional concept, not modelled adequately so far.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jul 2020 14:09:04 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Srakar", "Andrej", "", "1 and 2"], ["Vecco", "Marilena", "", "2\n  and 1"], ["Verbi\u010d", "Miroslav", "", "2\n  and 1"], ["Garibay", "Montserrat Gonzalez", ""], ["Sambt", "Jo\u017ee", ""]]}, {"id": "2008.02315", "submitter": "Poorvi Vora", "authors": "Filip Zag\\'orski, Grant McClearn, Sarah Morin, Neal McBurnett, Poorvi\n  L. Vora", "title": "The ATHENA Class of Risk-Limiting Ballot Polling Audits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main risk-limiting ballot polling audit in use today, BRAVO, is designed\nfor use when single ballots are drawn at random and a decision regarding\nwhether to stop the audit or draw another ballot is taken after each ballot\ndraw (ballot-by-ballot (B2) audits). On the other hand, real ballot polling\naudits draw many ballots in a single round before determining whether to stop\n(round-by-round (R2) audits). We show that BRAVO results in significant\ninefficiency when directly applied to real R2 audits. We present the ATHENA\nclass of R2 stopping rules, which we show are risk-limiting if the round\nschedule is pre-determined (before the audit begins). We prove that each rule\nis at least as efficient as the corresponding BRAVO stopping rule applied at\nthe end of the round. We have open-source software libraries implementing most\nof our results.\n  We show that ATHENA halves the number of ballots required, for all state\nmargins in the 2016 US Presidential election and a first round with $90\\%$\nstopping probability, when compared to BRAVO (stopping rule applied at the end\nof the round). We present simulation results supporting the 90% stopping\nprobability claims and our claims for the risk accrued in the first round.\nFurther, ATHENA reduces the number of ballots by more than a quarter for low\nmargins, when compared to the BRAVO stopping rule applied on ballots in\nselection order. This implies that keeping track of the order when drawing\nballots R2 is not beneficial, because ATHENA is more efficient even without\ninformation on selection order. These results are significant because current\napproaches to real ballot polling election audits use the B2 BRAVO rules,\nrequiring about twice as much work on the part of election officials. Applying\nthe rules in selection order requires fewer ballots, but keeping track of the\norder, and entering it into audit software, adds to the effort.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 18:47:37 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 11:05:26 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 20:50:01 GMT"}, {"version": "v4", "created": "Sun, 4 Oct 2020 23:57:23 GMT"}, {"version": "v5", "created": "Sun, 21 Feb 2021 20:16:18 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Zag\u00f3rski", "Filip", ""], ["McClearn", "Grant", ""], ["Morin", "Sarah", ""], ["McBurnett", "Neal", ""], ["Vora", "Poorvi L.", ""]]}, {"id": "2008.02322", "submitter": "Camila Lorenz", "authors": "Patricia Marques Moralejo Bermudi, Camila Lorenz, Breno Souza de\n  Aguiar, Marcelo Antunes Failla, Ligia Vizeu Barrozo and Francisco\n  Chiaravalloti-Neto", "title": "Spatiotemporal dynamic of COVID-19 mortality in the city of Sao Paulo,\n  Brazil: shifting the high risk from the best to the worst socio-economic\n  conditions", "comments": "22 pages, 6 figures, 2 tables, 3 supplementary materials", "journal-ref": null, "doi": "10.1016/j.tmaid.2020.101945", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, Brazil has one of the fastest increasing COVID-19 epidemics in the\nworld, that has caused at least 94 thousand confirmed deaths until now. The\ncity of Sao Paulo is particularly vulnerable because it is the most populous in\nthe country. Analyzing the spatiotemporal dynamics of COVID-19 is important to\nhelp the urgent need to integrate better actions to face the pandemic. Thus,\nthis study aimed to analyze the COVID-19 mortality, from March to July 2020,\nconsidering the spatio-time architectures, the socio-economic context of the\npopulation, and using a fine granular level, in the most populous city in\nBrazil. For this, we conducted an ecological study, using secondary public data\nfrom the mortality information system. We describe mortality rates for each\nepidemiological week and the entire period by sex and age. We modelled the\ndeaths using spatiotemporal and spatial architectures and Poisson probability\ndistributions in a latent Gaussian Bayesian model approach. We obtained the\nrelative risks for temporal and spatiotemporal trends and socio-economic\nconditions. To reduce possible sub notification, we considered the confirmed\nand suspected deaths. Our findings showed an apparent stabilization of the\ntemporal trend, at the end of the period, but that may change in the future.\nMortality rate increased with increasing age and was higher in men. The risk of\ndeath was greater in areas with the worst social conditions throughout the\nstudy period. However, this was not a uniform pattern over time, since we\nidentified a shift from the high risk in the areas with best socio-economic\nconditions to the worst ones. Our study contributed by emphasizing the\nimportance of geographic screening in areas with a higher risk of death, and,\ncurrently, worse socio-economic contexts, as a crucial aspect to reducing\ndisease mortality and health inequities, through integrated public health\nactions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 19:02:14 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Bermudi", "Patricia Marques Moralejo", ""], ["Lorenz", "Camila", ""], ["de Aguiar", "Breno Souza", ""], ["Failla", "Marcelo Antunes", ""], ["Barrozo", "Ligia Vizeu", ""], ["Chiaravalloti-Neto", "Francisco", ""]]}, {"id": "2008.02341", "submitter": "William Artman", "authors": "William J. Artman, Ashkan Ertefaie, Kevin G. Lynch, James R. McKay", "title": "Bayesian Set of Best Dynamic Treatment Regimes and Sample Size\n  Determination for SMARTs with Binary Outcomes", "comments": "20 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main goals of sequential, multiple assignment, randomized trials\n(SMART) is to find the most efficacious design embedded dynamic treatment\nregimes. The analysis method known as multiple comparisons with the best (MCB)\nallows comparison between dynamic treatment regimes and identification of a set\nof optimal regimes in the frequentist setting for continuous outcomes, thereby,\ndirectly addressing the main goal of a SMART. In this paper, we develop a\nBayesian generalization to MCB for SMARTs with binary outcomes. Furthermore, we\nshow how to choose the sample size so that the inferior embedded DTRs are\nscreened out with a specified power. We compare log-odds between different DTRs\nusing their exact distribution without relying on asymptotic normality in\neither the analysis or the power calculation. We conduct extensive simulation\nstudies under two SMART designs and illustrate our method's application to the\nAdaptive Treatment for Alcohol and Cocaine Dependence (ENGAGE) trial.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 20:11:31 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Artman", "William J.", ""], ["Ertefaie", "Ashkan", ""], ["Lynch", "Kevin G.", ""], ["McKay", "James R.", ""]]}, {"id": "2008.02497", "submitter": "Jaewoo Park", "authors": "Jaewoo Park", "title": "Bayesian Indirect Inference for Models with Intractable Normalizing\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for doubly intractable distributions is challenging because the\nintractable normalizing functions of these models include parameters of\ninterest. Previous auxiliary variable MCMC algorithms are infeasible for\nmulti-dimensional models with large data sets because they depend on expensive\nauxiliary variable simulation at each iteration. We develop a fast Bayesian\nindirect algorithm by replacing an expensive auxiliary variable simulation from\na probability model with a computationally cheap simulation from a surrogate\nmodel. We learn the relationship between the surrogate model parameters and the\nprobability model parameters using Gaussian process approximations. We apply\nour methods to challenging simulated and real data examples, and illustrate\nthat the algorithm addresses both computational and inferential challenges for\ndoubly intractable distributions. Especially for a large social network model\nwith 10 parameters, we show that our method can reduce computing time from\nabout 2 weeks to 5 hours, compared to the previous method. Our method allows\npractitioners to carry out Bayesian inference for more complex models with\nlarger data sets than before.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:43:41 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Park", "Jaewoo", ""]]}, {"id": "2008.02595", "submitter": "Peter Harrison", "authors": "Peter M. C. Harrison, Raja Marjieh, Federico Adolfi, Pol van Rijn,\n  Manuel Anglada-Tort, Ofer Tchernichovski, Pauline Larrouy-Maestri, Nori\n  Jacoby", "title": "Gibbs Sampling with People", "comments": "Accepted for oral presentation at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A core problem in cognitive science and machine learning is to understand how\nhumans derive semantic representations from perceptual objects, such as color\nfrom an apple, pleasantness from a musical chord, or seriousness from a face.\nMarkov Chain Monte Carlo with People (MCMCP) is a prominent method for studying\nsuch representations, in which participants are presented with binary choice\ntrials constructed such that the decisions follow a Markov Chain Monte Carlo\nacceptance rule. However, while MCMCP has strong asymptotic properties, its\nbinary choice paradigm generates relatively little information per trial, and\nits local proposal function makes it slow to explore the parameter space and\nfind the modes of the distribution. Here we therefore generalize MCMCP to a\ncontinuous-sampling paradigm, where in each iteration the participant uses a\nslider to continuously manipulate a single stimulus dimension to optimize a\ngiven criterion such as 'pleasantness'. We formulate both methods from a\nutility-theory perspective, and show that the new method can be interpreted as\n'Gibbs Sampling with People' (GSP). Further, we introduce an aggregation\nparameter to the transition step, and show that this parameter can be\nmanipulated to flexibly shift between Gibbs sampling and deterministic\noptimization. In an initial study, we show GSP clearly outperforming MCMCP; we\nthen show that GSP provides novel and interpretable results in three other\ndomains, namely musical chords, vocal emotions, and faces. We validate these\nresults through large-scale perceptual rating experiments. The final\nexperiments use GSP to navigate the latent space of a state-of-the-art image\nsynthesis network (StyleGAN), a promising approach for applying GSP to\nhigh-dimensional perceptual spaces. We conclude by discussing future cognitive\napplications and ethical implications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 11:57:07 GMT"}, {"version": "v2", "created": "Mon, 2 Nov 2020 16:55:40 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Harrison", "Peter M. C.", ""], ["Marjieh", "Raja", ""], ["Adolfi", "Federico", ""], ["van Rijn", "Pol", ""], ["Anglada-Tort", "Manuel", ""], ["Tchernichovski", "Ofer", ""], ["Larrouy-Maestri", "Pauline", ""], ["Jacoby", "Nori", ""]]}, {"id": "2008.02625", "submitter": "Mark Peter Little", "authors": "Mark P Little, Wei Zhang, Roy van Dusen, Nobuyuki Hamada", "title": "Pneumonia after bacterial or viral infection preceded or followed by\n  radiation exposure -- a reanalysis of older radiobiological data and\n  implications for low dose radiotherapy for COVID-19 pneumonia", "comments": "2 tables, 7 figures, 15 Appendix Tables, 36 references, 49 double\n  spaced pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.TO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, there are 14 ongoing clinical studies on low dose radiotherapy\n(LDRT) for COVID-19 pneumonia. An underlying assumption is that irradiation of\nabout 1 Gy is effective at ameliorating viral pneumonia. Its rationale,\nhowever, relies on early human case series or animal studies mostly obtained in\nthe pre-antibiotic era, where rigorous statistical analyses were not performed.\nIt therefore remains unclear whether those early data support such assumptions.\nWith standard statistical survival models, and based on a systematic literature\nreview, we re-analyzed 14 radiobiological animal datasets in which animals\nreceived mostly fractionated doses of radiation before or after bacterial/viral\ninoculation, and assessing various health endpoints (mortality, pneumonia\nmorbidity). In most datasets absorbed doses did not exceed 7 Gy. Various\ndifferent model systems and types of challenging infection are considered. For\n7 studies that evaluated post-inoculation radiation exposure (more relevant to\nLDRT for COVID-19 pneumonia) the results are heterogeneous, with 2 studies\nshowing a significant increase (p<0.001) and another showing a significant\ndecrease (p<0.001) in mortality associated with radiation exposure. For\npre-inoculation exposure the results are also heterogeneous, with 6 datasets\nshowing a significant increase (p<0.01) in mortality risk associated with\nradiation exposure and the other 2 showing a significant decrease (p<0.05) in\nmortality risk. Collectively, these data do not provide clear support for\nreductions in morbidity or mortality associated with post-infection radiation\nexposure. For pre-infection radiation exposure the inconsistency of direction\nof effect makes this body of data difficult to interpret. Nevertheless, one\nmust be cautious about adducing evidence from the published reports of these\nold animal datasets.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:02:47 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 15:14:08 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 14:56:16 GMT"}, {"version": "v4", "created": "Sat, 5 Sep 2020 16:38:01 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Little", "Mark P", ""], ["Zhang", "Wei", ""], ["van Dusen", "Roy", ""], ["Hamada", "Nobuyuki", ""]]}, {"id": "2008.02658", "submitter": "Florian Pein", "authors": "Florian Pein, Annika Bartsch, Claudia Steinem, Axel Munk", "title": "Heterogeneous Idealization of Ion Channel Recordings -- Open Channel\n  Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model-free segmentation method for idealizing ion channel\nrecordings. This method is designed to deal with heterogeneity of measurement\nerrors. This in particular applies to open channel noise which, in general, is\nparticularly difficult to cope with for model-free approaches. Our methodology\nis able to deal with lowpass filtered data which provides a further\ncomputational challenge. To this end we propose a multiresolution testing\napproach, combined with local deconvolution to resolve the lowpass filter.\nSimulations and statistical theory confirm that the proposed idealization\nrecovers the underlying signal very accurately at presence of heterogeneous\nnoise, even when events are shorter than the filter length. The method is\ncompared to existing approaches in computer experiments and on real data. We\nfind that it is the only one which allows to identify openings of the PorB\nporine at two different temporal scales. An implementation is available as an R\npackage.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 13:50:05 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Pein", "Florian", ""], ["Bartsch", "Annika", ""], ["Steinem", "Claudia", ""], ["Munk", "Axel", ""]]}, {"id": "2008.02765", "submitter": "Michael Spence", "authors": "Michael A. Spence, Robert B. Thorpe, Paul G. Blackwell, Finlay Scott,\n  Richard Southwell and Julia L. Blanchard", "title": "Quantifying uncertainty and dynamical changes in multi-species fishing\n  mortality rates, catches and biomass by combining state-space and mechanistic\n  multi-species models", "comments": "35 pages 6 figures. To view Supplementary material and Simulation\n  study, please download and extract the zipped source file listed under \"Other\n  formats\"", "journal-ref": null, "doi": "10.1111/faf.12543", "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In marine management, fish stocks are often managed on a stock-by-stock basis\nusing single-species models. Many of these models are based upon statistical\ntechniques and are good at assessing the current state and making short-term\npredictions; however, as they do not model interactions between stocks, they\nlack predictive power on longer timescales. Additionally, there are mechanistic\nmulti-species models that represent key biological processes and consider\ninteractions between stocks such as predation and competition for resources.\nDue to the complexity of these models, they are difficult to fit to data, and\nso many mechanistic multi-species models depend upon single-species models\nwhere they exist, or ad hoc assumptions when they don't, for parameters such as\nannual fishing mortality. In this paper we demonstrate that by taking a\nstate-space approach, many of the uncertain parameters can be treated\ndynamically, allowing us to fit, with quantifiable uncertainty, mechanistic\nmulti-species models directly to data. We demonstrate this by fitting uncertain\nparameters, including annual fishing mortality, of a size-based multi-species\nmodel of the Celtic Sea, for species with and without single-species\nstock-assessments. Consequently, errors in the single-species models no longer\npropagate through the multi-species model and underlying assumptions are more\ntransparent. Building mechanistic multi-species models that are internally\nconsistent, with quantifiable uncertainty, will improve their credibility and\nutility for management. This may lead to their uptake by being either used to\ncorroborate single-species models; directly in the advice process to make\npredictions into the future; or used to provide a new way of managing\ndata-limited stocks.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 17:09:03 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Spence", "Michael A.", ""], ["Thorpe", "Robert B.", ""], ["Blackwell", "Paul G.", ""], ["Scott", "Finlay", ""], ["Southwell", "Richard", ""], ["Blanchard", "Julia L.", ""]]}, {"id": "2008.02852", "submitter": "Andrew Miller", "authors": "Andrew C. Miller and Nicholas J. Foti and Emily Fox", "title": "Learning Insulin-Glucose Dynamics in the Wild", "comments": "Machine Learning for Healthcare 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new model of insulin-glucose dynamics for forecasting blood\nglucose in type 1 diabetics. We augment an existing biomedical model by\nintroducing time-varying dynamics driven by a machine learning sequence model.\nOur model maintains a physiologically plausible inductive bias and clinically\ninterpretable parameters -- e.g., insulin sensitivity -- while inheriting the\nflexibility of modern pattern recognition algorithms. Critical to modeling\nsuccess are the flexible, but structured representations of subject variability\nwith a sequence model. In contrast, less constrained models like the LSTM fail\nto provide reliable or physiologically plausible forecasts. We conduct an\nextensive empirical study. We show that allowing biomedical model dynamics to\nvary in time improves forecasting at long time horizons, up to six hours, and\nproduces forecasts consistent with the physiological effects of insulin and\ncarbohydrates.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 19:47:00 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Miller", "Andrew C.", ""], ["Foti", "Nicholas J.", ""], ["Fox", "Emily", ""]]}, {"id": "2008.03013", "submitter": "Cornelius Fritz", "authors": "Cornelius Fritz, G\\\"oran Kauermann", "title": "On the Interplay of Regional Mobility, Social Connectedness, and the\n  Spread of COVID-19 in Germany", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the primary mode of respiratory virus transmission is person-to-person\ninteraction, we are required to reconsider physical interaction patterns to\nmitigate the number of people infected with COVID-19. While research has shown\nthat non-pharmaceutical interventions (NPI) had an evident impact on national\nmobility patterns, we investigate the relative regional mobility behaviour to\nassess the effect of human movement on the spread of COVID-19. In particular,\nwe explore the impact of human mobility and social connectivity derived from\nFacebook activities on the weekly rate of new infections in Germany between\nMarch 3rd and June 22nd, 2020. Our results confirm that reduced social activity\nlowers the infection rate, accounting for regional and temporal patterns. The\nextent of social distancing, quantified by the percentage of people staying put\nwithin a federal administrative district, has an overall negative effect on the\nincidence of infections. Additionally, our results show spatial infection\npatterns based on geographic as well as social distances.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 06:53:35 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 10:17:08 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 13:55:59 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Fritz", "Cornelius", ""], ["Kauermann", "G\u00f6ran", ""]]}, {"id": "2008.03073", "submitter": "Clement Lee", "authors": "Clement Lee and Emma Eastoe", "title": "From the power law to extreme value mixture distributions", "comments": "35 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The power law is useful in describing count phenomena such as network degrees\nand word frequencies. With a single parameter, it captures the main feature\nthat the frequencies are linear on the log-log scale. Nevertheless, there have\nbeen criticisms of the power law, and various approaches have been proposed to\nresolve issues such as selecting the required threshold and quantifying the\nuncertainty around it, and to test hypotheses on whether the data could have\ncome from the power law. As extreme value theory generalises the (continuous)\npower law, it is natural to consider the former as a solution to these problems\naround the latter. In this paper, we propose two extreme value mixture\ndistributions, in one of which the power law is incorporated, without the need\nof pre-specifying the threshold. The proposed distributions are shown to fit\nthe data well, quantify the threshold uncertainty in a natural way, and\nsatisfactorily answer whether the power law is useful enough.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 10:14:34 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 17:22:02 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 11:38:47 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Lee", "Clement", ""], ["Eastoe", "Emma", ""]]}, {"id": "2008.03332", "submitter": "Gabriel Jos\\'e Gil P\\'erez", "authors": "Gabriel Gil and Alejandro Lage-Castellanos", "title": "Estimating undocumented Covid-19 infections in Cuba by means of a hybrid\n  mechanistic-statistical approach", "comments": "6 pages, 3 figures, 1 table, submitted to the special number on\n  Covid-19 of Revista de Ciencias Matem\\'aticas (CUBA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the hybrid mechanistic-statistical approach of Ref. [1] to estimate\nthe total number of undocumented Covid-19 infections in Cuba. This scheme is\nbased on the maximum likelihood estimation of a SIR-like model parameters for\nthe infected population, assuming that the detection process matches a\nBernoulli trial. Our estimations show that (a) 60% of the infections were\nundocumented, (b) the real epidemics behind the data peaked ten days before the\nreports suggested, and (c) the reproduction number swiftly vanishes after 80\nepidemic days.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 18:40:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gil", "Gabriel", ""], ["Lage-Castellanos", "Alejandro", ""]]}, {"id": "2008.03334", "submitter": "Jean-Gabriel Young", "authors": "Jean-Gabriel Young, George T. Cantwell, M. E. J. Newman", "title": "Bayesian inference of network structure from unreliable data", "comments": "16 pages, 7 figures", "journal-ref": "J. Complex Netw. 8, cnaa046 (2021)", "doi": "10.1093/comnet/cnaa046", "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most empirical studies of complex networks do not return direct, error-free\nmeasurements of network structure. Instead, they typically rely on indirect\nmeasurements that are often error-prone and unreliable. A fundamental problem\nin empirical network science is how to make the best possible estimates of\nnetwork structure given such unreliable data. In this paper we describe a fully\nBayesian method for reconstructing networks from observational data in any\nformat, even when the data contain substantial measurement error and when the\nnature and magnitude of that error is unknown. The method is introduced through\npedagogical case studies using real-world example networks, and specifically\ntailored to allow straightforward, computationally efficient implementation\nwith a minimum of technical input. Computer code implementing the method is\npublicly available.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 18:45:28 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 17:05:25 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Young", "Jean-Gabriel", ""], ["Cantwell", "George T.", ""], ["Newman", "M. E. J.", ""]]}, {"id": "2008.03545", "submitter": "Tipawan Silwattananusarn", "authors": "Tipawan Silwattananusarn and Pachisa Kulkanjanapiban", "title": "Mining and Analyzing Patron's Book-Loan Data and University Data to\n  Understand Library Use Patterns", "comments": "22 pages, 9 figures", "journal-ref": "International Journal of Information Science and Management,\n  Vol.18 No.2.(2020)", "doi": null, "report-no": null, "categories": "cs.CY cs.DB cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The purpose of this paper is to study the patron's usage behavior in an\nacademic library. This study investigates on pattern of patron's books\nborrowing in Khunying Long Athakravisunthorn Learning Resources Center, Prince\nof Songkla University that influence patron's academic achievement during on\nacademic year 2015-2018. The study collected and analyzed data from the\nlibraries, registrar, and human resources. The students' performance data was\nobtained from PSU Student Information System and the rest from ALIST library\ninformation system. WEKA was used as the data mining tool employing data mining\ntechniques of association rules and clustering. All data sets were mined and\nanalyzed to identify characteristics of the patron's book borrowing, to\ndiscover the association rules of patron's interest, and to analyze the\nrelationships between academic library use and undergraduate students'\nachievement. The results reveal patterns of patron's book loan behavior,\npatterns of book usage, patterns of interest rules with respect to patron's\ninterest in book borrowing, and patterns of relationships between patron's\nborrowing and their grade. The ability to clearly identify and describe library\npatron's behavior pattern can help library in managing resources and services\nmore effectively. This study provides a sample model as guideline or campus\npartnerships and for future collaborations that will take advantage of the\nacademic library information and data mining to improve library management and\nlibrary services.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 15:46:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Silwattananusarn", "Tipawan", ""], ["Kulkanjanapiban", "Pachisa", ""]]}, {"id": "2008.03551", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Mami Kajita, Seiji Kajita", "title": "Scalable model selection for spatial additive mixed modeling:\n  application to crime analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A rapid growth in spatial open datasets has led to a huge demand for\nregression approaches accommodating spatial and non-spatial effects in big\ndata. Regression model selection is particularly important to stably estimate\nflexible regression models. However, conventional methods can be slow for large\nsamples. Hence, we develop a fast and practical model-selection approach for\nspatial regression models, focusing on the selection of coefficient types that\ninclude constant, spatially varying, and non-spatially varying coefficients. A\npre-processing approach, which replaces data matrices with small inner products\nthrough dimension reduction dramatically accelerates the computation speed of\nmodel selection. Numerical experiments show that our approach selects the model\naccurately and computationally efficiently, highlighting the importance of\nmodel selection in the spatial regression context. Then, the present approach\nis applied to open data to investigate local factors affecting crime in Japan.\nThe results suggest that our approach is useful not only for selecting factors\ninfluencing crime risk but also for predicting crime events. This scalable\nmodel selection will be key to appropriately specifying flexible and\nlarge-scale spatial regression models in the era of big data. The developed\nmodel selection approach was implemented in the R package spmoran.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 16:04:13 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 07:27:24 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Murakami", "Daisuke", ""], ["Kajita", "Mami", ""], ["Kajita", "Seiji", ""]]}, {"id": "2008.03600", "submitter": "Andrii Babii", "authors": "Andrii Babii and Ryan T. Ball and Eric Ghysels and Jonas Striaukas", "title": "Machine Learning Panel Data Regressions with an Application to\n  Nowcasting Price Earnings Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces structured machine learning regressions for prediction\nand nowcasting with panel data consisting of series sampled at different\nfrequencies. Motivated by the empirical problem of predicting corporate\nearnings for a large cross-section of firms with macroeconomic, financial, and\nnews time series sampled at different frequencies, we focus on the sparse-group\nLASSO regularization. This type of regularization can take advantage of the\nmixed frequency time series panel data structures and we find that it\nempirically outperforms the unstructured machine learning methods. We obtain\noracle inequalities for the pooled and fixed effects sparse-group LASSO panel\ndata estimators recognizing that financial and economic data exhibit heavier\nthan Gaussian tails. To that end, we leverage on a novel Fuk-Nagaev\nconcentration inequality for panel data consisting of heavy-tailed\n$\\tau$-mixing processes which may be of independent interest in other\nhigh-dimensional panel data settings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:12:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Babii", "Andrii", ""], ["Ball", "Ryan T.", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "2008.03628", "submitter": "Lijun Wang", "authors": "Lijun Wang, Yanting Zhu, Jue Shi, Xiaodan Fan", "title": "Appearance-free Tripartite Matching for Multiple Object Tracking", "comments": "30 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Object Tracking (MOT) detects the trajectories of multiple objects\ngiven an input video, and it has become more and more popular in various\nresearch and industry areas, such as cell tracking for biomedical research and\nhuman tracking in video surveillance. We target at the general MOT problem\nregardless of the object appearance. The appearance-free tripartite matching is\nproposed to avoid the irregular velocity problem of traditional bipartite\nmatching. The tripartite matching is formulated as maximizing the likelihood of\nthe state vectors constituted of the position and velocity of objects, and a\ndynamic programming algorithm is employed to solve such maximum likelihood\nestimate (MLE). To overcome the high computational cost induced by the vast\nsearch space of dynamic programming, we decompose the space by the number of\ndisappearing objects and propose a reduced-space approach by truncating the\ndecomposition. Extensive simulations have shown the superiority and efficiency\nof our proposed method. We also applied our method to track the motion of\nnatural killer cells around tumor cells in a cancer research.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 02:16:44 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Wang", "Lijun", ""], ["Zhu", "Yanting", ""], ["Shi", "Jue", ""], ["Fan", "Xiaodan", ""]]}, {"id": "2008.03665", "submitter": "Paige Maas PhD", "authors": "Paige Maas and Zack Almquist and Eugenia Giraudy and JW Schneider", "title": "Using social media to measure demographic responses to natural disaster:\n  Insights from a large-scale Facebook survey following the 2019 Australia\n  Bushfires", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we explore a novel method for collecting survey data following\na natural disaster and then combine this data with device-derived mobility\ninformation to explore demographic outcomes. Using social media as a survey\nplatform for measuring demographic outcomes, especially those that are\nchallenging or expensive to field for, is increasingly of interest to the\ndemographic community. Recent work by Schneider and Harknett (2019) explores\nthe use of Facebook targeted advertisements to collect data on low-income shift\nworkers in the United States. Other work has addressed immigrant assimilation\n(Stewart et al, 2019), world fertility (Ribeiro et al, 2020), and world\nmigration stocks (Zagheni et al, 2017). We build on this work by introducing a\nrapid-response survey of post-disaster demographic and economic outcomes\nfielded through the Facebook app itself. We use these survey responses to\naugment app-derived mobility data that comprises Facebook Displacement Maps to\nassess the validity of and drivers underlying those observed behavioral trends.\nThis survey was deployed following the 2019 Australia bushfires to better\nunderstand how these events displaced residents. In doing so we are able to\ntest a number of key hypotheses around displacement and demographics. In\nparticular, we uncover several gender differences in key areas, including in\ndisplacement decision-making and timing, and in access to protective equipment\nsuch as smoke masks. We conclude with a brief discussion of research and policy\nimplications.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 05:55:26 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Maas", "Paige", ""], ["Almquist", "Zack", ""], ["Giraudy", "Eugenia", ""], ["Schneider", "JW", ""]]}, {"id": "2008.03760", "submitter": "Prateek Bansal", "authors": "Prasad Buddhavarapu, Prateek Bansal, Jorge A. Prozzi", "title": "A New Spatial Count Data Model with Time-varying Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent crash frequency studies incorporate spatiotemporal correlations, but\nthese studies have two key limitations: i) none of these studies accounts for\ntemporal variation in model parameters; and ii) Gibbs sampler suffers from\nconvergence issues due to non-conjugacy. To address the first limitation, we\npropose a new count data model that identifies the underlying temporal patterns\nof the regression parameters while simultaneously allowing for time-varying\nspatial correlation. The model is also extended to incorporate heterogeneity in\nnon-temporal parameters across spatial units. We tackle the second shortcoming\nby deriving a Gibbs sampler that ensures conditionally conjugate posterior\nupdates for all model parameters. To this end, we take the advantages of\nP\\'olya-Gamma data augmentation and forward filtering backward sampling (FFBS)\nalgorithm. After validating the properties of the Gibbs sampler in a Monte\nCarlo study, the advantages of the proposed specification are demonstrated in\nan empirical application to uncover relationships between crash frequency\nspanning across nine years and pavement characteristics. Model parameters\nexhibit practically significant temporal patterns (i.e., temporal instability).\nFor example, the safety benefits of better pavement ride quality are estimated\nto increase over time.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 16:52:06 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Buddhavarapu", "Prasad", ""], ["Bansal", "Prateek", ""], ["Prozzi", "Jorge A.", ""]]}, {"id": "2008.03786", "submitter": "Eben Kenah", "authors": "Eben Kenah", "title": "A potential outcomes approach to selection bias", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection bias occurs when the association between exposure and disease in\nthe study population differs from that in the population eligible for\ninclusion. Along with confounding, it is one of the fundamental threats to the\nvalidity of epidemiologic research. In this paper, we propose a definition of\nselection bias in terms of potential outcomes. This approach generalizes the\nstructural approach of Hernan et al. (2004), which defines selection bias as a\ndistortion of the exposure-disease association that is caused by conditioning\non a collider. Both approaches agree in all situations where the structural\napproach identifies selection bias, but the potential outcomes approach\nidentifies selection bias in situations where the earlier approach does not.\nSelection bias defined by potential outcomes can involve a collider at\nexposure, a collider at disease, or no collider at all. This broader definition\nof selection bias does not depend on the parameterization of the association\nbetween exposure and disease, so it can be analyzed using nonparametric\nsingle-world intervention graphs (SWIGs) both under the null hypothesis and\naway from it. It provides a more nuanced interpretation of the role of\nrandomization in clinical trials, simplifies the analysis of matched studies\nand case cohort studies, and distinguishes more clearly between the estimation\nof causal effects within the study population and generalization to the\neligible population. This analysis of selection bias is an important\ntheoretical and practical application of SWIGs in epidemiology.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 18:54:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Kenah", "Eben", ""]]}, {"id": "2008.03810", "submitter": "Shayan Fazeli", "authors": "Lionel Levine, Migyeong Gwak, Kimmo Karkkainen, Shayan Fazeli, Bita\n  Zadeh, Tara Peris, Alexander Young, Majid Sarrafzadeh", "title": "Anxiety Detection Leveraging Mobile Passive Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anxiety disorders are the most common class of psychiatric problems affecting\nboth children and adults. However, tools to effectively monitor and manage\nanxiety are lacking, and comparatively limited research has been applied to\naddressing the unique challenges around anxiety. Leveraging passive and\nunobtrusive data collection from smartphones could be a viable alternative to\nclassical methods, allowing for real-time mental health surveillance and\ndisease management. This paper presents eWellness, an experimental mobile\napplication designed to track a full-suite of sensor and user-log data off an\nindividual's device in a continuous and passive manner. We report on an initial\npilot study tracking ten people over the course of a month that showed a nearly\n76% success rate at predicting daily anxiety and depression levels based solely\non the passively monitored features.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 20:22:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Levine", "Lionel", ""], ["Gwak", "Migyeong", ""], ["Karkkainen", "Kimmo", ""], ["Fazeli", "Shayan", ""], ["Zadeh", "Bita", ""], ["Peris", "Tara", ""], ["Young", "Alexander", ""], ["Sarrafzadeh", "Majid", ""]]}, {"id": "2008.03943", "submitter": "Atlanta Chakraborty", "authors": "Atlanta Chakraborty (1) and Vijay Chandru (1) ((1) Indian Institute of\n  Science)", "title": "A robust and non-parametric model for prediction of dengue incidence", "comments": "2 figures, 11 pages, Presented at the 51st Annual Convention of the\n  Operational Research Society of India (ORSI), IIT Mumbai, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Disease surveillance is essential not only for the prior detection of\noutbreaks but also for monitoring trends of the disease in the long run. In\nthis paper, we aim to build a tactical model for the surveillance of dengue, in\nparticular. Most existing models for dengue prediction exploit its known\nrelationships between climate and socio-demographic factors with the incidence\ncounts, however they are not flexible enough to capture the steep and sudden\nrise and fall of the incidence counts. This has been the motivation for the\nmethodology used in our paper. We build a non-parametric, flexible, Gaussian\nProcess (GP) regression model that relies on past dengue incidence counts and\nclimate covariates, and show that the GP model performs accurately, in\ncomparison with the other existing methodologies, thus proving to be a good\ntactical and robust model for health authorities to plan their course of\naction.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 08:06:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Chakraborty", "Atlanta", ""], ["Chandru", "Vijay", ""]]}, {"id": "2008.03972", "submitter": "Audrey B\\\"urki", "authors": "Audrey B\\\"urki, F.-Xavier Alario, Shravan Vasishth", "title": "When words collide: Bayesian meta-analyses of distractor and target\n  properties in the picture-word interference paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the picture-word interference paradigm, participants name pictures while\nignoring a written or spoken distractor word. Naming times to the pictures are\nslowed down by the presence of the distractor word. Various properties of the\ndistractor modulate this slow down, for example naming times are shorter with\nfrequent vs. infrequent distractors. Building on this line of research, the\npresent study investigates in more detail the impact of distractor and target\nword properties on picture naming times. We report the results of several\nBayesian meta-analyses, based on 35 datasets. The aim of the first analysis was\nto obtain an estimation of the size of the distractor frequency effect, and of\nits precision, in typical picture-word interference experiments where this\nvariable is not manipulated. The analysis shows that a one-unit increase in log\nfrequency results in response times to the pictures decreasing by about 4ms\n(95% Credible Interval: [-6, -2]). With the second and third analyses, we show\nthat after accounting for the effect of frequency, two variables known to\ninfluence processing times in visual word processing tasks also influence\npicture naming times: distractor length and orthographic neighborhood. Finally,\nwe found that distractor word frequency and target word frequency interact; the\neffect of distractor frequency decreases as the frequency of the target word\nincreases. We discuss the theoretical and methodological implications of these\nfindings, as well as the importance of obtaining high-precision estimates of\nexperimental effects.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:13:57 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["B\u00fcrki", "Audrey", ""], ["Alario", "F. -Xavier", ""], ["Vasishth", "Shravan", ""]]}, {"id": "2008.03974", "submitter": "Anthony J  Webster", "authors": "Anthony J. Webster", "title": "Clustering parametric models and normally distributed data", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent UK Biobank study clustered 156 parameterised models associating risk\nfactors with common diseases, to identify shared causes of disease. Parametric\nmodels are often more familiar and interpretable than clustered data, can\nbuild-in prior knowledge, adjust for known confounders, and use marginalisation\nto emphasise parameters of interest. Estimates include a Maximum Likelihood\nEstimate (MLE) that is (approximately) normally distributed, and its\ncovariance. Clustering models rarely consider the covariances of data points,\nthat are usually unavailable. Here a clustering model is formulated that\naccounts for covariances of the data, and assumes that all MLEs in a cluster\nare the same. The log-likelihood is exactly calculated in terms of the fitted\nparameters, with the unknown cluster means removed by marginalisation. The\nprocedure is equivalent to calculating the Bayesian Information Criterion (BIC)\nwithout approximation, and can be used to assess the optimum number of clusters\nfor a given clustering algorithm. The log-likelihood has terms to penalise poor\nfits and model complexity, and can be maximised to determine the number and\ncomposition of clusters. Results can be similar to using the ad-hoc \"elbow\ncriterion\", but are less subjective. The model is also formulated as a\nDirichlet process mixture model (DPMM). The overall approach is equivalent to a\nmulti-layer algorithm that characterises features through the normally\ndistributed MLEs of a fitted model, and then clusters the normal distributions.\nExamples include simulated data, and clustering of diseases in UK Biobank data\nusing estimated associations with risk factors. The results can be applied\ndirectly to measured data and their estimated covariances, to the output from\nclustering models, or the DPMM implementation can be used to cluster fitted\nmodels directly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:18:14 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:14:33 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 14:49:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Webster", "Anthony J.", ""]]}, {"id": "2008.04257", "submitter": "Yun Li", "authors": "Yun Li, Irina Bondarenko, Michael R. Elliott, Timothy P. Hofer and\n  Jeremy M.G. Taylor", "title": "Using Multiple Imputation to Classify Potential Outcomes Subgroups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With medical tests becoming increasingly available, concerns about\nover-testing and over-treatment dramatically increase. Hence, it is important\nto understand the influence of testing on treatment selection in general\npractice. Most statistical methods focus on average effects of testing on\ntreatment decisions. However, this may be ill-advised, particularly for patient\nsubgroups that tend not to benefit from such tests. Furthermore, missing data\nare common, representing large and often unaddressed threats to the validity of\nstatistical methods. Finally, it is desirable to conduct analyses that can be\ninterpreted causally. We propose to classify patients into four potential\noutcomes subgroups, defined by whether or not a patient's treatment selection\nis changed by the test result and by the direction of how the test result\nchanges treatment selection. This subgroup classification naturally captures\nthe differential influence of medical testing on treatment selections for\ndifferent patients, which can suggest targets to improve the utilization of\nmedical tests. We can then examine patient characteristics associated with\npatient potential outcomes subgroup memberships. We used multiple imputation\nmethods to simultaneously impute the missing potential outcomes as well as\nregular missing values. This approach can also provide estimates of many\ntraditional causal quantities. We find that explicitly incorporating causal\ninference assumptions into the multiple imputation process can improve the\nprecision for some causal estimates of interest. We also find that bias can\noccur when the potential outcomes conditional independence assumption is\nviolated; sensitivity analyses are proposed to assess the impact of this\nviolation. We applied the proposed methods to examine the influence of 21-gene\nassay, the most commonly used genomic test, on chemotherapy selection among\nbreast cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 16:59:52 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Li", "Yun", ""], ["Bondarenko", "Irina", ""], ["Elliott", "Michael R.", ""], ["Hofer", "Timothy P.", ""], ["Taylor", "Jeremy M. G.", ""]]}, {"id": "2008.04284", "submitter": "Yishu Xue", "authors": "Hou-Cheng Yang, Yishu Xue, Yuqing Pan, Qingyang Liu, Guanyu Hu", "title": "Time Fused Coefficient SIR Model with Application to COVID-19 Epidemic\n  in the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a Susceptible-Infected-Removal (SIR) model with\ntime fused coefficients. In particular, our proposed model discovers the\nunderlying time homogeneity pattern for the SIR model's transmission rate and\nremoval rate via Bayesian shrinkage priors. MCMC sampling for the proposed\nmethod is facilitated by the nimble package in R. Extensive simulation studies\nare carried out to examine the empirical performance of the proposed methods.\nWe further apply the proposed methodology to analyze different levels of\nCOVID-19 data in the United States.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:27:48 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 04:01:34 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 05:36:32 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yang", "Hou-Cheng", ""], ["Xue", "Yishu", ""], ["Pan", "Yuqing", ""], ["Liu", "Qingyang", ""], ["Hu", "Guanyu", ""]]}, {"id": "2008.04295", "submitter": "Henry Wilde", "authors": "Henry Wilde and Vincent Knight and Jonathan Gillard and Kendal Smith", "title": "Segmentation analysis and the recovery of queuing parameters via the\n  Wasserstein distance: a study of administrative data for patients with\n  chronic obstructive pulmonary disease", "comments": "24 pages, 11 figures (19 including subfigures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work uses a data-driven approach to analyse how the resource\nrequirements of patients with chronic obstructive pulmonary disease (COPD) may\nchange, quantifying how those changes impact the hospital system with which the\npatients interact. This approach is composed of a novel combination of often\ndistinct modes of analysis: segmentation, operational queuing theory, and the\nrecovery of parameters from incomplete data. By combining these methods as\npresented here, this work demonstrates that potential limitations around the\navailability of fine-grained data can be overcome. Thus, finding useful\noperational results despite using only administrative data. The paper begins by\nfinding a useful clustering of the population from this granular data that\nfeeds into a multi-class M/M/c model, whose parameters are recovered from the\ndata via parameterisation and the Wasserstein distance. This model is then used\nto conduct an informative analysis of the underlying queuing system and the\nneeds of the population under study through several what-if scenarios. The\nanalyses used to form and study this model consider, in effect, all types of\npatient arrivals and how those types impact the system. With that, this study\nfinds that there are no quick solutions to reduce the impact of COPD patients\non the system, including adding capacity to the system. In this analysis, the\nonly effective intervention to reduce the strain caused by those presenting\nwith COPD is to enact external policies which directly improve the overall\nhealth of the COPD population before they arrive at the hospital.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:47:34 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 17:15:00 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 11:43:00 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Wilde", "Henry", ""], ["Knight", "Vincent", ""], ["Gillard", "Jonathan", ""], ["Smith", "Kendal", ""]]}, {"id": "2008.04382", "submitter": "Farhad Pourkamali-Anaraki", "authors": "Mohammad Amin Hariri-Ardebili, Farhad Pourkamali-Anaraki, Siamak\n  Sattar", "title": "Uncertainty Quantification of Structural Systems with Subset of Data", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification of the impact of uncertainty in material properties as well as\nthe input ground motion on structural responses is an important step in\nimplementing a performance-based earthquake engineering (PBEE) framework. Among\nvarious sources of uncertainty, the variability in the input ground motions,\na.k.a. record-to-record, greatly affects the assessment results. The objective\nof this paper is to quantify the uncertainty in structural response with hybrid\nuncertainty sources. In this paper, multiple matrix completion methods are\nproposed and applied on a case study structure. The matrix completion method is\na means to estimate the analyses results for the entire set of input parameters\nby conducting analysis for only a small subset of analyses. The main\nalgorithmic contributions of our proposed method are twofold. First, we develop\na sampling technique for choosing a subset of representative simulations, which\nallows improving the accuracy of the estimated response. An unsupervised\nmachine learning technique is used for this purpose. Next, the proposed matrix\ncompletion method for uncertainty quantification is further refined by\nincorporating a regression model that is trained on the available partial\nsimulations. The regression model improves the initial sampling as it provides\na rough estimation of the structural responses. Finally, the proposed algorithm\nis applied to a multi-degree-of-freedom system, and the structural responses\n(i.e., displacements and base shear) are estimated. Results show that the\nproposed algorithm can effectively estimate the response from a full set of\nnonlinear simulations by conducting analyses only on a small portion of the\nset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 00:20:40 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Hariri-Ardebili", "Mohammad Amin", ""], ["Pourkamali-Anaraki", "Farhad", ""], ["Sattar", "Siamak", ""]]}, {"id": "2008.04700", "submitter": "Tobia Boschi", "authors": "Tobia Boschi, Jacopo Di Iorio, Lorenzo Testa, Marzia A. Cremona, and\n  Francesca Chiaromonte", "title": "The shapes of an epidemic: using Functional Data Analysis to\n  characterize COVID-19 in Italy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate patterns of COVID-19 mortality across 20 Italian regions and\ntheir association with mobility, positivity, and socio-demographic,\ninfrastructural and environmental covariates. Notwithstanding limitations in\naccuracy and resolution of the data available from public sources, we pinpoint\nsignificant trends exploiting information in curves and shapes with Functional\nData Analysis techniques. These depict two starkly different epidemics; an\n\"exponential\" one unfolding in Lombardia and the worst hit areas of the north,\nand a milder, \"flat(tened)\" one in the rest of the country -- including Veneto,\nwhere cases appeared concurrently with Lombardia but aggressive testing was\nimplemented early on. We find that mobility and positivity can predict COVID-19\nmortality, also when controlling for relevant covariates. Among the latter,\nprimary care appears to mitigate mortality, and contacts in hospitals, schools\nand work places to aggravate it. The techniques we describe could capture\nadditional and potentially sharper signals if applied to richer data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:51:46 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Boschi", "Tobia", ""], ["Di Iorio", "Jacopo", ""], ["Testa", "Lorenzo", ""], ["Cremona", "Marzia A.", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "2008.04706", "submitter": "Pietro Fanti", "authors": "Antonio Bernini, Lorella Bonaccorsi, Pietro Fanti, Francesco Ranaldi,\n  Ugo Santosuosso", "title": "Use of IT tools to search for a correlation between weather factors and\n  onset of pulmonary thromboembolism", "comments": "30 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pulmonary embolism (PE) and deep vein thrombosis (DVT) are gathered in venous\nthromboembolism (VTE) and represent the third cause of cardiovascular diseases.\nRecent studies suggest that meteorological parameters as atmospheric pressure,\ntemperature, and humidity could affect PE incidence but, nowadays, the\nrelationship between these two phenomena is debated and the evidence is not\ncompletely explained. The clinical experience of the Department of Emergency\nMedicine at AOUC Hospital suggests the possibility that a relationship\neffectively exists. We have collected data concerning the Emergency Medicine\nUnit admissions of PE patients to confirm our hypothesis. At the same time,\natmospheric parameters are collected from the Lamma Consortium of Tuscany\nregion. We have implemented new IT models and statistic tools by using\nsemi-hourly records of weather time high resolution data to process the\ndataset. We have carried out tools from econometrics, like mobile means, and we\nhave studied anomalies through the search for peaks and possible patterns. We\nhave created a framework in Python to represent and study time series and to\nanalyze data and plot graphs. The project has been uploaded on GitHub. Our\nanalyses highlighted a strong correlation between the moving averages of\natmospheric pressure and those of the hospitalizations number (R= -0.9468,\np<0,001) although causality is still unknown. The existence of an increase in\nthe number of hospitalizations in the days following short-to-medium periods of\ntime characterized by a high number of half-hourly pressure changes is also\ndetected. The spectrograms studies obtained by the Fourier transform requires\nto increase the dataset. The analyzed data (especially hospitalization data)\nwere too few to carry out this kind of analyses.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:58:18 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bernini", "Antonio", ""], ["Bonaccorsi", "Lorella", ""], ["Fanti", "Pietro", ""], ["Ranaldi", "Francesco", ""], ["Santosuosso", "Ugo", ""]]}, {"id": "2008.04948", "submitter": "Jean-Gabriel Young", "authors": "Jean-Gabriel Young, Giovanni Petri, Tiago P. Peixoto", "title": "Hypergraph reconstruction from network data", "comments": "13 pages, 7 figures. Code is available at\n  https://graph-tool.skewed.de/", "journal-ref": "Communication Physics 4, 135 (2021)", "doi": "10.1038/s42005-021-00637-w", "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks can describe the structure of a wide variety of complex systems by\nspecifying which pairs of entities in the system are connected. While such\npairwise representations are flexible, they are not necessarily appropriate\nwhen the fundamental interactions involve more than two entities at the same\ntime. Pairwise representations nonetheless remain ubiquitous, because\nhigher-order interactions are often not recorded explicitly in network data.\nHere, we introduce a Bayesian approach to reconstruct latent higher-order\ninteractions from ordinary pairwise network data. Our method is based on the\nprinciple of parsimony and only includes higher-order structures when there is\nsufficient statistical evidence for them. We demonstrate its applicability to a\nwide range of datasets, both synthetic and empirical.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 18:30:06 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 19:01:58 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 17:31:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Young", "Jean-Gabriel", ""], ["Petri", "Giovanni", ""], ["Peixoto", "Tiago P.", ""]]}, {"id": "2008.04982", "submitter": "Claire-Alice H\\'ebert", "authors": "Claire-Alice H\\'ebert, Earl Lawrence, Kary Myers, James P. Colgan,\n  Elizabeth J. Judge", "title": "An Initial Exploration of Bayesian Model Calibration for Estimating the\n  Composition of Rocks and Soils on Mars", "comments": "10 pages, 5 figures, special issue", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mars Curiosity rover carries an instrument, ChemCam, designed to measure\nthe composition of surface rocks and soil using laser-induced breakdown\nspectroscopy (LIBS). The measured spectra from this instrument must be analyzed\nto identify the component elements in the target sample, as well as their\nrelative proportions. This process, which we call disaggregation, is\ncomplicated by so-called matrix effects, which describe nonlinear changes in\nthe relative heights of emission lines as an unknown function of composition\ndue to atomic interactions within the LIBS plasma. In this work we explore the\nuse of the plasma physics code ATOMIC, developed at Los Alamos National\nLaboratory, for the disaggregation task. ATOMIC has recently been used to model\nLIBS spectra and can robustly reproduce matrix effects from first principles.\nThe ability of ATOMIC to predict LIBS spectra presents an exciting opportunity\nto perform disaggregation in a manner not yet tried in the LIBS community,\nnamely via Bayesian model calibration. However, using it directly to solve our\ninverse problem is computationally intractable due to the large parameter space\nand the computation time required to produce a single output. Therefore we also\nexplore the use of emulators as a fast solution for this analysis. We discuss a\nproof of concept Gaussian process emulator for disaggregating two-element\ncompounds of sodium and copper. The training and test datasets were simulated\nwith ATOMIC using a Latin hypercube design. After testing the performance of\nthe emulator, we successfully recover the composition of 25 test spectra with\nBayesian model calibration.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 19:32:41 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["H\u00e9bert", "Claire-Alice", ""], ["Lawrence", "Earl", ""], ["Myers", "Kary", ""], ["Colgan", "James P.", ""], ["Judge", "Elizabeth J.", ""]]}, {"id": "2008.05040", "submitter": "Suyan Tian", "authors": "Suyan Tian, Chi Wang, Mayte Suarez-Farinas", "title": "GEE-TGDR: A longitudinal feature selection algorithm and its application\n  to lncRNA expression profiles for psoriasis patients treated with immune\n  therapies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast evolution of high-throughput technology, longitudinal gene\nexpression experiments have become affordable and increasingly common in\nbiomedical fields. Generalized estimating equation (GEE) approach is a widely\nused statistical method for the analysis of longitudinal data. Feature\nselection is imperative in longitudinal omics data analysis. Among a variety of\nexisting feature selection methods, an embedded method, namely, threshold\ngradient descent regularization (TGDR) stands out due to its excellent\ncharacteristics. An alignment of GEE with TGDR is a promising area for the\npurpose of identifying relevant markers that can explain the dynamic changes of\noutcomes across time. In this study, we proposed a new novel feature selection\nalgorithm for longitudinal outcomes:GEE-TGDR. In the GEE-TGDR method, the\ncorresponding quasi-likelihood function of a GEE model is the objective\nfunction to be optimized and the optimization and feature selection are\naccomplished by the TGDR method. We applied the GEE-TGDR method a longitudinal\nlncRNA gene expression dataset that examined the treatment response of\npsoriasis patients to immune therapy. Under different working correlation\nstructures, a list including 10 relevant lncRNAs were identified with a\npredictive accuracy of 80 % and meaningful biological interpretation. To\nconclude, a widespread application of the proposed GEE-TGDR method in omics\ndata analysis is anticipated.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 23:44:12 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Tian", "Suyan", ""], ["Wang", "Chi", ""], ["Suarez-Farinas", "Mayte", ""]]}, {"id": "2008.05109", "submitter": "Xingchen Yu", "authors": "Xingchen Yu, Abel Rodriguez", "title": "A Bayesian Approach to Spherical Factor Analysis for Binary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor models are widely used across diverse areas of application for\npurposes that include dimensionality reduction, covariance estimation, and\nfeature engineering. Traditional factor models can be seen as an instance of\nlinear embedding methods that project multivariate observations onto a lower\ndimensional Euclidean latent space. This paper discusses a new class of\ngeometric embedding models for multivariate binary data in which the embedding\nspace correspond to a spherical manifold, with potentially unknown dimension.\nThe resulting models include traditional factor models as a special case, but\nprovide additional flexibility. Furthermore, unlike other techniques for\ngeometric embedding, the models are easy to interpret, and the uncertainty\nassociated with the latent features can be properly quantified. These\nadvantages are illustrated using both simulation studies and real data on\nvoting records from the U.S. Senate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 04:55:18 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Yu", "Xingchen", ""], ["Rodriguez", "Abel", ""]]}, {"id": "2008.05153", "submitter": "Santosh Kumar", "authors": "Santosh Kumar", "title": "Wishart and random density matrices: Analytical results for the\n  mean-square Hilbert-Schmidt distance", "comments": "Published version", "journal-ref": "Phys. Rev. A 102, 012405 (2020)", "doi": "10.1103/PhysRevA.102.012405", "report-no": null, "categories": "quant-ph math-ph math.MP physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hilbert-Schmidt distance is one of the prominent distance measures in quantum\ninformation theory which finds applications in diverse problems, such as\nconstruction of entanglement witnesses, quantum algorithms in machine learning,\nand quantum state tomography. In this work, we calculate exact and compact\nresults for the mean square Hilbert-Schmidt distance between a random density\nmatrix and a fixed density matrix, and also between two random density\nmatrices. In the course of derivation, we also obtain corresponding exact\nresults for the distance between a Wishart matrix and a fixed Hermitian matrix,\nand two Wishart matrices. We verify all our analytical results using Monte\nCarlo simulations. Finally, we apply our results to investigate the\nHilbert-Schmidt distance between reduced density matrices generated using\ncoupled kicked tops.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 07:49:12 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kumar", "Santosh", ""]]}, {"id": "2008.05406", "submitter": "Bjoern Bornkamp", "authors": "Bj\\\"orn Bornkamp and Kaspar Rufibach and Jianchang Lin and Yi Liu and\n  Devan V. Mehrotra and Satrajit Roychoudhury and Heinz Schmidli and Yue Shentu\n  and Marcel Wolbers", "title": "Principal Stratum Strategy: Potential Role in Drug Development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A randomized trial allows estimation of the causal effect of an intervention\ncompared to a control in the overall population and in subpopulations defined\nby baseline characteristics. Often, however, clinical questions also arise\nregarding the treatment effect in subpopulations of patients, which would\nexperience clinical or disease related events post-randomization. Events that\noccur after treatment initiation and potentially affect the interpretation or\nthe existence of the measurements are called {\\it intercurrent events} in the\nICH E9(R1) guideline. If the intercurrent event is a consequence of treatment,\nrandomization alone is no longer sufficient to meaningfully estimate the\ntreatment effect. Analyses comparing the subgroups of patients without the\nintercurrent events for intervention and control will not estimate a causal\neffect. This is well known, but post-hoc analyses of this kind are commonly\nperformed in drug development. An alternative approach is the principal stratum\nstrategy, which classifies subjects according to their potential occurrence of\nan intercurrent event on both study arms. We illustrate with examples that\nquestions formulated through principal strata occur naturally in drug\ndevelopment and argue that approaching these questions with the ICH E9(R1)\nestimand framework has the potential to lead to more transparent assumptions as\nwell as more adequate analyses and conclusions. In addition, we provide an\noverview of assumptions required for estimation of effects in principal strata.\nMost of these assumptions are unverifiable and should hence be based on solid\nscientific understanding. Sensitivity analyses are needed to assess robustness\nof conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 16:01:48 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 08:59:45 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bornkamp", "Bj\u00f6rn", ""], ["Rufibach", "Kaspar", ""], ["Lin", "Jianchang", ""], ["Liu", "Yi", ""], ["Mehrotra", "Devan V.", ""], ["Roychoudhury", "Satrajit", ""], ["Schmidli", "Heinz", ""], ["Shentu", "Yue", ""], ["Wolbers", "Marcel", ""]]}, {"id": "2008.05561", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "The Right Tools for the Job: The Case for Spatial Science Tool-Building", "comments": null, "journal-ref": "Transactions in GIS, 2020", "doi": "10.1111/tgis.12678", "report-no": null, "categories": "cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper was presented as the 8th annual Transactions in GIS plenary\naddress at the American Association of Geographers annual meeting in\nWashington, DC. The spatial sciences have recently seen growing calls for more\naccessible software and tools that better embody geographic science and theory.\nUrban spatial network science offers one clear opportunity: from multiple\nperspectives, tools to model and analyze nonplanar urban spatial networks have\ntraditionally been inaccessible, atheoretical, or otherwise limiting. This\npaper reflects on this state of the field. Then it discusses the motivation,\nexperience, and outcomes of developing OSMnx, a tool intended to help address\nthis. Next it reviews this tool's use in the recent multidisciplinary spatial\nnetwork science literature to highlight upstream and downstream benefits of\nopen-source software development. Tool-building is an essential but poorly\nincentivized component of academic geography and social science more broadly.\nTo conduct better science, we need to build better tools. The paper concludes\nwith paths forward, emphasizing open-source software and reusable computational\ndata science beyond mere reproducibility and replicability.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 20:15:39 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "2008.05606", "submitter": "Shenyi Pan", "authors": "Shenyi Pan, Harry Joe, Guofu Li", "title": "Conditional Inferences Based on Vine Copulas with Applications to Credit\n  Spread Data of Corporate Bonds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the dependence relationship of credit spreads of corporate\nbonds is important for risk management. Vine copula models with tail dependence\nare used to analyze a credit spread dataset of Chinese corporate bonds,\nunderstand the dependence among different sectors and perform conditional\ninferences. It is shown how the effect of tail dependence affects risk\ntransfer, or the conditional distributions given one variable is extreme. Vine\ncopula models also provide more accurate cross prediction results compared with\nlinear regressions. These conditional inference techniques are a statistical\ncontribution for analysis of bond credit spreads of investment portfolios\nconsisting of corporate bonds from various sectors.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 23:17:41 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Pan", "Shenyi", ""], ["Joe", "Harry", ""], ["Li", "Guofu", ""]]}, {"id": "2008.05649", "submitter": "Qihuang Zhang", "authors": "Qihuang Zhang and Grace Y. Yi", "title": "Sensitivity Analysis of Error-Contaminated Time Series Data under\n  Autoregressive Models with Application of COVID-19 Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive (AR) models are useful tools in time series analysis.\nInferences under such models are distorted in the presence of measurement\nerror, which is very common in practice. In this article, we establish\nanalytical results for quantifying the biases of the parameter estimation in AR\nmodels if the measurement error effects are neglected. We propose two\nmeasurement error models to describe different processes of data contamination.\nAn estimating equation approach is proposed for the estimation of the model\nparameters with measurement error effects accounted for. We further discuss\nforecasting using the proposed method. Our work is inspired by COVID-19 data,\nwhich are error-contaminated due to multiple reasons including the asymptomatic\ncases and varying incubation periods. We implement our proposed method by\nconducting sensitivity analyses and forecasting of the mortality rate of\nCOVID-19 over time for the four most populated provinces in Canada. The results\nsuggest that incorporating or not incorporating measurement error effects\nyields rather different results for parameter estimation and forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 02:19:26 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhang", "Qihuang", ""], ["Yi", "Grace Y.", ""]]}, {"id": "2008.05693", "submitter": "Benjamin Avanzi", "authors": "Benjamin Avanzi, Gregory Clive Taylor, Melantha Wang, Bernard Wong", "title": "SynthETIC: an individual insurance claim simulator with feature control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen rapid increase in the application of machine learning\nto insurance loss reserving. They yield most value when applied to large data\nsets, such as individual claims, or large claim triangles. In short, they are\nlikely to be useful in the analysis of any data set whose volume is sufficient\nto obscure a naked-eye view of its features. Unfortunately, such large data\nsets are in short supply in the actuarial literature. Accordingly, one needs to\nturn to synthetic data. Although the ultimate objective of these methods is\napplication to real data, the use of synthetic data containing features\ncommonly observed in real data is also to be encouraged.\n  While there are a number of claims simulators in existence, each valuable\nwithin its own context, the inclusion of a number of desirable (but\ncomplicated) data features requires further development. Accordingly, in this\npaper we review those desirable features, and propose a new simulator of\nindividual claim experience called SynthETIC.\n  Our simulator is publicly available, open source, and fills a gap in the\nnon-life actuarial toolkit. The simulator specifically allows for desirable\n(but optionally complicated) data features typically occurring in practice,\nsuch as variations in rates of settlements and development patterns; as with\nsuperimposed inflation, and various discontinuities, and also enables various\ndependencies between variables. The user has full control of the mechanics of\nthe evolution of an individual claim. As a result, the complexity of the data\nset generated (meaning the level of difficulty of analysis) may be dialled\nanywhere from extremely simple to extremely complex.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 05:24:34 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 03:32:31 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 13:18:00 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Avanzi", "Benjamin", ""], ["Taylor", "Gregory Clive", ""], ["Wang", "Melantha", ""], ["Wong", "Bernard", ""]]}, {"id": "2008.05824", "submitter": "M. Andrea Arias-Serna", "authors": "M. Andrea Arias-Serna, Jean-Michel Loubes, Francisco J. Caro-Lopera", "title": "Risk Measures Estimation Under Wasserstein Barycenter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomness in financial markets requires modern and robust multivariate\nmodels of risk measures. This paper proposes a new approach for modeling\nmultivariate risk measures under Wasserstein barycenters of probability\nmeasures supported on location-scatter families. Simple and advanced copulas\nmultivariate Value at Risk models are compared with the derived technique. The\nperformance of the model is also checked in market indices of United States\ngenerated by the financial crisis due to COVID-19. The introduced model behaves\nsatisfactory in both common and volatile periods of asset prices, providing\nrealistic VaR forecast in this era of social distancing.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 11:23:20 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Arias-Serna", "M. Andrea", ""], ["Loubes", "Jean-Michel", ""], ["Caro-Lopera", "Francisco J.", ""]]}, {"id": "2008.05951", "submitter": "Antonio Remiro-Az\\'ocar Mr.", "authors": "Antonio Remiro-Az\\'ocar, Anna Heath, Gianluca Baio", "title": "Marginalization of Regression-Adjusted Treatment Effects in Indirect\n  Comparisons with Limited Patient-Level Data", "comments": "87 pages (28 of supplementary appendices and references), 5 figures.\n  arXiv admin note: text overlap with arXiv:2004.14800", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population adjustment methods such as matching-adjusted indirect comparison\n(MAIC), based on propensity score weighting, are increasingly used to compare\nmarginal treatment effects when there are cross-trial differences in effect\nmodifiers and limited patient-level data. Current outcome regression-based\nalternatives target a conditional treatment effect that is incompatible in the\nindirect comparison. When adjusting for covariates, one must integrate or\naverage the conditional estimate over the population of interest to recover a\ncompatible marginal treatment effect. We propose a marginalization method based\non the ideas of parametric G-computation that can be easily applied where the\noutcome regression is a generalized linear model or a Cox model. In addition,\nwe introduce a novel general-purpose method based on the ideas underlying\nmultiple imputation, which we term multiple imputation marginalization (MIM)\nand is applicable to a wide range of models, including parametric survival\nmodels. Both methods can accommodate a Bayesian statistical framework which\nnaturally integrates the analysis into a probabilistic framework, typically\nrequired for health technology assessment. A simulation study provides\nproof-of-principle for the methods and benchmarks their performance against\nMAIC and the conventional outcome regression. The simulations are based on\nscenarios with binary outcomes and continuous covariates, with the log-odds\nratio as the measure of effect. The marginalized outcome regression approaches\nachieve more precise and more accurate estimates than MAIC, particularly when\ncovariate overlap is poor, and yield unbiased treatment effect estimates under\nno failures of assumptions. Furthermore, the marginalized covariate-adjusted\nestimates provide greater precision than the conditional estimates produced by\nthe conventional outcome regression.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 10:56:15 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 09:55:24 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 18:00:32 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 21:12:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Remiro-Az\u00f3car", "Antonio", ""], ["Heath", "Anna", ""], ["Baio", "Gianluca", ""]]}, {"id": "2008.05968", "submitter": "Emiliano Valdez", "authors": "Shuang Yin, Dipak K. Dey, Emiliano A. Valdez, and Xiaomeng Li", "title": "Flexible Modeling of Hurdle Conway-Maxwell-Poisson Distributions with\n  Application to Mining Injuries", "comments": "23 pages, 7 Tables, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the hurdle Poisson regression is a popular class of models for count\ndata with excessive zeros, the link function in the binary component may be\nunsuitable for highly imbalanced cases. Ordinary Poisson regression is unable\nto handle the presence of dispersion. In this paper, we introduce\nConway-Maxwell-Poisson (CMP) distribution and integrate use of flexible skewed\nWeibull link functions as better alternative. We take a fully Bayesian approach\nto draw inference from the underlying models to better explain skewness and\nquantify dispersion, with Deviance Information Criteria (DIC) used for model\nselection. For empirical investigation, we analyze mining injury data for\nperiod 2013-2016 from the U.S. Mine Safety and Health Administration (MSHA).\nThe risk factors describing proportions of employee hours spent in each type of\nmining work are compositional data; the probabilistic principal components\nanalysis (PPCA) is deployed to deal with such covariates. The hurdle CMP\nregression is additionally adjusted for exposure, measured by the total\nemployee working hours, to make inference on rate of mining injuries; we tested\nits competitiveness against other models. This can be used as predictive model\nin the mining workplace to identify features that increase the risk of injuries\nso that prevention can be implemented.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:32:40 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Yin", "Shuang", ""], ["Dey", "Dipak K.", ""], ["Valdez", "Emiliano A.", ""], ["Li", "Xiaomeng", ""]]}, {"id": "2008.05987", "submitter": "Alqamah Sayeed", "authors": "Alqamah Sayeed, Yunsoo Choi, Ebrahim Eslami, Jia Jung, Yannic Lops,\n  Ahmed Khan Salman", "title": "A Novel CMAQ-CNN Hybrid Model to Forecast Hourly Surface-Ozone\n  Concentrations Fourteen Days in Advance", "comments": "15 pages, 4 main figures and supplemantary figures and tables", "journal-ref": null, "doi": "10.1038/s41598-021-90446-6", "report-no": null, "categories": "physics.ao-ph cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Issues regarding air quality and related health concerns have prompted this\nstudy, which develops an accurate and computationally fast, efficient hybrid\nmodeling system that combines numerical modeling and machine learning for\nforecasting concentrations of surface ozone. Currently available numerical\nmodeling systems for air quality predictions (e.g., CMAQ, NCEP EMP) can\nforecast 24 to 48 hours in advance. In this study, we develop a modeling system\nbased on a convolutional neural network (CNN) model that is not only fast but\ncovers a temporal period of two weeks with a resolution as small as a single\nhour for 255 stations. The CNN model uses forecasted meteorology from the\nWeather Research and Forecasting model (processed by the Meteorology-Chemistry\nInterface Processor), forecasted air quality from the Community Multi-scale Air\nQuality Model (CMAQ), and previous 24-hour concentrations of various measurable\nair quality parameters as inputs and predicts the following 14-day hourly\nsurface ozone concentrations. The model achieves an average accuracy of 0.91 in\nterms of the index of agreement for the first day and 0.78 for the fourteenth\nday while the average index of agreement for one day ahead prediction from the\nCMAQ is 0.77. Through this study, we intend to amalgamate the best features of\nnumerical modeling (i.e., fine spatial resolution) and a deep neural network\n(i.e., computation speed and accuracy) to achieve more accurate spatio-temporal\npredictions of hourly ozone concentrations. Although the primary purpose of\nthis study is the prediction of hourly ozone concentrations, the system can be\nextended to various other pollutants.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 16:02:05 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Sayeed", "Alqamah", ""], ["Choi", "Yunsoo", ""], ["Eslami", "Ebrahim", ""], ["Jung", "Jia", ""], ["Lops", "Yannic", ""], ["Salman", "Ahmed Khan", ""]]}, {"id": "2008.06051", "submitter": "Tatsushi Oka", "authors": "Tatsushi Oka and Wei Wei and Dan Zhu", "title": "A Spatial Stochastic SIR Model for Transmission Networks with\n  Application to COVID-19 Epidemic in China", "comments": "Typos were fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Governments around the world have implemented preventive measures against the\nspread of the coronavirus disease (COVID-19). In this study, we consider a\nmultivariate discrete-time Markov model to analyze the propagation of COVID-19\nacross 33 provincial regions in China. This approach enables us to evaluate the\neffect of mobility restriction policies on the spread of the disease. We use\ndata on daily human mobility across regions and apply the Bayesian framework to\nestimate the proposed model. The results show that the spread of the disease in\nChina was predominately driven by community transmission within regions and the\nlockdown policy introduced by local governments curbed the spread of the\npandemic. Further, we document that Hubei was only the epicenter of the early\nepidemic stage. Secondary epicenters, such as Beijing and Guangdong, had\nalready become established by late January 2020, and the disease spread out to\nconnected regions. The transmission from these epicenters substantially\ndeclined following the introduction of human mobility restrictions across\nregions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 14:25:40 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 01:54:26 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Oka", "Tatsushi", ""], ["Wei", "Wei", ""], ["Zhu", "Dan", ""]]}, {"id": "2008.06131", "submitter": "Cory McCartan", "authors": "Cory McCartan and Kosuke Imai", "title": "Sequential Monte Carlo for Sampling Balanced and Compact Redistricting\n  Plans", "comments": "44 pages, 11 figures; added additional validation example, improved\n  measurements in Section 6 comparison, reworked some language for precision\n  throughout", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random sampling of graph partitions under constraints has become a popular\ntool for evaluating legislative redistricting plans. Analysts detect partisan\ngerrymandering by comparing a proposed redistricting plan with an ensemble of\nsampled alternative plans. For successful application, sampling methods must\nscale to large maps with many districts, incorporate realistic legal\nconstraints, and accurately and efficiently sample from a selected target\ndistribution. Unfortunately, most existing methods struggle in at least one of\nthese three areas. We present a new Sequential Monte Carlo (SMC) algorithm that\ndraws representative redistricting plans from a realistic target distribution\nof choice. Because it samples directly, the SMC algorithm can efficiently\nexplore the relevant space of redistricting plans better than the existing\nMarkov chain Monte Carlo algorithms that yield dependent samples. Our algorithm\ncan simultaneously incorporate several constraints commonly imposed in\nreal-world redistricting problems, including equal population, compactness, and\npreservation of administrative boundaries. We validate the accuracy of the\nproposed algorithm by using a small map where all redistricting plans can be\nenumerated. We then apply the SMC algorithm to evaluate the partisan\nimplications of several maps submitted by relevant parties in a recent\nhigh-profile redistricting case in the state of Pennsylvania. We find that the\nproposed algorithm is roughly 40 times more efficient in sampling from the\ntarget distribution than a state-of-the-art MCMC algorithm. Open-source\nsoftware is available for implementing the proposed methodology.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:26:34 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 23:33:47 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["McCartan", "Cory", ""], ["Imai", "Kosuke", ""]]}, {"id": "2008.06168", "submitter": "Hamdi Raissi", "authors": "Hamdi Ra\\\"issi", "title": "On the correlation analysis of illiquid stocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The serial correlations of illiquid stock's price changes are studied,\nallowing for unconditional heteroscedasticity and time-varying zero returns\nprobability. Depending on the set up, we investigate how the usual\nautocorrelations can be accommodated, to deliver an accurate representation of\nthe price changes serial correlations. We shed some light on the properties of\nthe different serial correlations measures, by mean of Monte Carlo experiments.\nThe theoretical arguments are illustrated considering shares from the Chilean\nstock market.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 02:29:13 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 16:55:22 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ra\u00efssi", "Hamdi", ""]]}, {"id": "2008.06178", "submitter": "J\\\"org Stoye", "authors": "J\\\"org Stoye", "title": "Bounding Infection Prevalence by Bounding Selectivity and Accuracy of\n  Tests: With Application to Early COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose novel partial identification bounds on infection prevalence from\ninformation on test rate and test yield. The approach utilizes user-specified\nbounds on (i) test accuracy and (ii) the extent to which tests are targeted,\nformalized as restriction on the effect of true infection status on the odds\nratio of getting tested and thereby embeddable in logit specifications. The\nmotivating application is to the COVID-19 pandemic but the strategy may also be\nuseful elsewhere.\n  Evaluated on data from the pandemic's early stage, even the weakest of the\nnovel bounds are reasonably informative. Notably, and in contrast to\nspeculations that were widely reported at the time, they place the infection\nfatality rate for Italy well above the one of influenza by mid-April.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:43:41 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 19:05:33 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Stoye", "J\u00f6rg", ""]]}, {"id": "2008.06200", "submitter": "Michael Powers Ph.D.", "authors": "Jiansheng Dai, Ziheng Huang, Michael R. Powers, Jiaxin Xu", "title": "Characterizing the Zeta Distribution via Continuous Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We offer two novel characterizations of the Zeta distribution: first, as\ntractable continuous mixtures of Negative Binomial distributions (with fixed\nshape parameter, r > 0), and second, as a tractable continuous mixture of\nPoisson distributions. In both the Negative Binomial case for r >= 1 and the\nPoisson case, the resulting Zeta distributions are identifiable because each\nmixture can be associated with a unique mixing distribution. In the Negative\nBinomial case for 0 < r < 1, the mixing distributions are quasi-distributions\n(for which the quasi-probability density function assumes some negative\nvalues).\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 06:05:16 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 00:02:18 GMT"}, {"version": "v3", "created": "Tue, 23 Mar 2021 04:11:49 GMT"}, {"version": "v4", "created": "Fri, 4 Jun 2021 06:04:08 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Dai", "Jiansheng", ""], ["Huang", "Ziheng", ""], ["Powers", "Michael R.", ""], ["Xu", "Jiaxin", ""]]}, {"id": "2008.06296", "submitter": "Zeng Li", "authors": "Zeng Li, Chuanlong Xie, Qinwen Wang", "title": "Provable More Data Hurt in High Dimensional Least Squares Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the finite-sample prediction risk of the\nhigh-dimensional least squares estimator. We derive the central limit theorem\nfor the prediction risk when both the sample size and the number of features\ntend to infinity. Furthermore, the finite-sample distribution and the\nconfidence interval of the prediction risk are provided. Our theoretical\nresults demonstrate the sample-wise nonmonotonicity of the prediction risk and\nconfirm \"more data hurt\" phenomenon.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 11:33:30 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Li", "Zeng", ""], ["Xie", "Chuanlong", ""], ["Wang", "Qinwen", ""]]}, {"id": "2008.06344", "submitter": "Maria D. Ruiz-Medina", "authors": "A. Torres-Signes, M.P. Fr\\'ias and M.D. Ruiz-Medina", "title": "COVID-19 mortality analysis from soft-data multivariate curve regression\n  and machine learning", "comments": "This paper is currently submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multiple objective space-time forecasting approach is presented involving\ncyclical curve log-regression, and multivariate time series spatial residual\ncorrelation analysis. Specifically, the mean quadratic loss function is\nminimized in the framework of trigonometric regression. While, in our\nsubsequent spatial residual correlation analysis, maximization of the\nlikelihood allows us to compute the posterior mode in a Bayesian multivariate\ntime series soft-data framework. The presented approach is applied to the\nanalysis of COVID-19 mortality in the first wave affecting the Spanish\nCommunities, since March, 8, 2020 until May, 13, 2020. An empirical comparative\nstudy with Machine Learning (ML) regression, based on random k-fold\ncross-validation, and bootstrapping confidence interval and probability density\nestimation, is carried out. This empirical analysis also investigates the\nperformance of ML regression models in a hard- and soft- data frameworks. The\nresults could be extrapolated to other counts, countries, and posterior\nCOVID-19 waves.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 15:59:10 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 12:09:21 GMT"}, {"version": "v3", "created": "Sat, 27 Mar 2021 08:11:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Torres-Signes", "A.", ""], ["Fr\u00edas", "M. P.", ""], ["Ruiz-Medina", "M. D.", ""]]}, {"id": "2008.06366", "submitter": "Yanyi Song", "authors": "Yanyi Song, Xiang Zhou, Jian Kang, Max T. Aung, Min Zhang, Wei Zhao,\n  Belinda L. Needham, Sharon L. R. Kardia, Yongmei Liu, John D. Meeker,\n  Jennifer A. Smith, and Bhramar Mukherjee", "title": "Bayesian Sparse Mediation Analysis with Targeted Penalization of Natural\n  Indirect Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis aims to characterize an exposure's effect on an\noutcome and quantify the indirect effect that acts through a given mediator or\na group of mediators of interest. With the increasing availability of\nmeasurements on a large number of potential mediators, like the epigenome or\nthe microbiome, new statistical methods are needed to simultaneously\naccommodate high-dimensional mediators while directly target penalization of\nthe natural indirect effect (NIE) for active mediator identification. Here, we\ndevelop two novel prior models for identification of active mediators in\nhigh-dimensional mediation analysis through penalizing NIEs in a Bayesian\nparadigm. Both methods specify a joint prior distribution on the\nexposure-mediator effect and mediator-outcome effect with either (a) a\nfour-component Gaussian mixture prior or (b) a product threshold Gaussian\nprior. By jointly modeling the two parameters that contribute to the NIE, the\nproposed methods enable penalization on their product in a targeted way.\nResultant inference can take into account the four-component composite\nstructure underlying the NIE. We show through simulations that the proposed\nmethods improve both selection and estimation accuracy compared to other\ncompeting methods. We applied our methods for an in-depth analysis of two\nongoing epidemiologic studies: the Multi-Ethnic Study of Atherosclerosis (MESA)\nand the LIFECODES birth cohort. The identified active mediators in both studies\nreveal important biological pathways for understanding disease mechanisms.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 13:35:26 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Song", "Yanyi", ""], ["Zhou", "Xiang", ""], ["Kang", "Jian", ""], ["Aung", "Max T.", ""], ["Zhang", "Min", ""], ["Zhao", "Wei", ""], ["Needham", "Belinda L.", ""], ["Kardia", "Sharon L. R.", ""], ["Liu", "Yongmei", ""], ["Meeker", "John D.", ""], ["Smith", "Jennifer A.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2008.06656", "submitter": "Feng Wang", "authors": "Feng Wang, Mostafa Reisi Gahrooei, Zhen Zhong, Tao Tang, and Jianjun\n  Shi", "title": "An Augmented Regression Model for Tensors with Missing Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous but complementary sources of data provide an unprecedented\nopportunity for developing accurate statistical models of systems. Although the\nexisting methods have shown promising results, they are mostly applicable to\nsituations where the system output is measured in its complete form. In\nreality, however, it may not be feasible to obtain the complete output\nmeasurement of a system, which results in observations that contain missing\nvalues. This paper introduces a general framework that integrates tensor\nregression with tensor completion and proposes an efficient optimization\nframework that alternates between two steps for parameter estimation. Through\nmultiple simulations and a case study, we evaluate the performance of the\nproposed method. The results indicate the superiority of the proposed method in\ncomparison to a benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 05:16:05 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Feng", ""], ["Gahrooei", "Mostafa Reisi", ""], ["Zhong", "Zhen", ""], ["Tang", "Tao", ""], ["Shi", "Jianjun", ""]]}, {"id": "2008.06660", "submitter": "Stephen Lee J", "authors": "Max Luke, Priyanshi Somani, Turner Cotterman, Dhruv Suri, Stephen J.\n  Lee", "title": "No COVID-19 Climate Silver Lining in the US Power Sector", "comments": "13 pages, 6 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN physics.soc-ph q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies conclude that the global coronavirus (COVID-19) pandemic\ndecreased power sector CO$_2$ emissions globally and in the United States. In\nthis paper, we analyze the statistical significance of CO2 emissions reductions\nin the U.S. power sector from March through December 2020. We use Gaussian\nprocess (GP) regression to assess whether CO2 emissions reductions would have\noccurred with reasonable probability in the absence of COVID-19 considering\nuncertainty due to factors unrelated to the pandemic and adjusting for weather,\nseasonality, and recent emissions trends. We find that monthly CO2 emissions\nreductions are only statistically significant in April and May 2020 considering\nhypothesis tests at 5% significance levels. Separately, we consider the\npotential impact of COVID-19 on coal-fired power plant retirements through\n2022. We find that only a small percentage of U.S. coal power plants are at\nrisk of retirement due to a possible COVID-19-related sustained reduction in\nelectricity demand and prices. We observe and anticipate a return to\npre-COVID-19 CO2 emissions in the U.S. power sector.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 06:02:44 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 14:29:21 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Luke", "Max", ""], ["Somani", "Priyanshi", ""], ["Cotterman", "Turner", ""], ["Suri", "Dhruv", ""], ["Lee", "Stephen J.", ""]]}, {"id": "2008.06902", "submitter": "Federica Onori", "authors": "Federica Onori and Giovanna Jona Lasinio", "title": "Modeling \"Equitable and Sustainable Well-being\" (BES) using Bayesian\n  Networks: A Case Study of the Italian regions", "comments": "This is a pre-print of an article published in Social Indicators\n  Research. The final authenticated version is available online at:\n  https://doi.org/10.1007/s11205-020-02406-8", "journal-ref": "Soc Indic Res (2020)", "doi": "10.1007/s11205-020-02406-8", "report-no": null, "categories": "stat.AP cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Measurement of well-being has been a highly debated topic since the end of\nthe last century. While some specific aspects are still open issues, a\nmultidimensional approach as well as the construction of shared and well-rooted\nsystems of indicators are now accepted as the main route to measure this\ncomplex phenomenon. A meaningful effort, in this direction, is that of the\nItalian \"Equitable and Sustainable Well-being\" (BES) system of indicators,\ndeveloped by the Italian National Institute of Statistics (ISTAT) and the\nNational Council for Economics and Labour (CNEL). The BES framework comprises a\nnumber of atomic indicators measured yearly at the regional level and\nreflecting the different domains of well-being (e.g. Health, Education, Work \\&\nLife Balance, Environment,...). In this work we aim at dealing with the\nmultidimensionality of the BES system of indicators and try to answer three\nmain research questions: I) What is the structure of the relationships among\nthe BES atomic indicators; II) What is the structure of the relationships among\nthe BES domains; III) To what extent the structure of the relationships\nreflects the current BES theoretical framework. We address these questions by\nimplementing Bayesian Networks (BNs), a widely accepted class of multivariate\nstatistical models, particularly suitable for handling reasoning with\nuncertainty. Implementation of a BN results in a set of nodes and a set of\nconditional independence statements that provide an effective tool to explore\nassociations in a system of variables. In this work, we also suggest two\nstrategies for encoding prior knowledge in the BN estimating algorithm so that\nthe BES theoretical framework can be represented into the network.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:09:58 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Onori", "Federica", ""], ["Lasinio", "Giovanna Jona", ""]]}, {"id": "2008.06911", "submitter": "Marcos Prates O", "authors": "Douglas Roberto Mesquita Azevedo and Marcos Oliveira Prates and\n  Dipankar Bandyopadhyay", "title": "Alleviating Spatial Confounding in Spatial Frailty Models", "comments": "21 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial confounding is how is called the confounding between fixed and\nspatial random effects. It has been widely studied and it gained attention in\nthe past years in the spatial statistics literature, as it may generate\nunexpected results in modeling. The projection-based approach, also known as\nrestricted models, appears as a good alternative to overcome the spatial\nconfounding in generalized linear mixed models. However, when the support of\nfixed effects is different from the spatial effect one, this approach can no\nlonger be applied directly. In this work, we introduce a method to alleviate\nthe spatial confounding for the spatial frailty models family. This class of\nmodels can incorporate spatially structured effects and it is usual to observe\nmore than one sample unit per area which means that the support of fixed and\nspatial effects differs. In this case, we introduce a two folded\nprojection-based approach projecting the design matrix to the dimension of the\nspace and then projecting the random effect to the orthogonal space of the new\ndesign matrix. To provide fast inference in our analysis we employ the\nintegrated nested Laplace approximation methodology. The method is illustrated\nwith an application with lung and bronchus cancer in California - US that\nconfirms that the methodology efficiency.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 13:51:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Azevedo", "Douglas Roberto Mesquita", ""], ["Prates", "Marcos Oliveira", ""], ["Bandyopadhyay", "Dipankar", ""]]}, {"id": "2008.07005", "submitter": "Tiandong Wang", "authors": "Tiandong Wang and Sidney I. Resnick", "title": "A Directed Preferential Attachment Model with Poisson Measurement", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling a directed social network, one choice is to use the traditional\npreferential attachment model, which generates power-law tail distributions. In\na traditional directed preferential attachment, every new edge is added\nsequentially into the network. However, for real datasets, it is common to only\nhave coarse timestamps available, which means several new edges are created at\nthe same timestamp. Previous analyses on the evolution of social networks\nreveal that after reaching a stable phase, the growth of edge counts in a\nnetwork follows a non-homogeneous Poisson process with a constant rate across\nthe day but varying rates from day to day. Taking such empirical observations\ninto account, we propose a modified preferential attachment model with Poisson\nmeasurement, and study its asymptotic behavior. This modified model is then\nfitted to real datasets, and we see it provides a better fit than the\ntraditional one.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 21:35:45 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Wang", "Tiandong", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "2008.07044", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Jiayi Ji, Fan Li", "title": "Estimating heterogeneous survival treatment effect in observational data\n  using machine learning", "comments": "23 pages, 5 figures, 3 tables", "journal-ref": "Statistics in Medicine,2021;00:1-23 (2021)", "doi": "10.1002/sim.9090", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods for estimating heterogeneous treatment effect in observational data\nhave largely focused on continuous or binary outcomes, and have been relatively\nless vetted with survival outcomes. Using flexible machine learning methods in\nthe counterfactual framework is a promising approach to address challenges due\nto complex individual characteristics, to which treatments need to be tailored.\nTo evaluate the operating characteristics of recent survival machine learning\nmethods for the estimation of treatment effect heterogeneity and inform better\npractice, we carry out a comprehensive simulation study presenting a wide range\nof settings describing confounded heterogeneous survival treatment effects and\nvarying degrees of covariate overlap. Our results suggest that the\nnonparametric Bayesian Additive Regression Trees within the framework of\naccelerated failure time model (AFT-BART-NP) consistently yields the best\nperformance, in terms of bias, precision and expected regret. Moreover, the\ncredible interval estimators from AFT-BART-NP provide close to nominal\nfrequentist coverage for the individual survival treatment effect when the\ncovariate overlap is at least moderate. Including a non-parametrically\nestimated propensity score as an additional fixed covariate in the AFT-BART-NP\nmodel formulation can further improve its efficiency and frequentist coverage.\nFinally, we demonstrate the application of flexible causal machine learning\nestimators through a comprehensive case study examining the heterogeneous\nsurvival effects of two radiotherapy approaches for localized high-risk\nprostate cancer.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 01:02:14 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 14:09:58 GMT"}, {"version": "v3", "created": "Fri, 19 Feb 2021 19:38:10 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 15:54:08 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Hu", "Liangyuan", ""], ["Ji", "Jiayi", ""], ["Li", "Fan", ""]]}, {"id": "2008.07077", "submitter": "Francesco Denti", "authors": "Francesco Denti and Federico Camerlenghi and Michele Guindani and\n  Antonietta Mira", "title": "A Common Atom Model for the Bayesian Nonparametric Analysis of Nested\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of high-dimensional data for targeted therapeutic interventions\nrequires new ways to characterize the heterogeneity observed across subgroups\nof a specific population. In particular, models for partially exchangeable data\nare needed for inference on nested datasets, where the observations are assumed\nto be organized in different units and some sharing of information is required\nto learn distinctive features of the units. In this manuscript, we propose a\nnested Common Atoms Model (CAM) that is particularly suited for the analysis of\nnested datasets where the distributions of the units are expected to differ\nonly over a small fraction of the observations sampled from each unit. The\nproposed CAM allows a two-layered clustering at the distributional and\nobservational level and is amenable to scalable posterior inference through the\nuse of a computationally efficient nested slice-sampler algorithm. We further\ndiscuss how to extend the proposed modeling framework to handle discrete\nmeasurements, and we conduct posterior inference on a real microbiome dataset\nfrom a diet swap study to investigate how the alterations in intestinal\nmicrobiota composition are associated with different eating habits. We further\ninvestigate the performance of our model in capturing true distributional\nstructures in the population by means of a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 04:03:17 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Denti", "Francesco", ""], ["Camerlenghi", "Federico", ""], ["Guindani", "Michele", ""], ["Mira", "Antonietta", ""]]}, {"id": "2008.07183", "submitter": "Adam Griffin", "authors": "Adam Griffin and Simon E.F. Spencer and Gareth O. Roberts", "title": "An epidemic model for an evolving pathogen with strain-dependent\n  immunity", "comments": "34 pages, 7 figures, in review", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Between pandemics, the influenza virus exhibits periods of incremental\nevolution via a process known as antigenic drift. This process gives rise to a\nsequence of strains of the pathogen that are continuously replaced by newer\nstrains, preventing a build up of immunity in the host population. In this\npaper, a parsimonious epidemic model is defined that attempts to capture the\ndynamics of evolving strains within a host population. The `evolving strains'\nepidemic model has many properties that lie in-between the\nSusceptible-Infected-Susceptible and the Susceptible-Infected-Removed epidemic\nmodels, due to the fact that individuals can only be infected by each strain\nonce, but remain susceptible to reinfection by newly emerged strains. Coupling\nresults are used to identify key properties, such as the time to extinction. A\nrange of reproduction numbers are explored to characterize the model, including\na novel quasi-stationary reproduction number that can be used to describe the\nre-emergence of the pathogen into a population with `average' levels of strain\nimmunity, analogous to the beginning of the winter peak in influenza. Finally\nthe quasi-stationary distribution of the evolving strains model is explored via\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 09:53:54 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Griffin", "Adam", ""], ["Spencer", "Simon E. F.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2008.07320", "submitter": "Charlie Kirkwood", "authors": "Charlie Kirkwood, Theo Economou, Nicolas Pugeault", "title": "Bayesian deep learning for mapping via auxiliary information: a new era\n  for geostatistics?", "comments": "10 pages, 5 figures, version submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For geospatial modelling and mapping tasks, variants of kriging - the spatial\ninterpolation technique developed by South African mining engineer Danie Krige\n- have long been regarded as the established geostatistical methods. However,\nkriging and its variants (such as regression kriging, in which auxiliary\nvariables or derivatives of these are included as covariates) are relatively\nrestrictive models and lack capabilities that have been afforded to us in the\nlast decade by deep neural networks. Principal among these is feature learning\n- the ability to learn filters to recognise task-specific patterns in gridded\ndata such as images. Here we demonstrate the power of feature learning in a\ngeostatistical context, by showing how deep neural networks can automatically\nlearn the complex relationships between point-sampled target variables and\ngridded auxiliary variables (such as those provided by remote sensing), and in\ndoing so produce detailed maps of chosen target variables. At the same time, in\norder to cater for the needs of decision makers who require well-calibrated\nprobabilities, we obtain uncertainty estimates via a Bayesian approximation\nknown as Monte Carlo dropout. In our example, we produce a national-scale\nprobabilistic geochemical map from point-sampled assay data, with auxiliary\ninformation provided by a terrain elevation grid. Unlike traditional\ngeostatistical approaches, auxiliary variable grids are fed into our deep\nneural network raw. There is no need to provide terrain derivatives (e.g. slope\nangles, roughness, etc) because the deep neural network is capable of learning\nthese and arbitrarily more complex derivatives as necessary to maximise\npredictive performance. We hope our results will raise awareness of the\nsuitability of Bayesian deep learning - and its feature learning capabilities -\nfor large-scale geostatistical applications where uncertainty matters.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 13:56:43 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 12:11:59 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2020 20:22:04 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Kirkwood", "Charlie", ""], ["Economou", "Theo", ""], ["Pugeault", "Nicolas", ""]]}, {"id": "2008.07361", "submitter": "Luis John", "authors": "Luis H. John, Jan A. Kors, Jenna M. Reps, Patrick B. Ryan, Peter R.\n  Rijnbeek", "title": "How little data do we need for patient-level prediction?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: Provide guidance on sample size considerations for developing\npredictive models by empirically establishing the adequate sample size, which\nbalances the competing objectives of improving model performance and reducing\nmodel complexity as well as computational requirements.\n  Materials and Methods: We empirically assess the effect of sample size on\nprediction performance and model complexity by generating learning curves for\n81 prediction problems in three large observational health databases, requiring\ntraining of 17,248 prediction models. The adequate sample size was defined as\nthe sample size for which the performance of a model equalled the maximum model\nperformance minus a small threshold value.\n  Results: The adequate sample size achieves a median reduction of the number\nof observations between 9.5% and 78.5% for threshold values between 0.001 and\n0.02. The median reduction of the number of predictors in the models at the\nadequate sample size varied between 8.6% and 68.3%, respectively.\n  Discussion: Based on our results a conservative, yet significant, reduction\nin sample size and model complexity can be estimated for future prediction\nwork. Though, if a researcher is willing to generate a learning curve a much\nlarger reduction of the model complexity may be possible as suggested by a\nlarge outcome-dependent variability.\n  Conclusion: Our results suggest that in most cases only a fraction of the\navailable data was sufficient to produce a model close to the performance of\none developed on the full data set, but with a substantially reduced model\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 11:00:13 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["John", "Luis H.", ""], ["Kors", "Jan A.", ""], ["Reps", "Jenna M.", ""], ["Ryan", "Patrick B.", ""], ["Rijnbeek", "Peter R.", ""]]}, {"id": "2008.07438", "submitter": "Jiangbin Lyu Dr.", "authors": "Jiangbin Lyu, Dan Yu and Liqun Fu", "title": "Analysis and Optimization for Large-Scale LoRa Networks: Throughput\n  Fairness and Scalability", "comments": "Propose stochastic geometry-based framework for modeling/analyzing\n  large-scale LoRa networks with channel fading/aggregate interference/packet\n  overlapping/multi-GW reception. Jointly optimize SF/Tx-power/duty-cycle based\n  on channel statistics and UE distribution, achieving both fairness and power\n  savings while improving cell-edge throughput and spatial (sum) throughput for\n  the majority of UEs. arXiv admin note: substantial text overlap with\n  arXiv:1904.12300", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NI cs.SY eess.SY math.IT math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing popularity, LoRa networks are pivotally enabling Long Range\nconnectivity to low-cost and power-constrained user equipments (UEs). Due to\nits wide coverage area, a critical issue is to effectively allocate wireless\nresources to support potentially massive UEs while resolving the prominent\nnear-far fairness problem in the LoRa network, which is challenging due to the\nlack of tractable analytical model and its practical requirement for\nlow-complexity and low-overhead design. To achieve massive connectivity with\nfairness, we aim to maximize the minimum throughput of all UEs, and propose\nhigh-level policies of joint spreading factor (SF) allocation, power control,\nand duty cycle adjustment based only on average channel statistics and spatial\nUE distribution. By leveraging on the Poisson rain model along with tailored\nmodifications to our considered LoRa network under both single-cell and\nmulti-cell setups, we are able to account for channel fading, aggregate\ninterference, accurate packet overlapping, and/or multi-gateway packet\nreception, and still obtain tractable and accurate formulas for the packet\nsuccess probability and hence throughput. We further propose an iterative\nbalancing (IB) method to allocate the SFs in the cell such that the overall\nmax-min throughput can be achieved. Numerical results show that the proposed\nscheme with optimized design greatly alleviates the near-far fairness issue and\nalso reduces the spatial power consumption, while significantly improving the\ncell-edge throughput as well as the spatial (sum) throughput for the majority\nof UEs, especially for large-scale LoRa networks with massive UEs and high\ngateway density.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 15:56:43 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 09:17:23 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Lyu", "Jiangbin", ""], ["Yu", "Dan", ""], ["Fu", "Liqun", ""]]}, {"id": "2008.07478", "submitter": "Akisato Suzuki Dr", "authors": "Akisato Suzuki", "title": "Presenting the Probabilities of Different Effect Sizes: Towards a Better\n  Understanding and Communication of Statistical Uncertainty", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should social scientists understand and communicate the uncertainty of\nstatistically estimated causal effects? It is well-known that the conventional\nsignificance-vs.-insignificance approach is associated with misunderstandings\nand misuses. Behavioral research suggests people understand uncertainty more\nappropriately in a numerical, continuous scale than in a verbal, discrete\nscale. Motivated by these backgrounds, I propose presenting the probabilities\nof different effect sizes. Probability as an intuitive continuous measure of\nuncertainty allows researchers to better understand and communicate the\nuncertainty of the statistically estimated effects. In addition, my approach\nneeds no decision threshold for an uncertainty measure or effect size, unlike\nthe conventional approaches, allowing researchers to be agnostic about a\ndecision threshold such as p<5% and a justification for that. I apply my\napproach to a previous social scientific study, showing it enables richer\ninference than the significance-vs.-insignificance approach taken by the\noriginal study. The accompanying R package makes my approach easy to implement.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:09:53 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Suzuki", "Akisato", ""]]}, {"id": "2008.07486", "submitter": "Na Li", "authors": "Na Li, Fei Chiang, Douglas G. Down, Nancy M. Heddle", "title": "A decision integration strategy for short-term demand forecasting and\n  ordering for red blood cell components", "comments": "This file includes 30 pages with 10 figures in total. This work has\n  been submitted to Operations Research for Health Care for publication in\n  August 2020", "journal-ref": "Operations Research for Health Care, Volume 29, June 2021, 100290", "doi": "10.1016/j.orhc.2021.100290", "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blood transfusion is one of the most crucial and commonly administered\ntherapeutics worldwide. The need for more accurate and efficient ways to manage\nblood demand and supply is an increasing concern. Building a technology-based,\nrobust blood demand and supply chain that can achieve the goals of reducing\nordering frequency, inventory level, wastage and shortage, while maintaining\nthe safety of blood usage, is essential in modern healthcare systems. In this\nstudy, we summarize the key challenges in current demand and supply management\nfor red blood cells (RBCs). We combine ideas from statistical time series\nmodeling, machine learning, and operations research in developing an ordering\ndecision strategy for RBCs, through integrating a hybrid demand forecasting\nmodel using clinical predictors and a data-driven multi-period inventory\nproblem considering inventory and reorder constraints. We have applied the\nintegrated ordering strategy to the blood inventory management system in\nHamilton, Ontario using a large clinical database from 2008 to 2018. The\nproposed hybrid demand forecasting model provides robust and accurate\npredictions, and identifies important clinical predictors for short-term RBC\ndemand forecasting. Compared with the actual historical data, our integrated\nordering strategy reduces the inventory level by 40% and decreases the ordering\nfrequency by 60%, with low incidence of shortages and wastage due to\nexpiration. If implemented successfully, our proposed strategy can achieve\nsignificant cost savings for healthcare systems and blood suppliers. The\nproposed ordering strategy is generalizable to other blood products or even\nother perishable products.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 17:27:26 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Li", "Na", ""], ["Chiang", "Fei", ""], ["Down", "Douglas G.", ""], ["Heddle", "Nancy M.", ""]]}, {"id": "2008.07538", "submitter": "Kaisey Mandel", "authors": "Kaisey S. Mandel, Stephen Thorp, Gautham Narayan, Andrew S. Friedman,\n  Arturo Avelino", "title": "A Hierarchical Bayesian SED Model for Type Ia Supernovae in the Optical\n  to Near-Infrared", "comments": "Dedicated to the memory of Andy Friedman. submitted to MNRAS; more\n  polished", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While conventional Type Ia supernova (SN Ia) cosmology analyses rely\nprimarily on rest-frame optical light curves to determine distances, SNe Ia are\nexcellent standard candles in near-infrared (NIR) light, which is significantly\nless sensitive to dust extinction. A SN Ia spectral energy distribution (SED)\nmodel capable of fitting rest-frame NIR observations is necessary to fully\nleverage current and future SN Ia datasets from ground- and space-based\ntelescopes including HST, LSST, JWST, and RST. We construct a hierarchical\nBayesian model for SN Ia SEDs, continuous over time and wavelength, from the\noptical to NIR ($B$ through $H$, or $0.35 -1.8\\, \\mu$m). We model the SED as a\ncombination of physically-distinct host galaxy dust and intrinsic spectral\ncomponents. The distribution of intrinsic SEDs over time and wavelength is\nmodelled with probabilistic functional principal components and the covariance\nof residual functions. We train the model on a nearby sample of 79 SNe Ia with\njoint optical and NIR light curves by sampling the global posterior\ndistribution over dust and intrinsic latent variables, SED components, and\npopulation hyperparameters. The photometric distances of SNe Ia with NIR data\nnear maximum light obtain a total RMS error of 0.10 mag with our BayeSN model,\ncompared to 0.14 mag with SNooPy and SALT2 for the same sample. Jointly fitting\nthe optical and NIR data of the full sample for a global host dust law, we find\n$R_V = 2.9 \\pm 0.2$, consistent with the Milky Way average.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 18:00:01 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 17:56:50 GMT"}, {"version": "v3", "created": "Mon, 24 Aug 2020 17:58:18 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Mandel", "Kaisey S.", ""], ["Thorp", "Stephen", ""], ["Narayan", "Gautham", ""], ["Friedman", "Andrew S.", ""], ["Avelino", "Arturo", ""]]}, {"id": "2008.07581", "submitter": "Hoang Anh Ngo", "authors": "Hoang Anh Ngo (\\'Ecole Polytechnique, Palaiseau, France) and Thai Nam\n  HOANG (Beloit College, Wisconsin, United States)", "title": "A Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1,1) and\n  application in predicting total COVID-19 infected cases", "comments": "Accepted paper at the 2020 International Congress of Grey Systems and\n  Uncertainty Analysis (GSUA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Nonlinear Grey Bernoulli Model NGBM(1, 1) is a recently developed grey\nmodel which has various applications in different fields, mainly due to its\naccuracy in handling small time-series datasets with nonlinear variations. In\nthis paper, to fully improve the accuracy of this model, a novel model is\nproposed, namely Rolling Optimized Nonlinear Grey Bernoulli Model RONGBM(1, 1).\nThis model combines the rolling mechanism with the simultaneous optimization of\nall model parameters (exponential, background value and initial condition). The\naccuracy of this new model has significantly been proven through forecasting\nVietnam's GDP from 2013 to 2018, before it is applied to predict the total\nCOVID-19 infected cases globally by day.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:09:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ngo", "Hoang Anh", "", "\u00c9cole Polytechnique, Palaiseau, France"], ["HOANG", "Thai Nam", "", "Beloit College, Wisconsin, United States"]]}, {"id": "2008.07653", "submitter": "David Huberman", "authors": "David B. Huberman, Brian J. Reich, and Howard D. Bondell", "title": "Nonparametric Conditional Density Estimation In A Deep Learning\n  Framework For Short-Term Forecasting", "comments": "44 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Short-term forecasting is an important tool in understanding environmental\nprocesses. In this paper, we incorporate machine learning algorithms into a\nconditional distribution estimator for the purposes of forecasting tropical\ncyclone intensity. Many machine learning techniques give a single-point\nprediction of the conditional distribution of the target variable, which does\nnot give a full accounting of the prediction variability. Conditional\ndistribution estimation can provide extra insight on predicted response\nbehavior, which could influence decision-making and policy. We propose a\ntechnique that simultaneously estimates the entire conditional distribution and\nflexibly allows for machine learning techniques to be incorporated. A smooth\nmodel is fit over both the target variable and covariates, and a logistic\ntransformation is applied on the model output layer to produce an expression of\nthe conditional density function. We provide two examples of machine learning\nmodels that can be used, polynomial regression and deep learning models. To\nachieve computational efficiency we propose a case-control sampling\napproximation to the conditional distribution. A simulation study for four\ndifferent data distributions highlights the effectiveness of our method\ncompared to other machine learning-based conditional distribution estimation\ntechniques. We then demonstrate the utility of our approach for forecasting\npurposes using tropical cyclone data from the Atlantic Seaboard. This paper\ngives a proof of concept for the promise of our method, further computational\ndevelopments can fully unlock its insights in more complex forecasting and\nother applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 22:31:19 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Huberman", "David B.", ""], ["Reich", "Brian J.", ""], ["Bondell", "Howard D.", ""]]}, {"id": "2008.07840", "submitter": "Marcos Matabuena", "authors": "Marcos Matabuena, Alexander Petersen, Juan C.Vidal and Francisco Gude", "title": "Glucodensities: a new representation of glucose profiles using\n  distributional data analysis", "comments": null, "journal-ref": null, "doi": "10.1177/0962280221998064", "report-no": null, "categories": "stat.AP q-bio.QM stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biosensor data has the potential ability to improve disease control and\ndetection. However, the analysis of these data under free-living conditions is\nnot feasible with current statistical techniques. To address this challenge, we\nintroduce a new functional representation of biosensor data, termed the\nglucodensity, together with a data analysis framework based on distances\nbetween them. The new data analysis procedure is illustrated through an\napplication in diabetes with continuous-time glucose monitoring (CGM) data. In\nthis domain, we show marked improvement with respect to state of the art\nanalysis methods. In particular, our findings demonstrate that i) the\nglucodensity possesses an extraordinary clinical sensitivity to capture the\ntypical biomarkers used in the standard clinical practice in diabetes, ii)\nprevious biomarkers cannot accurately predict glucodensity, so that the latter\nis a richer source of information, and iii) the glucodensity is a natural\ngeneralization of the time in range metric, this being the gold standard in the\nhandling of CGM data. Furthermore, the new method overcomes many of the\ndrawbacks of time in range metrics, and provides deeper insight into assessing\nglucose metabolism.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 10:26:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Matabuena", "Marcos", ""], ["Petersen", "Alexander", ""], ["Vidal", "Juan C.", ""], ["Gude", "Francisco", ""]]}, {"id": "2008.07857", "submitter": "Jonas Bhend", "authors": "Regula Keller, Jan Rajczak, Jonas Bhend, Christoph Spirig, Stephan\n  Hemri, Mark A. Liniger, Heini Wernli", "title": "Seamless multi-model postprocessing for air temperature forecasts in\n  complex topography", "comments": "16 pages, 7 figures, submitted to Weather and Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical postprocessing is routinely applied to correct systematic errors\nof numerical weather prediction models (NWP) and to automatically produce\ncalibrated local forecasts for end-users. Postprocessing is particularly\nrelevant in complex terrain, where even state-of-the-art high-resolution NWP\nsystems cannot resolve many of the small-scale processes shaping local weather\nconditions. In addition, statistical postprocessing can also be used to combine\nforecasts from multiple NWP systems. Here we assess an ensemble model output\nstatistics (EMOS) approach to produce seamless temperature forecasts based on a\ncombination of short-term ensemble forecasts from a convection-permitting\nlimited-area ensemble and a medium-range global ensemble forecasting model. We\nquantify the benefit of this approach compared to only processing the\nhigh-resolution NWP. We calibrate and combine 2-m air temperature predictions\nfor a large set of Swiss weather stations at the hourly time-scale. The\nmulti-model EMOS approach ('Mixed EMOS') is able to improve forecasts by 30\\%\nwith respect to direct model output from the high-resolution NWP. A detailed\nevaluation of Mixed EMOS reveals that it outperforms either single-model EMOS\nversion by 8-12\\%. Valley location profit particularly from the model\ncombination. All forecast variants perform worst in winter (DJF), however\ncalibration and model combination improves forecast quality substantially.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 11:08:02 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 06:41:38 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Keller", "Regula", ""], ["Rajczak", "Jan", ""], ["Bhend", "Jonas", ""], ["Spirig", "Christoph", ""], ["Hemri", "Stephan", ""], ["Liniger", "Mark A.", ""], ["Wernli", "Heini", ""]]}, {"id": "2008.07881", "submitter": "Regina Stegherr", "authors": "Kaspar Rufibach, Regina Stegherr, Claudia Schmoor, Valentine Jehl,\n  Arthur Allignol, Annette Boeckenhoff, Cornelia Dunger-Baldauf, Lewin Eisele,\n  Thomas K\\\"unzel, Katrin Kupas, Friedhelm Leverkus, Matthias Trampisch, Yumin\n  Zhao, Tim Friede and Jan Beyersmann", "title": "Survival analysis for AdVerse events with VarYing follow-up times\n  (SAVVY) -- comparison of adverse event risks in randomized controlled trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of adverse events (AEs) are an important aspect of benefit-risk and\nhealth-technology assessments of therapies. The SAVVY project aims to improve\nthe analyses of AE data in clinical trials through the use of survival\ntechniques appropriately dealing with varying follow-up times and competing\nevents (CEs). In an empirical study including randomized clinical trials (RCT)\nfrom several sponsor organisations the effect of varying follow-up times and\nCEs on comparisons of two treatment arms with respect to AE risks is\ninvestigated. CEs definition does not only include death before AE but also end\nof follow-up for AEs due to events possibly related to the disease course or\nsafety of the treatment. The comparisons of relative risks (RRs) of standard\nprobability-based estimators to the gold-standard Aalen-Johansen estimator\n(AJE) or hazard-based estimators to an estimated hazard ratio (HR) from Cox\nregression are done descriptively, with graphical displays, and using a random\neffects meta-analysis. The influence of different factors on the size of the\nbias is investigated in a meta-regression. Ten sponsors provided 17 RCTs\nincluding 186 types of AEs. We confirm for estimation of the RR concerns\nregarding incidence densities: the probability transform incidence density\nignoring CEs performed worst. However, accounting for CEs in an analysis that\nparametrically mimicked the non-parametric AJE performed better than both one\nminus Kaplan-Meier and AJE that only considered death as a CE. The analysis\nbased on hazards revealed that the incidence density underestimates the HR of\nAE and CE of death hazard compared to the gold-standard Cox regression. Both\nthe choice of the estimator of the AE probability and a careful definition of\nCEs are crucial in estimation of RRs. For RRs based on hazards, the HR based on\nCox regression has better properties than the ratio of incidence densities.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 12:15:49 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Rufibach", "Kaspar", ""], ["Stegherr", "Regina", ""], ["Schmoor", "Claudia", ""], ["Jehl", "Valentine", ""], ["Allignol", "Arthur", ""], ["Boeckenhoff", "Annette", ""], ["Dunger-Baldauf", "Cornelia", ""], ["Eisele", "Lewin", ""], ["K\u00fcnzel", "Thomas", ""], ["Kupas", "Katrin", ""], ["Leverkus", "Friedhelm", ""], ["Trampisch", "Matthias", ""], ["Zhao", "Yumin", ""], ["Friede", "Tim", ""], ["Beyersmann", "Jan", ""]]}, {"id": "2008.07883", "submitter": "Regina Stegherr", "authors": "Regina Stegherr, Claudia Schmoor, Jan Beyersmann, Kaspar Rufibach,\n  Valentine Jehl, Andreas Br\\\"uckner, Lewin Eisele, Thomas K\\\"unzel, Katrin\n  Kupas, Frank Langer, Friedhelm Leverkus, Anja Loos, Christiane Norenberg,\n  Florian Voss and Tim Friede", "title": "Survival analysis for AdVerse events with VarYing follow-up times\n  (SAVVY) -- estimation of adverse event risks", "comments": null, "journal-ref": null, "doi": "10.1186/s13063-021-05354-x", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SAVVY project aims to improve the analyses of adverse event (AE) data in\nclinical trials through the use of survival techniques appropriately dealing\nwith varying follow-up times and competing events (CEs). Although statistical\nmethodologies have advanced, in AE analyses often the incidence proportion, the\nincidence density, or a non-parametric Kaplan-Meier estimator (KME) are used,\nwhich either ignore censoring or CEs. In an empirical study including\nrandomized clinical trials from several sponsor organisations, these potential\nsources of bias are investigated. The main aim is to compare the estimators\nthat are typically used in AE analysis to the Aalen-Johansen estimator (AJE) as\nthe gold-standard. Here, one-sample findings are reported, while a companion\npaper considers consequences when comparing treatment groups. Estimators are\ncompared with descriptive statistics, graphical displays and with a random\neffects meta-analysis. The influence of different factors on the size of the\nbias is investigated in a meta-regression. Comparisons are conducted at the\nmaximum follow-up time and at earlier evaluation time points. CEs definition\ndoes not only include death before AE but also end of follow-up for AEs due to\nevents possibly related to the disease course or the treatment. Ten sponsor\norganisations provided 17 trials including 186 types of AEs. The one minus KME\nwas on average about 1.2-fold larger than the AJE. Leading forces influencing\nbias were the amount of censoring and of CEs. As a consequence, the average\nbias using the incidence proportion was less than 5%. Assuming constant hazards\nusing incidence densities was hardly an issue provided that CEs were accounted\nfor. There is a need to improve the guidelines of reporting risks of AEs so\nthat the KME and the incidence proportion are replaced by the AJE with an\nappropriate definition of CEs.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 12:17:47 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Stegherr", "Regina", ""], ["Schmoor", "Claudia", ""], ["Beyersmann", "Jan", ""], ["Rufibach", "Kaspar", ""], ["Jehl", "Valentine", ""], ["Br\u00fcckner", "Andreas", ""], ["Eisele", "Lewin", ""], ["K\u00fcnzel", "Thomas", ""], ["Kupas", "Katrin", ""], ["Langer", "Frank", ""], ["Leverkus", "Friedhelm", ""], ["Loos", "Anja", ""], ["Norenberg", "Christiane", ""], ["Voss", "Florian", ""], ["Friede", "Tim", ""]]}, {"id": "2008.07924", "submitter": "Evelyne Vigneau", "authors": "Evelyne Vigneau", "title": "Clustering of variables for enhanced interpretability of predictive\n  models", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new strategy is proposed for building easy to interpret predictive models\nin the context of a high-dimensional dataset, with a large number of highly\ncorrelated explanatory variables. The strategy is based on a first step of\nvariables clustering using the CLustering of Variables around Latent Variables\n(CLV) method. The exploration of the hierarchical clustering dendrogram is\nundertaken in order to sequentially select the explanatory variables in a\ngroup-wise fashion. For model setting implementation, the dendrogram is used as\nthe base-learner in an L2-boosting procedure. The proposed approach, named\nlmCLV, is illustrated on the basis of a toy-simulated example when the clusters\nand predictive equation are already known, and on a real case study dealing\nwith the authentication of orange juices based on 1H-NMR spectroscopic\nanalysis. In both illustrative examples, this procedure was shown to have\nsimilar predictive efficiency to other methods, with additional\ninterpretability capacity. It is available in the R package ClustVarLV.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 13:32:41 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Vigneau", "Evelyne", ""]]}, {"id": "2008.08004", "submitter": "Jesus Lago", "authors": "Jesus Lago, Grzegorz Marcjasz, Bart De Schutter, Rafa{\\l} Weron", "title": "Forecasting day-ahead electricity prices: A review of state-of-the-art\n  algorithms, best practices and an open-access benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the field of electricity price forecasting has benefited from plenty of\ncontributions in the last two decades, it arguably lacks a rigorous approach to\nevaluating new predictive algorithms. The latter are often compared using\nunique, not publicly available datasets and across too short and limited to one\nmarket test samples. The proposed new methods are rarely benchmarked against\nwell established and well performing simpler models, the accuracy metrics are\nsometimes inadequate and testing the significance of differences in predictive\nperformance is seldom conducted. Consequently, it is not clear which methods\nperform well nor what are the best practices when forecasting electricity\nprices. In this paper, we tackle these issues by performing a literature survey\nof state-of-the-art models, comparing state-of-the-art statistical and deep\nlearning methods across multiple years and markets, and by putting forward a\nset of best practices. In addition, we make available the considered datasets,\nforecasts of the state-of-the-art models, and a specifically designed python\ntoolbox, so that new algorithms can be rigorously evaluated in future studies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:19:20 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 15:01:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Lago", "Jesus", ""], ["Marcjasz", "Grzegorz", ""], ["De Schutter", "Bart", ""], ["Weron", "Rafa\u0142", ""]]}, {"id": "2008.08006", "submitter": "Jesus Lago", "authors": "Grzegorz Marcjasz, Jesus Lago, Rafa{\\l} Weron", "title": "Neural networks in day-ahead electricity price forecasting: Single vs.\n  multiple outputs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in the fields of artificial intelligence and machine\nlearning methods resulted in a significant increase of their popularity in the\nliterature, including electricity price forecasting. Said methods cover a very\nbroad spectrum, from decision trees, through random forests to various\nartificial neural network models and hybrid approaches. In electricity price\nforecasting, neural networks are the most popular machine learning method as\nthey provide a non-linear counterpart for well-tested linear regression models.\nTheir application, however, is not straightforward, with multiple\nimplementation factors to consider. One of such factors is the network's\nstructure. This paper provides a comprehensive comparison of two most common\nstructures when using the deep neural networks -- one that focuses on each hour\nof the day separately, and one that reflects the daily auction structure and\nmodels vectors of the prices. The results show a significant accuracy advantage\nof using the latter, confirmed on data from five distinct power exchanges.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 16:20:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Marcjasz", "Grzegorz", ""], ["Lago", "Jesus", ""], ["Weron", "Rafa\u0142", ""]]}, {"id": "2008.08156", "submitter": "Roberto Rojas-Cessa", "authors": "Sina Fathi-Kazerooni, Roberto Rojas-Cessa, Ziqian Dong, and\n  Vatcharapan Umpaichitra", "title": "Time Series Analysis and Correlation of Subway Turnstile Usage and\n  COVID-19 Prevalence in New York City", "comments": "S. Fathi-Kazerooni, R. Rojas-Cessa, Z. Dong, and V. Umpaichitra,\n  \"Time Series Analysis and Correlation of Subway Turnstile Usage and COVID-19\n  Prevalence in New York City,\" pp. 1-8, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show a strong correlation between turnstile usage data of\nthe New York City subway provided by the Metropolitan Transport Authority of\nNew York City and COVID-19 deaths and cases reported by the New York City\nDepartment of Health. The turnstile usage data not only indicate the usage of\nthe city's subway but also people's activity that promoted the large prevalence\nof COVID-19 city dwellers experienced from March to May of 2020. While this\ncorrelation is apparent, no proof has been provided before. Here we demonstrate\nthis correlation through the application of a long short-term memory neural\nnetwork. We show that the correlation of COVID-19 prevalence and deaths\nconsiders the incubation and symptomatic phases on reported deaths. Having\nestablished this correlation, we estimate the dates when the number of COVID-19\ndeaths and cases would approach zero after the reported number of deaths were\ndecreasing by using the Auto-Regressive Integrated Moving Average model. We\nalso estimate the dates when the first cases and deaths occurred by\nback-tracing the data sets and compare them to the reported dates.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 18:32:25 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 21:53:22 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Fathi-Kazerooni", "Sina", ""], ["Rojas-Cessa", "Roberto", ""], ["Dong", "Ziqian", ""], ["Umpaichitra", "Vatcharapan", ""]]}, {"id": "2008.08197", "submitter": "Pedro Ramos", "authors": "Francisco Louzada, Jos\\'e A. Cuminato, Oscar M. H. Rodriguez, Vera L.\n  D. Tomazella, Eder A. Milani, Paulo H. Ferreira, Pedro L. Ramos, Gustavo\n  Bochio, Ivan C. Perissini, Oilson A. Gonzatto Junior, Alex L. Mota, Luis F.\n  A. Alegr\\'ia, Danilo Colombo, Paulo G. O. Oliveira, Hugo F. L. Santos, Marcus\n  V. C. Magalh\\~aes", "title": "Incorporation of frailties into a non-proportional hazard regression\n  model and its diagnostics for reliability modeling of downhole safety valves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, our proposal consists of incorporating frailty into a\nstatistical methodology for modeling time-to-event data, based on\nnon-proportional hazards regression model. Specifically, we use the generalized\ntime-dependent logistic (GTDL) model with a frailty term introduced in the\nhazard function to control for unobservable heterogeneity among the sampling\nunits. We also add a regression in the parameter that measures the effect of\ntime, since it can directly reflect the influence of covariates on the effect\nof time-to-failure. The practical relevance of the proposed model is\nillustrated in a real problem based on a data set for downhole safety valves\n(DHSVs) used in offshore oil and gas production wells. The reliability\nestimation of DHSVs can be used, among others, to predict the blowout\noccurrence, assess the workover demand and aid decision-making actions.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 23:53:11 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2020 16:05:25 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 19:43:21 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Louzada", "Francisco", ""], ["Cuminato", "Jos\u00e9 A.", ""], ["Rodriguez", "Oscar M. H.", ""], ["Tomazella", "Vera L. D.", ""], ["Milani", "Eder A.", ""], ["Ferreira", "Paulo H.", ""], ["Ramos", "Pedro L.", ""], ["Bochio", "Gustavo", ""], ["Perissini", "Ivan C.", ""], ["Junior", "Oilson A. Gonzatto", ""], ["Mota", "Alex L.", ""], ["Alegr\u00eda", "Luis F. A.", ""], ["Colombo", "Danilo", ""], ["Oliveira", "Paulo G. O.", ""], ["Santos", "Hugo F. L.", ""], ["Magalh\u00e3es", "Marcus V. C.", ""]]}, {"id": "2008.08236", "submitter": "Lu Cheng", "authors": "Lu Cheng, Ruocheng Guo, Huan Liu", "title": "Long-Term Effect Estimation with Surrogate Representation", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are many scenarios where short- and long-term causal effects of an\nintervention are different. For example, low-quality ads may increase\nshort-term ad clicks but decrease the long-term revenue via reduced clicks.\nThis work, therefore, studies the problem of long-term effect where the outcome\nof primary interest, or primary outcome, takes months or even years to\naccumulate. The observational study of long-term effect presents unique\nchallenges. First, the confounding bias causes large estimation error and\nvariance, which can further accumulate towards the prediction of primary\noutcomes. Second, short-term outcomes are often directly used as the proxy of\nthe primary outcome, i.e., the surrogate. Nevertheless, this method entails the\nstrong surrogacy assumption that is often impractical. To tackle these\nchallenges, we propose to build connections between long-term causal inference\nand sequential models in machine learning. This enables us to learn surrogate\nrepresentations that account for the temporal unconfoundedness and circumvent\nthe stringent surrogacy assumption by conditioning on the inferred time-varying\nconfounders. Experimental results show that the proposed framework outperforms\nthe state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:16:18 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 16:52:05 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Cheng", "Lu", ""], ["Guo", "Ruocheng", ""], ["Liu", "Huan", ""]]}, {"id": "2008.08262", "submitter": "Jessica Hoffmann", "authors": "Jessica Hoffmann, Matt Jordan, Constantine Caramanis", "title": "Quarantines as a Targeted Immunization Strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of the recent COVID-19 outbreak, quarantine has been used to\n\"flatten the curve\" and slow the spread of the disease. In this paper, we show\nthat this is not the only benefit of quarantine for the mitigation of an SIR\nepidemic spreading on a graph. Indeed, human contact networks exhibit a\npowerlaw structure, which means immunizing nodes at random is extremely\nineffective at slowing the epidemic, while immunizing high-degree nodes can\nefficiently guarantee herd immunity. We theoretically prove that if quarantines\nare declared at the right moment, high-degree nodes are disproportionately in\nthe Removed state, which is a form of targeted immunization. Even if\nquarantines are declared too early, subsequent waves of infection spread slower\nthan the first waves. This leads us to propose an opening and closing strategy\naiming at immunizing the graph while infecting the minimum number of\nindividuals, guaranteeing the population is now robust to future infections. To\nthe best of our knowledge, this is the only strategy that guarantees herd\nimmunity without requiring vaccines. We extensively verify our results on\nsimulated and real-life networks.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 04:57:42 GMT"}, {"version": "v2", "created": "Tue, 16 Feb 2021 20:43:20 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 01:43:28 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Hoffmann", "Jessica", ""], ["Jordan", "Matt", ""], ["Caramanis", "Constantine", ""]]}, {"id": "2008.08335", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Heather Booth", "title": "Synergy in fertility forecasting: Improving forecast accuracy through\n  model averaging", "comments": "38 pages, 5 figures, to appear at Genus: Journal of Population\n  Sciences", "journal-ref": "Genus, 2020, 76, Article number 27", "doi": "10.1186/s41118-020-00099-y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accuracy in fertility forecasting has proved challenging and warrants renewed\nattention. One way to improve accuracy is to combine the strengths of a set of\nexisting models through model averaging. The model-averaged forecast is derived\nusing empirical model weights that optimise forecast accuracy at each forecast\nhorizon based on historical data. We apply model averaging to fertility\nforecasting for the first time, using data for 17 countries and six models.\nFour model-averaging methods are compared: frequentist, Bayesian, model\nconfidence set, and equal weights. We compute individual-model and\nmodel-averaged point and interval forecasts at horizons of one to 20 years. We\ndemonstrate gains in average accuracy of 4-23\\% for point forecasts and 3-24\\%\nfor interval forecasts, with greater gains from the frequentist and\nequal-weights approaches at longer horizons. Data for England \\& Wales are used\nto illustrate model averaging in forecasting age-specific fertility to 2036.\nThe advantages and further potential of model averaging for fertility\nforecasting are discussed. As the accuracy of model-averaged forecasts depends\non the accuracy of the individual models, there is ongoing need to develop\nbetter models of fertility for use in forecasting and model averaging. We\nconclude that model averaging holds considerable promise for the improvement of\nfertility forecasting in a systematic way using existing models and warrants\nfurther investigation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 08:55:17 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Shang", "Han Lin", ""], ["Booth", "Heather", ""]]}, {"id": "2008.08358", "submitter": "Rohan Arambepola", "authors": "Rohan Arambepola, Suzanne H. Keddie, Emma L. Collins, Katherine A.\n  Twohig, Punam Amratia, Amelia Bertozzi-Villa, Elisabeth G. Chestnutt, Joseph\n  Harris, Justin Millar, Jennifer Rozier, Susan F. Rumisha, Tasmin L. Symons,\n  Camilo Vargas-Ruiz, Mauricette Andriamananjara, Saraha Rabeherisoa, Ars\\`ene\n  C. Ratsimbasoa, Rosalind E. Howes, Daniel J. Weiss, Peter W. Gething and Ewan\n  Cameron", "title": "Spatiotemporal mapping of malaria prevalence in Madagascar using routine\n  surveillance and health survey data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malaria transmission in Madagascar is highly heterogeneous, exhibiting\nspatial, seasonal and long-term trends. Previous efforts to map malaria risk in\nMadagascar used prevalence data from Malaria Indicator Surveys. These\ncross-sectional surveys, conducted during the high transmission season most\nrecently in 2013 and 2016, provide nationally representative prevalence data\nbut cover relatively short time frames. Conversely, monthly case data are\ncollected at health facilities but suffer from biases, including incomplete\nreporting.\n  We combined survey and case data to make monthly maps of prevalence between\n2013 and 2016. Health facility catchments were estimated and incidence\nsurfaces, environmental and socioeconomic covariates, and survey data informed\na Bayesian prevalence model. Prevalence estimates were consistently high in the\ncoastal regions and low in the highlands. Prevalence was lowest in 2014 and\npeaked in 2015, highlighting the importance of estimates between survey years.\nSeasonality was widely observed. Similar multi-metric approaches may be\napplicable across sub-Saharan Africa.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:06:41 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Arambepola", "Rohan", ""], ["Keddie", "Suzanne H.", ""], ["Collins", "Emma L.", ""], ["Twohig", "Katherine A.", ""], ["Amratia", "Punam", ""], ["Bertozzi-Villa", "Amelia", ""], ["Chestnutt", "Elisabeth G.", ""], ["Harris", "Joseph", ""], ["Millar", "Justin", ""], ["Rozier", "Jennifer", ""], ["Rumisha", "Susan F.", ""], ["Symons", "Tasmin L.", ""], ["Vargas-Ruiz", "Camilo", ""], ["Andriamananjara", "Mauricette", ""], ["Rabeherisoa", "Saraha", ""], ["Ratsimbasoa", "Ars\u00e8ne C.", ""], ["Howes", "Rosalind E.", ""], ["Weiss", "Daniel J.", ""], ["Gething", "Peter W.", ""], ["Cameron", "Ewan", ""]]}, {"id": "2008.08366", "submitter": "Nabaneet Das", "authors": "Nabaneet Das, Subir Kumar Bhandari", "title": "Observation on F.W.E.R. and F.D.R. for correlated normal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we have attempted to study the behaviour of the family wise\nerror rate (FWER) for Bonferroni's procedure and false discovery rate (FDR) of\nthe Benjamini-Hodgeberg procedure for simultaneous testing problem with\nequicorrelated normal observations. By simulation study, we have shown that\nF.W.E.R. is a concave function for small no. of hypotheses and asymptotically\nbecomes a convex function of the correlation. The plots of F.W.E.R. and F.D.R.\nconfirms that if non-negative correlation is present, then these procedures\ncontrol the type-I error rate at a much smaller rate than the desired level of\nsignificance. This confirms the conservative nature of these popular methods\nwhen correlation is present and provides a scope for improvement in power by\nappropriate adjustment for correlation.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 10:28:29 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Das", "Nabaneet", ""], ["Bhandari", "Subir Kumar", ""]]}, {"id": "2008.08536", "submitter": "Damjan Vukcevic", "authors": "Zhuoqun Huang, Ronald L. Rivest, Philip B. Stark, Vanessa Teague,\n  Damjan Vukcevic", "title": "A Unified Evaluation of Two-Candidate Ballot-Polling Election Auditing\n  Methods", "comments": "27 pages. This version includes substantially expanded appendices and\n  several corrections to the main text", "journal-ref": "Electronic Voting, E-Vote-ID 2020, Lecture Notes in Computer\n  Science 12455 (2020) 112-128", "doi": "10.1007/978-3-030-60347-2_8", "report-no": null, "categories": "stat.AP cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counting votes is complex and error-prone. Several statistical methods have\nbeen developed to assess election accuracy by manually inspecting randomly\nselected physical ballots. Two 'principled' methods are risk-limiting audits\n(RLAs) and Bayesian audits (BAs). RLAs use frequentist statistical inference\nwhile BAs are based on Bayesian inference. Until recently, the two have been\nthought of as fundamentally different.\n  We present results that unify and shed light upon 'ballot-polling' RLAs and\nBAs (which only require the ability to sample uniformly at random from all cast\nballot cards) for two-candidate plurality contests, which are building blocks\nfor auditing more complex social choice functions, including some preferential\nvoting systems. We highlight the connections between the methods and explore\ntheir performance.\n  First, building on a previous demonstration of the mathematical equivalence\nof classical and Bayesian approaches, we show that BAs, suitably calibrated,\nare risk-limiting. Second, we compare the efficiency of the methods across a\nwide range of contest sizes and margins, focusing on the distribution of sample\nsizes required to attain a given risk limit. Third, we outline several ways to\nimprove performance and show how the mathematical equivalence explains the\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 16:30:03 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 04:37:16 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Huang", "Zhuoqun", ""], ["Rivest", "Ronald L.", ""], ["Stark", "Philip B.", ""], ["Teague", "Vanessa", ""], ["Vukcevic", "Damjan", ""]]}, {"id": "2008.08741", "submitter": "Xiaoke Zhang", "authors": "Xiaoke Zhang, Wu Xue, and Qiyue Wang", "title": "Functional Data Analysis with Causation in Observational Studies:\n  Covariate Balancing Functional Propensity Score for Functional Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis, which handles data arising from curves, surfaces,\nvolumes, manifolds and beyond in a variety of scientific fields, is a rapidly\ndeveloping area in modern statistics and data science in the recent decades.\nThe effect of a functional variable on an outcome is an essential theme in\nfunctional data analysis, but a majority of related studies are restricted to\ncorrelational effects rather than causal effects. This paper makes the first\nattempt to study the causal effect of a functional variable as a treatment in\nobservational studies. Despite the lack of a probability density function for\nthe functional treatment, the propensity score is properly defined in terms of\na multivariate substitute. Two covariate balancing methods are proposed to\nestimate the propensity score, which minimize the correlation between the\ntreatment and covariates. The appealing performance of the proposed method in\nboth covariate balance and causal effect estimation is demonstrated by a\nsimulation study. The proposed method is applied to study the causal effect of\nbody shape on human visceral adipose tissue.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 02:44:45 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Zhang", "Xiaoke", ""], ["Xue", "Wu", ""], ["Wang", "Qiyue", ""]]}, {"id": "2008.08857", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Simple Analysis of Johnson-Lindenstrauss Transform under Neuroscience\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper re-analyzes a version of the celebrated Johnson-Lindenstrauss\nLemma, in which matrices are subjected to constraints that naturally emerge\nfrom neuroscience applications: a) sparsity and b) sign-consistency. This\nparticular variant was studied first by Allen-Zhu, Gelashvili, Micali, Shavit\nand more recently by Jagadeesan (RANDOM'19).\n  The contribution of this work is a novel proof, which in contrast to previous\nworks a) uses the modern probability toolkit, particularly basics of\nsub-gaussian and sub-gamma estimates b) is self-contained, with no dependencies\non subtle third-party results c) offers explicit constants.\n  At the heart of our proof is a novel variant of Hanson-Wright Lemma (on\nconcentration of quadratic forms). Of independent interest are also auxiliary\nfacts on sub-gaussian random variables.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 09:31:52 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2008.08993", "submitter": "Bidroha Basu", "authors": "Bidroha Basu, Enda Murphy, Anna Molter, Arunima Sarkar Basu, Srikanta\n  Sannigrahi, Miguel Belmonte and Francesco Pilla", "title": "Effect of COVID-19 on noise pollution change in Dublin, Ireland", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise pollution is considered to be the third most hazardous pollution after\nair and water pollution by the World Health Organization (WHO). Short as well\nas long-term exposure to noise pollution has several adverse effects on humans,\nranging from psychiatric disorders such as anxiety and depression,\nhypertension, hormonal dysfunction, and blood pressure rise leading to\ncardiovascular disease. One of the major sources of noise pollution is road\ntraffic. The WHO reports that around 40% of Europe's population are currently\nexposed to high noise levels. This study investigates noise pollution in\nDublin, Ireland before and after the lockdown imposed as a result of the\nCOVID-19 pandemic. The analysis was performed using 2020 hourly data from 12\nnoise monitoring stations. More than 80% of stations recorded high noise levels\nfor more that 60% of the time before the lockdown in Dublin. However, a\nsignificant reduction in average and minimum noise levels was observed at all\nstations during the lockdown period and this can be attributed to reductions in\nboth road and air traffic movements.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:29:16 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Basu", "Bidroha", ""], ["Murphy", "Enda", ""], ["Molter", "Anna", ""], ["Basu", "Arunima Sarkar", ""], ["Sannigrahi", "Srikanta", ""], ["Belmonte", "Miguel", ""], ["Pilla", "Francesco", ""]]}, {"id": "2008.09091", "submitter": "Ekaterina Poliakova", "authors": "Ekaterina Poliakova", "title": "Maximum likelihood estimation of parameters of spherical particle size\n  distributions from profile size measurements and its application for small\n  samples", "comments": "A shortened and partly rewritten version of this paper with more\n  focus on the novel approximation has been submitted to Image Analysis &\n  Stereology. The paper is not being considered for any other journal, neither\n  in that shortened, nor in this detailed, nor in any other version. Compared\n  to the previous version, there were corrected many misprints and several\n  sentences were clarified", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microscopy research often requires recovering particle-size distributions in\nthree dimensions from only a few (10 - 200) profile measurements in the\nsection. This problem is especially relevant for petrographic and mineralogical\nstudies, where parametric assumptions are reasonable and finding distribution\nparameters from the microscopic study of small sections is essential. This\npaper deals with the specific case where particles are approximately spherical\n(i.e. Wicksell's problem). The paper presents a novel approximation of the\nprobability density of spherical particle profile sizes. This approximation\nuses the actual non-smoothness of mineral particles rather than perfect\nspheres. The new approximation facilitates the numerically efficient use of the\nmaximum likelihood method, a generally powerful method that provides the\ndistribution parameter estimates of the minimal variance in most practical\ncases. The variance and bias of the estimates by the maximum likelihood method\nwere compared numerically for several typical particle-size distributions with\nthose by alternative parametric methods (method of moments and minimum distance\nestimation), and the maximum likelihood estimation was found to be preferable\nfor both small and large samples. The maximum likelihood method, along with the\nsuggested approximation, may also be used for selecting a model, for\nconstructing narrow confidence intervals for distribution parameters using all\nthe profiles without random sampling and for including the measurements of the\nprofiles intersected by section boundaries. The utility of the approach is\nillustrated using an example from glacier ice petrography.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 17:28:04 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 15:08:55 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 13:57:23 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Poliakova", "Ekaterina", ""]]}, {"id": "2008.09155", "submitter": "Karel Devriendt", "authors": "Karel Devriendt and Samuel Martin-Gutierrez and Renaud Lambiotte", "title": "Variance and covariance of distributions on graphs", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a theory to measure the variance and covariance of probability\ndistributions defined on the nodes of a graph, which takes into account the\ndistance between nodes. Our approach generalizes the usual (co)variance to the\nsetting of weighted graphs and retains many of its intuitive and desired\nproperties. Interestingly, we find that a number of famous concepts in graph\ntheory and network science can be reinterpreted in this setting as variances\nand covariances of particular distributions: we show this correspondence for\nKemeny's constant, the Kirchhoff index, network modularity and Markov\nstability. As a particular application, we define the maximum-variance problem\non graphs with respect to the effective resistance distance, and characterize\nthe solutions to this problem both numerically and theoretically. We show how\nthe maximum-variance distribution can be interpreted as a core-periphery\nmeasure, illustrated by the fact that these distributions are supported on the\nleaf nodes of tree graphs, low-degree nodes in a configuration-like graph and\nboundary nodes in random geometric graphs. Our theoretical results are\nsupported by a number of experiments on a network of mathematical concepts,\nwhere we use the variance and covariance as analytical tools to study the\n(co-)occurrence of concepts in scientific papers with respect to the (network)\nrelations between these concepts.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 18:48:10 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Devriendt", "Karel", ""], ["Martin-Gutierrez", "Samuel", ""], ["Lambiotte", "Renaud", ""]]}, {"id": "2008.09227", "submitter": "Tianyu Pan", "authors": "Tianyu Pan, Weining Shen, Guanyu Hu", "title": "Spatial homogeneity learning for spatially correlated functional data\n  with application to COVID-19 Growth rate curves", "comments": "33 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the spatial heterogeneity effect on regional COVID-19 pandemic\ntiming and severity by analyzing the COVID-19 growth rate curves in the United\nStates. We propose a geographically detailed functional data grouping method\nequipped with a functional conditional autoregressive (CAR) prior to fully\ncapture the spatial correlation in the pandemic curves. The spatial homogeneity\npattern can then be detected by a geographically weighted Chinese restaurant\nprocess prior which allows both locally spatially contiguous groups and\nglobally discontiguous groups. We design an efficient Markov chain Monte Carlo\n(MCMC) algorithm to simultaneously infer the posterior distributions of the\nnumber of groups and the grouping configuration of spatial functional data. The\nsuperior numerical performance of the proposed method over competing methods is\ndemonstrated using simulated studies and an application to COVID-19 state-level\nand county-level data study in the United States.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 23:24:45 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Pan", "Tianyu", ""], ["Shen", "Weining", ""], ["Hu", "Guanyu", ""]]}, {"id": "2008.09245", "submitter": "Tianwei Li", "authors": "Tianwei Li, Yitong Geng, Huai Jiang", "title": "Anomaly Detection on Seasonal Metrics via Robust Time Series\n  Decomposition", "comments": "6 pages, 3 figures, accepted by IJCAI20 AI4AN workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stability and persistence of web services are important to Internet\ncompanies to improve user experience and business performances. To keep eyes on\nnumerous metrics and report abnormal situations, time series anomaly detection\nmethods are developed and applied by various departments in companies and\ninstitutions. In this paper, we proposed a robust anomaly detection algorithm\n(MEDIFF) to monitor online business metrics in real time. Specifically, a\ndecomposition method using robust statistical metric--median--of the time\nseries was applied to decouple the trend and seasonal components. With the\neffects of daylight saving time (DST) shift and holidays, corresponding\ncomponents were decomposed from the time series. The residual after\ndecomposition was tested by a generalized statistics method to detect outliers\nin the time series. We compared the proposed MEDIFF algorithm with two open\nsource algorithms (SH-ESD and DONUT) by using our labeled internal business\nmetrics. The results demonstrated the effectiveness of the proposed MEDIFF\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:50:02 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Li", "Tianwei", ""], ["Geng", "Yitong", ""], ["Jiang", "Huai", ""]]}, {"id": "2008.09303", "submitter": "Natalia Rybnikova", "authors": "N. Rybnikova, B. A. Portnov, E. M. Mirkes, A. Zinovyev, A. Brook, A.\n  N. Gorban", "title": "Coloring Panchromatic Nighttime Satellite Images: Comparing the\n  Performance of Several Machine Learning Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial light-at-night (ALAN), emitted from the ground and visible from\nspace, marks human presence on Earth. Since the launch of the Suomi National\nPolar Partnership satellite with the Visible Infrared Imaging Radiometer Suite\nDay/Night Band (VIIRS/DNB) onboard, global nighttime images have significantly\nimproved; however, they remained panchromatic. Although multispectral images\nare also available, they are either commercial or free of charge, but sporadic.\nIn this paper, we use several machine learning techniques, such as linear,\nkernel, random forest regressions, and elastic map approach, to transform\npanchromatic VIIRS/DBN into Red Green Blue (RGB) images. To validate the\nproposed approach, we analyze RGB images for eight urban areas worldwide. We\nlink RGB values, obtained from ISS photographs, to panchromatic ALAN\nintensities, their pixel-wise differences, and several land-use type proxies.\nEach dataset is used for model training, while other datasets are used for the\nmodel validation. The analysis shows that model-estimated RGB images\ndemonstrate a high degree of correspondence with the original RGB images from\nthe ISS database. Yet, estimates, based on linear, kernel and random forest\nregressions, provide better correlations, contrast similarity and lower WMSEs\nlevels, while RGB images, generated using elastic map approach, provide higher\nconsistency of predictions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 04:51:42 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 14:16:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Rybnikova", "N.", ""], ["Portnov", "B. A.", ""], ["Mirkes", "E. M.", ""], ["Zinovyev", "A.", ""], ["Brook", "A.", ""], ["Gorban", "A. N.", ""]]}, {"id": "2008.09407", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz, Katarzyna Pawlukiewicz", "title": "Estimation of the number of irregular foreigners in Poland using\n  non-linear count regression models", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation requires access to unit-level data in order to\ncorrectly apply capture-recapture methods. Unfortunately, for reasons of\nconfidentiality access to such data may be limited. To overcome this issue we\napply and extend the hierarchical Poisson-Gamma model proposed by Zhang (2008),\nwhich initially was used to estimate the number of irregular foreigners in\nNorway.\n  The model is an alternative to the current capture-recapture approach as it\ndoes not require linking multiple sources and is solely based on aggregated\nadministrative data that include (1) the number of apprehended irregular\nforeigners, (2) the number of foreigners who faced criminal charges and (3) the\nnumber of foreigners registered in the central population register. The model\nexplicitly assumes a relationship between the unauthorized and registered\npopulation, which is motivated by the interconnection between these two groups.\nThis makes the estimation conditionally dependent on the size of regular\npopulation, provides interpretation with analogy to registered population and\nmakes the estimated parameter more stable over time.\n  In this paper, we modify the original idea to allow for covariates and\nflexible count distributions in order to estimate the number of irregular\nforeigners in Poland in 2019. We also propose a parametric bootstrap for\nestimating standard errors of estimates. Based on the extended model we\nconclude that in as of 31.03.2019 and 30.09.2019 around 15,000 and 20,000\nforeigners and were residing in Poland without valid permits. This means that\nthose apprehended by the Polish Border Guard account for around 15-20% of the\ntotal.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:26:49 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Pawlukiewicz", "Katarzyna", ""]]}, {"id": "2008.09416", "submitter": "Alexander Neergaard Olesen", "authors": "Alexander Neergaard Olesen, Poul Jennum, Emmanuel Mignot, Helge B D\n  Sorensen", "title": "Automatic sleep stage classification with deep residual networks in a\n  mixed-cohort setting", "comments": "Author's original version. This article has been accepted for\n  publication in SLEEP published by Oxford University Press", "journal-ref": null, "doi": "10.1093/sleep/zsaa161", "report-no": null, "categories": "cs.CV eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study Objectives: Sleep stage scoring is performed manually by sleep experts\nand is prone to subjective interpretation of scoring rules with low intra- and\ninterscorer reliability. Many automatic systems rely on few small-scale\ndatabases for developing models, and generalizability to new datasets is thus\nunknown. We investigated a novel deep neural network to assess the\ngeneralizability of several large-scale cohorts.\n  Methods: A deep neural network model was developed using 15684\npolysomnography studies from five different cohorts. We applied four different\nscenarios: 1) impact of varying time-scales in the model; 2) performance of a\nsingle cohort on other cohorts of smaller, greater or equal size relative to\nthe performance of other cohorts on a single cohort; 3) varying the fraction of\nmixed-cohort training data compared to using single-origin data; and 4)\ncomparing models trained on combinations of data from 2, 3, and 4 cohorts.\n  Results: Overall classification accuracy improved with increasing fractions\nof training data (0.25$\\%$: 0.782 $\\pm$ 0.097, 95$\\%$ CI [0.777-0.787];\n100$\\%$: 0.869 $\\pm$ 0.064, 95$\\%$ CI [0.864-0.872]), and with increasing\nnumber of data sources (2: 0.788 $\\pm$ 0.102, 95$\\%$ CI [0.787-0.790]; 3: 0.808\n$\\pm$ 0.092, 95$\\%$ CI [0.807-0.810]; 4: 0.821 $\\pm$ 0.085, 95$\\%$ CI\n[0.819-0.823]). Different cohorts show varying levels of generalization to\nother cohorts.\n  Conclusions: Automatic sleep stage scoring systems based on deep learning\nalgorithms should consider as much data as possible from as many sources\navailable to ensure proper generalization. Public datasets for benchmarking\nshould be made available for future research.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 10:48:35 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Olesen", "Alexander Neergaard", ""], ["Jennum", "Poul", ""], ["Mignot", "Emmanuel", ""], ["Sorensen", "Helge B D", ""]]}, {"id": "2008.09469", "submitter": "Qi Wang", "authors": "Qi Wang, Herke van Hoof", "title": "Doubly Stochastic Variational Inference for Neural Processes with\n  Hierarchical Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural processes (NPs) constitute a family of variational approximate models\nfor stochastic processes with promising properties in computational efficiency\nand uncertainty quantification. These processes use neural networks with latent\nvariable inputs to induce predictive distributions. However, the expressiveness\nof vanilla NPs is limited as they only use a global latent variable, while\ntarget specific local variation may be crucial sometimes. To address this\nchallenge, we investigate NPs systematically and present a new variant of NP\nmodel that we call Doubly Stochastic Variational Neural Process (DSVNP). This\nmodel combines the global latent variable and local latent variables for\nprediction. We evaluate this model in several experiments, and our results\ndemonstrate competitive prediction performance in multi-output regression and\nuncertainty estimation in classification.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:32:12 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 23:05:15 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Wang", "Qi", ""], ["van Hoof", "Herke", ""]]}, {"id": "2008.09693", "submitter": "Pierre Ablin", "authors": "Pierre Ablin, Jean-Fran\\c{c}ois Cardoso and Alexandre Gramfort", "title": "Spectral independent component analysis with noise modeling for M/EEG\n  source separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Independent Component Analysis (ICA) is a widespread tool for\nexploration and denoising of electroencephalography (EEG) or\nmagnetoencephalography (MEG) signals. In its most common formulation, ICA\nassumes that the signal matrix is a noiseless linear mixture of independent\nsources that are assumed non-Gaussian. A limitation is that it enforces to\nestimate as many sources as sensors or to rely on a detrimental PCA step.\n  Methods: We present the Spectral Matching ICA (SMICA) model. Signals are\nmodelled as a linear mixing of independent sources corrupted by additive noise,\nwhere sources and the noise are stationary Gaussian time series. Thanks to the\nGaussian assumption, the negative log-likelihood has a simple expression as a\nsum of divergences between the empirical spectral covariance matrices of the\nsignals and those predicted by the model. The model parameters can then be\nestimated by the expectation-maximization (EM) algorithm.\n  Results: Experiments on phantom MEG datasets show that SMICA can recover\ndipole locations more precisely than usual ICA algorithms or Maxwell filtering\nwhen the dipole amplitude is low. Experiments on EEG datasets show that SMICA\nidentifies a source subspace which contains sources that have less pairwise\nmutual information, and are better explained by the projection of a single\ndipole on the scalp.\n  Comparison with existing methods: Noiseless ICA models lead to degenerate\nlikelihood when there are fewer sources than sensors, while SMICA succeeds\nwithout resorting to prior dimension reduction.\n  Conclusions: SMICA is a promising alternative to other noiseless ICA models\nbased on non-Gaussian assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 22:06:37 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ablin", "Pierre", ""], ["Cardoso", "Jean-Fran\u00e7ois", ""], ["Gramfort", "Alexandre", ""]]}, {"id": "2008.09842", "submitter": "Florian Toqu\\'e", "authors": "Florian Toqu\\'e, Etienne C\\^ome, Martin Tr\\'epanier and Latifa\n  Oukhellou", "title": "Forecasting of the Montreal Subway Smart Card Entry Logs with Event Data", "comments": "18 pages, 13 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major goals of transport operators is to adapt the transport\nsupply scheduling to the passenger demand for existing transport networks\nduring each specific period. Another problem mentioned by operators is\naccurately estimating the demand for disposable ticket or pass to adapt ticket\navailability to passenger demand. In this context, we propose generic data\nshaping, allowing the use of well-known regression models (basic, statistical\nand machine learning models) for the long-term forecasting of passenger demand\nwith fine-grained temporal resolution. Specifically, this paper investigates\nthe forecasting until one year ahead of the number of passengers entering each\nstation of a transport network with a quarter-hour aggregation by taking\nplanned events into account (e.g., concerts, shows, and so forth). To compare\nthe models and the quality of the prediction, we use a real smart card and\nevent data set from the city of Montr\\'eal, Canada, that span a three-year\nperiod with two years for training and one year for testing.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 14:08:57 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Toqu\u00e9", "Florian", ""], ["C\u00f4me", "Etienne", ""], ["Tr\u00e9panier", "Martin", ""], ["Oukhellou", "Latifa", ""]]}, {"id": "2008.10034", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "A critical assessment of conformal prediction methods applied in binary\n  classification settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been an increase in the number of scientific papers\nthat suggest using conformal predictions in drug discovery. We consider that\nsome versions of conformal predictions applied in binary settings are embroiled\nin pitfalls, not obvious at first sight, and that it is important to inform the\nscientific community about them. In the paper we first introduce the general\ntheory of conformal predictions and follow with an explanation of the versions\ncurrently dominant in drug discovery research today. Finally, we provide cases\nfor their critical assessment in binary classification settings.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 12:49:55 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 17:43:39 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "2008.10064", "submitter": "Georg Heiler", "authors": "Georg Heiler, Tobias Reisch, Jan Hurt, Mohammad Forghani, Aida Omani,\n  Allan Hanbury, Farid Karimipour", "title": "Country-wide mobility changes observed using mobile phone data during\n  COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In March 2020, the Austrian government introduced a widespread lock-down in\nresponse to the COVID-19 pandemic. Based on subjective impressions and\nanecdotal evidence, Austrian public and private life came to a sudden halt.\nHere we assess the effect of the lock-down quantitatively for all regions in\nAustria and present an analysis of daily changes of human mobility throughout\nAustria using near-real-time anonymized mobile phone data. We describe an\nefficient data aggregation pipeline and analyze the mobility by quantifying\nmobile-phone traffic at specific point of interest (POI), analyzing individual\ntrajectories and investigating the cluster structure of the origin-destination\ngraph. We found a reduction of commuters at Viennese metro stations of over\n80\\% and the number of devices with a radius of gyration of less than 500 m\nalmost doubled. The results of studying crowd-movement behavior highlight\nconsiderable changes in the structure of mobility networks, revealed by a\nhigher modularity and an increase from 12 to 20 detected communities. We\ndemonstrate the relevance of mobility data for epidemiological studies by\nshowing a significant correlation of the outflow from the town of Ischgl (an\nearly COVID-19 hotspot) and the reported COVID-19 cases with an 8-day time lag.\nThis research indicates that mobile phone usage data permits the\nmoment-by-moment quantification of mobility behavior for a whole country. We\nemphasize the need to improve the availability of such data in anonymized form\nto empower rapid response to combat COVID-19 and future pandemics.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 16:00:57 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Heiler", "Georg", ""], ["Reisch", "Tobias", ""], ["Hurt", "Jan", ""], ["Forghani", "Mohammad", ""], ["Omani", "Aida", ""], ["Hanbury", "Allan", ""], ["Karimipour", "Farid", ""]]}, {"id": "2008.10104", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Yi-Hsuan Lee and Xiaoou Li", "title": "Item Quality Control in Educational Testing: Change Point Model,\n  Compound Risk, and Sequential Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In standardized educational testing, test items are reused in multiple test\nadministrations. To ensure the validity of test scores, psychometric properties\nof items should remain unchanged over time. In this paper, we consider the\nsequential monitoring of test items, in particular, the detection of abrupt\nchanges to their psychometric properties, where a change can be caused by, for\nexample, leakage of the item or change of corresponding curriculum. We propose\na statistical framework for the detection of abrupt changes in individual\nitems. This framework consists of (1) a multi-stream Bayesian change point\nmodel describing sequential changes in items, (2) a compound risk function\nquantifying the risk in sequential decisions, and (3) sequential decision rules\nthat control the compound risk. Throughout the sequential decision process, the\nproposed decision rule balances the trade-off between two sources of errors,\nthe false detection of pre-change items and the non-detection of post-change\nitems. An item-specific monitoring statistic is proposed based on an item\nresponse theory model that eliminates the confounding from the examinee\npopulation which changes over time. Sequential decision rules and their\ntheoretical properties are developed under two settings: the oracle setting\nwhere the Bayesian change point model is completely known and a more realistic\nsetting where some parameters of the model are unknown. Simulation studies are\nconducted under settings that mimic real operational tests.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 20:46:48 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Chen", "Yunxiao", ""], ["Lee", "Yi-Hsuan", ""], ["Li", "Xiaoou", ""]]}, {"id": "2008.10109", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Yan Shuo Tan, Briton Park, Mian Wei, Kevin Horgan, David\n  Madigan, Bin Yu", "title": "Stable discovery of interpretable subgroups via calibration in causal\n  studies", "comments": "Raaz Dwivedi and Yan Shuo Tan are joint first authors and contributed\n  equally to this work. 52 pages, 8 Figures, 9 Tables. To appear in\n  International Statistical Review, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on Yu and Kumbier's PCS framework and for randomized experiments, we\nintroduce a novel methodology for Stable Discovery of Interpretable Subgroups\nvia Calibration (StaDISC), with large heterogeneous treatment effects. StaDISC\nwas developed during our re-analysis of the 1999-2000 VIGOR study, an 8076\npatient randomized controlled trial (RCT), that compared the risk of adverse\nevents from a then newly approved drug, Rofecoxib (Vioxx), to that from an\nolder drug Naproxen. Vioxx was found to, on average and in comparison to\nNaproxen, reduce the risk of gastrointestinal (GI) events but increase the risk\nof thrombotic cardiovascular (CVT) events. Applying StaDISC, we fit 18 popular\nconditional average treatment effect (CATE) estimators for both outcomes and\nuse calibration to demonstrate their poor global performance. However, they are\nlocally well-calibrated and stable, enabling the identification of patient\ngroups with larger than (estimated) average treatment effects. In fact, StaDISC\ndiscovers three clinically interpretable subgroups each for the GI outcome\n(totaling 29.4% of the study size) and the CVT outcome (totaling 11.0%).\nComplementary analyses of the found subgroups using the 2001-2004 APPROVe\nstudy, a separate independently conducted RCT with 2587 patients, provides\nfurther supporting evidence for the promise of StaDISC.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 21:35:37 GMT"}, {"version": "v2", "created": "Tue, 29 Sep 2020 02:55:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Tan", "Yan Shuo", ""], ["Park", "Briton", ""], ["Wei", "Mian", ""], ["Horgan", "Kevin", ""], ["Madigan", "David", ""], ["Yu", "Bin", ""]]}, {"id": "2008.10300", "submitter": "Adriaan Hilbers", "authors": "Adriaan P Hilbers, David J Brayshaw, Axel Gandy", "title": "Importance subsampling for power system planning under multi-year demand\n  and weather uncertainty", "comments": "Runner-up for Roy Billinton Award for best student paper award at\n  16th International Conference on Probabilistic Methods Applied to Power\n  Systems (PMAPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a generalised version of importance subsampling for\ntime series reduction/aggregation in optimisation-based power system planning\nmodels. Recent studies indicate that reliably determining optimal electricity\n(investment) strategy under climate variability requires the consideration of\nmultiple years of demand and weather data. However, solving planning models\nover long simulation lengths is typically computationally unfeasible, and\nestablished time series reduction approaches induce significant errors. The\nimportance subsampling method reliably estimates long-term planning model\noutputs at greatly reduced computational cost, allowing the consideration of\nmulti-decadal samples. The key innovation is a systematic identification and\npreservation of relevant extreme events in modeling subsamples. Simulation\nstudies on generation and transmission expansion planning models illustrate the\nmethod's enhanced performance over established \"representative days\" clustering\napproaches. The models, data and sample code are made available as open-source\nsoftware.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 10:14:54 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 13:21:51 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Hilbers", "Adriaan P", ""], ["Brayshaw", "David J", ""], ["Gandy", "Axel", ""]]}, {"id": "2008.10344", "submitter": "Francisco Aparecido Rodrigues", "authors": "Pedro L. Ramos, Luciano da F. Costa, Francisco Louzada, Francisco A.\n  Rodrigues", "title": "Power laws in the Roman Empire: a survival analysis", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Roman Empire shaped Western civilization, and many Roman principles are\nembodied in modern institutions. Although its political institutions proved\nboth resilient and adaptable, allowing it to incorporate diverse populations,\nthe Empire suffered from many internal conflicts. Indeed, most emperors died\nviolently, from assassination, suicide, or in battle. These internal conflicts\nproduced patterns in the length of time that can be identified by statistical\nanalysis. In this paper, we study the underlying patterns associated with the\nreign of the Roman emperors by using statistical tools of survival data\nanalysis. We consider all the 175 Roman emperors and propose a new power-law\nmodel with change points to predict the time-to-violent-death of the Roman\nemperors. This model encompasses data in the presence of censoring and\nlong-term survivors, providing more accurate predictions than previous models.\nOur results show that power-law distributions can also occur in survival data,\nas verified in other data types from natural and artificial systems,\nreinforcing the ubiquity of power law distributions. The generality of our\napproach paves the way to further related investigations not only in other\nancient civilizations but also in applications in engineering and medicine.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:36:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Costa", "Luciano da F.", ""], ["Louzada", "Francisco", ""], ["Rodrigues", "Francisco A.", ""]]}, {"id": "2008.10431", "submitter": "David Orden", "authors": "David Orden, Encarnaci\\'on Fern\\'andez-Fern\\'andez, Marino\n  Tejedor-Romero, Alejandra Mart\\'inez-Moraian", "title": "Geometric and statistical techniques for projective mapping of chocolate\n  chip cookies with a large number of consumers", "comments": "21 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The so-called rapid sensory methods have proved to be useful for the sensory\nstudy of foods by different types of panels, from trained assessors to\nunexperienced consumers. Data from these methods have been traditionally\nanalyzed using statistical techniques, with some recent works proposing the use\nof geometric techniques and graph theory. The present work aims to deepen this\nline of research introducing a new method, mixing tools from statistics and\ngraph theory, for the analysis of data from Projective Mapping. In addition, a\nlarge number of n=349 unexperienced consumers is considered for the first time\nin Projective Mapping, evaluating nine commercial chocolate chips cookies which\ninclude a blind duplicate of a multinational best-selling brand and seven\nprivate labels. The data obtained are processed using the standard statistical\ntechnique Multiple Factor Analysis (MFA), the recently appeared geometric\nmethod SensoGraph using Gabriel clustering, and the novel variant introduced\nhere which is based on the pairwise distances between samples. All methods\nprovide the same groups of samples, with the blind duplicates appearing close\ntogether. Finally, the stability of the results is studied using bootstrapping\nand the RV and Mantel coefficients. The results suggest that, even for\nunexperienced consumers, highly stable results can be achieved for MFA and\nSensoGraph when considering a large enough number of assessors, around 200 for\nthe consensus map of MFA or the global similarity matrix of SensoGraph.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:32:23 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2020 07:34:35 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Orden", "David", ""], ["Fern\u00e1ndez-Fern\u00e1ndez", "Encarnaci\u00f3n", ""], ["Tejedor-Romero", "Marino", ""], ["Mart\u00ednez-Moraian", "Alejandra", ""]]}, {"id": "2008.10437", "submitter": "Jake Grainger", "authors": "Jake P. Grainger, Adam M. Sykulski, Philip Jonathan and Kevin Ewans", "title": "Estimating the parameters of ocean wave spectra", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.ao-ph physics.data-an stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wind-generated waves are often treated as stochastic processes. There is\nparticular interest in their spectral density functions, which are often\nexpressed in some parametric form. Such spectral density functions are used as\ninputs when modelling structural response or other engineering concerns.\nTherefore, accurate and precise recovery of the parameters of such a form, from\nobserved wave records, is important. Current techniques are known to struggle\nwith recovering certain parameters, especially the peak enhancement factor and\nspectral tail decay. We introduce an approach from the statistical literature,\nknown as the de-biased Whittle likelihood, and address some practical concerns\nregarding its implementation in the context of wind-generated waves. We\ndemonstrate, through numerical simulation, that the de-biased Whittle\nlikelihood outperforms current techniques, such as least squares fitting, both\nin terms of accuracy and precision of the recovered parameters. We also provide\na method for estimating the uncertainty of parameter estimates. We perform an\nexample analysis on a data-set recorded off the coast of New Zealand, to\nillustrate some of the extra practical concerns that arise when estimating the\nparameters of spectra from observed data.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 13:42:54 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 10:55:40 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Grainger", "Jake P.", ""], ["Sykulski", "Adam M.", ""], ["Jonathan", "Philip", ""], ["Ewans", "Kevin", ""]]}, {"id": "2008.10493", "submitter": "G\\'erald Gurtner", "authors": "G\\'erald Gurtner, Anne Graham, Andrew Cook, Samuel Crist\\'obal", "title": "The economic value of additional airport departure capacity", "comments": null, "journal-ref": "G. Gurnter, A. Graham, A. Cook, and S. Crist\\'obal (2018). \"The\n  economic value of additional airport departure capacity\", Journal of Air\n  Transport Management, 69, 1-14", "doi": "10.1016/j.jairtraman.2018.01.001", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article presents a model for the economic value of extra capacity at an\nairport. The model is based on a series of functional relationships linking the\nbenefits of extra capacity and the associated costs. It takes into account the\ncost of delay for airlines and its indirect consequences on the airport,\nthrough the loss or gain of aeronautical and non-aeronautical revenues. The\nmodel is highly data-driven and to this end a number of data sources have been\nused. In particular, special care has been used to take into account the full\ndistribution of delay at the airports rather than its average only. The results\nwith the simple version of the model show the existence of a unique maximum for\nthe operating profit of the airport in terms of capacity. The position of this\nmaximum is clearly dependent on the airport and also has an interesting\nbehaviour with the average number of passenger per aircraft at the airport and\nthe predictability of the flight departure times. In addition, we also show\nthat there exists an important trade-off between an increased predictability\nand the punctuality at the airport. Finally, it is shown that a more complex\nbehavioural model for passengers can introduce several local maxima in the\nairport profit and thus drive the airport towards suboptimal decisions.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 14:54:17 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Gurtner", "G\u00e9rald", ""], ["Graham", "Anne", ""], ["Cook", "Andrew", ""], ["Crist\u00f3bal", "Samuel", ""]]}, {"id": "2008.10679", "submitter": "Duncan Watson-Parris", "authors": "Duncan Watson-Parris", "title": "Machine learning for weather and climate are worlds apart", "comments": "Perspective submitted to \"Machine Learning for Weather and Climate\n  Modelling 2019\" Special Issue of Philosophical Transactions A", "journal-ref": null, "doi": "10.1098/rsta.2020.0098", "report-no": null, "categories": "physics.ao-ph stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern weather and climate models share a common heritage, and often even\ncomponents, however they are used in different ways to answer fundamentally\ndifferent questions. As such, attempts to emulate them using machine learning\nshould reflect this. While the use of machine learning to emulate weather\nforecast models is a relatively new endeavour there is a rich history of\nclimate model emulation. This is primarily because while weather modelling is\nan initial condition problem which intimately depends on the current state of\nthe atmosphere, climate modelling is predominantly a boundary condition\nproblem. In order to emulate the response of the climate to different drivers\ntherefore, representation of the full dynamical evolution of the atmosphere is\nneither necessary, or in many cases, desirable. Climate scientists are\ntypically interested in different questions also. Indeed emulating the\nsteady-state climate response has been possible for many years and provides\nsignificant speed increases that allow solving inverse problems for e.g.\nparameter estimation. Nevertheless, the large datasets, non-linear\nrelationships and limited training data make Climate a domain which is rich in\ninteresting machine learning challenges.\n  Here I seek to set out the current state of climate model emulation and\ndemonstrate how, despite some challenges, recent advances in machine learning\nprovide new opportunities for creating useful statistical models of the\nclimate.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 19:51:51 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 12:48:36 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Watson-Parris", "Duncan", ""]]}, {"id": "2008.10725", "submitter": "Anahita Nodehi", "authors": "Anahita Nodehi, Mousa Golalizadeh, Mehdi Maadooliat, Claudio\n  Agostinelli", "title": "Torus Probabilistic Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most common problems that any technique encounters is the high\ndimensionality of the input data. This yields several problems in the\nsubsequently statistical methods due to so called \"curse of dimensionality\".\nSeveral dimension reduction methods have been proposed in the literature, until\nnow, to accomplish the goal of having smaller dimension space. The most popular\namong them, is the so called Principal Component Analysis (PCA). One of the\nextensions of PCA is Probabilistic PCA (known as PPCA). In PPCA, a\nprobabilistic model is used to perform the dimension reduction. By convention,\nthere are cases in applied sciences, e.g. Bioinformatics, Biology and Geology\nthat the data at hand are in non Euclidean space. Elaborating further, an\nimportant situation is when each variable poses a direction so that a data\npoint is lying on a torus. According to its importance, the extension of the\nPCA to such data is being under attention during last decades. In this paper,\nwe introduce a Probabilistic PCA for data on torus (TPPCA), assuming a\nMultivariate Wrapped Normal distribution as the underline model. Furthermore,\nwe present certain appropriate algorithms to compute this dimension reduction\ntechnique and then, we compare our proposal with other dimension reduction\ntechniques using examples based on real datasets and a small Monte Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 22:01:19 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Nodehi", "Anahita", ""], ["Golalizadeh", "Mousa", ""], ["Maadooliat", "Mehdi", ""], ["Agostinelli", "Claudio", ""]]}, {"id": "2008.10789", "submitter": "A H M Jakaria", "authors": "A H M Jakaria, Md Mosharaf Hossain, Mohammad Ashiqur Rahman", "title": "Smart Weather Forecasting Using Machine Learning:A Case Study in\n  Tennessee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, weather predictions are performed with the help of large\ncomplex models of physics, which utilize different atmospheric conditions over\na long period of time. These conditions are often unstable because of\nperturbations of the weather system, causing the models to provide inaccurate\nforecasts. The models are generally run on hundreds of nodes in a large High\nPerformance Computing (HPC) environment which consumes a large amount of\nenergy. In this paper, we present a weather prediction technique that utilizes\nhistorical data from multiple weather stations to train simple machine learning\nmodels, which can provide usable forecasts about certain weather conditions for\nthe near future within a very short period of time. The models can be run on\nmuch less resource intensive environments. The evaluation results show that the\naccuracy of the models is good enough to be used alongside the current\nstate-of-the-art techniques. Furthermore, we show that it is beneficial to\nleverage the weather station data from multiple neighboring areas over the data\nof only the area for which weather forecasting is being performed.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 02:41:32 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Jakaria", "A H M", ""], ["Hossain", "Md Mosharaf", ""], ["Rahman", "Mohammad Ashiqur", ""]]}, {"id": "2008.10885", "submitter": "Asim Dey", "authors": "Asim Kumer Dey, Toufiqul Haq, Kumer Das, and Irina Panovska", "title": "Quantifying the impact of COVID-19 on the US stock market: An analysis\n  from multi-source information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel temporal complex network approach to quantify the US\ncounty level spread dynamics of COVID-19. The objective is to study the effects\nof the local spread dynamics, COVID-19 cases and death, and Google search\nactivities on the US stock market. We use both conventional econometric and\nMachine Learning (ML) models. The results suggest that COVID-19 cases and\ndeaths, its local spread, and Google searches have impacts on abnormal stock\nprices between January 2020 to May 2020. In addition, incorporating information\nabout local spread significantly improves the performance of forecasting models\nof the abnormal stock prices at longer forecasting horizons. On the other hand,\nalthough a few COVID-19 related variables, e.g., US total deaths and US new\ncases exhibit causal relationships on price volatility, COVID-19 cases and\ndeaths, local spread of COVID-19, and Google search activities do not have\nimpacts on price volatility.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 08:46:37 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2020 01:50:19 GMT"}, {"version": "v3", "created": "Sat, 3 Oct 2020 01:33:22 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Dey", "Asim Kumer", ""], ["Haq", "Toufiqul", ""], ["Das", "Kumer", ""], ["Panovska", "Irina", ""]]}, {"id": "2008.10903", "submitter": "Akisato Suzuki Dr", "authors": "Akisato Suzuki", "title": "Policy Implications of Statistical Estimates: A General Bayesian\n  Decision-Theoretic Model for Binary Outcomes", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should scholars evaluate the statistically estimated causal effect of a\npolicy intervention? I point out three limitations in the conventional\npractice. First, relying on statistical significance misses the fact that\nuncertainty is a continuous scale. Second, focusing on a standard point\nestimate overlooks variation in plausible effect sizes. Third, the criterion of\nsubstantive significance is rarely explained or justified. To address these\nissues, I propose an original Bayesian decision-theoretic model for binary\noutcomes. I incorporate the posterior distribution of a causal effect reducing\nthe likelihood of an undesirable event, into a loss function over the cost of a\npolicy to realize the effect and the cost of the event. The model can use an\neffect size of interest other than the standard point estimate, and the\nprobability of this effect as a continuous measure of uncertainty. It then\npresents approximately up to what ratio between the two costs an expected loss\nremains smaller if the policy is implemented than if not. I exemplify my model\nthrough three applications and provide an R package for easy implementation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 09:21:58 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 18:32:01 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Suzuki", "Akisato", ""]]}, {"id": "2008.11169", "submitter": "Qi Wang", "authors": "Hengfang Deng, Daniel P. Aldrich, Michael M. Danziger, Jianxi Gao,\n  Nolan E. Phillips, Sean P. Cornelius, and Qi Ryan Wang", "title": "High-resolution human mobility data reveal race and wealth disparities\n  in disaster evacuation patterns", "comments": "17 pages, 4 figures. The manuscript has been submitted to Scientific\n  Reports", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Major disasters such as extreme weather events can magnify and exacerbate\npre-existing social disparities, with disadvantaged populations bearing\ndisproportionate costs. Despite the implications for equity and emergency\nplanning, we lack a quantitative understanding of how these social fault lines\ntranslate to different behaviors in large-scale emergency contexts. Here we\ninvestigate this problem in the context of Hurricane Harvey, using over 30\nmillion anonymized GPS records from over 150,000 opted-in users in the Greater\nHouston Area to quantify patterns of disaster-inflicted relocation activities\nbefore, during, and after the shock. We show that evacuation distance is highly\nhomogenous across individuals from different types of neighborhoods classified\nby race and wealth, obeying a truncated power-law distribution. Yet here the\nsimilarities end: we find that both race and wealth strongly impact evacuation\npatterns, with disadvantaged minority populations less likely to evacuate than\nwealthier white residents. Finally, there are considerable discrepancies in\nterms of departure and return times by race and wealth, with strong social\ncohesion among evacuees from advantaged neighborhoods in their destination\nchoices. These empirical findings bring new insights into mobility and\nevacuations, providing policy recommendations for residents, decision makers,\nand disaster managers alike.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:02:19 GMT"}, {"version": "v2", "created": "Sun, 30 Aug 2020 00:23:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Deng", "Hengfang", ""], ["Aldrich", "Daniel P.", ""], ["Danziger", "Michael M.", ""], ["Gao", "Jianxi", ""], ["Phillips", "Nolan E.", ""], ["Cornelius", "Sean P.", ""], ["Wang", "Qi Ryan", ""]]}, {"id": "2008.11175", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "How Ominous is the Future Global Warming Premonition?", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global warming, the phenomenon of increasing global average temperature in\nthe recent decades, is receiving wide attention due to its very significant\nadverse effects on climate. Whether global warming will continue even in the\nfuture, is a question that is most important to investigate. In this regard,\nthe so-called general circulation models (GCMs) have attempted to project the\nfuture climate, and nearly all of them exhibit alarming rates of global\ntemperature rise in the future.\n  Although global warming in the current time frame is undeniable, it is\nimportant to assess the validity of the future predictions of the GCMs. In this\narticle, we attempt such a study using our recently-developed Bayesian multiple\ntesting paradigm for model selection in inverse regression problems. The model\nwe assume for the global temperature time series is based on Gaussian process\nemulation of the black box scenario, realistically treating the dynamic\nevolution of the time series as unknown.\n  We apply our ideas to datasets available from the Intergovernmental Panel on\nClimate Change (IPCC) website. The best GCM models selected by our method under\ndifferent assumptions on future climate change scenarios do not convincingly\nsupport the present global warming pattern when only the future predictions are\nconsidered known. Using our Gaussian process idea, we also forecast the future\ntemperature time series given the current one. Interestingly, our results do\nnot support drastic future global warming predicted by almost all the GCM\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:12:38 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2008.11251", "submitter": "Sangwon Hyun", "authors": "Sangwon Hyun, Mattias Rolf Cape, Francois Ribalet, Jacob Bien", "title": "Modeling Cell Populations Measured By Flow Cytometry With Covariates\n  Using Sparse Mixture of Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ocean is filled with microscopic microalgae called phytoplankton, which\ntogether are responsible for as much photosynthesis as all plants on land\ncombined. Our ability to predict their response to the warming ocean relies on\nunderstanding how the dynamics of phytoplankton populations is influenced by\nchanges in environmental conditions. One powerful technique to study the\ndynamics of phytoplankton is flow cytometry, which measures the optical\nproperties of thousands of individual cells per second. Today, oceanographers\nare able to collect flow cytometry data in real-time onboard a moving ship,\nproviding them with fine-scale resolution of the distribution of phytoplankton\nacross thousands of kilometers. One of the current challenges is to understand\nhow these small and large scale variations relate to environmental conditions,\nsuch as nutrient availability, temperature, light and ocean currents. In this\npaper, we propose a novel sparse mixture of multivariate regressions model to\nestimate the time-varying phytoplankton subpopulations while simultaneously\nidentifying the specific environmental covariates that are predictive of the\nobserved changes to these subpopulations. We demonstrate the usefulness and\ninterpretability of the approach using both synthetic data and real\nobservations collected on an oceanographic cruise conducted in the north-east\nPacific in the spring of 2017.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 20:03:24 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Hyun", "Sangwon", ""], ["Cape", "Mattias Rolf", ""], ["Ribalet", "Francois", ""], ["Bien", "Jacob", ""]]}, {"id": "2008.11301", "submitter": "Ashton Wiens", "authors": "Ashton Wiens and Henry B. Lovejoy and Zachary Mullen and Eric Vance", "title": "Estimating conditional probabilities of historical migrations in the\n  transatlantic slave trade using kriging and Markov decision process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intra-African conflicts during the collapse of the kingdom of Oyo from 1817\nto 1836 resulted in the enslavement of an estimated 121,000 people who were\nthen transported to coastal ports via complex trade networks and loaded onto\nslave ships destined for the Americas. Historians have a good record of where\nthese people went across the Atlantic, but little is known about where\nindividuals were from or enslaved \\textit{within} Africa. In this work, we\ndevelop a novel two-step statistical approach to describe the enslavement of\npeople given documented violent conflict, the transport of enslaved peoples\nfrom their location of capture to their port of departure, and---given an\nenslaved individual's location of departure---that person's probability of\norigin. We combine spatial prediction of conflict density via Kriging with a\nMarkov decision process characterising intra-African transportation. The\nresults of this model can be visualised using an interactive web application,\nplotting estimated conditional probabilities of historical migrations during\nthe African diaspora. These results help trace the uncertain origins of people\nenslaved in this region of Africa during this time period: using the two-step\nstatistical methodology developed here provides a probabilistic answer to this\nquestion.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 22:54:28 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Wiens", "Ashton", ""], ["Lovejoy", "Henry B.", ""], ["Mullen", "Zachary", ""], ["Vance", "Eric", ""]]}, {"id": "2008.11428", "submitter": "Tobin South", "authors": "Tobin South, Matthew Roughan and Lewis Mitchell", "title": "Popularity and Centrality in Spotify Networks: Critical transitions in\n  eigenvector centrality", "comments": "Submitted to EPJ Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern age of digital music access has increased the availability of data\nabout music consumption and creation, facilitating the large-scale analysis of\nthe complex networks that connect music together. Data about user streaming\nbehaviour, and the musical collaboration networks are particularly important\nwith new data-driven recommendation systems. Without thorough analysis, such\ncollaboration graphs can lead to false or misleading conclusions. Here we\npresent a new collaboration network of artists from the online music streaming\nservice Spotify, and demonstrate a critical change in the eigenvector\ncentrality of artists, as low popularity artists are removed. The critical\nchange in centrality, from classical artists to rap artists, demonstrates\ndeeper structural properties of the network. A Social Group Centrality model is\npresented to simulate this critical transition behaviour, and switching between\ndominant eigenvectors is observed. This model presents a novel investigation of\nthe effect of popularity bias on how centrality and importance are measured,\nand provides a new tool for examining such flaws in networks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 07:50:16 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["South", "Tobin", ""], ["Roughan", "Matthew", ""], ["Mitchell", "Lewis", ""]]}, {"id": "2008.11435", "submitter": "Philipp Arras", "authors": "Philipp Arras, Hertzog L. Bester, Richard A. Perley, Reimar Leike,\n  Oleg Smirnov, R\\\"udiger Westermann, Torsten A. En{\\ss}lin", "title": "Comparison of classical and Bayesian imaging in radio interferometry", "comments": "23 pages, 16 figures, 4 tables, data published at\n  https://doi.org/10.5281/zenodo.4267057", "journal-ref": "A&A 646, A84 (2021)", "doi": "10.1051/0004-6361/202039258", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CLEAN, the commonly employed imaging algorithm in radio interferometry,\nsuffers from a number of shortcomings: in its basic version it does not have\nthe concept of diffuse flux, and the common practice of convolving the CLEAN\ncomponents with the CLEAN beam erases the potential for super-resolution; it\ndoes not output uncertainty information; it produces images with unphysical\nnegative flux regions; and its results are highly dependent on the so-called\nweighting scheme as well as on any human choice of CLEAN masks to guiding the\nimaging. Here, we present the Bayesian imaging algorithm resolve which solves\nthe above problems and naturally leads to super-resolution. We take a VLA\nobservation of Cygnus~A at four different frequencies and image it with\nsingle-scale CLEAN, multi-scale CLEAN and resolve. Alongside the sky brightness\ndistribution resolve estimates a baseline-dependent correction function for the\nnoise budget, the Bayesian equivalent of weighting schemes. We report noise\ncorrection factors between 0.4 and 429. The enhancements achieved by resolve\ncome at the cost of higher computational effort.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 08:16:26 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 13:21:20 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2020 15:41:00 GMT"}, {"version": "v4", "created": "Mon, 25 Jan 2021 17:05:30 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Arras", "Philipp", ""], ["Bester", "Hertzog L.", ""], ["Perley", "Richard A.", ""], ["Leike", "Reimar", ""], ["Smirnov", "Oleg", ""], ["Westermann", "R\u00fcdiger", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "2008.11539", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran, Patr\\'icia Szokol and Marianna Szab\\'o", "title": "Truncated generalized extreme value distribution based EMOS model for\n  calibration of wind speed ensemble forecasts", "comments": "30 pages, 12 figures, 8 tables", "journal-ref": "Environmetrics (2021)", "doi": "10.1002/env.2678", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, ensemble weather forecasting have become a routine at all\nmajor weather prediction centres. These forecasts are obtained from multiple\nruns of numerical weather prediction models with different initial conditions\nor model parametrizations. However, ensemble forecasts can often be\nunderdispersive and also biased, so some kind of post-processing is needed to\naccount for these deficiencies. One of the most popular state of the art\nstatistical post-processing techniques is the ensemble model output statistics\n(EMOS), which provides a full predictive distribution of the studied weather\nquantity. We propose a novel EMOS model for calibrating wind speed ensemble\nforecasts, where the predictive distribution is a generalized extreme value\n(GEV) distribution left truncated at zero (TGEV). The truncation corrects the\ndisadvantage of the GEV distribution based EMOS models of occasionally\npredicting negative wind speed values, without affecting its favorable\nproperties. The new model is tested on four data sets of wind speed ensemble\nforecasts provided by three different ensemble prediction systems, covering\nvarious geographical domains and time periods. The forecast skill of the TGEV\nEMOS model is compared with the predictive performance of the truncated normal,\nlog-normal and GEV methods and the raw and climatological forecasts as well.\nThe results verify the advantageous properties of the novel TGEV EMOS approach.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 13:01:27 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 07:28:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Szokol", "Patr\u00edcia", ""], ["Szab\u00f3", "Marianna", ""]]}, {"id": "2008.11660", "submitter": "Bhagya Wickramasinghe", "authors": "Bhagya N. Wickramasinghe, Dhirendra Singh and Lin Padgham", "title": "Building a large synthetic population from Australian census data", "comments": "15 pages, 1 figure with 3 sub-figures. International Workshop on\n  Agent-Based Modelling of Urban Systems (ABMUS, 2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present work on creating a synthetic population from census data for\nAustralia, applied to the greater Melbourne region. We use a sample-free\napproach to population synthesis that does not rely on a disaggregate sample\nfrom the original population. The inputs for our algorithm are joint marginal\ndistributions from census of desired person-level and household-level\nattributes, and outputs are a set of comma-separated-value (.csv) files\ncontaining the full synthetic population of unique individuals in households;\nwith age, gender, relationship status, household type, and size, matched to\ncensus data. Our algorithm is efficient in that it can create the synthetic\npopulation for Melbourne comprising 4.5 million persons in 1.8 million\nhouseholds within three minutes on a modern computer. Code for the algorithm is\nhosted on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 05:38:15 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Wickramasinghe", "Bhagya N.", ""], ["Singh", "Dhirendra", ""], ["Padgham", "Lin", ""]]}, {"id": "2008.11664", "submitter": "Richard Berk", "authors": "Richard A. Berk and Arun Kumar Kuchibhotla", "title": "Improving Fairness in Criminal Justice Algorithmic Risk Assessments\n  Using Conformal Prediction Sets", "comments": "We found an interpretive error in the method. We are trying now to\n  develop a better approach", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk assessment algorithms have been correctly criticized for potential\nunfairness, and there is an active cottage industry trying to make repairs. In\nthis paper, we adopt a framework from conformal prediction sets to remove\nunfairness from risk algorithms themselves and the covariates used for\nforecasting. From a sample of 300,000 offenders at their arraignments, we\nconstruct a confusion table and its derived measures of fairness that are\neffectively free any meaningful differences between Black and White offenders.\nWe also produce fair forecasts for individual offenders coupled with valid\nprobability guarantees that the forecasted outcome is the true outcome. We see\nour work as a demonstration of concept for application in a wide variety of\ncriminal justice decisions. The procedures provided can be routinely\nimplemented in jurisdictions with the usual criminal justice datasets used by\nadministrators. The requisite procedures can be found in the scripting software\nR. However, whether stakeholders will accept our approach as a means to achieve\nrisk assessment fairness is unknown. There also are legal issues that would\nneed to be resolved although we offer a Pareto improvement.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 16:47:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 17:23:37 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 17:35:18 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Berk", "Richard A.", ""], ["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2008.11756", "submitter": "Daniel Eck", "authors": "Jilei Lin and Daniel J. Eck", "title": "Minimizing post-shock forecasting error through aggregation of outside\n  information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a forecasting methodology for providing credible forecasts for\ntime series that have recently undergone a shock. We achieve this by borrowing\nknowledge from other time series that have undergone similar shocks for which\npost-shock outcomes are observed. Three shock effect estimators are motivated\nwith the aim of minimizing average forecast risk. We propose risk-reduction\npropositions that provide conditions that establish when our methodology works.\nBootstrap and leave-one-out cross validation procedures are provided to\nprospectively assess the performance of our methodology. Several simulated data\nexamples, and a real data example of forecasting Conoco Phillips stock price\nare provided for verification and illustration.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:33:35 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Lin", "Jilei", ""], ["Eck", "Daniel J.", ""]]}, {"id": "2008.11797", "submitter": "Xizhen Cai", "authors": "Xizhen Cai, Donna L. Coffman, Megan E. Piper and Runze Li", "title": "Estimation and Inference for the Mediation Effect in a Time-varying\n  Mediation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional mediation analysis typically examines the relations among an\nintervention, a time-invariant mediator, and a time-invariant outcome variable.\nAlthough there may be a direct effect of the intervention on the outcome, there\nis a need to understand the process by which the intervention affects the\noutcome (i.e. the indirect effect through the mediator). This indirect effect\nis frequently assumed to be time-invariant. With improvements in data\ncollection technology, it is possible to obtain repeated assessments over time\nresulting in intensive longitudinal data. This calls for an extension of\ntraditional mediation analysis to incorporate time-varying variables as well as\ntime-varying effects. In this paper, we focus on estimation and inference for\nthe time-varying mediation model, which allows mediation effects to vary as a\nfunction of time. We propose a two-step approach to estimate the time-varying\nmediation effect. Moreover, we use a simulation based approach to derive the\ncorresponding point-wise confidence band for the time-varying mediation effect.\nSimulation studies show that the proposed procedures perform well when\ncomparing the confidence band and the true underlying model. We further apply\nthe proposed model and the statistical inference procedure to real-world data\ncollected from a smoking cessation study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:30:35 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Cai", "Xizhen", ""], ["Coffman", "Donna L.", ""], ["Piper", "Megan E.", ""], ["Li", "Runze", ""]]}, {"id": "2008.11805", "submitter": "Alexandre Lima Ph.D.", "authors": "Alexandre Barbosa de Lima", "title": "An exploratory time series analysis of total deaths per month in Brazil\n  since 2015", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the historical series of the total number of\ndeaths per month in Brazil since 2015 using time series analysis techniques, in\norder to assess whether the COVID-19 pandemic caused any change in the series'\ngenerating mechanism. The results obtained so far indicate that there was no\nstatistical significant impact.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:44:08 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["de Lima", "Alexandre Barbosa", ""]]}, {"id": "2008.11866", "submitter": "Maude Wagner", "authors": "Maude Wagner, Francine Grodstein, Karen Leffondre, C\\'ecilia Samieri,\n  and C\\'ecile Proust-Lima", "title": "Time-varying exposure history and subsequent health outcomes: a\n  two-stage approach to identify critical windows", "comments": "Pages 35, Main Figures 5, Web Figures 13, Work presented at the\n  Alzheimers Association International eConference (2020), eMELODEM conference,\n  MEthods for LOngitudinal studies in DEMentia (2020), and ISCB, International\n  Society for Clinical Biostatistics (2019)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long-term behavioral and health risk factors constitute a primary focus of\nresearch on the etiology of chronic diseases. Yet, identifying critical\ntime-windows during which risk factors have the strongest impact on disease\nrisk is challenging. To assess the trajectory of association of an exposure\nhistory with an outcome, the weighted cumulative exposure index (WCIE) has been\nproposed, with weights reflecting the relative importance of exposures at\ndifferent times. However, WCIE is restricted to a complete observed error-free\nexposure whereas exposures are often measured with intermittent missingness and\nerror. Moreover, it rarely explores exposure history that is very distant from\nthe outcome as usually sought in life-course epidemiology. We extend the WCIE\nmethodology to (i) exposures that are intermittently measured with error, and\n(ii) contexts where the exposure time-window precedes the outcome time-window\nusing a landmark approach. First, the individual exposure history up to the\nlandmark time is estimated using a mixed model that handles missing data and\nerror in exposure measurement, and the predicted complete error-free exposure\nhistory is derived. Then the WCIE methodology is applied to assess the\ntrajectory of association between the predicted exposure history and the health\noutcome collected after the landmark time. In our context, the health outcome\nis a longitudinal marker analyzed using a mixed model. A simulation study first\ndemonstrates the correct inference obtained with this approach. Then, applied\nto the Nurses' Health Study (19,415 women) to investigate the association\nbetween BMI history (collected from midlife) and subsequent cognitive decline\nafter age 70. In conclusion, this approach, easy to implement, provides a\nflexible tool for studying complex dynamic relationships and identifying\ncritical time windows while accounting for exposure measurement errors.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:16:09 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 08:18:41 GMT"}, {"version": "v3", "created": "Thu, 25 Feb 2021 19:17:37 GMT"}], "update_date": "2021-03-01", "authors_parsed": [["Wagner", "Maude", ""], ["Grodstein", "Francine", ""], ["Leffondre", "Karen", ""], ["Samieri", "C\u00e9cilia", ""], ["Proust-Lima", "C\u00e9cile", ""]]}, {"id": "2008.11874", "submitter": "Le Bao", "authors": "Le Bao, Ying Zhang, Xiaoyue Niu", "title": "What Can We Learn from the Travelers Data in Detecting Disease Outbreaks\n  -- A Case Study of the COVID-19 Epidemic", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Travel is a potent force in the emergence of disease. We\ndiscussed how the traveler case reports could aid in a timely detection of a\ndisease outbreak. Methods: Using the traveler data, we estimated a few\nindicators of the epidemic that affected decision making and policy, including\nthe exponential growth rate, the doubling time, and the probability of severe\ncases exceeding the hospital capacity, in the initial phase of the COVID-19\nepidemic in multiple countries. We imputed the arrival dates when they were\nmissing. We compared the estimates from the traveler data to the ones from\ndomestic data. We quantitatively evaluated the influence of each case report\nand knowing the arrival date on the estimation. Findings: We estimated the\ntravel origin's daily exponential growth rate and examined the date from which\nthe growth rate was consistently above 0.1 (equivalent to doubling time < 7\ndays). We found those dates were very close to the dates that critical\ndecisions were made such as city lock-downs and national emergency\nannouncement. Using only the traveler data, if the assumed epidemic start date\nwas relatively accurate and the traveler sample was representative of the\ngeneral population, the growth rate estimated from the traveler data was\nconsistent with the domestic data. We also discussed situations that the\ntraveler data could lead to biased estimates. From the data influence study, we\nfound more recent travel cases had a larger influence on each day's estimate,\nand the influence of each case report got smaller as more cases became\navailable. We provided the minimum number of exported cases needed to determine\nwhether the local epidemic growth rate was above a certain level, and developed\na user-friendly Shiny App to accommodate various scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 00:54:22 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Bao", "Le", ""], ["Zhang", "Ying", ""], ["Niu", "Xiaoyue", ""]]}, {"id": "2008.12142", "submitter": "Geoff Boeing", "authors": "Geoff Boeing", "title": "Exploring Urban Form Through Openstreetmap Data: A Visual Introduction", "comments": "arXiv admin note: substantial text overlap with arXiv:1910.00118", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter introduces OpenStreetMap - a crowd-sourced, worldwide mapping\nproject and geospatial data repository - to illustrate its usefulness in\nquickly and easily analyzing and visualizing planning and design outcomes in\nthe built environment. It demonstrates the OSMnx toolkit for automatically\ndownloading, modeling, analyzing, and visualizing spatial big data from\nOpenStreetMap. We explore patterns and configurations in street networks and\nbuildings around the world computationally through visualization methods -\nincluding figure-ground diagrams and polar histograms - that help compress\nurban complexity into comprehensible artifacts that reflect the human\nexperience of the built environment. Ubiquitous urban data and computation can\nopen up new urban form analyses from both quantitative and qualitative\nperspectives.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 18:48:37 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Boeing", "Geoff", ""]]}, {"id": "2008.12152", "submitter": "Rodrigo Rivera-Castro", "authors": "Aiusha Sangadiev, Rodrigo Rivera-Castro, Kirill Stepanov, Andrey\n  Poddubny, Kirill Bubenchikov, Nikita Bekezin, Polina Pilyugina and Evgeny\n  Burnaev", "title": "DeepFolio: Convolutional Neural Networks for Portfolios with Limit Order\n  Book Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.TR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes DeepFolio, a new model for deep portfolio management based\non data from limit order books (LOB). DeepFolio solves problems found in the\nstate-of-the-art for LOB data to predict price movements. Our evaluation\nconsists of two scenarios using a large dataset of millions of time series. The\nimprovements deliver superior results both in cases of abundant as well as\nscarce data. The experiments show that DeepFolio outperforms the\nstate-of-the-art on the benchmark FI-2010 LOB. Further, we use DeepFolio for\noptimal portfolio allocation of crypto-assets with rebalancing. For this\npurpose, we use two loss-functions - Sharpe ratio loss and minimum volatility\nrisk. We show that DeepFolio outperforms widely used portfolio allocation\ntechniques in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 14:28:18 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Sangadiev", "Aiusha", ""], ["Rivera-Castro", "Rodrigo", ""], ["Stepanov", "Kirill", ""], ["Poddubny", "Andrey", ""], ["Bubenchikov", "Kirill", ""], ["Bekezin", "Nikita", ""], ["Pilyugina", "Polina", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "2008.12233", "submitter": "Aranya Koshy", "authors": "Aranya Koshy, Shahin Tavakoli", "title": "Exploring British Accents: Modeling the Trap-Bath Split with Functional\n  Data Analysis", "comments": "36 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD eess.AS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sound of our speech is influenced by the places we come from. Great\nBritain contains a wide variety of distinctive accents which are of interest to\nlinguistics. In particular, the \"a\" vowel in words like \"class\" is pronounced\ndifferently in the North and the South. Speech recordings of this vowel can be\nrepresented as formant curves or as Mel-frequency cepstral coefficient curves.\nFunctional data analysis and generalized additive models offer techniques to\nmodel the variation in these curves. Our first aim is to model the difference\nbetween typical Northern and Southern vowels, by training two classifiers on\nthe North-South Class Vowels dataset collected for this paper (Koshy 2020). Our\nsecond aim is to visualize geographical variation of accents in Great Britain.\nFor this we use speech recordings from a second dataset, the British National\nCorpus (BNC) audio edition (Coleman et al. 2012). The trained models are used\nto predict the accent of speakers in the BNC, and then we model the\ngeographical patterns in these predictions using a soap film smoother. This\nwork demonstrates a flexible and interpretable approach to modeling phonetic\naccent variation in speech recordings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:29:50 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Koshy", "Aranya", ""], ["Tavakoli", "Shahin", ""]]}, {"id": "2008.12257", "submitter": "Kevin H\\\"ohlein", "authors": "Kevin H\\\"ohlein, Michael Kern, Timothy Hewson and R\\\"udiger Westermann", "title": "A comparative study of convolutional neural network models for wind\n  field downscaling", "comments": null, "journal-ref": "Meteorol Appl. 2020; 27:e1961", "doi": "10.1002/met.1961", "report-no": null, "categories": "physics.ao-ph physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyze the applicability of convolutional neural network (CNN)\narchitectures for downscaling of short-range forecasts of near-surface winds on\nextended spatial domains. Short-range wind field forecasts (at the 100 m level)\nfrom ECMWF ERA5 reanalysis initial conditions at 31 km horizontal resolution\nare downscaled to mimic HRES (deterministic) short-range forecasts at 9 km\nresolution. We evaluate the downscaling quality of four exemplary model\narchitectures and compare these against a multi-linear regression model. We\nconduct a qualitative and quantitative comparison of model predictions and\nexamine whether the predictive skill of CNNs can be enhanced by incorporating\nadditional atmospheric variables, such as geopotential height and forecast\nsurface roughness, or static high-resolution fields, like land-sea mask and\ntopography. We further propose DeepRU, a novel U-Net-based CNN architecture,\nwhich is able to infer situation-dependent wind structures that cannot be\nreconstructed by other models. Inferring a target 9 km resolution wind field\nfrom the low-resolution input fields over the Alpine area takes less than 10\nmilliseconds on our GPU target architecture, which compares favorably to an\noverhead in simulation time of minutes or hours between low- and\nhigh-resolution forecast simulations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:29:13 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 12:28:29 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["H\u00f6hlein", "Kevin", ""], ["Kern", "Michael", ""], ["Hewson", "Timothy", ""], ["Westermann", "R\u00fcdiger", ""]]}, {"id": "2008.12411", "submitter": "Jae Youn Ahn", "authors": "Jae Youn Ahn and Sebastian Fuchs and Rosy Oh", "title": "A copula transformation in multivariate mixed discrete-continuous models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copulas allow a flexible and simultaneous modeling of complicated dependence\nstructures together with various marginal distributions. Especially if the\ndensity function can be represented as the product of the marginal density\nfunctions and the copula density function, this leads to both an intuitive\ninterpretation of the conditional distribution and convenient estimation\nprocedures. However, this is no longer the case for copula models with mixed\ndiscrete and continuous marginal distributions, because the corresponding\ndensity function cannot be decomposed so nicely. In this paper, we introduce a\ncopula transformation method that allows to represent the density function of a\ndistribution with mixed discrete and continuous marginals as the product of the\nmarginal probability mass/density functions and the copula density function.\nWith the proposed method, conditional distributions can be described\nanalytically and the computational complexity in the estimation procedure can\nbe reduced depending on the type of copula used.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:44:12 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Ahn", "Jae Youn", ""], ["Fuchs", "Sebastian", ""], ["Oh", "Rosy", ""]]}, {"id": "2008.12477", "submitter": "Philippe Goulet Coulombe", "authors": "Philippe Goulet Coulombe, Maxime Leroux, Dalibor Stevanovic,\n  St\\'ephane Surprenant", "title": "How is Machine Learning Useful for Macroeconomic Forecasting?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We move beyond \"Is Machine Learning Useful for Macroeconomic Forecasting?\" by\nadding the \"how\". The current forecasting literature has focused on matching\nspecific variables and horizons with a particularly successful algorithm. In\ncontrast, we study the usefulness of the underlying features driving ML gains\nover standard macroeconometric methods. We distinguish four so-called features\n(nonlinearities, regularization, cross-validation and alternative loss\nfunction) and study their behavior in both the data-rich and data-poor\nenvironments. To do so, we design experiments that allow to identify the\n\"treatment\" effects of interest. We conclude that (i) nonlinearity is the true\ngame changer for macroeconomic prediction, (ii) the standard factor model\nremains the best regularization, (iii) K-fold cross-validation is the best\npractice and (iv) the $L_2$ is preferred to the $\\bar \\epsilon$-insensitive\nin-sample loss. The forecasting gains of nonlinear techniques are associated\nwith high macroeconomic uncertainty, financial stress and housing bubble\nbursts. This suggests that Machine Learning is useful for macroeconomic\nforecasting by mostly capturing important nonlinearities that arise in the\ncontext of uncertainty and financial frictions.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 04:23:52 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Coulombe", "Philippe Goulet", ""], ["Leroux", "Maxime", ""], ["Stevanovic", "Dalibor", ""], ["Surprenant", "St\u00e9phane", ""]]}, {"id": "2008.12625", "submitter": "Berent {\\AA}nund Str{\\o}mnes Lunde", "authors": "Berent {\\AA}nund Str{\\o}mnes Lunde, Tore Selland Kleppe", "title": "agtboost: Adaptive and Automatic Gradient Tree Boosting Computations", "comments": "16 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  agtboost is an R package implementing fast gradient tree boosting\ncomputations in a manner similar to other established frameworks such as\nxgboost and LightGBM, but with significant decreases in computation time and\nrequired mathematical and technical knowledge. The package automatically takes\ncare of split/no-split decisions and selects the number of trees in the\ngradient tree boosting ensemble, i.e., agtboost adapts the complexity of the\nensemble automatically to the information in the data. All of this is done\nduring a single training run, which is made possible by utilizing developments\nin information theory for tree algorithms {\\tt arXiv:2008.05926v1 [stat.ME]}.\nagtboost also comes with a feature importance function that eliminates the\ncommon practice of inserting noise features. Further, a useful model validation\nfunction performs the Kolmogorov-Smirnov test on the learned distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 12:42:19 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Lunde", "Berent \u00c5nund Str\u00f8mnes", ""], ["Kleppe", "Tore Selland", ""]]}, {"id": "2008.12706", "submitter": "Michael Pfarrhofer", "authors": "Florian Huber, Gary Koop, Luca Onorante, Michael Pfarrhofer, Josef\n  Schreiner", "title": "Nowcasting in a Pandemic using Non-Parametric Mixed Frequency VARs", "comments": "JEL: C11, C32, C53, E37; Keywords: Regression tree models, Bayesian,\n  macroeconomic forecasting, vector autoregressions", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops Bayesian econometric methods for posterior inference in\nnon-parametric mixed frequency VARs using additive regression trees. We argue\nthat regression tree models are ideally suited for macroeconomic nowcasting in\nthe face of extreme observations, for instance those produced by the COVID-19\npandemic of 2020. This is due to their flexibility and ability to model\noutliers. In an application involving four major euro area countries, we find\nsubstantial improvements in nowcasting performance relative to a linear mixed\nfrequency VAR.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 15:35:54 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 08:45:24 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 10:28:04 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Huber", "Florian", ""], ["Koop", "Gary", ""], ["Onorante", "Luca", ""], ["Pfarrhofer", "Michael", ""], ["Schreiner", "Josef", ""]]}, {"id": "2008.12802", "submitter": "Hao Chen Dr.", "authors": "Hao Chen, Minguang Zhang, Lanshan Han, Alvin Lim", "title": "Hierarchical Marketing Mix Models with Sign Constraints", "comments": null, "journal-ref": "Journal of Applied Statistics (2021)", "doi": "10.1080/02664763.2021.1946020", "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marketing mix models (MMMs) are statistical models for measuring the\neffectiveness of various marketing activities such as promotion, media\nadvertisement, etc. In this research, we propose a comprehensive marketing mix\nmodel that captures the hierarchical structure and the carryover, shape and\nscale effects of certain marketing activities, as well as sign restrictions on\ncertain coefficients that are consistent with common business sense. In\ncontrast to commonly adopted approaches in practice, which estimate parameters\nin a multi-stage process, the proposed approach estimates all the unknown\nparameters/coefficients simultaneously using a constrained maximum likelihood\napproach and solved with the Hamiltonian Monte Carlo algorithm. We present\nresults on real datasets to illustrate the use of the proposed solution\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 18:16:21 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chen", "Hao", ""], ["Zhang", "Minguang", ""], ["Han", "Lanshan", ""], ["Lim", "Alvin", ""]]}, {"id": "2008.12849", "submitter": "Tesary Lin", "authors": "Tesary Lin and Sanjog Misra", "title": "The Identity Fragmentation Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers interact with firms across multiple devices, browsers, and\nmachines; these interactions are often recorded with different identifiers for\nthe same consumer. The failure to correctly match different identities leads to\na fragmented view of exposures and behaviors. This paper studies the identity\nfragmentation bias, referring to the estimation bias resulted from using\nfragmented data. Using a formal framework, we decompose the contributing\nfactors of the estimation bias caused by data fragmentation and discuss the\ndirection of bias. Contrary to conventional wisdom, this bias cannot be signed\nor bounded under standard assumptions. Instead, upward biases and sign\nreversals can occur even in experimental settings. We then compare several\ncorrective measures, and discuss their respective advantages and caveats.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 21:15:37 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 01:00:38 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Lin", "Tesary", ""], ["Misra", "Sanjog", ""]]}, {"id": "2008.12887", "submitter": "Marta Bofill Roig", "authors": "Marta Bofill Roig, Yu Shen and Guadalupe G\\'omez Melis", "title": "Design of phase III trials with long-term survival outcomes based on\n  short-term binary results", "comments": null, "journal-ref": "Statistics in Medicine, 2021", "doi": "10.1002/sim.9018", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pathologic complete response (pCR) is a common primary endpoint for a phase\nII trial or even accelerated approval of neoadjuvant cancer therapy. If\ngranted, a two-arm confirmatory trial is often required to demonstrate the\nefficacy with a time-to-event outcome such as overall survival. However, the\ndesign of a subsequent phase III trial based on prior information on the pCR\neffect is not straightforward. Aiming at designing such phase III trials with\noverall survival as primary endpoint using pCR information from previous\ntrials, we consider a mixture model that incorporates both the survival and the\nbinary endpoints. We propose to base the comparison between arms on the\ndifference of the restricted mean survival times, and show how the effect size\nand sample size for overall survival rely on the probability of the binary\nresponse and the survival distribution by response status, both for each\ntreatment arm. Moreover, we provide the sample size calculation under different\nscenarios and accompany them with an R package where all the computations have\nbeen implemented. We evaluate our proposal with a simulation study, and\nillustrate its application through a neoadjuvant breast cancer trial.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 01:05:37 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 20:38:25 GMT"}, {"version": "v3", "created": "Mon, 24 May 2021 07:10:49 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Roig", "Marta Bofill", ""], ["Shen", "Yu", ""], ["Melis", "Guadalupe G\u00f3mez", ""]]}, {"id": "2008.13005", "submitter": "Rafael Izbicki", "authors": "Marco Henrique de Almeida In\\'acio, Rafael Izbicki, Danilo\n  Louren\\c{c}o Lopes, Luis Ernesto Salasar, Jo\\~ao Poloniato, Marcio Alves\n  Diniz", "title": "Wisdom of the crowds forecasting the 2018 FIFA Men's World Cup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The FIFA Men's World Cup Tournament (WCT) is the most important football\n(soccer) competition, attracting worldwide attention. A popular practice among\nfootball fans in Brazil is to organize contests in which each participant\ninforms guesses on the final score of each match. The participants are then\nranked according to some scoring rule. Inspired by these contests, we created a\nwebsite to hold an online contest, in which participants were asked for their\nprobabilities on the outcomes of upcoming matches of the WCT. After each round\nof the tournament, the ranking of all users based on a proper scoring rule were\npublished. This paper studies the performance of some methods intended to\nextract the wisdom of the crowds, which are aggregated forecasts that uses some\nor all of the forecasts available. The later methods are compared to simpler\nforecasting strategies as well as to statistical prediction models. Our\nfindings corroborate the hypothesis that, at least for sporting events, the\nwisdom of the crowds offers a competitive forecasting strategy. Specifically,\nsome of these strategies were able to achieve high scores in our contest.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 16:23:09 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["In\u00e1cio", "Marco Henrique de Almeida", ""], ["Izbicki", "Rafael", ""], ["Lopes", "Danilo Louren\u00e7o", ""], ["Salasar", "Luis Ernesto", ""], ["Poloniato", "Jo\u00e3o", ""], ["Diniz", "Marcio Alves", ""]]}, {"id": "2008.13068", "submitter": "Hsien-Wei Chen", "authors": "Hsien-Wei Chen", "title": "Modeling of Daily Precipitation Amounts Using the Mixed Gamma Weibull\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By recognizing that the main difficulty of the modeling of daily\nprecipitation amounts is the selection of an appropriate probability\ndistribution, this study aims to establish a model selection framework to\nidentify the appropriate probability distribution for the modeling of daily\nprecipitation amounts from the commonly used probability distributions, i.e.\nthe exponential, Gamma, Weibull, and mixed exponential distributions. The mixed\nGamma Weibull (MGW) distribution serves this purpose because all the commonly\nused probability distributions are special cases of the MGW distribution, and\nthe MGW distribution integrates all the commonly used probability distributions\ninto one framework. Finally, via the large sample inference of likelihood\nratios, a model selection criterion can be established to identify the\nappropriate model for the modeling of daily precipitation amounts from the MGW\ndistribution framework.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 22:53:07 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chen", "Hsien-Wei", ""]]}, {"id": "2008.13096", "submitter": "Amir Weiss", "authors": "Amir Weiss and Arie Yeredor", "title": "Blind Determination of the Number of Sources Using Distance Correlation", "comments": null, "journal-ref": "in IEEE Signal Processing Letters, vol. 26, no. 6, pp. 828-832,\n  June 2019", "doi": "10.1109/LSP.2019.2902118", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel blind estimate of the number of sources from noisy, linear mixtures\nis proposed. Based on Sz\\'ekely et al.'s distance correlation measure, we\ndefine the Sources' Dependency Criterion (SDC), from which our estimate arises.\nUnlike most previously proposed estimates, the SDC estimate exploits the full\nindependence of the sources and noise, as well as the non-Gaussianity of the\nsources (as opposed to the Gaussianity of the noise), via implicit use of\nhigh-order statistics. This leads to a more robust, resilient and stable\nestimate w.r.t. the mixing matrix and the noise covariance structure. Empirical\nsimulation results demonstrate these virtues, on top of superior performance in\ncomparison with current state of the art estimates.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 05:33:37 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Weiss", "Amir", ""], ["Yeredor", "Arie", ""]]}, {"id": "2008.13189", "submitter": "Amir Weiss", "authors": "Amir Weiss, Sher Ali Cheema, Martin Haardt, and Arie Yeredor", "title": "Performance Analysis of the Gaussian Quasi-Maximum Likelihood Approach\n  for Independent Vector Analysis", "comments": null, "journal-ref": "in IEEE Trans. on Signal Process., vol. 66, no. 19, pp. 5000-5013,\n  1 Oct.1, 2018", "doi": "10.1109/TSP.2018.2863656", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum Likelihood (ML) estimation requires precise knowledge of the\nunderlying statistical model. In Quasi ML (QML), a presumed model is used as a\nsubstitute to the (unknown) true model. In the context of Independent Vector\nAnalysis (IVA), we consider the Gaussian QML Estimate (QMLE) of the demixing\nmatrices set and present an (approximate) analysis of its asymptotic separation\nperformance. In Gaussian QML the sources are presumed to be Gaussian, with\ncovariance matrices specified by some \"educated guess\". The resulting\nquasi-likelihood equations of the demixing matrices take a special form,\nrecently termed an extended \"Sequentially Drilled\" Joint Congruence (SeDJoCo)\ntransformation, which is reminiscent of (though essentially different from)\nclassical joint diagonalization. We show that asymptotically this QMLE, i.e.,\nthe solution of the resulting extended SeDJoCo transformation, attains perfect\nseparation (under some mild conditions) regardless of the sources' true\ndistributions and/or covariance matrices. In addition, based on the\n\"small-errors\" assumption, we present a first-order perturbation analysis of\nthe extended SeDJoCo solution. Using the resulting closed-form expressions for\nthe errors in the solution matrices, we provide closed-form expressions for the\nresulting Interference-to-Source Ratios (ISRs) for IVA. Moreover, we prove that\nasymptotically the ISRs depend only on the sources' covariances, and not on\ntheir specific distributions. As an immediate consequence of this result, we\nprovide an asymptotically attainable lower bound on the resulting ISRs. We also\npresent empirical results, corroborating our analytical derivations, of three\nsimulation experiments concerning two possible model errors - inaccurate\ncovariance matrices and sources' distribution mismodeling.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 14:59:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Weiss", "Amir", ""], ["Cheema", "Sher Ali", ""], ["Haardt", "Martin", ""], ["Yeredor", "Arie", ""]]}, {"id": "2008.13199", "submitter": "Amir Weiss", "authors": "Amir Weiss, Arie Yeredor, Sher Ali Cheema, and Martin Haardt", "title": "The Extended \"Sequentially Drilled\" Joint Congruence Transformation and\n  its Application in Gaussian Independent Vector Analysis", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 65, no. 23, pp.\n  6332-6344, 1 Dec.1, 2017", "doi": "10.1109/TSP.2017.2750107", "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Vector Analysis (IVA) has emerged in recent years as an extension\nof Independent Component Analysis (ICA) into multiple sets of mixtures, where\nthe source signals in each set are independent, but may depend on source\nsignals in the other sets. In a semi-blind IVA (or ICA) framework, information\nregarding the probability distributions of the sources may be available, giving\nrise to Maximum Likelihood (ML) separation. In recent work we have shown that\nunder the multivariate Gaussian model, with arbitrary temporal covariance\nmatrices (stationary or non-stationary) of the source signals, ML separation\nrequires the solution of a \"Sequentially Drilled\" Joint Congruence (SeDJoCo)\ntransformation of a set of matrices, which is reminiscent of (but different\nfrom) classical joint diagonalization. In this paper we extend our results to\nthe IVA problem, showing how the ML solution for the Gaussian model (with\narbitrary covariance and cross-covariance matrices) takes the form of an\nextended SeDJoCo problem. We formulate the extended problem, derive a condition\nfor the existence of a solution, and propose two iterative solution algorithms.\nIn addition, we derive the induced Cram\\'er-Rao Lower Bound (iCRLB) on the\nresulting Interference-to-Source Ratios (ISR) matrices, and demonstrate by\nsimulation how the ML separation obtained by solving the extended SeDJoCo\nproblem indeed attains the iCRLB (asymptotically), as opposed to other\nseparation approaches, which cannot exploit prior knowledge regarding the\nsources' distributions.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 15:41:15 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Weiss", "Amir", ""], ["Yeredor", "Arie", ""], ["Cheema", "Sher Ali", ""], ["Haardt", "Martin", ""]]}, {"id": "2008.13296", "submitter": "Shahab Boumi", "authors": "Shahab Boumi, Adan Vela", "title": "Improving Graduation Rate Estimates Using Regularly Updating Multi-Level\n  Absorbing Markov Chains", "comments": null, "journal-ref": null, "doi": "10.3390/educsci10120377", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  American universities use a procedure based on a rolling six-year graduation\nrate to calculate statistics regarding their students' final educational\noutcomes (graduating or not graduating). As~an alternative to the six-year\ngraduation rate method, many studies have applied absorbing Markov chains for\nestimating graduation rates. In both cases, a frequentist approach is used.\nFor~the standard six-year graduation rate method, the frequentist approach\ncorresponds to counting the number of students who finished their program\nwithin six years and dividing by the number of students who entered that year.\nIn the case of absorbing Markov chains, the frequentist approach is used to\ncompute the underlying transition matrix, which is then used to estimate the\ngraduation rate. In this paper, we apply a sensitivity analysis to compare the\nperformance of the standard six-year graduation rate method with that of\nabsorbing Markov chains. Through the analysis, we highlight significant\nlimitations with regards to the estimation accuracy of both approaches when\napplied to small sample sizes or cohorts at a university. Additionally, we note\nthat the Absorbing Markov chain method introduces a significant bias, which\nleads to an underestimation of the true graduation rate. To~overcome both these\nchallenges, we propose and evaluate the use of a regularly updating multi-level\nabsorbing Markov chain (RUML-AMC) in which the transition matrix is updated\nyear to year. We empirically demonstrate that the proposed RUML-AMC approach\nnearly eliminates estimation bias while reducing the estimation variation by\nmore than 40%, especially for populations with small sample sizes.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 23:42:00 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 16:10:43 GMT"}, {"version": "v3", "created": "Fri, 18 Dec 2020 17:14:43 GMT"}], "update_date": "2020-12-21", "authors_parsed": [["Boumi", "Shahab", ""], ["Vela", "Adan", ""]]}, {"id": "2008.13412", "submitter": "Patrick Schwab", "authors": "Patrick Schwab, Arash Mehrjou, Sonali Parbhoo, Leo Anthony Celi,\n  J\\\"urgen Hetzel, Markus Hofer, Bernhard Sch\\\"olkopf, Stefan Bauer", "title": "Real-time Prediction of COVID-19 related Mortality using Electronic\n  Health Records", "comments": null, "journal-ref": null, "doi": "10.1038/s41467-020-20816-7", "report-no": null, "categories": "stat.AP cs.LG q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coronavirus Disease 2019 (COVID-19) is an emerging respiratory disease caused\nby the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) with rapid\nhuman-to-human transmission and a high case fatality rate particularly in older\npatients. Due to the exponential growth of infections, many healthcare systems\nacross the world are under pressure to care for increasing amounts of at-risk\npatients. Given the high number of infected patients, identifying patients with\nthe highest mortality risk early is critical to enable effective intervention\nand optimal prioritisation of care. Here, we present the COVID-19 Early Warning\nSystem (CovEWS), a clinical risk scoring system for assessing COVID-19 related\nmortality risk. CovEWS provides continuous real-time risk scores for individual\npatients with clinically meaningful predictive performance up to 192 hours (8\ndays) in advance, and is automatically derived from patients' electronic health\nrecords (EHRs) using machine learning. We trained and evaluated CovEWS using\nde-identified data from a cohort of 66430 COVID-19 positive patients seen at\nover 69 healthcare institutions in the United States (US), Australia, Malaysia\nand India amounting to an aggregated total of over 2863 years of patient\nobservation time. On an external test cohort of 5005 patients, CovEWS predicts\nCOVID-19 related mortality from $78.8\\%$ ($95\\%$ confidence interval [CI]:\n$76.0$, $84.7\\%$) to $69.4\\%$ ($95\\%$ CI: $57.6, 75.2\\%$) specificity at a\nsensitivity greater than $95\\%$ between respectively 1 and 192 hours prior to\nobserved mortality events - significantly outperforming existing generic and\nCOVID-19 specific clinical risk scores. CovEWS could enable clinicians to\nintervene at an earlier stage, and may therefore help in preventing or\nmitigating COVID-19 related mortality.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:07:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Schwab", "Patrick", ""], ["Mehrjou", "Arash", ""], ["Parbhoo", "Sonali", ""], ["Celi", "Leo Anthony", ""], ["Hetzel", "J\u00fcrgen", ""], ["Hofer", "Markus", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bauer", "Stefan", ""]]}, {"id": "2008.13424", "submitter": "Prosha Rahman", "authors": "Prosha A. Rahman, Boris Beranger, Matthew Roughan, Scott A. Sisson", "title": "Likelihood-based inference for modelling packet transit from thinned\n  flow summaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The substantial growth of network traffic speed and volume presents practical\nchallenges to network data analysis. Packet thinning and flow aggregation\nprotocols such as NetFlow reduce the size of datasets by providing structured\ndata summaries, but conversely this impedes statistical inference. Methods\nwhich aim to model patterns of traffic propagation typically do not account for\nthe packet thinning and summarisation process into the analysis, and are often\nsimplistic, e.g.~method-of-moments. As a result, they can be of limited\npractical use.\n  We introduce a likelihood-based analysis which fully incorporates packet\nthinning and NetFlow summarisation into the analysis. As a result, inferences\ncan be made for models on the level of individual packets while only observing\nthinned flow summary information. We establish consistency of the resulting\nmaximum likelihood estimator, derive bounds on the volume of traffic which\nshould be observed to achieve required levels of estimator accuracy, and\nidentify an ideal family of models. The robust performance of the estimator is\nexamined through simulated analyses and an application on a publicly available\ntrace dataset containing over 36m packets over a 1 minute period.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 08:27:10 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Rahman", "Prosha A.", ""], ["Beranger", "Boris", ""], ["Roughan", "Matthew", ""], ["Sisson", "Scott A.", ""]]}, {"id": "2008.13558", "submitter": "Santtu Tikka", "authors": "Santtu Tikka, Jussi Hakanen, Mirka Saarela, Juha Karvanen", "title": "Simulation Framework for Realistic Large-scale Individual-level Data\n  Generation with an Application in the Health Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a framework for realistic data generation and simulation of\ncomplex systems and demonstrate its capabilities in the health domain. The main\nuse cases of the framework are predicting the development of risk factors and\ndisease occurrence, evaluating the impact of interventions and policy\ndecisions, and statistical method development. We present the fundamentals of\nthe framework using rigorous mathematical definitions. The framework supports\ncalibration to a real population as well as various manipulations and data\ncollection processes. The freely available open-source implementation in R\nembraces efficient data structures, parallel computing and fast random number\ngeneration which ensure reproducibility and scalability. With the framework it\nis possible to run daily-level simulations for populations of millions of\nindividuals for decades of simulated time. An example on the occurrence of\nstroke, type 2 diabetes and mortality illustrates the usage of the framework in\nthe Finnish context. In the example, we demonstrate the data-collection\nfunctionality by studying the impact of non-participation on the estimated risk\nmodels and interventions related to controlling the additional salt intake.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 12:43:31 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:58:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tikka", "Santtu", ""], ["Hakanen", "Jussi", ""], ["Saarela", "Mirka", ""], ["Karvanen", "Juha", ""]]}, {"id": "2008.13575", "submitter": "Steven Turnbull", "authors": "Steven Martin Turnbull and Dion R.J. O'Neale", "title": "Entropy of Co-Enrolment Networks Reveal Disparities in High School STEM\n  Participation", "comments": "29 pages, 9 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current study uses a network analysis approach to explore the STEM\npathways that students take through their final year of high school in Aotearoa\nNew Zealand. By accessing individual-level microdata from New Zealand's\nIntegrated Data Infrastructure, we are able to create a co-enrolment network\ncomprised of all STEM assessment standards taken by students in New Zealand\nbetween 2010 and 2016. We explore the structure of this co-enrolment network\nthough use of community detection and a novel measure of entropy. We then\ninvestigate how network structure differs across sub-populations based on\nstudents' sex, ethnicity, and the socio-economic-status (SES) of the high\nschool they attended. Results show the structure of the STEM co-enrolment\nnetwork differs across these sub-populations, and also changes over time. We\nfind that, while female students were more likely to have been enrolled in life\nscience standards, they were less well represented in physics, calculus, and\nvocational (e.g., agriculture, practical technology) standards. Our results\nalso show that the enrolment patterns of the Maori and Pacific Islands\nsub-populations had higher levels of entropy, an observation that may be\nexplained by fewer enrolments in key science and mathematics standards. Through\nfurther investigation of this disparity, we find that ethnic group differences\nin entropy are moderated by high school SES, such that the difference in\nentropy between Maori and Pacific Islands students, and European and Asian\nstudents is even greater. We discuss these findings in the context of the New\nZealand education system and policy changes that occurred between 2010 and\n2016.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 23:20:00 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Turnbull", "Steven Martin", ""], ["O'Neale", "Dion R. J.", ""]]}, {"id": "2008.13600", "submitter": "Dionysis Manousakas", "authors": "Dionysis Manousakas and Cecilia Mascolo", "title": "$\\beta$-Cores: Robust Large-Scale Bayesian Data Summarization in the\n  Presence of Outliers", "comments": "25 pages, 5 figures, Accepted at the 14th ACM International\n  Conference on Web Search and Data Mining", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning applications should be able to address the intrinsic\nchallenges arising over inference on massive real-world datasets, including\nscalability and robustness to outliers. Despite the multiple benefits of\nBayesian methods (such as uncertainty-aware predictions, incorporation of\nexperts knowledge, and hierarchical modeling), the quality of classic Bayesian\ninference depends critically on whether observations conform with the assumed\ndata generating model, which is impossible to guarantee in practice. In this\nwork, we propose a variational inference method that, in a principled way, can\nsimultaneously scale to large datasets, and robustify the inferred posterior\nwith respect to the existence of outliers in the observed data. Reformulating\nBayes theorem via the $\\beta$-divergence, we posit a robustified\npseudo-Bayesian posterior as the target of inference. Moreover, relying on the\nrecent formulations of Riemannian coresets for scalable Bayesian inference, we\npropose a sparse variational approximation of the robustified posterior and an\nefficient stochastic black-box algorithm to construct it. Overall our method\nallows releasing cleansed data summaries that can be applied broadly in\nscenarios including structured data corruption. We illustrate the applicability\nof our approach in diverse simulated and real datasets, and various statistical\nmodels, including Gaussian mean inference, logistic and neural linear\nregression, demonstrating its superiority to existing Bayesian summarization\nmethods in the presence of outliers.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 13:47:12 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:25:11 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Manousakas", "Dionysis", ""], ["Mascolo", "Cecilia", ""]]}, {"id": "2008.13619", "submitter": "Jun-Ichi Takeshita", "authors": "Jun-ichi Takeshita and Tomomichi Suzuki", "title": "Precision for binary measurement methods and results under beta-binomial\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To handle typical problems from fields dealing with biological responses,\nthis study develops a new statistical model and method for analysing the\nprecision of binary measurement methods and results from collaborative studies.\nThe model is based on beta-binomial distributions. In other words, we assume\nthat the sensitivity of each laboratory obeys a beta distribution and the\nbinary measurement results under a given sensitivity follow a binomial\ndistribution. We propose the key precision indicators of repeatability and\nreproducibility for the model and derive their unbiased estimates. We further\npropose a confidence interval for repeatability by applying the Jeffreys\ninterval, which utilizes the assumption of beta distributions for sensitivity.\nMoreover, we propose a statistical test for determining laboratory effects,\nusing simultaneous confidence intervals based on the confidence interval of\neach laboratory's sensitivity. Finally, we apply the proposed method to\nreal-world examples in the fields of food safety and chemical risk assessment\nand management.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:05:14 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Takeshita", "Jun-ichi", ""], ["Suzuki", "Tomomichi", ""]]}, {"id": "2008.13620", "submitter": "Ruoqi Liu", "authors": "Ruoqi Liu, Changchang Yin, Ping Zhang", "title": "Estimating Individual Treatment Effects with Time-Varying Confounders", "comments": "Accepted to ICDM 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the individual treatment effect (ITE) from observational data is\nmeaningful and practical in healthcare. Existing work mainly relies on the\nstrong ignorability assumption that no hidden confounders exist, which may lead\nto bias in estimating causal effects. Some studies consider the hidden\nconfounders are designed for static environment and not easily adaptable to a\ndynamic setting. In fact, most observational data (e.g., electronic medical\nrecords) is naturally dynamic and consists of sequential information. In this\npaper, we propose Deep Sequential Weighting (DSW) for estimating ITE with\ntime-varying confounders. Specifically, DSW infers the hidden confounders by\nincorporating the current treatment assignments and historical information\nusing a deep recurrent weighting neural network. The learned representations of\nhidden confounders combined with current observed data are leveraged for\npotential outcome and treatment predictions. We compute the time-varying\ninverse probabilities of treatment for re-weighting the population. We conduct\ncomprehensive comparison experiments on fully-synthetic, semi-synthetic and\nreal-world datasets to evaluate the performance of our model and baselines.\nResults demonstrate that our model can generate unbiased and accurate treatment\neffect by conditioning both time-varying observed and hidden confounders,\npaving the way for personalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:21:56 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 16:34:59 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Liu", "Ruoqi", ""], ["Yin", "Changchang", ""], ["Zhang", "Ping", ""]]}, {"id": "2008.13655", "submitter": "Maria Osipenko", "authors": "Maria Osipenko", "title": "Directional Assessment of Traffic Flow Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze extremes of traffic flow profiles composed of traffic counts over\na day. The data is essentially curves and determining which trajectory should\nbe classified as extreme is not straight forward. To assess the extremes of the\ntraffic flow curves in a coherent way, we use a directional definition of\nextremeness and apply the dimension reduction technique called principal\ncomponent analysis (PCA) in an asymmetric norm. In the classical PCA one\nreduces the dimensions of the data by projecting it in the direction of the\nlargest variation of the projection around its mean. In the PCA in an\nasymmetric norm one chooses the projection directions, such that the\nasymmetrically weighted variation around a tail index -- an expectile -- of the\ndata is the largest possible. Expectiles are tail measures that generalize the\nmean in a similar manner as quantiles generalize the median. Focusing on the\nasymmetrically weighted variation around an expectile of the data, we find the\nappropriate projection directions and the low dimensional representation of the\ntraffic flow profiles that uncover different patterns in their extremes. Using\nthe traffic flow data from the roundabout on Ernst-Reuter-Platz in the city\ncenter of Berlin, Germany, we estimate, visualize and interpret the resulting\nprincipal expectile components. The corresponding directional extremes of the\ntraffic flow profiles are simple to identify and to connect to their location-\nand time-related specifics. Their shapes are driven by their scores on each\nprincipal expectile component which is useful for extracting and analyzing\ntraffic patterns. Our approach to dimensionality reduction towards the\ndirectional extremes of traffic flow extends the related methodological basis\nand gives promising results for subsequent analysis, prediction and control of\ntraffic flow patterns.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:46:45 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 14:01:03 GMT"}], "update_date": "2020-09-07", "authors_parsed": [["Osipenko", "Maria", ""]]}]