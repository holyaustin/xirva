[{"id": "1707.00014", "submitter": "Morten Valberg", "authors": "Morten Valberg, Mats Julius Stensrud, Odd O. Aalen", "title": "The surprising implications of familial association in disease risk", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": "10.1186/s12889-018-5033-5", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A wide range of diseases show some degree of clustering in\nfamilies; family history is therefore an important aspect for clinicians when\nmaking risk predictions. Familial aggregation is often quantified in terms of a\nfamilial relative risk (FRR), and although at first glance this measure may\nseem simple and intuitive as an average risk prediction, its implications are\nnot straightforward.\n  Methods: We use two statistical models for the distribution of disease risk\nin a population: a dichotomous risk model that gives an intuitive understanding\nof the implication of a given FRR, and a continuous risk model that facilitates\na more detailed computation of the inequalities in disease risk. Published\nestimates of FRRs are used to produce Lorenz curves and Gini indices that\nquantifies the inequalities in risk for a range of diseases.\n  Results: We demonstrate that even a moderate familial association in disease\nrisk implies a very large difference in risk between individuals in the\npopulation. We give examples of diseases for which this is likely to be true,\nand we further demonstrate the relationship between the point estimates of FRRs\nand the distribution of risk in the population.\n  Conclusions: The variation in risk for several severe diseases may be larger\nthan the variation in income in many countries. The implications of familial\nrisk estimates should be recognized by epidemiologists and clinicians.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 14:23:16 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 13:18:24 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 16:30:42 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Valberg", "Morten", ""], ["Stensrud", "Mats Julius", ""], ["Aalen", "Odd O.", ""]]}, {"id": "1707.00046", "submitter": "Alexandra Chouldechova", "authors": "Alexandra Chouldechova and Max G'Sell", "title": "Fairer and more accurate, but for whom?", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex statistical machine learning models are increasingly being used or\nconsidered for use in high-stakes decision-making pipelines in domains such as\nfinancial services, health care, criminal justice and human services. These\nmodels are often investigated as possible improvements over more classical\ntools such as regression models or human judgement. While the modeling approach\nmay be new, the practice of using some form of risk assessment to inform\ndecisions is not. When determining whether a new model should be adopted, it is\ntherefore essential to be able to compare the proposed model to the existing\napproach across a range of task-relevant accuracy and fairness metrics. Looking\nat overall performance metrics, however, may be misleading. Even when two\nmodels have comparable overall performance, they may nevertheless disagree in\ntheir classifications on a considerable fraction of cases. In this paper we\nintroduce a model comparison framework for automatically identifying subgroups\nin which the differences between models are most pronounced. Our primary focus\nis on identifying subgroups where the models differ in terms of\nfairness-related quantities such as racial or gender disparities. We present\nexperimental results from a recidivism prediction task and a hypothetical\nlending example.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 21:07:09 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Chouldechova", "Alexandra", ""], ["G'Sell", "Max", ""]]}, {"id": "1707.00222", "submitter": "Carlos Oscar Sorzano S.", "authors": "C.O.S. Sorzano, D. Tabas-Madrid, F. N\\'u\\~nez, C. Fern\\'andez-Criado,\n  and A. Naranjo", "title": "Sample Size for Pilot Studies and Precision Driven Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pilot studies are highly recommended in experiments with animals when little\nis known about the anticipated values of the mean of the variable under study,\nits variance or the probability of response. They are also recommended to test\nthe feasibility of the animal model or the experimental technique. However, the\nsample size required for a pilot study has received little attention and\ntypical sizes practically used go from 5 to 20 animals disregarding any\nstatistical consideration. Pilot studies are a particular case of precision\ndriven experiments in which the sample size is designed according to a desired\nprecision. In this article we provide some statistical guidance on the\nselection of the sample size of a pilot study whose driving force is the\naccuracy desired for the determination of the unknown parameters. We provide\nformulas and design tables for the sample size when trying to determine the\nstandard deviation of a population, its mean, the probability of a certain\nfeature or event, the correlation between two variables, and the survival time\nbefore an event occurs. All the calculations performed in this article can also\nbe freely performed on the web through the online calculator available at\nhttp://i2pc.es/coss/Programs/SampleSizeCalculator/index.html. An immediate\nconsequence of our analysis is that typical pilot sizes (5-20) are normally\nresult in a very small precision and that the researcher should be aware of\nthis fact before carrying out a pilot study.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 23:37:18 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 09:27:09 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Sorzano", "C. O. S.", ""], ["Tabas-Madrid", "D.", ""], ["N\u00fa\u00f1ez", "F.", ""], ["Fern\u00e1ndez-Criado", "C.", ""], ["Naranjo", "A.", ""]]}, {"id": "1707.00223", "submitter": "Wahab Ali Gulzar Khawaja", "authors": "Wahab Khawaja, Ismail Guvenc, and Arindam Chowdhury", "title": "Ultra-wideband Channel Modeling for Hurricanes", "comments": "Paper accepted in Proc. of VTC Fall 2017, Antenna Systems,\n  Propagation, and RF Design Papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maintaining communications during major hurricanes is critically important\nfor public safety operations by first responders. This requires accurate\nknowledge of the propagation channel during hurricane conditions. In this work,\nwe have carried out ultra-wideband (UWB) channel measurements during hurricane\nconditions ranging from Category-1 to Category-4, generated at the Wall of Wind\n(WoW) facility of Florida International University (FIU). Time Domain P410\nradios are used for channel measurements. From the empirical data analysis in\ntime domain, we developed a UWB statistical broadband channel model for\nhurricanes. In particular, we characterize the effects of rain and wind speed\non large scale and small scale UWB propagation parameters.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 23:46:23 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Khawaja", "Wahab", ""], ["Guvenc", "Ismail", ""], ["Chowdhury", "Arindam", ""]]}, {"id": "1707.00306", "submitter": "Michael Fop", "authors": "Michael Fop and Thomas Brendan Murphy", "title": "Variable Selection Methods for Model-based Clustering", "comments": null, "journal-ref": "Statistics Surveys, 12 (2018) 1-48", "doi": "10.1214/18-SS119", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 15:29:13 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 07:52:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fop", "Michael", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1707.00453", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston", "title": "Statistical Analysis of Functions on Surfaces, with an application to\n  Medical Imaging", "comments": "42 pages", "journal-ref": null, "doi": "10.1080/01621459.2019.1635479", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Functional Data Analysis, data are commonly assumed to be smooth functions\non a fixed interval of the real line. In this work, we introduce a\ncomprehensive framework for the analysis of functional data, whose domain is a\ntwo-dimensional manifold and the domain itself is subject to variability from\nsample to sample. We formulate a statistical model for such data, here called\nFunctions on Surfaces, which enables a joint representation of the geometric\nand functional aspects, and propose an associated estimation framework. We\nassess the validity of the framework by performing a simulation study and we\nfinally apply it to the analysis of neuroimaging data of cortical thickness,\nacquired from the brains of different subjects, and thus lying on domains with\ndifferent geometries.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 09:15:47 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 10:34:39 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 10:31:02 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""]]}, {"id": "1707.00514", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger, Brita Roy, Carley Riley, Harlan M. Krumholz", "title": "People Mover's Distance: Class level geometry using fast pairwise data\n  adaptive transportation costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of defining a network graph on a large collection of\nclasses. Each class is comprised of a collection of data points, sampled in a\nnon i.i.d. way, from some unknown underlying distribution. The application we\nconsider in this paper is a large scale high dimensional survey of people\nliving in the US, and the question of how similar or different are the various\ncounties in which these people live. We use a co-clustering diffusion metric to\nlearn the underlying distribution of people, and build an approximate earth\nmover's distance algorithm using this data adaptive transportation cost.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 12:57:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Cloninger", "Alexander", ""], ["Roy", "Brita", ""], ["Riley", "Carley", ""], ["Krumholz", "Harlan M.", ""]]}, {"id": "1707.00640", "submitter": "Matthew Price-Williams", "authors": "Matthew Price-Williams, Nick Heard, Melissa Turcotte", "title": "Detecting periodic subsequences in cyber security data", "comments": "31 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical approaches to cyber-security involve building realistic\nprobability models of computer network data. In a data pre-processing phase,\nseparating automated events from those caused by human activity should improve\nstatistical model building and enhance anomaly detection capabilities. This\narticle presents a changepoint detection framework for identifying periodic\nsubsequences of event times. The opening event of each subsequence can be\ninterpreted as a human action which then generates an automated, periodic\nprocess. Difficulties arising from the presence of duplicate and missing data\nare addressed. The methodology is demonstrated using authentication data from\nthe computer network of Los Alamos National Laboratory.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 03:26:31 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Price-Williams", "Matthew", ""], ["Heard", "Nick", ""], ["Turcotte", "Melissa", ""]]}, {"id": "1707.00647", "submitter": "Abdelghafour Halimi", "authors": "Abdelghafour Halimi, Hadj Batatia, Jimmy Le Digabel, Gwendal Josse,\n  Jean-Yves Tourneret", "title": "Statistical modeling and classification of reflectance confocal\n  microscopy images", "comments": "24 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the characterization and classification of reflectance\nconfocal microscopy images of human skin. The aim is to identify and\ncharacterize the lentigo, a phenomenon that originates at the dermo-epidermic\njunction of the skin. High resolution confocal images are acquired at different\nskin depths and are analyzed for each depth. Histograms of pixel intensities\nassociated with a given depth are determined, showing that the generalized\ngamma distribution (GGD) is a good statistical model for confocal images. A GGD\nis parameterized by translation, scale and shape parameters. These parameters\nare estimated using a new estimation method based on a natural gradient descent\nshowing fast convergence properties with respect to state-of-the-art estimation\nmethods. The resulting parameter estimates can be used to classify clinical\nimages of healthy and lentigo patients. The obtained results show that the\nscale and shape parameters are good features to identify and characterize the\npresence of lentigo in skin tissues.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 10:20:09 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Halimi", "Abdelghafour", ""], ["Batatia", "Hadj", ""], ["Digabel", "Jimmy Le", ""], ["Josse", "Gwendal", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1707.00781", "submitter": "David Sanchez", "authors": "Bruno Gon\\c{c}alves, Luc\\'ia Loureiro-Porto, Jos\\'e J. Ramasco, David\n  S\\'anchez", "title": "Mapping the Americanization of English in Space and Time", "comments": "16 pages, 6 figures, 2 tables. Published version", "journal-ref": "PLoS ONE 13: e0197741 (2018)", "doi": "10.1371/journal.pone.0197741", "report-no": null, "categories": "cs.CL cond-mat.stat-mech cs.CY physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As global political preeminence gradually shifted from the United Kingdom to\nthe United States, so did the capacity to culturally influence the rest of the\nworld. In this work, we analyze how the world-wide varieties of written English\nare evolving. We study both the spatial and temporal variations of vocabulary\nand spelling of English using a large corpus of geolocated tweets and the\nGoogle Books datasets corresponding to books published in the US and the UK.\nThe advantage of our approach is that we can address both standard written\nlanguage (Google Books) and the more colloquial forms of microblogging messages\n(Twitter). We find that American English is the dominant form of English\noutside the UK and that its influence is felt even within the UK borders.\nFinally, we analyze how this trend has evolved over time and the impact that\nsome cultural events have had in shaping it.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 23:32:55 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 12:27:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Gon\u00e7alves", "Bruno", ""], ["Loureiro-Porto", "Luc\u00eda", ""], ["Ramasco", "Jos\u00e9 J.", ""], ["S\u00e1nchez", "David", ""]]}, {"id": "1707.00849", "submitter": "Vahid Yaghoubi", "authors": "Vahid Yaghoubi, Majid K. Vakilzadeh, Thomas J.S. Abrahamsson", "title": "Automated Modal Parameter Estimation Using Correlation Analysis and\n  Bootstrap Sampling", "comments": null, "journal-ref": null, "doi": "10.1016/j.ymssp.2017.07.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of modal parameters from a set of noisy measured data is a\nhighly judgmental task, with user expertise playing a significant role in\ndistinguishing between estimated physical and noise modes of a test-piece.\nVarious methods have been developed to automate this procedure. The common\napproach is to identify models with different orders and cluster similar modes\ntogether. However, most proposed methods based on this approach suffer from\nhigh-dimensional optimization problems in either the estimation or clustering\nstep. To overcome this problem, this study presents an algorithm for autonomous\nmodal parameter estimation in which the only required optimization is performed\nin a three-dimensional space. To this end, a subspace-based identification\nmethod is employed for the estimation and a non-iterative correlation-based\nmethod is used for the clustering. This clustering is at the heart of the\npaper. The keys to success are correlation metrics that are able to treat the\nproblems of spatial eigenvector aliasing and nonunique eigenvectors of\ncoalescent modes simultaneously. The algorithm commences by the identification\nof an excessively high-order model from frequency response function test data.\nThe high number of modes of this model provide bases for two subspaces: one for\nlikely physical modes and one for its complement dubbed the subspace of noise\nmodes. By employing the bootstrap resampling technique, several subsets are\ngenerated from the same basic dataset and for each of them a model is\nidentified to form a set of models. Then, by correlation analysis, highly\ncorrelated modes of these models which appear repeatedly are clustered together\nand the noise modes are collected in a so-called Trashbox cluster. Stray noise\nmodes attracted to the mode clusters are trimmed away by correlation analysis.\nThe final step is a fuzzy c-means clustering procedure.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 08:40:27 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Yaghoubi", "Vahid", ""], ["Vakilzadeh", "Majid K.", ""], ["Abrahamsson", "Thomas J. S.", ""]]}, {"id": "1707.00883", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini, Marica Manisera and Paola Zuccolotto", "title": "Space-Time Analysis of Movements in Basketball using Sensor Data", "comments": "7 pages, 3 figures, proceedings of SIS17: Statistics and Data\n  Science: New Challenges, New Generations", "journal-ref": null, "doi": null, "report-no": "e-ISBN: 978-88-6453-521-0", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global Positioning Systems (GPS) are nowadays intensively used in Sport\nScience as they permit to capture the space-time trajectories of players, with\nthe aim to infer useful information to coaches in addition to traditional\nstatistics. In our application to basketball, we used Cluster Analysis in order\nto split the match in a number of separate time-periods, each identifying\nhomogeneous spatial relations among players in the court. Results allowed us to\nidentify differences in spacing among players, distinguish defensive or\noffensive actions, analyze transition probabilities from a certain group to\nanother one.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:55:18 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Metulini", "Rodolfo", ""], ["Manisera", "Marica", ""], ["Zuccolotto", "Paola", ""]]}, {"id": "1707.01052", "submitter": "Bahadir Y\\\"uzba\\c{s}i", "authors": "Bahad{\\i}r Y\\\"uzba\\c{s}{\\i}, Yasin A\\c{s}ar, \\c{S}amil \\c{S}{\\i}k and\n  Ahmet Demiralp", "title": "Improving Estimations in Quantile Regression Model with Autoregressive\n  Errors", "comments": null, "journal-ref": "THERMAL SCIENCE: Year 2018, Vol. 22, Suppl. 1, pp. S97-S107", "doi": "10.2298/TSCI170612275Y", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important issue is that the respiratory mortality may be a result of air\npollution which can be measured by the following variables: temperature,\nrelative humidity, carbon monoxide, sulfur dioxide, nitrogen dioxide,\nhydrocarbons, ozone and particulates. The usual way is to fit a model using the\nordinary least squares regression, which has some assumptions, also known as\nGauss-Markov assumptions, on the error term showing white noise process of the\nregression model. However, in many applications, especially for this example,\nthese assumptions are not satisfied. Therefore, in this study, a quantile\nregression approach is used to model the respiratory mortality using the\nmentioned explanatory variables. Moreover, improved estimation techniques such\nas preliminary testing and shrinkage strategies are also obtained when the\nerrors are autoregressive. A Monte Carlo simulation experiment, including the\nquantile penalty estimators such as Lasso, Ridge and Elastic Net, is designed\nto evaluate the performances of the proposed techniques. Finally, the\ntheoretical risks of the listed estimators are given.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 16:12:15 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Y\u00fczba\u015f\u0131", "Bahad\u0131r", ""], ["A\u015far", "Yasin", ""], ["\u015e\u0131k", "\u015eamil", ""], ["Demiralp", "Ahmet", ""]]}, {"id": "1707.01195", "submitter": "Thomas Miconi", "authors": "Thomas Miconi", "title": "The impossibility of \"fairness\": a generalized impossibility result for\n  decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various measures can be used to estimate bias or unfairness in a predictor.\nPrevious work has already established that some of these measures are\nincompatible with each other. Here we show that, when groups differ in\nprevalence of the predicted event, several intuitive, reasonable measures of\nfairness (probability of positive prediction given occurrence or\nnon-occurrence; probability of occurrence given prediction or non-prediction;\nand ratio of predictions over occurrences for each group) are all mutually\nexclusive: if one of them is equal among groups, the other two must differ. The\nonly exceptions are for perfect, or trivial (always-positive or\nalways-negative) predictors. As a consequence, any non-perfect, non-trivial\npredictor must necessarily be \"unfair\" under two out of three reasonable sets\nof criteria. This result readily generalizes to a wide range of well-known\nstatistical quantities (sensitivity, specificity, false positive rate,\nprecision, etc.), all of which can be divided into three mutually exclusive\ngroups. Importantly, The results applies to all predictors, whether algorithmic\nor human. We conclude with possible ways to handle this effect when assessing\nand designing prediction methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 02:02:42 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 16:31:06 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 21:18:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Miconi", "Thomas", ""]]}, {"id": "1707.01199", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti and Alessandra Micheletti", "title": "A clustering algorithm for multivariate data streams with correlated\n  components", "comments": "title changed, rewritten", "journal-ref": "J Big Data (2017) 4:48", "doi": "10.1186/s40537-017-0109-0", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common clustering algorithms require multiple scans of all the data to\nachieve convergence, and this is prohibitive when large databases, with data\narriving in streams, must be processed. Some algorithms to extend the popular\nK-means method to the analysis of streaming data are present in literature\nsince 1998 (Bradley et al. in Scaling clustering algorithms to large databases.\nIn: KDD. p. 9-15, 1998; O'Callaghan et al. in Streaming-data algorithms for\nhigh-quality clustering. In: Proceedings of IEEE international conference on\ndata engineering. p. 685, 2001), based on the memorization and recursive update\nof a small number of summary statistics, but they either don't take into\naccount the specific variability of the clusters, or assume that the random\nvectors which are processed and grouped have uncorrelated components.\nUnfortunately this is not the case in many practical situations. We here\npropose a new algorithm to process data streams, with data having correlated\ncomponents and coming from clusters with different covariance matrices. Such\ncovariance matrices are estimated via an optimal double shrinkage method, which\nprovides positive definite estimates even in presence of a few data points, or\nof data having components with small variance. This is needed to invert the\nmatrices and compute the Mahalanobis distances that we use for the data\nassignment to the clusters. We also estimate the total number of clusters from\nthe data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 02:54:48 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 09:17:57 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Aletti", "Giacomo", ""], ["Micheletti", "Alessandra", ""]]}, {"id": "1707.01280", "submitter": "Maria Luisa Merani Prof.", "authors": "Maria Luisa Merani", "title": "Outage Probability of Power-based Non-Orthogonal Multiple Access (NOMA)\n  on the Uplink of a 5G Cell", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter puts forth an analytical approach to evaluate the outage\nprobability of power-based NOMA on the uplink of a 5G cell, the outage being\ndefined as the event where the receiver fails to successfully decode all the\nsimultaneously received signals. In the examined scenario, Successive\nInterference Cancellation (SIC) is considered and an arbitrary number of\nsuperimposed signals is present. For the Rayleigh fading case, the outage\nprobability is provided in closed-form, clearly outlining its dependency on the\nsignal-to-noise ratio of the users that are simultaneously transmitting, as\nwell as on their distance from the receiver.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 09:32:01 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Merani", "Maria Luisa", ""]]}, {"id": "1707.01287", "submitter": "R\\\"udiger Hewer", "authors": "R\\\"udiger Hewer, Petra Friederichs, Andreas Hense and Martin Schlather", "title": "A Matern based multivariate Gaussian random process for a consistent\n  model of the horizontal wind components and related variables", "comments": "23 pages, 6 figures Submitted to Journal of the Atmospheric Sciences", "journal-ref": null, "doi": "10.1175/JAS-D-16-0369.1", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of physical relationships into stochastic models is of major\ninterest e.g. in data assimilation. Here, a multivariate Gaussian random field\nformulation is introduced, which represents the differential relations of the\ntwo-dimensional wind field and related variables such as streamfunction,\nvelocity potential, vorticity and divergence. The covariance model is based on\na flexible bivariate Mat\\'ern covariance function for streamfunction and\nvelocity potential. It allows for different variances in the potentials,\nnon-zero correlations between them, anisotropy and a flexible smoothness\nparameter. The joint covariance function of the related variables is derived\nanalytically. Further, it is shown that a consistent model with non-zero\ncorrelations between the potentials and positive definite covariance function\nis possible. The statistical model is fitted to forecasts of the horizontal\nwind fields of a mesoscale numerical weather prediction system. Parameter\nuncertainty is assessed by a parametric bootstrap method. The estimates reveal\nonly physically negligible correlations between the potentials. In contrast to\nthe numerical estimator, the statistical estimator of the ratio between the\nvariances of the rotational and divergent wind components is unbiased.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 09:50:09 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Hewer", "R\u00fcdiger", ""], ["Friederichs", "Petra", ""], ["Hense", "Andreas", ""], ["Schlather", "Martin", ""]]}, {"id": "1707.01311", "submitter": "Sylvain Le", "authors": "Ngoc Minh Nguyen, Sylvain Le Corff, Eric Moulines", "title": "Particle rejuvenation of Rao-Blackwellized Sequential Monte Carlo\n  smoothers for Conditionally Linear and Gaussian models", "comments": null, "journal-ref": null, "doi": "10.1186/s13634-017-0489-5", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on Sequential Monte Carlo approximations of smoothing\ndistributions in conditionally linear and Gaussian state spaces. To reduce\nMonte Carlo variance of smoothers, it is typical in these models to use\nRao-Blackwellization: particle approximation is used to sample sequences of\nhidden regimes while the Gaussian states are explicitly integrated conditional\non the sequence of regimes and observations, using variants of the Kalman\nfilter / smoother. The first successful attempt to use Rao-Blackwellization for\nsmoothing extends the Bryson-Frazier smoother for Gaussian linear state space\nmodels using the generalized two-filter formula together with Kalman filters /\nsmoothers. More recently, a forward backward decomposition of smoothing\ndistributions mimicking the Rauch-Tung-Striebel smoother for the regimes\ncombined with backward Kalman updates has been introduced. This paper\ninvestigates the benefit of introducing additional rejuvenation steps in all\nthese algorithms to sample at each time instant new regimes conditional on the\nforward and backward particles. This defines particle based approximations of\nthe smoothing distributions whose support is not restricted to the set of\nparticles sampled in the forward or backward filter. These procedures are\napplied to commodity markets which are described using a two factor model based\non the spot price and a convenience yield for crude oil data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:46:26 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Nguyen", "Ngoc Minh", ""], ["Corff", "Sylvain Le", ""], ["Moulines", "Eric", ""]]}, {"id": "1707.01430", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini, Marica Manisera and Paola Zuccolotto", "title": "Sensor Analytics in Basketball", "comments": "14 pages, 3 figures, proceedings of the conference \"MathSport\n  International 2017\". arXiv admin note: text overlap with arXiv:1602.06994 by\n  other authors", "journal-ref": null, "doi": null, "report-no": "ISBN: 978-88-6938-058-7", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach in team sports analysis consists in studying positioning and\nmovements of players during the game in relation to team performance. State of\nthe art tracking systems produce spatio-temporal traces of players that have\nfacilitated a variety of research aimed to extract insights from trajectories.\nSeveral methods borrowed from machine learning, network and complex systems,\ngeographic information system, computer vision and statistics have been\nproposed. After having reviewed the state of the art in those niches of\nliterature aiming to extract useful information to analysts and experts in\nterms of relation between players' trajectories and team performance, this\npaper presents preliminary results from analysing trajectories data and sheds\nlight on potential future research in this field of study. In particular, using\nconvex hulls, we find interesting regularities in players' movement patterns.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 11:13:48 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Metulini", "Rodolfo", ""], ["Manisera", "Marica", ""], ["Zuccolotto", "Paola", ""]]}, {"id": "1707.01473", "submitter": "Jann Spiess", "authors": "Jens Ludwig, Sendhil Mullainathan, Jann Spiess", "title": "Machine-Learning Tests for Effects on Multiple Outcomes", "comments": "Minor update: new abstract, introduction, and section on representing\n  distribution differences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present tools for applied researchers that re-purpose\noff-the-shelf methods from the computer-science field of machine learning to\ncreate a \"discovery engine\" for data from randomized controlled trials (RCTs).\nThe applied problem we seek to solve is that economists invest vast resources\ninto carrying out RCTs, including the collection of a rich set of candidate\noutcome measures. But given concerns about inference in the presence of\nmultiple testing, economists usually wind up exploring just a small subset of\nthe hypotheses that the available data could be used to test. This prevents us\nfrom extracting as much information as possible from each RCT, which in turn\nimpairs our ability to develop new theories or strengthen the design of policy\ninterventions. Our proposed solution combines the basic intuition of reverse\nregression, where the dependent variable of interest now becomes treatment\nassignment itself, with methods from machine learning that use the data\nthemselves to flexibly identify whether there is any function of the outcomes\nthat predicts (or has signal about) treatment group status. This leads to\ncorrectly-sized tests with appropriate $p$-values, which also have the\nimportant virtue of being easy to implement in practice. One open challenge\nthat remains with our work is how to meaningfully interpret the signal that\nthese methods find.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:13:08 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 18:10:56 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ludwig", "Jens", ""], ["Mullainathan", "Sendhil", ""], ["Spiess", "Jann", ""]]}, {"id": "1707.01585", "submitter": "Daniel Fraiman", "authors": "Daniel Fraiman and Ricardo Fraiman", "title": "Statistical comparison of (brain) networks", "comments": "Three references added. A new paragraph was added in the\n  Resting-state fMRI functional networks section", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of random networks in a neuroscientific context has developed\nextensively over the last couple of decades. By contrast, techniques for the\nstatistical analysis of these networks are less developed. In this paper, we\nfocus on the statistical comparison of brain networks in a nonparametric\nframework and discuss the associated detection and identification problems. We\ntested network differences between groups with an analysis of variance (ANOVA)\ntest we developed specifically for networks. We also propose and analyse the\nbehaviour of a new statistical procedure designed to identify different\nsubnetworks. As an example, we show the application of this tool in\nresting-state fMRI data obtained from the Human Connectome Project. Finally, we\ndiscuss the potential bias in neuroimaging findings that is generated by some\nbehavioural and brain structure variables. Our method can also be applied to\nother kind of networks such as protein interaction networks, gene networks or\nsocial networks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 21:26:58 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 22:12:41 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Fraiman", "Daniel", ""], ["Fraiman", "Ricardo", ""]]}, {"id": "1707.01591", "submitter": "Arya Farahi", "authors": "Alex Chojnacki, Chengyu Dai, Arya Farahi, Guangsha Shi, Jared Webb,\n  Daniel T. Zhang, Jacob Abernethy, Eric Schwartz", "title": "A Data Science Approach to Understanding Residential Water Contamination\n  in Flint", "comments": "Applied Data Science track paper at KDD 2017. For associated\n  promotional video, see https://www.youtube.com/watch?v=0g66ImaV8Ag", "journal-ref": null, "doi": "10.1145/3097983.3098078", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the residents of Flint learned that lead had contaminated their water\nsystem, the local government made water-testing kits available to them free of\ncharge. The city government published the results of these tests, creating a\nvaluable dataset that is key to understanding the causes and extent of the lead\ncontamination event in Flint. This is the nation's largest dataset on lead in a\nmunicipal water system.\n  In this paper, we predict the lead contamination for each household's water\nsupply, and we study several related aspects of Flint's water troubles, many of\nwhich generalize well beyond this one city. For example, we show that elevated\nlead risks can be (weakly) predicted from observable home attributes. Then we\nexplore the factors associated with elevated lead. These risk assessments were\ndeveloped in part via a crowd sourced prediction challenge at the University of\nMichigan. To inform Flint residents of these assessments, they have been\nincorporated into a web and mobile application funded by \\texttt{Google.org}.\nWe also explore questions of self-selection in the residential testing program,\nexamining which factors are linked to when and how frequently residents\nvoluntarily sample their water.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 22:12:14 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Chojnacki", "Alex", ""], ["Dai", "Chengyu", ""], ["Farahi", "Arya", ""], ["Shi", "Guangsha", ""], ["Webb", "Jared", ""], ["Zhang", "Daniel T.", ""], ["Abernethy", "Jacob", ""], ["Schwartz", "Eric", ""]]}, {"id": "1707.01815", "submitter": "Amrei Stammann", "authors": "Amrei Stammann (Heinrich-Heine University Duesseldorf)", "title": "Fast and Feasible Estimation of Generalized Linear Models with\n  High-Dimensional k-way Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and memory efficient algorithm for the estimation of\ngeneralized linear models with an additive separable k-way error component. The\nbrute force approach uses dummy variables to account for the unobserved\nheterogeneity, but quickly faces computational limits. Thus, we show how a\nweighted version of the Frisch-Waugh-Lovell theorem combined with the method of\nalternating projections can be incorporated into a Newton-Raphson algorithm to\ndramatically reduce the computational costs. The algorithm is especially useful\nin situations, where generalized linear models with k-way fixed effects based\non dummy variables are computationally demanding or even infeasible due to time\nor memory limitations. In a simulation study and an empirical application we\ndemonstrate the performance of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:36:18 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 19:45:08 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 13:04:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Stammann", "Amrei", "", "Heinrich-Heine University Duesseldorf"]]}, {"id": "1707.01822", "submitter": "Bowen Li", "authors": "Bowen Li", "title": "Nonparametric Marginal Analysis of Recurrent Events Data under Competing\n  Risks", "comments": "PhD Dissertation, 73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project was motivated by a dialysis study in northern Taiwan. Dialysis\npatients, after shunt implantation, may experience two types (\"acute\" or\n\"non-acute\") of shunt thrombosis, both of which may recur. We formulate the\nproblem under the framework of recurrent events data in the presence of\ncompeting risks. In particular we focus on marginal inference for the gap time\nvariable of specific type. The functions of interest are the cumulative\nincidence function and cause-specific hazard function. The major challenge of\nnonparametric inference is the problem of induced dependent censoring. We apply\nthe technique of inverse probability of censoring weighting (IPCW) to adjust\nfor the selection bias. Besides point estimation, we apply the bootstrap\nre-sampling method for further inference. Large sample properties of the\nproposed estimators are derived. Simulations are performed to examine the\nfinite-sample performances of the proposed methods. Finally we apply the\nproposed methodology to analyze the dialysis data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:49:42 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Li", "Bowen", ""]]}, {"id": "1707.01838", "submitter": "Vincent Briane", "authors": "Vincent Briane, Charles Kervrann and Myriam Vimond", "title": "A statistical analysis of particle trajectories in living cells", "comments": "Revised introduction. A clearer and shorter description of the model\n  (section 2)", "journal-ref": "Phys. Rev. E 97, 062121 (2018)", "doi": "10.1103/PhysRevE.97.062121", "report-no": null, "categories": "stat.AP physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in molecular biology and fluorescence microscopy imaging have\nmade possible the inference of the dynamics of single molecules in living\ncells. Such inference allows to determine the organization and function of the\ncell. The trajectories of particles in the cells, computed with tracking\nalgorithms, can be modelled with diffusion processes. Three types of diffusion\nare considered : (i) free diffusion; (ii) subdiffusion or (iii) superdiffusion.\nThe Mean Square Displacement (MSD) is generally used to determine the different\ntypes of dynamics of the particles in living cells (Qian, Sheetz and Elson\n1991). We propose here a non-parametric three-decision test as an alternative\nto the MSD method. The rejection of the null hypothesis -- free diffusion -- is\naccompanied by claims of the direction of the alternative (subdiffusion or a\nsuperdiffusion). We study the asymptotic behaviour of the test statistic under\nthe null hypothesis, and under parametric alternatives which are currently\nconsidered in the biophysics literature, (Monnier et al,2012) for example. In\naddition, we adapt the procedure of Benjamini and Hochberg (2000) to fit with\nthe three-decision test setting, in order to apply the test procedure to a\ncollection of independent trajectories. The performance of our procedure is\nmuch better than the MSD method as confirmed by Monte Carlo experiments. The\nmethod is demonstrated on real data sets corresponding to protein dynamics\nobserved in fluorescence microscopy.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 15:44:26 GMT"}, {"version": "v2", "created": "Wed, 13 Sep 2017 17:57:01 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Briane", "Vincent", ""], ["Kervrann", "Charles", ""], ["Vimond", "Myriam", ""]]}, {"id": "1707.01855", "submitter": "Konstantinos Pelechrinis", "authors": "Konstantinos Pelechrinis", "title": "LinNet: Probabilistic Lineup Evaluation Through Network Embedding", "comments": "New England Symposium on Statistics in Sports (oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Which of your team's possible lineups has the best chances against each of\nyour opponents possible lineups? In order to answer this question we develop\nLinNet. LinNet exploits the dynamics of a directed network that captures the\nperformance of lineups at their matchups. The nodes of this network represent\nthe different lineups, while an edge from node j to node i exists if lineup i\nhas outperformed lineup j. We further annotate each edge with the corresponding\nperformance margin (point margin per minute). We then utilize this structure to\nlearn a set of latent features for each node (i.e., lineup) using the node2vec\nframework. Consequently, LinNet builds a model on this latent space for the\nprobability of lineup A beating lineup B. We evaluate LinNet using NBA lineup\ndata from the five seasons between 2007-08 and 2011-12. Our results indicate\nthat our method has an out-of-sample accuracy of 69%. In comparison, utilizing\nthe adjusted plus-minus of the players within a lineup for the same prediction\nproblem provides an accuracy of 56%. More importantly, the probabilities are\nwell-calibrated as shown by the probability validation curves. One of the\nbenefits of LinNet - apart from its accuracy - is that it is generic and can be\napplied in different sports since the only input required is the lineups'\nmatchup performances, i.e., not sport-specific features are needed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 16:32:48 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 13:40:48 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Pelechrinis", "Konstantinos", ""]]}, {"id": "1707.01861", "submitter": "Maricela Cruz", "authors": "Maricela Cruz, Miriam Bender, Hernando Ombao", "title": "A Robust Interrupted Time Series Model for Analyzing Complex Healthcare\n  Intervention Data", "comments": "37 pages and 6 figures", "journal-ref": null, "doi": "10.1002/sim.7443", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current health policy calls for greater use of evidence based care delivery\nservices to improve patient quality and safety outcomes. Care delivery is\ncomplex, with interacting and interdependent components that challenge\ntraditional statistical analytic techniques, in particular when modeling a time\nseries of outcomes data that might be \"interrupted\" by a change in a particular\nmethod of health care delivery. Interrupted time series (ITS) is a robust\nquasi-experimental design with the ability to infer the effectiveness of an\nintervention that accounts for data dependency. Current standardized methods\nfor analyzing ITS data do not model changes in variation and correlation\nfollowing the intervention. This is a key limitation since it is plausible for\ndata variability and dependency to change because of the intervention.\nMoreover, present methodology either assumes a pre-specified interruption time\npoint with an instantaneous effect or removes data for which the effect of\nintervention is not fully realized. In this paper, we describe and develop a\nnovel `Robust-ITS' model that overcomes these omissions and limitations. The\nRobust-ITS model formally performs inference on: (a) identifying the change\npoint; (b) differences in pre- and post-intervention correlation; (c)\ndifferences in the outcome variance pre- and post-intervention; and (d)\ndifferences in the mean pre- and post-intervention. We illustrate the proposed\nmethod by analyzing patient satisfaction data from a hospital that implemented\nand evaluated a new nursing care delivery model as the intervention of\ninterest. The Robust-ITS model is implemented in a R Shiny toolbox which is\nfreely available to the community.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 16:46:46 GMT"}, {"version": "v2", "created": "Tue, 11 Jul 2017 21:28:26 GMT"}, {"version": "v3", "created": "Mon, 25 Mar 2019 20:11:27 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Cruz", "Maricela", ""], ["Bender", "Miriam", ""], ["Ombao", "Hernando", ""]]}, {"id": "1707.02032", "submitter": "Javad Sovizi", "authors": "Javad Sovizi, Sonjoy Das and Venkat Krovi", "title": "Matrix-Based Characterization of the Motion and Wrench Uncertainties in\n  Robotic Manipulators", "comments": "15 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterization of the uncertainty in robotic manipulators is the focus of\nthis paper. Based on the random matrix theory (RMT), we propose uncertainty\ncharacterization schemes in which the uncertainty is modeled at the macro\n(system) level. This is different from the traditional approaches that model\nthe uncertainty in the parametric space of micro (state) level. We show that\nperturbing the system matrices rather than the state of the system provides\nunique advantages especially for robotic manipulators. First, it requires only\nlimited statistical information that becomes effective when dealing with\ncomplex systems where detailed information on their variability is not\navailable. Second, the RMT-based models are aware of the system state and\nconfiguration that are significant factors affecting the level of uncertainty\nin system behavior. In this study, in addition to the motion uncertainty\nanalysis that was first proposed in our earlier work, we also develop an\nRMT-based model for the quantification of the static wrench uncertainty in\nmulti-agent cooperative systems. This model is aimed to be an alternative to\nthe elaborate parametric formulation when only rough bounds are available on\nthe system parameters. We discuss that how RMT-based model becomes advantageous\nwhen the complexity of the system increases. We perform experimental studies on\na KUKA youBot arm to demonstrate the superiority of the RMT-based motion\nuncertainty models. We show that how these models outperform the traditional\nmodels built upon Gaussianity assumption in capturing real-system uncertainty\nand providing accurate bounds on the state estimation errors. In addition, to\nexperimentally support our wrench uncertainty quantification model, we study\nthe behavior of a cooperative system of mobile robots. It is shown that one can\nrely on less demanding RMT-based formulation and yet meets the acceptable\naccuracy.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 04:30:38 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Sovizi", "Javad", ""], ["Das", "Sonjoy", ""], ["Krovi", "Venkat", ""]]}, {"id": "1707.02048", "submitter": "Yu-Min Yen", "authors": "Yu-Min Yen and Tso-Jung Yen", "title": "Testing Forecast Accuracy of Expectiles and Quantiles with the Extremal\n  Consistent Loss Functions", "comments": "72 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecast evaluations aim to choose an accurate forecast for making decisions\nby using loss functions. However, different loss functions often generate\ndifferent ranking results for forecasts, which complicates the task of\ncomparisons. In this paper, we develop statistical tests for comparing\nperformances of forecasting expectiles and quantiles of a random variable under\nconsistent loss functions. The test statistics are constructed with the\nextremal consistent loss functions of Ehm et.al. (2016). The null hypothesis of\nthe tests is that a benchmark forecast at least performs equally well as a\ncompeting one under all extremal consistent loss functions. It can be shown\nthat if such a null holds, the benchmark will also perform at least equally\nwell as the competitor under all consistent loss functions. Thus under the\nnull, when different consistent loss functions are used, the result that the\ncompetitor does not outperform the benchmark will not be altered. We establish\nasymptotic properties of the proposed test statistics and propose to use the\nre-centered bootstrap to construct their empirical distributions. Through\nsimulations, we show the proposed test statistics perform reasonably well. We\nthen apply the proposed method on (1) re-examining abilities of some often-used\npredictors on forecasting risk premium of the S&P500 index; (2) comparing\nperformances of experts' forecasts on annual growth of U.S. real gross domestic\nproduct; (3) evaluating performances of estimated daily value at risk of the\nS&P500 index.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:10:50 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 03:43:49 GMT"}, {"version": "v3", "created": "Sun, 15 Jul 2018 15:51:09 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Yen", "Yu-Min", ""], ["Yen", "Tso-Jung", ""]]}, {"id": "1707.02502", "submitter": "Daniel Gerhard", "authors": "Daniel Gerhard, Christian Ritz", "title": "Marginalization in nonlinear mixed-effects models with an application to\n  dose-response analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in hierarchical nonlinear models needs careful consideration about\ntargeting parameters that have either a conditional or population-average\ninterpretation. For the special case of mixed-effects nonlinear sigmoidal\nmodels we propose a method for the estimation of derived parameters with a\nmarginal interpretation, but also maintaining the random effect structure of\nthe nonlinear model, by using a combination of numerical quadrature and the\ndelta method, integrating over the random effect distribution conditional on\nthe estimated variance components. The difference between these marginalized\nestimates, generalized nonlinear least squares estimates, and conditional\nestimation is characterised by means of two representative case studies. The\ncase studies consist of the estimation of effective dose levels in a human\ntoxicology study, and the relative potency estimation for two herbicides in an\nagricultural field trial. Both case studies exhibit an experimental design that\nresults in data with at least one hierarchical level of between- and\nwithin-cluster variation. A user-friendly software implementation is made\navailable with the R package medrc, providing an automated framework for\nmixed-effects dose-response modelling.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 23:13:34 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Gerhard", "Daniel", ""], ["Ritz", "Christian", ""]]}, {"id": "1707.02587", "submitter": "Wilson Ye Chen", "authors": "Wilson Ye Chen, Gareth W. Peters, Richard H. Gerlach, Scott A. Sisson", "title": "Dynamic Quantile Function Models", "comments": "MATLAB code: https://github.com/wilson-ye-chen/aqua", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for effectively summarising, modelling, and forecasting\nthe distributional characteristics of intra-daily returns, as well as the\nrecent work on forecasting histogram-valued time-series in the area of symbolic\ndata analysis, we develop a time-series model for forecasting\nquantile-function-valued (QF-valued) daily summaries for intra-daily returns.\nWe call this model the dynamic quantile function (DQF) model. Instead of a\nhistogram, we propose to use a $g$-and-$h$ quantile function to summarise the\ndistribution of intra-daily returns. We work with a Bayesian formulation of the\nDQF model in order to make statistical inference while accounting for parameter\nuncertainty; an efficient MCMC algorithm is developed for sampling-based\nposterior inference. Using ten international market indices and approximately\n2,000 days of out-of-sample data from each market, the performance of the DQF\nmodel compares favourably, in terms of forecasting VaR of intra-daily returns,\nagainst the interval-valued and histogram-valued time-series models.\nAdditionally, we demonstrate that the QF-valued forecasts can be used to\nforecast VaR measures at the daily timescale via a simple quantile regression\nmodel on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model\nis able to provide competitive VaR forecasts for daily returns.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 14:31:09 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 07:19:26 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 05:59:04 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 03:49:30 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 15:24:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Peters", "Gareth W.", ""], ["Gerlach", "Richard H.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1707.03012", "submitter": "Douglas De Rizzo Meneghetti", "authors": "Douglas De Rizzo Meneghetti and Plinio Thomaz Aquino Junior", "title": "Application and Simulation of Computerized Adaptive Tests Through the\n  Package catsim", "comments": "Reviewed version according to answer from the Journal of Statistical\n  Software. 21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents catsim, the first package written in the Python language\nspecialized in computerized adaptive tests and the logistical models of Item\nResponse Theory. catsim provides functions for generating item and examinee\nparameters, simulating tests and plotting results, as well as enabling end\nusers to create new procedures for proficiency initialization, item selection,\nproficiency estimation and test stopping criteria. The simulator keeps a record\nof the items selected for each examinee as well as their answers and also\nenables the simulation of linear tests, in which all examinees answer the same\nitems. The various components made available by catsim can also be used in the\ncreation of third-party testing applications. Examples of such usages are also\npresented in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 18:37:58 GMT"}, {"version": "v2", "created": "Fri, 20 Jul 2018 00:11:58 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Meneghetti", "Douglas De Rizzo", ""], ["Junior", "Plinio Thomaz Aquino", ""]]}, {"id": "1707.03047", "submitter": "Perry Williams", "authors": "Perry J. Williams, Mevin B. Hooten, Jamie N. Womble, George G.\n  Esslinger, Michael R. Bower", "title": "Monitoring dynamic spatio-temporal ecological processes optimally", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population dynamics varies in space and time. Survey designs that ignore\nthese dynamics may be inefficient and fail to capture essential spatio-temporal\nvariability of a process. Alternatively, dynamic survey designs explicitly\nincorporate knowledge of ecological processes, the associated uncertainty in\nthose processes, and can be optimized with respect to monitoring objectives. We\ndescribe a cohesive framework for monitoring a spreading population that\nexplicitly links animal movement models with survey design and monitoring\nobjectives. We apply the framework to develop an optimal survey design for sea\notters in Glacier Bay. Sea otters were first detected in Glacier Bay in 1988\nand have since increased in both abundance and distribution; abundance\nestimates increased from 5 otters to >5,000 otters, and they have spread faster\nthan 2.7 km per year. By explicitly linking animal movement models and survey\ndesign, we were able to reduce uncertainty associated with predicted occupancy,\nabundance, and distribution. The framework we describe is general, and we\noutline steps to applying it to novel systems and taxa.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 20:20:05 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Williams", "Perry J.", ""], ["Hooten", "Mevin B.", ""], ["Womble", "Jamie N.", ""], ["Esslinger", "George G.", ""], ["Bower", "Michael R.", ""]]}, {"id": "1707.03258", "submitter": "Daniel Ambach", "authors": "Daniel Ambach, Wolfgang Schmid", "title": "A New High-Dimensional Time Series Approach for Wind Speed, Wind\n  Direction and Air Pressure Forecasting", "comments": null, "journal-ref": "Energy, Volume 135, 15 September 2017, pp 833--850", "doi": "10.1016/j.energy.2017.06.137", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many wind speed forecasting approaches have been proposed in literature. In\nthis paper a new statistical approach for jointly predicting wind speed, wind\ndirection and air pressure is introduced. The wind direction and the air\npressure are important to extend the forecasting accuracy of wind speed\nforecasts. A good forecast for the wind direction helps to bring the turbine\ninto the predominant wind direction. We combine a multivariate seasonal time\nvarying threshold autoregressive model with interactions (TVARX) with a\nthreshold seasonal autoregressive conditional heteroscedastic (TARCHX) model.\nThe model includes periodicity, conditional heteroscedasticity, interactions of\ndifferent dependent variables and a complex autoregressive structure with\nnon-linear impacts. In contrast to ordinary likelihood estimation approaches,\nwe apply a high-dimensional shrinkage technique instead of a distributional\nassumption for the dependent variables. The iteratively re-weighted least\nabsolute shrinkage and selection operator (LASSO) method allows to capture\nconditional heteroscedasticity and a comparatively fast computing time. The\nproposed approach yields accurate predictions of wind speed, wind direction and\nair pressure for a short-term period. Prediction intervals up to twenty-four\nhours are presented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 13:16:31 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 09:05:38 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Ambach", "Daniel", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1707.03301", "submitter": "Zhiguang Huo", "authors": "Zhiguang Huo, Chi Song and George Tseng", "title": "Bayesian latent hierarchical model for transcriptomic meta-analysis to\n  detect biomarkers with clustered meta-patterns of differential expression\n  signals", "comments": "29 pages, 4 figures", "journal-ref": "Ann Appl Stat. 2019 Mar 13(1)", "doi": "10.1214/18-AOAS1188", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the rapid development of high-throughput experimental techniques and\nfast-dropping prices, many transcriptomic datasets have been generated and\naccumulated in the public domain. Meta-analysis combining multiple\ntranscriptomic studies can increase the statistical power to detect\ndisease-related biomarkers. In this paper, we introduce a Bayesian latent\nhierarchical model to perform transcriptomic meta-analysis. This method is\ncapable of detecting genes that are differentially expressed (DE) in only a\nsubset of the combined studies, and the latent variables help quantify\nhomogeneous and heterogeneous differential expression signals across studies. A\ntight clustering algorithm is applied to detected biomarkers to capture\ndifferential meta-patterns that are informative to guide further biological\ninvestigation. Simulations and three examples, including a microarray dataset\nfrom metabolism-related knockout mice, an RNA-seq dataset from HIV transgenic\nrats, and cross-platform datasets from human breast cancer, are used to\ndemonstrate the performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 14:30:48 GMT"}, {"version": "v2", "created": "Sun, 19 Aug 2018 18:59:16 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Huo", "Zhiguang", ""], ["Song", "Chi", ""], ["Tseng", "George", ""]]}, {"id": "1707.03307", "submitter": "Matteo Fasiolo", "authors": "M. Fasiolo, S. N. Wood, M. Zaffran, R. Nedellec and Y.Goude", "title": "Fast calibrated additive quantile regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1725521", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 14:47:43 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 10:44:08 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 17:48:43 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 10:18:31 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Fasiolo", "M.", ""], ["Wood", "S. N.", ""], ["Zaffran", "M.", ""], ["Nedellec", "R.", ""], ["Goude", "Y.", ""]]}, {"id": "1707.03462", "submitter": "Pallavi Basu", "authors": "Tao Feng, Pallavi Basu, Wenguang Sun, Hsun Teresa Ku, and Wendy J.\n  Mack", "title": "Optimal design for high-throughput screening via false discovery rate\n  control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-throughput screening (HTS) is a large-scale hierarchical process in\nwhich a large number of chemicals are tested in multiple stages. Conventional\nstatistical analyses of HTS studies often suffer from high testing error rates\nand soaring costs in large-scale settings. This article develops new\nmethodologies for false discovery rate control and optimal design in HTS\nstudies. We propose a two-stage procedure that determines the optimal numbers\nof replicates at different screening stages while simultaneously controlling\nthe false discovery rate in the confirmatory stage subject to a constraint on\nthe total budget. The merits of the proposed methods are illustrated using both\nsimulated and real data. We show that the proposed screening procedure\neffectively controls the error rate and the design leads to improved detection\npower. This is achieved at the expense of a limited budget.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 21:06:39 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Feng", "Tao", ""], ["Basu", "Pallavi", ""], ["Sun", "Wenguang", ""], ["Ku", "Hsun Teresa", ""], ["Mack", "Wendy J.", ""]]}, {"id": "1707.03469", "submitter": "Evgeny Burnaev", "authors": "Alexander Kuleshov, Alexander Bernstein, Evgeny Burnaev, Yury Yanovich", "title": "Machine Learning in Appearance-based Robot Self-localization", "comments": "7 pages, 3 figures, ICMLA 2017 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG cs.RO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An appearance-based robot self-localization problem is considered in the\nmachine learning framework. The appearance space is composed of all possible\nimages, which can be captured by a robot's visual system under all robot\nlocalizations. Using recent manifold learning and deep learning techniques, we\npropose a new geometrically motivated solution based on training data\nconsisting of a finite set of images captured in known locations of the robot.\nThe solution includes estimation of the robot localization mapping from the\nappearance space to the robot localization space, as well as estimation of the\ninverse mapping for modeling visual image features. The latter allows solving\nthe robot localization problem as the Kalman filtering problem.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 15:32:53 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 22:17:24 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Kuleshov", "Alexander", ""], ["Bernstein", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Yanovich", "Yury", ""]]}, {"id": "1707.03575", "submitter": "Marco Iglesias", "authors": "Marco Iglesias, Minho Park, M.V. Tretyakov", "title": "Bayesian inversion in resin transfer molding", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aad1cc", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian inverse problem of inferring the permeability of a\nporous medium within the context of a moving boundary framework motivated by\nResin Transfer Molding (RTM), one of the most commonly used processes for\nmanufacturing fiber-reinforced composite materials. During the injection of\nresin in RTM, our aim is to update our probabilistic knowledge of the per-\nmeability of the material by inverting pressure measurements as well as\nobservations of the resin moving domain. We consider both one-dimensional and\ntwo-dimensional forward models for RTM. Based on the analytical solution for\nthe one-dimensional case, we prove existence of the sequence of posteriors that\narise from a sequential Bayesian formulation within the infinite-dimensional\nframework. For the numerical characterisation of the Bayesian posteriors in the\none-dimensional case, we investigate the application of a fully-Bayesian\nSequential Monte Carlo method (SMC) for high-dimensional inverse problems. By\nmeans of SMC we construct a benchmark against which we compare performance of a\nnovel regularizing ensemble Kalman algorithm (REnKA) that we propose to\napproximate the posteriors in a computationally efficient manner under\npractical scenarios. We investigate the robustness of the proposed REnKA with\nrespect to tuneable param- eters and computational cost, and display advantages\nof REnKA compared with SMC with a small number of particles. We further apply\nREnKA to investigate, in both the one-dimensional and two-dimensional settings,\npractical aspects relevant to RTM which include the effect of pressure sensors\nconfiguration and the observational noise level in the uncertainty in the\nlog-permeability quantified via the sequence of Bayesian posteriors.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 07:29:48 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 15:44:38 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Iglesias", "Marco", ""], ["Park", "Minho", ""], ["Tretyakov", "M. V.", ""]]}, {"id": "1707.03593", "submitter": "Olivier Bouaziz", "authors": "G Nuel (UPMC, LPMA), Antoine Lefebvre (UPMC, LPMA), O Bouaziz (MAP5)", "title": "Computing Individual Risks based on Family History in Genetic Disease in\n  the Presence of Competing Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering a genetic disease with variable age at onset (ex: diabetes ,\nfamilial amyloid neuropathy, cancers, etc.), computing the individual risk of\nthe disease based on family history (FH) is of critical interest both for\nclinicians and patients. Such a risk is very challenging to compute because: 1)\nthe genotype X of the individual of interest is in general unknown; 2) the\nposterior distribution P(X|FH, T > t) changes with t (T is the age at disease\nonset for the targeted individual); 3) the competing risk of death is not\nnegligible. In this work, we present a modeling of this problem using a\nBayesian network mixed with (right-censored) survival outcomes where hazard\nrates only depend on the genotype of each individual. We explain how belief\npropagation can be used to obtain posterior distribution of genotypes given the\nFH, and how to obtain a time-dependent posterior hazard rate for any individual\nin the pedigree. Finally, we use this posterior hazard rate to compute\nindividual risk, with or without the competing risk of death. Our method is\nillustrated using the Claus-Easton model for breast cancer (BC). This model\nassumes an autosomal dominant genetic risk factor such as non-carriers\n(genotype 00) have a BC hazard rate $\\lambda$ 0 (t) while carriers (genotypes\n01, 10 and 11) have a (much greater) hazard rate $\\lambda$ 1 (t). Both hazard\nrates are assumed to be piecewise constant with known values (cuts at 20, 30,.\n.. , 80 years). The competing risk of death is derived from the national French\nregistry.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 08:23:48 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 08:19:02 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Nuel", "G", "", "UPMC, LPMA"], ["Lefebvre", "Antoine", "", "UPMC, LPMA"], ["Bouaziz", "O", "", "MAP5"]]}, {"id": "1707.03706", "submitter": "Lo\\\"ic Ferrer", "authors": "Lo\\\"ic Ferrer, Hein Putter and C\\'ecile Proust-Lima", "title": "Individual dynamic predictions using landmarking and joint modelling:\n  validation of estimators and robustness assessment", "comments": "A Web Appendix may be found in the source package of this article on\n  arXiv. Detailed examples of the code can be found at\n  \"https://github.com/LoicFerrer\" for practical use", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the diagnosis of a disease, one major objective is to predict\ncumulative probabilities of events such as clinical relapse or death from the\nindividual information collected up to a prediction time, including usually\nbiomarker repeated measurements. Several competing estimators have been\nproposed to calculate these individual dynamic predictions, mainly from two\napproaches: joint modelling and landmarking. These approaches differ by the\ninformation used, the model assumptions and the complexity of the computational\nprocedures. It is essential to properly validate the estimators derived from\njoint models and landmark models, quantify their variability and compare them\nin order to provide key elements for the development and use of individual\ndynamic predictions in clinical follow-up of patients. Motivated by the\nprediction of two competing causes of progression of prostate cancer from the\nhistory of prostate-specific antigen, we conducted an in-depth simulation study\nto validate and compare the dynamic predictions derived from these two methods.\nSpecifically, we formally defined the quantity to estimate and its estimators,\nproposed techniques to assess the uncertainty around predictions and validated\nthem. We also compared the individual dynamic predictions derived from joint\nmodels and landmark models in terms of prediction error, discriminatory power,\nefficiency and robustness to model assumptions. We show that these prediction\ntools should be handled with care, in particular by properly specifying models\nand estimators.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 13:25:58 GMT"}, {"version": "v2", "created": "Sat, 11 Nov 2017 10:31:33 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 12:17:14 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Ferrer", "Lo\u00efc", ""], ["Putter", "Hein", ""], ["Proust-Lima", "C\u00e9cile", ""]]}, {"id": "1707.03746", "submitter": "Mariusz Tarnopolski", "authors": "Mariusz Tarnopolski", "title": "Modeling the price of Bitcoin with geometric fractional Brownian motion:\n  a Monte Carlo approach", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.EC q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The long-term dependence of Bitcoin (BTC), manifesting itself through a Hurst\nexponent $H>0.5$, is exploited in order to predict future BTC/USD price. A\nMonte Carlo simulation with $10^4$ geometric fractional Brownian motion\nrealisations is performed as extensions of historical data. The accuracy of\nstatistical inferences is 10\\%. The most probable Bitcoin price at the\nbeginning of 2018 is 6358 USD.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 14:47:37 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 22:33:06 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 07:27:29 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Tarnopolski", "Mariusz", ""]]}, {"id": "1707.03905", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Artem Papanov", "title": "Influence of Resampling on Accuracy of Imbalanced Classification", "comments": "5 pages, 2 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE9875, 2015", "doi": "10.1117/12.2228523", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world binary classification tasks (e.g. detection of certain\nobjects from images), an available dataset is imbalanced, i.e., it has much\nless representatives of a one class (a minor class), than of another.\nGenerally, accurate prediction of the minor class is crucial but it's hard to\nachieve since there is not much information about the minor class. One approach\nto deal with this problem is to preliminarily resample the dataset, i.e., add\nnew elements to the dataset or remove existing ones. Resampling can be done in\nvarious ways which raises the problem of choosing the most appropriate one. In\nthis paper we experimentally investigate impact of resampling on classification\naccuracy, compare resampling methods and highlight key points and difficulties\nof resampling.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 20:55:22 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Papanov", "Artem", ""]]}, {"id": "1707.03909", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Pavel Erofeev, Dmitry Smolyakov", "title": "Model Selection for Anomaly Detection", "comments": "6 pages, 3 figures, Eighth International Conference on Machine Vision\n  (December 8, 2015)", "journal-ref": "Proc. SPIE 9875, 2015", "doi": "10.1117/12.2228794", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detection based on one-class classification algorithms is broadly\nused in many applied domains like image processing (e.g. detection of whether a\npatient is \"cancerous\" or \"healthy\" from mammography image), network intrusion\ndetection, etc. Performance of an anomaly detection algorithm crucially depends\non a kernel, used to measure similarity in a feature space. The standard\napproaches (e.g. cross-validation) for kernel selection, used in two-class\nclassification problems, can not be used directly due to the specific nature of\na data (absence of a second, abnormal, class data). In this paper we generalize\nseveral kernel selection methods from binary-class case to the case of\none-class classification and perform extensive comparison of these approaches\nusing both synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:03:36 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Erofeev", "Pavel", ""], ["Smolyakov", "Dmitry", ""]]}, {"id": "1707.03916", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Alexey Zaytsev", "title": "Large Scale Variable Fidelity Surrogate Modeling", "comments": "21 pages, 4 figures, Ann Math Artif Intell (2017)", "journal-ref": null, "doi": "10.1007/s10472-017-9545-y", "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineers widely use Gaussian process regression framework to construct\nsurrogate models aimed to replace computationally expensive physical models\nwhile exploring design space. Thanks to Gaussian process properties we can use\nboth samples generated by a high fidelity function (an expensive and accurate\nrepresentation of a physical phenomenon) and a low fidelity function (a cheap\nand coarse approximation of the same physical phenomenon) while constructing a\nsurrogate model. However, if samples sizes are more than few thousands of\npoints, computational costs of the Gaussian process regression become\nprohibitive both in case of learning and in case of prediction calculation. We\npropose two approaches to circumvent this computational burden: one approach is\nbased on the Nystr\\\"om approximation of sample covariance matrices and another\nis based on an intelligent usage of a blackbox that can evaluate a~low fidelity\nfunction on the fly at any point of a design space. We examine performance of\nthe proposed approaches using a number of artificial and real problems,\nincluding engineering optimization of a rotating disk shape.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:21:32 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Zaytsev", "Alexey", ""]]}, {"id": "1707.04171", "submitter": "Farzaneh Ghasemi Tahrir", "authors": "Farzaneh Ghasemi Tahrir", "title": "Modeling Hormesis Using a Non-Monotonic Copula Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a probabilistic method for capturing non-monotonic\nbehavior under the biphasic dose-response regime observed in many biological\nsystems experiencing different types of stress. The proposed method is based on\nthe rolling-pin method introduced earlier to estimate highly nonlinear and\nnon-monotonic joint probability distributions from continuous domain data. We\nshow that the proposed method outperforms the conventional parametric methods\nin terms of the error (namely RMSE) and it needs fewer parameters to be\nestimated a priori, while offering high flexibility. The application and\nperformance of the proposed method are shown through an example.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 15:21:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Tahrir", "Farzaneh Ghasemi", ""]]}, {"id": "1707.04400", "submitter": "Antonio Punzo", "authors": "Antonio Punzo", "title": "A new look at the inverse Gaussian distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse Gaussian (IG) is one of the most famous and considered\ndistributions with positive support. We propose a convenient mode-based\nparameterization yielding the reparametrized IG (rIG) distribution; it\nallows/simplifies the use of the IG distribution in various statistical fields,\nand we give some examples in nonparametric statistics, robust statistics, and\nmodel-based clustering. In nonparametric statistics, we define a smoother based\non rIG kernels. By construction, the estimator is well-defined and free of\nboundary bias. We adopt likelihood cross-validation to select the smoothing\nparameter. In robust statistics, we propose the contaminated IG distribution, a\nheavy-tailed generalization of the rIG distribution to accommodate mild\noutliers; they can be automatically detected by the model via maximum a\nposteriori probabilities. To obtain maximum likelihood estimates of the\nparameters, we illustrate an expectation-maximization (EM) algorithm. Finally,\nfor model-based clustering and semiparametric density estimation, we present\nfinite mixtures of rIG distributions. We use the EM algorithm to obtain ML\nestimates of the parameters of the mixture model. Applications to economic and\ninsurance data are finally illustrated to exemplify and enhance the use of the\nproposed models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:26:09 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Punzo", "Antonio", ""]]}, {"id": "1707.04531", "submitter": "Hari Om Aggrawal", "authors": "Hari Om Aggrawal, Martin Skovgaard Andersen, Sean Rose, Emil Y. Sidky", "title": "A Convex Reconstruction Model for X-ray Tomographic Imaging with\n  Uncertain Flat-fields", "comments": "Accepted at IEEE Transactions on Computational Imaging", "journal-ref": null, "doi": "10.1109/TCI.2017.2723246", "report-no": null, "categories": "math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical methods for X-ray computed tomography are based on the assumption\nthat the X-ray source intensity is known, but in practice, the intensity is\nmeasured and hence uncertain. Under normal operating conditions, when the\nexposure time is sufficiently high, this kind of uncertainty typically has a\nnegligible effect on the reconstruction quality. However, in time- or\ndose-limited applications such as dynamic CT, this uncertainty may cause severe\nand systematic artifacts known as ring artifacts. By carefully modeling the\nmeasurement process and by taking uncertainties into account, we derive a new\nconvex model that leads to improved reconstructions despite poor quality\nmeasurements. We demonstrate the effectiveness of the methodology based on\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 14:59:53 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Aggrawal", "Hari Om", ""], ["Andersen", "Martin Skovgaard", ""], ["Rose", "Sean", ""], ["Sidky", "Emil Y.", ""]]}, {"id": "1707.04612", "submitter": "Tobias M\\\"utze", "authors": "Tobias M\\\"utze, Ekkehard Glimm, Heinz Schmidli, and Tim Friede", "title": "Group sequential designs for negative binomial outcomes", "comments": null, "journal-ref": "Statistical Methods in Medical Research, 2018,\n  https://journals.sagepub.com/doi/abs/10.1177/0962280218773115", "doi": "10.1177/0962280218773115", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data and recurrent events in clinical trials, such as the number of\nlesions in magnetic resonance imaging in multiple sclerosis, the number of\nrelapses in multiple sclerosis, the number of hospitalizations in heart\nfailure, and the number of exacerbations in asthma or in chronic obstructive\npulmonary disease (COPD) are often modeled by negative binomial distributions.\nIn this manuscript we study planning and analyzing clinical trials with group\nsequential designs for negative binomial outcomes. We propose a group\nsequential testing procedure for negative binomial outcomes based on Wald\nstatistics using maximum likelihood estimators. The asymptotic distribution of\nthe proposed group sequential tests statistics are derived. The finite sample\nsize properties of the proposed group sequential test for negative binomial\noutcomes and the methods for planning the respective clinical trials are\nassessed in a simulation study. The simulation scenarios are motivated by\nclinical trials in chronic heart failure and relapsing multiple sclerosis,\nwhich cover a wide range of practically relevant settings. Our research assures\nthat the asymptotic normal theory of group sequential designs can be applied to\nnegative binomial outcomes when the hypotheses are tested using Wald statistics\nand maximum likelihood estimators. We also propose two methods, one based on\nStudent's t-distribution and one based on resampling, to improve type I error\nrate control in small samples. The statistical methods studied in this\nmanuscript are implemented in the R package \\textit{gscounts}, which is\navailable for download on the Comprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 19:12:00 GMT"}, {"version": "v2", "created": "Wed, 6 Mar 2019 18:51:01 GMT"}], "update_date": "2019-03-07", "authors_parsed": [["M\u00fctze", "Tobias", ""], ["Glimm", "Ekkehard", ""], ["Schmidli", "Heinz", ""], ["Friede", "Tim", ""]]}, {"id": "1707.04666", "submitter": "Kristian Lum", "authors": "Kristian Lum and Mike Baiocchi", "title": "The causal impact of bail on case outcomes for indigent defendants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use near-far matching, a technique for estimating causal relationships, to\nexplore whether bail causes a higher likelihood of conviction. We find evidence\nof a strong causal impact. This paper was compiled as a submission to the 2017\nFairness, Accountability, and Transparency in Machine Learning (FAT ML)\nworkshop.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 00:05:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Lum", "Kristian", ""], ["Baiocchi", "Mike", ""]]}, {"id": "1707.04703", "submitter": "Debasis Kundu Professor", "authors": "Arnab Koley, D. Kundu, Ayon Ganguly", "title": "Analysis of Type-II hybrid censored competing risks data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kundu and Gupta (2007, Metrika, 65, 159 - 170) provided the analysis of\nType-I hybrid censored competing risks data, when the lifetime distribution of\nthe competing causes of failures follow exponential distribution. In this paper\nwe consider the analysis of Type-II hybrid censored competing risks data. It is\nassumed that latent lifetime distributions of the competing causes of failures\nfollow independent exponential distributions with different scale parameters.\nIt is observed that the maximum likelihood estimators of the unknown parameters\ndo not always exist. We propose the modified estimators of the scale\nparameters, which coincide with the corresponding maximum likelihood estimators\nwhen they exist, and asymptotically they are equivalent. We obtain the exact\ndistribution of the proposed estimators. Using the exact distributions of the\nproposed estimators, associated confidence intervals are obtained. The\nasymptotic and bootstrap confidence intervals of the unknown parameters are\nalso provided. Further, Bayesian inference of some unknown parametric functions\nunder a very flexible Beta-Gamma prior is considered. Bayes estimators and\nassociated credible intervals of the unknown parameters are obtained using\nMonte Carlo method. Extensive Monte Carlo simulations are performed to see the\neffectiveness of the proposed estimators and one real data set has been\nanalyzed for the illustrative purposes. It is observed that the proposed model\nand the method work quite well for this data set.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 07:37:40 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Koley", "Arnab", ""], ["Kundu", "D.", ""], ["Ganguly", "Ayon", ""]]}, {"id": "1707.04705", "submitter": "Debasis Kundu Professor", "authors": "Debashis Samanta, Debasis Kundu, Ayon Ganguly", "title": "Order Restricted Bayesian Analysis of a Simple Step Stress Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider a simple step stress set up under the cumulative\nexposure model assumption. At each stress level the lifetime distribution of\nthe experimental units are assumed to follow the generalized exponential\ndistribution. We provide the order restricted Bayesian inference of the model\nparameters by considering the fact that the expected lifetime of the\nexperimental units are larger in lower stress level. Analysis and the related\nresults are extended to different censoring schemes also. The Bayes estimates\nand the associated credible intervals of the unknown parameters are constructed\nusing importance sampling technique. We perform extensive simulation\nexperiments both for the complete and censored samples to see the performances\nof the proposed estimators. We analyze two simulated and one real data sets for\nillustrative purposes. An optimal value of the stress changing time is obtained\nby minimizing the total posterior coefficient of variations of the unknown\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 07:53:37 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Samanta", "Debashis", ""], ["Kundu", "Debasis", ""], ["Ganguly", "Ayon", ""]]}, {"id": "1707.04737", "submitter": "Kostas Gemenis", "authors": "Bastiaan Bruinsma and Kostas Gemenis", "title": "Validating Wordscores", "comments": "78 pages including 8 appendices, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wordscores is a popular quantitative text scaling method to estimate parties'\npositions on a priori specified dimensions, without requiring the researchers\nto read or even understand the language in the documents they are analysing.\nThis study tries to establish whereas Wordscores is able to deliver this\npromise by conducting a rigorous validation of its output using the\nEuromanifestos of 164 parties across 23 countries. We assess content validity\nby looking at the scored words in their context, criterion validity by\ncomparing the Wordscores output to expert surveys and other judgemental\nestimates of party positions, and construct validity by using the Wordscores\nestimates to predict party membership in the European Parliament groups. We\nconclude that, despite the promises, Wordscores fails to deliver valid party\npositions, and outline three conditions under which its performance can be\nimproved.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 13:59:35 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Bruinsma", "Bastiaan", ""], ["Gemenis", "Kostas", ""]]}, {"id": "1707.04958", "submitter": "Jonathan Rubin", "authors": "Jonathan Rubin, Cristhian Potes, Minnan Xu-Wilson, Junzi Dong, Asif\n  Rahman, Hiep Nguyen, David Moromisato", "title": "An Ensemble Boosting Model for Predicting Transfer to the Pediatric\n  Intensive Care Unit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our work focuses on the problem of predicting the transfer of pediatric\npatients from the general ward of a hospital to the pediatric intensive care\nunit. Using data collected over 5.5 years from the electronic health records of\ntwo medical facilities, we develop classifiers based on adaptive boosting and\ngradient tree boosting. We further combine these learned classifiers into an\nensemble model and compare its performance to a modified pediatric early\nwarning score (PEWS) baseline that relies on expert defined guidelines. To\ngauge model generalizability, we perform an inter-facility evaluation where we\ntrain our algorithm on data from one facility and perform evaluation on a\nhidden test dataset from a separate facility. We show that improvements are\nwitnessed over the PEWS baseline in accuracy (0.77 vs. 0.69), sensitivity (0.80\nvs. 0.68), specificity (0.74 vs. 0.70) and AUROC (0.85 vs. 0.73).\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 23:01:35 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Rubin", "Jonathan", ""], ["Potes", "Cristhian", ""], ["Xu-Wilson", "Minnan", ""], ["Dong", "Junzi", ""], ["Rahman", "Asif", ""], ["Nguyen", "Hiep", ""], ["Moromisato", "David", ""]]}, {"id": "1707.05232", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen, Mohamed Hebiri and Joseph Salmon", "title": "On Lasso refitting strategies", "comments": "revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-know drawback of l_1-penalized estimators is the systematic shrinkage\nof the large coefficients towards zero. A simple remedy is to treat Lasso as a\nmodel-selection procedure and to perform a second refitting step on the\nselected support. In this work we formalize the notion of refitting and provide\noracle bounds for arbitrary refitting procedures of the Lasso solution. One of\nthe most widely used refitting techniques which is based on Least-Squares may\nbring a problem of interpretability, since the signs of the refitted estimator\nmight be flipped with respect to the original estimator. This problem arises\nfrom the fact that the Least-Squares refitting considers only the support of\nthe Lasso solution, avoiding any information about signs or amplitudes. To this\nend we define a sign consistent refitting as an arbitrary refitting procedure,\npreserving the signs of the first step Lasso solution and provide Oracle\ninequalities for such estimators. Finally, we consider special refitting\nstrategies: Bregman Lasso and Boosted Lasso. Bregman Lasso has a fruitful\nproperty to converge to the Sign-Least-Squares refitting (Least-Squares with\nsign constraints), which provides with greater interpretability. We\nadditionally study the Bregman Lasso refitting in the case of orthogonal\ndesign, providing with simple intuition behind the proposed method. Boosted\nLasso, in contrast, considers information about magnitudes of the first Lasso\nstep and allows to develop better oracle rates for prediction. Finally, we\nconduct an extensive numerical study to show advantages of one approach over\nothers in different synthetic and semi-real scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 15:28:29 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 00:33:16 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2018 18:24:11 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Hebiri", "Mohamed", ""], ["Salmon", "Joseph", ""]]}, {"id": "1707.05745", "submitter": "Herv\\'e Cardot", "authors": "Herve Cardot and Antonio Musolesi", "title": "Modeling temporal treatment effects with zero inflated semi-parametric\n  regression models: the case of local development policies in France", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-parametric approach is proposed to estimate the variation along time\nof the effects of two distinct public policies that were devoted to boost rural\ndevelopment in France over the same period of time. At a micro data level, it\nis often observed that the dependent variable, such as local employment, does\nnot vary along time, so that we face a kind of zero inflated phenomenon that\ncannot be dealt with a continuous response model. We introduce a mixture model\nwhich combines a mass at zero and a continuous response. The suggested zero\ninflated semi-parametric statistical approach relies on the flexibility and\nmodularity of additive models with the ability of panel data to deal with\nselection bias and to allow for the estimation of dynamic treatment effects. In\nthis multiple treatment analysis, we find evidence of interesting patterns of\ntemporal treatment effects with relevant nonlinear policy effects. The adopted\nsemi-parametric modeling also offers the possibility of making a counterfactual\nanalysis at an individual level. The methodology is illustrated and compared\nwith parametric linear approaches on a few municipalities for which the mean\nevolution of the potential outcomes is estimated under the different possible\ntreatments.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:09:56 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 13:08:46 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Cardot", "Herve", ""], ["Musolesi", "Antonio", ""]]}, {"id": "1707.05759", "submitter": "Daniel Gamermann Dr.", "authors": "Carmen Moret-Tatay, Daniel Gamermann, Esperanza Navarro-Pardo and\n  Pedro Fernandez de C\\'ordoba", "title": "ExGUtils: A python package for statistical analysis with the ex-gaussian\n  probability density", "comments": "18 pages, 4 figures, 5 Tables", "journal-ref": null, "doi": "10.3389/fpsyg.2018.00612", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of reaction times and their underlying cognitive processes is an\nimportant field in Psychology. Reaction times are usually modeled through the\nex-Gaussian distribution, because it provides a good fit to multiple empirical\ndata. The complexity of this distribution makes the use of computational tools\nan essential element in the field. Therefore, there is a strong need for\nefficient and versatile computational tools for the research in this area. In\nthis manuscript we discuss some mathematical details of the ex-Gaussian\ndistribution and apply the ExGUtils package, a set of functions and numerical\ntools, programmed for python, developed for numerical analysis of data\ninvolving the ex-Gaussian probability density. In order to validate the\npackage, we present an extensive analysis of fits obtained with it, discuss\nadvantages and differences between the least squares and maximum likelihood\nmethods and quantitatively evaluate the goodness of the obtained fits (which is\nusually an overlooked point in most literature in the area). The analysis done\nallows one to identify outliers in the empirical datasets and criteriously\ndetermine if there is a need for data trimming and at which points it should be\ndone.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 17:36:08 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Moret-Tatay", "Carmen", ""], ["Gamermann", "Daniel", ""], ["Navarro-Pardo", "Esperanza", ""], ["de C\u00f3rdoba", "Pedro Fernandez", ""]]}, {"id": "1707.05834", "submitter": "James Long", "authors": "James P. Long, Rafael S. de Souza", "title": "Statistical methods in astronomy", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ed-ph astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a review of data types and statistical methods often encountered\nin astronomy. The aim is to provide an introduction to statistical applications\nin astronomy for statisticians and computer scientists. We highlight the\ncomplex, often hierarchical, nature of many astronomy inference problems and\nadvocate for cross-disciplinary collaborations to address these challenges.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 22:17:20 GMT"}, {"version": "v2", "created": "Thu, 19 Oct 2017 21:27:17 GMT"}], "update_date": "2017-10-23", "authors_parsed": [["Long", "James P.", ""], ["de Souza", "Rafael S.", ""]]}, {"id": "1707.05870", "submitter": "Stone Chen", "authors": "Evan Kodra, Singdhansu Chatterjee, Stone Chen, Auroop R. Ganguly", "title": "Physics-guided probabilistic modeling of extreme precipitation under\n  climate change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earth System Models (ESMs) are the state of the art for projecting the\neffects of climate change. However, longstanding uncertainties in their ability\nto simulate regional and local precipitation extremes and related processes\ninhibit decision making. Stakeholders would be best supported by probabilistic\nprojections of changes in extreme precipitation at relevant space-time scales.\nHere we propose an empirical Bayesian model that extends an existing skill and\nconsensus based weighting framework and test the hypothesis that nontrivial,\nphysics-guided measures of ESM skill can help produce reliable probabilistic\ncharacterization of climate extremes. Specifically, the model leverages\nknowledge of physical relationships between temperature, atmospheric moisture\ncapacity, and extreme precipitation intensity to iteratively weight and combine\nESMs and estimate probability distributions of return levels. Out-of-sample\nvalidation shows evidence that the Bayesian model is a sound method for\nderiving reliable probabilistic projections. Beyond precipitation extremes, the\nframework may be a basis for a generic, physics-guided approach to modeling\nprobability distributions of climate variables in general, extremes or\notherwise.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:35:47 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Kodra", "Evan", ""], ["Chatterjee", "Singdhansu", ""], ["Chen", "Stone", ""], ["Ganguly", "Auroop R.", ""]]}, {"id": "1707.06075", "submitter": "Volker Schmid", "authors": "Behzad Mahaki, Yadollah Mehrabi, Amir Kavousi and Volker J Schmid", "title": "A Spatio-Temporal Multivariate Shared Component Model with an\n  Application in Iran Cancer Data", "comments": null, "journal-ref": "Asian Pacific Journal of Cancer Prevention 2018 (19), pp.\n  1553-1560", "doi": "10.22034/APJCP.2018.19.6.1553", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the proposals for joint disease mapping, the shared component model has\nbecome more popular. Another recent advance to strengthen inference of disease\ndata has been the extension of purely spatial models to include time and\nspace-time interaction. Such analyses have additional benefits over purely\nspatial models. However, only a few proposed spatio-temporal models could\naddress analysing multiple diseases jointly.\n  In the proposed model, each component is shared by different subsets of\ndiseases, spatial and temporal trends are considered for each component, and\nthe relative weight of these trends for each component for each relevant\ndisease can be estimated. We present an application of the proposed method on\nincidence rates of seven prevalent cancers in Iran. The effect of the shared\ncomponents on the individual cancer types can be identified. Regional and\ntemporal variation in relative risks is shown. We present a model which\ncombines the benefits of shared-components with spatio-temporal techniques for\nmultivariate data. We show, how the model allows to analyse geographical and\ntemporal variation among diseases beyond previous approaches.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 13:20:11 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Mahaki", "Behzad", ""], ["Mehrabi", "Yadollah", ""], ["Kavousi", "Amir", ""], ["Schmid", "Volker J", ""]]}, {"id": "1707.06107", "submitter": "Chris Oates", "authors": "Chris. J. Oates, Jon Cockayne, Robert G. Aykroyd, Mark Girolami", "title": "Bayesian Probabilistic Numerical Methods in Time-Dependent State\n  Estimation for Industrial Hydrocyclone Equipment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of high-power industrial equipment, such as large-scale mixing\nequipment or a hydrocyclone for separation of particles in liquid suspension,\ndemands careful monitoring to ensure correct operation. The fundamental task of\nstate-estimation for the liquid suspension can be posed as a time-evolving\ninverse problem and solved with Bayesian statistical methods. In this paper, we\nextend Bayesian methods to incorporate statistical models for the error that is\nincurred in the numerical solution of the physical governing equations. This\nenables full uncertainty quantification within a principled\ncomputation-precision trade-off, in contrast to the over-confident inferences\nthat are obtained when all sources of numerical error are ignored. The method\nis cast within a sequential Monte Carlo framework and an optimised\nimplementation is provided in Python.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 14:12:11 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 11:41:32 GMT"}], "update_date": "2018-12-20", "authors_parsed": [["Oates", "Chris. J.", ""], ["Cockayne", "Jon", ""], ["Aykroyd", "Robert G.", ""], ["Girolami", "Mark", ""]]}, {"id": "1707.06227", "submitter": "Paul Sheridan", "authors": "Mikael Onsj\\\"o and Paul Sheridan", "title": "Theme Enrichment Analysis: A Statistical Test for Identifying\n  Significantly Enriched Themes in a List of Stories with an Application to the\n  Star Trek Television Franchise", "comments": "19 pages, 3 figures, 4 tables, 2 supplementary material; minor\n  revision", "journal-ref": "Digital Studies / Le champ num\\'erique 10 (1), (2020) 1", "doi": "10.16995/dscn.316", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we describe how the hypergeometric test can be used to\ndetermine whether a given theme of interest occurs in a storyset at a frequency\nmore than would be expected by chance. By a storyset we mean simply a list of\nstories defined according to a common attribute (e.g., author, movement,\nperiod). The test works roughly as follows: Given a background storyset and a\nsub-storyset of interest, the test determines whether a given theme is\nover-represented in the sub-storyset, based on comparing the proportions of\nstories in the sub-storyset and background storyset featuring the theme. A\nstoryset is said to be \"enriched\" for a theme with respect to a particular\nbackground storyset, when the theme is identified as being significantly\nover-represented by the test. Furthermore, we introduce here a toy dataset\nconsisting of 280 manually themed Star Trek television franchise episodes. As a\nproof of concept, we use the hypergeometric test to analyze the Star Trek\nstories for enriched themes. The hypergeometric testing approach to theme\nenrichment analysis is implemented for the Star Trek thematic dataset in the R\npackage stoRy. A related R Shiny web application can be found at\nhttps://github.com/theme-ontology/shiny-apps.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:39:56 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 21:49:35 GMT"}, {"version": "v3", "created": "Tue, 14 Aug 2018 23:04:50 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Onsj\u00f6", "Mikael", ""], ["Sheridan", "Paul", ""]]}, {"id": "1707.06452", "submitter": "Keiichi Fukaya", "authors": "Ai Kawamori, Keiichi Fukaya, Masumi Kitazawa, Makio Ishiguro", "title": "A self-excited threshold autoregressive state-space model for menstrual\n  cycles: forecasting menstruation and identifying ovarian phases based on\n  basal body temperature", "comments": "25 pages, 3 figures", "journal-ref": "Statistics in Medicine 38 (2019) 2157-2170", "doi": "10.1002/sim.8096", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The menstrual cycle is composed of the follicular phase and subsequent luteal\nphase based on events occurring in the ovary. Basal body temperature (BBT)\nreflects this biphasic aspect of menstrual cycle and tends to be relatively low\nduring the follicular phase. In the present study, we proposed a state-space\nmodel that explicitly incorporates the biphasic nature of the menstrual cycle,\nin which the probability density distributions for the advancement of the\nmenstrual phase and that for BBT switch depend on a latent state variable. Our\nmodel derives the predictive distribution of the day of the next menstruation\nonset that is adaptively adjusted by accommodating new observations of BBT\nsequentially. It also enables us to obtain conditional probabilities of the\nwoman being in the early or late stages of the cycle, which can be used to\nidentify the duration of follicular and luteal phases, as well as to estimate\nthe day of ovulation. By applying the model to real BBT and menstruation data,\nwe show that the proposed model can properly capture the biphasic\ncharacteristics of menstrual cycles, providing a good prediction of the\nmenstruation onset in a wide range of age groups. An application to a large\ndata set containing 25,622 cycles provided by 3,533 woman subjects further\nhighlighted the between-age differences in the population characteristics of\nmenstrual cycles, suggesting wide applicability of the proposed model.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 11:42:25 GMT"}, {"version": "v2", "created": "Wed, 26 Jul 2017 02:17:31 GMT"}, {"version": "v3", "created": "Fri, 8 Sep 2017 02:03:07 GMT"}], "update_date": "2019-04-25", "authors_parsed": [["Kawamori", "Ai", ""], ["Fukaya", "Keiichi", ""], ["Kitazawa", "Masumi", ""], ["Ishiguro", "Makio", ""]]}, {"id": "1707.06497", "submitter": "S\\'andor Kolumb\\'an", "authors": "S\\'andor Kolumb\\'an, Stella Kapodistria, Nazanin Nooraee", "title": "Short and long-term wind turbine power output prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the wind energy industry, it is of great importance to develop models that\naccurately forecast the power output of a wind turbine, as such predictions are\nused for wind farm location assessment or power pricing and bidding,\nmonitoring, and preventive maintenance. As a first step, and following the\nguidelines of the existing literature, we use the supervisory control and data\nacquisition (SCADA) data to model the wind turbine power curve (WTPC). We\nexplore various parametric and non-parametric approaches for the modeling of\nthe WTPC, such as parametric logistic functions, and non-parametric piecewise\nlinear, polynomial, or cubic spline interpolation functions. We demonstrate\nthat all aforementioned classes of models are rich enough (with respect to\ntheir relative complexity) to accurately model the WTPC, as their mean squared\nerror (MSE) is close to the MSE lower bound calculated from the historical\ndata. We further enhance the accuracy of our proposed model, by incorporating\nadditional environmental factors that affect the power output, such as the\nambient temperature, and the wind direction. However, all aforementioned\nmodels, when it comes to forecasting, seem to have an intrinsic limitation, due\nto their inability to capture the inherent auto-correlation of the data. To\navoid this conundrum, we show that adding a properly scaled ARMA modeling layer\nincreases short-term prediction performance, while keeping the long-term\nprediction capability of the model.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 21:30:38 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kolumb\u00e1n", "S\u00e1ndor", ""], ["Kapodistria", "Stella", ""], ["Nooraee", "Nazanin", ""]]}, {"id": "1707.06635", "submitter": "Marcel Ausloos", "authors": "ShiXue He and Marcel Ausloos", "title": "Impact of the Global Crisis on SME Internal vs. External Financing in\n  China", "comments": "17 pages, 6 tables, 43 references", "journal-ref": "Banking and Finance Review, 9(1) (2017) 1-17", "doi": null, "report-no": null, "categories": "q-fin.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Changes in the capital structure before and after the global financial crisis\nfor SMEs are studied, emphasizing their financing problems, distinguishing\nbetween internal financing and external financing determinants. The empirical\nresearch bears upon 158 small and medium-sized firms listed on Shenzhen and\nShanghai Stock Exchanges in China over the period of 2004-2014. A regression\nanalysis, along the lines of the Trade-Off Theory, shows that the leverage\ndecreases with profitability, non-debt tax shields and the liquidity, and\nincreases with firm size and tangibility. A positive relationship is found\nbetween firm growth and debt ratio, though not highly significantly. It is\nshown that the SMEs with high growth rates are those which will more easily\nobtain external financing after a financial crisis. It is recognized that the\nChina government should reconsider SMEs taxation laws.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 10:05:08 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["He", "ShiXue", ""], ["Ausloos", "Marcel", ""]]}, {"id": "1707.06842", "submitter": "Simon Michael Papalexiou Ph.D", "authors": "Simon Michael Papalexiou", "title": "A unified theory for exact stochastic modelling of univariate and\n  multivariate processes with continuous, mixed type, or discrete marginal\n  distributions and any correlation structure", "comments": "46 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydroclimatic processes are characterized by heterogeneous spatiotemporal\ncorrelation structures and marginal distributions that can be continuous,\nmixed-type, discrete or even binary. Simulating exactly such processes can\ngreatly improve hydrological analysis and design. Yet this challenging task is\naccomplished often by ad hoc and approximate methodologies that are devised for\nspecific variables and purposes. In this study, a single framework is proposed\nallowing the exact simulation of processes with any marginal and any\ncorrelation structure. We unify, extent, and improve of a general-purpose\nmodelling strategy based on the assumption that any process can emerge by\ntransforming a parent Gaussian process with a specific correlation structure. A\nnovel mathematical representation of the parent-Gaussian scheme provides a\nconsistent and fully general description that supersedes previous specific\nparameterizations, resulting in a simple, fast and efficient simulation\nprocedure for every spatiotemporal process. In particular, introducing a simple\nbut flexible procedure we obtain a parametric expression of the correlation\ntransformation function, allowing to assess the correlation structure of the\nparent-Gaussian process that yields the prescribed correlation of the target\nprocess after marginal back transformation. The same framework is also\napplicable for cyclostationary and multivariate modelling. The simulation of a\nvariety of hydroclimatic variables with very different correlation structures\nand marginals, such as precipitation, stream flow, wind speed, humidity,\nextreme events per year, etc., as well as a multivariate application,\nhighlights the flexibility, advantages, and complete generality of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:11:03 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Papalexiou", "Simon Michael", ""]]}, {"id": "1707.07066", "submitter": "Hayafumi Watanabe", "authors": "Hayafumi Watanabe", "title": "Ultraslow diffusion in language: Dynamics of appearance of already\n  popular adjectives on Japanese blogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.CL cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What dynamics govern a time series representing the appearance of words in\nsocial media data? In this paper, we investigate an elementary dynamics, from\nwhich word-dependent special effects are segregated, such as breaking news,\nincreasing (or decreasing) concerns, or seasonality. To elucidate this problem,\nwe investigated approximately three billion Japanese blog articles over a\nperiod of six years, and analysed some corresponding solvable mathematical\nmodels. From the analysis, we found that a word appearance can be explained by\nthe random diffusion model based on the power-law forgetting process, which is\na type of long memory point process related to ARFIMA(0,0.5,0). In particular,\nwe confirmed that ultraslow diffusion (where the mean squared displacement\ngrows logarithmically), which the model predicts in an approximate manner,\nreproduces the actual data. In addition, we also show that the model can\nreproduce other statistical properties of a time series: (i) the fluctuation\nscaling, (ii) spectrum density, and (iii) shapes of the probability density\nfunctions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 23:13:50 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 05:05:33 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 05:39:01 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Watanabe", "Hayafumi", ""]]}, {"id": "1707.07275", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Likelihood test in permutations with bias. Premier League and La Liga:\n  surprises during the last 25 seasons", "comments": "Bibliography updated. Thanks to Prof Karlsson to have suggested the\n  paper [8] H. Stern. Models for distributions on permutations. JASA (1990)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the models of permutations with bias, which are\nrandom permutations of a set, biased by some preference values. We present a\nnew parametric test, together with an efficient way to calculate its p-value.\nThe final tables of the English and Spanish major soccer leagues are tested\naccording to this new procedure, to discover whether these results were aligned\nwith expectations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 10:17:40 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 10:23:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1707.07379", "submitter": "Feras El Zarwi", "authors": "Feras El Zarwi, Akshay Vij, Joan Walker", "title": "A Discrete Choice Framework for Modeling and Forecasting The Adoption\n  and Diffusion of New Transportation Services", "comments": null, "journal-ref": "Transportation Research Part C 79 (2017) 207 - 223", "doi": "10.1016/j.trc.2017.03.004", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current travel demand models are unable to predict long-range trends in\ntravel behavior as they do not entail a mechanism that projects membership and\nmarket share of new modes of transport (Uber, Lyft, etc). We propose\nintegrating discrete choice and technology adoption models to address the\naforementioned issue. In order to do so, we build on the formulation of\ndiscrete mixture models and specifically Latent Class Choice Models (LCCMs),\nwhich were integrated with a network effect model. The network effect model\nquantifies the impact of the spatial/network effect of the new technology on\nthe utility of adoption. We adopted a confirmatory approach to estimating our\ndynamic LCCM based on findings from the technology diffusion literature that\nfocus on defining two distinct types of adopters: innovator/early adopters and\nimitators. LCCMs allow for heterogeneity in the utility of adoption for the\nvarious market segments i.e. innovators/early adopters, imitators and\nnon-adopters. We make use of revealed preference (RP) time series data from a\none-way carsharing system in a major city in the United States to estimate\nmodel parameters. The data entails a complete set of member enrollment for the\ncarsharing service for a time period of 2.5 years after being launched.\nConsistent with the technology diffusion literature, our model identifies three\nlatent classes whose utility of adoption have a well-defined set of preferences\nthat are significant and behaviorally consistent. The technology adoption model\npredicts the probability that a certain individual will adopt the service at a\ncertain time period, and is explained by social influences, network effect,\nsocio-demographics and level-of-service attributes. Finally, the model was\ncalibrated and then used to forecast adoption of the carsharing system for\npotential investment strategy scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 02:18:13 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Zarwi", "Feras El", ""], ["Vij", "Akshay", ""], ["Walker", "Joan", ""]]}, {"id": "1707.07620", "submitter": "Saptarshi Das", "authors": "Shre Kumar Chatterjee, Saptarshi Das, Koushik Maharatna, Elisa Masi,\n  Luisa Santopolo, Ilaria Colzi, Stefano Mancuso and Andrea Vitaletti", "title": "Comparison of Decision Tree Based Classification Strategies to Detect\n  External Chemical Stimuli from Raw and Filtered Plant Electrical Response", "comments": null, "journal-ref": "Sensors and Actuators B: Chemical, vol. 249, pp. 278-295, Oct.\n  2017", "doi": "10.1016/j.snb.2017.04.071", "report-no": null, "categories": "physics.bio-ph cs.LG physics.data-an stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plants monitor their surrounding environment and control their physiological\nfunctions by producing an electrical response. We recorded electrical signals\nfrom different plants by exposing them to Sodium Chloride (NaCl), Ozone (O3)\nand Sulfuric Acid (H2SO4) under laboratory conditions. After applying\npre-processing techniques such as filtering and drift removal, we extracted few\nstatistical features from the acquired plant electrical signals. Using these\nfeatures, combined with different classification algorithms, we used a decision\ntree based multi-class classification strategy to identify the three different\nexternal chemical stimuli. We here present our exploration to obtain the\noptimum set of ranked feature and classifier combination that can separate a\nparticular chemical stimulus from the incoming stream of plant electrical\nsignals. The paper also reports an exhaustive comparison of similar feature\nbased classification using the filtered and the raw plant signals, containing\nthe high frequency stochastic part and also the low frequency trends present in\nit, as two different cases for feature extraction. The work, presented in this\npaper opens up new possibilities for using plant electrical signals to monitor\nand detect other environmental stimuli apart from NaCl, O3 and H2SO4 in future.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 19:00:14 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Chatterjee", "Shre Kumar", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""], ["Masi", "Elisa", ""], ["Santopolo", "Luisa", ""], ["Colzi", "Ilaria", ""], ["Mancuso", "Stefano", ""], ["Vitaletti", "Andrea", ""]]}, {"id": "1707.07885", "submitter": "Harris Georgiou", "authors": "Harris V. Georgiou", "title": "Wind models and cross-site interpolation for the refugee reception\n  islands in Greece", "comments": "23 figures, 3 tables, 17 references", "journal-ref": null, "doi": null, "report-no": "HG/DA.0725.01v1", "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this study, the wind data series from five locations in Aegean Sea\nislands, the most active `hotspots' in terms of refugee influx during the\nOct/2015 - Jan/2016 period, are investigated. The analysis of the\nthree-per-site data series includes standard statistical analysis and\nparametric distributions, auto-correlation analysis, cross-correlation analysis\nbetween the sites, as well as various ARMA models for estimating the\nfeasibility and accuracy of such spatio-temporal linear regressors for\npredictive analytics. Strong correlations are detected across specific sites\nand appropriately trained ARMA(7,5) models achieve 1-day look-ahead error\n(RMSE) of less than 1.9 km/h on average wind speed. The results show that such\ndata-driven statistical approaches are extremely useful in identifying\nunexpected and sometimes counter-intuitive associations between the available\nspatial data nodes, which is very important when designing corresponding models\nfor short-term forecasting of sea condition, especially average wave height and\ndirection, which is in fact what defines the associated weather risk of\ncrossing these passages in refugee influx patterns.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 09:48:44 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Georgiou", "Harris V.", ""]]}, {"id": "1707.07903", "submitter": "Y\\^uki Kubo", "authors": "Y\\^uki Kubo, Mitsue Den and Mamoru Ishii", "title": "Verification of operational solar flare forecast: Case of Regional\n  Warning Center Japan", "comments": "29 pages, 7 figures and 6 tables. Accepted for publication in Journal\n  of Space Weather and Space Climate (SWSC)", "journal-ref": "Journal of Space Weather and Space Climate, 7, A20 (2017)", "doi": "10.1051/swsc/2017018", "report-no": null, "categories": "astro-ph.SR physics.data-an physics.space-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss a verification study of an operational solar\nflare forecast in the Regional Warning Center (RWC) Japan. The RWC Japan has\nbeen issuing four-categorical deterministic solar flare forecasts for a long\ntime. In this forecast verification study, we used solar flare forecast data\naccumulated over 16 years (from 2000 to 2015). We compiled the forecast data\ntogether with solar flare data obtained with the Geostationary Operational\nEnvironmental Satellites (GOES). Using the compiled data sets, we estimated\nsome conventional scalar verification measures with 95% confidence intervals.\nWe also estimated a multi-categorical scalar verification measure. These scalar\nverification measures were compared with those obtained by the persistence\nmethod and recurrence method. As solar activity varied during the 16 years, we\nalso applied verification analyses to four subsets of forecast-observation pair\ndata with different solar activity levels. We cannot conclude definitely that\nthere are significant performance difference between the forecasts of RWC Japan\nand the persistence method, although a slightly significant difference is found\nfor some event definitions. We propose to use a scalar verification measure to\nassess the judgment skill of the operational solar flare forecast. Finally, we\npropose a verification strategy for deterministic operational solar flare\nforecasting.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 10:36:02 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Kubo", "Y\u00fbki", ""], ["Den", "Mitsue", ""], ["Ishii", "Mamoru", ""]]}, {"id": "1707.07971", "submitter": "Sophie Donnet", "authors": "Sophie Donnet (1), St\\'ephane Robin (1) ((1) MIA-Paris)", "title": "Using deterministic approximations to accelerate SMC for posterior\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo has become a standard tool for Bayesian Inference of\ncomplex models. This approach can be computationally demanding, especially when\ninitialized from the prior distribution. On the other hand, deter-ministic\napproximations of the posterior distribution are often available with no\ntheoretical guaranties. We propose a bridge sampling scheme starting from such\na deterministic approximation of the posterior distribution and targeting the\ntrue one. The resulting Shortened Bridge Sampler (SBS) relies on a sequence of\ndistributions that is determined in an adaptive way. We illustrate the\nrobustness and the efficiency of the methodology on a large simulation study.\nWhen applied to network datasets, SBS inference leads to different statistical\nconclusions from the one supplied by the standard variational Bayes\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 13:04:51 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Donnet", "Sophie", "", "MIA-Paris"], ["Robin", "St\u00e9phane", "", "MIA-Paris"]]}, {"id": "1707.08015", "submitter": "Benjamin Bullough", "authors": "Benjamin L. Bullough, Anna K. Yanchenko, Christopher L. Smith, Joseph\n  R. Zipkin", "title": "Predicting Exploitation of Disclosed Software Vulnerabilities Using\n  Open-source Data", "comments": null, "journal-ref": "In Proceedings of the 3rd ACM on International Workshop on\n  Security And Privacy Analytics (IWSPA 2017). ACM, New York, NY, USA, 45-53", "doi": "10.1145/3041008.3041009", "report-no": null, "categories": "cs.CR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, thousands of software vulnerabilities are discovered and reported\nto the public. Unpatched known vulnerabilities are a significant security risk.\nIt is imperative that software vendors quickly provide patches once\nvulnerabilities are known and users quickly install those patches as soon as\nthey are available. However, most vulnerabilities are never actually exploited.\nSince writing, testing, and installing software patches can involve\nconsiderable resources, it would be desirable to prioritize the remediation of\nvulnerabilities that are likely to be exploited. Several published research\nstudies have reported moderate success in applying machine learning techniques\nto the task of predicting whether a vulnerability will be exploited. These\napproaches typically use features derived from vulnerability databases (such as\nthe summary text describing the vulnerability) or social media posts that\nmention the vulnerability by name. However, these prior studies share multiple\nmethodological shortcomings that inflate predictive power of these approaches.\nWe replicate key portions of the prior work, compare their approaches, and show\nhow selection of training and test data critically affect the estimated\nperformance of predictive models. The results of this study point to important\nmethodological considerations that should be taken into account so that results\nreflect real-world utility.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 14:40:18 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Bullough", "Benjamin L.", ""], ["Yanchenko", "Anna K.", ""], ["Smith", "Christopher L.", ""], ["Zipkin", "Joseph R.", ""]]}, {"id": "1707.08152", "submitter": "Phillip M. Alday", "authors": "Phillip M. Alday", "title": "How much baseline correction do we need in ERP research? Extended GLM\n  model can replace baseline correction while lifting its limits", "comments": null, "journal-ref": null, "doi": "10.1111/psyp.13451", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Baseline correction plays an important role in past and current\nmethodological debates in ERP research (e.g. the Tanner v. Maess debate in\nJournal of Neuroscience Methods), serving as a potential alternative to strong\nhighpass filtering. However, the very assumptions that underlie traditional\nbaseline also undermine it, making it statistically unnecessary and even\nundesirable and reducing signal-to-noise ratio. Including the baseline interval\nas a predictor in a GLM-based statistical approach allows the data to determine\nhow much baseline correction is needed, including both full traditional and no\nbaseline correction as subcases, while reducing the amount of variance in the\nresidual error term and thus potentially increasing statistical power.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:48:49 GMT"}, {"version": "v2", "created": "Thu, 6 Sep 2018 11:52:18 GMT"}, {"version": "v3", "created": "Mon, 18 Mar 2019 13:06:23 GMT"}, {"version": "v4", "created": "Fri, 7 Jun 2019 15:25:37 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Alday", "Phillip M.", ""]]}, {"id": "1707.08220", "submitter": "Yajuan Si", "authors": "Yajuan Si, Rob Trangucci, Jonah Sol Gabry and Andrew Gelman", "title": "Bayesian hierarchical weighting adjustment and survey inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Bayesian prediction and weighted inference as a unified approach\nto survey inference. The general principles of Bayesian analysis imply that\nmodels for survey outcomes should be conditional on all variables that affect\nthe probability of inclusion. We incorporate the weighting variables under the\nframework of multilevel regression and poststratification, as a byproduct\ngenerating model-based weights after smoothing. We investigate deep\ninteractions and introduce structured prior distributions for smoothing and\nstability of estimates. The computation is done via Stan and implemented in the\nopen source R package \"rstanarm\" ready for public use. Simulation studies\nillustrate that model-based prediction and weighting inference outperform\nclassical weighting. We apply the proposal to the New York Longitudinal Study\nof Wellbeing. The new approach generates robust weights and increases\nefficiency for finite population inference, especially for subsets of the\npopulation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 21:02:26 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 15:51:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Trangucci", "Rob", ""], ["Gabry", "Jonah Sol", ""], ["Gelman", "Andrew", ""]]}, {"id": "1707.08354", "submitter": "Mohamad Elmasri", "authors": "Mohamad Elmasri, Maxwell J. Farrell, T. Jonathan Davies and David A.\n  Stephens", "title": "A hierarchical Bayesian model for predicting ecological interactions\n  using scaled evolutionary relationships", "comments": "To appear in the Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Identifying undocumented or potential future interactions among species is a\nchallenge facing modern ecologists. Recent link prediction methods rely on\ntrait data, however large species interaction databases are typically sparse\nand covariates are limited to only a fraction of species. On the other hand,\nevolutionary relationships, encoded as phylogenetic trees, can act as proxies\nfor underlying traits and historical patterns of parasite sharing among hosts.\nWe show that using a network-based conditional model, phylogenetic information\nprovides strong predictive power in a recently published global database of\nhost-parasite interactions. By scaling the phylogeny using an evolutionary\nmodel, our method allows for biological interpretation often missing from\nlatent variable models. To further improve on the phylogeny-only model, we\ncombine a hierarchical Bayesian latent score framework for bipartite graphs\nthat accounts for the number of interactions per species with the host\ndependence informed by phylogeny. Combining the two information sources yields\nsignificant improvement in predictive accuracy over each of the submodels\nalone. As many interaction networks are constructed from presence-only data, we\nextend the model by integrating a correction mechanism for missing\ninteractions, which proves valuable in reducing uncertainty in unobserved\ninteractions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 10:17:49 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 23:27:20 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 13:41:27 GMT"}, {"version": "v4", "created": "Mon, 31 Dec 2018 09:57:27 GMT"}, {"version": "v5", "created": "Fri, 20 Sep 2019 03:24:49 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Elmasri", "Mohamad", ""], ["Farrell", "Maxwell J.", ""], ["Davies", "T. Jonathan", ""], ["Stephens", "David A.", ""]]}, {"id": "1707.08407", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, Min Zhu, Keith E. Muller", "title": "A Note on Implementing a Special Case of the LEAR Covariance Model in\n  Standard Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated measures analyses require proper choice of the correlation model to\nensure accurate inference and optimal efficiency. The linear exponent\nautoregressive (LEAR) correlation model provides a flexible two-parameter\ncorrelation structure that accommodates a variety of data types in which the\ncorrelation within-sampling unit decreases exponentially in time or space. The\nLEAR model subsumes three classic temporal correlation structures, namely\ncompound symmetry, continuous-time AR(1), and MA(1), while maintaining\nparsimony and providing appealing statistical and computational properties. It\nalso supplies a plausible correlation structure for power analyses across many\nexperimental designs. However, no commonly used statistical packages provide a\nstraightforward way to implement the model, limiting its use to those with the\nappropriate programming skills. Here we present a reparameterization of the\nLEAR model that allows easily implementing it in standard software for the\nspecial case of data with equally spaced temporal or spatial intervals.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:28:51 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Simpson", "Sean L.", ""], ["Zhu", "Min", ""], ["Muller", "Keith E.", ""]]}, {"id": "1707.08538", "submitter": "Jarod Yan Liang Lee", "authors": "Jarod Y.L. Lee, Peter J. Green and Louise M. Ryan", "title": "On the \"Poisson Trick\" and its Extensions for Fitting Multinomial\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with the fitting of multinomial regression models\nusing the so-called \"Poisson Trick\". The work is motivated by Chen & Kuo (2001)\nand Malchow-M{\\o}ller & Svarer (2003) which have been criticized for being\ncomputationally inefficient and sometimes producing nonsense results. We first\ndiscuss the case of independent data and offer a parsimonious fitting strategy\nwhen all covariates are categorical. We then propose a new approach for\nmodelling correlated responses based on an extension of the Gamma-Poisson\nmodel, where the likelihood can be expressed in closed-form. The parameters are\nestimated via an Expectation/Conditional Maximization (ECM) algorithm, which\ncan be implemented using functions for fitting generalized linear models\nreadily available in standard statistical software packages. Compared to\nexisting methods, our approach avoids the need to approximate the intractable\nintegrals and thus the inference is exact with respect to the approximating\nGamma-Poisson model. The proposed method is illustrated via a reanalysis of the\nyogurt data discussed by Chen & Kuo (2001).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 16:55:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Lee", "Jarod Y. L.", ""], ["Green", "Peter J.", ""], ["Ryan", "Louise M.", ""]]}, {"id": "1707.08681", "submitter": "Gregory S. Warrington", "authors": "Jeffrey S. Buzas and Gregory S. Warrington", "title": "Gerrymandering and the net number of US House seats won due to\n  vote-distribution asymmetries", "comments": "18 pages, 9 figures. Revisions: Emphasized fact that S-declination\n  measures asymmetry in vote distribution and cannot account directly for\n  geographic clustering; added \"greedy\" packing/cracking algorithm; added\n  paragraph on simulations; other minor edits and corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the recently introduced declination function, we estimate the net\nnumber of seats won in the US House of Representatives due to asymmetries in\nvote distributions. Such asymmetries can arise from combinations of partisan\ngerrymandering and inherent geographic advantage. Our estimates show\nsignificant biases in favor of the Democrats prior to the mid 1990s and\nsignificant biases in favor of Republicans since then. We find net differences\nof 28, 20 and 25 seats in favor of the Republicans in the years 2012, 2014 and\n2016, respectively. The validity of our results is supported by the technique\nof simulated packing and cracking. We also use this technique to show that the\npresidential-vote logistic regression model is insensitive to the packing and\ncracking by which partisan gerrymanders are achieved.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 01:46:28 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 10:46:23 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Buzas", "Jeffrey S.", ""], ["Warrington", "Gregory S.", ""]]}, {"id": "1707.08719", "submitter": "Bijju Veduruparthi Mr", "authors": "Bijju Kranthi Veduruparthi, Jayanta Mukherjee, Partha Pratim Das,\n  Mandira Saha, Raj Kumar Shrimali, Sanjoy Chatterjee, Soumendranath Ray,\n  Sriram Prasath", "title": "Analysis of Deformation Fields in Spatio-temporal CBCT images of lungs\n  for radiotherapy patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deformable registration of spatiotemporal Cone-Beam Computed Tomography\n(CBCT) images taken sequentially during the radiation treatment course yields a\ndeformation field for a pair of images. The Jacobian of this field at any voxel\nprovides a measure of the expansion or contraction of a unit volume. We analyze\nthe Jacobian at different sections of the tumor volumes obtained from\ndelineation done by radiation oncologists for lung cancer patients. The\ndelineations across the temporal sequence are compared post registration to\ncompute tumor areas namely, unchanged (U), newly grown (G), and reduced (R)\nthat have undergone changes. These three regions of the tumor are considered\nfor statistical analysis. In addition, statistics of non-tumor (N) regions are\ntaken into consideration. Sequential CBCT images of 29 patients were used in\nstudying the distribution of Jacobian in these four different regions, along\nwith a test set of 16 patients. Statistical tests performed over the dataset\nconsisting of first three weeks of treatment suggest that, means of the\nJacobian in the regions follow a particular order. Although, this observation\nis apparent when applied to the distribution over the whole population, it is\nfound that the ordering deviates for many individual cases. We propose a\nhypothesis to classify patients who have had partial response (PR). Early\nprediction of the response was studied using only three weeks of data. The\nearly prediction of response of treatment was supported by a Fisher's test with\nodds ratio of 5.13 and a p-value of 0.043.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 06:51:27 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Veduruparthi", "Bijju Kranthi", ""], ["Mukherjee", "Jayanta", ""], ["Das", "Partha Pratim", ""], ["Saha", "Mandira", ""], ["Shrimali", "Raj Kumar", ""], ["Chatterjee", "Sanjoy", ""], ["Ray", "Soumendranath", ""], ["Prasath", "Sriram", ""]]}, {"id": "1707.08774", "submitter": "Stephen Rush PhD", "authors": "Pavel Petrov, Stephen T Rush, Zhichun Zhai, Christine H Lee, Peter T\n  Kim, and Giseon Heo", "title": "Topological Data Analysis of Clostridioides difficile Infection and\n  Fecal Microbiota Transplantation", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational topologists recently developed a method, called persistent\nhomology to analyze data presented in terms of similarity or dissimilarity.\nIndeed, persistent homology studies the evolution of topological features in\nterms of a single index, and is able to capture higher order features beyond\nthe usual clustering techniques. There are three descriptive statistics of\npersistent homology, namely barcode, persistence diagram and more recently,\npersistence landscape. Persistence landscape is useful for statistical\ninference as it belongs to a space of $p-$integrable functions, a separable\nBanach space. We apply tools in both computational topology and statistics to\nDNA sequences taken from Clostridioides difficile infected patients treated\nwith an experimental fecal microbiota transplantation. Our statistical and\ntopological data analysis are able to detect interesting patterns among\npatients and donors. It also provides visualization of DNA sequences in the\nform of clusters and loops.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 08:28:15 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 07:09:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Petrov", "Pavel", ""], ["Rush", "Stephen T", ""], ["Zhai", "Zhichun", ""], ["Lee", "Christine H", ""], ["Kim", "Peter T", ""], ["Heo", "Giseon", ""]]}, {"id": "1707.08826", "submitter": "Daniel Gamermann Dr.", "authors": "Daniel Gamermann and Felipe Leite Antunes", "title": "Evidence of Fraud in Brazil's Electoral Campaigns Via the Benford's Law", "comments": "21 pages, 3 figures, 9 tables", "journal-ref": null, "doi": "10.1016/j.physa.2017.12.120", "report-no": null, "categories": "stat.AP cs.CY physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of democracy is that the people govern through elected\nrepresentatives. Therefore, a democracy is healthy as long as the elected\npoliticians do represent the people. We have analyzed data from the Brazilian\nelectoral court (Tribunal Superior Eleitoral, TSE) concerning money donations\nfor the electoral campaigns and the election results. Our work points to two\ndisturbing conclusions: money is a determining factor on whether a candidate is\nelected or not (as opposed to representativeness); secondly, the use of\nBenford's Law to analyze the declared donations received by the parties and\nelectoral campaigns shows evidence of fraud in the declarations. A better term\nto define Brazil's government system is what we define as chrimatocracy (govern\nby money).\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 12:47:14 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Gamermann", "Daniel", ""], ["Antunes", "Felipe Leite", ""]]}, {"id": "1707.08860", "submitter": "Huajin Wang", "authors": "Huajin Wang, Jianhui Li, Zhihong Shen and Yuanchun Zhou", "title": "Approximations and Bounds for (n, k) Fork-Join Queues: A Linear\n  Transformation Approach", "comments": "10 pages", "journal-ref": "2018 18th IEEE/ACM International Symposium on Cluster, Cloud and\n  Grid Computing (CCGRID)", "doi": "10.1109/CCGRID.2018.00069", "report-no": null, "categories": "cs.PF cs.DC cs.NI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compared to basic fork-join queues, a job in (n, k) fork-join queues only\nneeds its k out of all n sub-tasks to be finished. Since (n, k) fork-join\nqueues are prevalent in popular distributed systems, erasure coding based cloud\nstorages, and modern network protocols like multipath routing, estimating the\nsojourn time of such queues is thus critical for the performance measurement\nand resource plan of computer clusters. However, the estimating keeps to be a\nwell-known open challenge for years, and only rough bounds for a limited range\nof load factors have been given. In this paper, we developed a closed-form\nlinear transformation technique for jointly-identical random variables: An\norder statistic can be represented by a linear combination of maxima. This\nbrand-new technique is then used to transform the sojourn time of non-purging\n(n, k) fork-join queues into a linear combination of the sojourn times of basic\n(k, k), (k+1, k+1), ..., (n, n) fork-join queues. Consequently, existing\napproximations for basic fork-join queues can be bridged to the approximations\nfor non-purging (n, k) fork-join queues. The uncovered approximations are then\nused to improve the upper bounds for purging (n, k) fork-join queues.\nSimulation experiments show that this linear transformation approach is\npracticed well for moderate n and relatively large k.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 13:35:13 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 09:02:36 GMT"}, {"version": "v3", "created": "Tue, 1 Aug 2017 03:02:27 GMT"}, {"version": "v4", "created": "Sat, 5 Aug 2017 05:44:25 GMT"}, {"version": "v5", "created": "Fri, 8 Sep 2017 04:40:18 GMT"}, {"version": "v6", "created": "Sun, 3 Dec 2017 10:57:40 GMT"}, {"version": "v7", "created": "Mon, 11 Dec 2017 00:59:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Wang", "Huajin", ""], ["Li", "Jianhui", ""], ["Shen", "Zhihong", ""], ["Zhou", "Yuanchun", ""]]}, {"id": "1707.09021", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph, Oleg Sofrygin, Wenjing Zheng, Mark J. van der Laan", "title": "Robust and Flexible Estimation of Stochastic Mediation Effects: A\n  Proposed Method and Example in a Randomized Trial Setting", "comments": "24 pages, 2 tables, 2 figures", "journal-ref": "Epidemiologic Methods. 2017; 7(1)", "doi": "10.1515/em-2017-0007", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis can improve understanding of the mechanisms\nunderlying epidemiologic associations. However, the utility of natural direct\nand indirect effect estimation has been limited by the assumption of no\nconfounder of the mediator-outcome relationship that is affected by prior\nexposure---an assumption frequently violated in practice. We build on recent\nwork that identified alternative estimands that do not require this assumption\nand propose a flexible and double robust semiparametric targeted minimum\nloss-based estimator for data-dependent stochastic direct and indirect effects.\nThe proposed method treats the intermediate confounder affected by prior\nexposure as a time-varying confounder and intervenes stochastically on the\nmediator using a distribution which conditions on baseline covariates and\nmarginalizes over the intermediate confounder. In addition, we assume the\nstochastic intervention is given, conditional on observed data, which results\nin a simpler estimator and weaker identification assumptions. We demonstrate\nthe estimator's finite sample and robustness properties in a simple simulation\nstudy. We apply the method to an example from the Moving to Opportunity\nexperiment. In this application, randomization to receive a housing voucher is\nthe treatment/instrument that influenced moving to a low-poverty neighborhood,\nwhich is the intermediate confounder. We estimate the data-dependent stochastic\ndirect effect of randomization to the voucher group on adolescent marijuana use\nnot mediated by change in school district and the stochastic indirect effect\nmediated by change in school district. We find no evidence of mediation. Our\nestimator is easy to implement in standard statistical software, and we provide\nannotated R code to further lower implementation barriers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 19:44:44 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 23:41:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Sofrygin", "Oleg", ""], ["Zheng", "Wenjing", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1707.09133", "submitter": "Feras El Zarwi", "authors": "Feras El Zarwi, Akshay Vij, Joan Walker", "title": "Modeling and Forecasting the Evolution of Preferences over Time: A\n  Hidden Markov Model of Travel Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature suggests that preferences, as denoted by taste parameters and\nconsideration sets, may evolve over time in response to changes in demographic\nand situational variables, psychological, sociological and biological\nconstructs, and available alternatives and their attributes. However, existing\nrepresentations typically overlook the influence of past experiences on present\npreferences. This study develops, applies and tests a hidden Markov model with\na discrete choice kernel to model and forecast the evolution of individual\npreferences and behaviors over long-range forecasting horizons. The hidden\nstates denote different preferences i.e. modes considered in the choice set,\nand sensitivity to level-of-service attributes. The evolutionary path of those\nhidden states is hypothesized to be a first-order Markov process. The framework\nis applied to study the evolution of travel mode preferences, or modality\nstyles, over time, in response to a major change in the public transportation\nsystem. We use longitudinal travel diary from Santiago, Chile. The dataset\nconsists of four one-week pseudo travel diaries collected before and after the\nintroduction of Transantiago, a complete redesign of the public transportation\nsystem in the city. Our model identifies four modality styles in the\npopulation: drivers, bus users, bus-metro users, and auto-metro users. The\nmodality styles differ in terms of the travel modes that they consider and\ntheir sensitivity to level-of-service attributes. At the population level,\nthere are significant shifts in the distribution of individuals across modality\nstyles before and after the change in the system, but the distribution is\nrelatively stable in the periods after the change. Finally, a comparison\nbetween the proposed dynamic framework and comparable static frameworks reveals\ndifferences in aggregate forecasts for different policy scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 07:53:08 GMT"}], "update_date": "2017-07-31", "authors_parsed": [["Zarwi", "Feras El", ""], ["Vij", "Akshay", ""], ["Walker", "Joan", ""]]}, {"id": "1707.09308", "submitter": "Adam Sales", "authors": "Adam C. Sales and John F. Pane", "title": "The Role of Mastery Learning in Intelligent Tutoring Systems: Principal\n  Stratification on a Latent Variable", "comments": "Forthcoming in Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Students in Algebra I classrooms typically learn at different rates and\nstruggle at different points in the curriculum---a common challenge for math\nteachers. Cognitive Tutor Algebra I (CTA1), educational computer program,\naddresses such student heterogeneity via what they term \"mastery learning,\"\nwhere students progress from one section of the curriculum to the next by\ndemonstrating appropriate \"mastery\" at each stage. However, when students are\nunable to master a section's skills even after trying many problems, they are\nautomatically promoted to the next section anyway. Does promotion without\nmastery impair the program's effectiveness?\n  At least in certain domains, CTA1 was recently shown to improve student\nlearning on average in a randomized effectiveness study. This paper uses\nstudent log data from that study in a continuous principal stratification model\nto estimate the relationship between students' potential mastery and the CTA1\ntreatment effect. In contrast to extant principal stratification applications,\na student's propensity to master worked sections here is never directly\nobserved. Consequently we embed an item-response model, which measures\nstudents' potential mastery, within the larger principal stratification model.\nWe find that the tutor may, in fact, be more effective for students who are\nmore frequently promoted (despite unsuccessfully completing sections of the\nmaterial). However, since these students are distinctive in their educational\nstrength (as well as in other respects), it remains unclear whether this\nenhanced effectiveness can be directly attributed to aspects of the mastery\nlearning program.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 16:23:15 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 16:21:10 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Sales", "Adam C.", ""], ["Pane", "John F.", ""]]}, {"id": "1707.09334", "submitter": "Xun Huan", "authors": "Xun Huan, Cosmin Safta, Khachik Sargsyan, Zachary P. Vane, Guilhem\n  Lacaze, Joseph C. Oefelein, Habib N. Najm", "title": "Compressive Sensing with Cross-Validation and Stop-Sampling for Sparse\n  Polynomial Chaos Expansions", "comments": "Preprint 29 pages, 16 figures (56 small figures); v1 submitted to the\n  SIAM/ASA Journal on Uncertainty Quantification on July 28, 2017; v2 submitted\n  on March 12, 2018. v2 changes: minor edits involving some content\n  reorganization and clarification; v3 submitted on May 5, 2018. v3 changes:\n  minor edits", "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification 6 (2018) 907-936", "doi": "10.1137/17M1141096", "report-no": null, "categories": "stat.CO cs.IT math.IT physics.comp-ph physics.flu-dyn stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressive sensing is a powerful technique for recovering sparse solutions\nof underdetermined linear systems, which is often encountered in uncertainty\nquantification analysis of expensive and high-dimensional physical models. We\nperform numerical investigations employing several compressive sensing solvers\nthat target the unconstrained LASSO formulation, with a focus on linear systems\nthat arise in the construction of polynomial chaos expansions. With core\nsolvers of l1_ls, SpaRSA, CGIST, FPC_AS, and ADMM, we develop techniques to\nmitigate overfitting through an automated selection of regularization constant\nbased on cross-validation, and a heuristic strategy to guide the stop-sampling\ndecision. Practical recommendations on parameter settings for these techniques\nare provided and discussed. The overall method is applied to a series of\nnumerical examples of increasing complexity, including large eddy simulations\nof supersonic turbulent jet-in-crossflow involving a 24-dimensional input.\nThrough empirical phase-transition diagrams and convergence plots, we\nillustrate sparse recovery performance under structures induced by polynomial\nchaos, accuracy and computational tradeoffs between polynomial bases of\ndifferent degrees, and practicability of conducting compressive sensing for a\nrealistic, high-dimensional physical application. Across test cases studied in\nthis paper, we find ADMM to have demonstrated empirical advantages through\nconsistent lower errors and faster computational times.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 17:20:39 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 22:31:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 10:24:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Huan", "Xun", ""], ["Safta", "Cosmin", ""], ["Sargsyan", "Khachik", ""], ["Vane", "Zachary P.", ""], ["Lacaze", "Guilhem", ""], ["Oefelein", "Joseph C.", ""], ["Najm", "Habib N.", ""]]}, {"id": "1707.09380", "submitter": "Chris Glynn", "authors": "Chris Glynn and Emily B. Fox", "title": "Dynamics of homelessness in urban America", "comments": "54 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relationship between housing costs and homelessness has important\nimplications for the way that city and county governments respond to increasing\nhomeless populations. Though many analyses in the public policy literature have\nexamined inter-community variation in homelessness rates to identify causal\nmechanisms of homelessness (Byrne et al., 2013; Lee et al., 2003; Fargo et al.,\n2013), few studies have examined time-varying homeless counts within the same\ncommunity (McCandless et al., 2016). To examine trends in homeless population\ncounts in the 25 largest U.S. metropolitan areas, we develop a dynamic Bayesian\nhierarchical model for time-varying homeless count data. Particular care is\ngiven to modeling uncertainty in the homeless count generating and measurement\nprocesses, and a critical distinction is made between the counted number of\nhomeless and the true size of the homeless population. For each metro under\nstudy, we investigate the relationship between increases in the Zillow Rent\nIndex and increases in the homeless population. Sensitivity of inference to\npotential improvements in the accuracy of point-in-time counts is explored, and\nevidence is presented that the inferred increase in the rate of homelessness\nfrom 2011-2016 depends on prior beliefs about the accuracy of homeless counts.\nA main finding of the study is that the relationship between homelessness and\nrental costs is strongest in New York, Los Angeles, Washington, D.C., and\nSeattle.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 19:04:37 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Glynn", "Chris", ""], ["Fox", "Emily B.", ""]]}, {"id": "1707.09433", "submitter": "Dennis Feehan", "authors": "Dennis M. Feehan", "title": "Separating the signal from the noise: Evidence for deceleration in\n  old-age death rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widespread population aging has made it critical to understand death rates at\nold ages. However, studying mortality at old ages is challenging because the\ndata are sparse: numbers of survivors and deaths get smaller and smaller with\nage. We show how to address this challenge by using principled model selection\ntechniques to empirically evaluate theoretical mortality models. We test nine\ndifferent theoretical models of old-age death rates by fitting them to 360\nhigh-quality datasets on cohort mortality above age 80. Models that allow for\nthe possibility of decelerating death rates tend to fit better than models that\nassume exponentially increasing death rates. No single model is capable of\nuniversally explaining observed old-age mortality patterns, but the\nLog-Quadratic model most consistently predicts well. Patterns of model fit\ndiffer by country and sex; we discuss possible mechanisms, including sample\nsize, period effects, and regional or cultural factors that may be important\nkeys to understanding patterns of old-age mortality. We introduce a freely\navailable R package that enables researchers to extend our analysis to other\nmodels, age ranges, and data sources.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 22:43:42 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 00:14:08 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Feehan", "Dennis M.", ""]]}, {"id": "1707.09491", "submitter": "Slava Mikhaylov", "authors": "Stefano Gurciullo and Slava Mikhaylov", "title": "Topology Analysis of International Networks Based on Debates in the\n  United Nations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.CG math.AT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In complex, high dimensional and unstructured data it is often difficult to\nextract meaningful patterns. This is especially the case when dealing with\ntextual data. Recent studies in machine learning, information theory and\nnetwork science have developed several novel instruments to extract the\nsemantics of unstructured data, and harness it to build a network of relations.\nSuch approaches serve as an efficient tool for dimensionality reduction and\npattern detection. This paper applies semantic network science to extract\nideological proximity in the international arena, by focusing on the data from\nGeneral Debates in the UN General Assembly on the topics of high salience to\ninternational community. UN General Debate corpus (UNGDC) covers all high-level\ndebates in the UN General Assembly from 1970 to 2014, covering all UN member\nstates. The research proceeds in three main steps. First, Latent Dirichlet\nAllocation (LDA) is used to extract the topics of the UN speeches, and\ntherefore semantic information. Each country is then assigned a vector\nspecifying the exposure to each of the topics identified. This intermediate\noutput is then used in to construct a network of countries based on information\ntheoretical metrics where the links capture similar vectorial patterns in the\ntopic distributions. Topology of the networks is then analyzed through network\nproperties like density, path length and clustering. Finally, we identify\nspecific topological features of our networks using the map equation framework\nto detect communities in our networks of countries.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 10:09:04 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Gurciullo", "Stefano", ""], ["Mikhaylov", "Slava", ""]]}, {"id": "1707.09561", "submitter": "Jelena Bradic", "authors": "Jue Hou, Jelena Bradic, Ronghui Xu", "title": "Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:53:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:03:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1707.09587", "submitter": "Y. X. Rachel Wang", "authors": "Y. X. Rachel Wang, Purnamrita Sarkar, Oana Ursu, Anshul Kundaje, Peter\n  J. Bickel", "title": "Network modelling of topological domains using Hi-C data", "comments": null, "journal-ref": "Annals of Applied Statistics 2019, Vol. 13, No. 3, 1511-1536", "doi": "10.1214/19-AOAS1244", "report-no": null, "categories": "stat.AP q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chromosome conformation capture experiments such as Hi-C are used to map the\nthree-dimensional spatial organization of genomes. One specific feature of the\n3D organization is known as topologically associating domains (TADs), which are\ndensely interacting, contiguous chromatin regions playing important roles in\nregulating gene expression. A few algorithms have been proposed to detect TADs.\nIn particular, the structure of Hi-C data naturally inspires application of\ncommunity detection methods. However, one of the drawbacks of community\ndetection is that most methods take exchangeability of the nodes in the network\nfor granted; whereas the nodes in this case, i.e. the positions on the\nchromosomes, are not exchangeable. We propose a network model for detecting\nTADs using Hi-C data that takes into account this non-exchangeability. In\naddition, our model explicitly makes use of cell-type specific CTCF binding\nsites as biological covariates and can be used to identify conserved TADs\nacross multiple cell types. The model leads to a likelihood objective that can\nbe efficiently optimized via relaxation. We also prove that when suitably\ninitialized, this model finds the underlying TAD structure with high\nprobability. Using simulated data, we show the advantages of our method and the\ncaveats of popular community detection methods, such as spectral clustering, in\nthis application. Applying our method to real Hi-C data, we demonstrate the\ndomains identified have desirable epigenetic features and compare them across\ndifferent cell types.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 07:04:25 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 05:39:41 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 08:36:57 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Wang", "Y. X. Rachel", ""], ["Sarkar", "Purnamrita", ""], ["Ursu", "Oana", ""], ["Kundaje", "Anshul", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1707.09609", "submitter": "Mahdi Doostparast", "authors": "Mahdi Doostparast", "title": "Explicit expressions for European option pricing under a generalized\n  skew normal distribution", "comments": "12 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under a generalized skew normal distribution we consider the problem of\nEuropean option pricing. Existence of the martingale measure is proved. An\nexplicit expression for a given European option price is presented in terms of\nthe cumulative distribution function of the univariate skew normal and the\nbivariate standard normal distributions. Some special cases are investigated in\na greater detail. To carry out the sensitivity of the option price to the skew\nparameters, numerical methods are applied. Some concluding remarks and further\nworks are given. The results obtained are extensions of the results provided by\n[4].\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 11:03:40 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Doostparast", "Mahdi", ""]]}, {"id": "1707.09675", "submitter": "Yuchen Zheng", "authors": "Yuchen Zheng, Kun Lin, Thomas White, Jeremy Pickereign, Gigi Yuen-Reed", "title": "On Designing of a Low Leakage Patient-Centric Provider Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a patient in a provider network seeks services outside of their\ncommunity, the community experiences a leakage. Leakage is undesirable as it\ntypically leads to higher out-of-network cost for patient and increases barrier\nfor care coordination, which is particularly problematic for Accountable Care\nOrganization (ACO) as the in-network providers are financially responsible for\npatient quality and outcome. We aim to design a data-driven method to identify\nnaturally occurring provider networks driven by diabetic patient choices, and\nunderstand the relationship among provider composition, patient composition,\nand service leakage pattern. We construct a healthcare provider network based\non patients' historical medical insurance claims. A community detection\nalgorithm is used to identify naturally occurring communities of collaborating\nproviders. Finally, import-export analysis is conducted to benchmark their\nleakage pattern and identify further leakage reduction opportunity. The design\nyields six major provider communities with diverse profiles. Some communities\nare geographically concentrated, while others tend to draw patients with\ncertain diabetic co-morbidities. Providers from the same healthcare institution\nare likely to be assigned to the same community. While most communities have\nhigh within-community utilization and spending, at 85% and 86% respectively,\nleakage still persists. Hence, we utilize a metric from import-export analysis\nto detect leakage, gaining insight on how to minimizing leakage. In conclusion,\nwe identify patient-driven provider organization by surfacing providers who\nshare a large number of patients. By analyzing the import-export behavior of\neach identified community using a novel approach and profiling community\npatient and provider composition we understand the key features of having a\nbalanced number of PCP and specialists and provider heterogeneity.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 21:35:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Zheng", "Yuchen", ""], ["Lin", "Kun", ""], ["White", "Thomas", ""], ["Pickereign", "Jeremy", ""], ["Yuen-Reed", "Gigi", ""]]}, {"id": "1707.09706", "submitter": "Jing Mei", "authors": "Jing Mei, Eryu Xia, Xiang Li, Guotong Xie", "title": "Developing Knowledge-enhanced Chronic Disease Risk Prediction Models\n  from Regional EHR Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision medicine requires the precision disease risk prediction models. In\nliterature, there have been a lot well-established (inter-)national risk\nmodels, but when applying them into the local population, the prediction\nperformance becomes unsatisfactory. To address the localization issue, this\npaper exploits the way to develop knowledge-enhanced localized risk models. On\nthe one hand, we tune models by learning from regional Electronic Health Record\n(EHR) repositories, and on the other hand, we propose knowledge injection into\nthe EHR data learning process. For experiments, we leverage the Pooled Cohort\nEquations (PCE, as recommended in ACC/AHA guidelines to estimate the risk of\nASCVD) to develop a localized ASCVD risk prediction model in diabetes. The\nexperimental results show that, if directly using the PCE algorithm on our\ncohort, the AUC is only 0.653, while our knowledge-enhanced localized risk\nmodel can achieve higher prediction performance with AUC of 0.723 (improved by\n10.7%).\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 03:12:05 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Mei", "Jing", ""], ["Xia", "Eryu", ""], ["Li", "Xiang", ""], ["Xie", "Guotong", ""]]}, {"id": "1707.09734", "submitter": "Muralikrishnan Srinivasan", "authors": "Muralikrishnan Srinivasan, Sheetal Kalyani", "title": "Approximate Random Matrix Models for Generalized Fading MIMO Channels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate random matrix models for $\\kappa-\\mu$ and $\\eta-\\mu$ faded\nmultiple input multiple output (MIMO) communication channels are derived in\nterms of a complex Wishart matrix. The proposed approximation has the least\nKullback-Leibler (KL) divergence from the original matrix distribution. The\nutility of the results are demonstrated in a) computing the average\ncapacity/rate expressions of $\\kappa-\\mu$/$\\eta-\\mu$ MIMO systems b) computing\noutage probability (OP) expressions for maximum ratio combining (MRC) for\n$\\kappa-\\mu$/$\\eta-\\mu$ faded MIMO channels c) ergodic rate expressions for\nzero-forcing (ZF) receiver in an uplink single cell massive MIMO scenario with\nlow resolution analog-to-digital converters (ADCs) in the antennas. These\napproximate expressions are compared with Monte-Carlo simulations and a close\nmatch is observed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 06:49:51 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 08:43:23 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Srinivasan", "Muralikrishnan", ""], ["Kalyani", "Sheetal", ""]]}, {"id": "1707.09770", "submitter": "Elie Amani", "authors": "Elie Amani (LISSI, F'SATI), Karim Djouani (LISSI, F'SATI), Anish\n  Kurien (F'SATI), Jean-R\\'emi De Boer, Willy Vigneau, Lionel Ries (CNES)", "title": "GPS Multipath Detection in the Frequency Domain", "comments": "2016 European Navigation Conference (ENC 2016), May 2016, Helsinki,\n  Finland. Proceedings of the 2016 European Navigation Conference (ENC 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multipath is among the major sources of errors in precise positioning using\nGPS and continues to be extensively studied. Two Fast Fourier Transform\n(FFT)-based detectors are presented in this paper as GPS multipath detection\ntechniques. The detectors are formulated as binary hypothesis tests under the\nassumption that the multipath exists for a sufficient time frame that allows\nits detection based on the quadrature arm of the coherent Early-minus-Late\ndiscriminator (Q EmL) for a scalar tracking loop (STL) or on the quadrature (Q\nEmL) and/or in-phase arm (I EmL) for a vector tracking loop (VTL), using an\nobservation window of N samples. Performance analysis of the suggested\ndetectors is done on multipath signal data acquired from the multipath\nenvironment simulator developed by the German Aerospace Centre (DLR) as well as\non multipath data from real GPS signals. Application of the detection tests to\ncorrelator outputs of scalar and vector tracking loops shows that they may be\nused to exclude multipath contaminated satellites from the navigation solution.\nThese detection techniques can be extended to other Global Navigation Satellite\nSystems (GNSS) such as GLONASS, Galileo and Beidou.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 08:58:36 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Amani", "Elie", "", "LISSI, F'SATI"], ["Djouani", "Karim", "", "LISSI, F'SATI"], ["Kurien", "Anish", "", "F'SATI"], ["De Boer", "Jean-R\u00e9mi", "", "CNES"], ["Vigneau", "Willy", "", "CNES"], ["Ries", "Lionel", "", "CNES"]]}, {"id": "1707.09974", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Biplab Paul", "title": "Some variations of EM algorithms for Marshall-Olkin bivariate Pareto\n  distribution with location and scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Asimit et. al used an EM algorithm to estimate Marshall-Olkin\nbivariate Pareto distribution. The distribution has seven parameters. We\ndescribe few alternative approaches of EM algorithm. A numerical simulation is\nperformed to verify the performance of different proposed algorithms. A\nreal-life data analysis is also shown for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:40:40 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Paul", "Biplab", ""]]}]