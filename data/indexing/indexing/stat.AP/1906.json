[{"id": "1906.00129", "submitter": "Simon Couch", "authors": "Simon Couch, Heather Kitada Smalley", "title": "Encouraging Equitable Bikeshare: Implications of Docked and Dockless\n  Models for Spatial Equity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen a rapid rise in the number of bikeshare programs,\nwhere bikes are made available throughout a community on an as-needed basis.\nGiven that many of these programs are at least partially publicly funded, a\ncentral concern of operators and investors is whether these systems operate\nequitably. Though spatial equity has been well-studied under the docked model,\nwhere bikes are picked up and dropped off at prespecified docking stations,\nthere has been little work examining that of the increasingly popular dockless\nmodel, where bikes can be picked up and dropped off from anywhere within an\noperating area. We explore comparative equity in spatial access to bikeshare\nservices under these two models by collecting spatial data on 45,935 bikes from\n73 bikeshare systems using a novel querying approach (with generalizable and\nfreely available source code), and joining this data with newly-available\nsociodemographic data at the census tract level. Using Poisson count\nregression, we perform the first comparative analysis of the two docking\napproaches, finding that dockless systems operate more equitably than docked\nsystems by education, but do not differ in spatial access by socioeconomic\nclass.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 02:04:00 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Couch", "Simon", ""], ["Smalley", "Heather Kitada", ""]]}, {"id": "1906.00168", "submitter": "Nikolay K Vitanov", "authors": "Wang Bo, Zlatinka I. Dimitrova, Nikolay K. Vitanov", "title": "Statistical analysis of the water level of Huang He river (Yellow river)\n  in China", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very high water levels of the large rivers are extremely dangerous events\nthat can lead to large floods and loss of property and thousands and even tens\nof thousands human lives. The information from the systematical monitoring of\nthe water levels allows us to obtain probability distributions for the\nextremely high values of the water levels of the rivers of interest. In this\narticle we study time series containing information from more than 10 years of\nsatellite observation of the water level of the Huang He river (Yellow river)\nin China. We show that the corresponding extreme values distribution is the\nWeibull distribution and determine the parameters of the distribution. The\nobtained results may help for evaluation of risks associated with floods for\nthe population and villages in the corresponding region of the Huang He river.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 07:13:08 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bo", "Wang", ""], ["Dimitrova", "Zlatinka I.", ""], ["Vitanov", "Nikolay K.", ""]]}, {"id": "1906.00286", "submitter": "Anders Hildeman", "authors": "Anders Hildeman, David Bolin, Igor Rychlik", "title": "Joint spatial modeling of significant wave height and wave period using\n  the SPDE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ocean wave distribution in a specific region of space and time is\ndescribed by its sea state. Knowledge about the sea states a ship encounters on\na journey can be used to assess various parameters of risk and wear associated\nwith the journey.\n  Two important characteristics of the sea state are the significant wave\nheight and mean wave period. We propose a joint spatial model of these two\nquantities on the north Atlantic ocean. The model describes the distribution of\nthe logarithm of the two quantities as a bivariate Gaussian random field. This\nrandom field is modeled as a solution to a system of coupled stochastic partial\ndifferential equations. The bivariate random field can model a wide variety of\nnon-stationary anisotropy and allows for arbitrary, and different,\ndifferentiability for the two marginal fields.\n  The parameters of the model are estimated on data of the north Atlantic using\na stepwise maximum likelihood method. The fitted model is used to derive the\ndistribution of accumulated fatigue damage for a ship sailing a transatlantic\nroute. Also, a method for estimating the risk of capsizing due to broaching-to,\nbased on the joint distribution of the two sea state characteristics, is\ninvestigated. The risks are calculated for a transatlantic route between\nAmerica and Europe using both data and the fitted model.\n  The results show that the model compares well with observed data. Also, it\nshows that the bivariate model is needed and cannot simply be approximated by a\nmodel of significant wave height alone.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 20:37:07 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Hildeman", "Anders", ""], ["Bolin", "David", ""], ["Rychlik", "Igor", ""]]}, {"id": "1906.00454", "submitter": "Saeed Khaki", "authors": "Saeed Khaki, Zahra Khalilzadeh and Lizhi Wang", "title": "Classification of Crop Tolerance to Heat and Drought: A Deep\n  Convolutional Neural Networks Approach", "comments": "Won the Best Paper Award of the Second International Workshop on\n  Machine Learning for Cyber-Agricultural Systems (Ames, IA, USA). One of the\n  winning solutions to the 2019 INFORMS Syngenta Crop Challenge. Presented at\n  2019 INFORMS Conference on Business Analytics and Operations Research\n  (Austin, TX, USA). Published in the Agronomy Journal", "journal-ref": "Agronomy 2019, 9, 833", "doi": "10.3390/agronomy9120833", "report-no": null, "categories": "cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental stresses such as drought and heat can cause substantial yield\nloss in agriculture. As such, hybrid crops that are tolerant to drought and\nheat stress would produce more consistent yields compared to the hybrids that\nare not tolerant to these stresses. In the 2019 Syngenta Crop Challenge,\nSyngenta released several large datasets that recorded the yield performances\nof 2,452 corn hybrids planted in 1,560 locations between 2008 and 2017 and\nasked participants to classify the corn hybrids as either tolerant or\nsusceptible to drought stress, heat stress, and combined drought and heat\nstress. However, no data was provided that classified any set of hybrids as\ntolerant or susceptible to any type of stress. In this paper, we present an\nunsupervised approach to solving this problem, which was recognized as one of\nthe winners in the 2019 Syngenta Crop Challenge. Our results labeled 121\nhybrids as drought tolerant, 193 as heat tolerant, and 29 as tolerant to both\nstresses.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jun 2019 17:25:01 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 01:19:44 GMT"}, {"version": "v3", "created": "Mon, 5 Aug 2019 21:17:59 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 19:39:36 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2019 18:27:29 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Khaki", "Saeed", ""], ["Khalilzadeh", "Zahra", ""], ["Wang", "Lizhi", ""]]}, {"id": "1906.00538", "submitter": "Wentian Huang", "authors": "Wentian Huang, David Ruppert", "title": "Copula-based functional Bayes classification with principal components\n  and partial least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new functional Bayes classifier that uses principal component\n(PC) or partial least squares (PLS) scores from the common covariance function,\nthat is, the covariance function marginalized over groups. When the groups have\ndifferent covariance functions, the PC or PLS scores need not be independent or\neven uncorrelated. We use copulas to model the dependence. Our method is\nsemiparametric; the marginal densities are estimated nonparametrically by\nkernel smoothing and the copula is modeled parametrically. We focus on Gaussian\nand t-copulas, but other copulas could be used. The strong performance of our\nmethodology is demonstrated through simulation, real data examples, and\nasymptotic properties.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 02:51:56 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 00:37:11 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Huang", "Wentian", ""], ["Ruppert", "David", ""]]}, {"id": "1906.00573", "submitter": "Steven Pav", "authors": "Steven E. Pav", "title": "Conditional inference on the asset with maximum Sharpe ratio", "comments": "code and latex source available from github repo,\n  github.com/shabbychef/maxsharpe", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We apply the procedure of Lee et al. to the problem of performing inference\non the signal-noise ratio of the asset which displays maximum sample Sharpe\nratio over a set of possibly correlated assets. We find a multivariate analogue\nof the commonly used approximate standard error of the Sharpe ratio to use in\nthis conditional estimation procedure. We also consider several alternative\nprocedures, including the simple Bonferroni correction for multiple hypothesis\ntesting, which we fix for the case of positive common correlation among assets,\nthe chi-bar square test against one-sided alternatives, Follman's test, and\nHansen's asymptotic adjustments.\n  Testing indicates the conditional inference procedure achieves nominal type I\nrate, and does not appear to suffer from non-normality of returns. The\nconditional estimation test has low power under the alternative where there is\nlittle spread in the signal-noise ratios of the assets, and high power under\nthe alternative where a single asset has high signal-noise ratio. Unlike the\nalternative procedures, it appears to enjoy rejection probabilities montonic in\nthe signal-noise ratio of the selected asset.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 04:50:52 GMT"}, {"version": "v2", "created": "Sun, 9 Jun 2019 17:01:48 GMT"}, {"version": "v3", "created": "Tue, 23 Jul 2019 05:33:21 GMT"}, {"version": "v4", "created": "Fri, 26 Jul 2019 01:26:37 GMT"}, {"version": "v5", "created": "Sun, 27 Oct 2019 00:48:31 GMT"}, {"version": "v6", "created": "Mon, 25 Nov 2019 05:36:01 GMT"}, {"version": "v7", "created": "Sat, 7 Dec 2019 06:47:47 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pav", "Steven E.", ""]]}, {"id": "1906.00587", "submitter": "Antonio Punzo", "authors": "Luca Bagnato, Antonio Punzo", "title": "Unconstrained representation of orthogonal matrices with application to\n  common principle components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical problems involve the estimation of a $\\left(d\\times\nd\\right)$ orthogonal matrix $\\textbf{Q}$. Such an estimation is often\nchallenging due to the orthonormality constraints on $\\textbf{Q}$. To cope with\nthis problem, we propose a very simple decomposition for orthogonal matrices\nwhich we abbreviate as PLR decomposition. It produces a one-to-one\ncorrespondence between $\\textbf{Q}$ and a $\\left(d\\times d\\right)$ unit lower\ntriangular matrix $\\textbf{L}$ whose $d\\left(d-1\\right)/2$ entries below the\ndiagonal are unconstrained real values. Once the decomposition is applied,\nregardless of the objective function under consideration, we can use any\nclassical unconstrained optimization method to find the minimum (or maximum) of\nthe objective function with respect to $\\textbf{L}$. For illustrative purposes,\nwe apply the PLR decomposition in common principle components analysis (CPCA)\nfor the maximum likelihood estimation of the common orthogonal matrix when a\nmultivariate leptokurtic-normal distribution is assumed in each group. Compared\nto the commonly used normal distribution, the leptokurtic-normal has an\nadditional parameter governing the excess kurtosis; this makes the estimation\nof $\\textbf{Q}$ in CPCA more robust against mild outliers. The usefulness of\nthe PLR decomposition in leptokurtic-normal CPCA is illustrated by two\nbiometric data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 05:58:54 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Bagnato", "Luca", ""], ["Punzo", "Antonio", ""]]}, {"id": "1906.00928", "submitter": "Basil Saeed", "authors": "Basil Saeed, Anastasiya Belyaeva, Yuhao Wang, Caroline Uhler", "title": "Anchored Causal Inference in the Presence of Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning a causal graph in the presence of\nmeasurement error. This setting is for example common in genomics, where gene\nexpression is corrupted through the measurement process. We develop a provably\nconsistent procedure for estimating the causal structure in a linear Gaussian\nstructural equation model from corrupted observations on its nodes, under a\nvariety of measurement error models. We provide an estimator based on the\nmethod-of-moments, which can be used in conjunction with constraint-based\ncausal structure discovery algorithms. We prove asymptotic consistency of the\nprocedure and also discuss finite-sample considerations. We demonstrate our\nmethod's performance through simulations and on real data, where we recover the\nunderlying gene regulatory network from zero-inflated single-cell RNA-seq data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:01:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Saeed", "Basil", ""], ["Belyaeva", "Anastasiya", ""], ["Wang", "Yuhao", ""], ["Uhler", "Caroline", ""]]}, {"id": "1906.01100", "submitter": "Nicholas Sim", "authors": "Brian Gin, Nicholas Sim, Anders Skrondal, Sophia Rabe-Hesketh", "title": "A Dyadic IRT Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a dyadic Item Response Theory (dIRT) model for measuring\ninteractions of pairs of individuals when the responses to items represent the\nactions (or behaviors, perceptions, etc.) of each individual (actor) made\nwithin the context of a dyad formed with another individual (partner). Examples\nof its use include the assessment of collaborative problem solving, or the\nevaluation of intra-team dynamics. The dIRT model generalizes both Item\nResponse Theory (IRT) models for measurement and the Social Relations Model\n(SRM) for dyadic data. The responses of an actor when paired with a partner are\nmodeled as a function of not only the actor's inclination to act and the\npartner's tendency to elicit that action, but also the unique relationship of\nthe pair, represented by two directional, possibly correlated, interaction\nlatent variables. Generalizations are discussed, such as accommodating triads\nor larger groups. Estimation is performed using Markov-chain Monte Carlo\nimplemented in Stan, making it straightforward to extend the dIRT model in\nvarious ways. Specifically, we show how the basic dIRT model can be extended to\naccommodate latent regressions, multilevel settings with cluster-level random\neffects, as well as joint modeling of dyadic data and a distal outcome. A\nsimulation study demonstrates that estimation performs well. We apply our\nproposed approach to speed-dating data and find new evidence of pairwise\ninteractions between participants, describing a mutual attraction that is\ninadequately characterized by individual properties alone.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 22:07:09 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Gin", "Brian", ""], ["Sim", "Nicholas", ""], ["Skrondal", "Anders", ""], ["Rabe-Hesketh", "Sophia", ""]]}, {"id": "1906.01131", "submitter": "Andreas Groll", "authors": "Andreas Groll, Christophe Ley, Gunther Schauberger, Hans Van Eetvelde,\n  Achim Zeileis", "title": "Hybrid Machine Learning Forecasts for the FIFA Women's World Cup 2019", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.03208", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we combine two different ranking methods together with several\nother predictors in a joint random forest approach for the scores of soccer\nmatches. The first ranking method is based on the bookmaker consensus, the\nsecond ranking method estimates adequate ability parameters that reflect the\ncurrent strength of the teams best. The proposed combined approach is then\napplied to the data from the two previous FIFA Women's World Cups 2011 and\n2015. Finally, based on the resulting estimates, the FIFA Women's World Cup\n2019 is simulated repeatedly and winning probabilities are obtained for all\nteams. The model clearly favors the defending champion USA before the host\nFrance.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 23:48:30 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Groll", "Andreas", ""], ["Ley", "Christophe", ""], ["Schauberger", "Gunther", ""], ["Van Eetvelde", "Hans", ""], ["Zeileis", "Achim", ""]]}, {"id": "1906.01174", "submitter": "Ryan McNellis", "authors": "Ali Aouad, Adam N. Elmachtoub, Kris J. Ferreira, Ryan McNellis", "title": "Market Segmentation Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We seek to provide an interpretable framework for segmenting users in a\npopulation for personalized decision-making. The standard approach is to\nperform market segmentation by clustering users according to similarities in\ntheir contextual features, after which a \"response model\" is fit to each\nsegment to model how users respond to personalized decisions. However, this\nmethodology is not ideal for personalization, since two users could in theory\nhave similar features but different response behaviors. We propose a general\nmethodology, Market Segmentation Trees (MSTs), for learning interpretable\nmarket segmentations explicitly driven by identifying differences in user\nresponse patterns. To demonstrate the versatility of our methodology, we design\ntwo new, specialized MST algorithms: (i) Choice Model Trees (CMTs) which can be\nused to predict a user's choice amongst multiple options, and (ii) Isotonic\nRegression Trees (IRTs) which can be used to solve the bid landscape\nforecasting problem. We provide a customizable, open-source code base for\ntraining MSTs in Python which employs several strategies for scalability,\nincluding parallel processing and warm starts. We provide a theoretical\nanalysis of the asymptotic running time of our training method validating its\ncomputational tractability on large datasets. We assess the practical\nperformance of MSTs on several synthetic and real world datasets, showing our\nmethod reliably finds market segmentations which accurately model response\nbehavior. Further, when applying MSTs to historical bidding data from a leading\ndemand-side platform (DSP), we show that MSTs consistently achieve a 5-29%\nimprovement in bid landscape forecasting accuracy over the DSP's current model.\nOur findings indicate that integrating market segmentation with response\nmodeling consistently leads to improvements in response prediction accuracy,\nthereby aiding personalization.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 03:03:40 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 21:24:13 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Aouad", "Ali", ""], ["Elmachtoub", "Adam N.", ""], ["Ferreira", "Kris J.", ""], ["McNellis", "Ryan", ""]]}, {"id": "1906.01216", "submitter": "A.M.H. Van Der Veen", "authors": "Adriaan M.H. van der Veen and Gerard Nieuwenkamp", "title": "Revision of ISO 19229 to support the certification of calibration gases\n  for purity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second edition of ISO 19229 expands the guidance in its predecessor in\ntwo ways. Firstly, it provides more support and examples describing possible\nexperimental approaches for purity analysis. A novelty is that it describes how\nthe beta distribution, or some other suitable probability distribution can be\nused to approximate the distribution of the output quantity, i.e., the fraction\nof a component. It also provides guidance on how to report coverage intervals\nin those cases, where the usual approximation from the GUM (Guide to the\nexpression of Uncertainty in Measurement) to use the normal or t distribution\nis inappropriate because of vicinity of zero. Coverage intervals play an\nimportant role in conformity assessment, and it is also customary to report\nmeasurement uncertainty in the form of a coverage interval, notwithstanding\nthat ISO/IEC 17025 does not explicitly require it. ISO 6141, which sets\nrequirements for certificates of calibration gas mixtures, does require the\nstatement of an expanded uncertainty, which has been interpreted that in the\ncase of a non-symmetric output probability distribution, a coverage interval\nshould be stated, along with the value and the standard uncertainty. This paper\ngives a brief background to the choices made and examples in ISO 19229.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 06:24:21 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["van der Veen", "Adriaan M. H.", ""], ["Nieuwenkamp", "Gerard", ""]]}, {"id": "1906.01461", "submitter": "Kellyn Arnold", "authors": "Kellyn F. Arnold, Vinny Davies, Marc de Kamps, Peter W. G. Tennant,\n  John Mbotwa, Mark S. Gilthorpe", "title": "Generalised linear models for prognosis and intervention: Theory,\n  practice, and implications for machine learning", "comments": "15 pages, 1 figure; minor changes made following external feedback\n  [v2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and causal explanation are fundamentally distinct tasks of data\nanalysis. In health applications, this difference can be understood in terms of\nthe difference between prognosis (prediction) and prevention/treatment (causal\nexplanation). Nevertheless, these two concepts are often conflated in practice.\nWe use the framework of generalised linear models (GLMs) to illustrate that\npredictive and causal queries require distinct processes for their application\nand subsequent interpretation of results. In particular, we identify five\nprimary ways in which GLMs for prediction differ from GLMs for causal\ninference: (1) The covariates that should be considered for inclusion in (and\npossibly exclusion from) the model; (2) How a suitable set of covariates to\ninclude in the model is determined; (3) Which covariates are ultimately\nselected, and what functional form (i.e. parameterisation) they take; (4) How\nthe model is evaluated; and (5) How the model is interpreted. We outline some\nof the potential consequences of failing to acknowledge and respect these\ndifferences, and additionally consider the implications for machine learning\n(ML) methods. We then conclude with three recommendations which we hope will\nhelp ensure that both prediction and causal modelling are used appropriately\nand to greatest effect in health research.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 14:12:36 GMT"}, {"version": "v2", "created": "Sat, 11 Jan 2020 12:09:40 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Arnold", "Kellyn F.", ""], ["Davies", "Vinny", ""], ["de Kamps", "Marc", ""], ["Tennant", "Peter W. G.", ""], ["Mbotwa", "John", ""], ["Gilthorpe", "Mark S.", ""]]}, {"id": "1906.01468", "submitter": "David Dias", "authors": "Helder Rojas, David Dias", "title": "Stress Testing Network Reconstruction via Graphical Causal Model", "comments": "arXiv admin note: text overlap with arXiv:1809.07401", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An resilience optimal evaluation of financial portfolios implies having\nplausible hypotheses about the multiple interconnections between the\nmacroeconomic variables and the risk parameters. In this paper, we propose a\ngraphical model for the reconstruction of the causal structure that links the\nmultiple macroeconomic variables and the assessed risk parameters, it is this\nstructure that we call Stress Testing Network (STN). In this model, the\nrelationships between the macroeconomic variables and the risk parameter define\na \"relational graph\" among their time-series, where related time-series are\nconnected by an edge. Our proposal is based on the temporal causal models, but\nunlike, we incorporate specific conditions in the structure which correspond to\nintrinsic characteristics this type of networks. Using the proposed model and\ngiven the high-dimensional nature of the problem, we used regularization\nmethods to efficiently detect causality in the time-series and reconstruct the\nunderlying causal structure. In addition, we illustrate the use of model in\ncredit risk data of a portfolio. Finally, we discuss its uses and practical\nbenefits in stress testing.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:16:24 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 14:29:57 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 12:37:05 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Rojas", "Helder", ""], ["Dias", "David", ""]]}, {"id": "1906.01484", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt and Jorge Mateu", "title": "Partial and semi-partial measures of spatial associations for\n  multivariate lattice data", "comments": "submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the development of partial and semi-partial measures of\nspatial associations in the context of multivariate spatial lattice data which\ndescribe global or local associations among spatially aggregated measurements\nfor pairs of different components conditional on all remaining components. The\nnew measures are illustrated using aggregated data on crime counts at ward\nlevel.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 14:47:29 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1906.01699", "submitter": "Evgeny Burnaev", "authors": "Boris B. Velichkovsky and Nikita Khromov and Alexander Korotin and\n  Evgeny Burnaev and Andrey Somov", "title": "Visual Fixations Duration as an Indicator of Skill Level in eSports", "comments": "10 pages, 3 figures", "journal-ref": "17th IFIP TC.13 International Conference on Human-Computer\n  Interaction, Springer LNCS, 2019", "doi": null, "report-no": null, "categories": "cs.HC cs.CY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using highly interactive systems like computer games requires a lot of visual\nactivity and eye movements. Eye movements are best characterized by visual\nfixation - periods of time when the eyes stay relatively still over an object.\nWe analyzed the distributions of fixation duration of professional athletes,\namateur and newbie players. We show that the analysis of fixation durations can\nbe used to deduce the skill level in computer game players. Highly skilled\ngaming performance is characterized by more variability in fixation durations\nand by bimodal fixation duration distributions suggesting the presence of two\nfixation types in high skill gamers. These fixation types were identified as\nambient (automatic spatial processing) and focal (conscious visual processing).\nThe analysis of computer gamers' skill level via the analysis of fixation\ndurations may be used in developing adaptive interfaces and in interface\ndesign.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 19:45:50 GMT"}, {"version": "v2", "created": "Sun, 18 Aug 2019 08:27:29 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Velichkovsky", "Boris B.", ""], ["Khromov", "Nikita", ""], ["Korotin", "Alexander", ""], ["Burnaev", "Evgeny", ""], ["Somov", "Andrey", ""]]}, {"id": "1906.01716", "submitter": "Nicolas Menzies", "authors": "Nicolas A Menzies", "title": "High-resolution estimates of the foreign-born population and\n  international migration for the United States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detailed estimates of migration stocks and flows provides evidence for\nunderstanding population dynamics, and the impact of economic and political\nchanges that influence migration. Using data from the 2000 decennial census and\n2001-2016 American Community Survey (ACS), this study derives\nhighly-disaggregated estimates of the foreign-born population residing in the\nUnited States for the period 2000-2018, and annual foreign-born entries to the\nACS population as a measure of immigration volume. These estimates are derived\nfrom an evidence synthesis combining pooled survey data with auxiliary data on\npotential biases in raw survey estimates and other trends affecting the\nforeign-born population. For an individual population stratum (defined by\ncurrent age, entry year, country of origin, and calendar year) direct estimates\nusing survey data can have substantial sampling uncertainty. By imposing\nlogical and probabilistic constraints, data are pooled across survey years to\nproduce more precise estimates. Corrections are implemented for respondent\nmisreporting of demographic information, and undercount of the foreign-born\npopulation in the ACS. This paper describes the statistical approach used to\nmodel population change, demonstrates the validity of the approach via in- and\nout-of-sample predictive performance, provides the population estimates, and\nhighlights potential applications.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jun 2019 20:29:54 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Menzies", "Nicolas A", ""]]}, {"id": "1906.01760", "submitter": "Ronald Yurko", "authors": "Ronald Yurko, Francesca Matano, Lee F. Richardson, Nicholas Granered,\n  Taylor Pospisil, Konstantinos Pelechrinis, Samuel L. Ventura", "title": "Going Deep: Models for Continuous-Time Within-Play Valuation of Game\n  Outcomes in American Football with Tracking Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous-time assessments of game outcomes in sports have become\nincreasingly common in the last decade. In American football, only\ndiscrete-time estimates of play value were possible, since the most advanced\npublic football datasets were recorded at the play-by-play level. While\nmeasures such as expected points and win probability are useful for evaluating\nfootball plays and game situations, there has been no research into how these\nvalues change throughout the course of a play. In this work, we make two main\ncontributions: First, we introduce a general framework for continuous-time\nwithin-play valuation in the National Football League using player-tracking\ndata. Our modular framework incorporates several modular sub-models, to easily\nincorporate recent work involving player tracking data in football. Second, we\nuse a long short-term memory recurrent neural network to construct a\nball-carrier model to estimate how many yards the ball-carrier is expected to\ngain from their current position, conditional on the locations and trajectories\nof the ball-carrier, their teammates and opponents. Additionally, we\ndemonstrate an extension with conditional density estimation so that the\nexpectation of any measure of play value can be calculated in continuous-time,\nwhich was never before possible at such a granular level.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 00:21:48 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 16:27:50 GMT"}, {"version": "v3", "created": "Wed, 13 Nov 2019 04:38:14 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Yurko", "Ronald", ""], ["Matano", "Francesca", ""], ["Richardson", "Lee F.", ""], ["Granered", "Nicholas", ""], ["Pospisil", "Taylor", ""], ["Pelechrinis", "Konstantinos", ""], ["Ventura", "Samuel L.", ""]]}, {"id": "1906.01811", "submitter": "Ali Malik", "authors": "Chris Piech, Ali Malik, Laura M Scott, Robert T Chang, Charles Lin", "title": "The Stanford Acuity Test: A Precise Vision Test Using Bayesian\n  Techniques and a Discovery in Human Visual Response", "comments": "Proceedings of the 34th AAAI Conference on Artificial Intelligence,\n  New York, USA. 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chart-based visual acuity measurements are used by billions of people to\ndiagnose and guide treatment of vision impairment. However, the ubiquitous eye\nexam has no mechanism for reasoning about uncertainty and as such, suffers from\na well-documented reproducibility problem. In this paper we make two core\ncontributions. First, we uncover a new parametric probabilistic model of visual\nacuity response based on detailed measurements of patients with eye disease.\nThen, we present an adaptive, digital eye exam using modern artificial\nintelligence techniques which substantially reduces acuity exam error over\nexisting approaches, while also introducing the novel ability to model its own\nuncertainty and incorporate prior beliefs. Using standard evaluation metrics,\nwe estimate a 74% reduction in prediction error compared to the ubiquitous\nchart-based eye exam and up to 67% reduction compared to the previous best\ndigital exam. For patients with eye disease, the novel ability to finely\nmeasure acuity from home could be a crucial part in early diagnosis. We provide\na web implementation of our algorithm for anyone in the world to use. The\ninsights in this paper also provide interesting implications for the field of\npsychometric Item Response Theory.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 03:37:32 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 01:02:03 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Piech", "Chris", ""], ["Malik", "Ali", ""], ["Scott", "Laura M", ""], ["Chang", "Robert T", ""], ["Lin", "Charles", ""]]}, {"id": "1906.01983", "submitter": "Mark Ho", "authors": "Mark K. Ho and Joanna Korman and Thomas L. Griffiths", "title": "The Computational Structure of Unintentional Meaning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.MA stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Speech-acts can have literal meaning as well as pragmatic meaning, but these\nboth involve consequences typically intended by a speaker. Speech-acts can also\nhave unintentional meaning, in which what is conveyed goes above and beyond\nwhat was intended. Here, we present a Bayesian analysis of how, to a listener,\nthe meaning of an utterance can significantly differ from a speaker's intended\nmeaning. Our model emphasizes how comprehending the intentional and\nunintentional meaning of speech-acts requires listeners to engage in\nsophisticated model-based perspective-taking and reasoning about the history of\nthe state of the world, each other's actions, and each other's observations. To\ntest our model, we have human participants make judgments about vignettes where\nspeakers make utterances that could be interpreted as intentional insults or\nunintentional faux pas. In elucidating the mechanics of speech-acts with\nunintentional meanings, our account provides insight into how communication\nboth functions and malfunctions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 17:26:36 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Ho", "Mark K.", ""], ["Korman", "Joanna", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1906.01990", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies and Lutz D\\\"umbgen", "title": "Covariate Selection Based on a Model-free Approach to Linear Regression\n  with Exact Probabilities", "comments": "40 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a completely new approach to the problem of\ncovariate selection in linear regression. It is intuitive, very simple, fast\nand powerful, non-frequentist and non-Bayesian. It does not overfit, there is\nno shrinkage of the least squares coefficients, and it is model-free. A\ncovariate or a set of covariates is included only if they are better in the\nsense of least squares than the same number of Gaussian covariates consisting\nof i.i.d. $N(0,1)$ random variables. The degree to which they are better is\nmeasured by the P-value which is the probability that the Gaussian covariates\nare better. This probability is given in terms of the Beta distribution, it is\nexact and it holds for the data at hand whatever this may be. The idea extends\nto a stepwise procedure, the main contribution of the paper, where the best of\nthe remaining covariates is only accepted if it is better than the best of the\nsame number of random Gaussian covariates. Again this probability is given in\nterms of the Beta distribution, it is exact and it holds for the data at hand\nwhatever this may be. We use a version with default parameters which works for\na large collection of known data sets with up to a few hundred thousand\ncovariates. The computing time for the largest data sets was about four\nseconds, and it outperforms all other selection procedures of which we are\naware. The paper gives the results of simulations, applications to real data\nsets and theorems on the asymptotic behaviour under the standard linear model.\nAn R-package {\\it gausscov} is available. \\\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 12:42:04 GMT"}, {"version": "v2", "created": "Sun, 17 May 2020 17:01:25 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Davies", "Laurie", ""], ["D\u00fcmbgen", "Lutz", ""]]}, {"id": "1906.02252", "submitter": "Feng Liu", "authors": "Feng Liu, Li Wang, Yifei Lou, Rencang Li, Patrick Purdon", "title": "Probabilistic Structure Learning for EEG/MEG Source Imaging with\n  Hierarchical Graph Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain source imaging is an important method for noninvasively characterizing\nbrain activity using Electroencephalogram (EEG) or Magnetoencephalography (MEG)\nrecordings. Traditional EEG/MEG Source Imaging (ESI) methods usually assume\nthat either source activity at different time points is unrelated, or that\nsimilar spatiotemporal patterns exist across an entire study period. The former\nassumption makes ESI analyses sensitive to noise, while the latter renders ESI\nanalyses unable to account for time-varying patterns of activity. To\neffectively deal with noise while maintaining flexibility and continuity among\nbrain activation patterns, we propose a novel probabilistic ESI model based on\na hierarchical graph prior. Under our method, a spanning tree constraint\nensures that activity patterns have spatiotemporal continuity. An efficient\nalgorithm based on alternating convex search is presented to solve the proposed\nmodel and is provably convergent. Comprehensive numerical studies using\nsynthetic data on a real brain model are conducted under different levels of\nsignal-to-noise ratio (SNR) from both sensor and source spaces. We also examine\nthe EEG/MEG data in a real application, in which our ESI reconstructions are\nneurologically plausible. All the results demonstrate significant improvements\nof the proposed algorithm over the benchmark methods in terms of source\nlocalization performance, especially at high noise levels.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 19:03:26 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Liu", "Feng", ""], ["Wang", "Li", ""], ["Lou", "Yifei", ""], ["Li", "Rencang", ""], ["Purdon", "Patrick", ""]]}, {"id": "1906.02321", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A Vsevolozhskaya, Min Shi, Fengjiao Hu, Dmitri V Zaykin", "title": "DOT: Gene-set analysis by combining decorrelated association statistics", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pcbi.1007819", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Historically, the majority of statistical association methods have been\ndesigned assuming availability of SNP-level information. However, modern\ngenetic and sequencing data present new challenges to access and sharing of\ngenotype-phenotype datasets, including cost management, difficulties in\nconsolidation of records across research groups, etc. These issues make methods\nbased on SNP-level summary statistics for a joint analysis of variants in a\ngroup particularly appealing. The most common form of combining statistics is a\nsum of SNP-level squared scores, possibly weighted, as in burden tests for rare\nvariants. The overall significance of the resulting statistic is evaluated\nusing its distribution under the null hypothesis. Here, we demonstrate that\nthis basic approach can be substantially improved by decorrelating scores prior\nto their addition, resulting in remarkable power gains in situations that are\nmost commonly encountered in practice; namely, under heterogeneity of effect\nsizes and diversity between pairwise LD. In these situations, the power of the\ntraditional test, based on the added squared scores, quickly reaches a ceiling,\nas the number of variants increases. Thus, the traditional approach does not\nbenefit from information potentially contained in any additional SNPs, while\nour decorrelation by orthogonal transformation (DOT) method yields steady gain\nin power. We present theoretical and computational analyses of both approaches,\nand reveal causes behind sometimes dramatic difference in their respective\npowers. We showcase DOT by analyzing breast cancer data, in which our method\nstrengthened levels of previously reported associations and implied the\npossibility of multiple new alleles that jointly confer breast cancer risk.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jun 2019 21:49:09 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 20:26:07 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Vsevolozhskaya", "Olga A", ""], ["Shi", "Min", ""], ["Hu", "Fengjiao", ""], ["Zaykin", "Dmitri V", ""]]}, {"id": "1906.02384", "submitter": "Sayan Dasgupta", "authors": "Sayan Dasgupta and Ying Huang", "title": "Selecting Biomarkers for building optimal treatment selection rules\n  using Kernel Machines", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimal biomarker combinations for treatment-selection can be derived by\nminimizing total burden to the population caused by the targeted disease and\nits treatment. However, when multiple biomarkers are present, including all in\nthe model can be expensive and hurt model performance. To remedy this, we\nconsider feature selection in optimization by minimizing an extended total\nburden that additionally incorporates biomarker measurement costs. Formulating\nit as a 0-norm penalized weighted classification, we develop various procedures\nfor estimating linear and nonlinear combinations. Through simulations and a\nreal data example, we demonstrate the importance of incorporating\nfeature-selection and marker cost when deriving treatment-selection rules.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 02:17:48 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Dasgupta", "Sayan", ""], ["Huang", "Ying", ""]]}, {"id": "1906.02438", "submitter": "Feng Zhou", "authors": "Feng Zhou, Zhidong Li, Xuhui Fan, Yang Wang, Arcot Sowmya, Fang Chen", "title": "Fast Multi-resolution Segmentation for Nonstationary Hawkes Process\n  Using Cumulants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stationarity is assumed in vanilla Hawkes process, which reduces the\nmodel complexity but introduces a strong assumption. In this paper, we propose\na fast multi-resolution segmentation algorithm to capture the time-varying\ncharacteristics of nonstationary Hawkes process. The proposed algorithm is\nbased on the first and second order cumulants. Except for the computation\nefficiency, the algorithm can provide a hierarchical view of the segmentation\nat different resolutions. We extensively investigate the impact of\nhyperparameters on the performance of this algorithm. To ease the choice of one\nhyperparameter, a refined Gaussian process based segmentation algorithm is also\nproposed which proves to be robust. The proposed algorithm is applied to a real\nvehicle collision dataset and the outcome shows some interesting hierarchical\ndynamic time-varying characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 06:40:00 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Zhou", "Feng", ""], ["Li", "Zhidong", ""], ["Fan", "Xuhui", ""], ["Wang", "Yang", ""], ["Sowmya", "Arcot", ""], ["Chen", "Fang", ""]]}, {"id": "1906.02636", "submitter": "Nasrin Yousefi", "authors": "Timothy C. Y. Chan, Maria Eberg, Katharina Forster, Claire Holloway,\n  Luciano Ieraci, Yusuf Shalaby, Nasrin Yousefi", "title": "An Inverse Optimization Approach to Measuring Clinical Pathway\n  Concordance", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical pathways outline standardized processes in the delivery of care for\na specific disease. Patient journeys through the healthcare system, though, can\ndeviate substantially from these pathways. Given the positive benefits of\nclinical pathways, it is important to measure the concordance of patient\npathways so that variations in health system performance or bottlenecks in the\ndelivery of care can be detected, monitored, and acted upon. This paper\nproposes the first data-driven inverse optimization approach to measuring\npathway concordance in any problem context. Our specific application considers\nclinical pathway concordance for stage III colon cancer. We develop a novel\nconcordance metric and demonstrate using real patient data from Ontario, Canada\nthat it has a statistically significant association with survival. Our\nmethodological approach considers a patient's journey as a walk in a directed\ngraph, where the costs on the arcs are derived by solving an inverse shortest\npath problem. The inverse optimization model uses two sources of information to\nfind the arc costs: reference pathways developed by a provincial cancer agency\n(primary) and data from real-world patient-related activity from patients with\nboth positive and negative clinical outcomes (secondary). Thus, our inverse\noptimization framework extends existing models by including data points of both\nvarying \"primacy\" and \"alignment\". Data primacy is addressed through a\ntwo-stage approach to imputing the cost vector, while data alignment is\naddressed by a hybrid objective function that aims to minimize and maximize\nsuboptimality error for different subsets of input data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 15:11:57 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 21:54:38 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 06:02:17 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Chan", "Timothy C. Y.", ""], ["Eberg", "Maria", ""], ["Forster", "Katharina", ""], ["Holloway", "Claire", ""], ["Ieraci", "Luciano", ""], ["Shalaby", "Yusuf", ""], ["Yousefi", "Nasrin", ""]]}, {"id": "1906.02638", "submitter": "Jessie Hendricks", "authors": "Jessie Hendricks and Cedric Neumann", "title": "A Bayesian approach for the analysis of error rate studies in forensic\n  science", "comments": null, "journal-ref": "Forensic Science International, Volume 306, January 2020, Article\n  110047", "doi": "10.1016/j.forsciint.2019.110047", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, the field of forensic science has received\nrecommendations from the National Research Council of the U.S. National Academy\nof Sciences, the U.S. National Institute of Standards and Technology, and the\nU.S. President's Council of Advisors on Science and Technology to study the\nvalidity and reliability of forensic analyses. More specifically, these\ncommittees recommend estimation of the rates of occurrence of erroneous\nconclusions drawn from forensic analyses. \"Black box\" studies for the various\nsubjective feature-based comparison methods are intended for this purpose.\n  In general, \"black box\" studies often have unbalanced designs, comparisons\nthat are not independent, and missing data. These aspects pose difficulty in\nthe analysis of the results and are often ignored. Instead, interpretation of\nthe data relies on methods that assume independence between observations and a\nbalanced experiment. Furthermore, all of these projects are interpreted within\nthe frequentist framework and result in point estimates associated with\nconfidence intervals that are confusing to communicate and understand.\n  We propose to use an existing likelihood-free Bayesian inference method,\ncalled Approximate Bayesian Computation (ABC), that is capable of handling\nunbalanced designs, dependencies among the observations, and missing data. ABC\nallows for studying the parameters of interest without recourse to incoherent\nand misleading measures of uncertainty such as confidence intervals. By taking\ninto account information from all decision categories for a given examiner and\ninformation from the population of examiners, our method also allows for\nquantifying the risk of error for the given examiner, even when no error has\nbeen recorded for that examiner.\n  We illustrate our proposed method by reanalysing the results of the \"Noblis\nBlack Box\" study by Ulery et al. in 2011.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 15:13:42 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Hendricks", "Jessie", ""], ["Neumann", "Cedric", ""]]}, {"id": "1906.02822", "submitter": "Andrew Gelman", "authors": "Andrew Gelman and Yotam Margalit", "title": "The Political Significance of Social Penumbras", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To explain the political clout of different social groups, traditional\naccounts typically focus on the group's size, resources, or commonality and\nintensity of its members' interests. We contend that a group's \"penumbra\"-the\nset of individuals who are personally familiar with people in that group--is\nanother important explanatory factor that merits systematic analysis. To this\nend, we designed a panel study that allows us to learn about the\ncharacteristics of the penumbras of politically relevant groups such as gay\npeople, the unemployed or recent immigrants. Our study reveals major and\nsystematic differences in the penumbras of various social groups, even ones of\nsimilar size. Moreover, we find evidence that entering a group's penumbra is\nassociated with a change in attitude on related political questions. Taken\ntogether, our findings suggest that penumbras help account for variation in the\npolitical standing of different groups in society.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jun 2019 21:18:05 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 14:30:18 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Gelman", "Andrew", ""], ["Margalit", "Yotam", ""]]}, {"id": "1906.02863", "submitter": "Qi Zhao", "authors": "Qi Zhao, Lingli Zhang, Chun Shen, Jie Zhang, Jianfeng Feng", "title": "Double Generalized Linear Model Reveals Those with High Intelligence are\n  More Similar in Cortical Thickness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most studies indicate that intelligence (g) is positively correlated with\ncortical thickness. However, the interindividual variability of cortical\nthickness has not been taken into account. In this study, we aimed to identify\nthe association between intelligence and cortical thickness in adolescents from\nboth the group's mean and dispersion point of view, utilizing the structural\nbrain imaging from the Adolescent Brain and Cognitive Development (ABCD)\nConsortium, the largest cohort in early adolescents around 10 years old. The\nmean and dispersion parameters of cortical thickness and their association with\nintelligence were estimated using double generalized linear models(DGLM). We\nfound that for the mean model part, the thickness of the frontal lobe like\nsuperior frontal gyrus was negatively related to intelligence, while the\nsurface area was most positively associated with intelligence in the frontal\nlobe. In the dispersion part, intelligence was negatively correlated with the\ndispersion of cortical thickness in widespread areas, but not with the surface\narea. These results suggested that people with higher IQ are more similar in\ncortical thickness, which may be related to less differentiation or\nheterogeneity in cortical columns.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 02:11:21 GMT"}, {"version": "v2", "created": "Fri, 22 Nov 2019 10:34:16 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Zhao", "Qi", ""], ["Zhang", "Lingli", ""], ["Shen", "Chun", ""], ["Zhang", "Jie", ""], ["Feng", "Jianfeng", ""]]}, {"id": "1906.02956", "submitter": "Simon Meyer Lauritsen", "authors": "Simon Meyer Lauritsen, Mads Ellersgaard Kal{\\o}r, Emil Lund\n  Kongsgaard, Katrine Meyer Lauritsen, Marianne Johansson J{\\o}rgensen, Jeppe\n  Lange, Bo Thiesson", "title": "Early detection of sepsis utilizing deep learning on electronic health\n  record event sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The timeliness of detection of a sepsis event in progress is a crucial factor\nin the outcome for the patient. Machine learning models built from data in\nelectronic health records can be used as an effective tool for improving this\ntimeliness, but so far the potential for clinical implementations has been\nlargely limited to studies in intensive care units. This study will employ a\nricher data set that will expand the applicability of these models beyond\nintensive care units. Furthermore, we will circumvent several important\nlimitations that have been found in the literature: 1) Models are evaluated\nshortly before sepsis onset without considering interventions already\ninitiated. 2) Machine learning models are built on a restricted set of clinical\nparameters, which are not necessarily measured in all departments. 3) Model\nperformance is limited by current knowledge of sepsis, as feature interactions\nand time dependencies are hardcoded into the model. In this study, we present a\nmodel to overcome these shortcomings using a deep learning approach on a\ndiverse multicenter data set. We used retrospective data from multiple Danish\nhospitals over a seven-year period. Our sepsis detection system is constructed\nas a combination of a convolutional neural network and a long short-term memory\nnetwork. We suggest a retrospective assessment of interventions by looking at\nintravenous antibiotics and blood cultures preceding the prediction time.\nResults show performance ranging from AUROC 0.856 (3 hours before sepsis onset)\nto AUROC 0.756 (24 hours before sepsis onset). We present a deep learning\nsystem for early detection of sepsis that is able to learn characteristics of\nthe key factors and interactions from the raw event sequence data itself,\nwithout relying on a labor-intensive feature extraction work.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 08:38:02 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Lauritsen", "Simon Meyer", ""], ["Kal\u00f8r", "Mads Ellersgaard", ""], ["Kongsgaard", "Emil Lund", ""], ["Lauritsen", "Katrine Meyer", ""], ["J\u00f8rgensen", "Marianne Johansson", ""], ["Lange", "Jeppe", ""], ["Thiesson", "Bo", ""]]}, {"id": "1906.03052", "submitter": "S. Jalil Kazemitabar", "authors": "S. Jalil Kazemitabar, Arash A. Amini", "title": "Approximate Identification of the Optimal Epidemic Source in Complex\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the source of an epidemic, spreading\nthrough a network, from a complete observation of the infected nodes in a\nsnapshot of the network. Previous work on the problem has often employed\ngeometric, spectral or heuristic approaches to identify the source, with the\ntrees being the most studied network topology. We take a fully statistical\napproach and derive novel recursions to compute the Bayes optimal solution,\nunder a susceptible-infected (SI) epidemic model. Our analysis is time and rate\nindependent, and holds for general network topologies. We then provide two\ntractable algorithms for solving these recursions, a mean-field approximation\nand a greedy approach, and evaluate their performance on real and synthetic\nnetworks. Real networks are far from tree-like and an emphasis will be given to\nnetworks with high transitivity, such as social networks and those with\ncommunities. We show that on such networks, our approaches significantly\noutperform geometric and spectral centrality measures, most of which perform no\nbetter than random guessing. Both the greedy and mean-field approximation are\nscalable to large sparse networks.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 12:37:57 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 05:58:05 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 08:22:09 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Kazemitabar", "S. Jalil", ""], ["Amini", "Arash A.", ""]]}, {"id": "1906.03105", "submitter": "Giorgio Corani", "authors": "Giorgio Corani, Dario Azzimonti, Jo\\~ao P. S. C. Augusto and Marco\n  Zaffalon", "title": "Reconciling Hierarchical Forecasts via Bayes' Rule", "comments": null, "journal-ref": "ECML PKDD 2020: Proc. Machine Learning and Knowledge Discovery in\n  Databases, 211 - 226", "doi": "10.1007/978-3-030-67664-3_13", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for reconciling hierarchical forecasts, based on\nBayes rule. We define a prior distribution for the bottom time series of the\nhierarchy, based on the bottom base forecasts. Then we update their\ndistribution via Bayes rule, based on the base forecasts for the upper time\nseries. Under the Gaussian assumption, we derive the updating in closed-form.\nWe derive two algorithms, which differ as for the assumed independencies. We\ndiscuss their relation with the MinT reconciliation algorithm and with the\nKalman filter, and we compare them experimentally.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 13:57:02 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 14:24:47 GMT"}, {"version": "v3", "created": "Mon, 2 Dec 2019 09:28:23 GMT"}, {"version": "v4", "created": "Thu, 5 Dec 2019 15:22:52 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2020 11:53:28 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Corani", "Giorgio", ""], ["Azzimonti", "Dario", ""], ["Augusto", "Jo\u00e3o P. S. C.", ""], ["Zaffalon", "Marco", ""]]}, {"id": "1906.03161", "submitter": "Virginia Aglietti", "authors": "Virginia Aglietti, Edwin V. Bonilla, Theodoros Damoulas, Sally Cripps", "title": "Structured Variational Inference in Continuous Cox Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable framework for inference in an inhomogeneous Poisson\nprocess modeled by a continuous sigmoidal Cox process that assumes the\ncorresponding intensity function is given by a Gaussian process (GP) prior\ntransformed with a scaled logistic sigmoid function. We present a tractable\nrepresentation of the likelihood through augmentation with a superposition of\nPoisson processes. This view enables a structured variational approximation\ncapturing dependencies across variables in the model. Our framework avoids\ndiscretization of the domain, does not require accurate numerical integration\nover the input space and is not limited to GPs with squared exponential\nkernels. We evaluate our approach on synthetic and real-world data showing that\nits benefits are particularly pronounced on multivariate input settings where\nit overcomes the limitations of mean-field methods and sampling schemes. We\nprovide the state of-the-art in terms of speed, accuracy and uncertainty\nquantification trade-offs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:31:02 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Aglietti", "Virginia", ""], ["Bonilla", "Edwin V.", ""], ["Damoulas", "Theodoros", ""], ["Cripps", "Sally", ""]]}, {"id": "1906.03178", "submitter": "Paul Sharkey", "authors": "Paul Sharkey, Jonathan A. Tawn, Simon J. Brown", "title": "Modelling the spatial extent and severity of extreme European windstorms", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Windstorms are a primary natural hazard affecting Europe that are commonly\nlinked to substantial property and infrastructural damage and are responsible\nfor the largest spatially aggregated financial losses. Such extreme winds are\ntypically generated by extratropical cyclone systems originating in the North\nAtlantic and passing over Europe. Previous statistical studies tend to model\nextreme winds at a given set of sites, corresponding to inference in a Eulerian\nframework. Such inference cannot incorporate knowledge of the life cycle and\nprogression of extratropical cyclones across the region and is forced to make\nrestrictive assumptions about the extremal dependence structure. We take an\nentirely different approach which overcomes these limitations by working in a\nLagrangian framework. Specifically, we model the development of windstorms over\ntime, preserving the physical characteristics linking the windstorm and the\ncyclone track, the path of local vorticity maxima, and make a key finding that\nthe spatial extent of extratropical windstorms becomes more localised as its\nmagnitude increases irrespective of the location of the storm track. Our model\nallows simulation of synthetic windstorm events to derive the joint\ndistributional features over any set of sites giving physically consistent\nextrapolations to rarer events. From such simulations improved estimates of\nthis hazard can be achieved both in terms of intensity and area affected.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 15:45:11 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Sharkey", "Paul", ""], ["Tawn", "Jonathan A.", ""], ["Brown", "Simon J.", ""]]}, {"id": "1906.03182", "submitter": "Yonggen Zhang", "authors": "Yonggen Zhang, Marcel G. Schaap, and Zhongwang Wei", "title": "Hierarchical Multimodel Ensemble Estimates of Soil Water Retention with\n  Global Coverage", "comments": null, "journal-ref": null, "doi": "10.1029/2020GL088819", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A correct quantification of mass and energy exchange processes among land\nsurface and atmosphere requires an accurate description of unsaturated soil\nhydraulic properties. Soil pedotransfer functions (PTFs) have been widely used\nto predict soil hydraulic parameters. Here, 13 PTFs were grouped according to\ninput data requirements and evaluated against a well-documented soil database\nwith global coverage. Weighted ensembles (calibrated by four groups and the\nfull 13-member set of PTFs) were shown to have improved performance over\nindividual PTFs in terms of root mean square error and other model selection\ncriteria. Global maps of soil water retention data from the ensemble models as\nwell as their uncertainty were provided. These maps demonstrate that five PTF\nensembles tend to have different estimates, especially in middle and high\nlatitudes in the Northern Hemisphere. Our full 13-member ensemble model\nprovides more accurate estimates than PTFs that are currently being used in\nearth system models.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 08:42:04 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Zhang", "Yonggen", ""], ["Schaap", "Marcel G.", ""], ["Wei", "Zhongwang", ""]]}, {"id": "1906.03183", "submitter": "Amir Karami", "authors": "Amir Karami, Mehdi Ghasemi, Souvik Sen, Marcos Moraes, Vishal Shah", "title": "Exploring Diseases and Syndromes in Neurology Case Reports from 1955 to\n  2017 with Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CL cs.IR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: A large number of neurology case reports have been published, but\nit is a challenging task for human medical experts to explore all of these\npublications. Text mining offers a computational approach to investigate\nneurology literature and capture meaningful patterns. The overarching goal of\nthis study is to provide a new perspective on case reports of neurological\ndisease and syndrome analysis over the last six decades using text mining.\n  Methods: We extracted diseases and syndromes (DsSs) from more than 65,000\nneurology case reports from 66 journals in PubMed over the last six decades\nfrom 1955 to 2017. Text mining was applied to reports on the detected DsSs to\ninvestigate high-frequency DsSs, categorize them, and explore the linear trends\nover the 63-year time frame.\n  Results: The text mining methods explored high-frequency neurologic DsSs and\ntheir trends and the relationships between them from 1955 to 2017. We detected\nmore than 18,000 unique DsSs and found 10 categories of neurologic DsSs. While\nthe trend analysis showed the increasing trends in the case reports for top-10\nhigh-frequency DsSs, the categories had mixed trends.\n  Conclusion: Our study provided new insights into the application of text\nmining methods to investigate DsSs in a large number of medical case reports\nthat occur over several decades. The proposed approach can be used to provide a\nmacro level analysis of medical literature by discovering interesting patterns\nand tracking them over several years to help physicians explore these case\nreports more efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:38:06 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Karami", "Amir", ""], ["Ghasemi", "Mehdi", ""], ["Sen", "Souvik", ""], ["Moraes", "Marcos", ""], ["Shah", "Vishal", ""]]}, {"id": "1906.03187", "submitter": "Malka Gorfine", "authors": "Malka Gorfine, Nir Keret, Asaf Ben Arie, David Zucker, Li Hsu", "title": "Marginalized Frailty-Based Illness-Death Model: Application to the\n  UK-Biobank Survival Data", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1831922", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The UK Biobank is a large-scale health resource comprising genetic,\nenvironmental and medical information on approximately 500,000 volunteer\nparticipants in the UK, recruited at ages 40--69 during the years 2006--2010.\nThe project monitors the health and well-being of its participants. This work\ndemonstrates how these data can be used to estimate in a semi-parametric\nfashion the effects of genetic and environmental risk factors on the hazard\nfunctions of various diseases, such as colorectal cancer. An illness-death\nmodel is adopted, which inherently is a semi-competing risks model, since death\ncan censor the disease, but not vice versa. Using a shared-frailty approach to\naccount for the dependence between time to disease diagnosis and time to death,\nwe provide a new illness-death model that assumes Cox models for the marginal\nhazard functions. The recruitment procedure used in this study introduces\ndelayed entry to the data. An additional challenge arising from the recruitment\nprocedure is that information coming from both prevalent and incident cases\nmust be aggregated. Lastly, we do not observe any deaths prior to the minimal\nrecruitment age, 40. In this work we provide an estimation procedure for our\nnew illness-death model that overcomes all the above challenges.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 22:55:58 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gorfine", "Malka", ""], ["Keret", "Nir", ""], ["Arie", "Asaf Ben", ""], ["Zucker", "David", ""], ["Hsu", "Li", ""]]}, {"id": "1906.03224", "submitter": "Ishfaq Shah Syed", "authors": "Ishfaq Shah Ahmad, Anwar Hassan, Peer Bilal Ahmad", "title": "Negative binomial-reciprocal inverse Gaussian distribution: Statistical\n  properties with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new three parameter distribution by compounding\nnegative binomial with reciprocal inverse Gaussian model called negative\nbinomial-reciprocal inverse Gaussian distribution. This model is tractable with\nsome important properties not only in actuarial science but in other fields as\nwell where overdispersion pattern is seen. Some basic properties including\nrecurrence relation of probabilities for computation of successive\nprobabilities have been discussed. In its compound version, when the claims are\nabsolutely continuous, an integral equation for the probability density\nfunction is discussed. Brief discussion about extension of univariate version\nhave also been done to its respective multivariate version. parameters involved\nin the model have been estimated by Maximum Likelihood Estimation technique.\nApplications of the proposed distribution are carried out by taking two real\ncount data sets. The result shown that the negative binomial-reciprocal inverse\nGaussian distribution gives better fit when compared to the Poisson and\nnegative binomial distributions.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:44:22 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ahmad", "Ishfaq Shah", ""], ["Hassan", "Anwar", ""], ["Ahmad", "Peer Bilal", ""]]}, {"id": "1906.03226", "submitter": "Abhishek Srivastava", "authors": "Dhruv Agarwal, Abhishek Srivastava and Edward W Huang", "title": "Predicting Onset of Dementia in Parkinson's Disease Patients", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alzheimer's disease (AD) and Parkinson's disease (PD) are the two most common\nneurodegenerative disorders in humans. Because a significant percentage of\npatients have clinical and pathological features of both diseases, it has been\nhypothesized that the patho-cascades of the two diseases overlap. Despite this\nevidence, these two diseases are rarely studied in a joint manner. In this\npaper, we utilize clinical, imaging, genetic, and biospecimen features to\ncluster AD and PD patients into the same feature space. By training a machine\nlearning classifier on the combined feature space, we predict the disease stage\nof patients two years after their baseline visits. We observed a considerable\nimprovement in the prediction accuracy of Parkinson's dementia patients due to\ncombined training on Alzheimer's and Parkinson's patients, thereby affirming\nthe claim that these two diseases can be jointly studied.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 00:49:24 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Agarwal", "Dhruv", ""], ["Srivastava", "Abhishek", ""], ["Huang", "Edward W", ""]]}, {"id": "1906.03339", "submitter": "Konstantinos Pelechrinis", "authors": "Sarah Mallepalle, Ron Yurko, Konstantinos Pelechrinis, Samuel L.\n  Ventura", "title": "next-gen-scraPy: Extracting NFL Tracking Data from Images to Evaluate\n  Quarterbacks and Pass Defenses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The NFL collects detailed tracking data capturing the location of all players\nand the ball during each play. Although the raw form of this data is not\npublicly available, the NFL releases a set of aggregated statistics via their\nNext Gen Stats (NGS) platform. They also provide charts showing the locations\nof pass attempts and outcomes for individual quarterbacks. Our work aims to\npartially close the gap between what data is available privately (to NFL teams)\nand publicly, and our contribution is twofold. First, we introduce an image\nprocessing tool designed specifically for extracting the raw data from the NGS\npass charts. We extract the pass outcome, coordinates, and other metadata.\nSecond, we analyze the resulting dataset, examining the spatial tendencies and\nperformances of individual quarterbacks and defenses. We use a generalized\nadditive model for completion percentages by field location. We introduce a\nNaive Bayes approach for estimating the 2-D completion percentage surfaces of\nindividual teams and quarterbacks, and we provide a one-number summary,\ncompletion percentage above expectation (CPAE), for evaluating quarterbacks and\nteam defenses. We find that our pass location data closely matches the NFL's\ntracking data, and that our CPAE metric closely matches the NFL's proprietary\nCPAE metric.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jun 2019 21:38:35 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 03:07:27 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Mallepalle", "Sarah", ""], ["Yurko", "Ron", ""], ["Pelechrinis", "Konstantinos", ""], ["Ventura", "Samuel L.", ""]]}, {"id": "1906.03564", "submitter": "Roberto Rivera", "authors": "Roberto Rivera", "title": "A Low Rank Gaussian Process Prediction Model for Very Large Datasets", "comments": null, "journal-ref": "In 2015 IEEE First International Conference on Big Data Computing\n  Service and Applications (pp. 308-313). IEEE", "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial prediction requires expensive computation to invert the spatial\ncovariance matrix it depends on and also has considerable storage needs. This\nwork concentrates on computationally efficient algorithms for prediction using\nvery large datasets. A recent prediction model for spatial data known as Fixed\nRank Kriging is much faster than the kriging and can be easily implemented with\nless assumptions about the process. However, Fixed Rank Kriging requires the\nestimation of a matrix which must be positive definite and the original\nestimation procedure cannot guarantee this property. We present a result that\nshows when a matrix subtraction of a given form will give a positive definite\nmatrix. Motivated by this result, we present an iterative Fixed Rank Kriging\nalgorithm that ensures positive definiteness of the matrix required for\nprediction and show that under mild conditions the algorithm numerically\nconverges. The modified Fixed Rank Kriging procedure is implemented to predict\nmissing chlorophyll observations for very large regions of ocean color.\nPredictions are compared to those made by other well known methods of spatial\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 04:37:53 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""]]}, {"id": "1906.03604", "submitter": "Jae Youn Ahn", "authors": "Rosy Oh, Jae Youn Ahn, Woojoo Lee", "title": "On Copula-based Collective Risk Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several collective risk models have recently been proposed by relaxing the\nwidely used but controversial assumption of independence between claim\nfrequency and severity. Approaches include the bivariate copula model, random\neffect model, and two-part frequency-severity model. This study focuses on the\ncopula approach to develop collective risk models that allow a flexible\ndependence structure for frequency and severity. We first revisit the bivariate\ncopula method for frequency and average severity. After examining the inherent\ndifficulties of the bivariate copula model, we alternatively propose modeling\nthe dependence of frequency and individual severities using multivariate\nGaussian and t-copula functions. The proposed copula models have computational\nadvantages and provide intuitive interpretations for the dependence structure.\nOur analytical findings are illustrated by analyzing automobile insurance data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 09:25:39 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Oh", "Rosy", ""], ["Ahn", "Jae Youn", ""], ["Lee", "Woojoo", ""]]}, {"id": "1906.03661", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Junhao Xiong, Cencheng Shen, Jes\\\"us Arroyo, Joshua T. Vogelstein", "title": "Graph Independence Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying statistically significant dependency between variables is a key\nstep in scientific discoveries. Many recent methods, such as distance and\nkernel tests, have been proposed for valid and consistent independence testing\nand can be applied to data in Euclidean and non-Euclidean spaces. However, in\nthose works, $n$ pairs of points in $\\mathcal{X} \\times \\mathcal{Y}$ are\nobserved. Here, we consider the setting where a pair of $n \\times n$ graphs are\nobserved, and the corresponding adjacency matrices are treated as kernel\nmatrices. Under a $\\rho$-correlated stochastic block model, we demonstrate that\na na\\\"ive test (permutation and Pearson's) for a conditional dependency graph\nmodel is invalid. Instead, we propose a block-permutation procedure. We prove\nthat our procedure is valid and consistent -- even when the two graphs have\ndifferent marginal distributions, are weighted or unweighted, and the latent\nvertex assignments are unknown -- and provide sufficient conditions for the\ntests to estimate $\\rho$. Simulations corroborate these results on both binary\nand weighted graphs. Applying these tests to the whole-organism,\nsingle-cell-resolution structural connectomes of C. elegans, we identify strong\nstatistical dependency between the chemical synapse connectome and the gap\njunction connectome.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 15:40:23 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:51:44 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Xiong", "Junhao", ""], ["Shen", "Cencheng", ""], ["Arroyo", "Jes\u00fcs", ""], ["Vogelstein", "Joshua T.", ""]]}, {"id": "1906.03714", "submitter": "Roberto Rivera", "authors": "Roberto Rivera and Wolfgang Rolke", "title": "Modeling Excess Deaths After a Natural Disaster with Application to\n  Hurricane Maria", "comments": "(accepted)", "journal-ref": "Statistics in Medicine 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of excess deaths due to a natural disaster is an important public\nhealth problem. The CDC provides guidelines to fill death certificates to help\ndetermine the death toll of such events. But, even when followed by medical\nexaminers, the guidelines can not guarantee a precise calculation of excess\ndeaths.%particularly due to the ambiguity of indirect deaths. We propose two\nmodels to estimate excess deaths due to an emergency. The first model is\nsimple, permitting excess death estimation with little data through a profile\nlikelihood method. The second model is more flexible, incorporating: temporal\nvariation, covariates, and possible population displacement; while allowing\ninference on how the emergency's effect changes with time. The models are\nimplemented to build confidence intervals estimating Hurricane Maria's death\ntoll.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jun 2019 21:28:01 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""], ["Rolke", "Wolfgang", ""]]}, {"id": "1906.03751", "submitter": "Qingsong Wen", "authors": "Qingsong Wen, Jingkun Gao, Xiaomin Song, Liang Sun, Jian Tan", "title": "RobustTrend: A Huber Loss with a Combined First and Second Order\n  Difference Regularization for Time Series Trend Filtering", "comments": "Accepted to the 28th International Joint Conference on Artificial\n  Intelligence (IJCAI 2019), 7 pages. v2: added related references and adjusted\n  font size in figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting the underlying trend signal is a crucial step to facilitate time\nseries analysis like forecasting and anomaly detection. Besides noise signal,\ntime series can contain not only outliers but also abrupt trend changes in\nreal-world scenarios. To deal with these challenges, we propose a robust trend\nfiltering algorithm based on robust statistics and sparse learning.\nSpecifically, we adopt the Huber loss to suppress outliers, and utilize a\ncombination of the first order and second order difference on the trend\ncomponent as regularization to capture both slow and abrupt trend changes.\nFurthermore, an efficient method is designed to solve the proposed robust trend\nfiltering based on majorization minimization (MM) and alternative direction\nmethod of multipliers (ADMM). We compared our proposed robust trend filter with\nother nine state-of-the-art trend filtering algorithms on both synthetic and\nreal-world datasets. The experiments demonstrate that our algorithm outperforms\nexisting methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 01:00:36 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 00:48:34 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Wen", "Qingsong", ""], ["Gao", "Jingkun", ""], ["Song", "Xiaomin", ""], ["Sun", "Liang", ""], ["Tan", "Jian", ""]]}, {"id": "1906.03762", "submitter": "Roberto Rivera", "authors": "Roberto Rivera and Mario Marazzi and Pedro Torres", "title": "Incorporating Open Data into Introductory Courses in Statistics", "comments": "accepted", "journal-ref": "Journal of Statistics Education 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2016 Guidelines for Assessment and Instruction in Statistics Education\n(GAISE) College Report emphasized six recommendations to teach introductory\ncourses in statistics. Among them: use of real data with context and purpose.\nMany educators have created databases consisting of multiple data sets for use\nin class; sometimes making hundreds of data sets available. Yet `the context\nand purpose' component of the data may remain elusive if just a generic\ndatabase is made available.\n  We describe the use of open data in introductory courses. Countries and\ncities continue to share data through open data portals. Hence, educators can\nfind regional data that engages their students more effectively. We present\nexcerpts from case studies that show the application of statistical methods to\ndata on: crime, housing, rainfall, tourist travel, and others. Data wrangling\nand discussion of results are recognized as important case study components.\nThus the open data based case studies attend most GAISE College Report\nrecommendations. Reproducible \\textsf{R} code is made available for each case\nstudy. Example uses of open data in more advanced courses in statistics are\nalso described.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 01:49:56 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Rivera", "Roberto", ""], ["Marazzi", "Mario", ""], ["Torres", "Pedro", ""]]}, {"id": "1906.03828", "submitter": "Dan Li", "authors": "Dan Li and Adam Clements and Christopher Drovandi", "title": "Efficient Bayesian estimation for GARCH-type models via Sequential Monte\n  Carlo", "comments": "Minor revisions; replaced the normal innovation of GARCH and\n  GJR-GARCH model with a Student-t innovation; some updates to the results\n  based on Student-t GARCH and GJR-GARCH (Section 6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advantages of sequential Monte Carlo (SMC) are exploited to develop\nparameter estimation and model selection methods for GARCH (Generalized\nAutoRegressive Conditional Heteroskedasticity) style models. It provides an\nalternative method for quantifying estimation uncertainty relative to classical\ninference. Even with long time series, it is demonstrated that the posterior\ndistribution of model parameters are non-normal, highlighting the need for a\nBayesian approach and an efficient posterior sampling method. Efficient\napproaches for both constructing the sequence of distributions in SMC, and\nleave-one-out cross-validation, for long time series data are also proposed.\nFinally, an unbiased estimator of the likelihood is developed for the Bad\nEnvironment-Good Environment model, a complex GARCH-type model, which permits\nexact Bayesian inference not previously available in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 07:59:26 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2020 10:05:06 GMT"}], "update_date": "2020-03-06", "authors_parsed": [["Li", "Dan", ""], ["Clements", "Adam", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1906.03846", "submitter": "Oliver Stoner", "authors": "Oliver Stoner and Theo Economou", "title": "An Advanced Hidden Markov Model for Hourly Rainfall Time Series", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.csda.2020.107045", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For hydrological applications, such as urban flood modelling, it is often\nimportant to be able to simulate sub-daily rainfall time series from stochastic\nmodels. However, modelling rainfall at this resolution poses several\nchallenges, including a complex temporal structure including long dry periods,\nseasonal variation in both the occurrence and intensity of rainfall, and\nextreme values.\n  We illustrate how the hidden Markov framework can be adapted to construct a\ncompelling model for sub-daily rainfall, which is capable of capturing all of\nthese important characteristics well. These adaptations include clone states\nand non-stationarity in both the transition matrix and conditional models. Set\nin the Bayesian framework, a rich quantification of both parametric and\npredictive uncertainty is available, and thorough model checking is made\npossible through posterior predictive analyses. Results from the model are\ninterpretable, allowing for meaningful examination of seasonal variation and\nmedium to long term trends in rainfall occurrence and intensity. To demonstrate\nthe effectiveness of our approach, both in terms of model fit and\ninterpretability, we apply the model to an 8-year long time series of hourly\nobservations.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 08:51:00 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2019 15:13:01 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Stoner", "Oliver", ""], ["Economou", "Theo", ""]]}, {"id": "1906.04025", "submitter": "Chia-Yen Lee", "authors": "Chia-Yen Lee, Chen-Fu Chien", "title": "Pitfalls and Protocols in Practice of Manufacturing Data Science", "comments": null, "journal-ref": null, "doi": "10.1007/s10845-020-01711-w", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practical application of machine learning and data science (ML/DS)\ntechniques present a range of procedural issues to be examined and resolve\nincluding those relating to the data issues, methodologies, assumptions, and\napplicable conditions. Each of these issues can present difficulties in\npractice; particularly, associated with the manufacturing characteristics and\ndomain knowledge. The purpose of this paper is to highlight some of the\npitfalls that have been identified in real manufacturing application under each\nof these headings and to suggest protocols to avoid the pitfalls and guide the\npractical applications of the ML/DS methodologies from predictive analytics to\nprescriptive analytics.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 14:37:54 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:15:24 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Lee", "Chia-Yen", ""], ["Chien", "Chen-Fu", ""]]}, {"id": "1906.04116", "submitter": "Stephen Watts Prof.", "authors": "S. J. Watts, L. Crow", "title": "Big Variates: Visualizing and identifying key variables in a\n  multivariate world", "comments": "16 Pages, 7 Figures. Pre-print from talk at ULITIMA 2018, Argonne\n  National Laboratory, 11-14 September 2018", "journal-ref": null, "doi": "10.1016/j.nima.2019.06.060", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data involves both a large number of events but also many variables. This\npaper will concentrate on the challenge presented by the large number of\nvariables in a Big Dataset. It will start with a brief review of exploratory\ndata visualisation for large dimensional datasets and the use of parallel\ncoordinates. This motivates the use of information theoretic ideas to\nunderstand multivariate data. Two key information-theoretic statistics\n(Similarity Index and Class Distance Indicator) will be described which are\nused to identify the key variables and then guide the user in a subsequent\nmachine learning analysis. Key to the approach is a novel algorithm to\nhistogram data which quantifies the information content of the data. The Class\nDistance Indicator also sets a limit on the classification performance of\nmachine learning algorithms for the specific dataset.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:45:20 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Watts", "S. J.", ""], ["Crow", "L.", ""]]}, {"id": "1906.04119", "submitter": "Dirk Tasche", "authors": "Dirk Tasche", "title": "Confidence intervals for class prevalences under prior probability shift", "comments": "28 pages, 1 figure, 5 tables", "journal-ref": "Machine Learning and Knowledge Extraction 1, 805-831, 2019", "doi": "10.3390/make1030047", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point estimation of class prevalences in the presence of data set shift has\nbeen a popular research topic for more than two decades. Less attention has\nbeen paid to the construction of confidence and prediction intervals for\nestimates of class prevalences. One little considered question is whether or\nnot it is necessary for practical purposes to distinguish confidence and\nprediction intervals. Another question so far not yet conclusively answered is\nwhether or not the discriminatory power of the classifier or score at the basis\nof an estimation method matters for the accuracy of the estimates of the class\nprevalences. This paper presents a simulation study aimed at shedding some\nlight on these and other related questions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 16:50:08 GMT"}, {"version": "v2", "created": "Fri, 14 Jun 2019 15:03:11 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Tasche", "Dirk", ""]]}, {"id": "1906.04242", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Rocio Titiunik, Gonzalo Vazquez-Bare", "title": "The Regression Discontinuity Design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This handbook chapter gives an introduction to the sharp regression\ndiscontinuity design, covering identification, estimation, inference, and\nfalsification methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:20:25 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 17:22:12 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Titiunik", "Rocio", ""], ["Vazquez-Bare", "Gonzalo", ""]]}, {"id": "1906.04246", "submitter": "Mark Neuman", "authors": "Mark D. Neuman, Sean Hennessy, Dylan Small, Colleen Brensinger, Craig\n  Newcomb, Lakisha Gaskins, Duminda Wijeysundera, Brian T. Bateman, Hannah\n  Wunsch", "title": "Technical Preprint: Rationale and Design of a Planned Observational\n  Study to Evaluate the Impact of Hydrocodone Rescheduling on Opioid\n  Prescribing After Surgery", "comments": "Updated version 6/12/2019: Figure 2 corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In October 2014, the US Drug Enforcement Agency (DEA) reclassified\nhydrocodone from Schedule III to Schedule II of the Controlled Substances Act,\nresulting in a prohibition on refills in the initial prescription. While this\nschedule change was associated with overall decreases in the rate of filled\nhydrocodone prescriptions and opioid dispensing, available studies conflict\nregarding its impact on acute opioid prescribing among surgical patients. Here,\nwe present the rationale and design of a planned study to measure the effect of\nhydrocodone rescheduling using a difference-in-differences design that\nleverages anticipated variation in the relative impact of this policy on\npatients treated by surgeons that more or less frequently prescribed\nhydrocodone products versus other opioids prior to the schedule change.\nAdditionally, we present findings from preliminary study conducted on a subset\nof our full planned sample to assess for potential differences in outcome\ntrends over the 3 years prior to rescheduling among patients treated by\nsurgeons who commonly prescribed hydrocodone versus those treated by surgeons\nwho rarely prescribed hydrocodone.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 19:30:16 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 12:57:49 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Neuman", "Mark D.", ""], ["Hennessy", "Sean", ""], ["Small", "Dylan", ""], ["Brensinger", "Colleen", ""], ["Newcomb", "Craig", ""], ["Gaskins", "Lakisha", ""], ["Wijeysundera", "Duminda", ""], ["Bateman", "Brian T.", ""], ["Wunsch", "Hannah", ""]]}, {"id": "1906.04296", "submitter": "Lamin Juwara", "authors": "Lamin Juwara, Jennifer Boateng", "title": "Assessing the effects of exposure to sulfuric acid aerosol on\n  respiratory function in adults", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sulfuric acid aerosol is suspected to be a major contributor to mortality and\nmorbidity associated with air pollution. The objective of the study is to\ndetermine if exposure of human participants to anticipated levels of sulfuric\nacid aerosol ($\\sim 100\\mu g/m^3 $) in the near future would have an adverse\neffect on respiratory function. We used data from 28 adults exposed to sulfuric\nacid for 4 hours in a controlled exposure chamber over a 3 day period with\nrepeated measures of pulmonary function (FEV1) recorded at 2-hour intervals.\nMeasurements were also recorded after 2 and 24 hours post exposure. We\nformulated a linear mixed effect model for FEV1 with fixed effects (day of\ntreatment, hour, day-hour interaction, and smoking status), a random intercept\nand an AR1 covariance structure to estimate the effect of aerosol exposure on\nFEV1. We further assessed whether smoking status modified the exposure effects\nand compared the analysis to the method used by Kerr et al.,1981. The findings\nof the study show that the effect of day 3 exposure is negatively associated\nwith lung function (coefficient ($\\beta$), -0.08; 95% CI, -0.16 to -0.01). A\nweak negative association is also observed with increasing hours of exposure\n($\\beta$, -0.01; 95% CI, -0.03 to 0.00). Among the smokers, we found a\nsignificant negative association with hours of exposure ($\\beta$, -0.02; 95%\nCI, -0.03 to -0.00), day 3 exposure ($\\beta$, -0.11; 95% CI, -0.14 to -0.02)\nand a borderline adverse effect for day 2 treatment ($\\beta$, -0.06; 95% CI,\n-0.14 to 0.03) whilst no significant association was observed for nonsmokers.\nIn conclusion, anticipated deposits of sulfuric acid aerosol in the near would\nadversely affect respiratory function. The effect observed in smokers is\nsignificantly more adverse than in nonsmokers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 22:07:29 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 02:37:25 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2019 00:16:18 GMT"}, {"version": "v4", "created": "Mon, 24 Jun 2019 18:47:57 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Juwara", "Lamin", ""], ["Boateng", "Jennifer", ""]]}, {"id": "1906.04538", "submitter": "M{\\aa}ns Karlsson", "authors": "M{\\aa}ns Karlsson and Ola H\\\"ossjer", "title": "Identification of taxon through classification with partial reject\n  options", "comments": "About half are appendices, which contains mathematical details", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of taxa can significantly be assisted by statistical\nclassification based on trait measurements in two major ways; either\nindividually or by phylogenetic (clustering) methods. In this paper we present\na general Bayesian approach for classifying species individually based on\nmeasurements of a mixture of continuous and ordinal traits as well as any type\nof covariates. It is assumed that the trait vector is derived from a latent\nvariable with a multivariate Gaussian distribution. Decision rules based on\nsupervised learning are presented that estimate model parameters through\nblockwise Gibbs sampling. These decision regions allow for uncertainty (partial\nrejection), so that not necessarily one specific category (taxon) is output\nwhen new subjects are classified, but rather a set of categories including the\nmost probable taxa. This type of discriminant analysis employs reward functions\nwith a set-valued input argument, so that an optimal Bayes classifier can be\ndefined. We also present a way of safeguarding against outlying new\nobservations, using an analogue of a $p$-value within our Bayesian setting. Our\nmethod is illustrated on an original ornithological data set of birds. We also\nincorporate model selection through cross-validation, examplified on another\noriginal data set of birds.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 12:52:20 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 14:58:16 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 16:25:42 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Karlsson", "M\u00e5ns", ""], ["H\u00f6ssjer", "Ola", ""]]}, {"id": "1906.04582", "submitter": "Mark He", "authors": "Mark He, Joseph Glasser, Shankar Bhamidi, Nikhil Kaza", "title": "Intertemporal Community Detection in Human Mobility Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a community detection method that finds clusters in network\ntime-series by introducing an algorithm that finds significantly interconnected\nnodes across time. These connections are either increasing, decreasing, or\nconstant over time. Significance of nodal connectivity within a set is judged\nusing the Weighted Configuration Null Model at each time-point, then a novel\nsignificance-testing scheme is used to assess connectivity at all time points\nand the direction of its time-trend. We apply this method to bikeshare networks\nin New York City and Chicago and taxicab pickups and dropoffs in New York to\nfind and illustrate patterns in human mobility in urban zones. Results show\nstark geographical patterns in clusters that are growing and declining in\nrelative usage across time and potentially elucidate latent economic or\ndemographic trends.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jun 2019 17:05:33 GMT"}, {"version": "v2", "created": "Sun, 10 Nov 2019 23:08:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 02:38:19 GMT"}, {"version": "v4", "created": "Fri, 3 Apr 2020 18:47:06 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["He", "Mark", ""], ["Glasser", "Joseph", ""], ["Bhamidi", "Shankar", ""], ["Kaza", "Nikhil", ""]]}, {"id": "1906.04613", "submitter": "Alfredo Cartone Dr", "authors": "Alfredo Cartone, Geoffrey JD Hewings, Paolo Postiglione", "title": "Regional economic convergence and spatial quantile regression", "comments": "Draft version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The presence of \\b{eta}-convergence in European regions is an important issue\nto be analyzed. In this paper, we adopt a quantile regression approach in\nanalyzing economic convergence. While previous work has performed quantile\nregression at the national level, we focus on 187 European NUTS2 regions for\nthe period 1981-2009 and use spatial quantile regression to account for spatial\ndependence.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 14:16:44 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Cartone", "Alfredo", ""], ["Hewings", "Geoffrey JD", ""], ["Postiglione", "Paolo", ""]]}, {"id": "1906.04668", "submitter": "Fernando Alarid-Escudero", "authors": "Fernando Alarid-Escudero, Amy B. Knudsen, Jonathan Ozik, Nicholson\n  Collier, Karen M. Kuntz", "title": "Characterization and valuation of uncertainty of calibrated parameters\n  in stochastic decision models", "comments": "17 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluated the implications of different approaches to characterize\nuncertainty of calibrated parameters of stochastic decision models (DMs) in the\nquantified value of such uncertainty in decision making. We used a\nmicrosimulation DM of colorectal cancer (CRC) screening to conduct a\ncost-effectiveness analysis (CEA) of a 10-year colonoscopy screening. We\ncalibrated the natural history model of CRC to epidemiological data with\ndifferent degrees of uncertainty and obtained the joint posterior distribution\nof the parameters using a Bayesian approach. We conducted a probabilistic\nsensitivity analysis (PSA) on all the model parameters with different\ncharacterizations of uncertainty of the calibrated parameters and estimated the\nvalue of uncertainty of the different characterizations with a value of\ninformation analysis. All analyses were conducted using high performance\ncomputing resources running the Extreme-scale Model Exploration with Swift\n(EMEWS) framework. The posterior distribution had high correlation among some\nparameters. The parameters of the Weibull hazard function for the age of onset\nof adenomas had the highest posterior correlation of -0.958. Considering full\nposterior distributions and the maximum-a-posteriori estimate of the calibrated\nparameters, there is little difference on the spread of the distribution of the\nCEA outcomes with a similar expected value of perfect information (EVPI) of\n\\$653 and \\$685, respectively, at a WTP of \\$66,000/QALY. Ignoring correlation\non the posterior distribution of the calibrated parameters, produced the widest\ndistribution of CEA outcomes and the highest EVPI of \\$809 at the same WTP.\nDifferent characterizations of uncertainty of calibrated parameters have\nimplications on the expect value of reducing uncertainty on the CEA. Ignoring\ninherent correlation among calibrated parameters on a PSA overestimates the\nvalue of uncertainty.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 15:47:32 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Alarid-Escudero", "Fernando", ""], ["Knudsen", "Amy B.", ""], ["Ozik", "Jonathan", ""], ["Collier", "Nicholson", ""], ["Kuntz", "Karen M.", ""]]}, {"id": "1906.04698", "submitter": "Prableen Kaur", "authors": "Prableen Kaur, Agoritsa Polyzou and George Karypis", "title": "Causal Inference in Higher Education: Building Better Curriculums", "comments": "Learning at Scale 2019 June 24-25, 2019, Chicago, IL, USA", "journal-ref": null, "doi": "10.1145/3330430.3333663", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher educational institutions constantly look for ways to meet students'\nneeds and support them through graduation. Recent work in the field of learning\nanalytics have developed methods for grade prediction and course\nrecommendations. Although these methods work well, they often fail to discover\ncausal relationships between courses, which may not be evident through\ncorrelation-based methods. In this work, we aim at understanding the causal\nrelationships between courses to aid universities in designing better academic\npathways for students and to help them make better choices. Our methodology\nemploys methods of causal inference to study these relationships using\nhistorical student performance data. We make use of a doubly-robust method of\nmatching and regression in order to obtain the casual relationship between a\npair of courses. The results were validated by the existing prerequisite\nstructure and by cross-validation of the regression model. Further, our\napproach was also tested for robustness and sensitivity to certain\nhyperparameters. This methodology shows promising results and is a step forward\ntowards building better academic pathways for students.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 16:59:42 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Kaur", "Prableen", ""], ["Polyzou", "Agoritsa", ""], ["Karypis", "George", ""]]}, {"id": "1906.04711", "submitter": "Matias Barenstein", "authors": "Matias Barenstein", "title": "ProPublica's COMPAS Data Revisited", "comments": "28 pages, 13 figures (v3); fixed figure and footnote formatting;\n  edited writing, organization, references, and appendix, results unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.CY cs.LG q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I examine the COMPAS recidivism risk score and criminal history data\ncollected by ProPublica in 2016 that fueled intense debate and research in the\nnascent field of 'algorithmic fairness'. ProPublica's COMPAS data is used in an\nincreasing number of studies to test various definitions of algorithmic\nfairness. This paper takes a closer look at the actual datasets put together by\nProPublica. In particular, the sub-datasets built to study the likelihood of\nrecidivism within two years of a defendant's original COMPAS survey screening\ndate. I take a new yet simple approach to visualize these data, by analyzing\nthe distribution of defendants across COMPAS screening dates. I find that\nProPublica made an important data processing error when it created these\ndatasets, failing to implement a two-year sample cutoff rule for recidivists in\nsuch datasets (whereas it implemented a two-year sample cutoff rule for\nnon-recidivists). When I implement a simple two-year COMPAS screen date cutoff\nrule for recidivists, I estimate that in the two-year general recidivism\ndataset ProPublica kept over 40% more recidivists than it should have. This\nfundamental problem in dataset construction affects some statistics more than\nothers. It obviously has a substantial impact on the recidivism rate;\nartificially inflating it. For the two-year general recidivism dataset created\nby ProPublica, the two-year recidivism rate is 45.1%, whereas, with the simple\nCOMPAS screen date cutoff correction I implement, it is 36.2%. Thus, the\ntwo-year recidivism rate in ProPublica's dataset is inflated by over 24%. This\nalso affects the positive and negative predictive values. On the other hand,\nthis data processing error has little impact on some of the other key\nstatistical measures, which are less susceptible to changes in the relative\nshare of recidivists, such as the false positive and false negative rates, and\nthe overall accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 17:27:25 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 17:50:08 GMT"}, {"version": "v3", "created": "Mon, 8 Jul 2019 19:11:38 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Barenstein", "Matias", ""]]}, {"id": "1906.04763", "submitter": "Daniel Manrique-Vallier", "authors": "Daniel Manrique-Vallier, Patrick Ball, David Sulmont", "title": "Estimating the Number of Fatal Victims of the Peruvian Internal Armed\n  Conflict, 1980-2000: an application of modern multi-list Capture-Recapture\n  techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate the number of fatal victims of the Peruvian internal armed\nconflict between 1980-2000 using stratified seven-list Capture-Recapture\nmethods based on Dirichlet process mixtures, which we extend to accommodate\nincomplete stratification information. We use matched data from six sources,\noriginally analyzed by the Peruvian Truth and Reconciliation Commission in\n2003, together with a new large dataset, originally published in 2006 by the\nPeruvian government. We deal with missing stratification labels by developing a\ngeneral framework and estimation methods based on MCMC sampling for jointly\nfitting generic Bayesian Capture-Recapture models and the missing labels.\nThrough a detailed exploration driven by domain-knowledge, modeling and\nrefining, with special precautions to avoid cherry-picking of results, we\narrive to a conservative posterior estimate of 58,234 (CI95% = [56,741,\n61,289]), and a more liberal estimate of 65,958 (CI95% = [61,462, 75,387])\nfatal victims. We also determine that the Shining Path guerrillas killed more\npeople than the Peruvian armed forces. We additionally explore and discuss\nestimates based on log-linear modeling and multiple-imputation. We finish by\ndiscussing several lessons learned about the use of Capture-Recapture methods\nfor estimating casualties in conflicts.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 18:27:47 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 21:27:46 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Manrique-Vallier", "Daniel", ""], ["Ball", "Patrick", ""], ["Sulmont", "David", ""]]}, {"id": "1906.04776", "submitter": "Somabha Mukherjee", "authors": "Divyansh Agarwal, Somabha Mukherjee, Bhaswar Bikram Bhattacharya,\n  Nancy Ruonan Zhang", "title": "Distribution-Free Multisample Test Based on Optimal Matching with\n  Applications to Single Cell Genomics", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a nonparametric graphical test based on optimal\nmatching, for assessing the equality of multiple unknown multivariate\nprobability distributions. Our procedure pools the data from the different\nclasses to create a graph based on the minimum non-bipartite matching, and then\nutilizes the number of edges connecting data points from different classes to\nexamine the closeness between the distributions. The proposed test is exactly\ndistribution-free (the null distribution does not depend on the distribution of\nthe data) and can be efficiently applied to multivariate as well as\nnon-Euclidean data, whenever the inter-point distances are well-defined. We\nshow that the test is universally consistent, and prove a distributional limit\ntheorem for the test statistic under general alternatives. Through simulation\nstudies, we demonstrate its superior performance against other common and\nwell-known multisample tests. In scenarios where our test suggests\ndistributional differences across classes, we also propose an approach for\nidentifying which class or group contributes to this overall difference. The\nmethod is applied to single cell transcriptomics data obtained from the\nperipheral blood, cancer tissue, and tumor-adjacent normal tissue of human\nsubjects with hepatocellular carcinoma and non-small-cell lung cancer. Our\nmethod unveils patterns in how biochemical metabolic pathways are altered\nacross immune cells in a cancer setting, depending on the tissue location. All\nof the methods described herein are implemented in the R package multicross.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 19:12:39 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Agarwal", "Divyansh", ""], ["Mukherjee", "Somabha", ""], ["Bhattacharya", "Bhaswar Bikram", ""], ["Zhang", "Nancy Ruonan", ""]]}, {"id": "1906.05087", "submitter": "Pierrick Piette", "authors": "St\\'ephane Loisel (SAF), Pierrick Piette (SAF, LPSM UMR 8001), Jason\n  Tsai", "title": "Applying economic measures to lapse risk management with machine\n  learning approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling policyholders lapse behaviors is important to a life insurer since\nlapses affect pricing, reserving, profitability, liquidity, risk management, as\nwell as the solvency of the insurer. Lapse risk is indeed the most significant\nlife underwriting risk according to European Insurance and Occupational\nPensions Authority's Quantitative Impact Study QIS5. In this paper, we\nintroduce two advanced machine learning algorithms for lapse modeling. Then we\nevaluate the performance of different algorithms by means of classical\nstatistical accuracy and profitability measure. Moreover, we adopt an\ninnovative point of view on the lapse prediction problem that comes from churn\nmanagement. We transform the classification problem into a regression question\nand then perform optimization, which is new for lapse risk management. We apply\ndifferent algorithms to a large real-world insurance dataset. Our results show\nthat XGBoost and SVM outperform CART and logistic regression, especially in\nterms of the economic validation metric. The optimization after transformation\nbrings out significant and consistent increases in economic gains.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 12:36:49 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Loisel", "St\u00e9phane", "", "SAF"], ["Piette", "Pierrick", "", "SAF, LPSM UMR 8001"], ["Tsai", "Jason", ""]]}, {"id": "1906.05150", "submitter": "Dimitris Vavoulis", "authors": "Dimitrios V Vavoulis", "title": "Exploring Bayesian approaches to eQTL mapping through probabilistic\n  programming", "comments": "25 pages, 3 figures; to appear as a book chapter in \"eQTL Analysis:\n  Methods and Protocols\", a volume for the series \"Methods in Molecular\n  Biology\" published by Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The discovery of genomic polymorphisms influencing gene expression (also\nknown as expression quantitative trait loci or eQTLs) can be formulated as a\nsparse Bayesian multivariate/multiple regression problem. An important aspect\nin the development of such models is the implementation of bespoke inference\nmethodologies, a process which can become quite laborious, when multiple\ncandidate models are being considered. We describe automatic, black-box\ninference in such models using Stan, a popular probabilistic programming\nlanguage. The utilisation of systems like Stan can facilitate model prototyping\nand testing, thus accelerating the data modelling process. The code described\nin this chapter can be found at https://github.com/dvav/eQTLBookChapter.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 14:07:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Vavoulis", "Dimitrios V", ""]]}, {"id": "1906.05189", "submitter": "Alexandre Janon", "authors": "Alexandre Janon", "title": "Global optimization using Sobol indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and assess a new global (derivative-free) optimization algorithm,\ninspired by the LIPO algorithm, which uses variance-based sensitivity analysis\n(Sobol indices) to reduce the number of calls to the objective function. This\nmethod should be efficient to optimize costly functions satisfying the\nsparsity-of-effects principle.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 15:00:54 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Janon", "Alexandre", ""]]}, {"id": "1906.05232", "submitter": "Mehdi Maadooliat", "authors": "Hossein Haghbin, Seyed Morteza Najibi, Rahim Mahmoudvand, Jordan\n  Trinka, and Mehdi Maadooliat", "title": "Functional Singular Spectrum Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new extension of the Singular Spectrum Analysis\n(SSA) called functional SSA to analyze functional time series. The new\nmethodology is developed by integrating ideas from functional data analysis and\nunivariate SSA. We explore the advantages of the functional SSA in terms of\nsimulation results and two real data applications. We compare the proposed\napproach with Multivariate SSA (MSSA) and dynamic Functional Principal\nComponent Analysis (dFPCA). The results suggest that further improvement to\nMSSA is possible, and the new method provides an attractive alternative to the\ndFPCA approach that is used for analyzing correlated functions. We implement\nthe proposed technique to an application of remote sensing data and a call\ncenter dataset. We have also developed an efficient and user-friendly R package\nand a shiny web application to allow interactive exploration of the results.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:07:54 GMT"}, {"version": "v2", "created": "Sun, 27 Oct 2019 12:38:11 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Haghbin", "Hossein", ""], ["Najibi", "Seyed Morteza", ""], ["Mahmoudvand", "Rahim", ""], ["Trinka", "Jordan", ""], ["Maadooliat", "Mehdi", ""]]}, {"id": "1906.05244", "submitter": "Neil A. Spencer", "authors": "Neil A. Spencer and Jared S. Murray", "title": "A Bayesian Hierarchical Model for Evaluating Forensic Footwear Evidence", "comments": "Accepted for publication at the Annals of Applied Statistics. Code\n  available at http://github.com/neilspencer/cindRella", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a latent shoeprint is discovered at a crime scene, forensic analysts\ninspect it for distinctive patterns of wear such as scratches and holes (known\nas accidentals) on the source shoe's sole. If its accidentals correspond to\nthose of a suspect's shoe, the print can be used as forensic evidence to place\nthe suspect at the crime scene. The strength of this evidence depends on the\nrandom match probability---the chance that a shoe chosen at random would match\nthe crime scene print's accidentals. Evaluating random match probabilities\nrequires an accurate model for the spatial distribution of accidentals on shoe\nsoles. A recent report by the President's Council of Advisors in Science and\nTechnology criticized existing models in the literature, calling for new\nempirically validated techniques. We respond to this request with a new spatial\npoint process model for accidental locations, developed within a hierarchical\nBayesian framework. We treat the tread pattern of each shoe as a covariate,\nallowing us to pool information across large heterogeneous databases of shoes.\nExisting models ignore this information; our results show that including it\nleads to significantly better model fit. We demonstrate this by fitting our\nmodel to one such database.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 16:57:13 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 19:27:08 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Spencer", "Neil A.", ""], ["Murray", "Jared S.", ""]]}, {"id": "1906.05338", "submitter": "Sanjukta Krishnagopal", "authors": "Sanjukta Krishnagopal, Rainer Von Coelln, Lisa M. Shulman, Michelle\n  Girvan", "title": "Identifying and Predicting Parkinson's Disease Subtypes through\n  Trajectory Clustering via Bipartite Networks", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0233296", "report-no": null, "categories": "stat.AP math.DS physics.bio-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease (PD) is a common neurodegenerative disease with a high\ndegree of heterogeneity in its clinical features, rate of progression, and\nchange of variables over time. In this work, we present a novel data-driven,\nnetwork-based Trajectory Profile Clustering (TPC) algorithm for 1)\nidentification of PD subtypes and 2) early prediction of disease progression in\nindividual patients. Our subtype identification is based not only on PD\nvariables, but also on their complex patterns of progression, providing a\nuseful tool for the analysis of large heterogenous, longitudinal data.\nSpecifically, we cluster patients based on the similarity of their trajectories\nthrough a time series of bipartite networks connecting patients to demographic,\nclinical, and genetic variables. We apply this approach to demographic and\nclinical data from the Parkinson's Progression Markers Initiative (PPMI)\ndataset and identify 3 patient clusters, consistent with 3 distinct PD\nsubtypes, each with a characteristic variable progression profile.\nAdditionally, TPC predicts an individual patient's subtype and future disease\ntrajectory, based on baseline assessments. Application of our approach resulted\nin 74% accurate subtype prediction in year 5 in a test/validation cohort.\nFurthermore, we show that genetic variability can be integrated seamlessly in\nour TPC approach. In summary, using PD as a model for chronic progressive\ndiseases, we show that TPC leverages high-dimensional longitudinal datasets for\nsubtype identification and early prediction of individual disease subtype. We\nanticipate this approach will be broadly applicable to multidimensional\nlongitudinal datasets in diverse chronic diseases.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 19:21:29 GMT"}, {"version": "v2", "created": "Sun, 7 Jul 2019 11:50:19 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Krishnagopal", "Sanjukta", ""], ["Von Coelln", "Rainer", ""], ["Shulman", "Lisa M.", ""], ["Girvan", "Michelle", ""]]}, {"id": "1906.05399", "submitter": "Marcelo Costa Prof.", "authors": "Marcelo Azevedo Costa and Leandro Brioschi Mineti and Marcos Oliveira\n  Prates and Ramiro Ruiz Cardenas", "title": "Dynamic Time Scan Forecasting", "comments": "15 pages, 7 figures, working paper, version 1", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dynamic time scan forecasting method relies on the premise that the most\nimportant pattern in a time series precedes the forecasting window, i.e., the\nlast observed values. Thus, a scan procedure is applied to identify similar\npatterns, or best matches, throughout the time series. As oppose to euclidean\ndistance, or any distance function, a similarity function is dynamically\nestimated in order to match previous values to the last observed values.\nGoodness-of-fit statistics are used to find the best matches. Using the\nrespective similarity functions, the observed values proceeding the best\nmatches are used to create a forecasting pattern, as well as forecasting\nintervals. Remarkably, the proposed method outperformed statistical and machine\nlearning approaches in a real case wind speed forecasting problem.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jun 2019 22:03:09 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Costa", "Marcelo Azevedo", ""], ["Mineti", "Leandro Brioschi", ""], ["Prates", "Marcos Oliveira", ""], ["Cardenas", "Ramiro Ruiz", ""]]}, {"id": "1906.05467", "submitter": "Shixiang Zhu", "authors": "Shixiang Zhu, Shuang Li, Zhigang Peng, Yao Xie", "title": "Imitation Learning of Neural Spatio-Temporal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Neural Embedding Spatio-Temporal (NEST) point process\nmodel for spatio-temporal discrete event data and develop an efficient\nimitation learning (a type of reinforcement learning) based approach for model\nfitting. Despite the rapid development of one-dimensional temporal point\nprocesses for discrete event data, the study of spatial-temporal aspects of\nsuch data is relatively scarce. Our model captures complex spatio-temporal\ndependence between discrete events by carefully design a mixture of\nheterogeneous Gaussian diffusion kernels, whose parameters are parameterized by\nneural networks. This new kernel is the key that our model can capture\nintricate spatial dependence patterns and yet still lead to interpretable\nresults as we examine maps of Gaussian diffusion kernel parameters. The\nimitation learning model fitting for the NEST is more robust than the maximum\nlikelihood estimate. It directly measures the divergence between the empirical\ndistributions between the training data and the model-generated data. Moreover,\nour imitation learning-based approach enjoys computational efficiency due to\nthe explicit characterization of the reward function related to the likelihood\nfunction; furthermore, the likelihood function under our model enjoys tractable\nexpression due to Gaussian kernel parameterization. Experiments based on real\ndata show our method's good performance relative to the state-of-the-art and\nthe good interpretability of NEST's result.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 03:48:19 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:53:00 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 03:06:41 GMT"}, {"version": "v4", "created": "Fri, 22 Jan 2021 18:43:16 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Zhu", "Shixiang", ""], ["Li", "Shuang", ""], ["Peng", "Zhigang", ""], ["Xie", "Yao", ""]]}, {"id": "1906.05492", "submitter": "Dixin Luo", "authors": "Dixin Luo, Hongteng Xu and Lawrence Carin", "title": "Interpretable ICD Code Embeddings with Self- and Mutual-Attention\n  Mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel and interpretable embedding method to represent the\ninternational statistical classification codes of diseases and related health\nproblems (i.e., ICD codes). This method considers a self-attention mechanism\nwithin the disease domain and a mutual-attention mechanism jointly between\ndiseases and procedures. This framework captures the clinical relationships\nbetween the disease codes and procedures associated with hospital admissions,\nand it predicts procedures according to diagnosed diseases. A self-attention\nnetwork is learned to fuse the embeddings of the diseases for each admission.\nThe similarities between the fused disease embedding and the procedure\nembeddings indicate which procedure should potentially be recommended.\nAdditionally, when learning the embeddings of the ICD codes, the optimal\ntransport between the diseases and the procedures within each admission is\ncalculated as a regularizer of the embeddings. The optimal transport provides a\nmutual-attention map between diseases and the procedures, which suppresses the\nambiguity within their clinical relationships. The proposed method achieves\nclinically-interpretable embeddings of ICD codes, and outperforms\nstate-of-the-art embedding methods in procedure recommendation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 05:50:19 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Luo", "Dixin", ""], ["Xu", "Hongteng", ""], ["Carin", "Lawrence", ""]]}, {"id": "1906.05641", "submitter": "Jens Rauch", "authors": "Jens Rauch, Mathias Denter and Ursula H\\\"ubner", "title": "Use of Emergency Departments by Frail Elderly Patients: Temporal\n  Patterns and Case Complexity", "comments": "Submitted to 64th Annual Conference of the German Association for\n  Medical Informatics, 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emergency department (ED) care for frail elderly patients is associated with\nan increased use of resources due to their complex medical needs and frequently\ndifficult psycho-social situation. To better target their needs with specially\ntrained staff, it is vital to determine the times during which these particular\npatients present to the ED. Recent research was inconclusive regarding this\nquestion and the applied methods were limited to coarse time windows. Moreover,\nthere is little research on time variation of frail ED patients' case\ncomplexity. This study examines differences in arrival rates for frail vs.\nnon-frail patients in detail and compares case complexity in frail patients\nwithin vs. outside of regular GP working hours. Arrival times and case\nvariables (admission rate, ED length of stay [LOS], triage level and\ncomorbidities) were extracted from the EHR of an ED in an urban German teaching\nhospital. We employed Poisson time series regression to determine patterns in\nhourly arrival rates over the week. Frail elderly patients presented more\nlikely to the ED during already high frequented hours, especially at midday and\nin the afternoon. Case complexity for frail patients was significantly higher\ncompared to non-frail patients, but varied marginally in time only with respect\nto triage level and ED LOS. The results suggest that frailty-attuned emergency\ncare should be available in EDs during the busiest hours. Based on EHR data,\nhospitals thus can tailor their staff needs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 12:56:33 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Rauch", "Jens", ""], ["Denter", "Mathias", ""], ["H\u00fcbner", "Ursula", ""]]}, {"id": "1906.05684", "submitter": "David Leslie", "authors": "David Leslie", "title": "Understanding artificial intelligence ethics and safety", "comments": null, "journal-ref": "The Alan Turing Institute (June, 2019)", "doi": "10.5281/zenodo.3240529", "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A remarkable time of human promise has been ushered in by the convergence of\nthe ever-expanding availability of big data, the soaring speed and stretch of\ncloud computing platforms, and the advancement of increasingly sophisticated\nmachine learning algorithms. Innovations in AI are already leaving a mark on\ngovernment by improving the provision of essential social goods and services\nfrom healthcare, education, and transportation to food supply, energy, and\nenvironmental management. These bounties are likely just the start. The\nprospect that progress in AI will help government to confront some of its most\nurgent challenges is exciting, but legitimate worries abound. As with any new\nand rapidly evolving technology, a steep learning curve means that mistakes and\nmiscalculations will be made and that both unanticipated and harmful impacts\nwill occur.\n  This guide, written for department and delivery leads in the UK public sector\nand adopted by the British Government in its publication, 'Using AI in the\nPublic Sector,' identifies the potential harms caused by AI systems and\nproposes concrete, operationalisable measures to counteract them. It stresses\nthat public sector organisations can anticipate and prevent these potential\nharms by stewarding a culture of responsible innovation and by putting in place\ngovernance processes that support the design and implementation of ethical,\nfair, and safe AI systems. It also highlights the need for algorithmically\nsupported outcomes to be interpretable by their users and made understandable\nto decision subjects in clear, non-technical, and accessible ways. Finally, it\nbuilds out a vision of human-centred and context-sensitive implementation that\ngives a central role to communication, evidence-based reasoning, situational\nawareness, and moral justifiability.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jun 2019 22:14:07 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Leslie", "David", ""]]}, {"id": "1906.05776", "submitter": "Hoon Hwangbo", "authors": "Hoon Hwangbo, Yu Ding, Daniel Cabezon", "title": "Machine Learning Based Analysis and Quantification of Potential Power\n  Gain from Passive Device Installation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Passive device installation on wind turbine generators (WTGs) can potentially\nimprove the power generation of WTGs. Yet, how much impact the installation\nwill make is unclear because conducting controlled experiments is impossible\ndue to ever-changing wind and weather that affect the power generation\nsignificantly. In addition, the potential improvement is believed to be in a\nsmall scale, such as 1-5%, which is less than a typical 3-8% variation level\nobserved in wind data. This article proposes an adaptive kernel-based method\nand builds a surrogate model to reduce the level of unexplained variation in\nwind data. In addition, to establish experimental environments that are similar\nto a controlled situation, this article develops an analysis framework that\nutilizes two other nearby WTGs without any passive device installation.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 16:11:20 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Hwangbo", "Hoon", ""], ["Ding", "Yu", ""], ["Cabezon", "Daniel", ""]]}, {"id": "1906.05777", "submitter": "Maolin Shi", "authors": "Maolin Shi, Wei Sun, Xueguan Song, Hongyou Li", "title": "High-low level support vector regression prediction approach (HL-SVR)\n  for data modeling with input parameters of unequal sample sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector regression (SVR) has been widely used to reduce the high\ncomputational cost of computer simulation. SVR assumes the input parameters\nhave equal sample sizes, but unequal sample sizes are often encountered in\nengineering practices. To solve this issue, a new prediction approach based on\nSVR, namely as high-low-level SVR approach (HL-SVR) is proposed for data\nmodeling of input parameters of unequal sample sizes in this paper. The\nproposed approach is consisted of low-level SVR models for the input parameters\nof larger sample sizes and high-level SVR model for the input parameters of\nsmaller sample sizes. For each training point of the input parameters of\nsmaller sample sizes, one low-level SVR model is built based on its\ncorresponding input parameters of larger sample sizes and their responses of\ninterest. The high-level SVR model is built based on the obtained responses\nfrom the low-level SVR models and the input parameters of smaller sample sizes.\nSeveral numerical examples are used to validate the performance of HL-SVR. The\nexperimental results indicate that HL-SVR can produce more accurate prediction\nresults than conventional SVR. The proposed approach is applied on the stress\nanalysis of dental implant, which the structural parameters have massive\nsamples but the material of implant can only be selected from several Ti and\nits alloys. The prediction performance of the proposed approach is much better\nthan the conventional SVR. The proposed approach can be used for the design,\noptimization and analysis of engineering systems with input parameters of\nunequal sample sizes.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 00:55:43 GMT"}], "update_date": "2019-06-14", "authors_parsed": [["Shi", "Maolin", ""], ["Sun", "Wei", ""], ["Song", "Xueguan", ""], ["Li", "Hongyou", ""]]}, {"id": "1906.05918", "submitter": "Luca Faes", "authors": "Luca Faes, Manuel G\\`omez-Extremera, Riccardo Pernice, Pedro Carpena,\n  Giandomenico Nollo, Alberto Porta, Pedro Bernaola-Galv\\`an", "title": "Comparison of Methods for the Assessment of Nonlinearity in Short-Term\n  Heart Rate Variability under different Physiopathological States", "comments": null, "journal-ref": null, "doi": "10.1063/1.5115506", "report-no": null, "categories": "stat.AP q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread diffusion of nonlinear methods for heart rate\nvariability (HRV) analysis, the presence and the extent to which nonlinear\ndynamics contribute to short-term HRV is still controversial. This work aims at\ntesting the hypothesis that different types of nonlinearity can be observed in\nHRV depending on the method adopted and on the physiopathological state. Two\nentropy-based measures of time series complexity (normalized complexity index,\nNCI) and regularity (information storage, IS), and a measure quantifying\ndeviations from linear correlations in a time series (Gaussian linear contrast,\nGLC), are applied to short HRV recordings obtained in young (Y) and old (O)\nhealthy subjects and in myocardial infarction (MI) patients monitored in the\nresting supine position and in the upright position reached through head-up\ntilt. The method of surrogate data is employed to detect the presence of and\nquantify the contribution of nonlinear dynamics to HRV. We find that the three\nmeasures differ both in their variations across groups and conditions and in\nthe number and strength of nonlinear HRV dynamics detected: at rest, IS reveals\na significantly lower number of nonlinear dynamics in Y, whereas during tilt\nGLC reveals significantly stronger nonlinear HRV dynamics in MI; in the\ntransition from rest to tilt, all measures detect a significant weakening of\nnonlinear HRV dynamics in Y, while only GLC detects a significant strengthening\nof such dynamics in MI. These results suggest that distinct dynamic structures,\ndetected with different sensitivity by nonlinear measures, lie beneath\nshort-term HRV in different physiological states and pathological conditions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 20:33:14 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Faes", "Luca", ""], ["G\u00f2mez-Extremera", "Manuel", ""], ["Pernice", "Riccardo", ""], ["Carpena", "Pedro", ""], ["Nollo", "Giandomenico", ""], ["Porta", "Alberto", ""], ["Bernaola-Galv\u00e0n", "Pedro", ""]]}, {"id": "1906.05953", "submitter": "James Beck", "authors": "Pinaky Bhattacharyya and James L. Beck", "title": "Exploiting Convexification for Bayesian Optimal Sensor Placement by\n  Maximization of Mutual Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimal sensor placement, in its full generality, seeks to maximize\nthe mutual information between uncertain model parameters and the predicted\ndata to be collected from the sensors for the purpose of performing Bayesian\ninference. Equivalently, the expected information entropy of the posterior of\nthe model parameters is minimized over all possible sensor configurations for a\ngiven sensor budget. In the context of structural dynamical systems, this\nminimization is computationally expensive because of the large number of\npossible sensor configurations. Here, a very efficient convex relaxation scheme\nis presented to determine informative and possibly-optimal solutions to the\nproblem, thereby bypassing the necessity for an exhaustive, and often\ninfeasible, combinatorial search. The key idea is to relax the binary sensor\nlocation vector so that its components corresponding to all possible sensor\nlocations lie in the unit interval. Then, the optimization over this vector is\na convex problem that can be efficiently solved. This method always yields a\nunique solution for the relaxed problem, which is often binary and therefore\nthe optimal solution to the original problem. When not binary, the relaxed\nsolution is often suggestive of what the optimal solution for the original\nproblem is. An illustrative example using a fifty-story shear building model\nsubject to sinusoidal ground motion is presented, including a case where there\nare over 47 trillion possible sensor configurations. The solutions and\ncomputational effort are compared to greedy and heuristic methods.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 22:30:49 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Bhattacharyya", "Pinaky", ""], ["Beck", "James L.", ""]]}, {"id": "1906.05959", "submitter": "Boris Rabinovich", "authors": "Yoni Schamroth, Liron Gat Kahlon, Boris Rabinovich, David Steinberg", "title": "Early Detection of Long Term Evaluation Criteria in Online Controlled\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common dilemma encountered by many upon implementing an optimization method\nor experiment, whether it be a reinforcement learning algorithm, or A/B\ntesting, is deciding on what metric to optimize for. Very often short-term\nmetrics, which are easier to measure are chosen over long term metrics which\nhave undesirable time considerations and often a more complex calculation. In\nthis paper, we argue the importance of choosing a metrics that focuses on long\nterm effects. With this comes the necessity in the ability to measure\nsignificant differences between groups relatively early. We present here an\nefficient methodology for early detection of lifetime differences between\ngroups based on bootstrap hypothesis testing of the lifetime forecast of the\nresponse. We present an application of this method in the domain of online\nadvertising and we argue that approach not only allows one to focus on the\nultimate metric of importance but also provides a means of accelerating the\ntesting period.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jun 2019 23:16:18 GMT"}], "update_date": "2019-06-17", "authors_parsed": [["Schamroth", "Yoni", ""], ["Kahlon", "Liron Gat", ""], ["Rabinovich", "Boris", ""], ["Steinberg", "David", ""]]}, {"id": "1906.06020", "submitter": "Guy Hawkins", "authors": "Minh-Ngoc Tran, Marcel Scharth, David Gunawan, Robert Kohn, Scott D.\n  Brown, Guy E. Hawkins", "title": "Robustly estimating the marginal likelihood for cognitive models via\n  importance sampling", "comments": "38 pages, 4 tables, 5 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Markov chain Monte Carlo (MCMC) extend the scope of\nBayesian inference to models for which the likelihood function is intractable.\nAlthough these developments allow us to estimate model parameters, other basic\nproblems such as estimating the marginal likelihood, a fundamental tool in\nBayesian model selection, remain challenging. This is an important scientific\nlimitation because testing psychological hypotheses with hierarchical models\nhas proven difficult with current model selection methods. We propose an\nefficient method for estimating the marginal likelihood for models where the\nlikelihood is intractable, but can be estimated unbiasedly. It is based on\nfirst running a sampling method such as MCMC to obtain samples for the model\nparameters, and then using these samples to construct the proposal density in\nan importance sampling (IS) framework with an unbiased estimate of the\nlikelihood. Our method has several attractive properties: it generates an\nunbiased estimate of the marginal likelihood, it is robust to the quality and\ntarget of the sampling method used to form the IS proposals, and it is\ncomputationally cheap to estimate the variance of the marginal likelihood\nestimator. We also obtain the convergence properties of the method and provide\nguidelines on maximizing computational efficiency. The method is illustrated in\ntwo challenging cases involving hierarchical models: identifying the form of\nindividual differences in an applied choice scenario, and evaluating the best\nparameterization of a cognitive model in a speeded decision making context.\nFreely available code to implement the methods is provided. Extensions to\nposterior moment estimation and parallelization are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 04:49:32 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 07:22:23 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Tran", "Minh-Ngoc", ""], ["Scharth", "Marcel", ""], ["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Brown", "Scott D.", ""], ["Hawkins", "Guy E.", ""]]}, {"id": "1906.06248", "submitter": "Simon Schn\\\"urch", "authors": "Simon Schn\\\"urch and Andreas Wagner", "title": "Machine Learning on EPEX Order Books: Insights and Forecasts", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper employs machine learning algorithms to forecast German electricity\nspot market prices. The forecasts utilize in particular bid and ask order book\ndata from the spot market but also fundamental market data like renewable\ninfeed and expected demand. Appropriate feature extraction for the order book\ndata is developed. Using cross-validation to optimise hyperparameters, neural\nnetworks and random forests are proposed and compared to statistical reference\nmodels. The machine learning models outperform traditional approaches.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 15:21:58 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 08:43:38 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 08:00:07 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Schn\u00fcrch", "Simon", ""], ["Wagner", "Andreas", ""]]}, {"id": "1906.06382", "submitter": "Rendani Mbuvha", "authors": "Rendani Mbuvha, Illyes Boulkaibet, Tshilidzi Marwala", "title": "Automatic Relevance Determination Bayesian Neural Networks for Credit\n  Card Default Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit risk modelling is an integral part of the global financial system.\nWhile there has been great attention paid to neural network models for credit\ndefault prediction, such models often lack the required interpretation\nmechanisms and measures of the uncertainty around their predictions. This work\ndevelops and compares Bayesian Neural Networks(BNNs) for credit card default\nmodelling. This includes a BNNs trained by Gaussian approximation and the first\nimplementation of BNNs trained by Hybrid Monte Carlo(HMC) in credit risk\nmodelling. The results on the Taiwan Credit Dataset show that BNNs with\nAutomatic Relevance Determination(ARD) outperform normal BNNs without ARD. The\nresults also show that BNNs trained by Gaussian approximation display similar\npredictive performance to those trained by the HMC. The results further show\nthat BNN with ARD can be used to draw inferences about the relative importance\nof different features thus critically aiding decision makers in explaining\nmodel output to consumers. The robustness of this result is reinforced by high\nlevels of congruence between the features identified as important using the two\ndifferent approaches for training BNNs.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jun 2019 20:00:10 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Mbuvha", "Rendani", ""], ["Boulkaibet", "Illyes", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1906.06459", "submitter": "Zhou Lan", "authors": "Zhou Lan, Brian J Reich", "title": "Probabilistic Diffusion MRI Fiber Tracking Using a Directed Acyclic\n  Graph Auto-Regressive Model of Positive Definite Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion MRI is a neuroimaging technique measuring the anatomical structure\nof tissues. Using diffusion MRI to construct the connections of tissues, known\nas fiber tracking, is one of the most important uses of diffusion MRI. Many\ntechniques are available recently but few properly quantify statistical\nuncertainties. In this paper, we propose a directed acyclic graph\nauto-regressive model of positive definite matrices and apply a probabilistic\nfiber tracking algorithm. We use both real data analysis and numerical studies\nto demonstrate our proposal.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 03:20:14 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Lan", "Zhou", ""], ["Reich", "Brian J", ""]]}, {"id": "1906.06463", "submitter": "Jasjeet Sekhon", "authors": "S\\\"oren R. K\\\"unzel, Theo F. Saarinen, Edward W. Liu, Jasjeet S.\n  Sekhon", "title": "Linear Aggregation in Tree-based Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression trees and their ensemble methods are popular methods for\nnonparametric regression: they combine strong predictive performance with\ninterpretable estimators. To improve their utility for locally smooth response\nsurfaces, we study regression trees and random forests with linear aggregation\nfunctions. We introduce a new algorithm that finds the best axis-aligned split\nto fit linear aggregation functions on the corresponding nodes, and we offer a\nquasilinear time implementation. We demonstrate the algorithm's favorable\nperformance on real-world benchmarks and in an extensive simulation study, and\nwe demonstrate its improved interpretability using a large get-out-the-vote\nexperiment. We provide an open-source software package that implements several\ntree-based estimators with linear aggregation functions.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 04:25:55 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 20:00:42 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 23:17:20 GMT"}, {"version": "v4", "created": "Wed, 13 Jan 2021 20:45:03 GMT"}, {"version": "v5", "created": "Sat, 23 Jan 2021 02:18:06 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["K\u00fcnzel", "S\u00f6ren R.", ""], ["Saarinen", "Theo F.", ""], ["Liu", "Edward W.", ""], ["Sekhon", "Jasjeet S.", ""]]}, {"id": "1906.06529", "submitter": "Xinwei Ma", "authors": "Matias D. Cattaneo, Michael Jansson, Xinwei Ma", "title": "lpdensity: Local Polynomial Density Estimation and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimation and inference methods are widely used in empirical work.\nWhen the underlying distribution has compact support, conventional kernel-based\ndensity estimators are no longer consistent near or at the boundary because of\ntheir well-known boundary bias. Alternative smoothing methods are available to\nhandle boundary points in density estimation, but they all require additional\ntuning parameter choices or other typically ad hoc modifications depending on\nthe evaluation point and/or approach considered. This article discusses the R\nand Stata package lpdensity implementing a novel local polynomial density\nestimator proposed and studied in Cattaneo, Jansson, and Ma (2020, 2021), which\nis boundary adaptive and involves only one tuning parameter. The methods\nimplemented also cover local polynomial estimation of the cumulative\ndistribution function and density derivatives. In addition to point estimation\nand graphical procedures, the package offers consistent variance estimators,\nmean squared error optimal bandwidth selection, robust bias-corrected\ninference, and confidence bands construction, among other features. A\ncomparison with other density estimation packages available in R using a Monte\nCarlo experiment is provided.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 11:26:21 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2020 00:35:49 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 22:01:41 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Ma", "Xinwei", ""]]}, {"id": "1906.06580", "submitter": "Mike West", "authors": "Isaac Lavine, Michael Lindon, and Mike West", "title": "Adaptive Variable Selection for Sequential Prediction in Multivariate\n  Dynamic Models", "comments": "23 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian model uncertainty analysis and forecasting in sequential\ndynamic modeling of multivariate time series. The perspective is that of a\ndecision-maker with a specific forecasting objective that guides thinking about\nrelevant models. Based on formal Bayesian decision-theoretic reasoning, we\ndevelop a time-adaptive approach to exploring, weighting, combining and\nselecting models that differ in terms of predictive variables included. The\nadaptivity allows for changes in the sets of favored models over time, and is\nguided by the specific forecasting goals. A synthetic example illustrates how\ndecision-guided variable selection differs from traditional Bayesian model\nuncertainty analysis and standard model averaging. An applied study in one\nmotivating application of long-term macroeconomic forecasting highlights the\nutility of the new approach in terms of improving predictions as well as its\nability to identify and interpret different sets of relevant models over time\nwith respect to specific, defined forecasting goals.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 15:53:50 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2020 14:03:58 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["Lavine", "Isaac", ""], ["Lindon", "Michael", ""], ["West", "Mike", ""]]}, {"id": "1906.06583", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron and J\\'er\\^ome Dedecker and Bertrand Michel", "title": "Linear regression with stationary errors : the R package slm", "comments": "31 pages, 11 figures, 5 tables. The associated R package 'slm' is\n  available on the CRAN website (https://cran.r-project.org/index.html) or on\n  the GitHub website (https://github.com/E-Caron/slm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the R package slm which stands for Stationary Linear\nModels. The package contains a set of statistical procedures for linear\nregression in the general context where the error process is strictly\nstationary with short memory. We work in the setting of Hannan (1973), who\nproved the asymptotic normality of the (normalized) least squares estimators\n(LSE) under very mild conditions on the error process. We propose different\nways to estimate the asymptotic covariance matrix of the LSE, and then to\ncorrect the type I error rates of the usual tests on the parameters (as well as\nconfidence intervals). The procedures are evaluated through different sets of\nsimulations, and two examples of real datasets are studied.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jun 2019 16:09:48 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 14:51:36 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 18:45:33 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "1906.06713", "submitter": "Zhigang Yao", "authors": "Yan Liu, Zhiqiang Hou, Zhigang Yao, Zhidong Bai, Jiang Hu, Shurong\n  Zheng", "title": "Community Detection Based on the $L_\\infty$ convergence of eigenvectors\n  in DCBM", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral clustering is one of the most popular algorithms for community\ndetection in network analysis. Based on this rationale, in this paper we give\nthe convergence rate of eigenvectors for the adjacency matrix in the $l_\\infty$\nnorm, under the stochastic block model (BM) and degree corrected stochastic\nblock model (DCBM), adding some mild and rational conditions. We also extend\nthis result to a more general model, presented based on the DCBM such that the\nvalue of random variables in the adjacency matrix is not 0 or 1, but an\narbitrary real number. During the process of proving the above conclusion, we\nobtain the relationship of the eigenvalues in the adjacency matrix and the\ncorresponding `population' matrix, which vary in dimension from the\ncommunity-wise edge probability matrix. Using that result, we can give an\nestimate of the number of the communities in a known set of network data.\nMeanwhile we proved the consistency of the estimator. Furthermore, according to\nthe derivation of proof for the convergence of eigenvectors, we propose a new\napproach to community detection -- Spectral Clustering based on Difference of\nRatios of Eigenvectors (SCDRE). Our simulation experiments demonstrate the\nsuperiority of our method in community detection.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 15:01:19 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Liu", "Yan", ""], ["Hou", "Zhiqiang", ""], ["Yao", "Zhigang", ""], ["Bai", "Zhidong", ""], ["Hu", "Jiang", ""], ["Zheng", "Shurong", ""]]}, {"id": "1906.06714", "submitter": "Alec Chan-Golston", "authors": "Alec M. Chan-Golston, Sudipto Banerjee, and Mark S. Handcock", "title": "Bayesian Inference for Finite Populations Under Spatial Process Settings", "comments": "26 pages, 7 figures, LaTex; title modified, minor edits to abstract,\n  section 4.1 and 4.2 updated to reflect the notation used in regression model\n  presented in section 2, section 6 has been updated to reflect new choice of\n  priors (including Table 2, and Figure 3) but no significant changes,\n  supplemental information has condensed the derivations and includes\n  variograms from the data analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian model-based approach to finite population estimation\naccounting for spatial dependence. Our innovation here is a framework that\nachieves inference for finite population quantities in spatial process\nsettings. A key distinction from the small area estimation setting is that we\nanalyze finite populations referenced by their geographic coordinates.\nSpecifically, we consider a two-stage sampling design in which the primary\nunits are geographic regions, the secondary units are point-referenced\nlocations, and the measured values are assumed to be a partial realization of a\nspatial process. Estimation of finite population quantities from geostatistical\nmodels do not account for sampling designs, which can impair inferential\nperformance, while design-based estimates ignore the spatial dependence in the\nfinite population. We demonstrate using simulation experiments that\nprocess-based finite population sampling models improve model fit and inference\nover models that fail to account for spatial correlation. Furthermore, the\nprocess based models offer richer inference with spatially interpolated maps\nover the entire region. We reinforce these improvements and demonstrate\nscaleable inference for ground-water nitrate levels in the population of\nCalifornia Central Valley wells by offering estimates of mean nitrate levels\nand their spatially interpolated maps.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 15:05:35 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 22:09:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Chan-Golston", "Alec M.", ""], ["Banerjee", "Sudipto", ""], ["Handcock", "Mark S.", ""]]}, {"id": "1906.06744", "submitter": "John O'Sullivan", "authors": "John O'Sullivan (1), Conor Sweeney (1), Andrew C. Parnell (2) ((1)\n  School of Mathematics and Statistics, University College Dublin, (2) Hamilton\n  Institute, Insight Centre for Data Analytics, Maynooth University)", "title": "Bayesian spatial extreme value analysis of maximum temperatures in\n  County Dublin, Ireland", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we begin a comprehensive characterisation of temperature\nextremes in Ireland for the period 1981-2010. We produce return levels of\nanomalies of daily maximum temperature extremes for an area over Ireland, for\nthe 30-year period 1981-2010. We employ extreme value theory (EVT) to model the\ndata using the generalised Pareto distribution (GPD) as part of a three-level\nBayesian hierarchical model. We use predictive processes in order to solve the\ncomputationally difficult problem of modelling data over a very dense spatial\nfield. To our knowledge, this is the first study to combine predictive\nprocesses and EVT in this manner. The model is fit using Markov chain Monte\nCarlo (MCMC) algorithms. Posterior parameter estimates and return level\nsurfaces are produced, in addition to specific site analysis at synoptic\nstations, including Casement Aerodrome and Dublin Airport. Observational data\nfrom the period 2011-2018 is included in this site analysis to determine if\nthere is evidence of a change in the observed extremes. An increase in the\nfrequency of extreme anomalies, but not the severity, is observed for this\nperiod. We found that the frequency of observed extreme anomalies from\n2011-2018 at the Casement Aerodrome and Phoenix Park synoptic stations exceed\nthe upper bounds of the credible intervals from the model by 20% and 7%\nrespectively.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jun 2019 18:14:04 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["O'Sullivan", "John", ""], ["Sweeney", "Conor", ""], ["Parnell", "Andrew C.", ""]]}, {"id": "1906.06882", "submitter": "Phyllis Wan", "authors": "Juan-Juan Cai, Phyllis Wan, Gamze Ozel", "title": "Parametric and non-parametric estimation of extreme earthquake event:\n  the joint tail inference for mainshocks and aftershocks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an earthquake event, the combination of a strong mainshock and damaging\naftershocks is often the cause of severe structural damages and/or high death\ntolls. The objective of this paper is to provide estimation for the probability\nof such extreme events where the mainshock and the largest aftershocks exceed\ncertain thresholds. Two approaches are illustrated and compared -- a parametric\napproach based on previously observed stochastic laws in earthquake data, and a\nnon-parametric approach based on bivariate extreme value theory. We analyze the\nearthquake data from the North Anatolian Fault Zone (NAFZ) in Turkey during\n1965-2018 and show that the two approaches provide unifying results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 07:50:09 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Cai", "Juan-Juan", ""], ["Wan", "Phyllis", ""], ["Ozel", "Gamze", ""]]}, {"id": "1906.06908", "submitter": "Christian Bongiorno", "authors": "Christian Bongiorno, Salvatore Miccich\\`e, Rosario N. Mantegna", "title": "Nested partitions from hierarchical clustering statistical validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a greedy algorithm that is fast and scalable in the detection of a\nnested partition extracted from a dendrogram obtained from hierarchical\nclustering of a multivariate series. Our algorithm provides a $p$-value for\neach clade observed in the hierarchical tree. The $p$-value is obtained by\ncomputing a number of bootstrap replicas of the dissimilarity matrix and by\nperforming a statistical test on each difference between the dissimilarity\nassociated with a given clade and the dissimilarity of the clade of its parent\nnode. We prove the efficacy of our algorithm with a set of benchmarks generated\nby using a hierarchical factor model. We compare the results obtained by our\nalgorithm with those of Pvclust. Pvclust is a widely used algorithm developed\nwith a global approach originally motivated by phylogenetic studies. In our\nnumerical experiments we focus on the role of multiple hypothesis test\ncorrection and on the robustness of the algorithms to inaccuracy and errors of\ndatasets. We also apply our algorithm to a reference empirical dataset. We\nverify that our algorithm is much faster than Pvclust algorithm and has a\nbetter scalability both in the number of elements and in the number of records\nof the investigated multivariate set. Our algorithm provides a hierarchically\nnested partition in much shorter time than currently widely used algorithms\nallowing to perform a statistically validated cluster analysis detection in\nvery large systems.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 09:08:30 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Bongiorno", "Christian", ""], ["Miccich\u00e8", "Salvatore", ""], ["Mantegna", "Rosario N.", ""]]}, {"id": "1906.07036", "submitter": "Meridith Bartley", "authors": "Meridith L Bartley, Ephraim M Hanks, Erin M Schliep, Patricia A\n  Soranno, Tyler Wagner", "title": "Identifying and characterizing extrapolation in multivariate response\n  data", "comments": "28 pages, 2 supplementary files, 6 main figures, 2 supplementary\n  figures, 2 supplementary tables", "journal-ref": null, "doi": "10.1371/journal.pone.0225715", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrapolation is defined as making predictions beyond the range of the data\nused to estimate a statistical model. In ecological studies, it is not always\nobvious when and where extrapolation occurs because of the multivariate nature\nof the data. Previous work on identifying extrapolation has focused on\nunivariate response data, but these methods are not directly applicable to\nmultivariate response data, which are more and more common in ecological\ninvestigations. In this paper, we extend previous work that identified\nextrapolation by applying the predictive variance from the univariate setting\nto the multivariate case. We illustrate our approach through an analysis of\njointly modeled lake nutrients and indicators of algal biomass and water\nclarity in over 7000 inland lakes from across the Northeast and Mid-west US. In\naddition, we illustrate novel exploratory approaches for identifying regions of\ncovariate space where extrapolation is more likely to occur using\nclassification and regression trees.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 13:50:15 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 16:57:00 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Bartley", "Meridith L", ""], ["Hanks", "Ephraim M", ""], ["Schliep", "Erin M", ""], ["Soranno", "Patricia A", ""], ["Wagner", "Tyler", ""]]}, {"id": "1906.07294", "submitter": "Amanda Mejia", "authors": "Amanda F. Mejia, Mary Beth Nebel, Yikai Wang, Brian S. Caffo, Ying Guo", "title": "Template Independent Component Analysis: Targeted and Reliable\n  Estimation of Subject-level Brain Networks using Big Data Population Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large brain imaging databases contain a wealth of information on brain\norganization in the populations they target, and on individual variability.\nWhile such databases have been used to study group-level features of\npopulations directly, they are currently underutilized as a resource to inform\nsingle-subject analysis. Here, we propose leveraging the information contained\nin large functional magnetic resonance imaging (fMRI) databases by establishing\npopulation priors to employ in an empirical Bayesian framework. We focus on\nestimation of brain networks as source signals in independent component\nanalysis (ICA). We formulate a hierarchical \"template\" ICA model where source\nsignals---including known population brain networks and subject-specific\nsignals---are represented as latent variables. For estimation, we derive an\nexpectation maximization (EM) algorithm having an explicit solution. However,\nas this solution is computationally intractable, we also consider an\napproximate subspace algorithm and a faster two-stage approach. Through\nextensive simulation studies, we assess performance of both methods and compare\nwith dual regression, a popular but ad-hoc method. The two proposed algorithms\nhave similar performance, and both dramatically outperform dual regression. We\nalso conduct a reliability study utilizing the Human Connectome Project and\nfind that template ICA achieves substantially better performance than dual\nregression, achieving 75-250% higher intra-subject reliability.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jun 2019 22:40:32 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Mejia", "Amanda F.", ""], ["Nebel", "Mary Beth", ""], ["Wang", "Yikai", ""], ["Caffo", "Brian S.", ""], ["Guo", "Ying", ""]]}, {"id": "1906.07502", "submitter": "Delmiro Fernandez-Reyes Dr", "authors": "Biobele J. Brown, Alexander A. Przybylski, Petru Manescu, Fabio\n  Caccioli, Gbeminiyi Oyinloye, Muna Elmi, Michael J. Shaw, Vijay Pawar, Remy\n  Claveau, John Shawe-Taylor, Mandayam A. Srinivasan, Nathaniel K. Afolabi,\n  Adebola E. Orimadegun, Wasiu A. Ajetunmobi, Francis Akinkunmi, Olayinka\n  Kowobari, Kikelomo Osinusi, Felix O. Akinbami, Samuel Omokhodion, Wuraola A.\n  Shokunbi, Ikeoluwa Lagunju, Olugbemiro Sodeinde, and Delmiro Fernandez-Reyes", "title": "Data-Driven Malaria Prevalence Prediction in Large Densely-Populated\n  Urban Holoendemic sub-Saharan West Africa: Harnessing Machine Learning\n  Approaches and 22-years of Prospectively Collected Data", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasmodium falciparum malaria still poses one of the greatest threats to\nhuman life with over 200 million cases globally leading to half-million deaths\nannually. Of these, 90% of cases and of the mortality occurs in sub-Saharan\nAfrica, mostly among children. Although malaria prediction systems are central\nto the 2016-2030 malaria Global Technical Strategy, currently these are\ninadequate at capturing and estimating the burden of disease in highly endemic\ncountries. We developed and validated a computational system that exploits the\npredictive power of current Machine Learning approaches on 22-years of\nprospective data from the high-transmission holoendemic malaria\nurban-densely-populated sub-Saharan West-Africa metropolis of Ibadan. Our\ndataset of >9x104 screened study participants attending our clinical and\ncommunity services from 1996 to 2017 contains monthly prevalence, temporal,\nenvironmental and host features. Our Locality-specific Elastic-Net based\nMalaria Prediction System (LEMPS) achieves good generalization performance,\nboth in magnitude and direction of the prediction, when tasked to predict\nmonthly prevalence on previously unseen validation data (MAE<=6x10-2,\nMSE<=7x10-3) within a range of (+0.1 to -0.05) error-tolerance which is\nrelevant and usable for aiding decision-support in a holoendemic setting. LEMPS\nis well-suited for malaria prediction, where there are multiple features which\nare correlated with one another, and trading-off between\nregularization-strength L1-norm and L2-norm allows the system to retain\nstability. Data-driven systems are critical for regionally-adaptable\nsurveillance, management of control strategies and resource allocation across\nstretched healthcare systems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 11:25:35 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Brown", "Biobele J.", ""], ["Przybylski", "Alexander A.", ""], ["Manescu", "Petru", ""], ["Caccioli", "Fabio", ""], ["Oyinloye", "Gbeminiyi", ""], ["Elmi", "Muna", ""], ["Shaw", "Michael J.", ""], ["Pawar", "Vijay", ""], ["Claveau", "Remy", ""], ["Shawe-Taylor", "John", ""], ["Srinivasan", "Mandayam A.", ""], ["Afolabi", "Nathaniel K.", ""], ["Orimadegun", "Adebola E.", ""], ["Ajetunmobi", "Wasiu A.", ""], ["Akinkunmi", "Francis", ""], ["Kowobari", "Olayinka", ""], ["Osinusi", "Kikelomo", ""], ["Akinbami", "Felix O.", ""], ["Omokhodion", "Samuel", ""], ["Shokunbi", "Wuraola A.", ""], ["Lagunju", "Ikeoluwa", ""], ["Sodeinde", "Olugbemiro", ""], ["Fernandez-Reyes", "Delmiro", ""]]}, {"id": "1906.07798", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt, Jorge Mateu", "title": "A spatial dependence graph model for multivariate spatial hybrid\n  processes", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the joint analysis of multivariate mixed-type\nspatial data, where some components are point processes and some are of\nlattice-type by nature. After a survey of statistical methods for marked\nspatial point and lattice processes, the class of multivariate spatial hybrid\nprocesses is defined and embedded within the framework of spatial dependence\ngraph models. In this model, the point and lattice sub-processes are identified\nwith nodes of a graph whereas missing edges represent conditional independence\namong the components. This finally leads to a general framework for any type of\nspatial data in a multivariate setting. We demonstrate the application of our\nmethod in the analysis of a multivariate point-lattice pattern on crime and\nambulance service call-out incidents recorded in London, where the points are\nthe locations of different pre-classified crime events and the lattice\ncomponents report different aggregated incident rates at ward level.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:37:02 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Eckardt", "Matthias", ""], ["Mateu", "Jorge", ""]]}, {"id": "1906.07800", "submitter": "Tianwei Yu", "authors": "Tianwei Yu", "title": "Autoencoder-based integrative multi-omics data embedding that allows for\n  confounder adjustments", "comments": "20 pages, 5 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the integrative analyses of omics data, it is often of interest to extract\ndata representation from one data type that best reflect its relations with\nanother data type. This task is traditionally fulfilled by linear methods such\nas canonical correlation analysis (CCA) and partial least squares (PLS).\nHowever, information contained in one data type pertaining to the other data\ntype may be complex and in nonlinear form. Deep learning provides a convenient\nalternative to extract low-dimensional nonlinear data embedding. In addition,\nthe deep learning setup can naturally incorporate the effects of clinical\nconfounding factors into the integrative analysis. Here we report a deep\nlearning setup, named Autoencoder-based Integrative Multi-omics data Embedding\n(AIME), to extract data representation for omics data integrative analysis. The\nmethod can adjust for confounder variables, achieve informative data embedding,\nrank features in terms of their contributions, and find pairs of features from\nthe two data types that are related to each other through the data embedding.\nIn simulation studies, the method was highly effective in the extraction of\nmajor contributing features between data types. Using a real microRNA-gene\nexpression dataset, and a real DNA methylation-gene expression dataset, we show\nthat AIME excluded the influence of confounders including batch effects, and\nextracted biologically plausible novel information. The R package based on\nKeras and the TensorFlow backend is available at\nhttps://github.com/tianwei-yu/AIME.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:38:38 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 10:11:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yu", "Tianwei", ""]]}, {"id": "1906.07961", "submitter": "Georgina Hall", "authors": "Georgina Hall", "title": "Engineering and Business Applications of Sum of Squares Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY eess.SY stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing over the cone of nonnegative polynomials, and its dual\ncounterpart, optimizing over the space of moments that admit a representing\nmeasure, are fundamental problems that appear in many different applications\nfrom engineering and computational mathematics to business. In this paper, we\nreview a number of these applications. These include, but are not limited to,\nproblems in control (e.g., formal safety verification), finance (e.g., option\npricing), statistics and machine learning (e.g., shape-constrained regression\nand optimal design), and game theory (e.g., Nash equilibria computation in\npolynomial games). We then show how sum of squares techniques can be used to\ntackle these problems, which are hard to solve in general. We conclude by\nhighlighting some directions that could be pursued to further disseminate sum\nof squares techniques within more applied fields. Among other things, we\nbriefly address the current challenge that scalability represents for\noptimization problems that involve sum of squares polynomials and discuss\nrecent trends in software development.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 08:18:50 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Hall", "Georgina", ""]]}, {"id": "1906.08096", "submitter": "Cyrus Samii", "authors": "Rajeev Dehejia, Cristian Pop-Eleches, Cyrus Samii", "title": "From Local to Global: External Validity in a Fertility Natural\n  Experiment", "comments": "forthcoming at Journal of Business and Economic Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study issues related to external validity for treatment effects using over\n100 replications of the Angrist and Evans (1998) natural experiment on the\neffects of sibling sex composition on fertility and labor supply. The\nreplications are based on census data from around the world going back to 1960.\nWe decompose sources of error in predicting treatment effects in external\ncontexts in terms of macro and micro sources of variation. In our empirical\nsetting, we find that macro covariates dominate over micro covariates for\nreducing errors in predicting treatments, an issue that past studies of\nexternal validity have been unable to evaluate. We develop methods for two\napplications to evidence-based decision-making, including determining where to\nlocate an experiment and whether policy-makers should commission new\nexperiments or rely on an existing evidence base for making a policy decision.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 13:43:37 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Dehejia", "Rajeev", ""], ["Pop-Eleches", "Cristian", ""], ["Samii", "Cyrus", ""]]}, {"id": "1906.08206", "submitter": "Maria De-Arteaga", "authors": "Maria De-Arteaga, Benedikt Boecking", "title": "Killings of social leaders in the Colombian post-conflict: Data analysis\n  for investigative journalism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After the peace agreement of 2016 with FARC, the killings of social leaders\nhave emerged as an important post-conflict challenge for Colombia. We present a\ndata analysis based on official records obtained from the Colombian General\nAttorney's Office spanning the time period from 2012 to 2017. The results of\nthe analysis show a drastic increase in the officially recorded number of\nkillings of democratically elected leaders of community organizations, in\nparticular those belonging to Juntas de Acci\\'on Comunal [Community Action\nBoards]. These are important entities that have been part of the Colombian\ndemocratic apparatus since 1958, and enable communities to advocate for their\nneeds. We also describe how the data analysis guided a journalistic\ninvestigation that was motivated by the Colombian government's denial of the\nsystematic nature of social leaders killings.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 16:29:27 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["De-Arteaga", "Maria", ""], ["Boecking", "Benedikt", ""]]}, {"id": "1906.08227", "submitter": "Andr\\'es Hoyos-Idrobo", "authors": "Andr\\'es Hoyos-Idrobo", "title": "Local Bures-Wasserstein Transport: A Practical and Fast Mapping\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport (OT)-based methods have a wide range of applications and\nhave attracted a tremendous amount of attention in recent years. However, most\nof the computational approaches of OT do not learn the underlying transport\nmap. Although some algorithms have been proposed to learn this map, they rely\non kernel-based methods, which makes them prohibitively slow when the number of\nsamples increases. Here, we propose a way to learn an approximate transport map\nand a parametric approximation of the Wasserstein barycenter. We build an\napproximated transport mapping by leveraging the closed-form of Gaussian\n(Bures-Wasserstein) transport; we compute local transport plans between matched\npairs of the Gaussian components of each density. The learned map generalizes\nto out-of-sample examples. We provide experimental results on simulated and\nreal data, comparing our proposed method with other mapping estimation\nalgorithms. Preliminary experiments suggest that our proposed method is not\nonly faster, with a factor 80 overall running time, but it also requires fewer\ncomponents than state-of-the-art methods to recover the support of the\nbarycenter. From a practical standpoint, it is straightforward to implement and\ncan be used with a conventional machine learning pipeline.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 17:16:54 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Hoyos-Idrobo", "Andr\u00e9s", ""]]}, {"id": "1906.08339", "submitter": "Subhro Das", "authors": "Subhro Das, Chandramouli Maduri, Ching-Hua Chen, Pei-Yun S. Hsueh", "title": "Learning Patient Engagement in Care Management: Performance vs.\n  Interpretability", "comments": "9 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The health outcomes of high-need patients can be substantially influenced by\nthe degree of patient engagement in their own care. The role of care managers\nincludes that of enrolling patients into care programs and keeping them\nsufficiently engaged in the program, so that patients can attain various goals.\nThe attainment of these goals is expected to improve the patients' health\noutcomes. In this paper, we present a real world data-driven method and the\nbehavioral engagement scoring pipeline for scoring the engagement level of a\npatient in two regards: (1) Their interest in enrolling into a relevant care\nprogram, and (2) their interest and commitment to program goals. We use this\nscore to predict a patient's propensity to respond (i.e., to a call for\nenrollment into a program, or to an assigned program goal). Using real-world\ncare management data, we show that our scoring method successfully predicts\npatient engagement. We also show that we are able to provide interpretable\ninsights to care managers, using prototypical patients as a point of reference,\nwithout sacrificing prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 20:26:07 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Das", "Subhro", ""], ["Maduri", "Chandramouli", ""], ["Chen", "Ching-Hua", ""], ["Hsueh", "Pei-Yun S.", ""]]}, {"id": "1906.08357", "submitter": "Liying Luo", "authors": "Liying Luo and James Hodges", "title": "The Age-Period-Cohort-Interaction Model for Describing and Investigating\n  Inter-Cohort Deviations and Intra-Cohort Life-Course Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social scientists have frequently sought to understand the distinct effects\nof age, period, and cohort, but disaggregation of the three dimensions is\ndifficult because cohort = period - age. We argue that this technical\ndifficulty reflects a disconnection between how cohort effect is conceptualized\nand how it is modeled in the traditional age-period-cohort framework. We\npropose a new method, called the age-period-cohort-interaction (APC-I) model,\nthat is qualitatively different from previous methods in that it represents\nRyder's (1965) theoretical account about the conditions under which cohort\ndifferentiation may arise. This APC-I model does not require problematic\nstatistical assumptions and the interpretation is straightforward. It\nquantifies inter-cohort deviations from the age and period main effects and\nalso permits hypothesis testing about intra-cohort life-course dynamics. We\ndemonstrate how this new model can be used to examine age, period, and cohort\npatterns in women's labor force participation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jun 2019 01:33:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Luo", "Liying", ""], ["Hodges", "James", ""]]}, {"id": "1906.08359", "submitter": "Andrew L. Johnson", "authors": "Daisuke Yagi, Yining Chen, Andrew L. Johnson, and Hiroshi Morita", "title": "An axiomatic nonparametric production function estimator: Modeling\n  production in Japan's cardboard industry", "comments": "81 pages, 40 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach to estimate a production function based on the\neconomic axioms of the Regular Ultra Passum law and convex non-homothetic input\nisoquants. Central to the development of our estimator is stating the axioms as\nshape constraints and using shape constrained nonparametric regression methods.\nWe implement this approach using data from the Japanese corrugated cardboard\nindustry from 1997-2007. Using this new approach, we find most productive scale\nsize is a function of the capital-to-labor ratio and the largest firms operate\nclose to the largest most productive scale size associated with a high\ncapital-to-labor ratio. We measure the productivity growth across the panel\nperiods based on the residuals from our axiomatic model. We also decompose\nproductivity into scale, input mix, and unexplained effects to clarify the\nsources the productivity differences and provide managers guidance to make\nfirms more productive.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:19:09 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Yagi", "Daisuke", ""], ["Chen", "Yining", ""], ["Johnson", "Andrew L.", ""], ["Morita", "Hiroshi", ""]]}, {"id": "1906.08360", "submitter": "Paul Vos", "authors": "Paul Vos and Don Holbert", "title": "Frequentist Inference without Repeated Sampling", "comments": "15 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist inference typically is described in terms of hypothetical\nrepeated sampling but there are advantages to an interpretation that uses a\nsingle random sample. Contemporary examples are given that indicate\nprobabilities for random phenomena are interpreted as classical probabilities,\nand this interpretation is applied to statistical inference using urn models.\nBoth classical and limiting relative frequency interpretations can be used to\ncommunicate statistical inference, and the effectiveness of each is discussed.\nRecent descriptions of p-values, confidence intervals, and power are viewed\nthrough the lens of classical probability based on a single random sample from\nthe population.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jun 2019 21:19:48 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Vos", "Paul", ""], ["Holbert", "Don", ""]]}, {"id": "1906.08409", "submitter": "Peter Gilbert", "authors": "Peter B. Gilbert", "title": "Ongoing Vaccine and Monoclonal Antibody HIV Prevention Efficacy Trials\n  and Considerations for Sequel Efficacy Trial Designs", "comments": "This manuscript has been peer reviewed and accepted for publication\n  in Statistical Communications in Infectious Diseases. P.B.G. presented this\n  research at the HIV Prevention Efficacy Trial Designs of the Future Symposium\n  in November 2018 in Seattle, with talk title 'Ongoing Vaccine and Monoclonal\n  Antibody Efficacy Trials in the HVTN and Considerations for Sequel Designs.'", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Four randomized placebo-controlled efficacy trials of a candidate vaccine or\npassively infused monoclonal antibody for prevention of HIV-1 infection are\nunderway (HVTN 702 in South African men and women; HVTN 705 in sub-Saharan\nAfrican women; HVTN 703/HPTN 081 in sub-Saharan African women; HVTN 704/HPTN\n085 in U.S., Peruvian, Brazilian, and Swiss men or transgender persons who have\nsex with men). Several challenges are posed to the optimal design of the sequel\nefficacy trials, including: (1) how to account for the evolving mosaic of\neffective prevention interventions that may be part of the trial design or\nstandard of prevention; (2) how to define viable and optimal sequel trial\ndesigns depending on the primary efficacy results and secondary 'correlates of\nprotection' results of each of the ongoing trials; and (3) how to define the\nprimary objective of sequel efficacy trials if HIV-1 incidence is expected to\nbe very low in all study arms such that a standard trial design has a steep\nopportunity cost. After summarizing the ongoing trials, I discuss statistical\nscience considerations for sequel efficacy trial designs, both generally and\nspecifically to each trial listed above. One conclusion is that the results of\n'correlates of protection' analyses, which ascertain how different host\nimmunological markers and HIV-1 viral features impact HIV-1 risk and prevention\nefficacy, have an important influence on sequel trial design. This influence is\nespecially relevant for the monoclonal antibody trials because of the focused\npre-trial hypothesis that potency and coverage of serum neutralization\nconstitutes a surrogate endpoint for HIV-1 infection... (see manuscript for the\nfull abstract)\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 01:30:35 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Gilbert", "Peter B.", ""]]}, {"id": "1906.08421", "submitter": "David Edward Williams", "authors": "Georgia Miskell, Kyle Alberti, Brandon Feenstra, Geoff S Henshaw,\n  Vasileios Papapostolou, Hamesh Patel, Andrea Polidori, Jennifer A Salmond,\n  Lena Weissert, David E Williams", "title": "Reliable data from low cost ozone sensors in a hierarchical network", "comments": "28 pages, 12 figures, Supplementray information appended has 14 pages", "journal-ref": "Atmospheric Environment 214 (2019) 116870", "doi": "10.1016/j.atmosenv.2019.116870", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We demonstrate how a hierarchical network comprising a number of compliant\nreference stations and a much larger number of low-cost sensors can deliver\nreliable high temporal-resolution ozone data at neighbourhood scales. The\nframework, demonstrated originally for a smaller scale regional network\ndeployed in the Lower Fraser Valley, BC was tested and refined using two much\nmore extensive networks of gas-sensitive semiconductor-based (GSS) sensors\ndeployed at neighbourhood scales in Los Angeles: one of ~20 and one of ~45 GSS\nozone sensors. Of these, ten sensors were co-located with different regulatory\nmeasurement stations, allowing a rigorous test of the accuracy of the\nalgorithms used for off-site calibration and adjustment of low cost sensors.\nThe method is based on adjusting the gain and offset of the low-cost sensor to\nmatch the first two moments of the probability distribution of the sensor\nresult to that of a proxy: a calibrated independent measurement (usually\nderived from regulatory monitors) whose probability distribution evaluated over\na time that emphasizes diurnal variations is similar to that at the test\nlocation. The regulatory measurement station physically closest to the low-cost\nsensor was a good proxy for most sites. The algorithms developed were\nsuccessful in detecting and correcting sensor drift, and in identifying\nlocations where geographical features resulted in significantly different\npatterns of ozone variation due to the relative dominance of different\ndispersion, emission and chemical processes. The entire network results show\nvery large variations in ozone concentration that take place on short time- and\ndistance scales across the Los-Angeles region. Such patterns were not captured\nby the more sparsely distributed stations of the existing regulatory network\nand demonstrate the need for reliable data from dense networks of monitors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 02:36:25 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Miskell", "Georgia", ""], ["Alberti", "Kyle", ""], ["Feenstra", "Brandon", ""], ["Henshaw", "Geoff S", ""], ["Papapostolou", "Vasileios", ""], ["Patel", "Hamesh", ""], ["Polidori", "Andrea", ""], ["Salmond", "Jennifer A", ""], ["Weissert", "Lena", ""], ["Williams", "David E", ""]]}, {"id": "1906.08436", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Irena Chen", "title": "Regression Analysis of Dependent Binary Data for Estimating Disease\n  Etiology from Case-Control Studies", "comments": "26 pages of main text, 3 figures and 1 table; 18 pages of\n  supplementary materials, 6 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In large-scale disease etiology studies, epidemiologists often need to use\nmultiple binary measures of unobserved causes of disease that are not perfectly\nsensitive or specific to estimate cause-specific case fractions, referred to as\n\"population etiologic fractions\" (PEFs). Despite recent methodological\nadvances, the scientific need of incorporating control data to estimate the\neffect of explanatory variables upon the PEFs, however, remains unmet. In this\npaper, we build on and extend nested partially-latent class model (npLCMs, Wu\net al., 2017) to a general framework for etiology regression analysis in\ncase-control studies. Data from controls provide requisite information about\nmeasurement specificities and covariations, which is used to correctly assign\ncause-specific probabilities for each case given her measurements. We estimate\nthe distribution of the controls' diagnostic measures given the covariates via\na separate regression model and a priori encourage simpler conditional\ndependence structures. We use Markov chain Monte Carlo for posterior inference\nof the PEF functions, cases' latent classes and the overall PEFs of policy\ninterest. We illustrate the regression analysis with simulations and show less\nbiased estimation and more valid inference of the overall PEFs than an npLCM\nanalysis omitting covariates. A regression analysis of data from a childhood\npneumonia study site reveals the dependence of pneumonia etiology upon season,\nage, disease severity and HIV status.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 04:06:50 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Wu", "Zhenke", ""], ["Chen", "Irena", ""]]}, {"id": "1906.08488", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra and Neeraj Misra", "title": "On Relative Ageing of Coherent Systems with Dependent Identically\n  Distributed Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative ageing describes how a system ages with respect to another one. The\nageing faster orders are the ones which compare the relative ageings of two\nsystems. Here, we study ageing faster orders in the hazard and the reversed\nhazard rates. We provide some sufficient conditions for proving that one\ncoherent system dominates another system with respect to ageing faster orders.\nFurther, we investigate whether the active redundancy at the component level is\nmore effective than that at the system level with respect to ageing faster\norders, for a coherent system. Furthermore, a used coherent system and a\ncoherent system made out of used components are compared with respect to ageing\nfaster orders.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 08:04:21 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Misra", "Neeraj", ""]]}, {"id": "1906.08522", "submitter": "Christian Rohrbeck", "authors": "Christian Rohrbeck and Jonathan A Tawn", "title": "Bayesian spatial clustering of extremal behaviour for hydrological\n  variables", "comments": "41", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the need for efficient inference for a range of hydrological\nextreme value problems, spatial pooling of information is the standard approach\nfor marginal tail estimation. We propose the first extreme value spatial\nclustering methods which account for both the similarity of the marginal tails\nand the spatial dependence structure of the data to determine the appropriate\nlevel of pooling. Spatial dependence is incorporated in two ways: to determine\nthe cluster selection and to account for dependence of the data over sites\nwithin a cluster when making the marginal inference. We introduce a statistical\nmodel for the pairwise extremal dependence which incorporates distance between\nsites, and accommodates our belief that sites within the same cluster tend to\nexhibit a higher degree of dependence than sites in different clusters. We use\na Bayesian framework which learns about both the number of clusters and their\nspatial structure, and that enables the inference of site-specific marginal\ndistributions of extremes to incorporate uncertainty in the clustering\nallocation. The approach is illustrated using simulations, the analysis of\ndaily precipitation levels in Norway and daily river flow levels in the UK.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 09:41:28 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Rohrbeck", "Christian", ""], ["Tawn", "Jonathan A", ""]]}, {"id": "1906.08631", "submitter": "Inbar Seroussi", "authors": "Inbar Seroussi, Nir Levy, Daniela Paolotti, Nir Sochen, and Elad\n  Yom-Tov", "title": "On the use of multiple compartment epidemiological models to describe\n  the dynamics of influenza in Europe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a multiple compartment Susceptible-Infected-Recovered (SIR) model\nto analyze the spread of several infectious diseases through different\ngeographic areas. Additionally, we propose a data-quality sensitive\noptimization framework for fitting this model to observed data.\n  We fit the model to the temporal profile of the number of people infected by\none of six influenza strains in Europe over $7$ influenza seasons. In addition\nto describing the temporal and spatial spread of influenza, the model provides\nan estimate of the inter-country and intra-country infection and recovery rates\nof each strain and in each season. We find that disease parameters remain\nrelatively stable, with a correlation greater than $0.5$ over seasons and\nstains. Clustering of influenza strains by the inferred disease parameters is\nconsistent with genome sub-types. Surprisingly, our analysis suggests that\ninter-country human mobility plays a negligible role in the spread of influenza\nin Europe. Finally, we show that the model allows the estimation of disease\nload in countries with poor or none existent data from the disease load in\nadjacent countries.\n  Our findings reveal information on the spreading mechanism of influenza and\non disease parameters. These can be used to assist in disease surveillance and\nin control of influenza as well as of other infectious pathogens in a\nheterogenic environment.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jun 2019 20:41:54 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Seroussi", "Inbar", ""], ["Levy", "Nir", ""], ["Paolotti", "Daniela", ""], ["Sochen", "Nir", ""], ["Yom-Tov", "Elad", ""]]}, {"id": "1906.08726", "submitter": "Tenglong Li", "authors": "Tenglong Li, Kenneth A. Frank", "title": "On the probability of a causal inference is robust for internal validity", "comments": "33 pages , 4 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The internal validity of observational study is often subject to debate. In\nthis study, we define the counterfactuals as the unobserved sample and intend\nto quantify its relationship with the null hypothesis statistical testing\n(NHST). We propose the probability of a causal inference is robust for internal\nvalidity, i.e., the PIV, as a robustness index of causal inference. Formally,\nthe PIV is the probability of rejecting the null hypothesis again based on both\nthe observed sample and the counterfactuals, provided the same null hypothesis\nhas already been rejected based on the observed sample. Under either\nfrequentist or Bayesian framework, one can bound the PIV of an inference based\non his bounded belief about the counterfactuals, which is often needed when the\nunconfoundedness assumption is dubious. The PIV is equivalent to statistical\npower when the NHST is thought to be based on both the observed sample and the\ncounterfactuals. We summarize the process of evaluating internal validity with\nthe PIV into an eight-step procedure and illustrate it with an empirical\nexample (i.e., Hong and Raudenbush (2005)).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:13:03 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Li", "Tenglong", ""], ["Frank", "Kenneth A.", ""]]}, {"id": "1906.08735", "submitter": "Khanh To Duc", "authors": "Duc-Khanh To, Gianfranco Adimari and Monica Chiogna", "title": "Improving estimation of the volume under the ROC surface when data are\n  missing not at random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a mean score equation-based approach to estimate\nthe the volume under the receiving operating characteristic (ROC) surface (VUS)\nof a diagnostic test, under nonignorable (NI) verification bias. The proposed\napproach involves a parametric regression model for the verification process,\nwhich accommodates for possible NI missingness in the disease status of sample\nsubjects, and may use instrumental variables, which help avoid possible\nidentifiability problems. In order to solve the mean score equation derived by\nthe chosen verification model, we preliminarily need to estimate the parameters\nof a model for the disease process, but its specification is required only for\nverified subjects under study. Then, by using the estimated verification and\ndisease probabilities, we obtain four verification bias-corrected VUS\nestimators, which are alternative to those recently proposed by To Duc et al.\n(2019), based on a full likelihood approach. Consistency and asymptotic\nnormality of the new estimators are established. Simulation experiments are\nconducted to evaluate their finite sample performances, and an application to a\ndataset from a research on epithelial ovarian cancer is presented.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 16:32:06 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["To", "Duc-Khanh", ""], ["Adimari", "Gianfranco", ""], ["Chiogna", "Monica", ""]]}, {"id": "1906.08755", "submitter": "Corli van Zyl Dr", "authors": "C van Zyl and F Lombard", "title": "Signed Sequential Rank Shiryaev-Roberts Schemes", "comments": "Submitted to Quality and Reliability Engineering International", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop Shiryaev-Roberts schemes based on signed sequential ranks to\ndetect a persistent change in location of a continuous symmetric distribution\nwith known median. The in-control properties of these schemes are distribution\nfree, hence they do not require a parametric specification of an underlying\ndensity function or the existence of any moments. Tables of control limits are\nprovided. The out-of-control average run length properties of the schemes are\ngauged via theory-based calculations and Monte Carlo simulation. Comparisons\nare made with two existing distribution-free schemes. We conclude that the\nnewly proposed scheme has much to recommend its use in practice. Implementation\nof the methodology is illustrated in an application to a data set from an\nindustrial environment.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:16:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2021 13:22:57 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["van Zyl", "C", ""], ["Lombard", "F", ""]]}, {"id": "1906.08756", "submitter": "Purusharth Saxena", "authors": "Purusharth Saxena and Madhu Kashyap Jagdeesh", "title": "Similarity indexing & GIS analysis of air pollution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pollution has become a major threat in almost all metropolitan cities around\nthe world. Currently, atmospheric scientists are working on various models that\ncould help us understand air pollution. In this paper, we have formulated a new\nmetric tool called Delhi Similarity Index (DSI). The DSI is defined as the\ngeometrical mean of the trace gases such as ozone, sulfur-dioxide and\ncarbon-monoxide, which ranges from 0 (dissimilar to Delhi) to 0.9-1 (similar to\nDelhi). The limitation of the tool concerning the result of the\nnitrous-di-oxide data set is also analyzed. Also, the GIS projections of PM 2.5\nrole for Indian cities are graphically represented. The DSI results from 2011\nto 2014 data show that Bengaluru is in the threshold of becoming as polluted\nlike Delhi with values varying from 0.8 to 0.9 (i.e. 80-90%) and Jungfraujoch\nwith a 0.65 to 0.7 (i.e. 65-70%).\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 17:17:19 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Saxena", "Purusharth", ""], ["Jagdeesh", "Madhu Kashyap", ""]]}, {"id": "1906.08832", "submitter": "Niccol\\`o Dalmasso", "authors": "Niccol\\`o Dalmasso, Robin Dunn, Benjamin LeRoy, Chad Schafer", "title": "A Flexible Pipeline for Prediction of Tropical Cyclone Paths", "comments": "4 pages. The first three authors contributed equally. Presented at\n  the ICML 2019 Workshop on \"Climate Change: How can AI Help?\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricanes and, more generally, tropical cyclones (TCs) are rare, complex\nnatural phenomena of both scientific and public interest. The importance of\nunderstanding TCs in a changing climate has increased as recent TCs have had\ndevastating impacts on human lives and communities. Moreover, good prediction\nand understanding about the complex nature of TCs can mitigate some of these\nhuman and property losses. Though TCs have been studied from many different\nangles, more work is needed from a statistical approach of providing prediction\nregions. The current state-of-the-art in TC prediction bands comes from the\nNational Hurricane Center of the National Oceanographic and Atmospheric\nAdministration (NOAA), whose proprietary model provides \"cones of uncertainty\"\nfor TCs through an analysis of historical forecast errors.\n  The contribution of this paper is twofold. We introduce a new pipeline that\nencourages transparent and adaptable prediction band development by\nstreamlining cyclone track simulation and prediction band generation. We also\nprovide updates to existing models and novel statistical methodologies in both\nareas of the pipeline, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 20:24:03 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Dalmasso", "Niccol\u00f2", ""], ["Dunn", "Robin", ""], ["LeRoy", "Benjamin", ""], ["Schafer", "Chad", ""]]}, {"id": "1906.08843", "submitter": "Arnab Chakraborty", "authors": "Arnab Chakraborty and Soumendra N. Lahiri", "title": "On Statistical Properties of A Veracity Scoring Method for Spatial Data", "comments": "37 pages, 4 figures, 6 tables, submitted to JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring veracity or reliability of noisy data is of utmost importance,\nespecially in the scenarios where the information are gathered through\nautomated systems. In a recent paper, Chakraborty et. al. (2019) have\nintroduced a veracity scoring technique for geostatistical data. The authors\nhave used a high-quality `reference' data to measure the veracity of the\nvarying-quality observations and incorporated the veracity scores in their\nanalysis of mobile-sensor generated noisy weather data to generate efficient\npredictions of the ambient temperature process. In this paper, we consider the\nscenario when no reference data is available and hence, the veracity scores\n(referred as VS) are defined based on `local' summaries of the observations. We\ndevelop a VS-based estimation method for parameters of a spatial regression\nmodel. Under a non-stationary noise structure and fairly general assumptions on\nthe underlying spatial process, we show that the VS-based estimators of the\nregression parameters are consistent. Moreover, we establish the advantage of\nthe VS-based estimators as compared to the ordinary least squares (OLS)\nestimator by analyzing their asymptotic mean squared errors. We illustrate the\nmerits of the VS-based technique through simulations and apply the methodology\nto a real data set on mass percentages of ash in coal seams in Pennsylvania.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 20:47:38 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Chakraborty", "Arnab", ""], ["Lahiri", "Soumendra N.", ""]]}, {"id": "1906.09304", "submitter": "David Benkeser", "authors": "David Benkeser, Keith Horvath, Cathy Reback, Joshua Rusow, Michael\n  Hudgens", "title": "Design and analysis considerations for a sequentially randomized HIV\n  prevention trial", "comments": null, "journal-ref": null, "doi": "10.1007/s12561-020-09274-3", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TechStep is a randomized trial of a mobile health interventions targeted\ntowards transgender adolescents. The interventions include a short message\nsystem, a mobile-optimized web application, and electronic counseling. The\nprimary outcomes are self-reported sexual risk behaviors and uptake of HIV\npreventing medication. In order that we may evaluate the efficacy of several\ndifferent combinations of interventions, the trial has a sequentially\nrandomized design. We use a causal framework to formalize the estimands of the\nprimary and key secondary analyses of the TechStep trial data. Targeted minimum\nloss-based estimators of these quantities are described and studied in\nsimulation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 19:46:53 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Benkeser", "David", ""], ["Horvath", "Keith", ""], ["Reback", "Cathy", ""], ["Rusow", "Joshua", ""], ["Hudgens", "Michael", ""]]}, {"id": "1906.09350", "submitter": "Joshua Carmichael", "authors": "Joshua D Carmichael, Robert J Nemzek", "title": "Uncertainty in the Predictive Capability of Detectors that Process\n  Waveforms from Explosions", "comments": "40 pages, 13 figures, two appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.geo-ph stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Explosions near ground generate multiple geophysical waveforms in the\nradiation-dominated range of their signature fields. Multi-phenomological\nexplosion monitoring (MultiPEM) at these ranges requires the predictive\ncapability to forecast trigger rates of digital detectors that process such\nwaveform data, and thereby accurately anticipate the probability that\nhypothetical explosions can be identified in operations. To confront this\nchallenge, we derive and compare the predicted and observed performance of\nthree digital detectors that process radio, acoustic and seismic waveform data\nthat record a small, aboveground explosion. We measure this comparison with the\npeak range in magnitude (magnitude discrepancy) over which different\nperformance curves report the same probability of detection, within an interval\nof moderate detection probability, and thereby quantify solutions to three\ntopical monitoring questions. In particular, our solutions (1) demonstrate how\nempirically parameterized detectors that operate in a variable noisy\nenvironments provide fair-to-very good forecasting capability to detect small\nexplosions, (2) show that the observed performance of a particular waveform\ndetector can better forecast performance curves constructed from different\nobservations, when compared to theoretical performance curves, and (3) provide\nan upper bound on detection uncertainty, in terms of a physical source\nattribute (magnitude)\n", "versions": [{"version": "v1", "created": "Fri, 21 Jun 2019 22:54:53 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Carmichael", "Joshua D", ""], ["Nemzek", "Robert J", ""]]}, {"id": "1906.09365", "submitter": "Megan Evans", "authors": "Megan C Evans, Grace Chiu, Philip Gibbons, Andrew K Macintosh", "title": "Quantitative evaluation of regulatory policies for reducing\n  deforestation using the bent-cable regression model", "comments": "29 pages + appendices, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reducing and redressing the effects of deforestation is a complex public\npolicy challenge, and evaluating the efficacy of such policy efforts is crucial\nfor policy learning and adaptation. Deforestation in high-income nations can\ncontribute substantially to global forest loss, despite the presence of strong\ninstitutions and high policy capacity. In Queensland, Australia, over 5 million\nhectares of native forest has been lost since 1988. Successive regulatory\npolicies have aimed to reduce deforestation in Queensland, though debate exists\nover their effect given the influence of other drivers of forest loss. Using a\nhierarchical Bayesian statistical framework, we combine satellite imagery of\nforest loss with macroeconomic, land tenure, biophysical and climatic variables\nto collectively model deforestation for 50 local government areas (LGAs) across\nQueensland. We apply the spatially explicit bent-cable regression model to\ndetect trend change that may signal a regulatory policy effect. We find that\nannual % growth in GDP was the only clear driver of LGA-specific deforestation\nafter adjusting for other covariate effects. Our model shows strong evidence of\nspatial contagion in deforestation across Queensland, and this effect is\ninfluenced by the dominant land tenure type within each LGA. We find our model\nexhibits a \"bend\" mostly between 2000 and 2007, consistent with expectations,\nbut the signal is not particularly strong due extreme variation in\ndeforestation trends between and within LGAs. Our results demonstrate that the\nbent-cable model is a promising technique for detecting system changes in\nresponse to policy interventions, but future work should be conducted at a\nnational scale to provide more data points, and incorporate more LGA-specific\ndata to improve model goodness-of-fit.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 02:00:24 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Evans", "Megan C", ""], ["Chiu", "Grace", ""], ["Gibbons", "Philip", ""], ["Macintosh", "Andrew K", ""]]}, {"id": "1906.09388", "submitter": "Leming Qu", "authors": "Leming Qu and Yang Lu", "title": "Copula Density Estimation by Finite Mixture of Parametric Copula\n  Densities", "comments": null, "journal-ref": null, "doi": "10.1080/03610918.2019.1622720", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Copula density estimation method that is based on a finite mixture of\nheterogeneous parametric copula densities is proposed here. More specifically,\nthe mixture components are Clayton, Frank, Gumbel, T, and normal copula\ndensities, which are capable of capturing lower tail,strong central, upper\ntail, heavy tail, and symmetrical elliptical dependence, respectively. The\nmodel parameters are estimated by an interior-point algorithm for the\nconstrained maximum likelihood problem. The interior-point algorithm is\ncompared with the commonly used EM algorithm. Simulation and real data\napplication show that the proposed approach is effective to model complex\ndependencies for data in dimensions beyond two or three.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 05:11:35 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Qu", "Leming", ""], ["Lu", "Yang", ""]]}, {"id": "1906.09473", "submitter": "Yang Liu", "authors": "Yang Liu and David Ruppert", "title": "Density Estimation on a Network", "comments": "38 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a novel approach to density estimation on a network. We\nformulate nonparametric density estimation on a network as a nonparametric\nregression problem by binning. Nonparametric regression using local polynomial\nkernel-weighted least squares have been studied rigorously, and its asymptotic\nproperties make it superior to kernel estimators such as the Nadaraya-Watson\nestimator. When applied to a network, the best estimator near a vertex depends\non the amount of smoothness at the vertex. Often, there are no compelling\nreasons to assume that a density will be continuous or discontinuous at a\nvertex, hence a data driven approach is proposed. To estimate the density in a\nneighborhood of a vertex, we propose a two-step procedure. The first step of\nthis pretest estimator fits a separate local polynomial regression on each edge\nusing data only on that edge, and then tests for equality of the estimates at\nthe vertex. If the null hypothesis is not rejected, then the second step\nre-estimates the regression function in a small neighborhood of the vertex,\nsubject to a joint equality constraint. Since the derivative of the density may\nbe discontinuous at the vertex, we propose a piecewise polynomial local\nregression estimate to model the change in slope. We study in detail the\nspecial case of local piecewise linear regression and derive the leading bias\nand variance terms using weighted least squares theory. We show that the\nproposed approach will remove the bias near a vertex that has been noted for\nexisting methods, which typically do not allow for discontinuity at vertices.\nFor a fixed network, the proposed method scales sub-linearly with sample size\nand it can be extended to regression and varying coefficient models on a\nnetwork. We demonstrate the workings of the proposed model by simulation\nstudies and apply it to a dendrite network data set.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jun 2019 17:22:56 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 18:04:30 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Liu", "Yang", ""], ["Ruppert", "David", ""]]}, {"id": "1906.09757", "submitter": "Xuan Yin", "authors": "Xuan Yin and Liangjie Hong", "title": "The Identification and Estimation of Direct and Indirect Effects in A/B\n  Tests through Causal Mediation Analysis", "comments": "Accepted by The 25th ACM SIGKDD Conference on Knowledge Discovery and\n  DataMining (KDD '19), August 4-8, 2019, Anchorage, AK, USA", "journal-ref": "The 25th ACM SIGKDD Conference on Knowledge Discovery and Data\n  Mining (KDD '19), August 4-8, 2019, Anchorage, AK, USA. ACM, New York, NY,\n  USA", "doi": "10.1145/3292500.3330769", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  E-commerce companies have a number of online products, such as organic\nsearch, sponsored search, and recommendation modules, to fulfill customer\nneeds. Although each of these products provides a unique opportunity for users\nto interact with a portion of the overall inventory, they are all similar\nchannels for users and compete for limited time and monetary budgets of users.\nTo optimize users' overall experiences on an E-commerce platform, instead of\nunderstanding and improving different products separately, it is important to\ngain insights into the evidence that a change in one product would induce users\nto change their behaviors in others, which may be due to the fact that these\nproducts are functionally similar. In this paper, we introduce causal mediation\nanalysis as a formal statistical tool to reveal the underlying causal\nmechanisms. Existing literature provides little guidance on cases where\nmultiple unmeasured causally-dependent mediators exist, which are common in A/B\ntests. We seek a novel approach to identify in those scenarios direct and\nindirect effects of the treatment. In the end, we demonstrate the effectiveness\nof the proposed method in data from Etsy's real A/B tests and shed lights on\ncomplex relationships between different products.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 07:22:30 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Yin", "Xuan", ""], ["Hong", "Liangjie", ""]]}, {"id": "1906.09937", "submitter": "Nil Kamal Hazra", "authors": "Nil Kamal Hazra and Neeraj Misra", "title": "Coherent systems with dependent and identically distributed components:\n  A study of relative ageing based on cumulative hazard and cumulative reversed\n  hazard rate functions", "comments": "arXiv admin note: text overlap with arXiv:1906.08488", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative ageing is an important notion which is useful to measure how a\nsystem ages relative to another one. Among all existing stochastic orders,\nthere are two important orders describing the relative ageing of two systems,\nnamely, ageing faster orders in the cumulative hazard and the cumulative\nreversed hazard rate functions. In this paper, we give some sufficient\nconditions under which one coherent system ages faster than another one with\nrespect to the aforementioned stochastic orders. Further, we show that the\nproposed sufficient conditions are satisfied for $k$-out-of-$n$ systems.\nMoreover, some numerical examples are given to illustrate the developed\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 08:08:33 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Hazra", "Nil Kamal", ""], ["Misra", "Neeraj", ""]]}, {"id": "1906.10073", "submitter": "Josephine Lamp", "authors": "Josephine Lamp, Simone Silvetti, Marc Breton, Laura Nenzi, and Lu Feng", "title": "A Logic-Based Learning Approach to Explore Diabetes Patient Behaviors", "comments": "18 pages, 10 figures, submitted to 17th International Conference on\n  Computational Methods in Systems Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Type I Diabetes (T1D) is a chronic disease in which the body's ability to\nsynthesize insulin is destroyed. It can be difficult for patients to manage\ntheir T1D, as they must control a variety of behavioral factors that affect\nglycemic control outcomes. In this paper, we explore T1D patient behaviors\nusing a Signal Temporal Logic (STL) based learning approach. STL formulas\nlearned from real patient data characterize behavior patterns that may result\nin varying glycemic control. Such logical characterizations can provide\nfeedback to clinicians and their patients about behavioral changes that\npatients may implement to improve T1D control. We present both individual- and\npopulation-level behavior patterns learned from a clinical dataset of 21 T1D\npatients.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:51:34 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Lamp", "Josephine", ""], ["Silvetti", "Simone", ""], ["Breton", "Marc", ""], ["Nenzi", "Laura", ""], ["Feng", "Lu", ""]]}, {"id": "1906.10082", "submitter": "Weitao Duan", "authors": "Weitao Duan, Qian Wang, Rogier Verhulst, Ya Xu", "title": "Scalable Online Survey Framework: from Sampling to Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advancement in technology, raw event data generated by the digital\nworld have grown tremendously. However, such data tend to be insufficient and\nnoisy when it comes to measuring user intention or satisfaction. One effective\nway to measure user experience directly is through surveys. In particular, with\nthe popularity of online surveys, extensive work has been put in to study this\nfield. Surveys at LinkedIn play a major role in influencing product and\nmarketing decisions and supporting our sales efforts. We run an increasing\nnumber of surveys that help us understand shifts in awareness and perceptions\nwith regards to our own products and also to peer companies. As the need to\nsurvey grows, both sampling and analysis of surveys have become more\nchallenging. Instead of simply multiplying the number of surveys each user\ntakes, we need a scalable approach to collect enough and representative samples\nfor each survey analysis while maintaining good user experience. In this paper,\nwe start with discussions on how we handle multiple email surveys under such\nconstraints. We then shift our discussions to challenges of in-product surveys\nand how we address them at LinkedIn through a survey study conducted across two\nmobile apps. Finally, we share how in-product surveys can be utilized as\nmonitoring tools and connect surveys with A/B testing.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 16:59:36 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Duan", "Weitao", ""], ["Wang", "Qian", ""], ["Verhulst", "Rogier", ""], ["Xu", "Ya", ""]]}, {"id": "1906.10098", "submitter": "Chen Gu", "authors": "Chen Gu, Ulrich Mok, Youssef M. Marzouk, Germ\\'an A Prieto Gomez,\n  Farrokh Sheibani, J. Brian Evans, Bradford H. Hager", "title": "Bayesian waveform-based calibration of high-pressure acoustic emission\n  systems with ball drop measurements", "comments": null, "journal-ref": "Geophysical Journal International, 2019, ggz568", "doi": "10.1093/gji/ggz568", "report-no": null, "categories": "stat.AP physics.geo-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acoustic emission (AE) is a widely used technology to study source mechanisms\nand material properties during high-pressure rock failure experiments. It is\nimportant to understand the physical quantities that acoustic emission sensors\nmeasure, as well as the response of these sensors as a function of frequency.\nThis study calibrates the newly built AE system in the MIT Rock Physics\nLaboratory using a ball-bouncing system. Full waveforms of multi-bounce events\ndue to ball drops are used to infer the transfer function of lead zirconate\ntitanate (PZT) sensors in high pressure environments. Uncertainty in the sensor\ntransfer functions is quantified using a waveform-based Bayesian approach. The\nquantification of \\textit{in situ} sensor transfer functions makes it possible\nto apply full waveform analysis for acoustic emissions at high pressures.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 17:34:39 GMT"}, {"version": "v2", "created": "Wed, 8 Jan 2020 18:35:26 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Gu", "Chen", ""], ["Mok", "Ulrich", ""], ["Marzouk", "Youssef M.", ""], ["Gomez", "Germ\u00e1n A Prieto", ""], ["Sheibani", "Farrokh", ""], ["Evans", "J. Brian", ""], ["Hager", "Bradford H.", ""]]}, {"id": "1906.10163", "submitter": "Qian Li", "authors": "Qian Li, Zhe He, Yi Guo, Hansi Zhang, Thomas J George Jr, William\n  Hogan, Neil Charness, Jiang Bian", "title": "Assessing the Validity of a a priori Patient-Trial Generalizability\n  Score using Real-world Data from a Large Clinical Data Research Network: A\n  Colorectal Cancer Clinical Trial Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing trials had not taken enough consideration of their population\nrepresentativeness, which can lower the effectiveness when the treatment is\napplied in real-world clinical practice. We analyzed the eligibility criteria\nof Bevacizumab colorectal cancer treatment trials, assessed their a priori\ngeneralizability, and examined how it affects patient outcomes when applied in\nreal-world clinical settings. To do so, we extracted patient-level data from a\nlarge collection of electronic health records (EHRs) from the OneFlorida\nconsortium. We built a zero-inflated negative binomial model using a composite\npatient-trial generalizability (cPTG) score to predict patients clinical\noutcomes (i.e., number of serious adverse events, (SAEs)). Our study results\nprovide a body of evidence that 1) the cPTG scores can predict patient\noutcomes; and 2) patients who are more similar to the study population in the\ntrials that were used to develop the treatment will have a significantly lower\npossibility to experience serious adverse events.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 18:27:28 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Li", "Qian", ""], ["He", "Zhe", ""], ["Guo", "Yi", ""], ["Zhang", "Hansi", ""], ["George", "Thomas J", "Jr"], ["Hogan", "William", ""], ["Charness", "Neil", ""], ["Bian", "Jiang", ""]]}, {"id": "1906.10252", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens, David L. Buckeridge", "title": "Bayesian Clustering for Continuous-Time Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop clustering procedures for longitudinal trajectories based on a\ncontinuous-time hidden Markov model (CTHMM) and a generalized linear\nobservation model. Specifically in this paper, we carry out finite and infinite\nmixture model-based clustering for a CTHMM and achieve inference using Markov\nchain Monte Carlo (MCMC). For a finite mixture model with prior on the number\nof components, we implement reversible-jump MCMC to facilitate the\ntrans-dimensional move between different number of clusters. For a Dirichlet\nprocess mixture model, we utilize restricted Gibbs sampling split-merge\nproposals to expedite the MCMC algorithm. We employ proposed algorithms to the\nsimulated data as well as a real data example, and the results demonstrate the\ndesired performance of the new sampler.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jun 2019 22:17:53 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2020 14:29:33 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 08:12:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""], ["Buckeridge", "David L.", ""]]}, {"id": "1906.10286", "submitter": "Suchit Mehrotra", "authors": "Suchit Mehrotra and Arnab Maity", "title": "Simultaneous Variable Selection, Clustering, and Smoothing in Function\n  on Scalar Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We address the problem of multicollinearity in a function-on-scalar\nregression model by using a prior which simultaneously selects, clusters, and\nsmooths functional effects. Our methodology groups effects of highly correlated\npredictors, performing dimension reduction without dropping relevant predictors\nfrom the model. We validate our approach via a simulation study, showing\nsuperior performance relative to existing dimension reduction approaches in the\nfunction-on-scalar literature. We also demonstrate the use of our model on a\ndata set of age specific fertility rates from the United Nations Gender\nInformation database.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 00:59:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Mehrotra", "Suchit", ""], ["Maity", "Arnab", ""]]}, {"id": "1906.10320", "submitter": "Anna Guitart Atienza", "authors": "Anna Guitart, Shi Hui Tan, Ana Fern\\'andez del R\\'io, Pei Pei Chen and\n  \\'Africa Peri\\'a\\~nez", "title": "From Non-Paying to Premium: Predicting User Conversion in Video Games\n  with Ensemble Learning", "comments": "social games, conversion prediction, ensemble methods, survival\n  analysis, online games, user behavior", "journal-ref": "ACM Foundations of Digital Games (FDG'2019), 97, 9, 2019", "doi": "10.1145/3337722.3341855", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retaining premium players is key to the success of free-to-play games, but\nmost of them do not start purchasing right after joining the game. By\nexploiting the exceptionally rich datasets recorded by modern video\ngames--which provide information on the individual behavior of each and every\nplayer--survival analysis techniques can be used to predict what players are\nmore likely to become paying (or even premium) users and when, both in terms of\ntime and game level, the conversion will take place. Here we show that a\ntraditional semi-parametric model (Cox regression), a random survival forest\n(RSF) technique and a method based on conditional inference survival ensembles\nall yield very promising results. However, the last approach has the advantage\nof being able to correct the inherent bias in RSF models by dividing the\nprocedure into two steps: first selecting the best predictor to perform the\nsplitting and then the best split point for that covariate. The proposed\nconditional inference survival ensembles method could be readily used in\noperational environments for early identification of premium players and the\nparts of the game that may prompt them to become paying users. Such knowledge\nwould allow developers to induce their conversion and, more generally, to\nbetter understand the needs of their players and provide them with a\npersonalized experience, thereby increasing their engagement and paving the way\nto higher monetization.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 04:42:53 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 01:23:24 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Guitart", "Anna", ""], ["Tan", "Shi Hui", ""], ["del R\u00edo", "Ana Fern\u00e1ndez", ""], ["Chen", "Pei Pei", ""], ["Peri\u00e1\u00f1ez", "\u00c1frica", ""]]}, {"id": "1906.10422", "submitter": "Roel Ceballos", "authors": "Merry Christ E. Manayaga, Roel F. Ceballos", "title": "Forecasting the Remittances of the Overseas Filipino Workers in the\n  Philippines", "comments": null, "journal-ref": "International Journal of Statistics and Economics, 20(3), 2019", "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to find a Box-Jenkins time series model for the monthly OFW's\nremittance in the Philippines. Forecasts of OFW's remittance for the years 2018\nand 2019 will be generated using the appropriate time series model. The data\nwere retrieved from the official website of Bangko Sentral ng Pilipinas. There\nare 108 observations, 96 of which were used in model building and the remaining\n12 observations were used in forecast evaluation. ACF and PACF were used to\nexamine the stationarity of the series. Augmented Dickey Fuller test was used\nto confirm the stationarity of the series. The data was found to have a\nseasonal component, thus, seasonality has been considered in the final model\nwhich is SARIMA (2,1,0)x(0,0,2)_12. There are no significant spikes in the ACF\nand PACF of residuals of the final model and the L-jung Box Q* test confirms\nfurther that the residuals of the model are uncorrelated. Also, based on the\nresult of the Shapiro-Wilk test for the forecast errors, the forecast errors\ncan be considered a Gaussian white noise. Considering the results of diagnostic\nchecking and forecast evaluation, SARIMA (2,1,0)x(0,0,2)_12 is an appropriate\nmodel for the series. All necessary computations were done using the R\nstatistical software.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 09:53:29 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Manayaga", "Merry Christ E.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1906.10464", "submitter": "Thordis Thorarinsdottir", "authors": "Qifen Yuan, Thordis Thorarinsdottir, Stein Beldring, Wai Kwok Wong,\n  Shaochun Huang and Chong-Yu Xu", "title": "New approach for stochastic downscaling and bias correction of daily\n  mean temperatures to a high-resolution grid", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of climate information, coarse-resolution climate projections\ncommonly need to be downscaled to a finer grid. One challenge of this\nrequirement is the modeling of sub-grid variability and the spatial and\ntemporal dependence at the finer scale. Here, a post-processing procedure is\nproposed for temperature projections that addresses this challenge. The\nprocedure employs statistical bias correction and stochastic downscaling in two\nsteps. In a first step, errors that are related to spatial and temporal\nfeatures of the first two moments of the temperature distribution at model\nscale are identified and corrected. Secondly, residual space-time dependence at\nthe finer scale is analyzed using a statistical model, from which realizations\nare generated and then combined with appropriate climate change signal to form\nthe downscaled projection fields. Using a high-resolution observational gridded\ndata product, the proposed approach is applied in a case study where\nprojections of two regional climate models from the EURO-CORDEX ensemble are\nbias-corrected and downscaled to a 1x1 km grid in the Trondelag area of Norway.\nA cross-validation study shows that the proposed procedure generates results\nthat better reflect the marginal distributional properties of the data product\nand have better consistency in space and time than empirical quantile mapping.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 11:54:28 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Yuan", "Qifen", ""], ["Thorarinsdottir", "Thordis", ""], ["Beldring", "Stein", ""], ["Wong", "Wai Kwok", ""], ["Huang", "Shaochun", ""], ["Xu", "Chong-Yu", ""]]}, {"id": "1906.10591", "submitter": "Per Sid\\'en", "authors": "Per Sid\\'en, Finn Lindgren, David Bolin, Anders Eklund and Mattias\n  Villani", "title": "Spatial 3D Mat\\'ern priors for fast whole-brain fMRI analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian whole-brain functional magnetic resonance imaging (fMRI) analysis\nwith three-dimensional spatial smoothing priors has been shown to produce\nstate-of-the-art activity maps without pre-smoothing the data. The proposed\ninference algorithms are computationally demanding however, and the proposed\nspatial priors have several less appealing properties, such as being improper\nand having infinite spatial range. We propose a statistical inference framework\nfor whole-brain fMRI analysis based on the class of Mat\\'ern covariance\nfunctions. The framework uses the Gaussian Markov random field (GMRF)\nrepresentation of possibly anisotropic spatial Mat\\'ern fields via the\nstochastic partial differential equation (SPDE) approach of Lindgren et al.\n(2011). This allows for more flexible and interpretable spatial priors, while\nmaintaining the sparsity required for fast inference in the high-dimensional\nwhole-brain setting. We develop an accelerated stochastic gradient descent\n(SGD) optimization algorithm for empirical Bayes (EB) inference of the spatial\nhyperparameters. Conditionally on the inferred hyperparameters, we make a fully\nBayesian treatment of the brain activity. The Mat\\'ern prior is applied to both\nsimulated and experimental task-fMRI data and clearly demonstrates that it is a\nmore reasonable choice than the previously used priors, using comparisons of\nactivity maps, prior simulation and cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 15:13:39 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 09:26:53 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Sid\u00e9n", "Per", ""], ["Lindgren", "Finn", ""], ["Bolin", "David", ""], ["Eklund", "Anders", ""], ["Villani", "Mattias", ""]]}, {"id": "1906.10780", "submitter": "Khurram Javed Mr", "authors": "Samuel Sokota, Ryan D'Orazio, Khurram Javed, Humza Haider, Russell\n  Greiner", "title": "Simultaneous Prediction Intervals for Patient-Specific Survival Curves", "comments": "7 pages, 7 figures, IJCAI 19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate models of patient survival probabilities provide important\ninformation to clinicians prescribing care for life-threatening and terminal\nailments. A recently developed class of models - known as individual survival\ndistributions (ISDs) - produces patient-specific survival functions that offer\ngreater descriptive power of patient outcomes than was previously possible.\nUnfortunately, at the time of writing, ISD models almost universally lack\nuncertainty quantification. In this paper, we demonstrate that an existing\nmethod for estimating simultaneous prediction intervals from samples can easily\nbe adapted for patient-specific survival curve analysis and yields accurate\nresults. Furthermore, we introduce both a modification to the existing method\nand a novel method for estimating simultaneous prediction intervals and show\nthat they offer competitive performance. It is worth emphasizing that these\nmethods are not limited to survival analysis and can be applied in any context\nin which sampling the distribution of interest is tractable. Code is available\nat https://github.com/ssokota/spie .\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 23:03:29 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Sokota", "Samuel", ""], ["D'Orazio", "Ryan", ""], ["Javed", "Khurram", ""], ["Haider", "Humza", ""], ["Greiner", "Russell", ""]]}, {"id": "1906.10792", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh and James M. Robins and Sebastien J-P.A. Haneuse and\n  Miguel A. Hern\\'an", "title": "Generalizing causal inferences from randomized trials: counterfactual\n  and graphical identification", "comments": "first upload", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When engagement with a randomized trial is driven by factors that affect the\noutcome or when trial engagement directly affects the outcome independent of\ntreatment, the average treatment effect among trial participants is unlikely to\ngeneralize to a target population. In this paper, we use counterfactual and\ngraphical causal models to examine under what conditions we can generalize\ncausal inferences from a randomized trial to the target population of\ntrial-eligible individuals. We offer an interpretation of generalizability\nanalyses using the notion of a hypothetical intervention to \"scale-up\" trial\nengagement to the target population. We consider the interpretation of\ngeneralizability analyses when trial engagement does or does not directly\naffect the outcome, highlight connections with censoring in longitudinal\nstudies, and discuss identification of the distribution of counterfactual\noutcomes via g-formula computation and inverse probability weighting. Last, we\nshow how the methods can be extended to address time-varying treatments,\nnon-adherence, and censoring.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 00:29:15 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robins", "James M.", ""], ["Haneuse", "Sebastien J-P. A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1906.10838", "submitter": "Guy Hawkins", "authors": "David Gunawan, Guy E. Hawkins, Robert Kohn, Minh-Ngoc Tran, Scott D.\n  Brown", "title": "Time-evolving psychological processes over repeated decisions", "comments": "50 pages, 12 figures, 2 tables, 3 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological experiments have participants repeat a simple task. This\nrepetition is often necessary in order to gain the statistical precision\nrequired to answer questions about quantitative theories of the psychological\nprocesses underlying performance. In such experiments, time-on-task can have\nsizable effects on performance, changing the psychological processes under\ninvestigation in interesting ways. These changes are often ignored, and the\nunderlying process is treated as static. We propose modern statistical\napproaches that are based on recent advances in particle Markov chain\nMonte-Carlo (MCMC) to extend a static model of decision-making to account for\ntime-varying changes in a psychologically plausible manner. Using data from\nthree highly-cited experiments we show that there are changes in performance\nwith time-on-task, and that these changes vary substantially over individuals.\nWe find strong evidence in favor of a hidden Markov switching process as an\nexplanation of time-varying effects. This embodies the psychological assumption\nthat participants switch between different cognitive states, representing\ndifferent modes of decision-making, and explains key long- and short-term\ndynamic effects in the data. The central idea of our approach can be applied\nquite generally to quantitative psychological theories, beyond the models and\ndata sets that we investigate.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:12:50 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 02:34:39 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Gunawan", "David", ""], ["Hawkins", "Guy E.", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Brown", "Scott D.", ""]]}, {"id": "1906.10843", "submitter": "Ercan Yildiz", "authors": "Ercan Yildiz, Joshua Safyan, Marc Harper", "title": "User Sentiment as a Success Metric: Persistent Biases Under Full\n  Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study user sentiment (reported via optional surveys) as a metric for fully\nrandomized A/B tests. Both user-level covariates and treatment assignment can\nimpact response propensity. We propose a set of consistent estimators for the\naverage and local treatment effects on treated and respondent users. We show\nthat our problem can be mapped onto the intersection of the missing data\nproblem and observational causal inference, and we identify conditions under\nwhich consistent estimators exist. We evaluate the performance of estimators\nvia simulation studies and find that more complicated models do not necessarily\nprovide superior performance.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:40:50 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Yildiz", "Ercan", ""], ["Safyan", "Joshua", ""], ["Harper", "Marc", ""]]}, {"id": "1906.10957", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz and Dagmara Nikulin", "title": "Estimation of the size of informal employment based on administrative\n  records with non-ignorable selection mechanism", "comments": null, "journal-ref": "2021", "doi": "10.1111/rssc.12481", "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we used company level administrative data from the National\nLabour Inspectorate and The Polish Social Insurance Institution in order to\nestimate the prevalence of informal employment in Poland. Since the selection\nmechanism is non-ignorable we employed a generalization of Heckman's sample\nselection model assuming non-Gaussian correlation of errors and clustering by\nincorporation of random effects. We found that 5.7% (4.6%, 7.1%; 95% CI) of\nregistered enterprises in Poland, to some extent, take advantage of the\ninformal labour force. Our study exemplifies a new approach to measuring\ninformal employment, which can be implemented in other countries. It also\ncontributes to the existing literature by providing, to the best of our\nknowledge, the first estimates of informal employment at the level of companies\nbased solely on administrative data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 10:29:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Nikulin", "Dagmara", ""]]}, {"id": "1906.11099", "submitter": "Hajime Seya", "authors": "Hajime Seya, Daiki Shiroi", "title": "A comparison of apartment rent price prediction using a large dataset:\n  Kriging versus DNN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hedonic approach based on a regression model has been widely adopted for\nthe prediction of real estate property price and rent. In particular, a spatial\nregression technique called Kriging, a method of interpolation that was\nadvanced in the field of spatial statistics, are known to enable high accuracy\nprediction in light of the spatial dependence of real estate property data.\nMeanwhile, there has been a rapid increase in machine learning-based prediction\nusing a large (big) dataset and its effectiveness has been demonstrated in\nprevious studies. However, no studies have ever shown the extent to which\npredictive accuracy differs for Kriging and machine learning techniques using\nbig data. Thus, this study compares the predictive accuracy of apartment rent\nprice in Japan between the nearest neighbor Gaussian processes (NNGP) model,\nwhich enables application of Kriging to big data, and the deep neural network\n(DNN), a representative machine learning technique, with a particular focus on\nthe data sample size (n = 10^4, 10^5, 10^6) and differences in predictive\nperformance. Our analysis showed that, with an increase in sample size, the\nout-of-sample predictive accuracy of DNN approached that of NNGP and they were\nnearly equal on the order of n = 10^6. Furthermore, it is suggested that, for\nboth higher and lower end properties whose rent price deviates from the median,\nDNN may have a higher predictive accuracy than that of NNGP.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jun 2019 14:15:31 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Seya", "Hajime", ""], ["Shiroi", "Daiki", ""]]}, {"id": "1906.11111", "submitter": "Felipe Carraro", "authors": "Felipe Carraro, Rafael Holdorf Lopez, Leandro Fleck Fadel Miguel,\n  Andr\\'e Jacomel Torii", "title": "Monte Carlo Integration with adaptive variance selection for improved\n  stochastic Efficient Global Optimization", "comments": "24 pages", "journal-ref": "Struct Multidisc Optim (2019) 60: 245", "doi": "10.1007/s00158-019-02212-y", "report-no": null, "categories": "math.NA cs.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the minimization of computational cost on evaluating\nmulti-dimensional integrals is explored. More specifically, a method based on\nan adaptive scheme for error variance selection in Monte Carlo integration\n(MCI) is presented. It uses a stochastic Efficient Global Optimization (sEGO)\nframework to guide the optimization search. The MCI is employed to approximate\nthe integrals, because it provides the variance of the error in the\nintegration. In the proposed approach, the variance of the integration error is\nincluded into a Stochastic Kriging framework by setting a target variance in\nthe MCI. We show that the variance of the error of the MCI may be controlled by\nthe designer and that its value strongly influences the computational cost and\nthe exploration ability of the optimization process. Hence, we propose an\nadaptive scheme for automatic selection of the target variance during the sEGO\nsearch. The robustness and efficiency of the proposed adaptive approach were\nevaluated on global optimization stochastic benchmark functions as well as on a\ntuned mass damper design problem. The results showed that the proposed adaptive\napproach consistently outperformed the constant approach and a multi-start\noptimization method. Moreover, the use of MCI enabled the method application in\nproblems with high number of stochastic dimensions. On the other hand, the main\nlimitation of the method is inherited from sEGO coupled with the Kriging\nmetamodel: the efficiency of the approach is reduced when the number of design\nvariables increases.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 14:04:07 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Carraro", "Felipe", ""], ["Lopez", "Rafael Holdorf", ""], ["Miguel", "Leandro Fleck Fadel", ""], ["Torii", "Andr\u00e9 Jacomel", ""]]}, {"id": "1906.11158", "submitter": "Felipe Elorrieta", "authors": "Felipe Elorrieta, Susana Eyheramendy and Wilfredo Palma", "title": "Discrete-time autoregressive model for unequally spaced time-series\n  observations", "comments": "12 pages, 8 figures, 1 table. Accepted for publication in Astronomy &\n  Astrophysics", "journal-ref": "A&A 627, A120 (2019)", "doi": "10.1051/0004-6361/201935560", "report-no": null, "categories": "astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most time-series models assume that the data come from observations that are\nequally spaced in time. However, this assumption does not hold in many diverse\nscientific fields, such as astronomy, finance, and climatology, among others.\nThere are some techniques that fit unequally spaced time series, such as the\ncontinuous-time autoregressive moving average (CARMA) processes. These models\nare defined as the solution of a stochastic differential equation. It is not\nuncommon in astronomical time series, that the time gaps between observations\nare large. Therefore, an alternative suitable approach to modeling astronomical\ntime series with large gaps between observations should be based on the\nsolution of a difference equation of a discrete process. In this work we\npropose a novel model to fit irregular time series called the complex irregular\nautoregressive (CIAR) model that is represented directly as a discrete-time\nprocess. We show that the model is weakly stationary and that it can be\nrepresented as a state-space system, allowing efficient maximum likelihood\nestimation based on the Kalman recursions. Furthermore, we show via Monte Carlo\nsimulations that the finite sample performance of the parameter estimation is\naccurate. The proposed methodology is applied to light curves from periodic\nvariable stars, illustrating how the model can be implemented to detect poor\nadjustment of the harmonic model. This can occur when the period has not been\naccurately estimated or when the variable stars are multiperiodic. Last, we\nshow how the CIAR model, through its state space representation, allows\nunobserved measurements to be forecast.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 15:20:46 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Elorrieta", "Felipe", ""], ["Eyheramendy", "Susana", ""], ["Palma", "Wilfredo", ""]]}, {"id": "1906.11323", "submitter": "Lauren Kennedy", "authors": "Lauren Kennedy, Andrew Gelman", "title": "Know your population and know your model: Using model-based regression\n  and poststratification to generalize findings beyond the observed sample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychology research focuses on interactions, and this has deep implications\nfor inference from non-representative samples. For the goal of estimating\naverage treatment effects, we propose to fit a model allowing treatment to\ninteract with background variables and then average over the distribution of\nthese variables in the population. This can be seen as an extension of\nmultilevel regression and poststratification (MRP), a method used in political\nscience and other areas of survey research, where researchers wish to\ngeneralize from a sparse and possibly non-representative sample to the general\npopulation. In this paper, we discuss areas where this method can be used in\nthe psychological sciences. We use our method to estimate the norming\ndistribution for the Big Five Personality Scale using open source data. We\nargue that large open data sources like this and other collaborative data\nsources can be combined with MRP to help resolve current challenges of\ngeneralizability and replication in psychology.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 20:08:22 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 22:07:35 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Kennedy", "Lauren", ""], ["Gelman", "Andrew", ""]]}, {"id": "1906.11373", "submitter": "Rishav Dutta", "authors": "Rishav Dutta, Ronald Yurko, Samuel Ventura", "title": "Unsupervised Methods for Identifying Pass Coverage Among Defensive Backs\n  with NFL Player Tracking Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of player tracking data for American football is in its infancy,\nsince the National Football League (NFL) released its Next Gen Stats tracking\ndata publicly for the first time in December 2018. While tracking datasets in\nother sports often contain detailed annotations of on-field events, annotations\nin the NFL's tracking data are limited. Methods for creating these annotations\ntypically require extensive human labeling, which is difficult and expensive.\nWe begin tackling this class of problems by creating annotations for pass\ncoverage types by defensive backs using unsupervised learning techniques, which\nrequire no manual labeling or human oversight. We define a set of features from\nthe NFL's tracking data that help distinguish between \"zone\" and \"man\"\ncoverage. We use Gaussian mixture modeling and hierarchical clustering to\ncreate clusters corresponding to each group, and we assign the appropriate type\nof coverage to each cluster through qualitative analysis of the plays in each\ncluster. We find that the mixture model's \"soft\" cluster assignments allow for\nmore flexibility when identifying coverage types. Our work makes possible\nseveral potential avenues of future NFL research, and we provide a basic\nexploration of these in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 22:48:38 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 06:51:55 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2020 18:15:26 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Dutta", "Rishav", ""], ["Yurko", "Ronald", ""], ["Ventura", "Samuel", ""]]}, {"id": "1906.11396", "submitter": "Fran\\c{c}ois Waldner", "authors": "Julien Radoux and Fran\\c{c}ois Waldner and Patrick Bogaert", "title": "How response designs and class proportions affect the accuracy of\n  validation data", "comments": null, "journal-ref": null, "doi": "10.3390/rs12020257", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reference data collected to validate land cover maps are generally considered\nfree of errors. In practice, however, they contain errors despite all efforts\nto minimise them. These errors then propagate up to the accuracy assessment\nstage and impact the validation results. For photo-interpreted reference data,\nthe three most widely studied sources of error are systematic incorrect\nlabelling, vigilance drops, and demographic factors. How internal estimation\nerrors, i.e., errors intrinsic to the response design, affect the accuracy of\nreference data is far less understood. We analysed the impact of estimation\nerrors for two types of legends as well as for point-based and partition-based\nresponse designs with a range of sub-sample sizes. We showed that the accuracy\nof response designs depends on the class proportions within the sampling units,\nwith complex landscapes being more prone to errors. As a result, response\ndesigns where the number of sub-samples are fixed are inefficient, and the\nlabels of reference data sets have inconsistent confidence levels. To control\nestimation errors, to guarantee high accuracy standards of validation data, and\nto minimise data collection efforts, we proposed to rely on confidence\nintervals of the photo-interpreted data to define how many sub-samples should\nbe labelled. In practice, sub-samples are iteratively selected and labelled\nuntil the estimated class proportions reach the desired level of confidence. As\na result, less effort is spent on labelling obvious cases and the spared effort\ncan be allocated to more complex cases. This approach could reduce the\nlabelling effort by 50% to 75% in our study site, with greater gains in\nhomogeneous landscapes. We contend that adopting this optimisation approach\nwill not only increase the efficiency of reference data collection but will\nalso help deliver reliable accuracy estimates to the user community.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 00:10:04 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Radoux", "Julien", ""], ["Waldner", "Fran\u00e7ois", ""], ["Bogaert", "Patrick", ""]]}, {"id": "1906.11585", "submitter": "Nil Venet", "authors": "Nil Venet and Alessandro Fass\\`o", "title": "An anisotropic model for global climate data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new, elementary way to obtain axially symmetric Gaussian\nprocesses on the sphere, in order to accommodate for the directional anisotropy\nof global climate data in geostatistical analysis.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 12:21:33 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Venet", "Nil", ""], ["Fass\u00f2", "Alessandro", ""]]}, {"id": "1906.11609", "submitter": "Sebastian Mueller", "authors": "Abraham Gutierrez, Sebastian Mueller", "title": "Quality analysis in acyclic production networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production network under examination consists of a number of\nworkstations. Each workstation is a parallel configuration of machines\nperforming the same kind of tasks on a given part. Parts move from one\nworkstation to another and at each workstation a part is assigned randomly to a\nmachine. We assume that the production network is acyclic, that is, a part does\nnot return to a workstation where it previously received service. Furthermore,\nwe assume that the quality of the end product is additive, that is, the sum of\nthe quality contributions of the machines along the production path. The\ncontribution of each machine is modeled by a separate random variable.\n  Our main result is the construction of estimators that allow pairwise and\nmultiple comparison of the means and variances of machines in the same\nworkstation. These comparisons then may lead to the identification of\nunreliable machines. We also discuss the asymptotic distributions of the\nestimators that allow the use of standard statistical tests and decision\nmaking.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:17:28 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 10:25:43 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Gutierrez", "Abraham", ""], ["Mueller", "Sebastian", ""]]}, {"id": "1906.11653", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Antonio Canale", "title": "Simultaneous Transformation and Rounding (STAR) Models for\n  Integer-Valued Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet powerful framework for modeling integer-valued data,\nsuch as counts, scores, and rounded data. The data-generating process is\ndefined by Simultaneously Transforming and Rounding (STAR) a continuous-valued\nprocess, which produces a flexible family of integer-valued distributions\ncapable of modeling zero-inflation, bounded or censored data, and over- or\nunderdispersion. The transformation is modeled as unknown for greater\ndistributional flexibility, while the rounding operation ensures a coherent\ninteger-valued data-generating process. An efficient MCMC algorithm is\ndeveloped for posterior inference and provides a mechanism for adaptation of\nsuccessful Bayesian models and algorithms for continuous data to the\ninteger-valued data setting. Using the STAR framework, we design a new Bayesian\nAdditive Regression Tree (BART) model for integer-valued data, which\ndemonstrates impressive predictive distribution accuracy for both synthetic\ndata and a large healthcare utilization dataset. For interpretable\nregression-based inference, we develop a STAR additive model, which offers\ngreater flexibility and scalability than existing integer-valued models. The\nSTAR additive model is applied to study the recent decline in Amazon river\ndolphins.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 13:56:39 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 15:27:26 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Canale", "Antonio", ""]]}, {"id": "1906.11720", "submitter": "Rodolfo Metulini", "authors": "Tullio Facchinetti, Rodolfo Metulini, Paola Zuccolotto", "title": "Detecting and classifying moments in basketball matches using sensor\n  tracked data", "comments": "8 pages, 3 figures, Conference: SIS 2019 - Smart Statistics for Smart\n  Applications - Book of short papers, editors: Giuseppe Arbia, Stefano Peluso,\n  Alessia Pini, Giulia Rivellini. ISBN 9788891915108", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics in sports is crucial to evaluate the performance of single\nplayers and the whole team. The literature proposes a number of tools for both\noffence and defence scenarios. Data coming from tracking location of players,\nin this respect, may be used to enrich the amount of useful information. In\nbasketball, however, actions are interleaved with inactive periods. This paper\ndescribes a methodological approach to automatically identify active periods\nduring a game and to classify them as offensive or defensive. The method is\nbased on the application of thresholds to players kinematic parameters, whose\nvalues undergo a tuning strategy similar to Receiver Operating Characteristic\ncurves, using a ground truth extracted from the video of the games.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:06:46 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Facchinetti", "Tullio", ""], ["Metulini", "Rodolfo", ""], ["Zuccolotto", "Paola", ""]]}, {"id": "1906.11739", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini, Maurizio Carpita", "title": "A strategy for the matching of mobile phone signals with census data", "comments": "8 pages,5 figures; Conference: SIS 2019 - Smart Statistics for Smart\n  Applications - Book of short papers, editors: Giuseppe Arbia, Stefano Peluso,\n  Alessia Pini, Giulia Rivellini. ISBN 9788891915108", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Administrative data allows us to count for the number of residents. The\ngeo-localization of people by mobile phone, by quantifying the number of people\nat a given moment in time, enriches the amount of useful information for\n\"smart\" (cities) evaluations. However, using Telecom Italia Mobile (TIM) data,\nwe are able to characterize the spatio-temporal dynamic of the presences in the\ncity of just TIM users. A strategy to estimate total presences is needed. In\nthis paper we propose a strategy to extrapolate the number of total people by\nusing TIM data only. To do so, we apply a spatial record linkage of mobile\nphone data with administrative archives using the number of residents at the\nlevel of sezione di censimento.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 15:35:32 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Metulini", "Rodolfo", ""], ["Carpita", "Maurizio", ""]]}, {"id": "1906.11904", "submitter": "Natalya Pya Arnqvist", "authors": "Natalya Pya Arnqvist, Blaise Ngendangenzwa, Eric Lindahl, Leif\n  Nilsson, Jun Yu", "title": "Effective degrees of freedom for surface finish defect detection and\n  classification", "comments": "17 pages, 12 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the primary concerns of product quality control in the automotive\nindustry is an automated detection of defects of small sizes on specular car\nbody surfaces. A new statistical learning approach is presented for surface\nfinish defect detection based on spline smoothing method for feature extraction\nand $k$-nearest neighbour probabilistic classifier. Since the surfaces are\nspecular, structured lightning reflection technique is applied for image\nacquisition. Reduced rank cubic regression splines are used to smooth the pixel\nvalues while the effective degrees of freedom of the obtained smooths serve as\ncomponents of the feature vector. A key advantage of the approach is that it\nallows reaching near zero misclassification error rate when applying standard\nlearning classifiers. We also propose probability based performance evaluation\nmetrics as alternatives to the conventional metrics. The usage of those\nprovides the means for uncertainty estimation of the predictive performance of\na classifier. Experimental classification results on the images obtained from\nthe pilot system located at Volvo GTO Cab plant in Ume{\\aa}, Sweden, show that\nthe proposed approach is much more efficient than the compared methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jun 2019 11:13:52 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Arnqvist", "Natalya Pya", ""], ["Ngendangenzwa", "Blaise", ""], ["Lindahl", "Eric", ""], ["Nilsson", "Leif", ""], ["Yu", "Jun", ""]]}, {"id": "1906.11905", "submitter": "Xinjie Lan", "authors": "Xinjie Lan", "title": "A synthetic dataset for deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for generating a synthetic dataset\nobeying Gaussian distribution. Compared to the commonly used benchmark datasets\nwith unknown distribution, the synthetic dataset has an explicit distribution,\ni.e., Gaussian distribution. Meanwhile, it has the same characteristics as the\nbenchmark dataset MNIST. As a result, we can easily apply Deep Neural Networks\n(DNNs) on the synthetic dataset. This synthetic dataset provides a novel\nexperimental tool to verify the proposed theories of deep learning.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jun 2019 05:16:40 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Lan", "Xinjie", ""]]}, {"id": "1906.11920", "submitter": "Xiaojing Wang", "authors": "Xiaojing Wang, Jingang Miao, Yunting Sun", "title": "A Python Library For Empirical Calibration", "comments": "Corrected typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with biased data samples is a common task across many statistical\nfields. In survey sampling, bias often occurs due to unrepresentative samples.\nIn causal studies with observational data, the treated versus untreated group\nassignment is often correlated with covariates, i.e., not random. Empirical\ncalibration is a generic weighting method that presents a unified view on\ncorrecting or reducing the data biases for the tasks mentioned above. We\nprovide a Python library EC to compute the empirical calibration weights. The\nproblem is formulated as convex optimization and solved efficiently in the dual\nform. Compared to existing software, EC is both more efficient and robust. EC\nalso accommodates different optimization objectives, supports weight clipping,\nand allows inexact calibration, which improves usability. We demonstrate its\nusage across various experiments with both simulated and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:18:29 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 18:58:10 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Wang", "Xiaojing", ""], ["Miao", "Jingang", ""], ["Sun", "Yunting", ""]]}, {"id": "1906.11928", "submitter": "Mason Youngblood", "authors": "Mason Youngblood", "title": "Conformity bias in the cultural transmission of music sampling\n  traditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the fundamental questions of cultural evolutionary research is how\nindividual-level processes scale up to generate population-level patterns.\nPrevious studies in music have revealed that frequency-based bias (e.g.\nconformity and novelty) drives large-scale cultural diversity in different ways\nacross domains and levels of analysis. Music sampling is an ideal research\nmodel for this process because samples are known to be culturally transmitted\nbetween collaborating artists, and sampling events are reliably documented in\nonline databases. The aim of the current study was to determine whether\nfrequency-based bias has played a role in the cultural transmission of music\nsampling traditions, using a longitudinal dataset of sampling events across\nthree decades. Firstly, we assessed whether turn-over rates of popular samples\ndiffer from those expected under neutral evolution. Next, we used agent-based\nsimulations in an approximate Bayesian computation framework to infer what\nlevel of frequency-based bias likely generated the observed data. Despite\nanecdotal evidence of novelty bias, we found that sampling patterns at the\npopulation-level are most consistent with conformity bias.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 19:34:45 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Youngblood", "Mason", ""]]}, {"id": "1906.11950", "submitter": "Tim Sullivan", "authors": "Esfandiar Nava-Yazdani, Hans-Christian Hege, T. J. Sullivan and\n  Christoph von Tycowicz", "title": "Geodesic analysis in Kendall's shape space with epidemiological\n  applications", "comments": "15 pages, 3 figures", "journal-ref": "Journal of Mathematical Imaging and Vision 62(4):549--559, 2020", "doi": "10.1007/s10851-020-00945-w", "report-no": null, "categories": "math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analytically determine Jacobi fields and parallel transports and compute\ngeodesic regression in Kendall's shape space. Using the derived expressions, we\ncan fully leverage the geometry via Riemannian optimization and thereby reduce\nthe computational expense by several orders of magnitude over common, nonlinear\nconstrained approaches. The methodology is demonstrated by performing a\nlongitudinal statistical analysis of epidemiological shape data. As an example\napplication we have chosen 3D shapes of knee bones, reconstructed from image\ndata of the Osteoarthritis Initiative (OAI). Comparing subject groups with\nincident and developing osteoarthritis versus normal controls, we find clear\ndifferences in the temporal development of femur shapes. This paves the way for\nearly prediction of incident knee osteoarthritis, using geometry data alone.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 20:31:00 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 21:53:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Nava-Yazdani", "Esfandiar", ""], ["Hege", "Hans-Christian", ""], ["Sullivan", "T. J.", ""], ["von Tycowicz", "Christoph", ""]]}, {"id": "1906.11982", "submitter": "Amrit Dhar", "authors": "Amrit Dhar, Duncan K. Ralph, Vladimir N. Minin and Frederick A. Matsen\n  IV", "title": "A Bayesian Phylogenetic Hidden Markov Model for B Cell Receptor Sequence\n  Analysis", "comments": "26 pages", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008030", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The human body is able to generate a diverse set of high affinity antibodies,\nthe soluble form of B cell receptors (BCRs), that bind to and neutralize\ninvading pathogens. The natural development of BCRs must be understood in order\nto design vaccines for highly mutable pathogens such as influenza and HIV. BCR\ndiversity is induced by naturally occurring combinatorial \"V(D)J\"\nrearrangement, mutation, and selection processes. Most current methods for BCR\nsequence analysis focus on separately modeling the above processes. Statistical\nphylogenetic methods are often used to model the mutational dynamics of BCR\nsequence data, but these techniques do not consider all the complexities\nassociated with B cell diversification such as the V(D)J rearrangement process.\nIn particular, standard phylogenetic approaches assume the DNA bases of the\nprogenitor (or \"naive\") sequence arise independently and according to the same\ndistribution, ignoring the complexities of V(D)J rearrangement. In this paper,\nwe introduce a novel approach to Bayesian phylogenetic inference for BCR\nsequences that is based on a phylogenetic hidden Markov model (phylo-HMM). This\ntechnique not only integrates a naive rearrangement model with a phylogenetic\nmodel for BCR sequence evolution but also naturally accounts for uncertainty in\nall unobserved variables, including the phylogenetic tree, via posterior\ndistribution sampling.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jun 2019 22:10:32 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["Dhar", "Amrit", ""], ["Ralph", "Duncan K.", ""], ["Minin", "Vladimir N.", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1906.12000", "submitter": "Dennis Feehan", "authors": "Dennis M. Feehan, Gabriel M. Borges", "title": "Estimating adult death rates from sibling histories: A network approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hundreds of millions of people live in countries that do not have complete\ndeath registration systems, meaning that most deaths are not recorded and\ncritical quantities like life expectancy cannot be directly measured. The\nsibling survival method is a leading approach to estimating adult mortality in\nthe absence of death registration. The idea is to ask a survey respondent to\nenumerate her siblings and to report about their survival status. In many\ncountries and time periods, sibling survival data are the only\nnationally-representative source of information about adult mortality. Although\na huge amount of sibling survival data has been collected, important\nmethodological questions about the method remain unresolved. To help make\nprogress on this issue, we propose re-framing the sibling survival method as a\nnetwork sampling problem. This approach enables us to formally derive\nstatistical estimators for sibling survival data. Our derivation clarifies the\nprecise conditions that sibling history estimates rely upon; it leads to\ninternal consistency checks that can help assess data and reporting quality;\nand it reveals important quantities that could potentially be measured to relax\nassumptions in the future. We introduce the R package siblingsurvival, which\nimplements the methods we describe.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 00:03:22 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Feehan", "Dennis M.", ""], ["Borges", "Gabriel M.", ""]]}, {"id": "1906.12074", "submitter": "Santosh Kumar", "authors": "Peter J. Forrester, Santosh Kumar", "title": "Recursion scheme for the largest $\\beta$-Wishart-Laguerre eigenvalue and\n  Landauer conductance in quantum transport", "comments": "Published version; 20 pages, 2 figures in the main text + 2 in the\n  Mathematica code towards the end", "journal-ref": "Journal of Physics A: Mathematical and Theoretical, Volume 52,\n  Page 42LT02, Year 2019", "doi": "10.1088/1751-8121/ab433c", "report-no": null, "categories": "math-ph cond-mat.mes-hall cond-mat.stat-mech math.MP stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The largest eigenvalue distribution of the Wishart-Laguerre ensemble, indexed\nby Dyson parameter $\\beta$ and Laguerre parameter $a$, is fundamental in\nmultivariate statistics and finds applications in diverse areas. Based on a\ngeneralization of the Selberg integral, we provide an effective recursion\nscheme to compute this distribution explicitly in both the original model, and\na fixed-trace variant, for $a,\\beta$ non-negative integers and finite matrix\nsize. For $\\beta = 2$ this circumvents known symbolic evaluation based on\ndeterminants which become impractical for large dimensions. Our exact results\nhave immediate applications in the areas of multiple channel communication and\nbipartite entanglement. Moreover, we are also led to the exact solution of a\nlong standing problem of finding a general result for Landauer conductance\ndistribution in a chaotic mesoscopic cavity with two ideal leads. Thus far,\nexact closed-form results for this were available only in the Fourier-Laplace\nspace or could be obtained on a case-by-case basis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:44:33 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 17:59:11 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Forrester", "Peter J.", ""], ["Kumar", "Santosh", ""]]}, {"id": "1906.12077", "submitter": "Remi Flamary", "authors": "Laurent Dragoni, R\\'emi Flamary, Karim Lounici, Patricia\n  Reynaud-Bouret", "title": "Large scale Lasso with windowed active set for convolutional spike\n  sorting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.OC stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike sorting is a fundamental preprocessing step in neuroscience that is\ncentral to access simultaneous but distinct neuronal activities and therefore\nto better understand the animal or even human brain. But numerical complexity\nlimits studies that require processing large scale datasets in terms of number\nof electrodes, neurons, spikes and length of the recorded signals. We propose\nin this work a novel active set algorithm aimed at solving the Lasso for a\nclassical convolutional model. Our algorithm can be implemented efficiently on\nparallel architecture and has a linear complexity w.r.t. the temporal\ndimensionality which ensures scaling and will open the door to online spike\nsorting. We provide theoretical results about the complexity of the algorithm\nand illustrate it in numerical experiments along with results about the\naccuracy of the spike recovery and robustness to the regularization parameter.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 07:48:04 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Dragoni", "Laurent", ""], ["Flamary", "R\u00e9mi", ""], ["Lounici", "Karim", ""], ["Reynaud-Bouret", "Patricia", ""]]}, {"id": "1906.12106", "submitter": "Jaco Visagie", "authors": "I.J.H. Visagie and F. Lombard", "title": "On the conditional distribution of the mean of the two closest among a\n  set of three observations", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical analyses of raw materials are often repeated in duplicate or\ntriplicate. The assay values obtained are then combined using a predetermined\nformula to obtain an estimate of the true value of the material of interest.\nWhen duplicate observations are obtained, their average typically serves as an\nestimate of the true value. On the other hand, the \"best of three\" method\ninvolves taking three measurements and using the average of the two closest\nones as estimate of the true value.\n  In this paper, we consider another method which potentially involves three\nmeasurements. Initially two measurements are obtained and if their difference\nis sufficiently small, their average is taken as estimate of the true value.\nHowever, if the difference is too large then a third independent measurement is\nobtained. The estimator is then defined as the average between the third\nobservation and the one among the first two which is closest to it.\n  Our focus in the paper is the conditional distribution of the estimate in\ncases where the initial difference is too large. We find that the conditional\ndistributions are markedly different under the assumption of a normal\ndistribution and a Laplace distribution.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 09:11:25 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Visagie", "I. J. H.", ""], ["Lombard", "F.", ""]]}, {"id": "1906.12121", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean and Alberto De Luca and Chantal M. W. Tax and Max A.\n  Viergever and Alexander Leemans", "title": "Automated characterization of noise distributions in diffusion MRI data", "comments": "v3: Peer reviewed version v2: Manuscript as submitted to Medical\n  image analysis v1: Manuscript as submitted to Magnetic resonance in medicine", "journal-ref": "Medical Image Analysis, Volume 65, 2020, 101758, ISSN 1361-8415", "doi": "10.1016/j.media.2020.101758", "report-no": null, "categories": "eess.IV stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the noise distribution in diffusion MRI is the centerpiece to\nquantify uncertainties arising from the acquisition process. Accurate\nestimation beyond textbook distributions often requires information about the\nacquisition process, which is usually not available. We introduce two new\nautomated methods using the moments and maximum likelihood equations of the\nGamma distribution to estimate all unknown parameters using only the magnitude\ndata. A rejection step is used to make the framework automatic and robust to\nartifacts. Simulations were created for two diffusion weightings with parallel\nimaging. Furthermore, MRI data of a water phantom with different combinations\nof parallel imaging were acquired. Finally, experiments on freely available\ndatasets are used to assess reproducibility when limited information about the\nacquisition protocol is available. Additionally, we demonstrated the\napplicability of the proposed methods for a bias correction and denoising task\non an in vivo dataset. A generalized version of the bias correction framework\nfor non integer degrees of freedom is also introduced. The proposed framework\nis compared with three other algorithms with datasets from three vendors,\nemploying different reconstruction methods. Simulations showed that assuming a\nRician distribution can lead to misestimation of the noise distribution in\nparallel imaging. Results showed that signal leakage in multiband can also lead\nto a misestimation of the noise distribution. Repeated acquisitions of in vivo\ndatasets show that the estimated parameters are stable and have lower\nvariability than compared methods. Results show that the proposed methods\nreduce the appearance of noise at high b-value. The proposed algorithms herein\ncan estimate both parameters of the noise distribution automatically, are\nrobust to signal leakage artifacts and perform best when used on acquired noise\nmaps.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:08:24 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 11:08:33 GMT"}, {"version": "v3", "created": "Mon, 3 Feb 2020 13:22:40 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["St-Jean", "Samuel", ""], ["De Luca", "Alberto", ""], ["Tax", "Chantal M. W.", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1906.12331", "submitter": "Devashish Khulbe", "authors": "Devashish Khulbe and Manu Pathak", "title": "Modeling Food Popularity Dependencies using Social Media data", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise in popularity of major social media platforms have enabled people to\nshare photos and textual information about their daily life. One of the popular\ntopics about which information is shared is food. Since a lot of media about\nfood are attributed to particular locations and restaurants, information like\nspatio-temporal popularity of various cuisines can be analyzed. Tracking the\npopularity of food types and retail locations across space and time can also be\nuseful for business owners and restaurant investors. In this work, we present\nan approach using off-the shelf machine learning techniques to identify trends\nand popularity of cuisine types in an area using geo-tagged data from social\nmedia, Google images and Yelp. After adjusting for time, we use the Kernel\nDensity Estimation to get hot spots across the location and model the\ndependencies among food cuisines popularity using Bayesian Networks. We\nconsider the Manhattan borough of New York City as the location for our\nanalyses but the approach can be used for any area with social media data and\ninformation about retail businesses.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jun 2019 04:31:39 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2019 16:20:18 GMT"}], "update_date": "2019-12-11", "authors_parsed": [["Khulbe", "Devashish", ""], ["Pathak", "Manu", ""]]}]