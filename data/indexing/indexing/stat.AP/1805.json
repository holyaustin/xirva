[{"id": "1805.00159", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Shirley Ho, Jonathan Blazek, Siyu He, Rachel Mandelbaum,\n  Peter Melchior, Sukhdeep Singh", "title": "Detecting Galaxy-Filament Alignments in the Sloan Digital Sky Survey III", "comments": "14 pages, 13 figures. Accepted to the MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stz539", "report-no": null, "categories": "astro-ph.CO stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous studies have shown the filamentary structures in the cosmic web\ninfluence the alignments of nearby galaxies. We study this effect in the LOWZ\nsample of the Sloan Digital Sky Survey using the \"Cosmic Web Reconstruction\"\nfilament catalogue. We find that LOWZ galaxies exhibit a small but\nstatistically significant alignment in the direction parallel to the\norientation of nearby filaments. This effect is detectable even in the absence\nof nearby galaxy clusters, which suggests it is an effect from the matter\ndistribution in the filament. A nonparametric regression model suggests that\nthe alignment effect with filaments extends over separations of 30-40 Mpc. We\nfind that galaxies that are bright and early-forming align more strongly with\nthe directions of nearby filaments than those that are faint and late-forming;\nhowever, trends with stellar mass are less statistically significant, within\nthe narrow range of stellar mass of this sample.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 02:50:16 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 05:00:58 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Ho", "Shirley", ""], ["Blazek", "Jonathan", ""], ["He", "Siyu", ""], ["Mandelbaum", "Rachel", ""], ["Melchior", "Peter", ""], ["Singh", "Sukhdeep", ""]]}, {"id": "1805.00414", "submitter": "Sreekar Vadlamani", "authors": "Adway Mitra, Amit Apte, Rama Govindarajan, Vishal Vasan, Sreekar\n  Vadlamani", "title": "A Discrete View of the Indian Monsoon to Identify Spatial Patterns of\n  Rainfall", "comments": null, "journal-ref": "Dynamics and Statistics of the Climate System, Oxford University\n  Press, 2018", "doi": "10.1093/climsys/dzy009", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a representation of the Indian summer monsoon rainfall in terms of\na probabilistic model based on a Markov Random Field, consisting of discrete\nstate variables representing low and high rainfall at grid-scale and daily\nrainfall patterns across space and in time. These discrete states are\nconditioned on observed daily gridded rainfall data from the period 2000-2007.\nThe model gives us a set of 10 spatial patterns of daily monsoon rainfall over\nIndia, which are robust over a range of user-chosen parameters as well as\ncoherent in space and time. Each day in the monsoon season is assigned\nprecisely one of the spatial patterns, that approximates the spatial\ndistribution of rainfall on that day. Such approximations are quite accurate\nfor nearly 95% of the days. Remarkably, these patterns are representative (with\nsimilar accuracy) of the monsoon seasons from 1901 to 2000 as well. Finally, we\ncompare the proposed model with alternative approaches to extract spatial\npatterns of rainfall, using empirical orthogonal functions as well as\nclustering algorithms such as K-means and spectral clustering.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 16:16:05 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mitra", "Adway", ""], ["Apte", "Amit", ""], ["Govindarajan", "Rama", ""], ["Vasan", "Vishal", ""], ["Vadlamani", "Sreekar", ""]]}, {"id": "1805.00420", "submitter": "Sreekar Vadlamani", "authors": "Adway Mitra, Amit Apte, Rama Govindarajan, Vishal Vasan, Sreekar\n  Vadlamani", "title": "Spatio-temporal Patterns of Indian Monsoon Rainfall", "comments": null, "journal-ref": "Dynamics and Statistics of the Climate System, Oxford University\n  Press, 3, 1, 2019", "doi": "10.1093/climsys/dzy010", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The primary objective of this paper is to analyze a set of canonical spatial\npatterns that approximate the daily rainfall across the Indian region, as\nidentified in the companion paper where we developed a discrete representation\nof the Indian summer monsoon rainfall using state variables with\nspatio-temporal coherence maintained using a Markov Random Field prior. In\nparticular, we use these spatio-temporal patterns to study the variation of\nrainfall during the monsoon season. Firstly, the ten patterns are divided into\nthree families of patterns distinguished by their total rainfall amount and\ngeographic spread. These families are then used to establish `active' and\n`break' spells of the Indian monsoon at the all-India level. Subsequently, we\ncharacterize the behavior of these patterns in time by estimating probabilities\nof transition from one pattern to another across days in a season. Patterns\ntend to be `sticky': the self-transition is the most common. We also identify\nmost commonly occurring sequences of patterns. This leads to a simple seasonal\nevolution model for the summer monsoon rainfall. The discrete representation\nintroduced in the companion paper also identifies typical temporal rainfall\npatterns for individual locations. This enables us to determine wet and dry\nspells at local and regional scales. Lastly, we specify sets of locations that\ntend to have such spells simultaneously, and thus come up with a new\nregionalization of the landmass.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 16:35:46 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Mitra", "Adway", ""], ["Apte", "Amit", ""], ["Govindarajan", "Rama", ""], ["Vasan", "Vishal", ""], ["Vadlamani", "Sreekar", ""]]}, {"id": "1805.00628", "submitter": "Yuren Zhou", "authors": "Yuren Zhou, Billy Pik Lik Lau, Chau Yuen, Bige Tun\\c{c}er, Erik\n  Wilhelm", "title": "Understanding Urban Human Mobility through Crowdsensed Data", "comments": "This manuscript is published in IEEE Communications Magazine 56.11\n  (2018): 52-59. Please refer to the published version at\n  https://ieeexplore.ieee.org/abstract/document/8539021", "journal-ref": "IEEE Communications Magazine 56.11 (2018): 52-59", "doi": "10.1109/MCOM.2018.1700569", "report-no": null, "categories": "cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how people move in the urban area is important for solving\nurbanization issues, such as traffic management, urban planning, epidemic\ncontrol, and communication network improvement. Leveraging recent availability\nof large amounts of diverse crowdsensed data, many studies have made\ncontributions to this field in various aspects. They need proper review and\nsummary. In this paper, therefore, we first review these recent studies with a\nproper taxonomy with corresponding examples. Then, based on the experience\nlearnt from the studies, we provide a comprehensive tutorial for future\nresearch, which introduces and discusses popular crowdsensed data types,\ndifferent human mobility subjects, and common data preprocessing and analysis\nmethods. Special emphasis is made on the matching between data types and\nmobility subjects. Finally, we present two research projects as case studies to\ndemonstrate the entire process of understanding urban human mobility through\ncrowdsensed data in city-wide scale and building-wide scale respectively.\nBeyond demonstration purpose, the two case studies also make contributions to\ntheir category of certain crowdsensed data type and mobility subject.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 05:20:23 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 08:37:21 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Zhou", "Yuren", ""], ["Lau", "Billy Pik Lik", ""], ["Yuen", "Chau", ""], ["Tun\u00e7er", "Bige", ""], ["Wilhelm", "Erik", ""]]}, {"id": "1805.00874", "submitter": "Jaime San Martin", "authors": "Nancy Lacourly, Jaime San Martin, Monica Silva, Paula Uribe", "title": "IRT scoring and the principle of consistent order", "comments": "15 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IRT models are being increasingly used worldwide for test construction and\nscoring. The study examines the practical implications of estimating individual\nscores in a paper-and-pencil high-stakes test using 2PL and 3PL models,\nspecifically whether the principle of consistent order holds when scoring with\nIRT. The principle states that student A, who answers the same (or a larger)\nnumber of items of greater difficulty than student B, should outscore B.\nResults of analyses conducted using actual scores from the Chilean national\nadmission test in mathematics indicate the principle does not hold when scoring\nwith 2PL or 3PL models. Students who answer more items and of greater\ndifficulty may be assigned lower scores. The findings can be explained by\nexamining the mathematical models, since estimated ability scores are an\nincreasing function of the accumulated estimated discriminations for the\ncorrect items, not their difficulty. For high stakes tests the decision to use\ncomplex model should therefore be a matter of serious deliberation for policy\nmakers and test experts, since fairness and transparency may be compromised.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2018 21:28:42 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Lacourly", "Nancy", ""], ["Martin", "Jaime San", ""], ["Silva", "Monica", ""], ["Uribe", "Paula", ""]]}, {"id": "1805.00875", "submitter": "Tatsuki Inoue", "authors": "Tatsuki Inoue and Kota Ogasawara", "title": "Chain effects of clean water: The Mills-Reincke phenomenon in early\n  twentieth-century Japan", "comments": "17 pages, 1 figures, 4 tables, and appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study explores the validity of chain effects of clean water, which are\nknown as the \"Mills-Reincke phenomenon,\" in early twentieth-century Japan.\nRecent studies have reported that water purifications systems are responsible\nfor huge contributions to human capital. Although some studies have\ninvestigated the instantaneous effects of water-supply systems in pre-war\nJapan, little is known about the chain effects of these systems. By analyzing\ncity-level cause-specific mortality data from 1922-1940, we find that a decline\nin typhoid deaths by one per 1,000 people decreased the risk of death due to\nnon-waterborne diseases such as tuberculosis and pneumonia by 0.742-2.942 per\n1,000 people. Our finding suggests that the observed Mills-Reincke phenomenon\ncould have resulted in the relatively rapid decline in the mortality rate in\nearly twentieth-century Japan.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 10:48:24 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 04:07:06 GMT"}, {"version": "v3", "created": "Sun, 25 Aug 2019 06:23:30 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Inoue", "Tatsuki", ""], ["Ogasawara", "Kota", ""]]}, {"id": "1805.01098", "submitter": "Matthieu Vignes", "authors": "Olivia Angelin-Bonnet, Patrick J. Biggs and Matthieu Vignes", "title": "Gene regulatory networks: a primer in biological processes and\n  statistical modelling", "comments": "This chapter will appear in the forthcoming book \"Gene Regulatory\n  Networks: Methods and Protocols\", published by Springer Nature", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling gene regulatory networks not only requires a thorough understanding\nof the biological system depicted but also the ability to accurately represent\nthis system from a mathematical perspective. Throughout this chapter, we aim to\nfamiliarise the reader with the biological processes and molecular factors at\nplay in the process of gene expression regulation.We first describe the\ndifferent interactions controlling each step of the expression process, from\ntranscription to mRNA and protein decay. In the second section, we provide\nstatistical tools to accurately represent this biological complexity in the\nform of mathematical models. Amongst other considerations, we discuss the\ntopological properties of biological networks, the application of deterministic\nand stochastic frameworks and the quantitative modelling of regulation. We\nparticularly focus on the use of such models for the simulation of expression\ndata that can serve as a benchmark for the testing of network inference\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 03:31:16 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Angelin-Bonnet", "Olivia", ""], ["Biggs", "Patrick J.", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1805.01143", "submitter": "Shahin Boluki", "authors": "Shahin Boluki, Xiaoning Qian, Edward R. Dougherty", "title": "Experimental Design via Generalized Mean Objective Cost of Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mean objective cost of uncertainty (MOCU) quantifies the performance cost\nof using an operator that is optimal across an uncertainty class of systems as\nopposed to using an operator that is optimal for a particular system.\nMOCU-based experimental design selects an experiment to maximally reduce MOCU,\nthereby gaining the greatest reduction of uncertainty impacting the operational\nobjective. The original formulation applied to finding optimal system\noperators, where optimality is with respect to a cost function, such as\nmean-square error; and the prior distribution governing the uncertainty class\nrelates directly to the underlying physical system. Here we provide a\ngeneralized MOCU and the corresponding experimental design. We then demonstrate\nhow this new formulation includes as special cases MOCU-based experimental\ndesign methods developed for materials science and genomic networks when there\nis experimental error. Most importantly, we show that the classical Knowledge\nGradient and Efficient Global Optimization experimental design procedures are\nactually implementations of MOCU-based experimental design under their modeling\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 07:31:20 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Boluki", "Shahin", ""], ["Qian", "Xiaoning", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1805.01271", "submitter": "Zachary Binney", "authors": "Zachary O. Binney, Kyle E. Hammond, Mitchel Klein, Michael Goodman, A.\n  Cecile J.W. Janssens", "title": "NFL Injuries Before and After the 2011 Collective Bargaining Agreement\n  (CBA)", "comments": "22 pages, 4 figures, 5 tables; conference presentation at JSM 2018;\n  lay version at FootballOutsiders.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The National Football League's (NFL) 2011 collective bargaining agreement\n(CBA) with its players placed a number of contact and quantity limitations on\npractices and workouts. Some coaches and others have expressed a concern that\nthis has led to poor conditioning and a subsequent increase in injuries. We\nsought to assess whether the 2011 CBA's practice restrictions affected the\nnumber of overall, conditioning-dependent, and/or non-conditioning-dependent\ninjuries in the NFL or the number of games missed due to those injuries. The\nstudy population was player-seasons from 2007-2016. We included regular season,\nnon-illness, non-head, game-loss injuries. Injuries were identified using a\ndatabase from Football Outsiders. The primary outcomes were overall,\nconditioning-dependent and non-conditioning-dependent injury counts by season.\nWe examined time trends in injury counts before (2007-2010) and after\n(2011-2016) the CBA using a Poisson interrupted time series model. The number\nof game-loss regular season, non-head, non-illness injuries grew from 701 in\n2007 to 804 in 2016 (15% increase). The number of regular season weeks missed\nexhibited a similar increase. Conditioning-dependent injuries increased from\n197 in 2007 to 271 in 2011 (38% rise), but were lower and remained relatively\nunchanged at 220-240 injuries per season thereafter. Non-conditioning injuries\ndecreased by 37% in the first three years of the new CBA before returning to\nhistoric levels in 2014-2016. Poisson models for all, conditioning-dependent,\nand non-conditioning-dependent game-loss injury counts did not show\nstatistically significant or meaningful detrimental changes associated with the\nCBA. We did not observe an increase in injuries following the 2011 CBA. Other\nconcurrent injury-related rule and regulation changes limit specific causal\ninferences about the practice restrictions, however.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 12:55:24 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Binney", "Zachary O.", ""], ["Hammond", "Kyle E.", ""], ["Klein", "Mitchel", ""], ["Goodman", "Michael", ""], ["Janssens", "A. Cecile J. W.", ""]]}, {"id": "1805.01284", "submitter": "Can Yang", "authors": "Jian Huang and Yuling Jiao and Jin Liu and Can Yang", "title": "REMI: Regression with marginal information and its application in\n  genome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider the problem of variable selection and estimation\nin high-dimensional linear regression models when the complete data are not\naccessible, but only certain marginal information or summary statistics are\navailable. This problem is motivated from the Genome-wide association studies\n(GWAS) that have been widely used to identify risk variants underlying complex\nhuman traits/diseases. With a large number of completed GWAS, statistical\nmethods using summary statistics become more and more important because of\nrestricted accessibility to individual-level data sets. Theoretically\nguaranteed methods are highly demanding to advance the statistical inference\nwith a large amount of available marginal information. Here we propose an\n$\\ell_1$ penalized approach, REMI, to estimate high dimensional regression\ncoefficients with marginal information and external reference samples. We\nestablish an upper bound on the error of the REMI estimator, which has the same\norder as that of the minimax error bound of Lasso with complete\nindividual-level data. In particular, when marginal information is obtained\nfrom a large number of samples together with a small number of reference\nsamples, REMI yields good estimation and prediction results, and outperforms\nthe Lasso because the sample size of accessible individual-level data can be\nlimited. Through simulation studies and real data analysis of the NFBC1966 GWAS\ndata set, we demonstrate that REMI can be widely applicable. The developed R\npackage and the codes to reproduce all the results are available at\nhttps://github.com/gordonliu810822/REMI\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 13:20:41 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Liu", "Jin", ""], ["Yang", "Can", ""]]}, {"id": "1805.01586", "submitter": "Somya Mohanty", "authors": "Bin Luo, Qi Zhang, Somya D. Mohanty", "title": "Data-Driven Exploration of Factors Affecting Federal Student Loan\n  Repayment", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.OH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Student loans occupy a significant portion of the federal budget, as well as,\nthe largest financial burden in terms of debt for graduates. This paper\nexplores data-driven approaches towards understanding the repayment of such\nloans. Using statistical and machine learning models on the College Scorecard\nData, this research focuses on extracting and identifying key factors affecting\nthe repayment of a student loan. The specific factors can be used to develop\nmodels which provide predictive capability towards repayment rate, detect\nirregularities/non-repayment, and help understand the intricacies of student\nloans.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 15:02:35 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Luo", "Bin", ""], ["Zhang", "Qi", ""], ["Mohanty", "Somya D.", ""]]}, {"id": "1805.01603", "submitter": "Alexandra Chronopoulou", "authors": "Reza Yousefi Maragheh, Alexandra Chronopoulou, James Mario Davis", "title": "A Customer Choice Model with HALO Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an extension to the multinomial logit (MNL) model,\nthe Halo MNL, that takes into account the interaction effects among products in\nan assortment. In particular, this model incorporates pairwise interactions of\nitems in an effort to describe positive/negative effects among products that\nare present/absent in the assortment. Furthermore, we are interested in\nestablishing sufficient conditions for identifiability, in order to build\nrobust estimation methods. Under strict identifiability conditions, we use\nmaximum likelihood to estimate the model parameters for which we derive closed\nformulas. We also perform simulation experiments, in order to numerically\nevaluate our method, study the accuracy of the estimators and compare it with\nthe MNL. Last, we fit our model in the Hotel Chain dataset in Bodea et al., and\nwe compare it with MNL in terms of efficiency, accuracy and robustness. We\nconclude that for rich enough datasets the model that includes interaction\neffects performs better in terms of how well it fits the data.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 04:40:31 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Maragheh", "Reza Yousefi", ""], ["Chronopoulou", "Alexandra", ""], ["Davis", "James Mario", ""]]}, {"id": "1805.01608", "submitter": "Matthieu Vignes", "authors": "Alex White and Matthieu Vignes", "title": "Causal Queries from Observational Data in Biological Systems via\n  Bayesian Networks: An Empirical Study in Small Networks", "comments": "This chapter will appear in the forthcoming book \"Gene Regulatory\n  Networks: Methods and Protocols\", published by Springer Nature", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM q-bio.MN stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological networks are a very convenient modelling and visualisation tool to\ndiscover knowledge from modern high-throughput genomics and postgenomics data\nsets. Indeed, biological entities are not isolated, but are components of\ncomplex multi-level systems. We go one step further and advocate for the\nconsideration of causal representations of the interactions in living\nsystems.We present the causal formalism and bring it out in the context of\nbiological networks, when the data is observational. We also discuss its\nability to decipher the causal information flow as observed in gene expression.\nWe also illustrate our exploration by experiments on small simulated networks\nas well as on a real biological data set.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 05:09:48 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["White", "Alex", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1805.01868", "submitter": "Jongbin Jung", "authors": "Jongbin Jung, Ravi Shroff, Avi Feller, Sharad Goel", "title": "Algorithmic Decision Making in the Presence of Unmeasured Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a variety of complex decision-making tasks, from doctors prescribing\ntreatment to judges setting bail, machine learning algorithms have been shown\nto outperform expert human judgments. One complication, however, is that it is\noften difficult to anticipate the effects of algorithmic policies prior to\ndeployment, making the decision to adopt them risky. In particular, one\ngenerally cannot use historical data to directly observe what would have\nhappened had the actions recommended by the algorithm been taken. One standard\nstrategy is to model potential outcomes for alternative decisions assuming that\nthere are no unmeasured confounders (i.e., to assume ignorability). But if this\nignorability assumption is violated, the predicted and actual effects of an\nalgorithmic policy can diverge sharply. In this paper we present a flexible,\nBayesian approach to gauge the sensitivity of predicted policy outcomes to\nunmeasured confounders. We show that this policy evaluation problem is a\ngeneralization of estimating heterogeneous treatment effects in observational\nstudies, and so our methods can immediately be applied to that setting.\nFinally, we show, both theoretically and empirically, that under certain\nconditions it is possible to construct near-optimal algorithmic policies even\nwhen ignorability is violated. We demonstrate the efficacy of our methods on a\nlarge dataset of judicial actions, in which one must decide whether defendants\nawaiting trial should be required to pay bail or can be released without\npayment.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:29:14 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Jung", "Jongbin", ""], ["Shroff", "Ravi", ""], ["Feller", "Avi", ""], ["Goel", "Sharad", ""]]}, {"id": "1805.02109", "submitter": "Gaurav Sood", "authors": "Gaurav Sood, Suriyan Laohaprapanon", "title": "Predicting Race and Ethnicity From the Sequence of Characters in a Name", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To answer questions about racial inequality, we often need a way to infer\nrace and ethnicity from a name. Until now, a bulk of the focus has been on\noptimally exploiting the last names list provided by the Census Bureau. But\nthere is more information in the first names, especially for African Americans.\nTo estimate the relationship between full names and race, we exploit the\nFlorida voter registration data and the Wikipedia data. In particular, we model\nthe relationship between the sequence of characters in a name, and race and\nethnicity using Long Short Term Memory Networks. Our out of sample (OOS)\nprecision and recall for the full name model estimated on the Florida Voter\nRegistration data is .83 and .84 respectively. This compares to OOS precision\nand recall of .79 and .81 for the last name only model. Commensurate numbers\nfor Wikipedia data are .73 and .73 for the full name model and .66 and .67 for\nthe last name model. To illustrate the use of this method, we apply our method\nto the campaign finance data to estimate the share of donations made by people\nof various racial groups.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 20:04:49 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sood", "Gaurav", ""], ["Laohaprapanon", "Suriyan", ""]]}, {"id": "1805.02139", "submitter": "Wen Luo", "authors": "Wen Luo and Zden\\v{e}k P. Ba\\v{z}ant", "title": "Fishnet Model with Order Statistics for Tail Probability of Failure of\n  Nacreous Biomimetic Materials with Softening Interlaminar Links", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmps.2018.07.023", "report-no": null, "categories": "stat.AP cond-mat.soft", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The staggered (or imbricated) lamellar \"brick-and-mortar\" nanostructure of\nnacre endows nacre with strength and fracture toughness values exceeding by an\norder of magnitude those of the constituents, and inspires the advent of new\nrobust biomimetic materials. While many deterministic studies clarified these\nadvantageous features in the mean sense, a closed-form statistical model is\nindispensable for determining the tail probability of failure in the range of 1\nin a million, which is what is demanded for most engineering applications. In\nthe authors' preceding study, the so-called `fishnet' statistics, exemplified\nby a diagonally pulled fishnet, was conceived to describe the probability\ndistribution. The fishnet links, representing interlaminar bonds, were\nconsidered to be elastic perfectly-brittle. However, the links may be\nquasibrittle or almost ductile, exhibiting gradual postpeak softening in their\nstress-strain relation. This paper extends the fishnet statistics to links with\npost-peak softening slope of arbitrary steepness. Probabilistic analysis is\nenabled by assuming the postpeak softening of a link to occur as a series of\nfinite drops of stress and stiffness. The maximum load of the structure is\napproximated by the strength of the k-th weakest link (k >= 1), and the\ndistribution of structure strength is expressed as a weighted sum of the\ndistributions of order statistics. The analytically obtained probabilities are\ncompared and verified by histograms of strength data obtained by millions of\nMonte Carlo simulations for each of many nacreous bodies with different link\nsoftening steepness and with various overall shapes.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 02:59:06 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Luo", "Wen", ""], ["Ba\u017eant", "Zden\u011bk P.", ""]]}, {"id": "1805.02292", "submitter": "Subhadeep Paul", "authors": "Subhadeep Paul and Yuguo Chen", "title": "A random effects stochastic block model for joint community detection in\n  multiple networks with applications to neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by multi-subject experiments in neuroimaging studies, we develop a\nmodeling framework for joint community detection in a group of related\nnetworks, which can be considered as a sample from a population of networks.\nThe proposed random effects stochastic block model facilitates the study of\ngroup differences and subject-specific variations in the community structure.\nThe model proposes a putative mean community structure which is representative\nof the group or the population under consideration but is not the community\nstructure of any individual component network. Instead, the community\nmemberships of nodes vary in each component network with a transition matrix,\nthus modeling the variation in community structure across a group of subjects.\nTo estimate the quantities of interest we propose two methods, a variational EM\nalgorithm, and a model-free \"two-step\" method based on either spectral or\nnon-negative matrix factorization (NMF). Our NMF based method Co-OSNTF is of\nindependent interest and we study its convergence properties to a stationary\npoint. We also develop a resampling-based hypothesis test for differences in\ncommunity structure in two populations both at the whole network level and node\nlevel. The methodology is applied to a publicly available fMRI dataset from\nmulti-subject experiments involving schizophrenia patients. Our methods reveal\nan overall putative community structure representative of the group as well as\nsubject-specific variations within each group. Using our network level\nhypothesis tests we are able to ascertain statistically significant difference\nin community structure between the two groups, while our node level tests help\ndetermine the nodes that are driving the difference.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 23:25:28 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 23:42:07 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Paul", "Subhadeep", ""], ["Chen", "Yuguo", ""]]}, {"id": "1805.02411", "submitter": "Marinka Zitnik", "authors": "Marinka Zitnik and Rok Sosic and Jure Leskovec", "title": "Prioritizing network communities", "comments": null, "journal-ref": "Nature Communications, 9:2544, 2018", "doi": "10.1038/s41467-018-04948-5", "report-no": null, "categories": "cs.SI cs.LG q-bio.MN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering modular structure in networks is fundamental for systems in\nbiology, physics, and engineering. Community detection identifies candidate\nmodules as hypotheses, which then need to be validated through experiments,\nsuch as mutagenesis in a biological laboratory. Only a few communities can\ntypically be validated, and it is thus important to prioritize which\ncommunities to select for downstream experimentation. Here we develop CRank, a\nmathematically principled approach for prioritizing network communities. CRank\nefficiently evaluates robustness and magnitude of structural features of each\ncommunity and then combines these features into the community prioritization.\nCRank can be used with any community detection method. It needs only\ninformation provided by the network structure and does not require any\nadditional metadata or labels. However, when available, CRank can incorporate\ndomain-specific information to further boost performance. Experiments on many\nlarge networks show that CRank effectively prioritizes communities, yielding a\nnearly 50-fold improvement in community prioritization.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 09:30:34 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 18:36:42 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zitnik", "Marinka", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1805.02501", "submitter": "Rodolfo Metulini", "authors": "Rodolfo Metulini", "title": "Players Movements and Team Shooting Performance: a Data Mining approach\n  for Basketball", "comments": "10 pages, 2 figures, proceeding of the 49th Scientific meeting of the\n  Italian Statistical Society. arXiv admin note: text overlap with\n  arXiv:1707.00883", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of Sport Analytics, Global Positioning Systems devices are\nintensively used as they permit to retrieve players' movements. Team sports'\nmanagers and coaches are interested on the relation between players' patterns\nof movements and team performance, in order to better manage their team. In\nthis paper we propose a Cluster Analysis and Multidimensional Scaling approach\nto find and describe separate patterns of players movements. Using real data of\nmultiple professional basketball teams, we find, consistently over different\ncase studies, that in the defensive clusters players are close one to another\nwhile the transition cluster are characterized by a large space among them.\nMoreover, we find the pattern of players' positioning that produce the best\nshooting performance.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 09:49:18 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Metulini", "Rodolfo", ""]]}, {"id": "1805.02764", "submitter": "Thomas McAndrew PhD", "authors": "Thomas McAndrew, Bjorn Redfors, Aaron Crowley, Yiran Zhang, Maria Alu,\n  Matthew Finn, Ariel Furer, Shmuel Chen, Geraldine Ong, Dan Burkhoff, Ori\n  Ben-Yehuda, Wael A. Jaber, Rebecca Hahn, Martin Leon", "title": "Assimilated LVEF: A Bayesian technique combining human intuition with\n  machine measurement for sharper estimates of left ventricular ejection\n  fraction and stronger association with outcomes", "comments": "6 figures, Bayesian Analysis, Data Assimilation, Survival Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cardiologist's main tool for measuring systolic heart failure is left\nventricular ejection fraction (LVEF). Trained cardiologist's report both a\nvisual and machine-guided measurement of LVEF, but only use this machine-guided\nmeasurement in analysis. We use a Bayesian technique to combine visual and\nmachine-guided estimates from the PARTNER-IIA Trial, a cohort of patients with\naortic stenosis at moderate risk treated with bioprosthetic aortic valves, and\nfind our combined estimate reduces measurement errors and improves the\nassociation between LVEF and a 1-year composite endpoint.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 21:52:17 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["McAndrew", "Thomas", ""], ["Redfors", "Bjorn", ""], ["Crowley", "Aaron", ""], ["Zhang", "Yiran", ""], ["Alu", "Maria", ""], ["Finn", "Matthew", ""], ["Furer", "Ariel", ""], ["Chen", "Shmuel", ""], ["Ong", "Geraldine", ""], ["Burkhoff", "Dan", ""], ["Ben-Yehuda", "Ori", ""], ["Jaber", "Wael A.", ""], ["Hahn", "Rebecca", ""], ["Leon", "Martin", ""]]}, {"id": "1805.02765", "submitter": "Thanakorn Khamvilai", "authors": "Kevin Garanger, Thanakorn Khamvilai, and Eric Feron", "title": "3D printing of a leaf spring: A demonstration of closed-loop control in\n  additive manufacturing", "comments": "Accepted to CCTA 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the integration of a feedback control loop during the\nprinting of a plastic object using additive manufacturing. The printed object\nis a leaf spring made of several parts of different infill density values,\nwhich are the control variables in this problem. In order to achieve a desired\nobjective stiffness, measurements are taken after each part is completed and\nthe infill density is adjusted accordingly in a closed-loop framework. The\nabsolute error in the stiffness at the end of printing is reduced from 11.63%\nto 1.34% by using a closed-loop instead of an open-loop control. This\nexperiments serves as a proof of concept to show the relevance of using\nfeedback control in additive manufacturing. By considering the printing process\nand the measurements as stochastic processes, we show how stochastic optimal\ncontrol and Kalman filtering can be used to improve the quality of objects\nmanufactured with rudimentary printers.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 21:55:10 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Garanger", "Kevin", ""], ["Khamvilai", "Thanakorn", ""], ["Feron", "Eric", ""]]}, {"id": "1805.02821", "submitter": "Thomas McAndrew PhD", "authors": "Thomas McAndrew, Bjorn Redfors, Aaron Crowley, Yiran Zhang, Shmuel\n  Chen, Mordechai Golomb, Maria Alu, Dominic Francese, Ori Ben-Yehuda, Akiko\n  Maehara, Gary Mintz, Gregg Stone, Paul Jenkins", "title": "How Cox models react to a study-specific confounder in a patient-level\n  pooled dataset: Random-effects better cope with an imbalanced covariate\n  across trials unless baseline hazards differ", "comments": "9 Pages: Cox-Proportional Hazards, Frailty, Fixed-Effects,\n  Random-Effects, Pooling Data", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining patient-level data from clinical trials can connect rare phenomena\nwith clinical endpoints, but statistical techniques applied to a single trial\nmay become problematical when trials are pooled. Estimating the hazard of a\nbinary variable unevenly distributed across trials showcases a common pooled\ndatabase issue.\n  We studied how an unevenly distributed binary variable can compromise the\nintegrity of fixed and random effects Cox proportional hazards models.\n  We compared fixed effect and random effects Cox proportional hazards models\non a set of simulated datasets inspired by a 17-trial pooled database of\npatients presenting with ST-segment elevation myocardial infarction (STEMI) and\nnon-STEMI undergoing percutaneous coronary intervention.\n  An unevenly distributed covariate can bias hazard ratio estimates, inflate\nstandard errors, raise type I error, and reduce power. While uneveness causes\nproblems for all Cox proportional hazards models, random effects suffer least.\nCompared to fixed effect models, random effects suffer lower bias and trade\ninflated type I errors for improved power. Contrasting hazard rates between\ntrials prevent accurate estimates from both fixed and random effects models.\n  When modeling a covariate unevenly distributed across pooled trials with\nsimilar baseline hazard rates, Cox proportional hazards models with a random\ntrial effect more accurately estimate hazard ratios than fixed effects.\nDiffering between-trial baseline hazard rates bias both random and fixed effect\nmodels. With an unevenly-distributed covariate and similar baseline hazard\nrates across trials, a random effects Cox proportional hazards model\noutperforms a fixed effect model, but cannot overcome contrasting baseline\nhazard rates.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 03:51:59 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["McAndrew", "Thomas", ""], ["Redfors", "Bjorn", ""], ["Crowley", "Aaron", ""], ["Zhang", "Yiran", ""], ["Chen", "Shmuel", ""], ["Golomb", "Mordechai", ""], ["Alu", "Maria", ""], ["Francese", "Dominic", ""], ["Ben-Yehuda", "Ori", ""], ["Maehara", "Akiko", ""], ["Mintz", "Gary", ""], ["Stone", "Gregg", ""], ["Jenkins", "Paul", ""]]}, {"id": "1805.02835", "submitter": "Thomas McAndrew PhD", "authors": "Thomas McAndrew, Bjorn Redfors, Yiran Zhang, Aaron Crowley, Shmuel\n  Chen, Gregg Stone, Paul Jenkins", "title": "Crossing points in survival analysis sensitively depend on system\n  conditions", "comments": "6 Pages: Survival Analysis, Crossing Survival Curves, Error\n  Propogation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crossing survival curves complicate how we interpret results from a clinical\ntrial's primary endpoint. We find the function to determine a crossing point's\nlocation depends exponentially on individual survival curves. This exponential\nrelationship between survival curves and the crossing point transforms small\nsurvival curve errors into large crossing point errors. In most cases, crossing\npoints are sensitive to individual survival errors and may make accurately\nlocating a crossing point unsuccessful. We argue more complicated analyses for\nmitigating crossing points should be reserved only after first exploring a\ncrossing point's variability, or hypothesis tests account for crossing point\nvariability.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 05:07:29 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["McAndrew", "Thomas", ""], ["Redfors", "Bjorn", ""], ["Zhang", "Yiran", ""], ["Crowley", "Aaron", ""], ["Chen", "Shmuel", ""], ["Stone", "Gregg", ""], ["Jenkins", "Paul", ""]]}, {"id": "1805.02840", "submitter": "Maria Jofre", "authors": "Maria Jofre and Richard Gerlach", "title": "Fighting Accounting Fraud Through Forensic Data Analytics", "comments": "Working Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accounting fraud is a global concern representing a significant threat to the\nfinancial system stability due to the resulting diminishing of the market\nconfidence and trust of regulatory authorities. Several tricks can be used to\ncommit accounting fraud, hence the need for non-static regulatory interventions\nthat take into account different fraudulent patterns. Accordingly, this study\naims to improve the detection of accounting fraud via the implementation of\nseveral machine learning methods to better differentiate between fraud and\nnon-fraud companies, and to further assist the task of examination within the\nriskier firms by evaluating relevant financial indicators. Out-of-sample\nresults suggest there is a great potential in detecting falsified financial\nstatements through statistical modelling and analysis of publicly available\naccounting information. The proposed methodology can be of assistance to public\nauditors and regulatory agencies as it facilitates auditing processes, and\nsupports more targeted and effective examinations of accounting reports.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 05:31:49 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Jofre", "Maria", ""], ["Gerlach", "Richard", ""]]}, {"id": "1805.02941", "submitter": "Petter Mostad", "authors": "Petter Mostad, Fredrik Tamsen", "title": "Error Rates for Unvalidated Medical Age Assessment Procedures", "comments": null, "journal-ref": null, "doi": "10.1007/s00414-018-1916-3", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During 2014-15 Sweden received asylum applications from more than 240.000\npeople, of which more than 40.000 were termed unaccompanied minors. In a large\nnumber of cases, claims by asylum seekers of being below 18 years were not\ntrusted by Swedish authorities. To handle the situation, the Swedish national\nboard of forensic medicine (R\\\"attsmedicinalverket, RMV) was assigned by the\ngovernment to create a centralized system for medical age assessments. RMV\nintroduced a procedure including two biological age indicators; x-ray of the\nthird molars and magnetic resonance imaging of the distal femoral epiphysis. In\n2017 a total of 9617 males and 337 females were subjected to this procedure. No\nvalidation study for the procedure was however published, and the observed\nnumber of cases with different maturity combinations in teeth and femur were\nunexpected given the claims originally made by RMV. Such unexpected results\nmight be caused by systematic errors and need to be analysed thoroughly. In the\npresent paper we present a general stochastic model enabling us to study which\ncombinations of age indicator model parameters and age population profiles are\nconsistent with the observed 2017 data for males. We find that, contrary to\nsome RMV claims, maturity of the femur, as observed by RMV, appears on average\nwell before maturity of teeth. Although results naturally contain much\nuncertainty, we find that classification error rates for certain groups who\nbased on the RMV procedure are classified as above 18 years may be around\n10-30%, possibly as high as 50%.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 10:50:10 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Mostad", "Petter", ""], ["Tamsen", "Fredrik", ""]]}, {"id": "1805.02988", "submitter": "Claude Renaux", "authors": "Claude Renaux, Laura Buzdugan, Markus Kalisch and Peter B\\\"uhlmann", "title": "Hierarchical inference for genome-wide association studies: a view on\n  methodology with software", "comments": null, "journal-ref": "Comput Stat 35:1 (2020) 1-40", "doi": "10.1007/s00180-019-00939-2", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a view on high-dimensional statistical inference for genome-wide\nassociation studies (GWAS). It is in part a review but covers also new\ndevelopments for meta analysis with multiple studies and novel software in\nterms of an R-package hierinf. Inference and assessment of significance is\nbased on very high-dimensional multivariate (generalized) linear models: in\ncontrast to often used marginal approaches, this provides a step towards more\ncausal-oriented inference.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 12:56:40 GMT"}, {"version": "v2", "created": "Thu, 28 Mar 2019 14:24:53 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2019 12:08:06 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 09:00:53 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Renaux", "Claude", ""], ["Buzdugan", "Laura", ""], ["Kalisch", "Markus", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1805.02993", "submitter": "Jana Falt\\'ynkov\\'a", "authors": "Jana Svobodov\\'a", "title": "Bayesian models in geographic profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of geographic profiling and offer an approach to\nchoosing a suitable model for each offender. Based on the analysis of the\nexamined dataset, we divide offenders into several types with similar behavior.\nAccording to the spatial distribution of the offender's crime sites, each new\ncriminal is assigned to the corresponding group. Then we choose an appropriate\nmodel for the offender and using Bayesian methods we determine the posterior\ndistribution for the criminal's anchor point. Our models include\ndirectionality, similar to models of Mohler and Short (2012). Our approach also\nprovides a way to incorporate two possible situations into the model - when the\ncriminal is a resident or a non-resident. We test this methodology on a real\ndata set of offenders from Baltimore County and compare the results with\nRossmo's approach. Our approach leads to substantial improvement over Rossmo's\nmethod, especially in the presence of non-residents.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 13:12:26 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Svobodov\u00e1", "Jana", ""]]}, {"id": "1805.03098", "submitter": "Jonathan Stallings", "authors": "Md Nazmul Islam, Jonathan Stallings, Ana-Maria Staicu, Dustin Crouch,\n  Lizhi Pan, and He Huang", "title": "Functional Variable Selection for EMG-based Control of a Robotic Hand\n  Prosthetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art robotic hand prosthetics generate finger and wrist movement\nthrough pattern recognition (PR) algorithms using features of forearm\nelectromyogram (EMG) signals, but re- quires extensive training and is prone to\npoor predictions for conditions outside the training data (Peerdeman et al.,\n2011; Scheme et al., 2010). We propose a novel approach to develop a dynamic\nrobotic limb by utilizing the recent history of EMG signals in a model that\naccounts for physiological features of hand movement which are ignored by PR\nalgorithms. We do this by viewing EMG signals as functional covariates and\ndevelop a functional linear model that quantifies the effect of the EMG signals\non finger/wrist velocity through a bivariate coefficient function that is\nallowed to vary with current finger/wrist position. The model is made par-\nsimonious and interpretable through a two-step variable selection procedure,\ncalled Sequential Adaptive Functional Empirical group LASSO (SAFE-gLASSO).\nNumerical studies show excel- lent selection and prediction properties of\nSAFE-gLASSO compared to popular alternatives. For our motivating dataset, the\nmethod correctly identifies the few EMG signals that are known to be important\nfor an able-bodied subject with negligible false positives and the model can be\ndirectly implemented in a robotic prosthetic.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 15:16:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Islam", "Md Nazmul", ""], ["Stallings", "Jonathan", ""], ["Staicu", "Ana-Maria", ""], ["Crouch", "Dustin", ""], ["Pan", "Lizhi", ""], ["Huang", "He", ""]]}, {"id": "1805.03318", "submitter": "Marcela Alfaro C\\'ordoba", "authors": "Marcela Alfaro C\\'ordoba, Montserrat Fuentes, Joseph Guinness and Lian\n  Xie", "title": "Multivariate Spatial-Temporal Variable Selection with Applications to\n  Seasonal Tropical Cyclone Modeling", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Tropical cyclone and sea surface temperature data have been used in several\nstudies to forecast the total number of hurricanes in the Atlantic Basin. Sea\nsurface temperature (SST) and latent heat flux (LHF) are correlated with\ntropical cyclone occurrences, but this correlation is known to vary with\nlocation and strength of the storm. The objective of this article is to\nidentify features of SST and LHF that can explain the spatial-temporal\nvariation of tropical cyclone counts, categorized by their strength. We develop\na variable selection procedure for multivariate spatial-temporally varying\ncoefficients, under a Poisson hurdle model (PHM) framework, which takes into\naccount the zero-inflated nature of the counts. The method differs from current\nspatial-temporal variable selection techniques by offering a dynamic variable\nselection procedure, that shares information between responses, locations, time\nand levels in the PHM context. The model is used to study the association\nbetween SST and LHF and the number of tropical cyclones of different strengths\nin 400 locations in the Atlantic Basin over the period of 1950-2013. Results\nshow that it is possible to estimate the number of tropical storms by season\nand region. Furthermore, the model delimits areas with a significant\ncorrelation between SST and LHF features and the occurrence and strength of TCs\nin the North Atlantic Basin.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 23:19:59 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["C\u00f3rdoba", "Marcela Alfaro", ""], ["Fuentes", "Montserrat", ""], ["Guinness", "Joseph", ""], ["Xie", "Lian", ""]]}, {"id": "1805.03597", "submitter": "Avishek Kumar", "authors": "Avishek Kumar, Syed Ali Asad Rizvi, Benjamin Brooks, R. Ali\n  Vanderveld, Kevin H. Wilson, Chad Kenney, Sam Edelstein, Adria Finch, Andrew\n  Maxwell, Joe Zuckerbraun, Rayid Ghani", "title": "Using Machine Learning to Assess the Risk of and Prevent Water Main\n  Breaks", "comments": "SIGKDD'18 London, United Kingdom", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Water infrastructure in the United States is beginning to show its age,\nparticularly through water main breaks. Main breaks cause major disruptions in\neveryday life for residents and businesses. Water main failures in Syracuse,\nN.Y. (as in most cities) are handled reactively rather than proactively. A\nbarrier to proactive maintenance is the city's inability to predict the risk of\nfailure on parts of its infrastructure. In response, we worked with the city to\nbuild a ML system to assess the risk of a water mains breaking. Using\nhistorical data on which mains have failed, descriptors of pipes, and other\ndata sources, we evaluated several models' abilities to predict breaks three\nyears into the future. Our results show that our system using gradient boosted\ndecision trees performed the best out of several algorithms and expert\nheuristics, achieving precision at 1\\% (P@1) of 0.62. Our model outperforms a\nrandom baseline (P@1 of 0.08) and expert heuristics such as water main age (P@1\nof 0.10) and history of past main breaks (P@1 of 0.48). The model is deployed\nin the City of Syracuse. We are running a pilot by calculating the risk of\nfailure for each city block over the period 2016-2018 using data up to the end\nof 2015 and, as of the end of 2017, there have been 33 breaks on our riskiest\n52 mains. This has been a successful initiative for the city of Syracuse in\nimproving their infrastructure and we believe this approach can be applied to\nother cities.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 15:46:35 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Kumar", "Avishek", ""], ["Rizvi", "Syed Ali Asad", ""], ["Brooks", "Benjamin", ""], ["Vanderveld", "R. Ali", ""], ["Wilson", "Kevin H.", ""], ["Kenney", "Chad", ""], ["Edelstein", "Sam", ""], ["Finch", "Adria", ""], ["Maxwell", "Andrew", ""], ["Zuckerbraun", "Joe", ""], ["Ghani", "Rayid", ""]]}, {"id": "1805.03735", "submitter": "Benajmin Radford J", "authors": "Benjamin J. Radford and Bartley D. Richardson and Shawn E. Davis", "title": "Sequence Aggregation Rules for Anomaly Detection in Computer Network\n  Traffic", "comments": "Prepared for the American Statistical Associations Symposium on Data\n  Science and Statistics 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We evaluate methods for applying unsupervised anomaly detection to\ncybersecurity applications on computer network traffic data, or flow. We borrow\nfrom the natural language processing literature and conceptualize flow as a\nsort of \"language\" spoken between machines. Five sequence aggregation rules are\nevaluated for their efficacy in flagging multiple attack types in a labeled\nflow dataset, CICIDS2017. For sequence modeling, we rely on long short-term\nmemory (LSTM) recurrent neural networks (RNN). Additionally, a simple\nfrequency-based model is described and its performance with respect to attack\ndetection is compared to the LSTM models. We conclude that the frequency-based\nmodel tends to perform as well as or better than the LSTM models for the tasks\nat hand, with a few notable exceptions.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 21:14:17 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 14:37:54 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Radford", "Benjamin J.", ""], ["Richardson", "Bartley D.", ""], ["Davis", "Shawn E.", ""]]}, {"id": "1805.03743", "submitter": "Luke Keele", "authors": "Luke Keele and Dylan Small", "title": "Comparing Covariate Prioritization via Matching to Machine Learning\n  Methods for Causal Inference using Five Empirical Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When investigators seek to estimate causal effects, they often assume that\nselection into treatment is based only on observed covariates. Under this\nidentification strategy, analysts must adjust for observed confounders. While\nbasic regression models have long been the dominant method of statistical\nadjustment, more robust methods based on matching or weighting have become more\ncommon. Of late, even more flexible methods based on machine learning methods\nhave been developed for statistical adjustment. These machine learning methods\nare designed to be black box methods with little input from the researcher.\nRecent research used a data competition to evaluate various methods of\nstatistical adjustment and found that black box methods out performed all other\nmethods of statistical adjustment. Matching methods with covariate\nprioritization are designed for direct input from substantive investigators in\ndirect contrast to black methods. In this article, we use a different research\ndesign to compare matching with covariate prioritization to black box methods.\nWe use black box methods to replicate results from five studies where matching\nwith covariate prioritization was used to customize the statistical adjustment\nin direct response to substantive expertise. We find little difference across\nthe methods. We conclude with advice for investigators.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 22:10:19 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 15:13:44 GMT"}, {"version": "v3", "created": "Tue, 8 Jan 2019 14:00:36 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Keele", "Luke", ""], ["Small", "Dylan", ""]]}, {"id": "1805.03777", "submitter": "Hussain Kazmi", "authors": "Adam Nagy, Hussain Kazmi, Farah Cheaib, Johan Driesen", "title": "Deep Reinforcement Learning for Optimal Control of Space Heating", "comments": "Accepted at Building Simulation and Optimization (BSO 2018),\n  Cambridge, England", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical methods to control heating systems are often marred by suboptimal\nperformance, inability to adapt to dynamic conditions and unreasonable\nassumptions e.g. existence of building models. This paper presents a novel deep\nreinforcement learning algorithm which can control space heating in buildings\nin a computationally efficient manner, and benchmarks it against other known\ntechniques. The proposed algorithm outperforms rule based control by between\n5-10% in a simulation environment for a number of price signals. We conclude\nthat, while not optimal, the proposed algorithm offers additional practical\nadvantages such as faster computation times and increased robustness to\nnon-stationarities in building dynamics.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 01:35:54 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Nagy", "Adam", ""], ["Kazmi", "Hussain", ""], ["Cheaib", "Farah", ""], ["Driesen", "Johan", ""]]}, {"id": "1805.03909", "submitter": "Razvan Marinescu", "authors": "Razvan V. Marinescu, Neil P. Oxtoby, Alexandra L. Young, Esther E.\n  Bron, Arthur W. Toga, Michael W. Weiner, Frederik Barkhof, Nick C. Fox,\n  Stefan Klein, Daniel C. Alexander, the EuroPOND Consortium (for the\n  Alzheimer's Disease Neuroimaging Initiative)", "title": "TADPOLE Challenge: Prediction of Longitudinal Evolution in Alzheimer's\n  Disease", "comments": "For more details on TADPOLE Challenge, see\n  https://tadpole.grand-challenge.org/ This paper outlines the design of the\n  TADPOLE Challenge. Paper contains 8 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Alzheimer's Disease Prediction Of Longitudinal Evolution (TADPOLE)\nChallenge compares the performance of algorithms at predicting future evolution\nof individuals at risk of Alzheimer's disease. TADPOLE Challenge participants\ntrain their models and algorithms on historical data from the Alzheimer's\nDisease Neuroimaging Initiative (ADNI) study or any other datasets to which\nthey have access. Participants are then required to make monthly forecasts over\na period of 5 years from January 2018, of three key outcomes for ADNI-3\nrollover participants: clinical diagnosis, Alzheimer's Disease Assessment Scale\nCognitive Subdomain (ADAS-Cog13), and total volume of the ventricles. These\nindividual forecasts are later compared with the corresponding future\nmeasurements in ADNI-3 (obtained after the TADPOLE submission deadline). The\nfirst submission phase of TADPOLE was open for prize-eligible submissions\nbetween 15 June and 15 November 2017. The submission system remains open via\nthe website: https://tadpole.grand-challenge.org, although since 15 November\n2017 submissions are not eligible for the first round of prizes. This paper\ndescribes the design of the TADPOLE Challenge.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 09:50:09 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 17:05:50 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Marinescu", "Razvan V.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Oxtoby", "Neil P.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Young", "Alexandra L.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Bron", "Esther E.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Toga", "Arthur W.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Weiner", "Michael W.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Barkhof", "Frederik", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Fox", "Nick C.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Klein", "Stefan", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Alexander", "Daniel C.", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"], ["Consortium", "the EuroPOND", "", "for the\n  Alzheimer's Disease Neuroimaging Initiative"]]}, {"id": "1805.04160", "submitter": "Melody Huang", "authors": "Melody Y. Huang, Randall R. Rojas, Patrick D. Convery", "title": "News Sentiment as Leading Indicators for Recessions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the following paper, we use a topic modeling algorithm and sentiment\nscoring methods to construct a novel metric that serves as a leading indicator\nin recession prediction models. We hypothesize that the inclusion of such a\nsentiment indicator, derived purely from unstructured news data, will improve\nour capabilities to forecast future recessions because it provides a direct\nmeasure of the polarity of the information consumers and producers are exposed\nto. We go on to show that the inclusion of our proposed news sentiment\nindicator, with traditional sentiment data, such as the Michigan Index of\nConsumer Sentiment and the Purchasing Manager's Index, and common factors\nderived from a large panel of economic and financial indicators helps improve\nmodel performance significantly.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:21:28 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 17:34:12 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Huang", "Melody Y.", ""], ["Rojas", "Randall R.", ""], ["Convery", "Patrick D.", ""]]}, {"id": "1805.04203", "submitter": "Yang Tang", "authors": "Yang Tang and Paul D. McNicholas and Antonio Punzo", "title": "Robust Model-Based Clustering of Voting Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the possibility of discovering extreme voting patterns in the U.S.\nCongressional voting records by drawing ideas from the mixture of contaminated\nnormal distributions. A mixture of latent trait models via contaminated normal\ndistributions is proposed. We assume that the low dimensional continuous latent\nvariable comes from a contaminated normal distribution and, therefore, picks up\nextreme patterns in the observed binary data while clustering. We consider in\nparticular such model for the analysis of voting records. The model is applied\nto a U.S. Congressional Voting data set on 16 issues. Note this approach is the\nfirst instance within the literature of a mixture model handling binary data\nwith possible extreme patterns.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 23:10:49 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""], ["Punzo", "Antonio", ""]]}, {"id": "1805.04274", "submitter": "Linda Altieri", "authors": "Linda Altieri and Daniela Cocchi and Giulia Roli", "title": "Measuring heterogeneity in urban expansion via spatial entropy", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of efficiency in urban diffusion is a debated issue, important for\nbiologists, urban specialists, planners and statisticians, both in developed\nand new developing countries. Many approaches have been considered to measure\nurban sprawl, i.e. chaotic urban expansion; such idea of chaos is here linked\nto the concept of entropy. Entropy, firstly introduced in information theory,\nrapidly became a standard tool in ecology, biology and geography to measure the\ndegree of heterogeneity among observations; in these contexts, entropy measures\nshould include spatial information. The aim of this paper is to employ a\nrigorous spatial entropy based approach to measure urban sprawl associated to\nthe diffusion of metropolitan cities. In order to assess the performance of the\nconsidered measures, a comparative study is run over alternative urban\nscenarios; afterwards, measures are used to quantify the degree of disorder in\nthe urban expansion of three cities in Europe. Results are easily interpretable\nand can be used both as an absolute measure of urban sprawl and for comparison\nover space and time.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 08:40:06 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Altieri", "Linda", ""], ["Cocchi", "Daniela", ""], ["Roli", "Giulia", ""]]}, {"id": "1805.04582", "submitter": "Tammo Rukat", "authors": "Tammo Rukat, Chris C. Holmes, Christopher Yau", "title": "TensOrMachine: Probabilistic Boolean Tensor Decomposition", "comments": "To be published at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boolean tensor decomposition approximates data of multi-way binary\nrelationships as product of interpretable low-rank binary factors, following\nthe rules of Boolean algebra. Here, we present its first probabilistic\ntreatment. We facilitate scalable sampling-based posterior inference by\nexploitation of the combinatorial structure of the factor conditionals. Maximum\na posteriori decompositions feature higher accuracies than existing techniques\nthroughout a wide range of simulated conditions. Moreover, the probabilistic\napproach facilitates the treatment of missing data and enables model selection\nwith much greater accuracy. We investigate three real-world data-sets. First,\ntemporal interaction networks in a hospital ward and behavioural data of\nuniversity students demonstrate the inference of instructive latent patterns.\nNext, we decompose a tensor with more than 10 billion data points, indicating\nrelations of gene expression in cancer patients. Not only does this demonstrate\nscalability, it also provides an entirely novel perspective on relational\nproperties of continuous data and, in the present example, on the molecular\nheterogeneity of cancer. Our implementation is available on GitHub:\nhttps://github.com/TammoR/LogicalFactorisationMachines.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 20:23:35 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Rukat", "Tammo", ""], ["Holmes", "Chris C.", ""], ["Yau", "Christopher", ""]]}, {"id": "1805.04634", "submitter": "Min Xu", "authors": "Kai Wen Wang, Xiangrui Zeng, Xiaodan Liang, Zhiguang Huo, Eric P.\n  Xing, Min Xu", "title": "Image-derived generative modeling of pseudo-macromolecular structures -\n  towards the statistical assessment of Electron CryoTomography template\n  matching", "comments": null, "journal-ref": "British Machine Vision Conference (BMVC) 2018", "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular Electron CryoTomography (CECT) is a 3D imaging technique that\ncaptures information about the structure and spatial organization of\nmacromolecular complexes within single cells, in near-native state and at\nsub-molecular resolution. Although template matching is often used to locate\nmacromolecules in a CECT image, it is insufficient as it only measures the\nrelative structural similarity. Therefore, it is preferable to assess the\nstatistical credibility of the decision through hypothesis testing, requiring\nmany templates derived from a diverse population of macromolecular structures.\nDue to the very limited number of known structures, we need a generative model\nto efficiently and reliably sample pseudo-structures from the complex\ndistribution of macromolecular structures. To address this challenge, we\npropose a novel image-derived approach for performing hypothesis testing for\ntemplate matching by constructing generative models using the generative\nadversarial network. Finally, we conducted hypothesis testing experiments for\ntemplate matching on both simulated and experimental subtomograms, allowing us\nto conclude the identity of subtomograms with high statistical credibility and\nsignificantly reducing false positives.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 02:00:30 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Wang", "Kai Wen", ""], ["Zeng", "Xiangrui", ""], ["Liang", "Xiaodan", ""], ["Huo", "Zhiguang", ""], ["Xing", "Eric P.", ""], ["Xu", "Min", ""]]}, {"id": "1805.05170", "submitter": "Jacob Rhyne", "authors": "Jacob Rhyne, Eric Chi, Jung-Ying Tzeng, and X. Jessie Jeng", "title": "FastLORS: Joint Modeling for eQTL Mapping in R", "comments": "All functions are available in the FastLORS R package, available at\n  https://github.com/jdrhyne2/FastLORS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang et al. (2013) introduced LORS, a method that jointly models the\nexpression of genes, SNPs, and hidden factors for eQTL mapping. LORS solves a\nconvex optimization problem and has guaranteed convergence. However, it can be\ncomputationally expensive for large datasets. In this paper we introduce\nFast-LORS which uses the proximal gradient method to solve the LORS problem\nwith significantly reduced computational burden. We apply Fast-LORS and LORS to\ndata from the third phase of the International HapMap Project and obtain\ncomparable results. Nevertheless, Fast-LORS shows substantial computational\nimprovement compared to LORS.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 13:41:48 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Rhyne", "Jacob", ""], ["Chi", "Eric", ""], ["Tzeng", "Jung-Ying", ""], ["Jeng", "X. Jessie", ""]]}, {"id": "1805.05232", "submitter": "Mike West", "authors": "Lindsay Berry and Mike West", "title": "Bayesian forecasting of many count-valued time series", "comments": "26 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops forecasting methodology and application of new classes of\ndynamic models for time series of non-negative counts. Novel univariate models\nsynthesise dynamic generalized linear models for binary and conditionally\nPoisson time series, with dynamic random effects for over-dispersion. These\nmodels allow use of dynamic covariates in both binary and non-zero count\ncomponents. Sequential Bayesian analysis allows fast, parallel analysis of sets\nof decoupled time series. New multivariate models then enable information\nsharing in contexts when data at a more highly aggregated level provide more\nincisive inferences on shared patterns such as trends and seasonality. A novel\nmulti-scale approach-- one new example of the concept of decouple/recouple in\ntime series-- enables information sharing across series. This incorporates\ncross-series linkages while insulating parallel estimation of univariate\nmodels, hence enables scalability in the number of series. The major motivating\ncontext is supermarket sales forecasting. Detailed examples drawn from a case\nstudy in multi-step forecasting of sales of a number of related items showcase\nforecasting of multiple series, with discussion of forecast accuracy metrics\nand broader questions of probabilistic forecast accuracy assessment.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 15:33:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Berry", "Lindsay", ""], ["West", "Mike", ""]]}, {"id": "1805.05502", "submitter": "Jia Chen", "authors": "Jia Chen, Gang Wang, and Georgios B. Giannakis", "title": "Nonlinear Dimensionality Reduction for Discriminative Analytics of\n  Multiple Datasets", "comments": "final version", "journal-ref": null, "doi": "10.1109/TSP.2018.2885478", "report-no": null, "categories": "cs.LG eess.SP stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is widely used for feature extraction and\ndimensionality reduction, with documented merits in diverse tasks involving\nhigh-dimensional data. Standard PCA copes with one dataset at a time, but it is\nchallenged when it comes to analyzing multiple datasets jointly. In certain\ndata science settings however, one is often interested in extracting the most\ndiscriminative information from one dataset of particular interest (a.k.a.\ntarget data) relative to the other(s) (a.k.a. background data). To this end,\nthis paper puts forth a novel approach, termed discriminative (d) PCA, for such\ndiscriminative analytics of multiple datasets. Under certain conditions, dPCA\nis proved to be least-squares optimal in recovering the component vector unique\nto the target data relative to background data. To account for nonlinear data\ncorrelations, (linear) dPCA models for one or multiple background datasets are\ngeneralized through kernel-based learning. Interestingly, all dPCA variants\nadmit an analytical solution obtainable with a single (generalized) eigenvalue\ndecomposition. Finally, corroborating dimensionality reduction tests using both\nsynthetic and real datasets are provided to validate the effectiveness of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 00:24:43 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 03:58:12 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2018 17:42:22 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 01:55:03 GMT"}, {"version": "v5", "created": "Tue, 4 Dec 2018 20:26:48 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Chen", "Jia", ""], ["Wang", "Gang", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "1805.05617", "submitter": "Jichang Zhao", "authors": "Huiwen Wang, Shan Lu and Jichang Zhao", "title": "Aggregating multiple types of complex data in stock market prediction: A\n  model-independent framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.CE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing richness in volume, and especially types of data in the\nfinancial domain provides unprecedented opportunities to understand the stock\nmarket more comprehensively and makes the price prediction more accurate than\nbefore. However, they also bring challenges to classic statistic approaches\nsince those models might be constrained to a certain type of data. Aiming at\naggregating differently sourced information and offering type-free capability\nto existing models, a framework for predicting stock market of scenarios with\nmixed data, including scalar data, compositional data (pie-like) and functional\ndata (curve-like), is established. The presented framework is\nmodel-independent, as it serves like an interface to multiple types of data and\ncan be combined with various prediction models. And it is proved to be\neffective through numerical simulations. Regarding to price prediction, we\nincorporate the trading volume (scalar data), intraday return series\n(functional data), and investors' emotions from social media (compositional\ndata) through the framework to competently forecast whether the market goes up\nor down at opening in the next day. The strong explanatory power of the\nframework is further demonstrated. Specifically, it is found that the intraday\nreturns impact the following opening prices differently between bearish market\nand bullish market. And it is not at the beginning of the bearish market but\nthe subsequent period in which the investors' \"fear\" comes to be indicative.\nThe framework would help extend existing prediction models easily to scenarios\nwith multiple types of data and shed light on a more systemic understanding of\nthe stock market.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:58:31 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Wang", "Huiwen", ""], ["Lu", "Shan", ""], ["Zhao", "Jichang", ""]]}, {"id": "1805.05657", "submitter": "Ioanna Manolopoulou", "authors": "James Pitkin, Ioanna Manolopoulou and Gordon Ross", "title": "Bayesian hierarchical modelling of sparse count processes in retail\n  analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of retail analytics has been transformed by the availability of\nrich data which can be used to perform tasks such as demand forecasting and\ninventory management. However, one task which has proved more challenging is\nthe forecasting of demand for products which exhibit very few sales. The\nsparsity of the resulting data limits the degree to which traditional analytics\ncan be deployed. To combat this, we represent sales data as a structured sparse\nmultivariate point process which allows for features such as auto-correlation,\ncross-correlation, and temporal clustering, known to be present in sparse sales\ndata. We introduce a Bayesian point process model to capture these phenomena,\nwhich includes a hurdle component to cope with sparsity and an exciting\ncomponent to cope with temporal clustering within and across products. We then\ncast this model within a Bayesian hierarchical framework, to allow the\nborrowing of information across different products, which is key in addressing\nthe data sparsity per product. We conduct a detailed analysis using real sales\ndata to show that this model outperforms existing methods in terms of\npredictive power and we discuss the interpretation of the inference.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:25:06 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Pitkin", "James", ""], ["Manolopoulou", "Ioanna", ""], ["Ross", "Gordon", ""]]}, {"id": "1805.05671", "submitter": "Ioanna Manolopoulou", "authors": "James Pitkin, Gordon Ross and Ioanna Manolopoulou", "title": "Dirichlet Process Mixtures of Order Statistics with Applications to\n  Retail Analytics", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12296", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of \"big data\" has led to the frequent need to process and store\ndatasets containing large numbers of high dimensional observations. Due to\nstorage restrictions, these observations might be recorded in a\nlossy-but-sparse manner, with information collapsed onto a few entries which\nare considered important. This results in informative missingness in the\nobserved data. Our motivating application comes from retail analytics, where\nthe behaviour of product sales is summarised by the price elasticity of each\nproduct with respect to a small number of its top competitors. The resulting\ndata are vectors of order statistics, due to only the top few entries being\nobserved. Interest lies in characterising the behaviour of a product's\ncompetitors, and clustering products based on how their competition is spread\nacross the market. We develop nonparametric Bayesian methodology for modelling\nvectors of order statistics that utilises a Dirichlet Process Mixture Model\nwith an Exponentiated Weibull kernel. Our approach allows us added flexibility\nfor the distribution of each vector, while providing parameters that\ncharacterise the decay of the leading entries. We implement our methods on a\nretail analytics dataset of the cross-elasticity coefficients, and our analysis\nreveals distinct types of behaviour across the different products of interest.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 09:44:23 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 14:43:09 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Pitkin", "James", ""], ["Ross", "Gordon", ""], ["Manolopoulou", "Ioanna", ""]]}, {"id": "1805.05977", "submitter": "Helmut Strey", "authors": "Helmut H. Strey", "title": "On the Estimation of Parameters from Time Traces originating from an\n  Ornstein-Uhlenbeck Process", "comments": "11 pages, 9 figures", "journal-ref": "Phys. Rev. E 100, 062142 (2019)", "doi": "10.1103/PhysRevE.100.062142", "report-no": null, "categories": "cond-mat.soft stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop a Bayesian approach to estimate parameters from\ntime traces that originate from an overdamped Brownian particle in a harmonic\npotential, or Ornstein-Uhlenbeck process (OU). We show that least-square\nfitting the autocorrelation function, which is often the standard way of\nanalyzing such data, is significantly underestimating the confidence intervals\nof the fitted parameters. Here, we develop a rigorous maximum likelihood theory\nthat properly captures the underlying statistics. From the analytic solution,\nwe found that there exists an optimal measurement spacing ($\\Delta t = 0.7968\n\\tau$) that maximizes the statistical accuracy of the estimate for the\ndecay-time $\\tau$ of the process for a fixed number of samples $N$, which plays\na similar role than the Nyquist-Shannon theorem for the OU-process. In summary,\nour results have strong implications for parameter estimation for processes\nthat result in a single exponential decay in the autocorrelation function. Our\nanalysis can directly be applied to single-component dynamic light scattering\nexperiments or optical trap calibration experiments.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 18:18:34 GMT"}, {"version": "v2", "created": "Tue, 6 Aug 2019 15:56:59 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Strey", "Helmut H.", ""]]}, {"id": "1805.06084", "submitter": "Gregory Bopp", "authors": "Gregory P. Bopp, Benjamin A. Shaby and Rapha\\\"el Huser", "title": "A Hierarchical Max-Infinitely Divisible Spatial Model for Extreme\n  Precipitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Understanding the spatial extent of extreme precipitation is necessary for\ndetermining flood risk and adequately designing infrastructure (e.g.,\nstormwater pipes) to withstand such hazards. While environmental phenomena\ntypically exhibit weakening spatial dependence at increasingly extreme levels,\nlimiting max-stable process models for block maxima have a rigid dependence\nstructure that does not capture this type of behavior. We propose a flexible\nBayesian model from a broader family of (conditionally) max-infinitely\ndivisible processes that allows for weakening spatial dependence at\nincreasingly extreme levels, and due to a hierarchical representation of the\nlikelihood in terms of random effects, our inference approach scales to large\ndatasets. Therefore, our model not only has a flexible dependence structure,\nbut it also allows for fast, fully Bayesian inference, prediction and\nconditional simulation in high dimensions. The proposed model is constructed\nusing flexible random basis functions that are estimated from the data,\nallowing for straightforward inspection of the predominant spatial patterns of\nextremes. In addition, the described process possesses (conditional)\nmax-stability as a special case, making inference on the tail dependence class\npossible. We apply our model to extreme precipitation in North-Eastern America,\nand show that the proposed model adequately captures the extremal behavior of\nthe data. Interestingly, we find that the principal modes of spatial variation\nestimated from our model resemble observed patterns in extreme precipitation\nevents occurring along the coast (e.g., with localized tropical cyclones and\nconvective storms) and mountain range borders. Our model, which can easily be\nadapted to other types of environmental datasets, is therefore useful to\nidentify extreme weather patterns and regions at risk.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:17:49 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 22:17:29 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 21:57:25 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Bopp", "Gregory P.", ""], ["Shaby", "Benjamin A.", ""], ["Huser", "Rapha\u00ebl", ""]]}, {"id": "1805.06094", "submitter": "Yang Liu", "authors": "Yang Liu, Prateek Bansal, Ricardo Daziano, Samitha Samaranayake", "title": "A Framework to Integrate Mode Choice in the Design of Mobility-on-Demand\n  Systems", "comments": "31 pages, 11 figures. This manuscript version is made available under\n  the CC-BY-NC-ND 4.0 license http://creativecommons.org/licenses/by-nc-nd/4.0/", "journal-ref": null, "doi": "10.1016/j.trc.2018.09.022", "report-no": null, "categories": "cs.SY math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobility-on-Demand (MoD) systems are generally designed and analyzed for a\nfixed and exogenous demand, but such frameworks fail to answer questions about\nthe impact of these services on the urban transportation system, such as the\neffect of induced demand and the implications for transit ridership. In this\nstudy, we propose a unified framework to design, optimize and analyze MoD\noperations within a multimodal transportation system where the demand for a\ntravel mode is a function of its level of service. An application of Bayesian\noptimization (BO) to derive the optimal supply-side MoD parameters (e.g., fleet\nsize and fare) is also illustrated. The proposed framework is calibrated using\nthe taxi demand data in Manhattan, New York. Travel demand is served by public\ntransit and MoD services of varying passenger capacities (1, 4 and 10), and\npassengers are predicted to choose travel modes according to a mode choice\nmodel. This choice model is estimated using stated preference data collected in\nNew York City. The convergence of the multimodal supply-demand system and the\nsuperiority of the BO-based optimization method over earlier approaches are\nestablished through numerical experiments. We finally consider a policy\nintervention where the government imposes a tax on the ride-hailing service and\nillustrate how the proposed framework can quantify the pros and cons of such\npolicies for different stakeholders.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 01:57:52 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 02:45:40 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 03:25:33 GMT"}, {"version": "v4", "created": "Sun, 7 Oct 2018 19:10:56 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Liu", "Yang", ""], ["Bansal", "Prateek", ""], ["Daziano", "Ricardo", ""], ["Samaranayake", "Samitha", ""]]}, {"id": "1805.06208", "submitter": "Caifa Zhou", "authors": "Caifa Zhou and Andreas Wieser", "title": "CDM: Compound dissimilarity measure and an application to\n  fingerprinting-based positioning", "comments": "7 pages, 5 figures, 3 tables, a paper accepted to be published IPIN\n  2018, Nantes France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-vector-based dissimilarity measure is proposed by combining\nvector-based distance metrics and set operations. This proposed compound\ndissimilarity measure (CDM) is applicable to quantify similarity of collections\nof attribute/feature pairs where not all attributes are present in all\ncollections. This is a typical challenge in the context of e.g.,\nfingerprinting-based positioning (FbP). Compared to vector-based distance\nmetrics (e.g., Minkowski), the merits of the proposed CDM are i) the data do\nnot need to be converted to vectors of equal dimension, ii) shared and unshared\nattributes can be weighted differently within the assessment, and iii)\nadditional degrees of freedom within the measure allow to adapt its properties\nto application needs in a data-driven way. We indicate the validity of the\nproposed CDM by demonstrating the improvements of the positioning performance\nof fingerprinting-based WLAN indoor positioning using four different datasets,\nthree of them publicly available. When processing these datasets using CDM\ninstead of conventional distance metrics the accuracy of identifying buildings\nand floors improves by about 5% on average. The 2d positioning errors in terms\nof root mean squared error (RMSE) are reduced by a factor of two, and the\npercentage of position solutions with less than 2m error improves by over 10%.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 09:44:05 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 08:15:05 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Zhou", "Caifa", ""], ["Wieser", "Andreas", ""]]}, {"id": "1805.06245", "submitter": "Malcolm Hillebrand", "authors": "Malcolm Hillebrand, Guy Paterson-Jones, George Kalosakas and\n  Charalampos Skokos", "title": "Distribution of Base Pair Alternations in a Periodic DNA Chain:\n  Application of Polya Counting to a Physical System", "comments": "17 pages, 12 figures, published in the journal Regular and Chaotic\n  Dynamics", "journal-ref": "Regular and Chaotic Dynamics (2018) 23: 135", "doi": "10.1134/S1560354718020016", "report-no": null, "categories": "math.CO math.PR q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling DNA chains, the number of alternations between Adenine-Thymine\n(AT) and Guanine-Cytosine (GC) base pairs can be considered as a measure of the\nheterogeneity of the chain, which in turn could affect its dynamics. A\nprobability distribution function of the number of these alternations is\nderived for circular or periodic DNA. Since there are several symmetries to\naccount for in the periodic chain, necklace counting methods are used. In\nparticular, Polya's Enumeration Theorem is extended for the case of a group\naction that preserves partitioned necklaces. This, along with the treatment of\ngenerating functions as formal power series, allows for the direct calculation\nof the number of possible necklaces with a given number of AT base pairs, GC\nbase pairs and alternations. The theoretically obtained probability\ndistribution functions of the number of alternations are accurately reproduced\nby Monte Carlo simulations and fitted by Gaussians. The effect of the number of\nbase pairs on the characteristics of these distributions is also discussed, as\nwell as the effect of the ratios of the numbers of AT and GC base pairs.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 11:12:59 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Hillebrand", "Malcolm", ""], ["Paterson-Jones", "Guy", ""], ["Kalosakas", "George", ""], ["Skokos", "Charalampos", ""]]}, {"id": "1805.06381", "submitter": "Jinghui Yuan", "authors": "Xuesong Wang, Jinghui Yuan, Grant G. Schultz, Wenjing Meng", "title": "Investigating Safety Impacts of Roadway Network Features of Suburban\n  Arterials in Shanghai, China", "comments": "Presented at Transportation Research Board 95th Annual Meeting (No.\n  16-2508)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid changes in land use development along suburban arterials in\nShanghai, there is also a corresponding increase in traffic demand along these\narterials. With a preference toward increased accessibility and efficiency,\nthese arterials have been installed with an increased number of signalized\nintersections and accesses to serve local traffic needs. The absence of a\ndefined functional hierarchy along the road network, together with the\nnon-uniform installation of signals and accesses tends to deteriorate arterial\nsafety. Previous studies on arterial safety have generally been based on a\nsingle type of road entity (either intersection or roadway segment). These\nstudies only analyzed partial safety impacts of signal spacing and access\ndensity, as these factors would significantly influence the safety performance\nof both intersections and roadway segments. Macro level safety modeling was\nusually applied to investigate the relationship between the zonal crash\nfrequencies and demographics, road network features and traffic\ncharacteristics. In this study, a new modeling strategy was proposed to analyze\nthe safety impacts of roadway network features (i.e., road network patterns,\nsignal spacing and access density) of arterials by applying a macro level\nsafety modeling technique. Bayesian Conditional Autoregressive models were\ndeveloped for arterials covering 173 Traffic Analysis Zones in the suburban\narea in Shanghai. The results identified that the road network pattern with\ncollector roads parallel to the arterials was shown to be associated with fewer\ncrashes than those without parallel collectors. Higher signal density and\naccess density also tended to increase crash frequencies on arterials.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:53:05 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 03:17:48 GMT"}, {"version": "v3", "created": "Sat, 26 May 2018 05:03:59 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Wang", "Xuesong", ""], ["Yuan", "Jinghui", ""], ["Schultz", "Grant G.", ""], ["Meng", "Wenjing", ""]]}, {"id": "1805.06396", "submitter": "Jinghui Yuan", "authors": "Xuesong Wang, Jinghui Yuan, Xiaohan Yang", "title": "Modelling of crash types at signalized intersections based on random\n  effect model", "comments": "in Chinese", "journal-ref": "Tongji Daxue Xuebao/Journal of Tongji University. 2016, 44(1):\n  81-86", "doi": "10.11908/j.issn.0253-374x.2016.01.012", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Approach-level models were developed to accommodate the diversity of\napproaches within the same intersection. A random effect term, which indicates\nthe intersection-specific effect, was incorporated into each crash type model\nto deal with the spatial correlation between different approaches within the\nsame intersection. The model parameters were estimated under the Bayesian\nframework. Results show that different crash types are correlated with\ndifferent groups of factors, and each factor shows diverse effects on different\ncrash types, which indicates the importance of crash type models. Besides, the\nsignificance of random effect term confirms the existence of spatial\ncorrelations among different approaches within the same intersection.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:08:34 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Wang", "Xuesong", ""], ["Yuan", "Jinghui", ""], ["Yang", "Xiaohan", ""]]}, {"id": "1805.06399", "submitter": "Marine Dufournet", "authors": "Marine Dufournet", "title": "Magnitude of selection bias in road safety epidemiology, a primer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of road safety epidemiology, it is common to use responsibility\nanalyses to assess the effect of a given factor on the risk of being\nresponsible for an accident, among drivers involved in an accident only. Using\nthe SCM framework, we formally showed in previous works that the causal\nodds-ratio of a given factor correlated with high speed cannot be unbiasedly\nestimated through responsibility analyses if inclusion into the dataset depends\non the accident severity. The objective of this present work is to present\nnumerical results to give a first quantification of the magnitude of the\nselection bias induced by responsibility analyses. We denote the binary\nvariables by X the exposure of interest, V the high speed, F the driving fault,\nR the responsibility of a severe accident, A the severe accident, and W a set\nof categorical confounders. We illustrate the potential bias by comparing the\ncausal effect of interest of X on R, COR(X,R|W=w), and the estimable odds-ratio\navailable in responsibility analyses, OR(X,R|W=w, A=1). By considering a binary\nexposure, and by varying a set of parameters, we describe a situation where X\ncould represent alcohol or cannabis intoxication. We confirm that the estimable\nodds-ratio available in responsibility analyses is a biased measure of the\ncausal effect when X is correlated with high speed V and V is related to the\naccident severity A. In this case, the magnitude of the bias is all the more\nimportant that these two relationships are strong. When X is likely to increase\nthe risk to drive fast V, the estimable odds-ratio underestimates the causal\neffect. When X is likely to decrease the risk to drive fast V, the estimable\nodds-ratio upperestimates the causal effect. The values of the different causal\nquantities considered here are from one to five times higher (or lower) than\nthe estimable quantity available in responsability analyses.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 16:10:52 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Dufournet", "Marine", ""]]}, {"id": "1805.06497", "submitter": "Madeline Ausdemore", "authors": "Madeline Ausdemore and Cedric Neumann", "title": "Deconvolution of dust mixtures by latent Dirichlet allocation in\n  forensic science", "comments": null, "journal-ref": "Journal of Forensic Identification 2020", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dust particles recovered from the soles of shoes may be indicative of the\nsites recently visited by an individual, and, in particular, of the presence of\nan individual at a particular site of interest, e.g., the scene of a crime. By\ndescribing the dust profile of a given site by a multinomial distribution over\na fixed number of dust particle types, we can define the probability\ndistribution of the mixture of dust recovered from the sole of a shoe via\nLatent Dirichlet Allocation. We use Variational Bayesian Inference to study the\nparameters of the model, and use their resulting posterior distributions to\nmake inference on (a) the contributions of sites of interest to a dust mixture,\nand (b) the particle profiles associated with these sites.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 19:30:04 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 01:16:43 GMT"}, {"version": "v3", "created": "Thu, 31 Jan 2019 22:43:57 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Ausdemore", "Madeline", ""], ["Neumann", "Cedric", ""]]}, {"id": "1805.06619", "submitter": "Neema Kachappilly Davis", "authors": "Neema Davis, Gaurav Raina, Krishna Jagannathan", "title": "Taxi demand forecasting: A HEDGE based tessellation strategy for\n  improved accuracy", "comments": "Under revision in Special Issue on Knowledge Discovery from Mobility\n  Data for Intelligent Transportation Systems (Transactions on ITS)", "journal-ref": "IEEE Transactions on Intelligent Transportation Systems ( Volume:\n  19 , Issue: 11 , Nov. 2018 )", "doi": "10.1109/TITS.2018.2860925", "report-no": null, "categories": "cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key problem in location-based modeling and forecasting lies in identifying\nsuitable spatial and temporal resolutions. In particular, judicious spatial\npartitioning can play a significant role in enhancing the performance of\nlocation-based forecasting models. In this work, we investigate two widely used\ntessellation strategies for partitioning city space, in the context of\nreal-time taxi demand forecasting. Our study compares (i) Geohash tessellation,\nand (ii) Voronoi tessellation, using two distinct taxi demand datasets, over\nmultiple time scales. For the purpose of comparison, we employ classical\ntime-series tools to model the spatio-temporal demand. Our study finds that the\nperformance of each tessellation strategy is highly dependent on the city\ngeography, spatial distribution of the data, and the time of the day, and that\nneither strategy is found to perform optimally across the forecast horizon. We\npropose a hybrid tessellation algorithm that picks the best tessellation\nstrategy at each instant, based on their performance in the recent past. Our\nhybrid algorithm is a non-stationary variant of the well-known HEDGE algorithm\nfor choosing the best advice from multiple experts. We show that the hybrid\ntessellation strategy performs consistently better than either of the two\nstrategies across the data sets considered, at multiple time scales, and with\ndifferent performance metrics. We achieve an average accuracy of above 80% per\nkm^2 for both data sets considered at 60 minute aggregation levels.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 06:59:11 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 05:40:03 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Davis", "Neema", ""], ["Raina", "Gaurav", ""], ["Jagannathan", "Krishna", ""]]}, {"id": "1805.06639", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:09 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06640", "submitter": "Ze Jin", "authors": "Ze Jin, Xiaohan Yan, David S. Matteson", "title": "Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:48 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Yan", "Xiaohan", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06649", "submitter": "Florian Ziel", "authors": "Florian Ziel, Rafal Weron", "title": "Day-ahead electricity price forecasting with high-dimensional\n  structures: Univariate vs. multivariate modeling frameworks", "comments": null, "journal-ref": "Energy Economics, 70 (2018), 396-420", "doi": "10.1016/j.eneco.2017.12.016", "report-no": null, "categories": "stat.AP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We conduct an extensive empirical study on short-term electricity price\nforecasting (EPF) to address the long-standing question if the optimal model\nstructure for EPF is univariate or multivariate. We provide evidence that\ndespite a minor edge in predictive performance overall, the multivariate\nmodeling framework does not uniformly outperform the univariate one across all\n12 considered datasets, seasons of the year or hours of the day, and at times\nis outperformed by the latter. This is an indication that combining advanced\nstructures or the corresponding forecasts from both modeling approaches can\nbring a further improvement in forecasting accuracy. We show that this indeed\ncan be the case, even for a simple averaging scheme involving only two models.\nFinally, we also analyze variable selection for the best performing\nhigh-dimensional lasso-type models, thus provide guidelines to structuring\nbetter performing forecasting model designs.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 08:29:27 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Ziel", "Florian", ""], ["Weron", "Rafal", ""]]}, {"id": "1805.06839", "submitter": "Sylwia Bujkiewicz", "authors": "David Jenkins, Humaira Hussein, Reynaldo Martina, Pascale\n  Dequen-O'Byrne, Keith R Abrams, Sylwia Bujkiewicz", "title": "Methods for the inclusion of real world evidence in network\n  meta-analysis", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Network Meta-Analysis (NMA) is a key component of submissions to\nreimbursement agencies world-wide, especially when there is limited direct\nhead-to-head evidence for multiple technologies from randomised controlled\ntrials (RCTs). Many NMAs include only data from RCTs. However, real-world\nevidence (RWE) is also becoming widely recognised as a valuable source of\nclinical data. We investigate methods for the inclusion of RWE in NMA and its\nimpact on the uncertainty around the effectiveness estimates.\n  Methods: A range of methods for inclusion of RWE in evidence synthesis,\nincluding Bayesian hierarchical and power prior models, were investigated by\napplying them to an example in relapsing remitting multiple sclerosis. The\neffect of the inclusion of RWE was investigated by varying the degree of down\nweighting of this part of evidence by the use of a power prior.\n  Results: Whilst the inclusion of the RWE led to an increase in the level of\nuncertainty surrounding effect estimates in this example, this depended on the\nmethod of inclusion adopted for the RWE. Power prior NMA model resulted in\nstable effect estimates for fingolimod yet increasing the width of the credible\nintervals with increasing weight given to RWE data. The hierarchical NMA models\nwere effective in allowing for heterogeneity between study designs; however,\nthis also increased the level of uncertainty.\n  Conclusion: The power prior approach for the inclusion of RWE in NMAs\nindicates that the degree to which RWE is taken into account can have a\nsignificant impact on the overall level of uncertainty. The hierarchical\nmodelling approach further allowed for accommodating differences between study\ntypes. Consequently, further work investigating both empirical evidence for\nbiases associated with individual RWE studies and methods of elicitation from\nexperts on the extent of such biases is warranted.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:12:50 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 09:39:23 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Jenkins", "David", ""], ["Hussein", "Humaira", ""], ["Martina", "Reynaldo", ""], ["Dequen-O'Byrne", "Pascale", ""], ["Abrams", "Keith R", ""], ["Bujkiewicz", "Sylwia", ""]]}, {"id": "1805.06923", "submitter": "Yi Zhao", "authors": "Yi Zhao, Xi Luo, Martin Lindquist, Brian Caffo", "title": "Functional Mediation Analysis with an Application to Functional Magnetic\n  Resonance Imaging Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is widely utilized to separate the causal effect of\ntreatment into its direct effect on the outcome and its indirect effect through\nan intermediate variable (the mediator). In this study we introduce a\nfunctional mediation analysis framework in which the three key variables, the\ntreatment, mediator, and outcome, are all continuous functions. With functional\nmeasures, causal assumptions and interpretations are not immediately\nwell-defined. Motivated by a functional magnetic resonance imaging (fMRI)\nstudy, we propose two functional mediation models based on the influence of the\nmediator: (1) a concurrent mediation model and (2) a historical mediation\nmodel. We further discuss causal assumptions, and elucidate causal\ninterpretations. Our proposed models enable the estimation of individual causal\neffect curves, where both the direct and indirect effects vary across time.\nApplied to a task-based fMRI study, we illustrate how our functional mediation\nframework provides a new perspective for studying dynamic brain connectivity.\nThe R package cfma is available on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 18:54:51 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Zhao", "Yi", ""], ["Luo", "Xi", ""], ["Lindquist", "Martin", ""], ["Caffo", "Brian", ""]]}, {"id": "1805.06930", "submitter": "Quinten Meertens", "authors": "Q. A. Meertens, C. G. H. Diks, H. J. van den Herik and F. W. Takes", "title": "A Data-Driven Supply-Side Approach for Measuring Cross-Border Internet\n  Purchases", "comments": "27 pages, 5 figures, submitted to Journal of the Royal Statistical\n  Society, Series A (Statistics in Society)", "journal-ref": null, "doi": "10.1111/rssa.12487", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The digital economy is a highly relevant item on the European Union's policy\nagenda. Cross-border internet purchases are part of the digital economy, but\ntheir total value can currently not be accurately measured or estimated.\nTraditional approaches based on consumer surveys or business surveys are shown\nto be inadequate for this purpose, due to language bias and sampling issues,\nrespectively. We address both problems by proposing a novel approach based on\nsupply-side data, namely tax returns. The proposed data-driven record-linkage\ntechniques and machine learning algorithms utilize two additional open data\nsources: European business registers and internet data. Our main finding is\nthat the value of total cross-border internet purchases within the European\nUnion by Dutch consumers was over EUR 1.3 billion in 2016. This is more than 6\ntimes as high as current estimates. Our finding motivates the implementation of\nthe proposed methodology in other EU member states. Ultimately, it could lead\nto more accurate estimates of cross-border internet purchases within the entire\nEuropean Union.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 19:04:28 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meertens", "Q. A.", ""], ["Diks", "C. G. H.", ""], ["Herik", "H. J. van den", ""], ["Takes", "F. W.", ""]]}, {"id": "1805.07149", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio, Rachael Hunter, Alexina J. Mason, and Gianluca Baio", "title": "Joint longitudinal models for dealing with missing at random data in\n  trial-based economic evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic evaluations based on patient-level data collected alongside\nclinical trials~(e.g. health related quality of life and resource use measures)\nare an important component of the process which informs resource allocation\ndecisions. Almost inevitably, the analysis is complicated by the fact that some\nindividuals drop out from the study, which causes their data to be unobserved\nat some time point. Current practice performs the evaluation by handling the\nmissing data at the level of aggregated variables (e.g. QALYs), which are\nobtained by combining the economic data over the duration of the study, and are\noften conducted under a missing at random (MAR) assumption. However, this\napproach may lead to incorrect inferences since it ignores the longitudinal\nnature of the data and may end up discarding a considerable amount of\nobservations from the analysis. We propose the use of joint longitudinal models\nto extend standard cost-effectiveness analysis methods by taking into account\nthe longitudinal structure and incorporate all available data to improve the\nestimation of the targeted quantities under MAR. Our approach is compared to\npopular missingness approaches in trial-based analyses, motivated by an\nexploratory simulation study, and applied to data from two real case studies.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:29:59 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 08:04:34 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Gabrio", "Andrea", ""], ["Hunter", "Rachael", ""], ["Mason", "Alexina J.", ""], ["Baio", "Gianluca", ""]]}, {"id": "1805.07152", "submitter": "Florent Leclercq", "authors": "Florent Leclercq", "title": "Bayesian optimisation for likelihood-free cosmological inference", "comments": "16+9 pages, 12 figures. Matches PRD published version after minor\n  modifications", "journal-ref": "Phys. Rev. D 98, 063511 (2018)", "doi": "10.1103/PhysRevD.98.063511", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many cosmological models have only a finite number of parameters of interest,\nbut a very expensive data-generating process and an intractable likelihood\nfunction. We address the problem of performing likelihood-free Bayesian\ninference from such black-box simulation-based models, under the constraint of\na very limited simulation budget (typically a few thousand). To do so, we adopt\nan approach based on the likelihood of an alternative parametric model.\nConventional approaches to approximate Bayesian computation such as\nlikelihood-free rejection sampling are impractical for the considered problem,\ndue to the lack of knowledge about how the parameters affect the discrepancy\nbetween observed and simulated data. As a response, we make use of a strategy\npreviously developed in the machine learning literature (Bayesian optimisation\nfor likelihood-free inference, BOLFI), which combines Gaussian process\nregression of the discrepancy to build a surrogate surface with Bayesian\noptimisation to actively acquire training data. We extend the method by\nderiving an acquisition function tailored for the purpose of minimising the\nexpected uncertainty in the approximate posterior density, in the parametric\napproach. The resulting algorithm is applied to the problems of summarising\nGaussian signals and inferring cosmological parameters from the Joint\nLightcurve Analysis supernovae data. We show that the number of required\nsimulations is reduced by several orders of magnitude, and that the proposed\nacquisition function produces more accurate posterior approximations, as\ncompared to common strategies.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:34:20 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 16:13:24 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Leclercq", "Florent", ""]]}, {"id": "1805.07300", "submitter": "Leon Chlon", "authors": "Leon Chlon, Andrew Song, Sandya Subramanian, Hugo Soulat, John Tauber,\n  Demba Ba, Michael Prerau", "title": "Multitaper Spectral Estimation HDP-HMMs for EEG Sleep Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electroencephalographic (EEG) monitoring of neural activity is widely used\nfor sleep disorder diagnostics and research. The standard of care is to\nmanually classify 30-second epochs of EEG time-domain traces into 5 discrete\nsleep stages. Unfortunately, this scoring process is subjective and\ntime-consuming, and the defined stages do not capture the heterogeneous\nlandscape of healthy and clinical neural dynamics. This motivates the search\nfor a data-driven and principled way to identify the number and composition of\nsalient, reoccurring brain states present during sleep. To this end, we propose\na Hierarchical Dirichlet Process Hidden Markov Model (HDP-HMM), combined with\nwide-sense stationary (WSS) time series spectral estimation to construct a\ngenerative model for personalized subject sleep states. In addition, we employ\nmultitaper spectral estimation to further reduce the large variance of the\nspectral estimates inherent to finite-length EEG measurements. By applying our\nmethod to both simulated and human sleep data, we arrive at three main results:\n1) a Bayesian nonparametric automated algorithm that recovers general temporal\ndynamics of sleep, 2) identification of subject-specific \"microstates\" within\ncanonical sleep stages, and 3) discovery of stage-dependent sub-oscillations\nwith shared spectral signatures across subjects.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:44:15 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Chlon", "Leon", ""], ["Song", "Andrew", ""], ["Subramanian", "Sandya", ""], ["Soulat", "Hugo", ""], ["Tauber", "John", ""], ["Ba", "Demba", ""], ["Prerau", "Michael", ""]]}, {"id": "1805.07395", "submitter": "Marco Helbich", "authors": "Marco Helbich, Nadja Klein, Hannah Roberts, Paulien Hagedoorn, Peter\n  Groenewegen", "title": "More green space is related to less antidepressant prescription rates in\n  the Netherlands: A Bayesian geoadditive quantile regression approach", "comments": null, "journal-ref": "Environmental Research 2018", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Exposure to green space seems to be beneficial for self-reported mental\nhealth. In this study we used an objective health indicator, namely\nantidepressant prescription rates. Current studies rely exclusively upon mean\nregression models assuming linear associations. It is, however, plausible that\nthe presence of green space is non-linearly related with different quantiles of\nthe outcome antidepressant prescription rates. These restrictions may\ncontribute to inconsistent findings. Our aim was to assess antidepressant\nprescription rates in relation to green space, and to analyze how the\nrelationship varies non-linearly across different quantiles of antidepressant\nprescription rates. We used cross-sectional data for the year 2014 at a\nmunicipality level in the Netherlands. Ecological Bayesian geoadditive quantile\nregressions were fitted for the 15, 50, and 85 percent quantiles to estimate\ngreen space-prescription rate correlations, controlling for confounders. The\nresults suggested that green space was overall inversely and non-linearly\nassociated with antidepressant prescription rates. More important, the\nassociations differed across the quantiles, although the variation was modest.\nSignificant non-linearities were apparent: The associations were slightly\npositive in the lower quantile and strongly negative in the upper one. Our\nfindings imply that an increased availability of green space within a\nmunicipality may contribute to a reduction in the number of antidepressant\nprescriptions dispensed. Green space is thus a central health and community\nasset, whilst a minimum level of 28 percent needs to be established for health\ngains. The highest effectiveness occurred at a municipality surface percentage\nhigher than 79 percent. This inverse dose-dependent relation has important\nimplications for setting future community-level health and planning policies.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 18:55:25 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 20:04:56 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 05:28:03 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Helbich", "Marco", ""], ["Klein", "Nadja", ""], ["Roberts", "Hannah", ""], ["Hagedoorn", "Paulien", ""], ["Groenewegen", "Peter", ""]]}, {"id": "1805.07435", "submitter": "Rafael S. de Souza", "authors": "M. W. Hattab, R. S. de Souza, B. Ciardi, J.-P. Paardekooper, S.\n  Khochfar, C. Dalla Vecchia", "title": "A case study of hurdle and generalized additive models in astronomy: the\n  escape of ionizing radiation", "comments": null, "journal-ref": "MNRAS, Volume 483, Issue 3, p.3307-3321 (2019)", "doi": "10.1093/mnras/sty3314", "report-no": null, "categories": "astro-ph.IM astro-ph.CO stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dark ages of the Universe end with the formation of the first generation\nof stars residing in primeval galaxies. These objects were the first to produce\nultraviolet ionizing photons in a period when the cosmic gas changed from a\nneutral state to an ionized one, known as Epoch of Reionization (EoR). A\npivotal aspect to comprehend the EoR is to probe the intertwined relationship\nbetween the fraction of ionizing photons capable to escape dark haloes, also\nknown as the escape fraction ($f_{esc}$), and the physical properties of the\ngalaxy. This work develops a sound statistical model suitable to account for\nsuch non-linear relationships and the non-Gaussian nature of $f_{esc}$. This\nmodel simultaneously estimates the probability that a given primordial galaxy\nstarts the ionizing photon production and estimates the mean level of the\n$f_{esc}$ once it is triggered. The model was employed in the First Billion\nYears simulation suite, from which we show that the baryonic fraction and the\nrate of ionizing photons appear to have a larger impact on $f_{esc}$ than\npreviously thought. A naive univariate analysis of the same problem would\nsuggest smaller effects for these properties and a much larger impact for the\nspecific star formation rate, which is lessened after accounting for other\ngalaxy properties and non-linearities in the statistical model.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 20:40:02 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 13:17:58 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Hattab", "M. W.", ""], ["de Souza", "R. S.", ""], ["Ciardi", "B.", ""], ["Paardekooper", "J. -P.", ""], ["Khochfar", "S.", ""], ["Vecchia", "C. Dalla", ""]]}, {"id": "1805.07465", "submitter": "Elias Chaibub Neto", "authors": "Elias Chaibub Neto", "title": "Using permutations to detect, quantify and correct for confounding in\n  machine learning predictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical machine learning applications are often plagued with confounders\nthat are clinically irrelevant, but can still artificially boost the predictive\nperformance of the algorithms. Confounding is especially problematic in mobile\nhealth studies run \"in the wild\", where it is challenging to balance the\ndemographic characteristics of participants that self select to enter the\nstudy. An effective approach to remove the influence of confounders is to match\nsamples in order to improve the balance in the data. The caveat is that we\nend-up with a smaller number of participants to train and evaluate the machine\nlearning algorithm. Alternative confounding adjustment methods that make more\nefficient use of the data (e.g., inverse probability weighting) usually rely on\nmodeling assumptions, and it is unclear how robust these methods are to\nviolations of these assumptions. Here, rather than proposing a new approach to\nprevent/reduce the learning of confounding signals by a machine learning\nalgorithm, we develop novel statistical tools to detect, quantify and correct\nfor the influence of observed confounders. Our tools are based on restricted\nand standard permutation approaches and can be used to evaluate how well a\nconfounding adjustment method is actually working. We use restricted\npermutations to test if an algorithm has learned disease signal in the presence\nof confounding signal, and to develop a novel statistical test to detect\nconfounding learning per se. Furthermore, we prove that restricted permutations\nprovide an alternative method to compute partial correlations, and use this\nresult as a motivation to develop a novel approach to estimate the corrected\npredictive performance of a learner. We evaluate the statistical properties of\nour methods in simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 22:55:48 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 23:47:43 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Neto", "Elias Chaibub", ""]]}, {"id": "1805.07575", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Sequential adaptive elastic net approach for single-snapshot source\n  localization", "comments": "12 pages, 5 figures, in the publication to the Journal of the\n  Acoustical Society of America", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.CV math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes efficient algorithms for accurate recovery of\ndirection-of-arrival (DoA) of sources from single-snapshot measurements using\ncompressed beamforming (CBF). In CBF, the conventional sensor array signal\nmodel is cast as an underdetermined complex-valued linear regression model and\nsparse signal recovery methods are used for solving the DoA finding problem. We\ndevelop a complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm\nthat finds solutions at knots of penalty parameter values over a path (or grid)\nof EN tuning parameter values. c-PW-WEN also computes Lasso or weighted Lasso\nin its path. We then propose a sequential adaptive EN (SAEN) method that is\nbased on c-PW-WEN algorithm with adaptive weights that depend on the previous\nsolution. Extensive simulation studies illustrate that SAEN improves the\nprobability of exact recovery of true support compared to conventional sparse\nsignal recovery approaches such as Lasso, elastic net or orthogonal matching\npursuit in several challenging multiple target scenarios. The effectiveness of\nSAEN is more pronounced in the presence of high mutual coherence.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 11:57:54 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1805.07643", "submitter": "Yan Chang", "authors": "Yan Chang, Weiqing Yang, Ding Zhao", "title": "Energy Efficiency and Emission Testing for Connected and Automated\n  Vehicles Using Real-World Driving Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH eess.SP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By using the onboard sensing and external connectivity technology, connected\nand automated vehicles (CAV) could lead to improved energy efficiency, better\nrouting, and lower traffic congestion. With the rapid development of the\ntechnology and adaptation of CAV, it is more critical to develop the universal\nevaluation method and the testing standard which could evaluate the impacts on\nenergy consumption and environmental pollution of CAV fairly, especially under\nthe various traffic conditions. In this paper, we proposed a new method and\nframework to evaluate the energy efficiency and emission of the vehicle based\non the unsupervised learning methods. Both the real-world driving data of the\nevaluated vehicle and the large naturalistic driving dataset are used to\nperform the driving primitive analysis and coupling. Then the linear weighted\nestimation method could be used to calculate the testing result of the\nevaluated vehicle. The results show that this method can successfully identify\nthe typical driving primitives. The couples of the driving primitives from the\nevaluated vehicle and the typical driving primitives from the large real-world\ndriving dataset coincide with each other very well. This new method could\nenhance the standard development of the energy efficiency and emission testing\nof CAV and other off-cycle credits.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 19:20:31 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:29:51 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Chang", "Yan", ""], ["Yang", "Weiqing", ""], ["Zhao", "Ding", ""]]}, {"id": "1805.07672", "submitter": "Pedro Ramos", "authors": "Pedro L. Ramos, Dipak K. Dey, Francisco Louzada, Victor H. Lachos", "title": "An Extended Poisson Family of Life Distribution: A Unified Approach in\n  Competitive and Complementary Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new approach to generate flexible parametric\nfamilies of distributions. These models arise on competitive and complementary\nrisks scenario, in which the lifetime associated with a particular risk is not\nobservable, rather, we observe only the minimum/maximum lifetime value among\nall risks. The latent variables have a zero truncated Poisson distribution. For\nthe proposed family of distribution, the extra shape parameter has an important\nphysical interpretation in the competing and complementary risks scenario. The\nmathematical properties and inferential procedures are discussed. The proposed\napproach is applied in some existing distributions in which it is fully\nillustrated by an important data set.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 22:46:47 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Dey", "Dipak K.", ""], ["Louzada", "Francisco", ""], ["Lachos", "Victor H.", ""]]}, {"id": "1805.07688", "submitter": "Ningren Han", "authors": "Ningren Han and Rajeev J. Ram", "title": "Bayesian Modeling and Computation for Analyte Quantification in Complex\n  Mixtures Using Raman Spectroscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a two-stage algorithm based on Bayesian modeling and\ncomputation aiming at quantifying analyte concentrations or quantities in\ncomplex mixtures with Raman spectroscopy. A hierarchical Bayesian model is\nbuilt for spectral signal analysis, and reversible-jump Markov chain Monte\nCarlo (RJMCMC) computation is carried out for model selection and spectral\nvariable estimation. Processing is done in two stages. In the first stage, the\npeak representations for a target analyte spectrum are learned. In the second,\nthe peak variables learned from the first stage are used to estimate the\nconcentration or quantity of the target analyte in a mixture. Numerical\nexperiments validated its quantification performance over a wide range of\nsimulation conditions and established its advantages for analyte quantification\ntasks under the small training sample size regime over conventional\nmultivariate regression algorithms. We also used our algorithm to analyze\nexperimental spontaneous Raman spectroscopy data collected for glucose\nconcentration estimation in biopharmaceutical process monitoring applications.\nOur work shows that this algorithm can be a promising complementary tool\nalongside conventional multivariate regression algorithms in Raman\nspectroscopy-based mixture quantification studies, especially when collecting a\nlarge training dataset with high quality is challenging or resource-intensive.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 01:23:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Han", "Ningren", ""], ["Ram", "Rajeev J.", ""]]}, {"id": "1805.07770", "submitter": "Peter Zeidman", "authors": "Peter Zeidman, Samira M Kazan, Nick Todd, Nikolaus Weiskopf, Karl J.\n  Friston, Martina F. Callaghan", "title": "Optimising data for modelling neuronal responses", "comments": "Equal contribution by Peter Zeidman and Samira M Kazan", "journal-ref": null, "doi": "10.3389/fnins.2018.00986", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical note, we address an unresolved challenge in neuroimaging\nstatistics: how to determine which of several datasets is the best for\ninferring neuronal responses. Comparisons of this kind are important for\nexperimenters when choosing an imaging protocol - and for developers of new\nacquisition methods. However, the hypothesis that one dataset is better than\nanother cannot be tested using conventional statistics (based on likelihood\nratios), as these require the data to be the same under each hypothesis. Here\nwe present Bayesian data comparison, a principled framework for evaluating the\nquality of functional imaging data, in terms of the precision with which\nneuronal connectivity parameters can be estimated and competing models can be\ndisambiguated. For each of several candidate datasets, neuronal responses are\ninferred using Dynamic Casual Modelling (DCM) - a commonly used Bayesian\nprocedure for modelling neuroimaging data. Next, the parameters from\nsubject-specific models are summarised at the group level using a Bayesian\nGeneral Linear Model (GLM). A series of measures, which we introduce here, are\nthen used to evaluate each dataset in terms of the precision of (group-level)\nparameter estimates and the ability of the data to distinguish similar models.\nTo exemplify the approach, we compared four datasets that were acquired in a\nstudy evaluating multiband fMRI acquisition schemes. To enable people to\nreproduce these analyses using their own data and experimental paradigms, we\nprovide general-purpose Matlab code via the SPM software.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 14:57:32 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Zeidman", "Peter", ""], ["Kazan", "Samira M", ""], ["Todd", "Nick", ""], ["Weiskopf", "Nikolaus", ""], ["Friston", "Karl J.", ""], ["Callaghan", "Martina F.", ""]]}, {"id": "1805.07777", "submitter": "Yu Li", "authors": "Yu Li, Fan Xu, Fa Zhang, Pingyong Xu, Mingshu Zhang, Ming Fan, Lihua\n  Li, Xin Gao, Renmin Han", "title": "DLBI: Deep learning guided Bayesian inference for structure\n  reconstruction of super-resolution fluorescence microscopy", "comments": "Accepted by ISMB 2018", "journal-ref": "Bioinformatics, Volume 34, Issue 13, 1 July 2018", "doi": "10.1093/bioinformatics/bty241", "report-no": null, "categories": "cs.CV stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Super-resolution fluorescence microscopy, with a resolution beyond the\ndiffraction limit of light, has become an indispensable tool to directly\nvisualize biological structures in living cells at a nanometer-scale\nresolution. Despite advances in high-density super-resolution fluorescent\ntechniques, existing methods still have bottlenecks, including extremely long\nexecution time, artificial thinning and thickening of structures, and lack of\nability to capture latent structures. Here we propose a novel deep learning\nguided Bayesian inference approach, DLBI, for the time-series analysis of\nhigh-density fluorescent images. Our method combines the strength of deep\nlearning and statistical inference, where deep learning captures the underlying\ndistribution of the fluorophores that are consistent with the observed\ntime-series fluorescent images by exploring local features and correlation\nalong time-axis, and statistical inference further refines the ultrastructure\nextracted by deep learning and endues physical meaning to the final image.\nComprehensive experimental results on both real and simulated datasets\ndemonstrate that our method provides more accurate and realistic local patch\nand large-field reconstruction than the state-of-the-art method, the 3B\nanalysis, while our method is more than two orders of magnitude faster. The\nmain program is available at https://github.com/lykaust15/DLBI\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 15:28:56 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 05:32:56 GMT"}, {"version": "v3", "created": "Sat, 1 Sep 2018 20:07:42 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Li", "Yu", ""], ["Xu", "Fan", ""], ["Zhang", "Fa", ""], ["Xu", "Pingyong", ""], ["Zhang", "Mingshu", ""], ["Fan", "Ming", ""], ["Li", "Lihua", ""], ["Gao", "Xin", ""], ["Han", "Renmin", ""]]}, {"id": "1805.07826", "submitter": "Jinghui Yuan", "authors": "Jinghui Yuan, Mohamed Abdel-Aty, Ling Wang, Jaeyoung Lee, Xuesong\n  Wang, Rongjie Yu", "title": "Real-Time Crash Risk Analysis of Urban Arterials Incorporating\n  Bluetooth, Weather, and Adaptive Signal Control Data", "comments": "Presented at Transportation Research Board 97th Annual Meeting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Real-time safety analysis has become a hot research topic as it can reveal\nthe relationship between real-time traffic characteristics and crash occurrence\nmore accurately, and these results could be applied to improve active traffic\nmanagement systems and enhance safety performance. Most of the previous studies\nhave been applied to freeways and seldom to arterials. Therefore, this study\nattempts to examine the relationship between crash occurrence and real-time\ntraffic and weather characteristics based on four urban arterials in Central\nFlorida. Considering the substantial difference between the interrupted traffic\nflow on urban arterials and the free flow on freeways, the adaptive signal\nphasing was also introduced in this study. Bayesian conditional logistic models\nwere developed by incorporating the Bluetooth, adaptive signal control, and\nweather data, which were extracted for a period of 20 minutes (four 5-minute\ninterval) before the time of crash occurrence. Model comparison results\nindicate that the model based on 5-10 minute interval dataset is the most\nappropriate model. It reveals that the average speed, upstream volume, and\nrainy weather indicator were found to have significant effects on crash\noccurrence. Furthermore, both Bayesian logistic and Bayesian random effects\nlogistic models were developed to compare with the Bayesian conditional\nlogistic model, and the Bayesian conditional logistic model was found to be\nmuch better than the other two models. These results are important in real-time\nsafety applications in the context of Integrated Active Traffic Management.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 21:35:38 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Yuan", "Jinghui", ""], ["Abdel-Aty", "Mohamed", ""], ["Wang", "Ling", ""], ["Lee", "Jaeyoung", ""], ["Wang", "Xuesong", ""], ["Yu", "Rongjie", ""]]}, {"id": "1805.07827", "submitter": "Jinghui Yuan", "authors": "Jinghui Yuan, Mohamed Abdel-Aty, Ling Wang, Jaeyoung Lee, Rongjie Yu,\n  Xuesong Wang", "title": "Utilizing Bluetooth and Adaptive Signal Control Data for Urban Arterials\n  Safety Analysis", "comments": "Under second round review at Transportation Research Part C", "journal-ref": "https://doi.org/10.1016/j.trc.2018.10.009", "doi": "10.1016/j.trc.2018.10.009", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time safety analysis has become a hot research topic as it can more\naccurately reveal the relationships between real-time traffic characteristics\nand crash occurrence, and these results could be applied to improve active\ntraffic management systems and enhance safety performance. Most of the previous\nstudies have been applied to freeways and seldom to arterials. This study\nattempts to examine the relationship between crash occurrence and real-time\ntraffic and weather characteristics based on four urban arterials in Central\nFlorida. Considering the substantial difference between the interrupted urban\narterials and the access controlled freeways, the adaptive signal phasing data\nwas introduced in addition to the traditional traffic data. Bayesian\nconditional logistic models were developed by incorporating the Bluetooth,\nadaptive signal control, and weather data, which were extracted for a period of\n20 minutes (four 5-minute intervals) before the time of crash occurrence. Model\ncomparison results indicated that the model based on 5-10 minute interval\ndataset performs the best. It revealed that the average speed, upstream\nleft-turn volume, downstream green ratio, and rainy indicator were found to\nhave significant effects on crash occurrence. Furthermore, both Bayesian random\nparameters logistic and Bayesian random parameters conditional logistic models\nwere developed to compare with the Bayesian conditional logistic model, and the\nBayesian random parameters conditional logistic model was found to have the\nbest model performance in terms of the AUC and DIC values. These results are\nimportant in real-time safety applications in the context of Integrated Active\nTraffic Management.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 21:45:21 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Yuan", "Jinghui", ""], ["Abdel-Aty", "Mohamed", ""], ["Wang", "Ling", ""], ["Lee", "Jaeyoung", ""], ["Yu", "Rongjie", ""], ["Wang", "Xuesong", ""]]}, {"id": "1805.07834", "submitter": "Cheng Zhang", "authors": "Cheng Zhang, Frederick A. Matsen IV", "title": "Generalizing Tree Probability Estimation via Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probability estimation is one of the fundamental tasks in statistics and\nmachine learning. However, standard methods for probability estimation on\ndiscrete objects do not handle object structure in a satisfactory manner. In\nthis paper, we derive a general Bayesian network formulation for probability\nestimation on leaf-labeled trees that enables flexible approximations which can\ngeneralize beyond observations. We show that efficient algorithms for learning\nBayesian networks can be easily extended to probability estimation on this\nchallenging structured space. Experiments on both synthetic and real data show\nthat our methods greatly outperform the current practice of using the empirical\ndistribution, as well as a previous effort for probability estimation on trees.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 22:50:31 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 03:57:25 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Zhang", "Cheng", ""], ["Matsen", "Frederick A.", "IV"]]}, {"id": "1805.07849", "submitter": "Andrew Ying", "authors": "Andrew Ying, Ronghui Xu and James Murphy", "title": "Two-Stage Residual Inclusion under the Additive Hazards Model - An\n  Instrumental Variable Approach with Application to SEER-Medicare Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable is an essential tool for addressing unmeasured\nconfounding in observational studies. Two stage predictor substitution (2SPS)\nestimator and two stage residual inclusion(2SRI) are two commonly used\napproaches in applying instrumental variables. Recently 2SPS was studied under\nthe additive hazards model in the presence of competing risks of time-to-events\ndata, where linearity was assumed for the relationship between the treatment\nand the instrument variable. This assumption may not be the most appropriate\nwhen we have binary treatments. In this paper, we consider the 2SRI estimator\nunder the additive hazards model for general survival data and in the presence\nof competing risks, which allows generalized linear models for the relation\nbetween the treatment and the instrumental variable. We derive the asymptotic\nproperties including a closed-form asymptotic variance estimate for the 2SRI\nestimator. We carry out numerical studies in finite samples, and apply our\nmethodology to the linked Surveillance, Epidemiology and End Results (SEER) -\nMedicare database comparing radical prostatectomy versus conservative treatment\nin early-stage prostate cancer patients.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 00:20:42 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 00:26:34 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Ying", "Andrew", ""], ["Xu", "Ronghui", ""], ["Murphy", "James", ""]]}, {"id": "1805.08110", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio, Laurent Remontet, Nicholas P. Jewell, Aur\\'elien\n  Belot", "title": "On a general structure for hazard-based regression models: an\n  application to population-based cancer research", "comments": "To appear in Statistical Methods in Medical Research. Supplementary\n  material and software available here:\n  https://sites.google.com/site/fjavierrubio67/home/papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional hazards model represents the most commonly assumed hazard\nstructure when analysing time to event data using regression models. We study a\ngeneral hazard structure which contains, as particular cases, proportional\nhazards, accelerated hazards, and accelerated failure time structures, as well\nas combinations of these. We propose an approach to apply these different\nhazard structures, based on a flexible parametric distribution (Exponentiated\nWeibull) for the baseline hazard. This distribution allows us to cover the\nbasic hazard shapes of interest in practice: constant, bathtub, increasing,\ndecreasing, and unimodal. In an extensive simulation study, we evaluate our\napproach in the context of excess hazard modelling, which is the main quantity\nof interest in descriptive cancer epidemiology. This study exhibits good\ninferential properties of the proposed model, as well as good performance when\nusing the Akaike Information Criterion for selecting the hazard structure. An\napplication on lung cancer data illustrates the usefulness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:06:50 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 22:22:09 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Remontet", "Laurent", ""], ["Jewell", "Nicholas P.", ""], ["Belot", "Aur\u00e9lien", ""]]}, {"id": "1805.08141", "submitter": "Diego Marcondes", "authors": "Diego Marcondes, Cl\\'audia Peixoto and Julio Michael Stern", "title": "Assessing randomness in case assignment: the case study of the Brazilian\n  Supreme Court", "comments": null, "journal-ref": null, "doi": "10.1093/lpr/mgz006", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sortition, i.e., random appointment for public duty, has been employed by\nsocieties throughout the years, especially for duties related to the judicial\nsystem, as a firewall designated to prevent illegitimate interference between\nparties in a legal case and agents of the legal system. In judicial systems of\nmodern western countries, random procedures are mainly employed to select the\njury, the court and/or the judge in charge of judging a legal case, so that\nthey have a significant role in the course of a case. Therefore, these random\nprocedures must comply with some principles, as statistical soundness; complete\nauditability; open-source programming; and procedural, cryptographical and\ncomputational security. Nevertheless, some of these principles are neglected by\nsome random procedures in judicial systems, that are, in some cases, performed\nin secrecy and are not auditable by the involved parts. The assignment of cases\nin the Brazilian Supreme Court (Supremo Tribunal Federal) is an example of such\nprocedures, for it is performed by a closed-source algorithm, unknown to the\npublic and to the parts involved in the judicial cases, that allegedly assign\nthe cases randomly to the justice chairs based on their caseload.\n  In this context, this article presents a review of how sortition has been\nemployed historically by societies, and discusses how Mathematical Statistics\nmay be applied to random procedures of the judicial system, as it has been\napplied for almost a century on clinical trials, for example. Based on this\ndiscussion, a statistical model for assessing randomness in case assignment is\nproposed and applied to the Brazilian Supreme Court in order to shed light on\nhow this assignment process is performed by the closed-source algorithm.\nGuidelines for random procedures are outlined and topics for further researches\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:47:34 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Marcondes", "Diego", ""], ["Peixoto", "Cl\u00e1udia", ""], ["Stern", "Julio Michael", ""]]}, {"id": "1805.08148", "submitter": "Kota Ogasawara", "authors": "Kota Ogasawara", "title": "Persistence of Natural Disasters on Children's Health: Evidence from the\n  Great Kanto Earthquake of 1923", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses a catastrophic earthquake in 1923 to analyze the long-term\neffects of a one-off disaster on children's health. I find that fetal exposure\nto Japan's Great Kanto Earthquake had stunting effects on girls in the\ndevastated area. Disaster relief spending helped remediate stunting among boys\nby late primary school ages, whereas it did not ameliorate girls' stunting,\nsuggesting a biased remediation mechanism before birth and compensating\ninvestment after birth. While the maternal mental stress via strong vibrations\nplayed a role in the adverse health effects, the maternal nutritional stress\nvia physical disruption also enhanced those effects.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:59:36 GMT"}, {"version": "v10", "created": "Sun, 23 Feb 2020 11:38:45 GMT"}, {"version": "v11", "created": "Thu, 3 Dec 2020 03:21:28 GMT"}, {"version": "v12", "created": "Tue, 9 Feb 2021 02:50:21 GMT"}, {"version": "v13", "created": "Mon, 1 Mar 2021 01:28:27 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 13:51:19 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 14:23:12 GMT"}, {"version": "v4", "created": "Mon, 30 Jul 2018 08:01:23 GMT"}, {"version": "v5", "created": "Mon, 6 Aug 2018 00:02:43 GMT"}, {"version": "v6", "created": "Wed, 15 Aug 2018 13:08:35 GMT"}, {"version": "v7", "created": "Wed, 17 Oct 2018 00:55:40 GMT"}, {"version": "v8", "created": "Mon, 25 Mar 2019 08:56:58 GMT"}, {"version": "v9", "created": "Wed, 12 Jun 2019 07:45:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ogasawara", "Kota", ""]]}, {"id": "1805.08233", "submitter": "Michael Tzen", "authors": "Michael Tzen", "title": "Multilevel Models Allow Modular Specification of What and Where to\n  Regularize, Especially in Small Area Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Through the lense of multilevel model (MLM) specification and regularization,\nthis is a connect-the-dots introductory summary of Small Area Estimation, e.g.\nsmall group prediction informed by a complex sampling design. While a\ncomprehensive book is (Rao and Molina 2015), the goal of this paper is to get\ninterested researchers up to speed with some current developments. We first\nprovide historical context of two kinds of regularization: 1) the\nregularization 'within' the components of a predictor and 2) the regularization\n'between' outcome and predictor. We focus on the MLM framework as it allows the\nanalyst to flexibly control the targets of the regularization. The flexible\ncontrol is useful when analysts want to overcome shortcomings in design-based\nestimates. We'll describe the precision deficiencies (high variance) typical of\ndesign-based estimates of small groups. We then highlight an interesting MLM\nexample from (Chaudhuri and Ghosh 2011) that integrates both kinds of\nregularization (between and within). The key idea is to use the design-based\nvariance to control the amount of 'between' regularization and prior\ninformation to regularize the components 'within' a predictor. The goal is to\nlet the design-based estimate have authority (when precise) but defer to a\nmodel-based prediction when imprecise. We conclude by discussing optional\ncriteria to incorporate into a MLM prediction and possible entrypoints for\nextensions.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:04:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Tzen", "Michael", ""]]}, {"id": "1805.08275", "submitter": "Jorge Balat", "authors": "Jorge Balat, Sukjin Han", "title": "Multiple Treatments with Strategic Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical framework to identify and estimate the effects of\ntreatments on outcomes of interest when the treatments are the result of\nstrategic interaction (e.g., bargaining, oligopolistic entry, peer effects). We\nconsider a model where agents play a discrete game with complete information\nwhose equilibrium actions (i.e., binary treatments) determine a post-game\noutcome in a nonseparable model with endogeneity. Due to the simultaneity in\nthe first stage, the model as a whole is incomplete and the selection process\nfails to exhibit the conventional monotonicity. Without imposing parametric\nrestrictions or large support assumptions, this poses challenges in recovering\ntreatment parameters. To address these challenges, we first establish a\nmonotonic pattern of the equilibria in the first-stage game in terms of the\nnumber of treatments selected. Based on this finding, we derive bounds on the\naverage treatment effects (ATEs) under nonparametric shape restrictions and the\nexistence of excluded exogenous variables. We show that instrument variation\nthat compensates strategic substitution helps solve the multiple equilibria\nproblem. We apply our method to data on airlines and air pollution in cities in\nthe U.S. We find that (i) the causal effect of each airline on pollution is\npositive, and (ii) the effect is increasing in the number of firms but at a\ndecreasing rate.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:05:36 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 22:19:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Balat", "Jorge", ""], ["Han", "Sukjin", ""]]}, {"id": "1805.08314", "submitter": "Claudiu Herteliu", "authors": "Bogdan Vasile Ileanu, Marcel Ausloos, Claudiu Herteliu, Marian\n  Pompiliu Cristescu", "title": "Intriguing behavior when testing the impact of quotation marks usage in\n  Google search results", "comments": "14 pages, 4 figures, 1 table, 53 references, accepted within Quality\n  & Quantity", "journal-ref": null, "doi": "10.1007/s11135-018-0771-0", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet research on search engine quality and validity of results demand\nmuch concern. Thus, the focus in our study has been to measure the impact of\nquotation marks usage on the internet search outputs in terms of google search\noutcomes distributions, through Benford Law. The current paper is focused on\napplying a Benford Law analysis on two related types of internet searches\ndistinguished by the usage or absence of quotation marks. Both search results\nvalues are assumed as variables. We found that the first digit of outcomes does\nnot follow the Benford Law first digit of numbers in the case of searching text\nwithout quotation marks. Unexpectedly, the Benford Law is obeyed when quotation\nmarks are used, even if the variability of search outcomes is considerably\nreduced. By studying outputs demonstrating influences of (apparently at first)\n\"details\", in using a search engine, the authors are able to further warn the\nusers concerning the validity of such outputs.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:47:05 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ileanu", "Bogdan Vasile", ""], ["Ausloos", "Marcel", ""], ["Herteliu", "Claudiu", ""], ["Cristescu", "Marian Pompiliu", ""]]}, {"id": "1805.08323", "submitter": "Nicholas Clark", "authors": "Nicholas J Clark, Philip M. Dixon", "title": "A Class of Spatially Correlated Self-Exciting Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.spasta.2021.100493", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The statistical modeling of multivariate count data observed on a space-time\nlattice has generally focused on using a hierarchical modeling approach where\nspace-time correlation structure is placed on a continuous, latent, process.\nThe count distribution is then assumed to be conditionally independent given\nthe latent process. However, in many real-world applications, especially in the\nmodeling of criminal or terrorism data, the conditional independence between\nthe count distributions is inappropriate. In this manuscript we propose a class\nof models that capture spatial variation and also account for the possibility\nof data model dependence. The resulting model allows both data model\ndependence, or self-excitation, as well as spatial dependence in a latent\nstructure. We demonstrate how second-order properties can be used to\ncharacterize the spatio-temporal process and how misspecificaiton of error may\ninflate self-excitation in a model. Finally, we give an algorithm for efficient\nBayesian inference for the model demonstrating its use in capturing the\nspatio-temporal structure of burglaries in Chicago from 2010-2015.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 23:47:53 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2020 12:57:16 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2021 17:32:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Clark", "Nicholas J", ""], ["Dixon", "Philip M.", ""]]}, {"id": "1805.08374", "submitter": "Jinghui Yuan", "authors": "Jinghui Yuan, Xuesong Wang", "title": "Modeling the Safety Effect of Access and Signal Density on Suburban\n  Arterials: Using Macro Level Analysis Method", "comments": "Presented at the 10th Annual Meeting of China Intelligent\n  Transportation System Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With rapidly increasing of the land development density along suburban\narterials, much more irregular signal spacing appeared on suburban arterials,\nand high access density is commonly observed on suburban arterials. These\nissues tend to increase the risk of crash occurrence of arterials. By\ndeveloping safety performance functions on road segments and intersections\nseparately, the previous research analyzed the partial safety effects of the\ninfluence factors. In this study, Bayesian Conditional Autoregressive (CAR)\nmodels were developed at traffic analysis zone (TAZ) level for suburban\narterials laid in suburban area in Shanghai. The model result showed that\nhigher access and signal density tend to increase crash frequencies occurred on\narterials. At this point, designing frontage roads paralleled to arterials to\ncollect the access traffic instead of those intensive access could reduce\ncrashes occurred on arterials.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 03:25:48 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Yuan", "Jinghui", ""], ["Wang", "Xuesong", ""]]}, {"id": "1805.08463", "submitter": "Ho Chung Leon Law", "authors": "Ho Chung Leon Law, Dino Sejdinovic, Ewan Cameron, Tim CD Lucas, Seth\n  Flaxman, Katherine Battle, Kenji Fukumizu", "title": "Variational Learning on Aggregate Outputs with Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a typical supervised learning framework assumes that the inputs and the\noutputs are measured at the same levels of granularity, many applications,\nincluding global mapping of disease, only have access to outputs at a much\ncoarser level than that of the inputs. Aggregation of outputs makes\ngeneralization to new inputs much more difficult. We consider an approach to\nthis problem based on variational learning with a model of output aggregation\nand Gaussian processes, where aggregation leads to intractability of the\nstandard evidence lower bounds. We propose new bounds and tractable\napproximations, leading to improved prediction accuracy and scalability to\nlarge datasets, while explicitly taking uncertainty into account. We develop a\nframework which extends to several types of likelihoods, including the Poisson\nmodel for aggregated count data. We apply our framework to a challenging and\nimportant problem, the fine-scale spatial modelling of malaria incidence, with\nover 1 million observations.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 09:08:01 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Law", "Ho Chung Leon", ""], ["Sejdinovic", "Dino", ""], ["Cameron", "Ewan", ""], ["Lucas", "Tim CD", ""], ["Flaxman", "Seth", ""], ["Battle", "Katherine", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1805.08561", "submitter": "Xanthi Pedeli", "authors": "Xanthi Pedeli and Dimitris Karlis", "title": "An integer-valued time series model for multivariate surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent days different types of surveillance data are becoming available\nfor public health reasons. In most cases several variables are monitored and\nevents of different types are reported. As the amount of surveillance data\nincreases, statistical methods that can effectively address multivariate\nsurveillance scenarios are demanded. Even though research activity in this\nfield is increasing rapidly in recent years, only a few approaches have\nsimultaneously addressed the integer-valued property of the data and its\ncorrelation (both time correlation and cross correlation) structure. In this\npaper, we suggest a multivariate integer-valued autoregressive model that\nallows for both serial and cross correlation between the series and can easily\naccommodate overdispersion and covariate information. Moreover, its structure\nimplies a natural decomposition into an endemic and an epidemic component, a\ncommon distinction in dynamic models for infectious disease counts. Detection\nof disease outbreaks is achieved through the comparison of surveillance data\nwith one-step-ahead predictions obtained after fitting the suggested model to a\nset of clean historical data. The performance of the suggested model is\nillustrated on a trivariate series of syndromic surveillance data collected\nduring Athens 2004 Olympic Games.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 13:05:05 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 07:16:02 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 12:48:45 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Pedeli", "Xanthi", ""], ["Karlis", "Dimitris", ""]]}, {"id": "1805.08740", "submitter": "Carla Sciarra", "authors": "Carla Sciarra, Guido Chiarotti, Francesco Laio, Luca Ridolfi", "title": "A change of perspective in network centrality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typing Yesterday into the search-bar of your browser provides a long list of\nwebsites with, in top places, a link to a video by The Beatles. The order your\nbrowser shows its search results is a notable example of the use of network\ncentrality. Centrality is a measure of the importance of the nodes in a network\nand it plays a crucial role in a huge number of fields, ranging from sociology\nto engineering, and from biology to economics. Many metrics are available to\nevaluate centrality. However, centrality measures are generally based on ad hoc\nassumptions, and there is no commonly accepted way to compare the effectiveness\nand reliability of different metrics. Here we propose a new perspective where\ncentrality definition arises naturally from the most basic feature of a\nnetwork, its adjacency matrix. Following this perspective, different centrality\nmeasures naturally emerge, including the degree, eigenvector, and hub-authority\ncentrality. Within this theoretical framework, the accuracy of different\nmetrics can be compared. Tests on a large set of networks show that the\nstandard centrality metrics perform unsatisfactorily, highlighting intrinsic\nlimitations of these metrics for describing the centrality of nodes in complex\nnetworks. More informative multi-component centrality metrics are proposed as\nthe natural extension of standard metrics.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:03:54 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 07:03:01 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Sciarra", "Carla", ""], ["Chiarotti", "Guido", ""], ["Laio", "Francesco", ""], ["Ridolfi", "Luca", ""]]}, {"id": "1805.08907", "submitter": "Henrike H\\\"abel Ph.D.", "authors": "Henrike H\\\"abel, Andr\\'as Bal\\'azs and Mari Myllym\\\"aki", "title": "Spatial analysis of airborne laser scanning point clouds for predicting\n  forest variables", "comments": "22 pages, 5 figures, 5 tables (including the appendix). Taking\n  valuable comments on our manuscript [v2] into consideration, we have decided\n  to emphasize the focus on the spatial analysis of airborne laser scanning\n  (ALS) point clouds and the new spatial ALS feature variables. By doing so, we\n  have reduced the methodology for the spatial structure of forests evaluated\n  on the field plot level", "journal-ref": "Mathematical and Computational Forestry & Natural-Resource\n  Sciences 13(1), 15-28, 2021", "doi": null, "report-no": null, "categories": "stat.AP q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent developments in remote sensing technologies, plot-level forest\nresources can be predicted utilizing airborne laser scanning (ALS). The\nprediction is often assisted by mostly vertical summaries of the ALS point\nclouds. We present a spatial analysis of the point cloud by studying the\nhorizontal distribution of the pulse returns through canopy height models\nthresholded at different height levels. The resulting patterns of patches of\nvegetation and gabs on each layer are summarized to spatial ALS features. We\npropose new features based on the Euler number, which is the number of patches\nminus the number of gaps, and the empty-space function, which is a spatial\nsummary function of the gab space. The empty-space function is also used to\ndescribe differences in the gab structure between two different layers. We\nillustrate usefulness of the proposed spatial features for predicting different\nforest variables that summarize the spatial structure of forests or their\nbreast height diameter distribution. We employ the proposed spatial features,\nin addition to commonly used features from literature, in the well-known k-nn\nestimation method to predict the forest variables. We present the methodology\non the example of a study site in Central Finland.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 23:29:32 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 10:51:49 GMT"}, {"version": "v3", "created": "Wed, 14 Nov 2018 09:05:16 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["H\u00e4bel", "Henrike", ""], ["Bal\u00e1zs", "Andr\u00e1s", ""], ["Myllym\u00e4ki", "Mari", ""]]}, {"id": "1805.08937", "submitter": "Brynjulf Owren", "authors": "Kjetil Haugen and Brynjulf Owren", "title": "Predicting football tables by a maximally parsimonious model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents some useful mathematical results involved in football\ntable prediction. In addition, some empirical results indicate that an\nalternative methodology for football table prediction may produce high quality\nforecasts with far less resource usage than conventional methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 02:25:04 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Haugen", "Kjetil", ""], ["Owren", "Brynjulf", ""]]}, {"id": "1805.08968", "submitter": "Arturo Erdely", "authors": "Arturo Erdely", "title": "An\\'alisis estad\\'istico ex post del conteo r\\'apido institucional de la\n  elecci\\'on de gobernador del Estado de M\\'exico en 2017", "comments": "16 pages, 6 tables, 2 figures, in Spanish", "journal-ref": "Apuntes Electorales 60, 207-242. ISSN 1665-0921", "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical analysis of an electoral quick count based on the total count\nof votes in the election of the State of Mexico's governor in 2017 is performed\nin order to verify precision, confidence level of interval estimations,\npossible bias and derived conclusions therein, with the main purpose of\nchecking compliance with the objectives of such statistical procedure.\n  -----\n  Se realiza un an\\'alisis estad\\'istico de las estimaciones del conteo\nr\\'apido institucional desde la perspectiva ideal de los resultados de los\nc\\'omputos distritales de la elecci\\'on de gobernador del Estado de M\\'exico\ndel a\\~no 2017, particularmente aspectos como la precisi\\'on de las\nestimaciones, el nivel de confianza de los intervalos, el posible sesgo\nrespecto al c\\'omputo distrital y las conclusiones que se derivaron y\nreportaron, con el objetivo de determinar el grado de cumplimiento de los\nobjetivos de este ejercicio estad\\'istico de car\\'acter informativo.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 05:41:49 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 05:58:53 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Erdely", "Arturo", ""]]}, {"id": "1805.09038", "submitter": "Joseph Mur\\'e", "authors": "Joseph Mur\\'e", "title": "Trans-Gaussian Kriging in a Bayesian framework : a case study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Gaussian Process Regression or Kriging, we propose a\nfull-Bayesian solution to deal with hyperparameters of the covariance function.\nThis solution can be extended to the Trans-Gaussian Kriging framework, which\nmakes it possible to deal with spatial data sets that violate assumptions\nrequired for Kriging. It is shown to be both elegant and efficient. We propose\nan application to computer experiments in the field of nuclear safety, where it\nis necessary to model non-destructive testing procedures based on eddy currents\nto detect possible wear in steam generator tubes.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 10:12:24 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mur\u00e9", "Joseph", ""]]}, {"id": "1805.09091", "submitter": "Sebastian Lerch", "authors": "Stephan Rasp and Sebastian Lerch", "title": "Neural networks for post-processing ensemble weather forecasts", "comments": null, "journal-ref": "Monthly Weather Review 2018, 146, 3885-3900", "doi": "10.1175/MWR-D-18-0187.1", "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 12:30:28 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Rasp", "Stephan", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1805.09153", "submitter": "Jinghui Yuan", "authors": "Jinghui Yuan, Mohamed Abdel-Aty", "title": "Approach-Level Real-Time Crash Risk Analysis for Signalized\n  Intersections", "comments": "Yuan, J., and M. Abdel-Aty. Approach-Level Real-Time Crash Risk\n  Analysis for Signalized Intersections. Accident Analysis & Prevention, Vol.\n  119, 2018, pp. 274-289. arXiv admin note: text overlap with arXiv:1805.07827", "journal-ref": "https://doi.org/10.1016/j.aap.2018.07.031", "doi": "10.1016/j.aap.2018.07.031", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study attempts to investigate the relationship between crash occurrence\nat signalized intersections and real-time traffic, signal timing, and weather\ncharacteristics based on 23 signalized intersections in Central Florida. The\nintersection and intersection-related crashes were collected and then divided\ninto two types, i.e., within intersection crashes and intersection entrance\ncrashes. Bayesian conditional logistic models were developed for these two\nkinds of crashes, respectively. For the within intersection models, the model\nresults showed that the through volume from \"A\" approach (the traveling\napproach of at-fault vehicle), the left turn volume from \"B\" approach\n(near-side crossing approach), and the overall average flow ratio (OAFR) from\n\"D\" approach (far-side crossing approach), were found to have significant\npositive effects on the odds of crash occurrence. Moreover, the increased\nadaptability for the left turn signal timing of \"B\" approach and more priority\nfor \"A\" approach could significantly decrease the odds of crash occurrence. For\nthe intersection entrance models, average speed was found to have significant\nnegative effect on the odds of crash occurrence. The longer average green time\nand longer average waiting time for the left turn phase, higher green ratio for\nthe through phase, and higher adaptability for the through phase can\nsignificantly improve the safety performance of intersection entrance area. In\naddition, the average queue length on the through lanes was found to have\npositive effect on the odds of crash occurrence. These results are important in\nreal-time safety applications at signalized intersections in the context of\nproactive traffic management.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:58:44 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 19:01:37 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Yuan", "Jinghui", ""], ["Abdel-Aty", "Mohamed", ""]]}, {"id": "1805.09216", "submitter": "Nicole Augustin H", "authors": "Nadine Eickenscheidt, Nicole H. Augustin and Nicole Wellbrock", "title": "Spatio-temporal modelling of forest monitoring data: Modelling German\n  tree defoliation data collected between 1989 and 2015 for trend estimation\n  and survey grid examination using GAMMs", "comments": null, "journal-ref": "iForest - Biogeosciences and Forestry, Volume 12, Issue 4, Pages\n  338-348 (2019)", "doi": "10.3832/ifor2932-012", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatio-temporal modelling of tree defoliation data of German forest condition\nsurvey is presented. In the present study generalized additive mixed models\nwere used to estimate the spatio-temporal trends of defoliation of the main\ntree species from 1989 to 2015 and to examine the suitability of different\nmonitoring grid resolutions. Although data has been collected since 1989, this\nis the first time the spatio-temporal modelling for entire Germany has been\ncarried out. Besides the space-time component, stand age showed a significant\neffect on defoliation. The mean age and the species-specific relation between\ndefoliation and age determined the general level of defoliation whereas\nfluctuations of defoliation were primarily related to weather conditions. The\nstudy indicates a strong association between drought stress and defoliation of\nall four main tree species. Besides direct effects of weather conditions,\nindirect effects seem to play a further role. Defoliation of the comparably\ndrought-tolerant species pine and oak was primarily affected by insect\ninfestations following drought whereas considerable time for regeneration was\nrequired by beech following drought stress and recurring substantial\nfructification. South-eastern Germany has emerged as the region with the\nhighest defoliation since the drought year 2003. This region was characterized\nby the strongest water deficits in 2003 compared to the long-term reference\nperiod. The present study gives evidence that the focus has moved from air\npollution to climate change. Furthermore, the spatio-temporal model was used to\ncarry out a simulation study to compare different survey grid resolutions. This\ngrid examination indicated that an 8 x 8 km grid instead of the standard 16 x\n16 km grid is necessary for spatio-temporal trend estimation and for detecting\nhot-spots in defoliation in space and time, especially regarding oak.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 15:17:56 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Eickenscheidt", "Nadine", ""], ["Augustin", "Nicole H.", ""], ["Wellbrock", "Nicole", ""]]}, {"id": "1805.09397", "submitter": "Sukjin Han", "authors": "Sukjin Han", "title": "Identification in Nonparametric Models for Dynamic Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a nonparametric model that represents how sequences of\noutcomes and treatment choices influence one another in a dynamic manner. In\nthis setting, we are interested in identifying the average outcome for\nindividuals in each period, had a particular treatment sequence been assigned.\nThe identification of this quantity allows us to identify the average treatment\neffects (ATE's) and the ATE's on transitions, as well as the optimal treatment\nregimes, namely, the regimes that maximize the (weighted) sum of the average\npotential outcomes, possibly less the cost of the treatments. The main\ncontribution of this paper is to relax the sequential randomization assumption\nwidely used in the biostatistics literature by introducing a flexible\nchoice-theoretic framework for a sequence of endogenous treatments. We show\nthat the parameters of interest are identified under each period's two-way\nexclusion restriction, i.e., with instruments excluded from the\noutcome-determining process and other exogenous variables excluded from the\ntreatment-selection process. We also consider partial identification in the\ncase where the latter variables are not available. Lastly, we extend our\nresults to a setting where treatments do not appear in every period.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 19:37:47 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 15:12:50 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 21:48:34 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Han", "Sukjin", ""]]}, {"id": "1805.09410", "submitter": "Fei Li", "authors": "Fei Li, Jukka-Pekka Onnela, and Victor DeGruttola", "title": "Bayesian method for inferring the impact of geographical distance on\n  intensity of communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both theoretical models and empirical findings suggest that the intensity of\ncommunication among groups of people declines with their degree of geographical\nseparation. There is some evidence that rather than decaying uniformly with\ndistance, the intensity of communication might decline at different rates for\nshorter and longer distances. Using Bayesian LASSO for model selection, we\nintroduce a statistical model for estimating the rate of communication decline\nwith geographic distance that allows for discontinuities in this rate. We apply\nour method to an anonymized mobile phone communication dataset. Our results are\npotentially useful in settings where understanding social and spatial mixing of\npeople is important, such as in cluster randomized trials design.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 20:08:16 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Li", "Fei", ""], ["Onnela", "Jukka-Pekka", ""], ["DeGruttola", "Victor", ""]]}, {"id": "1805.09505", "submitter": "Ranjan Maitra", "authors": "Israel Almod\\'ovar-Rivera and Ranjan Maitra", "title": "Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering", "comments": "32 pages, 22 figures, 9 tables: published in JMLR at:\n  http://jmlr.org/papers/v21/18-435.html", "journal-ref": "Journal of Machine Learning Research 21:122, 1-54 (2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly-used clustering algorithms usually find ellipsoidal, spherical or\nother regular-structured clusters, but are more challenged when the underlying\ngroups lack formal structure or definition. Syncytial clustering is the name\nthat we introduce for methods that merge groups obtained from standard\nclustering algorithms in order to reveal complex group structure in the data.\nHere, we develop a distribution-free fully-automated syncytial clustering\nalgorithm that can be used with $k$-means and other algorithms. Our approach\nestimates the cumulative distribution function of the normed residuals from an\nappropriately fit $k$-groups model and calculates the estimated nonparametric\noverlap between each pair of clusters. Groups with high pairwise overlap are\nmerged as long as the estimated generalized overlap decreases. Our methodology\nis always a top performer in identifying groups with regular and irregular\nstructures in several datasets and can be applied to datasets with scatter or\nincomplete records. The approach is also used to identify the distinct kinds of\ngamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and\nthe distinct kinds of activation in a functional Magnetic Resonance Imaging\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:50:10 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 01:40:05 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 02:38:06 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 17:10:38 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 18:09:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Almod\u00f3var-Rivera", "Israel", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1805.09570", "submitter": "Rafael Lima Goncalves de", "authors": "Rafael Lima and Jaesik Choi", "title": "Hawkes Process Kernel Structure Parametric Search with Renormalization\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hawkes Processes are a type of point process for modeling self-excitation,\ni.e., when the occurrence of an event makes future events more likely to occur.\nThe corresponding self-triggering function of this type of process may be\ninferred through an Unconstrained Optimization-based method for maximization of\nits corresponding Loglikelihood function. Unfortunately, the non-convexity of\nthis procedure, along with the ill-conditioning of the initialization of the\nself- triggering function parameters, may lead to a consequent instability of\nthis method. Here, we introduce Renormalization Factors, over four types of\nparametric kernels, as a solution to this instability. These factors are\nderived for each of the self-triggering function parameters, and also for more\nthan one parameter considered jointly. Experimental results show that the\nMaximum Likelihood Estimation method shows improved performance with\nRenormalization Factors over sets of sequences of several different lengths.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:37:01 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 05:03:33 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 04:55:41 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Lima", "Rafael", ""], ["Choi", "Jaesik", ""]]}, {"id": "1805.09674", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A D-vine copula mixed model for joint meta-analysis and comparison of\n  diagnostic tests", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.03920,\n  arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2019, 28 (10-11),\n  3286-3300", "doi": "10.1177/0962280218796685", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a particular disease there may be two diagnostic tests developed, where\neach of the tests is subject to several studies. A quadrivariate generalized\nlinear mixed model (GLMM) has been recently proposed to joint meta-analyse and\ncompare two diagnostic tests. We propose a D-vine copula mixed model for joint\nmeta-analysis and comparison of two diagnostic tests. Our general model\nincludes the quadrivariate GLMM as a special case and can also operate on the\noriginal scale of sensitivities and specificities. The method allows the direct\ncalculation of sensitivity and specificity for each test, as well as, the\nparameters of the summary receiver operator characteristic (SROC) curve, along\nwith a comparison between the SROCs of each test. Our methodology is\ndemonstrated with an extensive simulation study and illustrated by\nmeta-analysing two examples where 2 tests for the diagnosis of a particular\ndisease are compared. Our study suggests that there can be an improvement on\nGLMM in fit to data since our model can also provide tail dependencies and\nasymmetries.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:49:41 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 17:15:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1805.09799", "submitter": "Nicha Dvornek", "authors": "Nicha C. Dvornek, Daniel Yang, Archana Venkataraman, Pamela Ventola,\n  Lawrence H. Staib, Kevin A. Pelphrey, and James S. Duncan", "title": "Prediction of Autism Treatment Response from Baseline fMRI using Random\n  Forests and Tree Bagging", "comments": "Multimodal Learning for Clinical Decision Support (ML-CDS) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Treating children with autism spectrum disorders (ASD) with behavioral\ninterventions, such as Pivotal Response Treatment (PRT), has shown promise in\nrecent studies. However, deciding which therapy is best for a given patient is\nlargely by trial and error, and choosing an ineffective intervention results in\nloss of valuable treatment time. We propose predicting patient response to PRT\nfrom baseline task-based fMRI by the novel application of a random forest and\ntree bagging strategy. Our proposed learning pipeline uses random forest\nregression to determine candidate brain voxels that may be informative in\npredicting treatment response. The candidate voxels are then tested stepwise\nfor inclusion in a bagged tree ensemble. After the predictive model is\nconstructed, bias correction is performed to further increase prediction\naccuracy. Using data from 19 ASD children who underwent a 16 week trial of PRT\nand a leave-one-out cross-validation framework, the presented learning pipeline\nwas tested against several standard methods and variations of the pipeline and\nresulted in the highest prediction accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 17:41:40 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Dvornek", "Nicha C.", ""], ["Yang", "Daniel", ""], ["Venkataraman", "Archana", ""], ["Ventola", "Pamela", ""], ["Staib", "Lawrence H.", ""], ["Pelphrey", "Kevin A.", ""], ["Duncan", "James S.", ""]]}, {"id": "1805.09859", "submitter": "Erica Rodrigues", "authors": "Jos\\'e Francisco Soares, Erica Castilho Rodrigues, Victor Maia Senna\n  Delgado", "title": "Measure of gap and inequalities in basic education students\n  proficiencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study uses students performance on standardized tests as evidence of the\nquality of education and introduces a methodology based on the comparison of\nperformance distributions to produce indicators for both the level achieved by\nthe students and the learning gap between social groups, two inseparable\ndimensions of quality of education. In the first case, the study compares the\ndistribution of the group observed with a reference distribution, which\nrepresents an ideal situation of where students should be. In the second, it\ncompares the performance distribution of students belonging to social groups\ndefined by socioeconomic characteristics. This article uses the\nKullback-Leibler divergence to characterize the differences between the\ndistributions. This measure takes into account types of diferences not\nconsidered by other measures and have solid conceptual justifications. The\nproposed methodology is used to describe the quality of Brazilian basic\neducation using the test results applied biannually to all Brazilian students\nof basic education.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:22:56 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 10:45:37 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Soares", "Jos\u00e9 Francisco", ""], ["Rodrigues", "Erica Castilho", ""], ["Delgado", "Victor Maia Senna", ""]]}, {"id": "1805.09937", "submitter": "Tatsushi Oka", "authors": "Dukpa Kim, Tatsushi Oka, Francisco Estrada, Pierre Perron", "title": "Inference Related to Common Breaks in a Multivariate System with Joined\n  Segmented Trends with Applications to Global and Hemispheric Temperatures", "comments": "42 pages, 8 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What transpires from recent research is that temperatures and radiative\nforcing seem to be characterized by a linear trend with two changes in the rate\nof growth. The first occurs in the early 60s and indicates a very large\nincrease in the rate of growth of both temperature and radiative forcing\nseries. This was termed as the \"onset of sustained global warming\". The second\nis related to the more recent so-called hiatus period, which suggests that\ntemperatures and total radiative forcing have increased less rapidly since the\nmid-90s compared to the larger rate of increase from 1960 to 1990. There are\ntwo issues that remain unresolved. The first is whether the breaks in the slope\nof the trend functions of temperatures and radiative forcing are common. This\nis important because common breaks coupled with the basic science of climate\nchange would strongly suggest a causal effect from anthropogenic factors to\ntemperatures. The second issue relates to establishing formally via a proper\ntesting procedure that takes into account the noise in the series, whether\nthere was indeed a `hiatus period' for temperatures since the mid 90s. This is\nimportant because such a test would counter the widely held view that the\nhiatus is the product of natural internal variability. Our paper provides tests\nrelated to both issues. The results show that the breaks in temperatures and\nradiative forcing are common and that the hiatus is characterized by a\nsignificant decrease in their rate of growth. The statistical results are of\nindependent interest and applicable more generally.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 00:13:42 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Kim", "Dukpa", ""], ["Oka", "Tatsushi", ""], ["Estrada", "Francisco", ""], ["Perron", "Pierre", ""]]}, {"id": "1805.10036", "submitter": "Reza Hajargasht", "authors": "Gholamreza Hajargasht and Tomasz Wo\\'zniak", "title": "Accurate Computation of Marginal Data Densities Using Variational Bayes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new marginal data density estimator (MDDE) that uses the\nvariational Bayes posterior density as a weighting density of the reciprocal\nimportance sampling (RIS) MDDE. This computationally convenient estimator is\nbased on variational Bayes posterior densities that are available for many\nmodels and requires simulated draws only from the posterior distribution. It\nprovides accurate estimates with a moderate number of posterior draws, has a\nfinite variance, and provides a minimum variance candidate for the class of RIS\nMDDEs. Its reciprocal is consistent, asymptotically normally distributed, and\nunbiased. These properties are obtained without truncating the weighting\ndensity, which is typical for other such estimators. Our proposed estimators\noutperform many existing MDDEs in terms of bias and numerical standard errors.\nIn particular, our RIS MDDE performs uniformly better than other estimators\nfrom this class.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:37:20 GMT"}, {"version": "v2", "created": "Sat, 9 May 2020 12:45:54 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Hajargasht", "Gholamreza", ""], ["Wo\u017aniak", "Tomasz", ""]]}, {"id": "1805.10054", "submitter": "Pierre Ablin", "authors": "Pierre Ablin, Alexandre Gramfort, Jean-Fran\\c{c}ois Cardoso and\n  Francis Bach", "title": "Stochastic algorithms with descent guarantees for ICA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent component analysis (ICA) is a widespread data exploration\ntechnique, where observed signals are modeled as linear mixtures of independent\ncomponents. From a machine learning point of view, it amounts to a matrix\nfactorization problem with a statistical independence criterion. Infomax is one\nof the most used ICA algorithms. It is based on a loss function which is a\nnon-convex log-likelihood. We develop a new majorization-minimization framework\nadapted to this loss function. We derive an online algorithm for the streaming\nsetting, and an incremental algorithm for the finite sum setting, with the\nfollowing benefits. First, unlike most algorithms found in the literature, the\nproposed methods do not rely on any critical hyper-parameter like a step size,\nnor do they require a line-search technique. Second, the algorithm for the\nfinite sum setting, although stochastic, guarantees a decrease of the loss\nfunction at each iteration. Experiments demonstrate progress on the\nstate-of-the-art for large scale datasets, without the necessity for any manual\nparameter tuning.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 09:29:33 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 09:33:30 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Ablin", "Pierre", ""], ["Gramfort", "Alexandre", ""], ["Cardoso", "Jean-Fran\u00e7ois", ""], ["Bach", "Francis", ""]]}, {"id": "1805.10097", "submitter": "Cl\\'emence Karmann Mrs", "authors": "Aur\\'elie Deveau, Anne G\\'egout-Petit, Cl\\'emence Karmann", "title": "Penalized polytomous ordinal logistic regression using cumulative\n  logits. Application to network inference of zero-inflated variables", "comments": "This article is an old version of arXiv:1907.03153", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection when the response is ordinal,\nthat is an ordered categorical variable. In particular, we are interested in\nselecting quantitative explanatory variables linked with the ordinal response\nvariable and we want to determine which predictors are relevant. In this\nframework, we choose to use the polytomous ordinal logistic regression model\nusing cumulative logits which generalizes the logistic regression. We then\nintroduce the Lasso estimation of the regression coefficients using the\nFrank-Wolfe algorithm. To deal with the choice of the penalty parameter, we use\nthe stability selection method and we develop a new method based on the\nknockoffs idea. This knockoffs method is general and suitable to any regression\nand besides, gives an order of importance of the covariates. Finally, we\nprovide some experimental results to corroborate our method. We then present an\napplication of this regression method for network inference of zero-inflated\nvariables and use it in practice on real abundance data in an agronomic\ncontext.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:04:21 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 17:43:30 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Deveau", "Aur\u00e9lie", ""], ["G\u00e9gout-Petit", "Anne", ""], ["Karmann", "Cl\u00e9mence", ""]]}, {"id": "1805.10205", "submitter": "Anthony Hu", "authors": "Anthony Hu, Seth Flaxman", "title": "Multimodal Sentiment Analysis To Explore the Structure of Emotions", "comments": "Accepted as a conference paper at KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3219853", "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach to multimodal sentiment analysis using deep\nneural networks combining visual analysis and natural language processing. Our\ngoal is different than the standard sentiment analysis goal of predicting\nwhether a sentence expresses positive or negative sentiment; instead, we aim to\ninfer the latent emotional state of the user. Thus, we focus on predicting the\nemotion word tags attached by users to their Tumblr posts, treating these as\n\"self-reported emotions.\" We demonstrate that our multimodal model combining\nboth text and image features outperforms separate models based solely on either\nimages or text. Our model's results are interpretable, automatically yielding\nsensible word lists associated with emotions. We explore the structure of\nemotions implied by our model and compare it to what has been posited in the\npsychology literature, and validate our model on a set of images that have been\nused in psychology studies. Finally, our work also provides a useful tool for\nthe growing academic study of images - both photographs and memes - on social\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:40:11 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Hu", "Anthony", ""], ["Flaxman", "Seth", ""]]}, {"id": "1805.10214", "submitter": "Maxime Rischard", "authors": "Maxime Rischard, Natesh Pillai, Karen A. McKinnon", "title": "Bias correction in daily maximum and minimum temperature measurements\n  through Gaussian process modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Global Historical Climatology Network-Daily database contains, among\nother variables, daily maximum and minimum temperatures from weather stations\naround the globe. It is long known that climatological summary statistics based\non daily temperature minima and maxima will not be accurate, if the bias due to\nthe time at which the observations were collected is not accounted for. Despite\nsome previous work, to our knowledge, there does not exist a satisfactory\nsolution to this important problem. In this paper, we carefully detail the\nproblem and develop a novel approach to address it. Our idea is to impute the\nhourly temperatures at the location of the measurements by borrowing\ninformation from the nearby stations that record hourly temperatures, which\nthen can be used to create accurate summaries of temperature extremes. The key\ndifficulty is that these imputations of the temperature curves must satisfy the\nconstraint of falling between the observed daily minima and maxima, and\nattaining those values at least once in a twenty-four hour period. We develop a\nspatiotemporal Gaussian process model for imputing the hourly measurements from\nthe nearby stations, and then develop a novel and easy to implement Markov\nChain Monte Carlo technique to sample from the posterior distribution\nsatisfying the above constraints. We validate our imputation model using hourly\ntemperature data from four meteorological stations in Iowa, of which one is\nhidden and the data replaced with daily minima and maxima, and show that the\nimputed temperatures recover the hidden temperatures well. We also demonstrate\nthat our model can exploit information contained in the data to infer the time\nof daily measurements.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:52:12 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 16:13:53 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Rischard", "Maxime", ""], ["Pillai", "Natesh", ""], ["McKinnon", "Karen A.", ""]]}, {"id": "1805.10244", "submitter": "Nicolas Gu\\'enon Des Mesnards", "authors": "Nicolas Guenon des Mesnards and Tauhid Zaman", "title": "Detecting Influence Campaigns in Social Networks Using the Ising Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying coordinated influence campaigns\nconducted by automated agents or bots in a social network. We study several\ndifferent Twitter datasets which contain such campaigns and find that the bots\nexhibit heterophily - they interact more with humans than with each other. We\nuse this observation to develop a probability model for the network structure\nand bot labels based on the Ising model from statistical physics. We present a\nmethod to find the maximum likelihood assignment of bot labels by solving a\nminimum cut problem. Our algorithm allows for the simultaneous detection of\nmultiple bots that are potentially engaging in a coordinated influence\ncampaign, in contrast to other methods that identify bots one at a time. We\nfind that our algorithm is able to more accurately find bots than existing\nmethods when compared to a human labeled ground truth. We also look at the\ncontent posted by the bots we identify and find that they seem to have a\ncoordinated agenda.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 16:49:56 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Mesnards", "Nicolas Guenon des", ""], ["Zaman", "Tauhid", ""]]}, {"id": "1805.10933", "submitter": "Jacopo Diquigiovanni", "authors": "Jacopo Diquigiovanni and Bruno Scarpa", "title": "Analysis of association football playing styles: an innovative method to\n  cluster networks", "comments": null, "journal-ref": null, "doi": "10.1177/1471082X18808628", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop an innovative hierarchical clustering method to\ndivide a sample of undirected weighted networks into groups. The methodology\nconsists of two phases: the first phase is aimed at putting the single networks\nin a broader framework by including the characteristics of the population in\nthe data, while the second phase creates a subdivision of the sample on the\nbasis of the similarity between the community structures of the processed\nnetworks. Starting from the representation of the team's playing style as a\nnetwork, we apply the method to group the Italian Serie A teams' performances\nand consequently detect the main 15 tactics shown during the 2015-2016 season.\nThe information obtained is used to verify the effect of the styles of play on\nthe number of goals scored, and we prove the key role of one of them by\nimplementing an extension of the Dixon and Coles model (Dixon and Coles, 1997).\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 14:18:14 GMT"}, {"version": "v2", "created": "Sun, 20 Jan 2019 10:55:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Diquigiovanni", "Jacopo", ""], ["Scarpa", "Bruno", ""]]}, {"id": "1805.11126", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa, Xijia Liu, Anders Garpebring, and Jun Yu", "title": "Statistical Methods in Computed Tomography Image Estimation", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13204", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: There is increasing interest in computed tomography (CT) image\nestimations from magnetic resonance (MR) images. The estimated CT images can be\nutilised for attenuation correction, patient positioning, and dose planning in\ndiagnostic and radiotherapy workflows. This study aims to introduce a novel\nstatistical learning approach for improving CT estimation from MR images and to\ncompare the performance of our method with the existing model based CT image\nestimation methods.\n  Methods: The statistical learning approach proposed here consists of two\nstages. At the training stage, prior knowledges about tissue-types from CT\nimages were used together with a Gaussian mixture model (GMM) to explore CT\nimage estimations from MR images. Since the prior knowledges are not available\nat the prediction stage, a classifier based on RUSBoost algorithm was trained\nto estimate the tissue-types from MR images. For a new patient, the trained\nclassifier and GMMs were used to predict CT image from MR images. The\nclassifier and GMMs were validated by using voxel level 10-fold\ncross-validation and patient-level leave-one-out cross-validation,\nrespectively.\n  Results: The proposed approach has outperformance in CT estimation quality in\ncomparison with the existing model based methods, especially on bone tissues.\nOur method improved CT image estimation by 5% and 23% on the whole brain and\nbone tissues, respectively.\n  Conclusions: Evaluation of our method shows that it is a promising method to\ngenerate CT image substitutes for the implementation of fully MR-based\nradiotherapy and PET/MRI applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 18:42:43 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:38:56 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["Liu", "Xijia", ""], ["Garpebring", "Anders", ""], ["Yu", "Jun", ""]]}, {"id": "1805.11456", "submitter": "James Tucker", "authors": "J. Derek Tucker, John Lewis, and Anuj Srivastava", "title": "Elastic Functional Principal Component Regression", "comments": null, "journal-ref": null, "doi": "10.1002/sam.11399", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regression using functional predictors in situations where these\nfunctions contain both phase and amplitude variability. In other words, the\nfunctions are misaligned due to errors in time measurements, and these errors\ncan significantly degrade both model estimation and prediction performance. The\ncurrent techniques either ignore the phase variability, or handle it via\npre-processing, i.e., use an off-the-shelf technique for functional alignment\nand phase removal. We develop a functional principal component regression model\nwhich has comprehensive approach in handling phase and amplitude variability.\nThe model utilizes a mathematical representation of the data known as the\nsquare-root slope function. These functions preserve the $\\mathbf{L}^2$ norm\nunder warping and are ideally suited for simultaneous estimation of regression\nand warping parameters. Using both simulated and real-world data sets, we\ndemonstrate our approach and evaluate its prediction performance relative to\ncurrent models. In addition, we propose an extension to functional logistic and\nmultinomial logistic regression\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 13:40:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Tucker", "J. Derek", ""], ["Lewis", "John", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1805.11524", "submitter": "Prashanth R", "authors": "R. Prashanth, Sumantra Dutta Roy", "title": "Novel and Improved Stage Estimation in Parkinson's Disease using\n  Clinical Scales and Machine Learning", "comments": "Article accepted in the Neurocomputing journal", "journal-ref": "R Prashanth, S. Dutta Roy, \"Novel and improved stage estimation in\n  Parkinson's disease using clinical scales and machine learning\",\n  Neurocomputing, 305, pp. 78-103, 2018", "doi": "10.1016/j.neucom.2018.04.049", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stage and severity of Parkinson's disease (PD) is an important factor to\nconsider for taking effective therapeutic decisions. Although the Movement\nDisorder Society-Unified Parkinson's Disease Rating Scale (MDS-UPDRS) provides\nan effective instrument evaluating the most pertinent features of PD, it does\nnot allow PD staging. On the other hand, the Hoehn and Yahr (HY) scale which\nprovides staging, does not evaluate many relevant features of PD. In this\npaper, we propose a novel and improved staging for PD using the MDS-UPDRS\nfeatures and the HY scale, and developing prediction models to estimate the\nstage (normal, early or moderate) and severity of PD using machine learning\ntechniques such as ordinal logistic regression (OLR), support vector machine\n(SVM), AdaBoost- and RUSBoost-based classifiers. Along with this, feature\nimportance in PD is also estimated using Random forests. We observe that the\npredictive models of SVM, Adaboost-based ensemble, Random forests and\nprobabilistic generative model performed well with the AdaBoost-based ensemble\ngiving the highest accuracy of 97.46%. Body bradykinesia, tremor, facial\nexpression (hypomimia), constancy of rest tremor and handwriting (micrographia)\nwere observed to be the most important features in PD. It is inferred that\nMDS-UPDRS combined with classifiers can form effective tools to predict PD\nstaging which can aid clinicians in the diagnostic process.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:56:11 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Prashanth", "R.", ""], ["Roy", "Sumantra Dutta", ""]]}, {"id": "1805.11538", "submitter": "Felipe Montes", "authors": "Felipe Montes, Roberto C. Jimenez and Jukka-Pekka Onnela", "title": "Connected but Segregated: Social Networks in Rural Villages", "comments": "11 pages, 4 figures", "journal-ref": "Journal of Complex Networks, November 2017", "doi": "10.1093/comnet/cnx054", "report-no": null, "categories": "stat.AP cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increased appreciation for, and utilization of, social networks\nto disseminate various kinds of interventions in a target population.\nHomophily, the tendency of people to be similar to those they interact with,\ncan create within-group cohesion but at the same time can also lead to societal\nsegregation. In public health, social segregation can form barriers to the\nspread of health interventions from one group to another. We analyzed the\nstructure of social networks in 75 villages in Karnataka, India, both at the\nlevel of individuals and network communities. We found all villages to be\nstrongly segregated at the community level, especially along the lines of caste\nand sex, whereas other socioeconomic variables, such as age and education, were\nonly weakly associated with these groups in the network. While the studied\nnetworks are densely connected, our results indicate that the villages are\nhighly segregated.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 15:16:59 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Montes", "Felipe", ""], ["Jimenez", "Roberto C.", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1805.11557", "submitter": "Daniel Rigobon", "authors": "Daniel E Rigobon, Eaman Jahani, Yoshihiko Suhara, Khaled AlGhoneim,\n  Abdulaziz Alghunaim, Alex Pentland, Abdullah Almaatouq", "title": "Winning Models for GPA, Grit, and Layoff in the Fragile Families\n  Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss and analyze our approach to the Fragile Families\nChallenge. The challenge involved predicting six outcomes for 4,242 children\nfrom disadvantaged families from around the United States. The data consisted\nof over 12,000 features (covariates) about the children and their parents,\nschools, and overall environments from birth to age 9. Our approach relied\nprimarily on existing data science techniques, including: (1) data\npreprocessing: elimination of low variance features, imputation of missing\ndata, and construction of composite features; (2) feature selection through\nunivariate Mutual Information and extraction of non-zero LASSO coefficients;\n(3) three machine learning models: Random Forest, Elastic Net, and\nGradient-Boosted Trees; and finally (4) prediction aggregation according to\nperformance. The top-performing submissions produced winning out-of-sample\npredictions for three outcomes: GPA, grit, and layoff. However, predictions\nwere at most 20% better than a baseline that predicted the mean value of the\ntraining data of each outcome.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 16:05:29 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 14:03:35 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2018 19:23:12 GMT"}, {"version": "v4", "created": "Wed, 14 Nov 2018 01:28:06 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Rigobon", "Daniel E", ""], ["Jahani", "Eaman", ""], ["Suhara", "Yoshihiko", ""], ["AlGhoneim", "Khaled", ""], ["Alghunaim", "Abdulaziz", ""], ["Pentland", "Alex", ""], ["Almaatouq", "Abdullah", ""]]}, {"id": "1805.11636", "submitter": "Samuel Berchuck", "authors": "Samuel I. Berchuck, Jean-Claude Mwanza and Joshua L. Warren", "title": "Diagnosing Glaucoma Progression with Visual Field Data Using a\n  Spatiotemporal Boundary Detection Method", "comments": "This is a preprint of an article submitted for publication in the\n  Journal of the American Statistical Association\n  (https://www.tandfonline.com/toc/uasa20/current). The article contains 35\n  pages, 4 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diagnosing glaucoma progression is critical for limiting irreversible vision\nloss. A common method for assessing glaucoma progression uses a longitudinal\nseries of visual fields (VF) acquired at regular intervals. VF data are\ncharacterized by a complex spatiotemporal structure due to the data generating\nprocess and ocular anatomy. Thus, advanced statistical methods are needed to\nmake clinical determinations regarding progression status. We introduce a\nspatiotemporal boundary detection model that allows the underlying anatomy of\nthe optic disc to dictate the spatial structure of the VF data across time. We\nshow that our new method provides novel insight into vision loss that improves\ndiagnosis of glaucoma progression using data from the Vein Pulsation Study\nTrial in Glaucoma and the Lions Eye Institute trial registry. Simulations are\npresented, showing the proposed methodology is preferred over existing spatial\nmethods for VF data. Supplementary materials for this article are available\nonline and the method is implemented in the R package womblR.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 18:22:03 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Berchuck", "Samuel I.", ""], ["Mwanza", "Jean-Claude", ""], ["Warren", "Joshua L.", ""]]}, {"id": "1805.11804", "submitter": "Vilislav Boutchaktchiev", "authors": "Vilislav Boutchaktchiev", "title": "A Markov Chain Model for the Cure Rate of Non-Performing Loans", "comments": null, "journal-ref": null, "doi": "10.2139/ssrn.3175475", "report-no": null, "categories": "q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Markov-chain model is developed for the purpose estimation of the cure rate\nof non-performing loans. The technique is performed collectively, on portfolios\nand it can be applicable in the process of calculation of credit impairment. It\nis efficient in terms of data manipulation costs which makes it accessible even\nto smaller financial institutions. In addition, several other applications to\nportfolio optimization are suggested.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 04:50:08 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 09:41:09 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Boutchaktchiev", "Vilislav", ""]]}, {"id": "1805.11956", "submitter": "Kunjin Chen", "authors": "Kunjin Chen, Kunlong Chen, Qin Wang, Ziyu He, Jun Hu, Jinliang He", "title": "Short-term Load Forecasting with Deep Residual Networks", "comments": "This paper is currently accepted by IEEE Transactions on Smart Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a model for forecasting short-term power loads based\non deep residual networks. The proposed model is able to integrate domain\nknowledge and researchers' understanding of the task by virtue of different\nneural network building blocks. Specifically, a modified deep residual network\nis formulated to improve the forecast results. Further, a two-stage ensemble\nstrategy is used to enhance the generalization capability of the proposed\nmodel. We also apply the proposed model to probabilistic load forecasting using\nMonte Carlo dropout. Three public datasets are used to prove the effectiveness\nof the proposed model. Multiple test cases and comparison with existing models\nshow that the proposed model is able to provide accurate load forecasting\nresults and has high generalization capability.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 13:45:12 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Chen", "Kunjin", ""], ["Chen", "Kunlong", ""], ["Wang", "Qin", ""], ["He", "Ziyu", ""], ["Hu", "Jun", ""], ["He", "Jinliang", ""]]}, {"id": "1805.11999", "submitter": "Raj Thilak Rajan", "authors": "Raj Thilak Rajan, Rob-van Schaijk, Anup Das, Jac Romme and Frank\n  Pasveer", "title": "Reference-free Calibration in Sensor Networks", "comments": "Submitted to IEEE Sensor Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensor calibration is one of the fundamental challenges in large-scale IoT\nnetworks. In this article, we address the challenge of reference-free\ncalibration of a densely deployed sensor network. Conventionally, to calibrate\nan in-place sensor network (or sensor array), a reference is arbitrarily chosen\nwith or without prior information on sensor performance. However, an arbitrary\nselection of a reference could prove fatal, if an erroneous sensor is\ninadvertently chosen. To avert single point of dependence, and to improve\nestimator performance, we propose unbiased reference-free algorithms. Although,\nour focus is on reference-free solutions, the proposed framework, allows the\nincorporation of additional references, if available. We show with the help of\nsimulations that the proposed solutions achieve the derived statistical lower\nbounds asymptotically. In addition, the proposed algorithms show improvements\non real-life datasets, as compared to prevalent algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 14:26:41 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Rajan", "Raj Thilak", ""], ["Schaijk", "Rob-van", ""], ["Das", "Anup", ""], ["Romme", "Jac", ""], ["Pasveer", "Frank", ""]]}, {"id": "1805.12071", "submitter": "Samuel St-Jean", "authors": "Samuel St-Jean, Alberto De Luca, Max A. Viergever, Alexander Leemans", "title": "Automatic, fast and robust characterization of noise distributions for\n  diffusion MRI", "comments": "v2: added publisher DOI statement, fixed text typo in appendix A2", "journal-ref": "St-Jean S. et al. (2018) Automatic, Fast and Robust\n  Characterization of Noise Distributions for Diffusion MRI. In: Medical Image\n  Computing and Computer Assisted Intervention - MICCAI 2018. LNCS, vol 11070.\n  Springer, Cham", "doi": "10.1007/978-3-030-00928-1_35", "report-no": null, "categories": "cs.CV stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge of the noise distribution in magnitude diffusion MRI images is the\ncenterpiece to quantify uncertainties arising from the acquisition process. The\nuse of parallel imaging methods, the number of receiver coils and imaging\nfilters applied by the scanner, amongst other factors, dictate the resulting\nsignal distribution. Accurate estimation beyond textbook Rician or noncentral\nchi distributions often requires information about the acquisition process\n(e.g. coils sensitivity maps or reconstruction coefficients), which is not\nusually available. We introduce a new method where a change of variable\nnaturally gives rise to a particular form of the gamma distribution for\nbackground signals. The first moments and maximum likelihood estimators of this\ngamma distribution explicitly depend on the number of coils, making it possible\nto estimate all unknown parameters using only the magnitude data. A rejection\nstep is used to make the method automatic and robust to artifacts. Experiments\non synthetic datasets show that the proposed method can reliably estimate both\nthe degrees of freedom and the standard deviation. The worst case errors range\nfrom below 2% (spatially uniform noise) to approximately 10% (spatially\nvariable noise). Repeated acquisitions of in vivo datasets show that the\nestimated parameters are stable and have lower variances than compared methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 16:38:25 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 10:10:48 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["St-Jean", "Samuel", ""], ["De Luca", "Alberto", ""], ["Viergever", "Max A.", ""], ["Leemans", "Alexander", ""]]}, {"id": "1805.12217", "submitter": "Gregor Kastner", "authors": "Florian Huber, Gregor Kastner, Michael Pfarrhofer", "title": "Introducing shrinkage in heavy-tailed state space models to predict\n  equity excess returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We forecast S&P 500 excess returns using a flexible Bayesian econometric\nstate space model with non-Gaussian features at several levels. More precisely,\nwe control for overparameterization via novel global-local shrinkage priors on\nthe state innovation variances as well as the time-invariant part of the state\nspace model. The shrinkage priors are complemented by heavy tailed state\ninnovations that cater for potential large breaks in the latent states.\nMoreover, we allow for leptokurtic stochastic volatility in the observation\nequation. The empirical findings indicate that several variants of the proposed\napproach outperform typical competitors frequently used in the literature, both\nin terms of point and density forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 20:30:12 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 22:54:36 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Huber", "Florian", ""], ["Kastner", "Gregor", ""], ["Pfarrhofer", "Michael", ""]]}, {"id": "1805.12257", "submitter": "Angelos Alexopoulos", "authors": "Angelos Alexopoulos, Petros Dellaportas and Jonathan J. Forster", "title": "Bayesian forecasting of mortality rates using latent Gaussian models", "comments": "26 pages, 8 Figures", "journal-ref": "Journal of the Royal Statistical Society: Series A (Statistics in\n  Society) 182 (2019) 689-711", "doi": "10.1111/rssa.12422", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide forecasts for mortality rates by using two different approaches.\nFirst we employ dynamic non-linear logistic models based on Heligman-Pollard\nformula. Second, we assume that the dynamics of the mortality rates can be\nmodelled through a Gaussian Markov random field. We use efficient Bayesian\nmethods to estimate the parameters and the latent states of the proposed\nmodels. Both methodologies are tested with past data and are used to forecast\nmortality rates both for large (UK and Wales) and small (New Zealand)\npopulations up to 21 years ahead. We demonstrate that predictions for\nindividual survivor functions and other posterior summaries of demographic and\nactuarial interest are readily obtained. Our results are compared with other\ncompeting forecasting methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 23:10:15 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Dellaportas", "Petros", ""], ["Forster", "Jonathan J.", ""]]}, {"id": "1805.12572", "submitter": "Gregory S. Warrington", "authors": "Gregory S. Warrington", "title": "A comparison of partisan-gerrymandering measures", "comments": "21 pages + 12-page appendix, 5 figures, 4 tables. Changes from v2:\n  Some material removed and some clarifying remarks added", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare and contrast fourteen measures that have been proposed for the\npurpose of quantifying partisan gerrymandering. We consider measures that,\nrather than examining the shapes of districts, utilize only the partisan vote\ndistribution among districts. The measures considered are two versions of\npartisan bias; the efficiency gap and several of its variants; the mean-median\ndifference and the equal vote weight standard; the declination and one variant;\nand the lopsided-means test. Our primary means of evaluating these measures is\na suite of hypothetical elections we classify from the start as fair or unfair.\nWe conclude that the declination is the most successful measure in terms of\navoiding false positives and false negatives on the elections considered. We\ninclude in an appendix the most extreme outliers for each measure among\nhistorical congressional and state legislative elections.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 17:28:41 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 14:48:52 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 17:29:08 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Warrington", "Gregory S.", ""]]}]