[{"id": "1409.0503", "submitter": "Lisa Pham", "authors": "Lisa M. Pham, Luis Carvalho, Scott Schaus, Eric D. Kolaczyk", "title": "Perturbation Detection Through Modeling of Gene Expression on a Latent\n  Biological Pathway Network: A Bayesian hierarchical approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cellular response to a perturbation is the result of a dynamic system of\nbiological variables linked in a complex network. A major challenge in drug and\ndisease studies is identifying the key factors of a biological network that are\nessential in determining the cell's fate.\n  Here our goal is the identification of perturbed pathways from\nhigh-throughput gene expression data. We develop a three-level hierarchical\nmodel, where (i) the first level captures the relationship between gene\nexpression and biological pathways using confirmatory factor analysis, (ii) the\nsecond level models the behavior within an underlying network of pathways\ninduced by an unknown perturbation using a conditional autoregressive model,\nand (iii) the third level is a spike-and-slab prior on the perturbations. We\nthen identify perturbations through posterior-based variable selection.\n  We illustrate our approach using gene transcription drug perturbation\nprofiles from the DREAM7 drug sensitivity predication challenge data set. Our\nproposed method identified regulatory pathways that are known to play a\ncausative role and that were not readily resolved using gene set enrichment\nanalysis or exploratory factor models. Simulation results are presented\nassessing the performance of this model relative to a network-free variant and\nits robustness to inaccuracies in biological databases.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 18:55:02 GMT"}], "update_date": "2014-09-02", "authors_parsed": [["Pham", "Lisa M.", ""], ["Carvalho", "Luis", ""], ["Schaus", "Scott", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1409.0743", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Does non-stationary spatial data always require non-stationary random\n  fields?", "comments": "Minor change from previous version. arXiv admin note: text overlap\n  with arXiv:1306.0408", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stationary spatial model is an idealization and we expect that the true\ndependence structures of physical phenomena are spatially varying, but how\nshould we handle this non-stationarity in practice? We study the challenges\ninvolved in applying a flexible non-stationary model to a dataset of annual\nprecipitation in the conterminous US, where exploratory data analysis shows\nstrong evidence of a non-stationary covariance structure.\n  The aim of this paper is to investigate the modelling pipeline once\nnon-stationarity has been detected in spatial data. We show that there is a\nreal danger of over-fitting the model and that careful modelling is necessary\nin order to properly account for varying second-order structure. In fact, the\nexample shows that sometimes non-stationary Gaussian random fields are not\nnecessary to model non-stationary spatial data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:05:14 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 15:09:44 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 09:34:07 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 11:55:14 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1409.0849", "submitter": "Deukwoo Kwon", "authors": "Deukwoo Kwon, F. Owen Hoffman, Brian E. Moroz, Steven L. Simon", "title": "Bayesian dose-response analysis for epidemiological studies with complex\n  uncertainty in dose estimation", "comments": null, "journal-ref": "Stat Med. 2016 10;35(3):399-423", "doi": "10.1002/sim.6635", "report-no": null, "categories": "stat.AP", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Most conventional risk analysis methods rely on a single best estimate of\nexposure per person which does not allow for adjustment for exposure-related\nuncertainty. Here, we propose a Bayesian model averaging method to properly\nquantify the relationship between radiation dose and disease outcomes by\naccounting for shared and unshared uncertainty in estimated dose. Our Bayesian\nrisk analysis method utilizes multiple realizations of sets (vectors) of doses\ngenerated by a two-dimensional Monte Carlo simulation method that properly\nseparates shared and unshared errors in dose estimation. The exposure model\nused in this work is taken from a study of the risk of thyroid nodules among a\ncohort of 2,376 subjects following exposure to fallout resulting from nuclear\ntesting in Kazakhstan. We assessed the performance of our method through an\nextensive series of simulation tests and comparisons against conventional\nregression risk analysis methods. We conclude that when estimated doses contain\nrelatively small amounts of uncertainty, the Bayesian method using multiple\nrealizations of possibly true dose vectors gave similar results to the\nconventional regression-based methods of dose-response analysis. However, when\nlarge and complex mixtures of shared and unshared uncertainties are present,\nthe Bayesian method using multiple dose vectors had significantly lower\nrelative bias than conventional regression-based risk analysis methods as well\nas a markedly increased capability to include the pre-established 'true' risk\ncoefficient within the credible interval of the Bayesian-based risk estimate.\nAn evaluation of the dose-response using our method is presented for an\nepidemiological study of thyroid disease following radiation exposure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 19:57:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kwon", "Deukwoo", ""], ["Hoffman", "F. Owen", ""], ["Moroz", "Brian E.", ""], ["Simon", "Steven L.", ""]]}, {"id": "1409.1360", "submitter": "S\\\"oren Christensen", "authors": "Bj\\\"orn Christensen, S\\\"oren Christensen, Tim Hoppe and Michael\n  Spandel", "title": "Everything counts! - Warum kleine Gemeinden die Gewinner der\n  Zensuserhebung 2011 sind", "comments": "in German", "journal-ref": null, "doi": "10.1007/s11943-015-0173-x", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population and housing census 2011 was an EU-wide census in all EU member\nstates. In Germany, the basis was a largely register-based method. In this\npaper, it is shown that communities with less than 10.000 inhabitants have\nsignificantly less relative losses in the number of inhabitants compared to\ncommunities with more than 10.000 inhabitants.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 08:42:20 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Christensen", "Bj\u00f6rn", ""], ["Christensen", "S\u00f6ren", ""], ["Hoppe", "Tim", ""], ["Spandel", "Michael", ""]]}, {"id": "1409.1446", "submitter": "Houssam Alrachid", "authors": "Houssam Alrachid, Virginie Ehrlacher, Alexis Marceau and Karim Tekkal", "title": "Statistical methods for critical scenarios in aeronautics", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present numerical results obtained on the CEMRACS project Predictive SMS\nproposed by Safety Line. The goal of this work was to elaborate a purely\nstatistical method in order to reconstruct the deceleration profile of a plane\nduring landing under normal operating conditions, from a database containing\naround $1500$ recordings. The aim of Safety Line is to use this model to detect\nmalfunctions of the braking system of the plane from deviations of the measured\ndeceleration profile of the plane to the one predicted by the model. This\nyields to a multivariate nonparametric regression problem, which we chose to\ntackle using a Bayesian approach based on the use of gaussian processes. We\nalso compare this approach with other statistical methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 14:00:34 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 10:33:54 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Alrachid", "Houssam", ""], ["Ehrlacher", "Virginie", ""], ["Marceau", "Alexis", ""], ["Tekkal", "Karim", ""]]}, {"id": "1409.1542", "submitter": "Norman Poh", "authors": "Norman Poh, Andrew McGovern and Simon de Lusignan", "title": "Towards automated identification of changes in laboratory measurement of\n  renal function: implications for longitudinal research and observing trends\n  in glomerular filtration rate (GFR)", "comments": null, "journal-ref": null, "doi": null, "report-no": "TR-14-03", "categories": "q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduction: Kidney function is reported using estimates of glomerular\nfiltration rate (eGFR). However, eGFR values are recorded without reference to\nthe creatinine (SCr) assays used to derive them, and newer assays were\nintroduced at different time points across laboratories in UK. These changes\nmay cause systematic bias in eGFR reported in routinely collected data; even\nthough laboratory reported eGFR values have a correction factor applied.\n  Design: An algorithm to detect changes in SCr which affect eGFR calculation\nmethod by comparing the mapping of SCr values on to eGFR values across a\ntime-series of paired eGFR and SCr measurements.\n  Setting: Routinely collected primary care data from 20,000 people with the\nrichest renal function data from the Quality Improvement in Chronic Kidney\nDisease (QICKD) trial.\n  Results: The algorithm identified a change in eGFR calculation method in 80\n(63%) of the 127 included practices. This change was identified in 4,736\n(23.7%) patient time series analysed. This change in calibration method was\nfound to cause a significant step change in reported eGFR values producing a\nsystematic bias. eGFR values could not be recalibrated by applying the\nModification of Diet in Renal Disease (MDRD) equation to the laboratory\nreported SCr values.\n  Conclusions: This algorithm can identify laboratory changes in eGFR\ncalculation methods and changes in SCr assay. Failure to account for these\nchanges may misconstrue renal function changes over time. Researchers using\nroutine eGFR data should account for these effects.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 09:53:20 GMT"}], "update_date": "2014-09-05", "authors_parsed": [["Poh", "Norman", ""], ["McGovern", "Andrew", ""], ["de Lusignan", "Simon", ""]]}, {"id": "1409.1609", "submitter": "Steven Prestwich D", "authors": "Steven Prestwich, Armagan Tarim, Roberto Rossi, Brahim Hnich", "title": "Intermittency and Obsolescence: a Croston Method With Linear Decay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Only two Croston-style forecasting methods are currently known for handling\nstochastic intermittent demand with possible demand obsolescence: TSB and HES,\nboth shown to be unbiased. When an item becomes obsolescent then TSB's\nforecasts decay exponentially, while HES's decay hyperbolically. We describe a\nthird variant called Linear-Exponential Smoothing that is also unbiased, decays\nlinearly to zero in a finite time, is asymptotically the best variant for\nhandling obsolescence, and performs well in experiments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 11:50:24 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Prestwich", "Steven", ""], ["Tarim", "Armagan", ""], ["Rossi", "Roberto", ""], ["Hnich", "Brahim", ""]]}, {"id": "1409.1620", "submitter": "Yevgeniy Kovchegov", "authors": "Yevgeniy Kovchegov, Nese Yildiz", "title": "Orthogonal Polynomials for Seminonparametric Instrumental Variables\n  Model", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach that resolves a {\\it polynomial basis problem} for a\nclass of models with discrete endogenous covariate, and for a class of\neconometric models considered in the work of Newey and Powell (2003), where the\nendogenous covariate is continuous. Suppose $X$ is a $d$-dimensional endogenous\nrandom variable, $Z_1$ and $Z_2$ are the instrumental variables (vectors), and\n$Z=\\left(\\begin{array}{c}Z_1 \\\\Z_2\\end{array}\\right)$. Now, assume that the\nconditional distributions of $X$ given $Z$ satisfy the conditions sufficient\nfor solving the identification problem as in Newey and Powell (2003) or as in\nProposition 1.1 of the current paper. That is, for a function $\\pi(z)$ in the\nimage space there is a.s. a unique function $g(x,z_1)$ in the domain space such\nthat $$E[g(X,Z_1)~|~Z]=\\pi(Z) \\qquad Z-a.s.$$ In this paper, for a class of\nconditional distributions $X|Z$, we produce an orthogonal polynomial basis\n$Q_j(x,z_1)$ such that for a.e. $Z_1=z_1$, and for all $j \\in \\mathbb{Z}_+^d$,\nand a certain $\\mu(Z)$, $$P_j(\\mu(Z))=E[Q_j(X, Z_1)~|~Z ],$$ where $P_j$ is a\npolynomial of degree $j$. This is what we call solving the {\\it polynomial\nbasis problem}.\n  Assuming the knowledge of $X|Z$ and an inference of $\\pi(z)$, our approach\nprovides a natural way of estimating the structural function of interest\n$g(x,z_1)$. Our polynomial basis approach is naturally extended to Pearson-like\nand Ord-like families of distributions.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 21:43:29 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Kovchegov", "Yevgeniy", ""], ["Yildiz", "Nese", ""]]}, {"id": "1409.1798", "submitter": "Adam Kapelner", "authors": "Richard Berk, Justin Bleich, Adam Kapelner, Jaime Henderson, Geoffrey\n  Barnes, and Ellen Kurtz", "title": "Using Regression Kernels to Forecast A Failure to Appear in Court", "comments": "43 pages, 5 figures, 2 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasts of prospective criminal behavior have long been an important\nfeature of many criminal justice decisions. There is now substantial evidence\nthat machine learning procedures will classify and forecast at least as well,\nand typically better, than logistic regression, which has to date dominated\nconventional practice. However, machine learning procedures are adaptive. They\n\"learn\" inductively from training data. As a result, they typically perform\nbest with very large datasets. There is a need, therefore, for forecasting\nprocedures with the promise of machine learning that will perform well with\nsmall to moderately-sized datasets. Kernel methods provide precisely that\npromise. In this paper, we offer an overview of kernel methods in regression\nsettings and compare such a method, regularized with principle components, to\nstepwise logistic regression. We apply both to a timely and important criminal\njustice concern: a failure to appear (FTA) at court proceedings following an\narraignment. A forecast of an FTA can be an important factor is a judge's\ndecision to release a defendant while awaiting trial and can influence the\nconditions imposed on that release. Forecasting accuracy matters, and our\nkernel approach forecasts far more accurately than stepwise logistic\nregression. The methods developed here are implemented in the R package kernReg\ncurrently available on CRAN.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 13:49:18 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Berk", "Richard", ""], ["Bleich", "Justin", ""], ["Kapelner", "Adam", ""], ["Henderson", "Jaime", ""], ["Barnes", "Geoffrey", ""], ["Kurtz", "Ellen", ""]]}, {"id": "1409.1956", "submitter": "Enrique ter Horst A", "authors": "Roberto Casarin and Fabrizio Leisen and German Molina and Enrique ter\n  Horst", "title": "A Bayesian Beta Markov Random Field Calibration of the Term Structure of\n  Implied Risk Neutral Densities", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build on the work in Fackler and King 1990, and propose a more general\ncalibration model for implied risk neutral densities. Our model allows for the\njoint calibration of a set of densities at different maturities and dates\nthrough a Bayesian dynamic Beta Markov Random Field. Our approach allows for\npossible time dependence between densities with the same maturity, and for\ndependence across maturities at the same point in time. This approach to the\nproblem encompasses model flexibility, parameter parsimony and, more\nimportantly, information pooling across densities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 21:45:55 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Casarin", "Roberto", ""], ["Leisen", "Fabrizio", ""], ["Molina", "German", ""], ["ter Horst", "Enrique", ""]]}, {"id": "1409.2027", "submitter": "Siddharth Arora Dr.", "authors": "Siddharth Arora, James W. Taylor", "title": "Short-term Forecasting of Anomalous Load Using Rule-based Triple\n  Seasonal Methods", "comments": "8 Pages, 11 Figures", "journal-ref": "IEEE Transactions on Power Systems, 28, 3235-3242, 2013", "doi": "10.1109/TPWRS.2013.2252929", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous methods have been proposed for forecasting load for normal days.\nModeling of anomalous load, however, has often been ignored in the research\nliterature. Occurring on special days, such as public holidays, anomalous load\nconditions pose considerable modeling challenges due to their infrequent\noccurrence and significant deviation from normal load. To overcome these\nlimitations, we adopt a rule-based approach, which allows incorporation of\nprior expert knowledge of load profiles into the statistical model. We use\ntriple seasonal Holt-Winters-Taylor (HWT) exponential smoothing, triple\nseasonal autoregressive moving average (ARMA), artificial neural networks\n(ANNs), and triple seasonal intraweek singular value decomposition (SVD) based\nexponential smoothing. These methods have been shown to be competitive for\nmodeling load for normal days. The methodological contribution of this paper is\nto demonstrate how these methods can be adapted to model load for special days,\nwhen used in conjunction with a rule-based approach. The proposed rule-based\nmethod is able to model normal and anomalous load in a unified framework. Using\nnine years of half-hourly load for Great Britain, we evaluate point forecasts,\nfor lead times from one half-hour up to a day ahead. A combination of two\nrule-based methods generated the most accurate forecasts.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 16:04:18 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Arora", "Siddharth", ""], ["Taylor", "James W.", ""]]}, {"id": "1409.2080", "submitter": "Pierre Bellec", "authors": "P. Bellec and Y. Benhajali and F. Carbonell and C. Dansereau and G.\n  Albouy and M. Pelland and C. Craddock and O. Collignon and J. Doyon and E.\n  Stip and P. Orban", "title": "Multiscale statistical testing for connectome-wide association studies\n  in fMRI", "comments": "54 pages, 12 main figures, 1 main table, 10 supplementary figures, 1\n  supplementary table", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.CV stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Alterations in brain connectivity have been associated with a variety of\nclinical disorders using functional magnetic resonance imaging (fMRI). We\ninvestigated empirically how the number of brain parcels (or scale) impacted\nthe results of a mass univariate general linear model (GLM) on connectomes. The\nbrain parcels used as nodes in the connectome analysis were functionnally\ndefined by a group cluster analysis. We first validated that a classic\nBenjamini-Hochberg procedure with parametric GLM tests did control\nappropriately the false-discovery rate (FDR) at a given scale. We then observed\non realistic simulations that there was no substantial inflation of the FDR\nacross scales, as long as the FDR was controlled independently within each\nscale, and the presence of true associations could be established using an\nomnibus permutation test combining all scales. Second, we observed both on\nsimulations and on three real resting-state fMRI datasets (schizophrenia,\ncongenital blindness, motor practice) that the rate of discovery varied\nmarkedly as a function of scales, and was relatively higher for low scales,\nbelow 25. Despite the differences in discovery rate, the statistical maps\nderived at different scales were generally very consistent in the three real\ndatasets. Some seeds still showed effects better observed around 50,\nillustrating the potential benefits of multiscale analysis. On real data, the\nstatistical maps agreed well with the existing literature. Overall, our results\nsupport that the multiscale GLM connectome analysis with FDR is statistically\nvalid and can capture biologically meaningful effects in a variety of\nexperimental conditions.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 04:07:22 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 21:00:45 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Bellec", "P.", ""], ["Benhajali", "Y.", ""], ["Carbonell", "F.", ""], ["Dansereau", "C.", ""], ["Albouy", "G.", ""], ["Pelland", "M.", ""], ["Craddock", "C.", ""], ["Collignon", "O.", ""], ["Doyon", "J.", ""], ["Stip", "E.", ""], ["Orban", "P.", ""]]}, {"id": "1409.2441", "submitter": "Andrea Mercatanti", "authors": "Andrea Mercatanti, Fan Li", "title": "Do debit cards increase household spending? Evidence from a\n  semiparametric causal analysis of a survey", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS784 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 4, 2485-2508", "doi": "10.1214/14-AOAS784", "report-no": "IMS-AOAS-AOAS784", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent findings in the field of consumer science, this paper\nevaluates the causal effect of debit cards on household consumption using\npopulation-based data from the Italy Survey on Household Income and Wealth\n(SHIW). Within the Rubin Causal Model, we focus on the estimand of population\naverage treatment effect for the treated (PATT). We consider three existing\nestimators, based on regression, mixed matching and regression, propensity\nscore weighting, and propose a new doubly-robust estimator. Semiparametric\nspecification based on power series for the potential outcomes and the\npropensity score is adopted. Cross-validation is used to select the order of\nthe power series. We conduct a simulation study to compare the performance of\nthe estimators. The key assumptions, overlap and unconfoundedness, are\nsystematically assessed and validated in the application. Our empirical results\nsuggest statistically significant positive effects of debit cards on the\nmonthly household spending in Italy.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 17:40:37 GMT"}, {"version": "v2", "created": "Tue, 3 Feb 2015 13:43:00 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Mercatanti", "Andrea", ""], ["Li", "Fan", ""]]}, {"id": "1409.2448", "submitter": "Omer Weissbrod", "authors": "Omer Weissbrod, Christoph Lippert, Dan Geiger, and David Heckerman", "title": "Accurate Liability Estimation Improves Power in Ascertained Case Control\n  Studies", "comments": null, "journal-ref": "Nature methods 12(4):332-334 (2015)", "doi": "10.1038/nmeth.3285", "report-no": null, "categories": "q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed models (LMMs) have emerged as the method of choice for\nconfounded genome-wide association studies. However, the performance of LMMs in\nnon-randomly ascertained case-control studies deteriorates with increasing\nsample size. We propose a framework called LEAP (Liability Estimator As a\nPhenotype, https://github.com/omerwe/LEAP) that tests for association with\nestimated latent values corresponding to severity of phenotype, and demonstrate\nthat this can lead to a substantial power increase.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 18:12:11 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 17:12:23 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2016 11:01:49 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Weissbrod", "Omer", ""], ["Lippert", "Christoph", ""], ["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1409.2546", "submitter": "Alexey Miroshnikov", "authors": "Alexey Miroshnikov, Erin Conlon", "title": "parallelMCMCcombine: An R Package for Bayesian Methods for Big Data and\n  Analytics", "comments": "for published version see:\n  http://www.plosone.org/article/fetchObject.action?uri=info%3Adoi%2F10.1371%2Fjournal.pone.0108425&representation=PDF", "journal-ref": "PLoS ONE (2014), Volume 9, Issue 9, e108425", "doi": "10.1371/journal.pone.0108425", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in big data and analytics research have provided a wealth of\nlarge data sets that are too big to be analyzed in their entirety, due to\nrestrictions on computer memory or storage size. New Bayesian methods have been\ndeveloped for large data sets that are only large due to large sample sizes;\nthese methods partition big data sets into subsets, and perform independent\nBayesian Markov chain Monte Carlo analyses on the subsets. The methods then\ncombine the independent subset posterior samples to estimate a posterior\ndensity given the full data set. These approaches were shown to be effective\nfor Bayesian models including logistic regression models, Gaussian mixture\nmodels and hierarchical models. Here, we introduce the R package\nparallelMCMCcombine which carries out four of these techniques for combining\nindependent subset posterior samples. We illustrate each of the methods using a\nBayesian logistic regression model for simulation data and a Bayesian Gamma\nmodel for real data; we also demonstrate features and capabilities of the R\npackage. The package assumes the user has carried out the Bayesian analysis and\nhas produced the independent subposterior samples outside of the package. The\nmethods are primarily suited to models with unknown parameters of fixed\ndimension that exist in continuous parameter spaces. We envision this tool will\nallow researchers to explore the various methods for their specific\napplications, and will assist future progress in this rapidly developing field.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 23:11:05 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 16:22:08 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Miroshnikov", "Alexey", ""], ["Conlon", "Erin", ""]]}, {"id": "1409.2642", "submitter": "Fulvia Pennoni Mrs", "authors": "Leonardo Grilli, Fulvia Pennoni, Carla Rampichini, Isabella Romeo", "title": "Exploiting TIMSS and PIRLS combined data: multivariate multilevel\n  modelling of student achievement", "comments": "25 pages, 5 figures, Presented at the VI European Congress of\n  Methodology, 23-25 July 2014, Utrecht, Netherlands; Presented at the 47th\n  Scientific meeting of the Italian Statistical Society, 11-13 June 2014,\n  Cagliari, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a multivariate multilevel model for the analysis of the Italian\nsample of the TIMSS\\&PIRLS 2011 Combined International Database on fourth grade\nstudents. The multivariate approach jointly considers educational achievement\non Reading, Mathematics and Science, thus allowing us to test for differential\nassociations of the covariates with the three outcomes, and to estimate the\nresidual correlations between pairs of outcomes at student and class levels.\nMultilevel modelling allows us to disentangle student and contextual factors\naffecting achievement. We also account for territorial differences in wealth by\nmeans of an index from an external source. The model residuals point out\nclasses with high or low performance. As educational achievement is measured by\nplausible values, the estimates are obtained through multiple imputation\nformulas. The results, while confirming the role of traditional student and\ncontextual factors, reveal interesting patterns of achievement in Italian\nprimary schools.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 08:53:14 GMT"}, {"version": "v2", "created": "Sun, 16 Aug 2015 09:02:37 GMT"}], "update_date": "2015-08-18", "authors_parsed": [["Grilli", "Leonardo", ""], ["Pennoni", "Fulvia", ""], ["Rampichini", "Carla", ""], ["Romeo", "Isabella", ""]]}, {"id": "1409.2856", "submitter": "Siddharth Arora Dr.", "authors": "Siddharth Arora, James W. Taylor", "title": "Forecasting Electricity Smart Meter Data Using Conditional Kernel\n  Density Estimation", "comments": "28 Pages, 12 Figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent advent of smart meters has led to large micro-level datasets. For\nthe first time, the electricity consumption at individual sites is available on\na near real-time basis. Efficient management of energy resources, electric\nutilities, and transmission grids, can be greatly facilitated by harnessing the\npotential of this data. The aim of this study is to generate probability\ndensity estimates for consumption recorded by individual smart meters. Such\nestimates can assist decision making by helping consumers identify and minimize\ntheir excess electricity usage, especially during peak times. For suppliers,\nthese estimates can be used to devise innovative time-of-use pricing strategies\naimed at their target consumers. We consider methods based on conditional\nkernel density (CKD) estimation with the incorporation of a decay parameter.\nThe methods capture the seasonality in consumption, and enable a nonparametric\nestimation of its conditional density. Using eight months of half-hourly data\nfor one thousand meters, we evaluate point and density forecasts, for lead\ntimes ranging from one half-hour up to a week ahead. We find that the\nkernel-based methods outperform a simple benchmark method that does not account\nfor seasonality, and compare well with an exponential smoothing method that we\nuse as a sophisticated benchmark. To gauge the financial impact, we use density\nestimates of consumption to derive prediction intervals of electricity cost for\ndifferent time-of-use tariffs. We show that a simple strategy of switching\nbetween different tariffs, based on a comparison of cost densities, delivers\nsignificant cost savings for the great majority of consumers.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 19:55:28 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Arora", "Siddharth", ""], ["Taylor", "James W.", ""]]}, {"id": "1409.2903", "submitter": "Eftychios A. Pnevmatikakis", "authors": "Eftychios A. Pnevmatikakis, Yuanjun Gao, Daniel Soudry, David Pfau,\n  Clay Lacefield, Kira Poskanzer, Randy Bruno, Rafael Yuste, Liam Paninski", "title": "A structured matrix factorization framework for large scale calcium\n  imaging data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a structured matrix factorization approach to analyzing calcium\nimaging recordings of large neuronal ensembles. Our goal is to simultaneously\nidentify the locations of the neurons, demix spatially overlapping components,\nand denoise and deconvolve the spiking activity of each neuron from the slow\ndynamics of the calcium indicator. The matrix factorization approach relies on\nthe observation that the spatiotemporal fluorescence activity can be expressed\nas a product of two matrices: a spatial matrix that encodes the location of\neach neuron in the optical field and a temporal matrix that characterizes the\ncalcium concentration of each neuron over time. We present a simple approach\nfor estimating the dynamics of the calcium indicator as well as the observation\nnoise statistics from the observed data. These parameters are then used to set\nup the matrix factorization problem in a constrained form that requires no\nfurther parameter tuning. We discuss initialization and post-processing\ntechniques that enhance the performance of our method, along with efficient and\nlargely parallelizable algorithms. We apply our method to {\\it in vivo} large\nscale multi-neuronal imaging data and also demonstrate how similar methods can\nbe used for the analysis of {\\it in vivo} dendritic imaging data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 21:25:59 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Pnevmatikakis", "Eftychios A.", ""], ["Gao", "Yuanjun", ""], ["Soudry", "Daniel", ""], ["Pfau", "David", ""], ["Lacefield", "Clay", ""], ["Poskanzer", "Kira", ""], ["Bruno", "Randy", ""], ["Yuste", "Rafael", ""], ["Paninski", "Liam", ""]]}, {"id": "1409.3174", "submitter": "Eytan Bakshy", "authors": "Eytan Bakshy, Dean Eckles, Michael S. Bernstein", "title": "Designing and Deploying Online Field Experiments", "comments": "Proceedings of the 23rd international conference on World wide web,\n  283-292", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.PL cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online experiments are widely used to compare specific design alternatives,\nbut they can also be used to produce generalizable knowledge and inform\nstrategic decision making. Doing so often requires sophisticated experimental\ndesigns, iterative refinement, and careful logging and analysis. Few tools\nexist that support these needs. We thus introduce a language for online field\nexperiments called PlanOut. PlanOut separates experimental design from\napplication code, allowing the experimenter to concisely describe experimental\ndesigns, whether common \"A/B tests\" and factorial designs, or more complex\ndesigns involving conditional logic or multiple experimental units. These\nlatter designs are often useful for understanding causal mechanisms involved in\nuser behaviors. We demonstrate how experiments from the literature can be\nimplemented in PlanOut, and describe two large field experiments conducted on\nFacebook with PlanOut. For common scenarios in which experiments are run\niteratively and in parallel, we introduce a namespaced management system that\nencourages sound experimental practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 18:27:47 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Bakshy", "Eytan", ""], ["Eckles", "Dean", ""], ["Bernstein", "Michael S.", ""]]}, {"id": "1409.3246", "submitter": "Tadilo E Bogale", "authors": "Tadilo Endeshaw Bogale, Luc Vandendorpe, Long Bao Le", "title": "Wideband Sensing and Optimization for Cognitive Radio Networks with\n  Noise Variance Uncertainty", "comments": "Submitted to IEEE Transactions on Communications (Revised version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers wide-band spectrum sensing and optimization for\ncognitive radio (CR) networks with noise variance uncertainty. It is assumed\nthat the considered wide-band contains one or more white sub-bands. Under this\nassumption, we consider throughput maximization of the CR network while\nappropriately protecting the primary network. We address this problem as\nfollows. First, we propose novel ratio based test statistics for detecting the\nedges of each sub-band. Second, we employ simple energy comparison approach to\nchoose one reference white sub-band. Third, we propose novel generalized energy\ndetector (GED) for examining each of the remaining sub-bands by exploiting the\nnoise information of the reference white sub-band. Finally, we optimize the\nsensing time ($T_o$) to maximize the CR network throughput using the detection\nand false alarm probabilities of the GED. The proposed GED does not suffer from\nsignal to noise ratio (SNR) wall and outperforms the existing signal detectors.\nMoreover, the relationship between the proposed GED and conventional energy\ndetector (CED) is quantified analytically. We show that the optimal $T_o$\ndepends on the noise variance information. In particular, with $10$TV bands,\nSNR=$-20$dB and $2$s frame duration, we found that the optimal $T_o$ is\n$28.5$ms ($50.6$ms) with perfect (imperfect) noise variance scenario.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 20:18:36 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Bogale", "Tadilo Endeshaw", ""], ["Vandendorpe", "Luc", ""], ["Le", "Long Bao", ""]]}, {"id": "1409.3408", "submitter": "Emanuele  Giorgi", "authors": "Emanuele Giorgi and Peter J. Diggle", "title": "On The Inverse Geostatistical Problem of Inference on Missing Locations", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard geostatistical problem is to predict the values of a spatially\ncontinuous phenomenon, $S(x)$ say, at locations $x$ using data\n$(y_i,x_i):i=1,..,n$ where $y_i$ is the realization at location $x_i$ of\n$S(x_i)$, or of a random variable $Y_i$ that is stochastically related to\n$S(x_i)$. In this paper we address the inverse problem of predicting the\nlocations of observed measurements $y$. We discuss how knowledge of the\nsampling mechanism can and should inform a prior specification, $\\pi(x)$ say,\nfor the joint distribution of the measurement locations $X = \\{x_i:\ni=1,...,n\\}$, and propose an efficient Metropolis-Hastings algorithm for\ndrawing samples from the resulting predictive distribution of the missing\nelements of $X$. An important feature in many applied settings is that this\npredictive distribution is multi-modal, which severely limits the usefulness of\nsimple summary measures such as the mean or median. We present two simulated\nexamples to demonstrate the importance of the specification for $\\pi(x)$, and\nanalyze rainfall data from Paran\\'a State, Brazil to show how, under additional\nassumptions, an empirical of estimate of $\\pi(x)$ can be used when no prior\ninformation on the sampling design is available.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 12:19:18 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Giorgi", "Emanuele", ""], ["Diggle", "Peter J.", ""]]}, {"id": "1409.3918", "submitter": "Daniel Kosiorowski", "authors": "Ewa Kosiorowska, Daniel Kosiorowski, Zygmunt Zawadzki", "title": "Evaluation of the Fourth Millennium Development Goal Realisation Using\n  Robust and Nonparametric Tools Offered by Data Depth Concept", "comments": "The paper is basing on a poster submitted to IASC 2014 Data\n  Competition - the poster was the runner-up (the second place)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We briefly communicate results of a nonparametric and robust evaluation of\neffects of \\emph{the Fourth Millennium Development Goal of United Nations}.\nMain aim of the goal was reducing by two thirds, between 1990--2015, the under\nfive months child mortality. Our novel analysis was conducted by means of very\npowerful and user friendly tools offered by the \\emph{Data Depth Concept} being\na collection of multivariate techniques basing on multivariate generalizations\nof quantiles, ranges and order statistics. Results of our analysis are more\nconvincing than results obtained using classical statistical tools.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 06:30:56 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 19:05:45 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Kosiorowska", "Ewa", ""], ["Kosiorowski", "Daniel", ""], ["Zawadzki", "Zygmunt", ""]]}, {"id": "1409.3954", "submitter": "Shunqiao Sun Shunqiao Sun", "authors": "Shunqiao Sun, Waheed U. Bajwa and Athina P. Petropulu", "title": "MIMO-MC Radar: A MIMO Radar Approach Based on Matrix Completion", "comments": "29 pages, 13 figures, IEEE Trans. on Aerospace and Electronic Systems", "journal-ref": "IEEE Trans. Aerosp. Electron. Syst., vol. 51, no. 3, pp.\n  1839-1852, Jul. 2015", "doi": "10.1109/TAES.2015.140452", "report-no": null, "categories": "cs.IT math.IT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a typical MIMO radar scenario, transmit nodes transmit orthogonal\nwaveforms, while each receive node performs matched filtering with the known\nset of transmit waveforms, and forwards the results to the fusion center. Based\non the data it receives from multiple antennas, the fusion center formulates a\nmatrix, which, in conjunction with standard array processing schemes, such as\nMUSIC, leads to target detection and parameter estimation. In MIMO radars with\ncompressive sensing (MIMO-CS), the data matrix is formulated by each receive\nnode forwarding a small number of compressively obtained samples. In this\npaper, it is shown that under certain conditions, in both sampling cases, the\ndata matrix at the fusion center is low-rank, and thus can be recovered based\non knowledge of a small subset of its entries via matrix completion (MC)\ntechniques. Leveraging the low-rank property of that matrix, we propose a new\nMIMO radar approach, termed, MIMO-MC radar, in which each receive node either\nperforms matched filtering with a small number of randomly selected dictionary\nwaveforms or obtains sub-Nyquist samples of the received signal at random\nsampling instants, and forwards the results to a fusion center. Based on the\nreceived samples, and with knowledge of the sampling scheme, the fusion center\npartially fills the data matrix and subsequently applies MC techniques to\nestimate the full matrix. MIMO-MC radars share the advantages of the recently\nproposed MIMO-CS radars, i.e., high resolution with reduced amounts of data,\nbut unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar\nconcept is illustrated through a linear uniform array configuration, and its\ntarget estimation performance is demonstrated via simulations.\n", "versions": [{"version": "v1", "created": "Sat, 13 Sep 2014 14:37:13 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Sun", "Shunqiao", ""], ["Bajwa", "Waheed U.", ""], ["Petropulu", "Athina P.", ""]]}, {"id": "1409.4080", "submitter": "Henrik Singmann", "authors": "Nicolas Gauvrit, Henrik Singmann, Fernando Soler-Toscano, Hector Zenil", "title": "Algorithmic complexity for psychology: A user-friendly implementation of\n  the coding theorem method", "comments": "to appear in \"Behavioral Research Methods\", 14 pages in journal\n  format, R package at http://cran.r-project.org/web/packages/acss/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kolmogorov-Chaitin complexity has long been believed to be impossible to\napproximate when it comes to short sequences (e.g. of length 5-50). However,\nwith the newly developed \\emph{coding theorem method} the complexity of strings\nof length 2-11 can now be numerically estimated. We present the theoretical\nbasis of algorithmic complexity for short strings (ACSS) and describe an\nR-package providing functions based on ACSS that will cover psychologists'\nneeds and improve upon previous methods in three ways: (1) ACSS is now\navailable not only for binary strings, but for strings based on up to 9\ndifferent symbols, (2) ACSS no longer requires time-consuming computing, and\n(3) a new approach based on ACSS gives access to an estimation of the\ncomplexity of strings of any length. Finally, three illustrative examples show\nhow these tools can be applied to psychology.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 17:31:40 GMT"}, {"version": "v2", "created": "Thu, 19 Feb 2015 22:22:13 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Gauvrit", "Nicolas", ""], ["Singmann", "Henrik", ""], ["Soler-Toscano", "Fernando", ""], ["Zenil", "Hector", ""]]}, {"id": "1409.4119", "submitter": "Jungsuk Kwac", "authors": "Jungsuk Kwac, Ram Rajagopal", "title": "Targeting Customers for Demand Response Based on Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting customers for demand response programs is challenging and existing\nmethodologies are hard to scale and poor in performance. The existing methods\nwere limited by lack of temporal consumption information at the individual\ncustomer level. We propose a scalable methodology for demand response targeting\nutilizing novel data available from smart meters. The approach relies on\nformulating the problem as a stochastic integer program involving predicted\ncustomer responses. A novel approximation is developed algorithm so it can\nscale to problems involving millions of customers. The methodology is tested\nexperimentally using real utility data.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2014 23:26:31 GMT"}], "update_date": "2014-09-16", "authors_parsed": [["Kwac", "Jungsuk", ""], ["Rajagopal", "Ram", ""]]}, {"id": "1409.4294", "submitter": "Hector Javier Hortua", "authors": "H\\'ector J. Hort\\'ua", "title": "Proceedings of the First Astrostatistics School: Bayesian Methods in\n  Cosmology", "comments": "Proceedings (in Spanish), supplemental electronic material accessible\n  via links within the PDF. Appears in Cuadernos en Estadistica aplicada,\n  Edicion especial, (2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.GA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are the proceedings of the First Astrostatistics School: Bayesian\nMethods in Cosmology, held in Bogot\\'a D.C., Colombia, June 9-13, 2014. The\nfirst astrostatistics school has been the first event in Colombia where\nstatisticians and cosmologists from some universities in Bogot\\'a met to\ndiscuss the statistic methods applied to cosmology, especially the use of\nBayesian statistics in the study of Cosmic Microwave Background (CMB), Baryonic\nAcoustic Oscillations (BAO), Large Scale Structure (LSS) and weak lensing.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 21:35:57 GMT"}, {"version": "v2", "created": "Wed, 10 Dec 2014 22:57:24 GMT"}], "update_date": "2014-12-12", "authors_parsed": [["Hort\u00faa", "H\u00e9ctor J.", ""]]}, {"id": "1409.4512", "submitter": "Ali Mohammadian Mosammam", "authors": "A.M. Mosammam, J.T. Kent", "title": "Estimation and Testing for Covariance-Spectral Spatial-Temporal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a covariance spectral modelling strategy for\nspatial-temporal processes which involves a spectral approach for time but a\ncovariance approach for space.It facilitates the analysis of coherence between\nthe temporal frequency components at different spatial sites. Stein(2005)\ndeveloped a semi-parametric model within this framework.The purpose of this\npaper is to give a deeper insight into the properties of his model and to\ndevelop simple and more intuitive methods of estimation and testing. An example\nis given using the Irish wind speed data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 06:10:29 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Mosammam", "A. M.", ""], ["Kent", "J. T.", ""]]}, {"id": "1409.4671", "submitter": "Mudassir Masood", "authors": "Mudassir Masood, Laila H. Afify, and Tareq Y. Al-Naffouri", "title": "Efficient Coordinated Recovery of Sparse Channels in Massive MIMO", "comments": "16 pages, 12 figures", "journal-ref": null, "doi": "10.1109/TSP.2014.2369005", "report-no": null, "categories": "stat.AP cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of estimating sparse channels in massive\nMIMO-OFDM systems. Most wireless channels are sparse in nature with large delay\nspread. In addition, these channels as observed by multiple antennas in a\nneighborhood have approximately common support. The sparsity and common support\nproperties are attractive when it comes to the efficient estimation of large\nnumber of channels in massive MIMO systems. Moreover, to avoid pilot\ncontamination and to achieve better spectral efficiency, it is important to use\na small number of pilots. We present a novel channel estimation approach which\nutilizes the sparsity and common support properties to estimate sparse channels\nand require a small number of pilots. Two algorithms based on this approach\nhave been developed which perform Bayesian estimates of sparse channels even\nwhen the prior is non-Gaussian or unknown. Neighboring antennas share among\neach other their beliefs about the locations of active channel taps to perform\nestimation. The coordinated approach improves channel estimates and also\nreduces the required number of pilots. Further improvement is achieved by the\ndata-aided version of the algorithm. Extensive simulation results are provided\nto demonstrate the performance of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 20:55:37 GMT"}, {"version": "v2", "created": "Wed, 1 Oct 2014 08:33:37 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Masood", "Mudassir", ""], ["Afify", "Laila H.", ""], ["Al-Naffouri", "Tareq Y.", ""]]}, {"id": "1409.4815", "submitter": "P. Richard Hahn", "authors": "P. Richard Hahn and Indranil Goswami and Carl Mela", "title": "A Bayesian hierarchical model for inferring player strategy types in a\n  number guessing game", "comments": "46 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an in-depth statistical analysis of an experiment\ndesigned to measure the extent to which players in a simple game behave\naccording to a popular behavioral economic model. The p-beauty contest is a\nmulti-player number guessing game that has been widely used to study strategic\nbehavior. This paper describes beauty contest experiments for an audience of\ndata analysts, with a special focus on a class of models for game play called\nk-step thinking models, which allow each player in the game to employ an\nidiosyncratic strategy. We fit a Bayesian statistical model to estimate the\nproportion of our player population whose game play is compatible with a k-step\nthinking model. Our findings put this number at approximately 25%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 21:46:03 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Hahn", "P. Richard", ""], ["Goswami", "Indranil", ""], ["Mela", "Carl", ""]]}, {"id": "1409.4837", "submitter": "Alan Sokal", "authors": "Nicholas J.L. Brown, Alan D. Sokal, Harris L. Friedman", "title": "The persistence of wishful thinking: Response to \"Updated thinking on\n  positivity ratios\"", "comments": "LaTeX2e, 10 pages including 6 Postscript figures", "journal-ref": "American Psychologist 69, 629-632 (2014)", "doi": "10.1037/a0037050", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a response to Barbara Fredrickson's comment [American Psychologist\n68, 814-822 (2013)] on our article arXiv:1307.7006.\n  We analyze critically the renewed claims made by Fredrickson (2013)\nconcerning positivity ratios and \"flourishing\", and attempt to disentangle some\nconceptual confusions; we also address the alleged empirical evidence for\nnonlinear effects. We conclude that there is no evidence whatsoever for the\nexistence of any \"tipping points\", and only weak evidence for the existence of\nany nonlinearity of any kind. Our original concern, that the application of\nadvanced mathematical techniques in psychology and related disciplines may not\nalways be appropriate, remains undiminished.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 00:14:24 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Brown", "Nicholas J. L.", ""], ["Sokal", "Alan D.", ""], ["Friedman", "Harris L.", ""]]}, {"id": "1409.4896", "submitter": "Matteo Formenti", "authors": "Matteo Formenti", "title": "Mean of Ratios or Ratio of Means: statistical uncertainty applied to\n  estimate Multiperiod Probability of Defaul", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimate of a Multiperiod probability of default applied to residential\nmortgages can be obtained using the mean of the observed default, so called the\nMean of ratios estimator, or aggregating the default and the issued mortgages\nand computing the ratio of their sum, that is the Ratio of means. This work\nstudies the statistical properties of the two estimators with the result that\nthe Ratio of means has a lower statistical uncertainty. The application on a\nprivate residential mortgage portfolio leads to a lower probability of default\non the overall portfolio by eleven basis points.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 08:13:20 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Formenti", "Matteo", ""]]}, {"id": "1409.5172", "submitter": "Alan Sokal", "authors": "Nicholas J.L. Brown, Alan D. Sokal, Harris L. Friedman", "title": "Positive psychology and romantic scientism: Reply to comments on Brown,\n  Sokal, & Friedman (2013)", "comments": "PDF, 9 pages", "journal-ref": "American Psychologist 69, 636-637 (2014)", "doi": "10.1037/a0037390", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a response to five comments [American Psychologist 69, 626-629 and\n632-635 (2014)] on our article arXiv:1307.7006.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 01:39:11 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Brown", "Nicholas J. L.", ""], ["Sokal", "Alan D.", ""], ["Friedman", "Harris L.", ""]]}, {"id": "1409.5530", "submitter": "Tirza Routtenberg", "authors": "Tirza Routtenberg, Yao Xie, Rebecca M. Willett, and Lang Tong", "title": "PMU based Detection of Imbalance in Three-Phase Power Systems", "comments": "Accepted to IEEE Trans. on Power System Sep. 2014", "journal-ref": null, "doi": "10.1109/TPWRS.2014.2359630", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of imbalance detection in a three-phase power system using a\nphasor measurement unit (PMU) is considered. A general model for the zero,\npositive, and negative sequences from a PMU measurement at off-nominal\nfrequencies is presented and a hypothesis testing framework is formulated. The\nnew formulation takes into account the fact that minor degree of imbalance in\nthe system is acceptable and does not indicate subsequent interruptions,\nfailures, or degradation of physical components. A generalized likelihood ratio\ntest (GLRT) is developed and shown to be a function of the negative-sequence\nphasor estimator and the acceptable level of imbalances for nominal system\noperations. As a by-product to the proposed detection method, a constrained\nestimation of the positive and negative phasors and the frequency deviation is\nobtained for both balanced and unbalanced situations. The theoretical and\nnumerical performance analyses show improved performance over benchmark\ntechniques and robustness to the presence of additional harmonics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 06:41:06 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Routtenberg", "Tirza", ""], ["Xie", "Yao", ""], ["Willett", "Rebecca M.", ""], ["Tong", "Lang", ""]]}, {"id": "1409.5830", "submitter": "Richard Vale", "authors": "Richard Vale", "title": "Bayesian Prediction for The Winds of Winter", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictions are made for the number of chapters told from the point of view\nof each character in the next two novels in George R. R. Martin's \\emph{A Song\nof Ice and Fire} series by fitting a random effects model to a matrix of\npoint-of-view chapters in the earlier novels using Bayesian methods.\n{\\textbf{SPOILER WARNING: readers who have not read all five existing novels in\nthe series should not read further, as major plot points will be spoiled.}}\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 22:57:20 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Vale", "Richard", ""]]}, {"id": "1409.6034", "submitter": "Nicholas Polson", "authors": "Nicholas Polson, Vadim Sokolov", "title": "Bayesian analysis of traffic flow on interstate I-55: The LWR model", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS853 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 1864-1888", "doi": "10.1214/15-AOAS853", "report-no": "IMS-AOAS-AOAS853", "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transportation departments take actions to manage traffic flow and reduce\ntravel times based on estimated current and projected traffic conditions.\nTravel time estimates and forecasts require information on traffic density\nwhich are combined with a model to project traffic flow such as the\nLighthill-Whitham-Richards (LWR) model. We develop a particle filtering and\nlearning algorithm to estimate the current traffic density state and the LWR\nparameters. These inputs are related to the so-called fundamental diagram,\nwhich describes the relationship between traffic flow and density. We build on\nexisting methodology by allowing real-time updating of the posterior\nuncertainty for the critical density and capacity parameters. Our methodology\nis applied to traffic flow data from interstate highway I-55 in Chicago. We\nprovide a real-time data analysis of how to learn the drop in capacity as a\nresult of a major traffic accident. Our algorithm allows us to accurately\nassess the uncertainty of the current traffic state at shock waves, where the\nuncertainty is a mixture distribution. We show that Bayesian learning can\ncorrect the estimation bias that is present in the model with fixed parameters.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 19:12:30 GMT"}, {"version": "v2", "created": "Sun, 12 Oct 2014 02:22:05 GMT"}, {"version": "v3", "created": "Thu, 12 Feb 2015 20:40:43 GMT"}, {"version": "v4", "created": "Fri, 29 Jan 2016 11:10:09 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""]]}, {"id": "1409.6239", "submitter": "Leonardo Bastos", "authors": "Leonardo Soares Bastos, Raquel de Vasconcellos Carvalhaes de Oliveira,\n  Luciane de Souza Velasque", "title": "Obtaining adjusted prevalence ratios from logistic regression model in\n  cross-sectional studies", "comments": null, "journal-ref": null, "doi": "10.1590/0102-311X00175413", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decades, it has been discussed the use of epidemiological\nprevalence ratio (PR) rather than odds ratio as a measure of association to be\nestimated in cross-sectional studies. The main difficulties in use of\nstatistical models for the calculation of PR are convergence problems,\navailability of adequate tools and strong assumptions. The goal of this study\nis to illustrate how to estimate PR and its confidence interval directly from\nlogistic regression estimates. We present three examples and compare the\nadjusted estimates of PR with the estimates obtained by use of log-binomial,\nrobust Poisson regression and adjusted prevalence odds ratio (POR). The\nmarginal and conditional prevalence ratios estimated from logistic regression\nshowed the following advantages: no numerical instability; simple to implement\nin a statistical software; and assumes the adequate probability distribution\nfor the outcome.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 16:50:47 GMT"}], "update_date": "2015-04-07", "authors_parsed": [["Bastos", "Leonardo Soares", ""], ["de Oliveira", "Raquel de Vasconcellos Carvalhaes", ""], ["Velasque", "Luciane de Souza", ""]]}, {"id": "1409.6439", "submitter": "Benjamin Trendelkamp-Schroer", "authors": "Benjamin Trendelkamp-Schroer and Frank Noe", "title": "Efficient estimation of rare-event kinetics", "comments": "16 pages, 14 figures", "journal-ref": "Phys. Rev. X 6, 011009 (2016)", "doi": "10.1103/PhysRevX.6.011009", "report-no": null, "categories": "physics.chem-ph q-bio.BM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient calculation of rare-event kinetics in complex dynamical\nsystems, such as the rate and pathways of ligand dissociation from a protein,\nis a generally unsolved problem. Markov state models can systematically\nintegrate ensembles of short simulations and thus effectively parallelize the\ncomputational effort, but the rare events of interest still need to be\nspontaneously sampled in the data. Enhanced sampling approaches, such as\nparallel tempering or umbrella sampling, can accelerate the computation of\nequilibrium expectations massively - but sacrifice the ability to compute\ndynamical expectations. In this work we establish a principle to combine\nknowledge of the equilibrium distribution with kinetics from fast \"downhill\"\nrelaxation trajectories using reversible Markov models. This approach is\ngeneral as it does not invoke any specific dynamical model, and can provide\naccurate estimates of the rare event kinetics. Large gains in sampling\nefficiency can be achieved whenever one direction of the process occurs more\nrapid than its reverse, making the approach especially attractive for downhill\nprocesses such as folding and binding in biomolecules.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:15:14 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2015 14:17:06 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Trendelkamp-Schroer", "Benjamin", ""], ["Noe", "Frank", ""]]}, {"id": "1409.6895", "submitter": "Richard Mann", "authors": "Richard P. Mann and Roman Garnett", "title": "The entropic basis of collective behaviour", "comments": "7 pages, 6 figures", "journal-ref": "Journal of the Royal Society Interface 2015 12 20150037", "doi": "10.1098/rsif.2015.0037", "report-no": null, "categories": "q-bio.OT physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we identify a radically new viewpoint on the collective\nbehaviour of groups of intelligent agents. We first develop a highly general\nabstract model for the possible future lives that these agents may encounter as\na result of their decisions. In the context of these possible futures, we show\nthat the causal entropic principle, whereby agents follow behavioural rules\nthat maximise their entropy over all paths through the future, predicts many of\nthe observed features of social interactions between individuals in both human\nand animal groups. Our results indicate that agents are often able to maximise\ntheir future path entropy by remaining cohesive as a group, and that this\ncohesion leads to collectively intelligent outcomes that depend strongly on the\ndistribution of the number of future paths that are possible. We derive social\ninteraction rules that are consistent with maximum-entropy group behaviour for\nboth discrete and continuous decision spaces. Our analysis further predicts\nthat social interactions are likely to be fundamentally based on Weber's law of\nresponse to proportional stimuli, supporting many studies that find a\nneurological basis for this stimulus-response mechanism, and providing a novel\nbasis for the common assumption of linearly additive 'social forces' in\nsimulation studies of collective behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 10:57:18 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Mann", "Richard P.", ""], ["Garnett", "Roman", ""]]}, {"id": "1409.6994", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella", "title": "Bayesian complementary clustering, MCMC and Anglo-Saxon placenames", "comments": "33 pages, 13 figures. Version 4: minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common cluster models for multi-type point processes model the aggregation of\npoints of the same type. In complete contrast, in the study of Anglo-Saxon\nsettlements it is hypothesized that administrative clusters involving\ncomplementary names tend to appear. We investigate the evidence for such an\nhypothesis by developing a Bayesian Random Partition Model based on clusters\nformed by points of different types (complementary clustering).\n  As a result we obtain an intractable posterior distribution on the space of\nmatchings contained in a k-partite hypergraph. We apply the Metropolis-Hastings\n(MH) algorithm to sample from this posterior. We consider the problem of\nchoosing an efficient MH proposal distribution and we obtain consistent mixing\nimprovements compared to the choices found in the literature. Simulated\nTempering techniques can be used to overcome multimodality and a multiple\nproposal scheme is developed to allow for parallel programming. Finally, we\ndiscuss results arising from the careful use of convergence diagnostic\ntechniques.\n  This allows us to study a dataset including locations and placenames of 1316\nAnglo-Saxon settlements dated approximately around 750-850 AD. Without strong\nprior knowledge, the model allows for explicit estimation of the number of\nclusters, the average intra-cluster dispersion and the level of interaction\namong placenames. The results support the hypothesis of organization of\nsettlements into administrative clusters based on complementary names.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 15:36:28 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 18:53:41 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 11:27:58 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 14:47:23 GMT"}], "update_date": "2015-07-03", "authors_parsed": [["Zanella", "Giacomo", ""]]}, {"id": "1409.7086", "submitter": "Sean Simpson", "authors": "Sean L. Simpson and Paul J. Laurienti", "title": "A Two-Part Mixed-Effects Modeling Framework For Analyzing Whole-Brain\n  Network Data", "comments": null, "journal-ref": "NeuroImage 113, 310-319, 2015", "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole-brain network analyses remain the vanguard in neuroimaging research,\ncoming to prominence within the last decade. Network science approaches have\nfacilitated these analyses and allowed examining the brain as an integrated\nsystem. However, statistical methods for modeling and comparing groups of\nnetworks have lagged behind. Fusing multivariate statistical approaches with\nnetwork science presents the best path to develop these methods. Toward this\nend, we propose a two-part mixed-effects modeling framework that allows\nmodeling both the probability of a connection (presence/absence of an edge) and\nthe strength of a connection if it exists. Models within this framework enable\nquantifying the relationship between an outcome (e.g., disease status) and\nconnectivity patterns in the brain while reducing spurious correlations through\ninclusion of confounding covariates. They also enable prediction about an\noutcome based on connectivity structure and vice versa, simulating networks to\ngain a better understanding of normal ranges of topological variability, and\nthresholding networks leveraging group information. Thus, they provide a\ncomprehensive approach to studying system level brain properties to further our\nunderstanding of normal and abnormal brain function.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:17:08 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Simpson", "Sean L.", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1409.7105", "submitter": "Michael Ward", "authors": "Andreas Beger, Cassy L. Dorff, and Michael D. Ward", "title": "Irregular Leadership Changes in 2014: Forecasts using ensemble,\n  split-population duration models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We forecast Irregular Leadership Changes (ILC)--unexpected leadership changes\nin contravention of a state's established laws and conventions--for mid-2014\nusing predictions generated from an innovative ensemble model that is composed\nof several split-population duration regression models. This approach uses\ndistinct thematic models, combining them into one aggregate forecast developed\non the basis of their predictive accuracy and uniqueness. The data are based on\n45 ILCs that occurred from March 2001 through March 2014, with monthly\nobservations for up to 168 countries worldwide. The ensemble model provides\nforecasts for the period from April to September 2014. Notably, the countries\nwith the highest probability of irregular leadership change in the middle six\nmonths of 2014 include the Ukraine, Bosnia & Herzegovina, Yemen, Egypt, and\nThailand. The leadership in these countries have exhibited fragility during\nthis forecast window.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 21:15:47 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Beger", "Andreas", ""], ["Dorff", "Cassy L.", ""], ["Ward", "Michael D.", ""]]}, {"id": "1409.7454", "submitter": "Wanchuang Zhu", "authors": "Wanchuang Zhu, Jinsong Ouyang, Yothin Rakvongthai, N. J. Guehl, D. W.\n  Wooten, G. El Fakhri, M. D. Normandin, Yanan Fan", "title": "A Bayesian spatial temporal mixtures approach to kinetic parametric\n  images in dynamic Positron Emission Tomography", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fully Bayesian statistical approach to the problem of\ncompartmental modelling in the context of Positron Emission Tomography. We\ncluster homogeneous region of interest and perform kinetic parameter estimation\nsimultaneously. A mixture modelling approach is adopted, incorporating both\nspatial and temporal information based on reconstructed dynamic PET image. Our\nmodelling approach is flexible, and provides uncertainty estimates for the\nestimated kinetic parameters. Crucially, the proposed method allows us to\ndetermine the unknown number of clusters, which has a great impact on resulting\nestimated kinetic parameters. We demonstrate our method on simulated dynamic\nMyocardial PET data, and show that our method is superior to standard\ncurve-fitting approach.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 01:18:24 GMT"}, {"version": "v2", "created": "Mon, 29 Sep 2014 08:30:35 GMT"}, {"version": "v3", "created": "Sun, 7 Feb 2016 23:09:46 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Zhu", "Wanchuang", ""], ["Ouyang", "Jinsong", ""], ["Rakvongthai", "Yothin", ""], ["Guehl", "N. J.", ""], ["Wooten", "D. W.", ""], ["Fakhri", "G. El", ""], ["Normandin", "M. D.", ""], ["Fan", "Yanan", ""]]}, {"id": "1409.7598", "submitter": "C\\'ecile Proust-Lima", "authors": "C\\'ecile Proust-Lima, Jean-Fran\\c{c}ois Dartigues and H\\'el\\`ene\n  Jacqmin-Gadda", "title": "Joint modelling of repeated multivariate cognitive measures and\n  competing risks of dementia and death: a latent process and latent class\n  approach", "comments": null, "journal-ref": "Statistics in Medicine (2016) 35(3) 382-398", "doi": "10.1002/sim.6731", "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models initially dedicated to a single longitudinal marker and a single\ntime-to-event need to be extended to account for the rich longitudinal data of\ncohort studies. Multiple causes of clinical progression are indeed usually\nobserved, and multiple longitudinal markers are collected when the true latent\ntrait of interest is hard to capture (e.g. quality of life, functional\ndependency, cognitive level). These multivariate and longitudinal data also\nusually have nonstandard distributions (discrete, asymmetric, bounded,...). We\npropose a joint model based on a latent process and latent classes to analyze\nsimultaneously such multiple longitudinal markers of different natures, and\nmultiple causes of progression. A latent process model describes the latent\ntrait of interest and links it to the observed longitudinal outcomes using\nflexible measurement models adapted to different types of data, and a latent\nclass structure links the longitudinal and the cause-specific survival models.\nThe joint model is estimated in the maximum likelihood framework. A score test\nis developed to evaluate the assumption of conditional independence of the\nlongitudinal markers and each cause of progression given the latent classes. In\naddition, individual dynamic cumulative incidences of each cause of progression\nbased on the repeated marker data are derived. The methodology is validated in\na simulation study and applied on real data about cognitive aging coming from a\nlarge population-based study. The aim is to predict the risk of dementia by\naccounting for the competing death according to the profiles of semantic memory\nmeasured by two asymmetric psychometric tests.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 15:06:46 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 09:53:06 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Proust-Lima", "C\u00e9cile", ""], ["Dartigues", "Jean-Fran\u00e7ois", ""], ["Jacqmin-Gadda", "H\u00e9l\u00e8ne", ""]]}, {"id": "1409.7675", "submitter": "Mauricio Romero", "authors": "Mauricio Romero and Alvaro Riascos and Diego Jara", "title": "A derivation of the optimal answer-copying index and some applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple-choice exams are frequently used as an efficient and objective\nmethod to assess learning but they are more vulnerable to answer-copying than\ntests based on open questions. Several statistical tests (known as indices in\nthe literature) have been proposed to detect cheating; however, to the best of\nour knowledge they all lack mathematical support that guarantees optimality in\nany sense. We partially fill this void by deriving the uniform most powerful\n(UMP) under the assumption that the response distribution is known. In\npractice, however, we must estimate a behavioral model that yields a response\ndistribution for each question. We calculate the empirical type-I and type-II\nerror rates for several indices that assume different behavioral models using\nsimulations based on real data from twelve nationwide multiple-choice exams\ntaken by 5th and 9th graders in Colombia. We find that the index with the\nhighest power among those studied, subject to the restriction of preserving the\ntype-I error, is one based on the work of Wollack (1997) and Linden and\nSotaridona (2006) and is superior to the indices studied and developed by\nWesolowsky (2000) and Frary, Tideman, and Watts (1977). We compare the results\nof applying this index to all 12 exams and find that examination rooms with\nstricter proctoring have a lower level of copying. Finally, a Bonferroni\ncorrection to control for the false positive rate is proposed to detect massive\ncheating.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 19:36:09 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Romero", "Mauricio", ""], ["Riascos", "Alvaro", ""], ["Jara", "Diego", ""]]}, {"id": "1409.7686", "submitter": "Matthias K\\\"ummerer", "authors": "Matthias K\\\"ummerer, Thomas Wallis, Matthias Bethge", "title": "How close are we to understanding image-based saliency?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.NC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the set of the many complex factors driving gaze placement, the\nproperities of an image that are associated with fixations under free viewing\nconditions have been studied extensively. There is a general impression that\nthe field is close to understanding this particular association. Here we frame\nsaliency models probabilistically as point processes, allowing the calculation\nof log-likelihoods and bringing saliency evaluation into the domain of\ninformation. We compared the information gain of state-of-the-art models to a\ngold standard and find that only one third of the explainable spatial\ninformation is captured. We additionally provide a principled method to show\nwhere and how models fail to capture information in the fixations. Thus,\ncontrary to previous assertions, purely spatial saliency remains a significant\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 19:59:44 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["K\u00fcmmerer", "Matthias", ""], ["Wallis", "Thomas", ""], ["Bethge", "Matthias", ""]]}, {"id": "1409.7715", "submitter": "Libo Sun", "authors": "Libo Sun, Chihoon Lee, Jennifer A. Hoeting", "title": "Parameter inference and model selection in deterministic and stochastic\n  dynamical models via approximate Bayesian computation: modeling a wildlife\n  epidemic", "comments": "24 pages, 4 figures, submitted", "journal-ref": "Environmetrics 2015, 26: 451-462", "doi": "10.1002/env.2353", "report-no": null, "categories": "stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting deterministic or stochastic models for a\nbiological, ecological, or environmental dynamical process. In most cases, one\nprefers either deterministic or stochastic models as candidate models based on\nexperience or subjective judgment. Due to the complex or intractable likelihood\nin most dynamical models, likelihood-based approaches for model selection are\nnot suitable. We use approximate Bayesian computation for parameter estimation\nand model selection to gain further understanding of the dynamics of two\nepidemics of chronic wasting disease in mule deer. The main novel contribution\nof this work is that under a hierarchical model framework we compare three\ntypes of dynamical models: ordinary differential equation, continuous time\nMarkov chain, and stochastic differential equation models. To our knowledge\nmodel selection between these types of models has not appeared previously.\nSince the practice of incorporating dynamical models into data models is\nbecoming more common, the proposed approach may be very useful in a variety of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 20:33:25 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 16:39:41 GMT"}], "update_date": "2015-10-26", "authors_parsed": [["Sun", "Libo", ""], ["Lee", "Chihoon", ""], ["Hoeting", "Jennifer A.", ""]]}, {"id": "1409.7994", "submitter": "Byung Mook Weon", "authors": "Yeseul Kim and Byung Mook Weon", "title": "Randomness is valid at large numbers", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomness is a central concept to statistics and physics. Here, a\nstatistical analysis shows experimental evidence that tossing coins and finding\nlast digits of prime numbers are identical regarding statistics for equally\nlikely outcomes. This analysis explains why randomness in equally likely\noutcomes can be valid only at large numbers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 04:14:12 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 08:23:57 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Kim", "Yeseul", ""], ["Weon", "Byung Mook", ""]]}, {"id": "1409.8109", "submitter": "Sara Sommariva", "authors": "Sara Sommariva and Alberto Sorrentino", "title": "Sequential Monte Carlo samplers for semilinear inverse problems and\n  application to magnetoencephalography", "comments": "26 pages, 6 figures", "journal-ref": "Inverse Problems 30 (2014) 114020", "doi": "10.1088/0266-5611/30/11/114020", "report-no": null, "categories": "stat.AP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the use of a recent class of sequential Monte Carlo methods for\nsolving inverse problems characterized by a semi-linear structure, i.e. where\nthe data depend linearly on a subset of variables and nonlinearly on the\nremaining ones. In this type of problems, under proper Gaussian assumptions one\ncan marginalize the linear variables. This means that the Monte Carlo procedure\nneeds only to be applied to the nonlinear variables, while the linear ones can\nbe treated analytically; as a result, the Monte Carlo variance and/or the\ncomputational cost decrease. We use this approach to solve the inverse problem\nof magnetoencephalography, with a multi-dipole model for the sources. Here,\ndata depend nonlinearly on the number of sources and their locations, and\ndepend linearly on their current vectors. The semi-analytic approach enables us\nto estimate the number of dipoles and their location from a whole time-series,\nrather than a single time point, while keeping a low computational cost.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 13:15:29 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Sommariva", "Sara", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1409.8137", "submitter": "Benjamin Hackl", "authors": "Benjamin Hackl, Daniel Kurz, Clemens Heuberger, J\\\"urgen Pilz, Martin\n  Deutschmann", "title": "A statistical noise model for a class of Physically Unclonable Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in \"Physically Unclonable Function\"-devices has increased\nrapidly over the last few years, as they have several interesting properties\nfor system security related applications like, for example, the management of\ncryptographic keys. Unfortunately, the output provided by these devices is\nnoisy and needs to be corrected for these applications.\n  Related error correcting mechanisms are typically constructed on the basis of\nan equal error probability for each output bit. This assumption does not hold\nfor Physically Unclonable Functions, where varying error probabilities can be\nobserved. This results in a generalized binomial distribution for the number of\nerrors in the output.\n  The intention of this paper is to discuss a novel Bayesian statistical model\nfor the noise of an especially wide-spread class of Physically Unclonable\nFunctions, which properly handles the varying output stability and also\nreflects the different noise behaviors observed in a collection of such\ndevices. Furthermore, we compare several different methods for estimating the\nmodel parameters and apply the proposed model to concrete measurements obtained\nwithin the CODES research project in order to evaluate typical correction and\nstabilization approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 14:48:57 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Hackl", "Benjamin", ""], ["Kurz", "Daniel", ""], ["Heuberger", "Clemens", ""], ["Pilz", "J\u00fcrgen", ""], ["Deutschmann", "Martin", ""]]}, {"id": "1409.8202", "submitter": "Matteo De Felice", "authors": "Matteo De Felice, Marcello Petitta, Paolo M. Ruti", "title": "Short-Term Predictability of Photovoltaic Production over Italy", "comments": "Submitted to Renewable Energy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Photovoltaic (PV) power production increased drastically in Europe throughout\nthe last years. About the 6% of electricity in Italy comes from PV and for an\nefficient management of the power grid an accurate and reliable forecasting of\nproduction would be needed. Starting from a dataset of electricity production\nof 65 Italian solar plants for the years 2011-2012 we investigate the\npossibility to forecast daily production from one to ten days of lead time\nwithout using on site measurements. Our study is divided in two parts: an\nassessment of the predictability of meteorological variables using weather\nforecasts and an analysis on the application of data-driven modelling in\npredicting solar power production. We calibrate a SVM model using available\nobservations and then we force the same model with the predicted variables from\nweather forecasts with a lead time from one to ten days. As expected, solar\npower production is strongly influenced by cloudiness and clear sky, in fact we\nobserve that while during summer we obtain a general error under the 10%\n(slightly lower in south Italy), during winter the error is abundantly above\nthe 20%.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:24:29 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["De Felice", "Matteo", ""], ["Petitta", "Marcello", ""], ["Ruti", "Paolo M.", ""]]}, {"id": "1409.8481", "submitter": "An Zeng", "authors": "Hao Liao and An Zeng", "title": "Reconstructing propagation networks with temporal similarity metrics", "comments": "8 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Node similarity is a significant property driving the growth of real\nnetworks. In this paper, based on the observed spreading results we apply the\nnode similarity metrics to reconstruct propagation networks. We find that the\nreconstruction accuracy of the similarity metrics is strongly influenced by the\ninfection rate of the spreading process. Moreover, there is a range of\ninfection rate in which the reconstruction accuracy of some similarity metrics\ndrops to nearly zero. In order to improve the similarity-based reconstruction\nmethod, we finally propose a temporal similarity metric to take into account\nthe time information of the spreading. The reconstruction results are\nremarkably improved with the new method.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:06:59 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Liao", "Hao", ""], ["Zeng", "An", ""]]}, {"id": "1409.8500", "submitter": "Radu Horaud P", "authors": "Antoine Deleforge, Florence Forbes, Sileye Ba and Radu Horaud", "title": "Hyper-Spectral Image Analysis with Partially-Latent Regression and\n  Spatial Markov Dependencies", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": "IEEE Journal on Selected Topics in Signal Processing, volume 9,\n  number 6, 1037-1048, 2015", "doi": "10.1109/JSTSP.2015.2416677", "report-no": null, "categories": "stat.AP cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyper-spectral data can be analyzed to recover physical properties at large\nplanetary scales. This involves resolving inverse problems which can be\naddressed within machine learning, with the advantage that, once a relationship\nbetween physical parameters and spectra has been established in a data-driven\nfashion, the learned relationship can be used to estimate physical parameters\nfor new hyper-spectral observations. Within this framework, we propose a\nspatially-constrained and partially-latent regression method which maps\nhigh-dimensional inputs (hyper-spectral images) onto low-dimensional responses\n(physical parameters such as the local chemical composition of the soil). The\nproposed regression model comprises two key features. Firstly, it combines a\nGaussian mixture of locally-linear mappings (GLLiM) with a partially-latent\nresponse model. While the former makes high-dimensional regression tractable,\nthe latter enables to deal with physical parameters that cannot be observed or,\nmore generally, with data contaminated by experimental artifacts that cannot be\nexplained with noise models. Secondly, spatial constraints are introduced in\nthe model through a Markov random field (MRF) prior which provides a spatial\nstructure to the Gaussian-mixture hidden variables. Experiments conducted on a\ndatabase composed of remotely sensed observations collected from the Mars\nplanet by the Mars Express orbiter demonstrate the effectiveness of the\nproposed model.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:59:01 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2015 16:09:52 GMT"}], "update_date": "2015-12-31", "authors_parsed": [["Deleforge", "Antoine", ""], ["Forbes", "Florence", ""], ["Ba", "Sileye", ""], ["Horaud", "Radu", ""]]}, {"id": "1409.8502", "submitter": "Juho Kokkala", "authors": "Juho Kokkala and Simo S\\\"arkk\\\"a", "title": "Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking", "comments": "Revised version. 43 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.dsp.2015.04.004", "report-no": null, "categories": "stat.ME math.DS math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:59:41 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 21:03:46 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kokkala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}]